{"id": "qJsShqQgWw", "number": 16657, "cdate": 1758267349926, "mdate": 1759897226890, "content": {"title": "Algorithm Generation via Creative Ideation", "abstract": "Designing system algorithms remains challenging, where the discontinuous nature of the solution space often forces system engineers to rely on generic heuristics at the expense of performance. We study whether LLMs can practically drive algorithm generation, and find that they are biased towards well-known generic designs, rather than making the creative leaps needed to navigate the discontinuous solution space. To address this limitation, we introduce MetaMuse, a framework for creative ideation built on three self-reflection principles: (1) quantifying solution diversity and usefulness in measurable performance space, rather than abstract idea space, (2) steering ideation through external stimuli, rather than internal randomness, and (3) constructing executable solutions using waypoint reasoning, rather than free-form chain-of-thought. Extensive evaluation shows that MetaMuse can generate high-performing solutions for two critical problems at a global cloud provider: cache replacement (reducing cache misses by up to 35.76%) and online bin packing (reducing bin usage by up to 30.93%).", "tldr": "", "keywords": ["system algorithm design", "creativity", "ideation", "self-reflection"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58594bf13ba09e120c0e8859a77da052ef00c648.pdf", "supplementary_material": "/attachment/83a013d32eaede2a697f27b8ea0cd545db820bc4.zip"}, "replies": [{"content": {"summary": {"value": "The goal is to generate algorithms with LLMs. The paper notes that diversity is necessary for generating good algorithms, but that LLMs are inherently bad at creating diverse output. A method is thus proposed to encourage LLMs to generate diverse outputs using a rather clever mechanism that adds a completely unrelated set of words to the prompt input. The method uses a Gaussian Process Regression (GPR) model that tries to find a set of words that will result in diverse outputs. The method is tested on a \"cache replacement\" problem and an online bin packing problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is simple and effective.\n2. The paper follows a strong line of argumentation that goes through the entire work.\n3. The paper is relatively easy to understand (with one small exception, see below) and I think the method should be fairly easy to implement for anyone else."}, "weaknesses": {"value": "The main weaknesses of this paper lie in the experiments.\n\n1. The problems chosen are kind of strange picks, since many of the approaches compared against do not use these two problems.\n2. The training is apparently cheap, but only two problems are compared against, which does not give me so much confidence in the method.\n3. The waypoint reasoning is not entirely clear to me. Maybe I am overthinking it, but I do not entirely understand the flow of the prompts leading to an algorithm. The flowchart in Figure 2 is not helping -- certainly you do not just prompt with \"flower\". Please not that any text in the Appendix about this is insufficient -- this needs to be clear in the main paper.\n4. The results for the online bin packing are somewhat unsatisfying, as I do not see any state-of-the-art methods discussed or compared against. Beating a bunch of simple heuristics does not seem like the best argument for this work. \n5. I also wonder why only online algorithms are chosen here rather than the problems in, e.g,. ReEvo or OpenEvolve."}, "questions": {"value": "1. See point about waypoints.\n2. See last point of weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "li4ki9l6CH", "forum": "qJsShqQgWw", "replyto": "qJsShqQgWw", "signatures": ["ICLR.cc/2026/Conference/Submission16657/Reviewer_eCqA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16657/Reviewer_eCqA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756767188, "cdate": 1761756767188, "tmdate": 1762926717317, "mdate": 1762926717317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MetaMuse offers a general framework for helping LLMs break free of their inherent availability bias, in order to generate meaningfully diverse, creative solutions to difficult problems -- particularly those with complex, non-linear, discontinuous landscapes within and/or between the design and performance spaces. MetaMuse describes three principles of self-reflection that can help diversify LLM responses by activating and applying knowledge from probabilistically unrelated concepts (\"external stimuli\"). The principles (1) explicitly measure existing solution diversity, and (2) use these metrics to select external stimuli that steer future iterations toward different approaches. Given a set of stimuli, MetaMuse (3) develops an executable implementation using waypoint reasoning, which encourages non-trivial analysis and application of the external stimuli in the context of the problem at hand. The authors demonstrate the efficacy and generalizability of their approach by showing strong performance across several different models and two real-world problem domains (cache replacement and bin packing)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well written, and the approach within is well motivated. The proposed principles of ideation and self-reflection are thought provoking, and serve as a great baseline for systematizing the elusive concept of creative ideation. The ideas are intentionally described in a general manner, and could easily be applied or adapted to many exciting domains. The present manuscript already offers two compelling case studies, with extensive baseline comparisons that highlight MetaMuse's strong performance on both. The ablation studies offer strong evidence that the proposed principles are effective and essential for achieving the stated performance. Overall, this paper offers considerable value and impact for the scientific community."}, "weaknesses": {"value": "1. The authors partially motivate their work by citing that previous solutions ignore higher-level design parameters like data structure selection, in favor of tweaks to the scoring function. Yet, the authors measure diversity in performance/feedback space, and explicitly dismiss the efficacy of observing semantic differences in language or code, including data structure changes (l. 200-205). It seems like the emphasis on performance-only diversity metrics would also fail to incentivize/meaningfully reward the higher-level tweaks, but these larger jumps in algorithmic design space feel critical. How often do you witness higher-level changes in your experiments? How much of the increased diversity stems from scoring function changes (or other tweaks mirroring previous works' exhibited tendencies) vs. more novel types of changes? Would you consider the examples discussed in Section 4.4 to be scoring function changes, or larger-scope ones (this might be clear to a system algorithms expert, but wasn't clear to me)? I would be interested to see more discussion on this point.\n\n2. The coding safeguards are referenced several times (e.g. l. 299, 393), but the relevant discussion in Section 5 is quite limited; I like the idea quite a lot, but I expected more based on the earlier mentions. To what extent are these safeguards actually realized vs. hypothetical? How often are they used/required in your experiments? I would like to see this piece either clarified (with a detailed description, perhaps in the appendix) or tempered in the earlier references."}, "questions": {"value": "Please see weaknesses. Additionally:\n\n1. Figure 1 / l. 137 - is the clustering algorithm such that it forms a partition of the 1200 solutions, based on the closest human-derived heuristic in the set? i.e., if something is sufficiently unique (like the solutions described in l.399-409), would it have to belong to the nearest cluster, or would it be separately labeled? If it's the former, the mere presence of the cluster isn't necessarily compelling -- it's there by construction. We'd have to know what the radius/desnity pattern of the cluster was, similar to the discussion you present in l. 376. Perhaps a visualization like a scatter plot might be more illustrative in that case. Relatedly, as someone with little knowledge about cache replacement, I'm curious whether the bias toward LRU/LFU/FIFO might reflect something deeper than mere availability/prevalence bias. For example: since you're measuring by similarity of hit ratios, do the algorithms with large clusters have some inherent performance advantage? This might explain the demonstrated bias in a few ways: maybe it makes sense for the LLM to offer variants of these, since they perform well; alternatively, maybe the metric causes highly performant algorithms cluster near these baselines, even if the underlying approach is different.\n\n2. l. 210 -- how important is it that the performance metrics be bounded? What would be required to accommodate unbounded metrics? Relatedly, how well could this approach generalize to multiple metrics? What are the scaling considerations?\n\n3. l. 259 -- when using RSDict-RF, are the $w$ RSDict solutions for bootstrapping done as a preprocessing step before the experiment (such that $c_0$ is based on RSDict-RF stimuli) or are they used as part of the exploration (such that $c_0$ ... $c_w$ are the result of RSDict and only $c_{w+1}$ stem from RSDict-RF stimuli)?\n\n4. Figures 3/4 -- could you clarify the phrase \"top...solutions generated by...\"? Specifically, is each box plot representative of the single best solution from each model (where the displayed variance comes from that single solution performing over 96/288 traces), or does each box plot reflect the top n solutions for each model (each performing over the 96/288 traces, then also averaged together)?\n\n\n## Minor comments\n- l. 87 - the antecedent of \"This\" is unclear\n- l. 137 - \"centroids of human heuristics\" was unclear to me until reading the explanation in l. 375.\n- l. 244 - \"in embedding\" --> \"in the (feedback) embedding\"\n- l. 399 - NSE is not defined; it would be helpful to expand the acronym or provide a short explanation for context"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OaVis9ox3l", "forum": "qJsShqQgWw", "replyto": "qJsShqQgWw", "signatures": ["ICLR.cc/2026/Conference/Submission16657/Reviewer_7Wx3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16657/Reviewer_7Wx3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943223332, "cdate": 1761943223332, "tmdate": 1762926716843, "mdate": 1762926716843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the challenge of algorithm generation using Large Language Models (LLMs), specifically addressing the issue where LLMs tend to generate known heuristics due to \"availability bias.\" The authors propose a framework named MetaMuse to overcome this bias by evaluating diversity in the performance space, steering ideation with external stimuli, and employing structured waypoint reasoning. Experiments demonstrate that this approach outperforms LLM baselines and human-designed algorithms on cache replacement and online bin packing problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Automatic algorithm design is a frontier in LLM applications, and the paper's exploration of LLM's creative limitations is highly valuable.\nThe proposed \"external stimuli\" and \"waypoint reasoning\" offer a novel approach to enhance LLM performance on algorithmic tasks, distinct from traditional evolutionary methods."}, "weaknesses": {"value": "1. The paper fails to specify the exact version numbers or model checkpoints for the public APIs used (e.g., GPT-40). Given the rapid iteration of LLM models, this makes the experimental results nearly impossible to reproduce accurately.\n\n2. While the paper provides a final monetary cost estimate in ยง4.3, this analysis is superficial. It completely overlooks the analysis of token consumption, a core driver of cost. This omission prevents readers from assessing the framework's true computational overhead and token efficiency, and precludes any meaningful cost-benefit comparison with other methods.\n\n3. The effectiveness of external stimuli lacks theoretical support, and its success might be coincidental. Its generalizability and stability are questionable, making it seem more like a sophisticated random search than a reliable method for creative ideation.\n\n4. The comparison with state-of-the-art baselines like OpenEvolve is methodologically shallow. The paper's central thesis is that MetaMuse overcomes availability bias while existing methods do not. However, the comparison only presents final performance metrics without providing evidence to support this core claim. A convincing comparison should have analyzed and shown whether the solutions from baselines like OpenEvolve indeed cluster around known heuristics as theorized, while MetaMuse successfully explores a broader, more novel solution space."}, "questions": {"value": "1. Code Correctness Verification: The generated code's safety (e.g., against timeouts) is mentioned. How was the logical correctness of the code (i.e., does the implementation correctly match the algorithm's description) verified? What was the first-pass success rate of the code generation step?\n\n2. Experimental Cost Analysis: Could you provide a detailed breakdown of the token costs for generating a single solution with MetaMuse, including prompt and completion tokens for each stage? How does the framework's token efficiency compare to a simple repeated sampling baseline?\n\n3. API Versioning: To ensure reproducibility, please provide the specific API version numbers or model release dates for GPT-40, Llama3.3-70B, and DeepSeek-V3 used in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UavgL5jxAx", "forum": "qJsShqQgWw", "replyto": "qJsShqQgWw", "signatures": ["ICLR.cc/2026/Conference/Submission16657/Reviewer_wtEQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16657/Reviewer_wtEQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994601926, "cdate": 1761994601926, "tmdate": 1762926716487, "mdate": 1762926716487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MetaMuse to address the lack of creativity in LLMs when applied to algorithm design. The framework controls the LLM to generate novel algorithms through the use of external \"keywords\" and a structured chain-of-thought (Waypoint Reasoning). The method demonstrates notable performance improvements on the two evaluated system problems: cache replacement and online bin packing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and easy to understand. The authors exhibit a deep and accurate understanding of the diversity problem inherent in LLM-based algorithm design.\n\n2. The use of performance as a feature for algorithms is a smart and feasible approach. This overcomes the difficulty in clustering and analysis caused by the discontinuous nature of the algorithm space.\n\n3. The attribute extraction and problem mapping of the keywords are interesting and make sense. The experiments successfully demonstrate that this external stimulation mechanism significantly increases the diversity of algorithms generated by the LLM.\n\n4. The paper is largely successful in its argument and introduces a degree of novelty to the field of LLM-driven algorithm design."}, "weaknesses": {"value": "1. Missing Mechanism for Incremental Improvement: The framework overly emphasizes \"Eureka\" ideas to improve algorithm quality. However, iterative algorithm design often requires continuity and gradual improvement. Significant gains often come from small adjustments (e.g., hyperparameter tuning) to a good algorithm. MetaMuse currently lacks a mechanism to effectively capture and execute this type of small-scale, continuous refinement, focusing only on large, creative leaps.\n\n2. Insufficient Justification for \"Unbiased Stimuli\": The authors state that \"Stimuli should be unbiased to the problem, forcing LLMs to associate with knowledge that seems probabilistically irrelevant.\" While this is experimentally successful, the theoretical justification needs strengthening. Why are completely neutral keywords more effective than carefully curated, domain-relevant concepts in avoiding the availability bias? As demonstrated in the authors' examples, unrelated keywords must still first be linked to domain-relevant concepts before further reasoning can occur. Furthermore, the authors should supplement the experiments by showing the performance of non-neutral keywords to demonstrate the necessity of using unbiased stimuli, and discuss the risk of generating incoherent algorithms due to unrelated stimuli.\n\n3. Scalability Concerns with RSDict-SF: The RSDict-SF strategy relies on building a performance prediction model (GPR), which requires a large amount of multi-dimensional feedback embedding data. The practicality of RSDict-SF is limited for domain problems where each evaluation is prohibitively expensive. The paper should offer a more in-depth discussion on how MetaMuse can be adapted to handle sparse feedback or difficult-to-quantify tasks.\n\n4.  Limited Application Scope: The framework is only validated on two specific system problems (cache replacement and online bin packing). Its robustness and generalizability to other types of algorithmic design problems remain to be thoroughly verified.\n\n5. Missing Hyperparameter Analysis: The paper lacks analysis regarding the determination and sensitivity of the framework's hyperparameters. This omission impacts the reproducibility and reliability of the reported results."}, "questions": {"value": "1. Please thoroughly check the paper for adherence to formatting guidelines. For example, acronyms like LRU should only be defined upon their first occurrence.\n\n2. FunSearch and EoH have shown promising results in the Online Bin Packing problem. Why were these LLM-based approaches not included in the experimental comparison? This absence makes it difficult to assess MetaMuse's actual advantage over current advanced techniques.\n\n3. In Figure 1, please clarify how Repeated sampling with previous solutions is specifically implemented. Does the input of previous solutions include all prior sampling results, or only a recent subset? Furthermore, a detailed comparison with this baseline is more crucial than a simple Repeated Sampling analysis to substantiate the paper's claim regarding diversity.\n\n4. Figure 1 illustrates the failure of LLMs to explore the algorithm space effectively. Please clarify what an ideal sampling distribution looks like under the authors' claims (I assume a uniform distribution across different clusters?). More importantly, the authors should display the actual sampling points of MetaMuse to demonstrate that the framework successfully overcomes local clustering and achieves effective exploration of the discontinuous space.\n\n5. The diversity evaluation in MetaMuse is strongly correlated with the distribution and number of traces used to generate the performance feedback embeddings, which directly impacts the clustering results. What is the effect of these factors on MetaMuse? The authors should consider and present how the performance of the generated algorithms is affected by the variance and quantity of the evaluation traces."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pAuv2XYjjk", "forum": "qJsShqQgWw", "replyto": "qJsShqQgWw", "signatures": ["ICLR.cc/2026/Conference/Submission16657/Reviewer_kD1D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16657/Reviewer_kD1D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083390223, "cdate": 1762083390223, "tmdate": 1762926716088, "mdate": 1762926716088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}