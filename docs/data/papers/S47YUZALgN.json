{"id": "S47YUZALgN", "number": 581, "cdate": 1756750307071, "mdate": 1763364093793, "content": {"title": "Classifier-Driven Diffusion Model and Plug-and-Play of Weakly-Supervised Learning for Conditional Generation", "abstract": "Can a diffusion model for conditional generation be trained as a classifier? We address this question with the Classifier-Driven Diffusion Model (CLDDM), which trains a diffusion model by minimizing a per-timestep cross-entropy loss under class-label supervision, while achieving high-quality class-conditional generation. In other words, CLDDM establishes a unified framework that demonstrates the equivalence between classification and generation. This equivalence enables new training strategies for conditional diffusion models. In particular, we show that ``weakly supervised'' generation can be realized by leveraging established classification objectives from weakly supervised learning. Experimental results on a toy dataset and image benchmarks demonstrate both quantitative and qualitative equivalence between CLDDM and standard diffusion models, and further confirm that CLDDM supports conditional generation under weak supervision, such as learning with noisy labels and learning from label proportions.", "tldr": "We propose novel diffusion model trained with multi-class classification and plug-and-play of weakly-supervised learning for conditional generation", "keywords": ["Diffusion model", "weakly supervised learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c04ee30cca70b9c7975e9397582200d888c3767b.pdf", "supplementary_material": "/attachment/0a7a255a657442fa8ce0abedc0f6cb3effc57748.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents an innovative approach to diffusion modeling by first training a classifier and then leveraging it for generation. This is achieved by minimizing a per-timestep cross-entropy loss under class-label supervision during training. Experiments are conducted on the CIFAR-10 and 2D Gaussian mixture datasets to validate the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The author presents an innovative investigation into how generative models can be utilized for classification tasks. In contrast, this work explores the reverse direction: training a classifier first, then using it for generation.\n\n* The author demonstrates an equivalence between the cross-entropy loss used in their framework and the noise prediction loss employed in standard diffusion models."}, "weaknesses": {"value": "* The motivation appears relatively weak. In line 42, the author states, \"In other words, the framework flows: train generation, then use for classification.\" This aligns with intuition, as generation tasks (taking y as input to produce x ) are inherently more complex than classification tasks (taking x as input to predict y ). Therefore, it is reasonable to expect that generative models can support classification. However, the author aims to explore the reverse direction—\"train classification, then use for generation\"—which seems less intuitive and potentially more challenging.\n\n* A notable limitation of the proposed \"train classification, then use for generation\" methodology is the requirement to specify class labels during cross-entropy training. This constraint may hinder the model's ability to perform zero-shot conditioned generation.\n\n* The research evaluates the proposed methodology using CIFAR-10 and 2D Gaussian mixture datasets. However, I am concerned that the \"train classification, then use for generation\" approach may constrain the diversity and zero-shot capabilities of the diffusion model. Given the simplicity of the chosen datasets, the experiments may not sufficiently demonstrate whether generation diversity is preserved. While large-scale experiments are not strictly necessary, it would be valuable to include additional experiments that convincingly show CLDDM does not compromise diversity or generalization capacity.\n\n\n* The results presented in Tables 3 and 4 do not provide compelling evidence that CLDDM achieves superior performance compared to standard diffusion models."}, "questions": {"value": "* During cross-entropy training, in addition to computing the logit for the target class (used in the numerator), it is also necessary to calculate logits for all remaining classes to form the denominator. This raises the question: does this approach incur significantly more computational overhead compared to standard diffusion models, which typically require computation only for the corresponding class?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PHKd0bApoT", "forum": "S47YUZALgN", "replyto": "S47YUZALgN", "signatures": ["ICLR.cc/2026/Conference/Submission581/Reviewer_gBqt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission581/Reviewer_gBqt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760929169974, "cdate": 1760929169974, "tmdate": 1762915552956, "mdate": 1762915552956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you for the constructive reviews. We take the reviewer's suggestions into account and realize that the study needs improvement and the paper requires significant changes."}}, "id": "w9QqExIR3C", "forum": "S47YUZALgN", "replyto": "S47YUZALgN", "signatures": ["ICLR.cc/2026/Conference/Submission581/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission581/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763364092932, "cdate": 1763364092932, "tmdate": 1763364092932, "mdate": 1763364092932, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Classifier-Driven Diffusion Models (CLDDM). This framework trains a conditional diffusion model purely with per-timestep classification objectives (cross-entropy plus a simple regularizer) yet still enables direct class-conditional generation. The key technical move is to define a per-timestep classifier using logits derived from the standard noise-prediction loss and to train with cross-entropy; when the regularization weight = 1, the objective is provably equivalent to the standard diffusion loss (Appendix A). The same construction yields a plug-and-play route to weakly-supervised learning (WSL) for diffusion models by swapping the cross-entropy with established WSL losses (e.g., forward correction for noisy labels and proportion loss for label proportions). Experiments on 2D Gaussian mixtures, MNIST, and CIFAR-10 show that CLDDM matches standard diffusion models on supervised training and enables conditional generation under WSL settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Originality: The attempt to train a diffusion model with a class-discriminative loss is new.\n\n- Scope: This paper covers supervised conditional learning as well as several semi-supervised settings."}, "weaknesses": {"value": "- Originality: Training a diffusion model with a discriminative loss is not a very new concept. Please consider comparing with Discriminator Guidance [1] and Direct Discriminative Optimization [2]. They focused on real vs. fake, but real and fake can also be seen as a binary class.\n\n- Poor baseline: The method only compares to the vanilla diffusion models. It could also be compared with the papers you cited as semi-supervised diffusion learning.\n\n- Missing ablation on $\\lambda$. What happens if $\\lambda=0$?\n\n- For SL, the author uses  $\\lambda=1$, which is identical to the baseline. Then what is the meaning of Table 1?\n\n\n\n\n[1] (ICML 23) Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models\n\n\n[2] (ICML 25) Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator"}, "questions": {"value": "- Without regularization in Eq.10, why does the denominator in Eq.9 diverge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "s81wK0WKl5", "forum": "S47YUZALgN", "replyto": "S47YUZALgN", "signatures": ["ICLR.cc/2026/Conference/Submission581/Reviewer_CrGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission581/Reviewer_CrGb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552545443, "cdate": 1761552545443, "tmdate": 1762915552812, "mdate": 1762915552812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes reformulating the conditional diffusion model objective as a classification task. This approach allows weakly-supervised learning methods based on cross-entropy loss, originally developed for traditional classification tasks, to be directly applied to diffusion model training without requiring additional modifications. The paper presents examples focusing on two types of weak supervision: learning with noisy labels and learning from label proportions. Through toy experiments and evaluations on the MNIST and CIFAR-10 datasets, the paper demonstrates the applicability of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Weakly-supervised learning is an important problem in diffusion models, yet it has been less explored compared to supervised learning tasks.\n* In particular, diffusion models introduce the additional dimension of diffusion timesteps, which makes their application less intuitive compared to other generative models. Developing a unified framework to integrate this aspect can be highly valuable.\n* The derivation of the proposed method is intuitive and well-written."}, "weaknesses": {"value": "* In the Introduction, the motivation for reformulating diffusion models with a classification objective should be clarified. Simply stating that the \"reverse direction\" has been underexplored is not sufficiently strong reason. The fact that this reformulation enables the direct application of existing weakly-supervised learning methods based on cross-entropy loss could serve as a stronger motivation and should be mentioned early in the introduction.\n\n* A theoretical justification is needed for why the classifier can be formulated as in Eq. (9). For example, it is known that the standard diffusion loss $l_{DM}$, based on the L2 loss, can be interpreted as a bound on the log-likelihood given an proper temporal weighting function (Song et al., NeurIPS 2021). In this paper, $p_\\theta(y|x_t, t)$ is defined bia a softmax over $-l_{DM}$, and the model is trained using cross entropy $L_{ce}$. It remains unclear in which direction $p_\\theta$ is optimized with respect to the data distribution under this formulation.\n\n(Song et al., NeurIPS 2021) Maximum Likelihood Training of Score-Based Diffusion Models.\n\n* The claim that the generative objective is not directly applied is somewhat questionable. Similar to the derivation of Appendix A, for general $\\lambda_{reg}$, one can derive that $L_{CLDDM}$ is equivalent to $l_{DM}(x_t, t, y^*)+ (1-\\lambda_{reg}) \\log \\sum \\exp (- l_{DM})$. This indicates that the proposed objective is essentially a regularized version of the standard generative loss, where the additional term acts as a regularizer weighted by $(1-\\lambda_{reg})$. Thus, I think that the main methodological difference lies in the introduction of $L_{reg}$.\n\n* Given this, the role of $L_{reg}$ should be discussed beyond its function of preventing divergence in the denominator. Specifically, how does introducing $L_{reg}$ alter the training dynamics compared to using only $L_{ce}$? This discussion is currently missing.\n\n* The current manuscript also appears to overclaim that equivalence between the CE-based formulation and the original generative objective. The equivalence only holds when the regularization term is included and $\\lambda_{reg}=1$. Since the regularization term significantly affects the relationship between the two formulations, it should not be treated as a minor regularization for divergence control, but as a core component influencing equivalence.\n\n* In the experiments, the supervised learning comparison is conducted with $\\lambda_{reg}=1$, which makes the objective identical to the original generative formulation and thus trivially yields the same results. The paper should report and discuss the performance variations with respect to $\\lambda_{reg}$, including results for different values in the supervised setting.\n\n* Although the regularization term is described as \"simple\", in practice it requires computing $\\epsilon$ for all conditions, which can be computationally expensive. The potential computational burden and scalability of the proposed method should be discussed. While the paper mentions using a multi-head setup from prior work, this approach may also introduce challenges. For example, the need to train new heads from scratch, which can be problematic for large diffusion models where pre-trained weights are typically required. Discussion on whether and how pre-trained model can be effectively utilized is needed.\n\n* The method is only applicable to cross-entropy-style weakly supervised learning approaches. It would be better to discuss whether this limitation affects the generality of the framework. Moreover, the paper only experiments with relatively old and base WSL methods, which makes the empirical evaluation incomplete. So, the applicability to more recent WSL methods should be verified, as many of them operate under weaker assumptions and achieve improved performance compared to traditional methods.\n\n* A comparison with existing diffusion-based WSL appraoches is necessary, both methodologically and experimentally. For example, it would be valuable to compare with Na et al. (2024), which also addresses learning under noisy labels, as mentioned in the paper, to better position the proposed method and demonstrate its advantages or differences.\n\n* It would be beneficial to extend the experimental validation to higher-resolution datasets or more complex tasks (e.g., continuous or text-conditioned labels) to verify the method's scalability and general applicability.\n\n* Minor points\n  - In Eq. (4), the variance term on the right-hand side should be expressed in matrix form, i.e., $\\beta_t I$.\n  - In Eq. (10), the variable $y$ in $L_{ce}$ should be replaced with $y^*$.\n  - In Section 4.1, the citation for Chen et al. (2024) should be corrected to: Chen et al., ICML 2024, Robust Classification via a Single Diffusion Model."}, "questions": {"value": "Please provide explanations or clarifications to the points raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vh8GsRg7gx", "forum": "S47YUZALgN", "replyto": "S47YUZALgN", "signatures": ["ICLR.cc/2026/Conference/Submission581/Reviewer_FDYt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission581/Reviewer_FDYt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701137407, "cdate": 1761701137407, "tmdate": 1762915552638, "mdate": 1762915552638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a Classifier-Driven Diffusion Model (CLDDM), which trains a diffusion model using a classification objective while still enabling data generation. They also extend the approach to weakly-supervised generation by leveraging established weakly-supervised learning techniques. Experiments are conducted on synthetic 2D Gaussian datasets and on image synthesis tasks using MNIST and CIFAR-10."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Elegant theoretical link between classification and diffusion training objectives.\n\nPractical and flexible framework that supports weakly-supervised generation without requiring architectural changes."}, "weaknesses": {"value": "Experiments are limited to low-resolution datasets (MNIST, CIFAR-10) and a toy 2D Gaussian example.\n\nAnalysis of scalability and performance on more complex or higher-resolution data is missing; only one network architecture is tested for images and a simple MLP for 2D Gaussian data.\n\nNo comparison with other related works.\n\nThe results between the standard diffusion model (DM) and CLDDM are very similar, making it harder to assess the advantages of the proposed method."}, "questions": {"value": "How does the model scale to higher-resolution images (ImageNet? or even STL) or more complex datasets?\n\nCan the CLDDM framework support more diverse weakly-supervised settings, such as partial or complementary labels, beyond noisy labels and label proportions?\n\nHow sensitive is the method to the choice of network architecture or hyperparameters?\n\nCan the authors provide a quantitative analysis of training efficiency or sampling speed compared to standard diffusion models? does it converge faster?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nlOIKtnkYI", "forum": "S47YUZALgN", "replyto": "S47YUZALgN", "signatures": ["ICLR.cc/2026/Conference/Submission581/Reviewer_ruds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission581/Reviewer_ruds"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997421713, "cdate": 1761997421713, "tmdate": 1762915552472, "mdate": 1762915552472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}