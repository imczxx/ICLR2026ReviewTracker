{"id": "TbbMyr3E0x", "number": 13868, "cdate": 1758224049629, "mdate": 1759897407565, "content": {"title": "ARC-AGI Without Pretraining", "abstract": "Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce *CompressARC*, a 76K parameter model without any pretraining that solves 20\\% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a fraction of one sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI \"training set\". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative, highly feasible way to produce intelligence, besides conventional massive pretraining.", "tldr": "We solve 20% of ARC-AGI-1 with no pretraining by minimizing the description length during inference time, with the only data involved being the target puzzle itself.", "keywords": ["ARC-AGI", "compression", "test time training", "CompressARC", "minimum description length"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07beb497357cf453c2c6233f36c991ae7380b2e5.pdf", "supplementary_material": "/attachment/cc19bb344c94d5684eeceea77d1093e06f3a9027.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces a method for solving ARC-AGI-1 evaluation puzzles, based on minimum description length approach (i.e., the so-called code golf). It is worthy of noting that this work does not requite any pretraining, and only trains a rather small neural network during inference time, showing potentials for efficiency and the rid of massive LLMs training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "As shown in the summary:\n1. A novel inference-time perspective for ARC-AGI, as the pertaining is omitted.\n2. Rich discussions on the proposed schemes in the solving process and explanations.\n3. Clarity for pros and cons in the work."}, "weaknesses": {"value": "1. The proposed pipeline may lack formal theoretical analyses for the mathematical  fundaments (which though I think applies to other related methods in this field).\n2. Is it possible to apply MDL to other tasks? How and why is possible/promising?\n3. It seems that the running time for (total iterations and per iteration) remain possible to be greatly improved. How do the author explain such cost/efficiency and what can be done for improvement?\n4. Despite its focus on the pretraining-free advanatage, it would still be nice to have more baselines(related work) comparisons and discussions on the main context."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GBGpOtzYVf", "forum": "TbbMyr3E0x", "replyto": "TbbMyr3E0x", "signatures": ["ICLR.cc/2026/Conference/Submission13868/Reviewer_zdRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13868/Reviewer_zdRF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837290380, "cdate": 1761837290380, "tmdate": 1762924386251, "mdate": 1762924386251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a lightweight CompressARC to solve puzzles from the ARC-AGI-1 benchmark without any pretraining. By applying the Minimum Description Length (MDL) principle during inference, the model learns purely from the target puzzle itself, without using any training set. The results suggest MDL as a data-efficient solution for solving puzzles."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe research addresses a compelling and timely question on data-efficient methods for solving ARC-AGI puzzles, offering a novel alternative to large-scale pretraining.\n2.\tThe manuscript and supplementary materials provide comprehensive implementation details, ensuring reproducibility and methodological clarity.\n3.\tThe experimental results are supported by in-depth analysis of the proposed method."}, "weaknesses": {"value": "1.\tThe emphasis on \"no pretraining\" and single-puzzle focus does not sufficiently establish methodological novelty; it remains unclear whether performance stems from genuine innovation or inherent dataset shortcuts.\n2.\tClaims of novelty are inadequately supported by architectural or algorithmic innovation, as the model relies on established components without clear differentiation.\n3.\tSection 3 lacks intuitive motivation, proceeding directly into technical details without conceptual justification, hindering reader comprehension.\n4.\tThe three core algorithms appear to lack substantive novelty, with insufficient emphasis on what constitutes a technical advance.\n5.\tThe network architecture is relatively conventional, predominantly building on widely adopted components.\n6.\tComparative evaluation with state-of-the-art methods is absent, limiting validation of claimed advantages.\n7.\tGeneralization claims are not fully convincing, as the method may exploit dataset-specific shortcuts rather than learning generalizable reasoning, limiting applicability to tasks like RAVENs."}, "questions": {"value": "1.\tWhat specifically constitutes the novel component(s) in the proposed architecture or algorithm?\n2.\tAppendix K offers supplementary dataset details but lacks illustrative puzzle examples such as the structure of the question panel, the format of a correct solution, and the criteria for matching a proposed answer to the ground truth. Including such examples would significantly improve readers' understanding of the task and the proposed method's problem-solving process.\n3.\tDuring inference-time learning, are solutions to training/test samples used to fine-tune the model parameters?\n4.\tHow are \"steps\" and \"attempts\" formally defined? What operations occur per step, and what constitutes an attempt?\n5.\tHow does the method compare to existing few-shot or zero-shot learning approaches on comparable tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kChm8YxUkS", "forum": "TbbMyr3E0x", "replyto": "TbbMyr3E0x", "signatures": ["ICLR.cc/2026/Conference/Submission13868/Reviewer_Y2bJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13868/Reviewer_Y2bJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970878262, "cdate": 1761970878262, "tmdate": 1762924385729, "mdate": 1762924385729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CompressARC, a 76K-parameter neural model that tackles the ARC-AGI-1 benchmark without any pretraining or usage of the provided training set. The model applies the Minimum Description Length (MDL) principle to infer solutions purely at inference time, treating ARC puzzles as a code-golfing problem—that is, searching for the shortest program capable of reproducing the dataset. Despite its simplicity, CompressARC achieves 20% accuracy on the evaluation set and 34.75% on the training set, which is remarkable given that no pretraining or external data are used."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framing of ARC-AGI solving as a code-golfing / MDL minimization problem is deeply original.\n2. The implementation of inference-time learning via MDL provides a clean and theoretically motivated path to “training-free” intelligence."}, "weaknesses": {"value": "1. Although the 20% accuracy result is interesting, it remains far from state-of-the-art (50%+).\n2. Each puzzle takes 20 minutes and 2000 steps of inference-time optimization, which raises scalability and practicality concerns.\n3. The discussion contrasts CompressARC mainly with large pretrained models (LLMs) but does not include comparisons to smaller ARC solvers or neuro-symbolic baselines."}, "questions": {"value": "1. Compare with baseline models (random, heuristic, small CNN/VAE) under identical inference-time constraints.\n2. Explicitly mention the restricted scalability and the dependency on ARC’s small grid size; discuss whether CompressARC could generalize to larger, more open-ended domains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KVMyRybyKW", "forum": "TbbMyr3E0x", "replyto": "TbbMyr3E0x", "signatures": ["ICLR.cc/2026/Conference/Submission13868/Reviewer_EfCx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13868/Reviewer_EfCx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058218163, "cdate": 1762058218163, "tmdate": 1762924385256, "mdate": 1762924385256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper poses solving ARC-AGI problems as a search for a minimal program that reproduces the example outputs given the example inputs. The program found can then be applied to new inputs and its output be the solution to those new inputs. The benefit of this approach is it does not require one to train on a large dataset beforehand as training is done on a per-instance basis."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The justification for the approach using Occam’s razor is convincing. The ability to solve problems without pre-training is also appealing. Informative case studies are performed on success and failure cases."}, "weaknesses": {"value": "The paper describes their approach in Algorithms 1, 2, and 3. There is a lot that is not clear in these algorithms. \n\n-\tThese algorithms seek to minimize the seed length used. What is the relation between seed length and program complexity?\n\n-\tThere are three seeds mentioned in Algorithm1 (seed_z1, seed_error, and seed_z2). Which seed length is the one that is to be minimized?\n\n-\tAlgorithm 2 says “Measure n_exmpl,n_colors,width,height from P to initialize equivariant_NN”. What is meant by “measure”? How is this to be used to initialize the neural network? What is the initialization procedure?\n\n-\tHow are \\mu and \\Sigma initialized?\n\nThe paper also does not contextualize the results in the broader research landscape. For someome unfamiliar with the ARC-AGI benchmark, one cannot know how these results compare to other existing results, especially for those that do not rely on pre-training. Is this method the only one that does not rely on pretraining?"}, "questions": {"value": "How does this approach compare to other methods that use pretraining and those that do not use pretraining?\n\nSee other questions in Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UKYDwcMpCF", "forum": "TbbMyr3E0x", "replyto": "TbbMyr3E0x", "signatures": ["ICLR.cc/2026/Conference/Submission13868/Reviewer_Q3TN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13868/Reviewer_Q3TN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223964323, "cdate": 1762223964323, "tmdate": 1762924384412, "mdate": 1762924384412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}