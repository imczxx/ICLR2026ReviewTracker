{"id": "hFxivbAgVP", "number": 9390, "cdate": 1758120839297, "mdate": 1759897727736, "content": {"title": "Log Probability Tracking of LLM APIs", "abstract": "When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.", "tldr": "We show that logprobs, when they are available, can be used to audit LLM APIs for changes, beating existing approaches by large margins on cost and sensitivity to small changes.", "keywords": ["API drift", "audit", "monitoring", "LLM API", "black-box"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8fe127e123441f53a8e652694985fc8900345c8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work addresses the problem of detecting changes in the model behind a blackbox LLM API. The authors propose detecting changes by monitoring the logprobs output by the API, which is possible for the ~23% of APIs that return logprobs. The key challenge is that logprobs can differ from call-to-call due to unintentional non-determinism. To address this, the authors propose logprob tracking (LT), which performs a permutation test on the difference between logrpobs. The authors find that this method achieves better detection AUC than baselines that do not use logprobs, while being less-costly to serve."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The method is very simple to implement, when logprobs are available.\n- The result that the detection AUC is not affected by prompt length is somewhat interesting.\n- The authors release a benchmark for evaluating methods like this one.\n- I like the plots showing how logprobs evolve over time."}, "weaknesses": {"value": "- There is limited technical novelty in the methodology. Checking for differences in logprobs is the de facto approach for checking the correctness of language model implementations and APIs (*e.g.* from the VLLM tests https://github.com/vllm-project/vllm/blob/66a168a197ba214a5b70a74fa2e713c9eeb3251a/tests/models/utils.py#L90). It is well known that this is a more sensitive test than simply checking for text equality. That this is more effective than just checking text outputs is not a surprising result.\n- I’m concerned about the significance of the problem and the practical utility of the method given the practices of frontier model providers today:\n    - Most LLM APIs today (77%), do not return log-probs. Frontier model providers like OpenAI and Anthropic do not provide log-probs in responses. Given that the most popular API endpoints due not provide log-probs in response, it is unclear the practical utility of the method for most practitioners.\n    - It’s unclear the degree to which the claim in the first sentence is true: “users of LLM APIs (developers, researchers, regulators) generally rely on the assumption that calling the same API endpoint will consistently serve the same model.” Most frontier LLMs are updated quite regularly (sometimes weekly or monthly), and most users are unaffected by the fact that the model is being updated. What should the user do if the model is updated? Frontier API providers do not provide the ability to use the old API."}, "questions": {"value": "The method is not effective at detecting changes with LoRA fine-tuning. Why do you think this is?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2nxJ19SBH6", "forum": "hFxivbAgVP", "replyto": "hFxivbAgVP", "signatures": ["ICLR.cc/2026/Conference/Submission9390/Reviewer_uCKu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9390/Reviewer_uCKu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433439060, "cdate": 1761433439060, "tmdate": 1762921000558, "mdate": 1762921000558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank all three reviewers for their constructive feedback. Based on the comments, we will revise the paper in the coming weeks by clarifying several points and adding additional experiments.\n\nBelow, we respond to concerns and questions shared between several reviewers. \n\n### Reliance on API providers supporting logprobs\n\nAs pLmm, k7Vu and uCKu noted, LT is contingent on API providers supporting and returning logprobs, which currently represents a minority of providers.\n\nThis is an important point, and we haven't discussed it sufficiently in the paper.\n\nWe agree with this limitation, but we believe that LT has value regardless of the current prevalence of logprob support:\n\n- **motivating greater transparency**: API providers always have the option of returning logprobs, and our results show that doing so significantly improves transparency in the LLM ecosystem. We also found that many providers perform undisclosed changes that alter model outputs, including on open-weight models. We hope this will motivate users to demand logprob support and change disclosure from API providers, and to be suspicious of those that refuse. This is especially important for open-weight models, which have been praised for increasing transparency, yet are often accessed through APIs that remain unaudited.\n\n- **upper bound on other methods' effectiveness**: We show for the first time that under inference non-determinism, changes as small as a single step of fine-tuning can be reliably detected by a black-box method. This makes our approach a practical “upper bound” baseline for techniques that do not rely on logprobs, and our TinyChange benchmark enables researchers to rigorously evaluate the sensitivity of their own methods.\n\n- **possible internal use**: even if providers don't expose logprobs publicly, they might use this method as part of their internal continuous monitoring methods.\n\nWe will add these points in the paper.\n\n### Extending LT beyond binary detection\n\n> **k7Vu**: Could LT be extended to detect which type of change (e.g., quantization vs. fine-tuning) occurred?\n\n> **pLmm**: What mechanisms, beyond binary detection, can be integrated into the LT framework to help auditors diagnose the likely source or severity of the detected change (e.g., distinguishing an infrastructure update from a behavioral fine-tuning step)?\n\nRegarding the *severity* or *magnitude* of the change, the LT test statistic can be a good indicator − the larger the change, the larger the statistic should be. If time permits, we will validate this relationship experimentally.\n\nDetecting the *source* of the change is more challenging. While different modification types might leave distinctive signatures in the resulting logprobs, this is not guaranteed. In fact, we are not aware of any black-box method that can identify the cause of the difference between two LLMs without knowing the LLM and a list of possible changes in advance.\n\nSo our approach is to focus on the *impact* of the change rather than its source: with LT, we aim to provide the first method that can detect changes at scale, allowing to alert API users when a model has changed. Once notified, users − who best understand their own applications − can evaluate how the change affects their particular use case."}}, "id": "7oOjVLTIM8", "forum": "hFxivbAgVP", "replyto": "hFxivbAgVP", "signatures": ["ICLR.cc/2026/Conference/Submission9390/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9390/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9390/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763210275707, "cdate": 1763210275707, "tmdate": 1763210553259, "mdate": 1763210553259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper prsents Logprob Tracking (LT), a technique for LLM drift detection under resource constraints. The key idea is to sample the log probability of the first generated token, and then perform a permutation test. This allows LT to detect minor model updates with 1000x lower cost than existing methods. The authors also propose a benchmark to test small model modification and conduct experiments on controllable model drifts and real-world API drifts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important and underexplored reproducibility problem: LLM APIs are increasingly widely applied while continously updated, and monitoring these behavior shift is an important topic.\n\n- Simple, cost-efficient, and elegant approach: The proposed technique is easy to understand and implement: it basically tests if the distribution of the log probability of the first generated token has changed or not, using a permutation test. This is neat, and also cost-efficient, as it only requires the first token of a given query.\n\n\n- New benchmark (TinyChange) for fine-grained change detection: This benchmark offers a systemtical way to generate model drift with different magnitudes."}, "weaknesses": {"value": "- Depends on APIs exposing logprobs: As the authors notice too, only a small fraction of existing API providers (~23% in openrouter) offers logprob access.\n\n- Detections w/o directions: The proposed method only detects whether a change occurred, not what changed or how the change looked like. In particular, it is unclear if the change leads to better responses to user queries, or what kind of biases or skills were introduced or forgotten in the model update. In practice, this is often more important than just detecting a change.\n\n\n- Limited evaluation and analysis of real-world APIs: Section 3.2.3 briefly mentions drift detection of real-world APIs and the model providers' responses. A more detailed analysis on real-world API drift would improve the paper a lot and make it more relevant to practical senarios."}, "questions": {"value": "- How robust is LT when the logprob sampling temperature or top-k cutoff changes across time?\n\n- Could LT be extended to detect which type of change (e.g., quantization vs. fine-tuning) occurred?\n\n- Have the authors considered multi-token extensions or dynamic prompting to improve evasion resistance?\n\n- Can TinyChange be used to evaluate fingerprinting methods as well, given the conceptual overlap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W4tWGaGpvt", "forum": "hFxivbAgVP", "replyto": "hFxivbAgVP", "signatures": ["ICLR.cc/2026/Conference/Submission9390/Reviewer_k7Vu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9390/Reviewer_k7Vu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894804994, "cdate": 1761894804994, "tmdate": 1762920999792, "mdate": 1762920999792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Logprob Tracking (LT), a cost-effective and highly sensitive method for continuously monitoring LLM APIs for unintended or undisclosed changes.  Users rely on LLM APIs remaining consistent, yet providers frequently implement model modifications that go unmonitored. LT addresses this by exploiting the log probabilities of the first output token, which provide a significantly richer information source than the generated tokens alone. To overcome the non-deterministic nature of log probabilities in production environments, LT employs a simple two-sample permutation test based on the average absolute distance between per-token mean log probabilities. Experiments on the TinyChange benchmark show LT can detect changes as small as a single fine-tuning step, while achieving cost reductions of up to 1,000 times compared to baselines like MET and MMLU-ALG."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "LT drastically reduces the cost of continuous monitoring, achieving sensitivity gains at a cost that is up to three orders of magnitude cheaper than competing state-of-the-art methods\n\nLT provides substantially higher discriminative power and sensitivity than existing approaches. It reliably detects small modifications such as a single step of fine-tuning and demonstrates detection performance for weight pruning at an amplitude 512 times smaller than MET.\n\nThe authors use permutation tests on the mean absolute difference of per-token average log probabilities to handle the inherent non-determinism observed in production APIs.  This is a nice addition that addresses a key limitation in monitoring production systems.  Because the permutation test only requires minimally acquired data from a single output token from a 1-token input prompt, the overall cost of monitoring is drastically reduced."}, "weaknesses": {"value": "The entire methodology is contingent on the API provider supporting and returning log probabilities. Data presented in the paper indicates that only 23% of reachable endpoints on OpenRouter support this.  This limits the applicability of the approach.\n\nLLM providers can obstruct LT by requiring minimum output token lengths \n\n The reliance on log probabilities for only the first output token might miss certain modifications such as adjusting the generation-length parameter."}, "questions": {"value": "What mechanisms, beyond binary detection, can be integrated into the LT framework to help auditors diagnose the likely source or severity of the detected change (e.g., distinguishing an infrastructure update from a behavioral fine-tuning step)?\n\nHow does LT perform against variants created in the TinyChange benchmark where only the EOS token log probability bias is subtly modified, rather than the core model weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NJhnk6MU2w", "forum": "hFxivbAgVP", "replyto": "hFxivbAgVP", "signatures": ["ICLR.cc/2026/Conference/Submission9390/Reviewer_pLmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9390/Reviewer_pLmm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762300504802, "cdate": 1762300504802, "tmdate": 1762920999395, "mdate": 1762920999395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}