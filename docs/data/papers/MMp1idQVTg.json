{"id": "MMp1idQVTg", "number": 8263, "cdate": 1758076773818, "mdate": 1759897795576, "content": {"title": "Signature-Informed Transformer for Asset Allocation", "abstract": "Robust asset allocation is a key challenge in quantitative finance, where deep-learning forecasters often fail due to objective mismatch and error amplification. We introduce the Signature-Informed Transformer (SIT), a novel framework that learns end-to-end allocation policies by directly optimizing a risk-aware financial objective. SIT's core innovations include path signatures for a rich geometric representation of asset dynamics and a signature-augmented attention mechanism embedding financial inductive biases, like lead-lag effects, into the model. Evaluated on daily S\\&P 100 equity data, SIT decisively outperforms traditional and deep-learning baselines, especially when compared to predict-then-optimize models. These results indicate that portfolio-aware objectives and geometry-aware inductive biases are essential for risk-aware capital allocation in machine-learning systems. The code is available at: https://anonymous.4open.science/r/Signature-Informed-Transformer-For-Asset-Allocation-DB88", "tldr": "We introduce the Signature-Informed Transformer (SIT), a framework for robust asset allocation that embeds financial inductive biases using signatures to directly optimize a risk-aware portfolio objective.", "keywords": ["Asset Allocation", "Decision Focused Learning", "Forecasting"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd448b73cf33189f3f61a5e3c0f2d46ca766a66e.pdf", "supplementary_material": "/attachment/90a1bbe16e480aab8cf498f726bc0289ab1dd782.zip"}, "replies": [{"content": {"summary": {"value": "This paper uses rough path signatures to assist in extracting asset features, a signature-augmented attention to help represent lead-lag relationships between assets, and directly optimizes the conditional risk of the portfolio to address the issues of objective function mismatch and error amplification in traditional deep learning models for portfolio construction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This article focuses on the construction of investment portfolios in the financial field, utilizing path signatures to assist in feature extraction and incorporating prior relationships between assets into the self-attention mechanism to improve the transformer model architecture.\n2. Unlike traditional models that optimize prediction accuracy metrics, the proposed model SIT chooses to optimize the portfolio risk metric CVaR."}, "weaknesses": {"value": "1. The truncated level M of signature influences the dimension of input feature, the computation cost and the model performance. The experimental and theoretical sections of the article do not seem to discuss this parameter much.\n2. Regarding the writing, Theorem 3.1 (line 169) does not appear to be a novel result. It seems to be equivalent to equations (34)-(36) in Chevyrev et al. (2016), merely restated here in a calculus form."}, "questions": {"value": "1. The article $\\texttit{Rough Transformers: Lightweight and Continuous Time Series Modelling through Signature Patching[Neurips 2024]}$  also uses signature and transformers, but the author does not seem to cite it and include it as a baseline. Can the author add some discussion about the difference between your paper and this paper?\n2. This article cites SigFormer: $\\texit{Signature Transformers for Deep Hedging[Tong et.al. 2023]}$, which also uses signatures and transformers, but the author does not seem to regard it as a baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xepholfiib", "forum": "MMp1idQVTg", "replyto": "MMp1idQVTg", "signatures": ["ICLR.cc/2026/Conference/Submission8263/Reviewer_ZAVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8263/Reviewer_ZAVi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790102425, "cdate": 1761790102425, "tmdate": 1762920203440, "mdate": 1762920203440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for deep learning-based asset allocation. It addresses two fundamental problems in previous approaches: (1) forecasting models lacked financial inductive biases unique to asset allocation problems; and (2) the conventional two-stage pipeline separated forecasting from portfolio optimization, leading to objective mismatch and error amplification. The authors overcome these issues by introducing path-wise feature representation (Signature) into an attention network and directly aligning the training objective with portfolio performance via Conditional Value at Risk (CVaR) optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of signature-path features and Transformer architecture is highly effective for forecasting in finance, yet their combination has been computationally prohibitive until now; this work is notable for successfully bridging that gap.\n\n2. The use of a CVaR loss objective is a significant innovation. Objective mismatch frequently hinders deep learning applications across domains, so this end-to-end approach is broadly motivating for those adapting deep learning to domain-specific tasks.\n\n3. The paper demonstrates rigorous experimental validation of its novelty, including ablation studies. Notably, the analysis of the concentration parameter (τ) adds a degree of transparency and interpretability to the traditionally black-box nature of deep learning models."}, "weaknesses": {"value": "1. Log-signature is a highly effective tool for compressing time-series data. However, details regarding the parameters for log-signature are missing, and the dimensionality curse due to signature truncation order (M) is insufficiently addressed.\n\n2. The learnable gate (γ) appears to have the potential to enhance interpretability, analogous to the concentration parameter, but its empirical dynamics and impact under varying market conditions are not investigated.\n\n3. While a range of baseline models are used, the experimental setup lacks detail regarding the optimization methods applied after forecasting (e.g., for DLinear, FEDformer, PatchTST), as well as comparative analysis based on those methods."}, "questions": {"value": "1. What is the computational cost and inference time for your proposed SIT model in practical terms?\n\n2. What is the truncation order (M) for the Signature features used, and what is the resulting input dimensionality in your implementation?\n\n3. To what extent does the learnable gate (γ) influence signature-augmented attention, especially under extreme market conditions or during risk events? Any insights from observed behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sXYIzPJqDE", "forum": "MMp1idQVTg", "replyto": "MMp1idQVTg", "signatures": ["ICLR.cc/2026/Conference/Submission8263/Reviewer_3n1u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8263/Reviewer_3n1u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961704600, "cdate": 1761961704600, "tmdate": 1762920203063, "mdate": 1762920203063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the Signature-Informed Transformer (SIT), a deep learning framework for risk-aware asset allocation. It tackles objective mismatch and error amplification in existing \"predict-then-optimize\" methods via three key designs: path signatures for asset dynamics, signature-augmented attention for financial biases (e.g., lead-lag effects), and end-to-end CVaR optimization. Tested on S&P 100 data (2000–2024) across 30–50 asset universes, SIT outperforms traditional (e.g., EWP, GMV) and deep learning (e.g., Autoformer) baselines in Sharpe Ratio, wealth growth, and other metrics, with ablations validating its components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation and well-articulated problem setup, addressing known weaknesses of the predict–then–optimize paradigm.\n2. The integration of path signatures into Transformer attention is technically novel and theoretically grounded.\n3. Experimental results are comprehensive on one dataset, showing strong and consistent performance gains."}, "weaknesses": {"value": "1. Missing comparisons with other end-to-end decision-focused methods. The paper rightly emphasizes the benefits of optimizing CVaR in an end-to-end manner, but current comparisons are limited to forecasting-based approaches. Including or discussing results relative to existing decision-focused or end-to-end optimization frameworks such as Deep Hedging[A]  would clarify whether SIT’s advantage stems from its geometric inductive bias rather than simply from joint training.\n2. More dataset evaluations are recommended. All experiments are conducted on S&P 100 equities. Even a small-scale test on another market (e.g., European equities, ETFs, or futures) would greatly strengthen claims of generality and demonstrate robustness to different regimes and asset structures.\n3. Empirical validation of the signature bias. The signature-informed attention is theoretically motivated, but the paper lacks direct evidence that the bias effectively shapes attention patterns. A simple correlation analysis between attention weights and lead–lag signature strength, would help validate this key design choice.\n4. Evaluation under extreme or risk-sensitive conditions. Since the model’s objective is risk-aware optimization, additional analyses and visualizations during turbulent periods (e.g., the 2008 financial crisis) would better demonstrate SIT’s robustness and performance in high-risk environments.\n5. The signature representation grows rapidly with truncation order and number of assets, but the paper does not quantify this cost. Reporting the truncation level, runtime, and memory footprint would clarify scalability to larger universes.\n6. The construction of the bias term $B$in the attention mechanism remains somewhat abstract. It is unclear whether $B$ is recomputed per head or shared across layers, and how it is normalized; further implementation details would improve reproducibility.\n7. The paper assumes long-only portfolios generated via softmax, but provides little analysis of how the temperature parameter affects weight concentration or whether the approach can handle leverage or shorting constraints.\n8. While average Sharpe ratios are reported, no confidence intervals or p-values are shown. Including bootstrapped or paired-test statistics would establish that improvements are statistically robust.\n\n\n**Reference**\n\n[A] Buehler, Hans, et al. \"Deep hedging.\" Quantitative Finance 19.8 (2019): 1271-1291."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a3rTVEMfLG", "forum": "MMp1idQVTg", "replyto": "MMp1idQVTg", "signatures": ["ICLR.cc/2026/Conference/Submission8263/Reviewer_Ar7Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8263/Reviewer_Ar7Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762172426216, "cdate": 1762172426216, "tmdate": 1762920202748, "mdate": 1762920202748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}