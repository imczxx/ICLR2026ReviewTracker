{"id": "sncc2E4jSy", "number": 11572, "cdate": 1758201911097, "mdate": 1763114173071, "content": {"title": "PROBE: Probing Residual Concept Capacity in Erased Text-to-Video Models", "abstract": "Text-to-video (T2V) diffusion models have achieved remarkable progress in generating temporally coherent, high-quality videos. However, their ability to generate sensitive or undesired concepts has raised concerns, motivating the development of concept erasure techniques that aim to suppress specific semantics while preserving general generation quality. Despite rapid progress in text-to-image (T2I) concept erasure, the effectiveness of these methods in T2V settings remains largely unquantified. In this work, we introduce PROBE, a systematic framework to measure the residual capacity of erased T2V models to represent and regenerate a target concept. PROBE learns a compact token embedding by jointly optimizing across all frames and timesteps, augmented with a latent alignment loss that anchors the recovered representation to the spatiotemporal structure of the original concept. The resulting embedding serves as a reusable probe that enables reproducible, large-scale robustness evaluation across different erasure methods and models. Experiments on multiple T2V architectures demonstrate that PROBE reveals substantial residual concept capacity even after erasure, providing new insights into the limitations of existing techniques and establishing a principled benchmark for future research on safe video generation.", "tldr": "", "keywords": ["Text-to-Video", "Concept Erasure", "Concept Inversion", "Erasure Robustness"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0c9828fcabd2ecb303069504e894d12c039276d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a learning-based framework designed to induce erased text-to-video (T2V) diffusion models to regenerate previously unlearned target concepts. Building upon Textual Inversion, the method learns a special token representing the target concept with the proposed latent alignment loss using a reference dataset.\nExperiments on two T2V models (CogVideoX-2B and CogVideoX-5B) combined with three different T2V concept-erasure methods show that the proposed method can partially restore erased concepts."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed method is simple.\n- The paper is mostly clear and well-written."}, "weaknesses": {"value": "- It is unclear why existing adversarial attack or prompt manipulation methods developed for text-to-image (T2I) models, such as P4D [1], Ring-A-Bell, MMA [3], UnlearnDiff [4], cannot be directly applied to T2V models. These methods also aim to elicit erased or undesired content, and a discussion or comparison with them would strengthen the motivation.\n\n- The process of curating the reference set $\\mathcal{D}_{ref}$ is unclear. Since it is used to learn the special token, it’s important to know the details, e.g., the total duration and the required quality or diversity of the video clips.\n- Both evaluated models belong to the CogVideoX family, limiting the claim that the learned token $v*$ can be reused for any T2V models. Including results from other T2V models, e.g., LTX-Video [5] and Open-Sora [6], would strengthen the evaluation and demonstrate cross-model transferability of the learned token $v*$.\n- Although the paper said that the temporal consistency (frame-level CLIP feature variance) and qualitative human evaluation are also included in evaluation dimensions, these metrics are not reported in the tables. Therefore, it’s unclear whether the proposed method affects visual quality, generation diversity, or temporal coherence of the erased T2V model.\n- The method’s effectiveness appears limited. When applied to the strongest erasure method (T2VUnlearning), it improves accuracy by less than 3% in Table 1 and only 1.28% for CogVideoX-5B in Table 2. Also, results for object-related concepts are reported only for CogVideoX-2B, with no corresponding experiments presented for CogVideoX-5B.\n\n- (minor) More training details should be provided, including the learning rate, gradient clipping threshold, and the value of the loss-balancing coefficient $\\lambda$.\n- (minor) The main figure (Fig. 2) is not well designed. It doesn’t contain sufficient information about the proposed method and occupies a large space. The diffusion model, the reference set, the ground-truth latents, and the predicted latents can be shrunk since they are not the main focus of this paper. Moreover, adding notations in the figure could provide better clarity, e.g., $z_0^{(i)}$, $z_t^{(i)}$, and $\\hat{z}_0^{(i)}(v, t)$.\n\n[1] Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. Zhi-Yi Chin et al. [ICML 2024]  \n[2] Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models? Yu-Lin Tsai et al. [ICLR 2024]  \n[3] MMA-Diffusion: MultiModal Attack on Diffusion Models. Yijun Yang et al. [CVPR 2024]  \n[4] To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images ... For Now. Yimeng Zhang et al. [ECCV 2024]  \n[5] LTX-Video: Realtime Video Latent Diffusion. Yoav HaCohen et al. [arXiv:2501]  \n[6] Open-Sora. https://github.com/hpcaitech/Open-Sora."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7OGu15XAKR", "forum": "sncc2E4jSy", "replyto": "sncc2E4jSy", "signatures": ["ICLR.cc/2026/Conference/Submission11572/Reviewer_eMjV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11572/Reviewer_eMjV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394285920, "cdate": 1761394285920, "tmdate": 1762922658487, "mdate": 1762922658487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "RoeTrdr0tt", "forum": "sncc2E4jSy", "replyto": "sncc2E4jSy", "signatures": ["ICLR.cc/2026/Conference/Submission11572/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11572/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763114171912, "cdate": 1763114171912, "tmdate": 1763114171912, "mdate": 1763114171912, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PROBE, a framework to quantify how much of a target concept remains in a text-to-video diffusion model after an erasure or safety-guarding procedure. PROBE learns a compact pseudo-token by optimizing a shared embedding across frames and timesteps and adds a latent-alignment objective to encourage clip-level semantic faithfulness. The learned token becomes a reusable probe for standardized robustness evaluation across models and erasure methods. Experiments on CogVideoX-2B/5B and three representative erasure strategies show that measurable residual capacity persists post-erasure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Timely problem formulation for video, where temporal dependencies can reintroduce erased concepts; the paper articulates these video-specific challenges well. \n- Clear, modular method with an interpretable probe token that standardizes evaluation across models and erasure methods."}, "weaknesses": {"value": "1. While the paper covers safety-sensitive and object categories, the generality to other safety domains (e.g., violence, celebrities, logos/copyrighted entities) is not demonstrated here.\n2. Cost and scalability: although step counts are given, detailed wall-clock and resource scaling with clip length, resolution, and model size would help assess deployability.\n3. The paper shows heatmaps and aggregate rates, but doesn’t dig into where PROBE fails (which concepts or motion patterns resist reactivation), or when it over-reactivates unrelated semantics. Understanding limits could guide defense design.\n4. Results are mostly under in-distribution prompt families and specific models. How does PROBE transfer across datasets, durations (longer clips), camera motions, or negative contexts (e.g., “a beach with no people”)? The token’s portability is a key claim; more cross-domain tests would help.\n5. Robustness to adversarial or diverse paraphrases remains uncertain. Reporting performance under adversarially optimized prompts (beyond long/refined ones) would be useful.\n6. Related works of concept erasure in T2I models are outdated. Incorporating more recent works [1,2,3] would be more complete.\n\n[1] EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers\n\n[2] Erased or Dormant? Rethinking Concept Erasure Through Reversibility\n\n[3] Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "giFs8A1MaH", "forum": "sncc2E4jSy", "replyto": "sncc2E4jSy", "signatures": ["ICLR.cc/2026/Conference/Submission11572/Reviewer_9i28"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11572/Reviewer_9i28"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573395821, "cdate": 1761573395821, "tmdate": 1762922658095, "mdate": 1762922658095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to regenerate erased concepts in text-to-video models. The authors propose training a shared token embedding across frames by aligning latent representations with the embeddings of the target concept. Experiments on the erasure of nudity and object-related concepts indicate that the proposed method can act as an attack to regenerate erased concepts against concept-erasure techniques."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The latent alignment used in this approach leverages shared latent embeddings across different models, which potentially enables transferability among them.\n2. The proposed method considers the characteristics of videos and integrates them into the optimization objective."}, "weaknesses": {"value": "1. The claimed contributions of this paper appear to be somewhat overstated. The authors argue that PROBE constitutes a principled framework for quantifying residual concept capacity. However, in practice, PROBE primarily functions as an attack mechanism that reconstructs erased concepts rather than a truly principled measurement framework. First, although the latent alignment loss aims to enhance semantic consistency, it cannot ensure that the recovered signal reflects true residual concept capacity, as the optimization mainly amplifies superficial cues from reconstruction and latent correlations. Second, since model parameters, attention maps, and other internal mechanisms remain unaltered, PROBE effectively behaves as a token-level adversarial probe rather than a comprehensive diagnostic tool.\n\n2. The experimental setup is unclear. In Table 1 and 2, is PROBE optimized on one erasure method and then transferred to attack others, or is it separately optimized for each erasure method? And in Table 5, is PROBE optimized on CogX-2B (Object) and then transferred to attack others? This paper also claimed that the resulting embedding is a reusable probe across different erasure methods and models, but it is unclear which experiments or tables demonstrate this reusable performance.\n\n3. PROBE’s performance heavily depends on the data selection for the regenerated concept, yet the authors do not discuss this influence, limiting its applicability."}, "questions": {"value": "1. Clarification on claimed contributions\n2. Experimental setup and transferability\n3. Influence of data selection"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "buDdEiu67R", "forum": "sncc2E4jSy", "replyto": "sncc2E4jSy", "signatures": ["ICLR.cc/2026/Conference/Submission11572/Reviewer_xrN5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11572/Reviewer_xrN5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891317533, "cdate": 1761891317533, "tmdate": 1762922657679, "mdate": 1762922657679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PROBE, a method to quantify residual concept capacity in text, to, video (T2V) diffusion models after concept erasure.\nThe idea is to optimize a pseudo, token embedding that reactivates an erased concept when inserted into prompts. Unlike standard textual inversion, PROBE performs joint spatiotemporal optimization and introduces a latent alignment loss to maintain temporal coherence. The learned embedding is later distilled into a reusable “probe token” to benchmark erasure methods.\n\nExperiments on CogVideoX, 2B and 5B models with NegPrompt, SAFREE, and T2VUnlearning erasure strategies show that erased concepts—both object, level and NSFW—can still be partially recovered. The authors argue that current unlearning techniques suppress rather than remove representations and propose PROBE as a standardized evaluation tool for erasure completeness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Relevance and timeliness: Addresses an urgent open issue — whether “concept erasure” in T2V models truly eliminates internal representations or only suppresses them.\n\nMethodological soundness: The proposed spatiotemporal pseudo, token optimization and latent, alignment loss are sensible and practically valuable extensions of textual inversion to video.\n\nEmpirical breadth: The study covers multiple erasure techniques and model scales, consistently demonstrating measurable recovery of erased concepts.\n\nDiagnostic utility: The concept of a reusable “probe token” provides a simple, reproducible framework for future evaluation of model erasure and unlearning methods.\n\nWriting and figures: The paper is clearly structured, visually polished, and easy to follow."}, "weaknesses": {"value": "1. Limited novelty.\nThe core idea largely builds on known textual inversion techniques with minor adaptations (temporal optimization and latent alignment). These are incremental engineering extensions, not conceptual innovations. The paper lacks theoretical justification for why these adaptations uniquely capture residual capacity in erased models.\n\n2. Weak methodological rigor and incomplete reporting.\nKey hyperparameters (learning rate, λ grid, token length, reference set size, seeds, clip length) are not specified, making the work difficult to reproduce. Statistical measures such as variance, confidence intervals, or hypothesis testing are absent, weakening the empirical support for the main claims.\n\n3. Evaluation metrics and human study insufficiently justified.\nThe use of ResNet, 50 and NudeNet as recovery metrics is limited and prone to false positives/negatives. These detectors are not temporally aware, and the paper does not provide an analysis of their reliability. Human evaluation is briefly mentioned but lacks methodological transparency (number of annotators, criteria, agreement scores).\n\n4. Missing mechanistic insight.\nThe study confirms that residual capacity exists but does not explore where it resides (e.g., cross, attention vs. U, Net layers). The qualitative visualizations shown are descriptive but not explanatory. A deeper interpretability analysis would strengthen the work’s scientific impact.\n\n5. Missing baselines and ablations.\nThe paper targets three erasure methods, but it omits several relevant comparisons: e.g., text, encoder editing methods (some are cited in related work), attention, pruning approaches, or stronger adversarial/paraphrase attacks adapted from T2I. It is unclear whether PROBE is strictly stronger than naïve textual inversion per, frame or per, timestep. Ablations on reference set size, token length choices (A.2 hints at token lengths but lacks systematic study), and prompt augmentation choices are needed to quantify which design choices matter most.\n\n6. Ethical and dual, use concerns.\nThe method demonstrates recovery of NSFW content from erased models but lacks a clear discussion of ethical implications, responsible disclosure, or mitigation strategies. This omission is significant given the safety implications of reactivating restricted content."}, "questions": {"value": "Questions for Authors\n\n1) How sensitive is PROBE’s success to the λ hyperparameter, reference set size, and token length? Please include quantitative ablations if possible.\n\n2) Does PROBE recover semantically similar alternatives (e.g., “nude” → “bare”) or the exact erased concept? How is this verified?\nHow does PROBE compare to per, frame textual inversion or simple CLIP, guided optimization?\n\n3) Can the learned pseudo, token transfer between model scales (e.g., from CogVideoX, 2B → 5B)?\n\n4) Could the authors clarify whether the probe embeddings or code will be released, and if so, under what ethical restrictions?\n\n5) Please elaborate on the human evaluation setup (number of evaluators, sampling process, agreement statistics)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pkq8Fljm72", "forum": "sncc2E4jSy", "replyto": "sncc2E4jSy", "signatures": ["ICLR.cc/2026/Conference/Submission11572/Reviewer_jr5B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11572/Reviewer_jr5B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762447985116, "cdate": 1762447985116, "tmdate": 1762922657170, "mdate": 1762922657170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}