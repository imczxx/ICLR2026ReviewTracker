{"id": "mDuton6Tg7", "number": 17971, "cdate": 1758282525688, "mdate": 1759897141833, "content": {"title": "Towards Consistent Cross-Modal Alignment in Continual Learning for Vision-Language Models", "abstract": "Vision-language models (VLMs) such as CLIP face significant challenges in continual learning (CL), where they must retain both pre-trained and incremental knowledge. Existing methods often rely on reference datasets or domain discriminators, leading to high overhead or limited generalization. Moreover, the semantic gap between modalities hinders effective alignment. While prototypes can partially mitigate this issue, they introduce new challenges: 1) inconsistent prototype fidelity across classes can impede modality fusion and fine-grained alignment, and 2) prototype separability degrades as tasks accumulate in CL. To tackle these, we propose a residual prototype coupled with uncertainty-aware fusion to achieve consistent CLIP alignment. Class-wise prototypes derived from the backbone capture task-specific distributions, supporting both knowledge retention and generalization. Residual prototypes then refine these class representations, mitigating fidelity inconsistency and preserving cross-task separability. In parallel, Bayesian uncertainty-aware estimation and fusion draws on the complementarity between visual prototypes and textual descriptions to dynamically balance multiple objectives, effectively promoting more robust modality fusion and unbiased semantic alignment. Extensive experiments across challenging CL scenarios demonstrate that our method outperforms state-of-the-art approaches, including strong rehearsal-based baselines, across key metrics.", "tldr": "", "keywords": ["Continual learning", "Vision-Language model", "Prototype Learning", "Prompt Tuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bde6e45c9da23361ff7ec4d4c5ec5152f45471a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method of continual learning for CLIP-like multi-modal contrastive models. The proposed method consists of four components: 1) task identification (for seen domains) or ensemble (for unseen domains) weighted by cosine similarity to prototype vectors during inference; 2) adapter-tuning for prototype vectors; 3) cross-attention transformation for image/text/prototype vectors; 4) balanced weighting for continual learning losses. Experiments show that the proposed method achieves superior or comparable performance to the existing methods, even without using rehearsal data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The experimental results suggest that the proposed method works well on cross-domain or multi-domain continual learning scenarios, even without retaining past training data. In some cases the method outperforms the existing methods that retain past training data, but in other cases it performs worse than several previous methods.\n- The ablation study shows how each component in the proposed method contributes to the performance gain."}, "weaknesses": {"value": "- The paper is not well-structured; the explanations for the training and inference phases are mixed up, except for the caption of Figure 2, which prevent readers from precise understanding for the overall procedure.\n- Their method and evaluation only focus on continual learning for the CLIP architecture, which raises a concern about its limitation of applicability to other multi-modal or vision-language models.\n- Theorem 1 claims that inference with the weighted average of adapter parameters yields higher output probability than averaging the output probabilities with the same weighting. However, since model outputs are not calibrated probabilities in general, the practical implication of this claim remains unclear. Moreover, the proof relies on assuming the concavity of model outputs with respect to the adapter's parameter P, which does not hold for actual neural networks. Even if such an assumption was true, the claim is just a direct application of Jensen's inequality.\n- Theorem 2 claims the MAP estimation is approximately achieved by a weighted sum of (Gaussian) random variables $z_k$, leading to the derivation of the weighted distribution in eq (13). However, the resulting distribution seems NOT the corresponding distribution for Theorem 2. Indeed, the distribution of a weighted sum of random variables should be the form of convolution, while the distribution in eq (13) is just a variant of the mixture distribution. Could you clarify the relationship between Theorem 2 and the distribution in eq (13)?"}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5gQNg1Iitj", "forum": "mDuton6Tg7", "replyto": "mDuton6Tg7", "signatures": ["ICLR.cc/2026/Conference/Submission17971/Reviewer_F1Ss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17971/Reviewer_F1Ss"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562432984, "cdate": 1761562432984, "tmdate": 1762927764869, "mdate": 1762927764869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses continual learning for vision-language models (VLMs) pre-trained on large-scale text-image pair datasets like CLIP. Existing baselines prevent catastrophic forgetting by distilling from pre-trained models or utilizing rehearsal that preserves past data, but this incurs significant computational overhead. Recent approaches introduce adapters for additional training or class-specific prototypes to prevent knowledge interference between tasks and efficiently adapt models. The paper specifically argues that prototype-based methods suffer from insufficient prototype fidelity and separability, leading to suboptimal performance. To address this challenge, the paper proposes a rehearsal-free continual learning method that incorporates (i) improved fidelity in residual prototypes and (ii) dynamic fusion of visual/textual semantics via uncertainty estimation. Experiments report that the proposed method achieves competitive performance compared to rehearsal-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **S1.** The paper proposes a novel method for improving prototype-based VLM continuous learning.\n- **S2.** The paper confirms the effectiveness of the proposed method through standard sequential learning benchmarks and ablation studies."}, "weaknesses": {"value": "- **W1.** There is insufficient comparison with the prototype baseline. In particular, comparison with LADA [a], which is directly related to the proposed method, is essential. When compared with the X-TAIL results reported by [a], little difference is observed with the proposed method, which may suggest that the paper is solving a problem that is not particularly significant in continuous learning for VLMs.\n- **W2.** The verification of the research question is insufficient. The paper does not clearly define fidelity and separability, instead evaluating them using task performance as a proxy. To confirm the paper's claims, these definitions must be clarified, and an analysis must be conducted to determine the actual extent of improvement achieved by the proposed method. Furthermore, the paper should clarify whether the estimated uncertainty truly captures the semantic reliability of the modality, or if it merely balances the loss according to task difficulty.\n- **W3.** The differences from existing prototype bases and the positioning of the proposed method have not been discussed accurately. Section 2 merely lists existing research without explicitly discussing how this paper relates to it or where its novelty lies.\n- **W4.** No validation has been performed outside of classification tasks. If the proposed method improves semantic alignment between images and text, its effectiveness should be greater in cross-modal search tasks than in classification using short text.\n\n[a] Luo, Mao-Lin, et al. \"LADA: Scalable Label-Specific CLIP Adapter for Continual Learning.\" ICML2025."}, "questions": {"value": "Please response the concerns raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HEWJHwPd6m", "forum": "mDuton6Tg7", "replyto": "mDuton6Tg7", "signatures": ["ICLR.cc/2026/Conference/Submission17971/Reviewer_sVeE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17971/Reviewer_sVeE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707875251, "cdate": 1761707875251, "tmdate": 1762927764487, "mdate": 1762927764487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel continual learning method for CLIP.\nThe proposed method addresses the problems of conventional continual learning methods: reliability of prototypes and modality alignment\nby dynamically adjusting mean prototypes of visual embeddings and text embeddings via prompt tuning.\nExperimental results demonstrate that the proposed method alleviates catastrophic forgetting and improves accuracy in continual classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1: The experiments are well conducted.\n- S2: Addressing the uncertainty of prototypes is interesting."}, "weaknesses": {"value": "- W1: The definitions of some terms, e.g., seen and unseen, domain and task, are ambiguous, which makes understanding of the proposed method harder. They should be clarified in the problem setting.\n- W2: During inference, according to Sec. 3.2, the proposed method first identifies the input's domain. However, the procedure of domain identification is not described (as well as the definition of domain).\n- W3: The motivation behind the design of the task-shared cross-attention is not clear. What problem does this module address? Ablation on this module would also be helpful.\n- W4: Performance improvements by the proposed method are marginal; most improvements compared to the second baselines are less than 1%pt.\n- W5: Since the proposed method addresses the modality gap, directly evaluating and visualizing it would be beneficial. For example, computing pairwise similarities or visualizing the embedding space, like [a] and [b], would be interesting.\n- W6 (minor): Is Theorem 1 misplaced? It should be introduced after Eq. (5).\n\n\n\n[a] Eslami and de Melo, Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP, ICLR 2025.   \n[b] Liang et al., Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning, NeurIPS 2022."}, "questions": {"value": "Q1: How were the relative contributions in Fig. 4 computed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DDI64MHhTT", "forum": "mDuton6Tg7", "replyto": "mDuton6Tg7", "signatures": ["ICLR.cc/2026/Conference/Submission17971/Reviewer_6Bgd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17971/Reviewer_6Bgd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723983409, "cdate": 1761723983409, "tmdate": 1762927764038, "mdate": 1762927764038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multimodal few-shot learning framework integrating prompt augmentation, residual prototype refinement, and uncertainty-weighted fusion. The authors present Theorem 1, claiming theoretical advantages of weighted aggregation, and demonstrate empirical improvements on benchmark datasets. However, the work primarily combines existing techniques — residual connections from ResNet and uncertainty weighting from multi-task learning — without substantial innovation. Critical theoretical claims lack validation and rest on unverified assumptions (e.g., concavity of output probabilities in deep networks)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a practically relevant problem of multimodal few-shot learning with a well-structured framework that systematically combines multiple components.\n\n2. Comprehensive experimental validation across multiple benchmarks demonstrates consistent performance improvements, with thorough ablation studies showing the contribution of each module.\n\n3. The writing is generally clear and well-organized, with effective use of figures and tables to illustrate the proposed approach and experimental results."}, "weaknesses": {"value": "1. Theorem 1 lacks empirical validation and rests on invalid assumptions. The claim that weighted aggregation outperforms any single prompt assumes concavity of output probabilities with respect to prompt parameters, which typically does not hold in deep neural networks. The paper provides no empirical validation of this assumption. Table 4 only contrasts \"with DA\" versus \"without DA,\" failing to verify Theorem 1's central claim through direct comparison between weighted aggregation and individual prompts. Moreover, when a significantly superior prompt exists, weighted aggregation may degrade performance by incorporating suboptimal prompts, potentially underperforming direct use of the superior prompt alone, and contradicting the theoretical assertion.\n\n2. The residual prototype offers no methodological novelty. This component directly transplants ResNet's residual connection to class prototypes ($Z^{\\text{aug}} = Z + R$), representing a mere engineering application of standard techniques from model architecture to prototype space. The authors neither justify why the residual form outperforms alternative augmentation schemes (e.g., attention-based weighting) nor provide a theoretical analysis explaining how it mitigates prototype interference. While ablation studies show effectiveness, this likely stems from increased model parameters rather than the inherent superiority of the residual design.\n\n3. The uncertainty weighting mechanism lacks originality. Equation 12 directly adopts the homoscedastic uncertainty framework from [1], including the loss formulation $\\mathcal{L} = \\sum [1/\\sigma_k^2 \\cdot \\mathcal{L}_k + \\log \\sigma_k]$ and theoretical derivation, merely transferring this technique from multi-task learning to multimodal fusion without modifications or extensions. While possessing engineering value, this constitutes no methodological contribution. The authors neither discuss the method's suitability for the current scenario nor compare it with alternative uncertainty estimation methods (e.g., evidential deep learning [2] or Monte Carlo dropout [3]).\n\n[1] Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. CVPR 2018.\n[2] Evidential deep learning to quantify classification uncertainty. NeurIPS 2018.\n[3] Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. ICML 2016."}, "questions": {"value": "1. Can the authors provide empirical evidence supporting the concavity assumption in Theorem 1, and conduct experiments directly comparing weighted aggregation against each prompt?\n\n2. What is the theoretical justification for using residual connections in prototype space? Have the authors considered alternative augmentation designs?\n\n3. Why was Kendall et al.'s uncertainty weighting chosen for this multimodal scenario? How does it compare with other uncertainty estimation methods, such as evidential deep learning or Monte Carlo dropout, in this context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8EOz4zetS1", "forum": "mDuton6Tg7", "replyto": "mDuton6Tg7", "signatures": ["ICLR.cc/2026/Conference/Submission17971/Reviewer_zsqA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17971/Reviewer_zsqA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762884266812, "cdate": 1762884266812, "tmdate": 1762927763609, "mdate": 1762927763609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}