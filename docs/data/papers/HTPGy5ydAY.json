{"id": "HTPGy5ydAY", "number": 3168, "cdate": 1757347577170, "mdate": 1759898104572, "content": {"title": "Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability", "abstract": "Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score estimates as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.", "tldr": "Detecting and mitigating memorization in diffusion models through angular alignment of score estimates in low-noise anisotropic regime of denoising process.", "keywords": ["Memorization", "Diffusion Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8b9b2550bcfe7dcac439e97ac6724a11bb5f469.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a memorization detection (and mitigation) method for Stable Diffusion models that prioritizes angular alignment of scores over their norms, especially in the low-noise (late) phase of generation. It is positioned as a generalization of prior work (e.g., Wen 2024; Jeon 2025): earlier metrics largely emphasize the magnitude of a (conditional - unconditional) score difference, whereas this paper argues that direction (cosine similarity) carries the decisive signal under anisotropic noise. Empirically, the paper reports detection gains and mitigation impacts, but several theoretical claims and some experimental details need strengthening."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptually extends prior works (Wen 2024; Jeon 2025) to anisotropic noise settings in diffusion models. \n2. Provides initial empirical evidence suggesting angular features may correlate better with memorization risk."}, "weaknesses": {"value": "1. The theoretical argument for “angular alignment is important” (Remark 1, Sec. 4.1) lacks rigor; Appendix A.1 remains heuristic without formal derivation or proof.\n2. Confuses the norm of the score with the norm of the score difference; prior methods are not clearly distinguished. \n3. The key computation $s(X_T, t\\approx 0, c)$ is conceptually inconsistent, evaluating the final-time score on the initial noise requires justification or on-manifold correction.\n4. Experimental details (Figure 2 setup, time-step sampling, model version, datasets) are missing, making reproduction difficult. \n5. Comparison with Jeon 2025 mitigation results is absent, weakening empirical completeness. \n6. Overall, the method’s novelty is somewhat limited without stronger theoretical grounding or broader validation."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Op2gviMx4u", "forum": "HTPGy5ydAY", "replyto": "HTPGy5ydAY", "signatures": ["ICLR.cc/2026/Conference/Submission3168/Reviewer_sxxs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3168/Reviewer_sxxs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741197864, "cdate": 1761741197864, "tmdate": 1762916581622, "mdate": 1762916581622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies memorization in diffusion models through the lens of anisotropy in the log-probability landscape. It argues that prior norm-based metrics are only effective under isotropy and introduces a denoising-free detection metric that combines (i) cosine alignment between conditional and unconditional scores in the anisotropic regime and (ii) score-norm magnitude in the isotropic regime. The method requires only two forward passes—conditional and unconditional—and is shown to outperform existing denoising-free metrics on Stable Diffusion v1.4 and v2.0 while being significantly faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe identification of anisotropy as a key factor in memorization detection is conceptually strong and empirically supported (variance of Hessian eigenvalues increasing toward low noise).\n\n•\tThe method is denoising-free, computationally efficient, and integrates both magnitude and directional cues.\n\n•\tExperiments follow prior evaluation standards and report competitive or superior results. Using new designated bencharks such as MemBench is also a bonus."}, "weaknesses": {"value": "•\tThe section titled “Failure of Norm-Based Methods in Anisotropy”\t starts with “We now prove…” However it does not constitute a formal proof. Rephrase to “demonstrate” or “show analytically” or similar.\n\n\n\n•\tIn Table 1, please report standard deviations or confidence intervals. With 500 prompts, small improvements (such as the +0.001 AUC over Jeon et al. at n = 4, SD v1.4) may fall within statistical noise.\n\n\n\n•\tFig. 3 (a), (b) should be key visualizations supporting the main hypothesis - however they are insufficiently explained.\n\n\n\n•\tQualitative examples of generations with mitigated memorization are absent. Please provide such a comparison with the competitors. In addition, it is preferable to add an exhaustive set of such generations to the appendix.\n\n\n\n•\tPlease refer to the concurrent [1], which employs a different criterion that similarly emphasizes direction over magnitude. The numerical computation of its curvature criterion sums angles (cosines) between the conditioning score and surface normals. Clarifying how this differs from your approach, and whether it fits within your anisotropic framework, would aid contextualization for future reference.\n\n\n\n•\tPlease acknowledge [2] in the related work. The claim that “this phenomenon was firstly identified by Somepalli et al. (2023a)” overlooks [2], which concurrently with Somepalli et. al, demonstrated training data extraction (memorized samples) from diffusion models. \n\n\n\n•\tThe inference-time mitigation description lacks clarity - reducing reproducibility. Please specify how γ₁ and γ₂ were selected and how the score at ($t \\approx 0$) is obtained without full denoising (a clarification in the Appendix would suffice).\n\n\n\n-----\n\n\n\n[1] Brokman et al. (2025). Tracking Memorization Geometry throughout the Diffusion Model Generative Process. NeurIPS 2025 Workshop on Symmetry and Geometry in Neural Representations.\n\n\n\nLink: https://openreview.net/forum?id=4XSVk26sHj\n\n\n\n[2] Carlini et al. (2023). Extracting Training Data from Diffusion Models. USENIX Security Symposium.\n\n\n\nLink: https://www.usenix.org/conference/usenixsecurity23/presentation/carlini"}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "76UHL22t1J", "forum": "HTPGy5ydAY", "replyto": "HTPGy5ydAY", "signatures": ["ICLR.cc/2026/Conference/Submission3168/Reviewer_Kb53"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3168/Reviewer_Kb53"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857758697, "cdate": 1761857758697, "tmdate": 1762916581337, "mdate": 1762916581337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates memorization in text-to-image diffusion models and argues that existing norm-based detection methods only work under isotropic log-probability assumptions, which is the case for high or medium noise levels. It formalizes how anisotropy arises in the low-noise regime and proposes a new denoising-free metric that is defined as the cosine similarity between conditional and unconditional score estimates. This new metric is combined with the known “isotropic term” - norm of the guidance vector. The method requires only two denoising steps (only noise estimation, not real denoising). The method improves AUC and TPR (at 1% FPR) on Stable Diffusion v1.4/v2.0. It also integrates the metric into an inference-time prompt-optimization scheme for mitigation, evaluated on MemBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tSound theoretical framing. The analysis clearly connects memorization signatures to the isotropy and anisotropy regimes of the log-probability, filling a conceptual gap in prior isotropic norm-based methods.\n2.\tEfficient and simple metric. The proposed anisotropy-aware score is computationally cheap (two model steps, no actual denoising) yet achieves higher detection accuracy.\n3.\tThe method is not limited to detection, it also demonstrates practical mitigation by optimizing prompt embeddings, producing non-memorized images while maintaining text alignment and aesthetic quality."}, "weaknesses": {"value": "1. The reported “speed-up” is only shown relative to one denoising-free baseline, while simpler existing methods remain faster with no meaningful loss in performance, limiting the practical efficiency benefit of the proposed approach.\n2. Incremental improvement weakness: AUC/TPR gains are marginal because prior methods already near saturation, making the contribution incremental rather than substantively advancing the state of the art, even if the method itself is technically sound.\n3. No code to allow future comparison, and no indication that it will be provided in the future (Wen et al., which the authors compare to, do provide code). I urge the authors to add code so that their work will be maximally helpful to others. \n4. The imporovements are small. For mitigation the improvement looks more consistent, and for detection, Wen et al. are (~3x) faster and slighty worse, while Jeon et al. are on par and much slower (~6x). \n5. Minor - Equation 8 typo: the unconditional score in the cosine similarity term should not have “c” in it."}, "questions": {"value": "1.\tGiven that prior methods already achieve near-saturated AUC/TPR, how do you justify the significance of the contribution beyond small numerical gains (with x2-3 in latency)?\n2.\tSince your claimed speed-up is only relative to a single denoising-free baseline, and simpler non-denoising-free methods remain faster with comparable performance (Wen et al.), can you clarify in what scenarios your method is actually the preferred choice in practice?\n3.\tRegarding the t=0 probe using high-noise latents - the model is inputted a high-noise latent but set with t=0, meaning the model “believes” it is operating at the end of the denoising process, which the authors showed is the low-noise regime. However the actual noise latent is far from the data manifold. Can you explain why a cosine similarity computed under this deliberate mismatch, based on what the model thinks rather than the actual noise level, should remain reliable and theoretically meaningful for detecting memorization (as empirically demonstrated)?\n4.\tAblation - table 3 (in the Appendix) shows that gamma_1 differs is 2 and 0.1in  SD v1.4 and SD v2.0, suggesting that the anisotropy term is unstable and requires model-specific tuning.  Can you explain why such tuning is necessary, and can you provide an ablation where we can understand the individual contribution and necessity of each component (gamma1 =0  and gamma2=0 in separate settings)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qHeeRIET79", "forum": "HTPGy5ydAY", "replyto": "HTPGy5ydAY", "signatures": ["ICLR.cc/2026/Conference/Submission3168/Reviewer_y8dV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3168/Reviewer_y8dV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920183623, "cdate": 1761920183623, "tmdate": 1762916581111, "mdate": 1762916581111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses memorization in text-to-image diffusion models, a phenomenon where models reproduce training samples. The authors argue that existing norm-based memorization detection methods, which rely on the magnitude of score estimates, implicitly assume isotropic log-probability distributions, an assumption that holds mainly in high- or mid-noise regimes. The paper identifies that in low-noise (anisotropic) regimes, memorization manifests instead through strong angular alignment between conditional and unconditional score functions. Based on this insight, the authors propose a new denoising-free memorization detection metric that combines (1) the cosine similarity between guidance and unconditional score vectors in the anisotropic regime and (2) the score-norm difference in the isotropic regime. This metric requires only two forward passes (conditional and unconditional), making it substantially faster than prior methods. Experiments on Stable Diffusion v1.4 and v2.0 show improved AUC and TPR@1%FPR compared to previous denoising-free baselines, with up to 5× speedup. The method is also used for inference-time mitigation via prompt augmentation, demonstrating improved trade-offs between similarity, CLIP, and aesthetic scores on MemBench."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized, making it easy to follow. The visualizations clearly illustrate the messages and insights it wishes to deliver.\n2. This paper contributes to privacy-preserving text-to-image diffusion models, which are practically significant for preventing copyright risks.\n3. The proposed metric’s efficiency (denoising-free and fast) makes it appealing for large-scale model auditing. \n4. The paper introduces a new theoretical and empirical perspective on diffusion model memorization by analyzing the anisotropy of log-probability. The conceptual shift from norm magnitude to angular alignment in the low-noise regime is novel and well-motivated. The derived connection between isotropy, anisotropy, and curvature sharpness is mathematically supported and empirically verified.\n5. Experimental results are comprehensive, covering multiple Stable Diffusion versions, standard benchmarks, and ablations. Comparisons include strong baselines and standard metrics."}, "weaknesses": {"value": "1. The paper could benefit from additional discussion on whether the proposed metric generalizes across architectures beyond SD v1.4/v2.0.\n2. Although Appendix A.4 shows some robustness, the weights are empirically tuned per model, suggesting potential calibration issues when scaling to new settings.\n3. The proposed cosine-similarity measure is intuitive but may be sensitive to normalization choices. A sensitivity analysis on score normalization or noise schedule parameters would strengthen the robustness claims.\n4. Despite being well-organized, there are several equations in the paper that remain unlabelled on page 5 (between equations 5 and 6).\n5. The discussion on Figure 2’s overlap could benefit from quantitative metrics (e.g., KL divergence between distributions)."}, "questions": {"value": "See the above strengths and weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NfMrRJZrz8", "forum": "HTPGy5ydAY", "replyto": "HTPGy5ydAY", "signatures": ["ICLR.cc/2026/Conference/Submission3168/Reviewer_WjxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3168/Reviewer_WjxY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987138065, "cdate": 1761987138065, "tmdate": 1762916580980, "mdate": 1762916580980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}