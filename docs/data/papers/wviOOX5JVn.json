{"id": "wviOOX5JVn", "number": 21131, "cdate": 1758314098027, "mdate": 1759896940613, "content": {"title": "Causally Robust Preference Learning with Reasons", "abstract": "Preference‑based reward learning is widely used for shaping agent behavior to match a user's preference, yet its sparse binary feedback makes it especially vulnerable to causal confusion. The learned reward often latches onto spurious features that merely co‑occur with preferred trajectories during training, collapsing when those correlations disappear or reverse at test time. We introduce ReCouPLe, a lightweight framework that uses natural language rationales to provide the missing causal signal. Each rationale is treated as a guiding projection axis in embedding space, training the model to score trajectories based on features aligned with that axis while de-emphasizing context that is unrelated to the stated reason. Because identical rationales can arise across multiple tasks (e.g., \"_it avoids collisions with a fragile object_\"), ReCouPLe naturally reuses the same causal direction whenever tasks share semantics, and transfers preference knowledge to novel tasks without extra data or language‑model fine‑tuning. \nOur learned reward model can ground preferences on the articulated reason, aligning better with user intent and generalizing beyond spurious features.", "tldr": "We develop a framework that utilizes natural language rationales to prevent causal confusion in preference learning.", "keywords": ["Preference-based learning", "causal confusion", "learning from human feedback"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88a8d25be81760a40f80c3ff530ba6f899700d18.pdf", "supplementary_material": "/attachment/4a851b9d6b0082026ae54315eb53e55368d2fa9e.zip"}, "replies": [{"content": {"summary": {"value": "This paper considers the problem of reward learning in PbRL, where the data can be confounded. The authors propose ReCouPLe, which use natural language task description/explanations to resolve such spurious correlations. Specifically, the rewards are decomposed into two components (i.e., a component parallel to the reason and an orthogonal component), and only the parallel component is used to capture preferences via the BT model. The orthogonal component (non-causal feature) is then penalized to be consistent between trajectories. Experiments show that the proposed approach outperforms standard baselines that do not utilize the auxiliary information."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and easy to follow. The environments and datasets are carefully designed to capture confounding problems in PbRL and the experiments clearly demonstrate the effectiveness of the proposed approach. Although it is not immediately clear how frequently causal confusions occur in real-world settings, I believe the method has potential applications beyond the specific cases studied here. In particular, it could be useful for more general problems where explanations are available during reward modeling (e.g., Table 4 shows that in-distribution tasks can also benefit from the proposed method), making it of interest to the broader community."}, "weaknesses": {"value": "My main concern is that it is well-known that language models can be sensitive to phrasing, and this is not really explored in the paper (e.g., using different LMs, paraphrasing).\nIt is also not clear what kind of LMs should be used to best extract the task/reason embeddings.\nFor Fig 3, I think it would be nice to have in-distribution performances also reported; otherwise, it is difficult to tell if the performances stagnate because of problems from RL training or reward modeling.\nAside from the ablations presented in Sec 5.3, I would suggest the authors also ablate on the LM side to demonstrate the robustness of the method."}, "questions": {"value": "1. Line 393: This sampling distribution may lead to completely incorrect reasons given trajectory pairs. Can the authors elaborate on this design choice?\n2. Line 409: What does it mean to decompose and recompose semantically? Can the authors provide additional evidence for this behavior?\n3. How sensitive is the method regarding task descriptions / reason codes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eXjZdWm8tV", "forum": "wviOOX5JVn", "replyto": "wviOOX5JVn", "signatures": ["ICLR.cc/2026/Conference/Submission21131/Reviewer_NmL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21131/Reviewer_NmL7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500450247, "cdate": 1761500450247, "tmdate": 1762941404429, "mdate": 1762941404429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method to incorporate preference rationales into the learning of the trajectory representation used by the reward model to assign rewards per state-action pair. The method relies on the presence of a language-based task description that is embedded and then compared to each state-action pair embedding from the given trajectory. The trajectory learning approach combines the current representation of each state-action pair in a trajectory with an embedding of the provided preference rationale, and relies on a representation of the trajectory that excludes those features most relevant to the preference rationale. The training objective then combines three objectives that aim to: (1) force the state-action pair embedding to focus on rationale-relevant features, (2) ignore features not relevant to the rationale, and (3) prevent a collapse of the trajectory representation. The experiments are run on two different domains, ManiSkill and Metaworld, each of which have different levels of difficulty and rationale complexity. The results suggest the proposed method, ReCouPLE, out performs baselines on reward modelling and learned-policy quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper applies a method that has been used in goal-conditioned RL to the preference learning setting to boost the limited amount of information available in a preference label.\n- Addressing the limited amount of information available in a preference signal is a key problem to solve improve general field ability to learn effective and robust reward models.\n- The paper is easy to read and follow."}, "weaknesses": {"value": "- Some experiments are missing to truly understand where ReCouPLE performs well:\n     - impact of number of preference samples on reward and learned-policy quality\n     - impact of noisy preference labels\n     - impact of noisy rationales\n     - combining data with and without rationales as, in practice, collecting large datasets with rationales will be expensive and impractical\n- The ManiSkill experiments lack diversity in rationales, so it is not clear how well the results will generalize.\n- While one of ReCouPLe-EC or -IC typically are a best performer, frequently one or more baseline is better than at least one of the two versions, and this varies by task for ManiSkill. This suggests the method is fragile and task-specific. There is no discussion of what might lead to this performance sensitivity. Given the sensitivity of ReCouPLEe performance to the task in ManiSkill, there should be more test tasks for MetaWorld. \n- There should be MetaWorld test tasks less closely related to the training tasks to understand how well the method works in more challenging and realistic multi-task settings.\n- It is not possible to assess how impact or meaningful the ReCouPLe performance gains are. When presenting the test results for policy training, results should be provided when training on the true reward. For example, if the training on the true reward provides a success rate of 0.8, a success rate difference of 0.07 is less meaningful.\n- For the ablations and analysis results in Section 5.3 (ablations and image-based control tasks), results should be provided for policy learning, not only reward model accuracy, especially as the reward model accuracy score differences are large. While there may be a difference in reward model accuracy, it is important to understand how much of an impact that difference has on learned-policy performance."}, "questions": {"value": "- How much data is used in the ManiSkill experiments?\n- What is the strength of correlation between the reward model's performance and policy performance? This important to answer as there is evidence on the LLM side of reward learning and PbRL that reward model preference-labelling performance is not always strongly predictive of how well the reward can be used to train a policy. This makes reward model selection challenging.\n- Why is the color swap reward model accuracy higher in Table 1 for 4-task + pull + RFP?\n- How do the baseline methods compare ReCouPLe in terms of number of learnable parameters? This would speak to the generally upper bound on method expressivity, which impacts general overall performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dYtYxNlosS", "forum": "wviOOX5JVn", "replyto": "wviOOX5JVn", "signatures": ["ICLR.cc/2026/Conference/Submission21131/Reviewer_fFvS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21131/Reviewer_fFvS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605181302, "cdate": 1761605181302, "tmdate": 1762941403799, "mdate": 1762941403799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This proposes a method to reduce causal confusion in preference-based reinforcement learning, where reward models often overfit to spurious correlations. ReCouPLe augments standard binary preference data with short natural-language rationales, treating each rationale as a causal projection axis that aligns trajectory embeddings with the stated reasons. This decomposition isolates causal features that truly drive preferences while ignoring irrelevant correlations. Experiments on ManiSkill and Meta-World show that ReCouPLe significantly improves out-of-distribution robustness and enables zero-shot reward transfer to new tasks without additional annotations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Employs language-based causal alignment by combining simple preference data with natural-language rationales, guiding the model to focus on causally relevant features that reflect the user’s true intent.\n- Demonstrates zero-shot reward transfer to unseen environments without requiring any additional preference collection or reward model training."}, "weaknesses": {"value": "- The task instructions used are overly simple, and it is unclear whether incorporating language reasoning truly provides an advantage in this setup. It would strengthen the work to include experiments using more diverse language rationales or to analyze whether varying the linguistic expressions for the same rationale improves performance.\n- The rationale extraction process appears heuristic and heavily dependent on ground-truth rewards (especially in MetaWorld). This reliance limits the method’s applicability to benchmarks where reward engineering is difficult or unavailable.\n- Because each preference annotation requires a corresponding rationale, the method is not easily scalable to online RL settings. It would be valuable to discuss how ReCouPLe could be modified or extended for online applications.\n- Prior work [1] has shown that image-based control tends to exacerbate causal confusion—particularly in MetaWorld—but the paper does not provide results for visual reward learning and policy training on MetaWorld to validate this.\n\n**References**\\\n[1] Subtask-Aware Visual Reward Learning from Segmented Demonstrations, ICLR 2025."}, "questions": {"value": "- It seems that the RFP baseline is just simple extension of Multi-BT operations twice, once for each of two distinct text instructions. Given this, what could explain RFP’s superior performance compared to Multi-BT, despite their apparent similarity?\n- Could the authors clarify the specific domains or conditions under which ReCouPLe-EC and ReCouPLe-IC are most advantageous? At present, the explanations appear somewhat post-hoc, matched to results rather than grounded in theory.\n- Can the method scale to setups with more than three distractors? For instance, beyond object size and color, could it handle additional factors such as object shape while maintaining consistent reasoning and performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zpqh5JV1p3", "forum": "wviOOX5JVn", "replyto": "wviOOX5JVn", "signatures": ["ICLR.cc/2026/Conference/Submission21131/Reviewer_mHRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21131/Reviewer_mHRs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980664099, "cdate": 1761980664099, "tmdate": 1762941402834, "mdate": 1762941402834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}