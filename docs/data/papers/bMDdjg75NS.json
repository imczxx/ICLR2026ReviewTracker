{"id": "bMDdjg75NS", "number": 5884, "cdate": 1757943372189, "mdate": 1759897947395, "content": {"title": "TARG: Training-Free Adaptive Retrieval Gating for Efficient RAG", "abstract": "Retrieval-Augmented Generation (RAG) improves factuality but retrieving for every query often hurts quality while inflating tokens and latency. We propose Training-free Adaptive Retrieval Gating (TARG), a single-shot policy that decides when to retrieve using only a short, no-context draft from the base model. From the draft’s prefix logits, TARG computes lightweight uncertainty scores—mean token entropy, a margin signal derived from the top-1/top-2 logit gap via a monotone link, or small-$N$ variance across a handful of stochastic prefixes—and triggers retrieval only when the score exceeds a threshold. The gate is model-agnostic, adds only tens to hundreds of draft tokens, and requires no additional training or auxiliary heads. On NQ-Open, TriviaQA, and PopQA, TARG consistently shifts the accuracy–efficiency frontier: relative to Always-RAG it matches or improves EM/F1 while reducing retrieval by 70–90\\% and cutting end-to-end latency, and it remains close to Never-RAG in overhead. A central empirical finding is that under modern instruction-tuned LLMs the margin signal is a robust default (entropy compresses as backbones sharpen), with small-$N$ variance offering a conservative, budget-first alternative. We provide ablations over gate type and prefix length and use a $\\Delta$-latency view to make budget trade-offs explicit.", "tldr": "", "keywords": ["RAG", "LLM", "Traning-Free Gates"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0889303a7c8d077baf5ba06ba3c957167d5dc2e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TARG (Training-free Adaptive Retrieval Gating), a single-shot policy designed to enhance the efficiency and reliability of RAG systems by moving beyond the costly and error-prone Always-RAG approach. This approach enables the LLM to conduct a cost-effective self-assessment before retrieval by generating a brief, context-free prefix (around 20 tokens). It then calculates a lightweight Uncertainty Score based on the prefix's raw logits, such as the Margin (the difference between the top-1 and top-2 logits), and only initiates retrieval if the score exceeds a set threshold. The proposed method consistently enhances the accuracy-efficiency frontier compared to Always-RAG, achieving similar or better EM/F1 scores while greatly reducing the retrieval rate and significantly decreasing end-to-end latency, demonstrating its utility as a practical and cost-effective solution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses the three most urgent challenges for RAG deployment: high cost, increased latency, and accuracy decline caused by noisy context from unconditional retrieval. By solving when to retrieve, the paper offers a foundational solution for making RAG economically feasible and reliable in real-world applications.\n\n2. The authors introduce a novel training-free method for conditional retrieval. It avoids the complexity and cost of training extra models or control heads by relying only on inherent uncertainty signals (Margin, Variance) from the base LLM's raw prefix logits.\n\n3. The method addresses RAG's main cost issue by significantly lowering the retrieval rate compared to Always-RAG. This results in substantial token savings and nearly matches the minimal latency of the Never-RAG baseline."}, "weaknesses": {"value": "1. Questions on Experimental Rigor and Integrity\n> The paper's validation requires greater statistical rigor and transparency regarding the experimental baselines. First, the reporting uses single-point estimates (EM/F1) without confidence intervals or standard deviations. The notable performance shifts observed at minimal retrieval rates (e.g., 0.001 Retrieval Rate) necessitate a comprehensive statistical significance test (such as a t-test) across multiple independent runs to confirm the reliability and stability of these minimal-budget operating points. Second, the experimental design raises methodological questions regarding the Always-RAG baseline, which consistently and significantly underperforms Never-RAG across all datasets. This result suggests that the retrieval context is largely noisy or distracting in the current setup, meaning the reported gains primarily demonstrate TARG's ability to filter suboptimal retrieval rather than its capacity to maximize the benefit from high-quality external evidence.\n\n2. Limited Scope of Comparative Baselines\n> The comparison is restricted to only the unconditional baselines (Always-RAG and Never-RAG). However, since the main advantage of TARG is its ability to reduce latency through context filtering, the comparison remains incomplete. The authors should evaluate TARG's accuracy and efficiency against established methods for context compression and summarization. Comparing TARG's gating approach with these alternative methods for reducing context length and latency is essential to demonstrate its full competitive advantage.\n\n3. Non-Trivial Calibration Cost Challenges the \"Training-Free\" Claim\n> The simplicity of TARG is fundamentally challenged by the high real-world overhead of threshold calibration. Since performance and latency are highly sensitive to the decision threshold, finding the optimal threshold requires a development-set sweep for every new domain or model. We question whether this non-trivial, domain-specific optimization process is, in practice, as demanding to maintain as the auxiliary training that TARG is designed to replace."}, "questions": {"value": "1. Statistical Reliability: Given the use of single-point estimates (EM/F1) and the dramatic shifts at minimal retrieval rates (e.g., 0.001), can the authors confirm the stability of these results by reporting the statistical significance (e.g., t-test) across multiple independent runs?\n\n2. Baseline Quality: Since Always-RAG significantly underperforms Never-RAG across all datasets, the retrieval context appears to be consistently noisy. Can the authors discuss whether the reported gains primarily demonstrate filtering suboptimal retrieval rather than maximizing the benefit from high-quality external evidence?\n\n3. Missing Comparative Baselines: Since TARG's main advantage is latency reduction via context filtering, can the authors compare its accuracy-efficiency frontier against established context compression or summarization methodologies to demonstrate TARG's full competitive advantage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "agDFIGLNfm", "forum": "bMDdjg75NS", "replyto": "bMDdjg75NS", "signatures": ["ICLR.cc/2026/Conference/Submission5884/Reviewer_hzTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5884/Reviewer_hzTK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534750351, "cdate": 1761534750351, "tmdate": 1762918324174, "mdate": 1762918324174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Predictions of the retriever could be noisy and in-turn lead to degradation in the performance, as well as efficiency due to increased context length. This paper focuses on when to trigger the retriever in the RAG setup. Retriever adds additional context to disambiguate the query, it could be that the parameters of LLM already has enough information that the query can be answered correctly without the need for context enrichment. The internal knowledge of LLM can measured using heuristic-metric based on the output or the internal state of the LLM. To efficiently use the retriever, it is triggered only when the internal knowledge is not sufficient. This paper comes up with three such metrics that measure uncertainty in the LLM generations (measure of internal knowledge) -- 1) mean token entropy, 2) margin score from top-1 versus top-2 logit gap and 3) a small-N variance from handfull of stochastic prefixes. Among these metrics top-1 vs top-2 margin is found to be more robust. On NQ-open, Trivia-QA and PopQA, their approach, TRAG, shows improvements in both accuracy and efficiency over systems that always triggers retriever and that does not use one at all."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes three signal to measure the uncertainity in the LLM generations -- 1) entropy, 2) margin score and 3) small-N variance.\n2. Gating decision is just based on a single scalar threhold, and for long generations an optional single re-checking is done after every m token generations if retreiver is not yet used.\n3. Proposes method to calibrate the threshold based on retrieval budget and to maximize accuracy."}, "weaknesses": {"value": "1. Paper shows results on only three simple QA datasets containing short answers.\n2. $u_t$ described in lines 153-154 is not used anywhere else.\n3. Existence of $\\tau_*$ (lines 223-225): There is no relation between the quality of retrieval and the proposed uncertainty measure so you cannot guarantee the existence of such a threshold. Take for example two cases; a) the retriever always gives the correct answer to the question as context, here Always-RAG should do better than TRAG and b) always give the same random text as context, then here Zero-RAG is better than TRAG. So the in-equality (lines 223-225) does not hold.\n4. Paper only considers two LLMs: Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct.\n5. Paper contains lots of repeated text like the discussion section, pointed mentioned in the section is already coverd before."}, "questions": {"value": "1. The method assumes that the three signals proposed are good proxy for parametric knowledge of the LLM. Could you show an analysis of this correlation? That is when the signal is low base generator generates correct answers and when signal is high the answer is wrong.\n2. Paper presents results on simple QA that requires short generations: NQ, TriviaQA and PopQA. Please show numbers on other datasets: a) Complex QA requiring mult-hop reasoning to answer the question -- 2WikiMultiHopQA, HotpotQA, b) requiring long form generations --Biography, ALCE-ASQA c) PubHealth covering true-false questions d) Arc-Challenge consisting multiple choice questions.\n3. There are other signals proposed in the papers mentioned in the related works, which are based on the output or the internal state of the LLM. How does your approach fare against them? Like a) \"semantic entropy\" in SUGAR, b) \"Self-aware Uncertainty Estimator\" in SEAKR which uses determinant of Gram matrix of hidden representation to measure uncertainity in generations. Create a comparison table something like Table 1 and 2 in SUGAR comparing different adaptive RAG methods.\n4. Show performance on more backbone LLMs, Gemma, Phi, Mistral series in both the 1-3 billion and 6-9 billion parameter range.\n5. Perform a more comprehensive hyperparameter sweeps, for both k and the threhold. Paper considers only 3 different values for k = {10, 20, 30}. Pick more values for both k and threshold, and create plots to show how performance varies with k and threshold -- separately using line plots and combined using heat-maps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BPTT2ip1HW", "forum": "bMDdjg75NS", "replyto": "bMDdjg75NS", "signatures": ["ICLR.cc/2026/Conference/Submission5884/Reviewer_oF17"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5884/Reviewer_oF17"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753725771, "cdate": 1761753725771, "tmdate": 1762918322869, "mdate": 1762918322869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inefficiencies inherent in standard Retrieval-Augmented Generation (RAG) pipelines. While RAG improves factuality, the common practice of retrieving for every query (\"Always-RAG\") significantly increases latency and token consumption. Furthermore, it can degrade performance if the retrieved context is noisy or irrelevant.\n\nThe authors propose TARG (Training-free Adaptive Retrieval Gating), a lightweight, model-agnostic, single-shot policy to decide when to retrieve. TARG operates by first generating a short, no-context draft (prefix) using the base LLM. It then computes an uncertainty score based on the logits of this prefix. If the uncertainty exceeds a calibrated threshold $\\tau$, retrieval is triggered; otherwise, the model proceeds using only its parametric memory.\n\nThe paper investigates three training-free uncertainty signals:\n- Entropy: Mean token entropy of the prefix.\n- Margin: Derived from the gap between the top-1 and top-2 logits (a smaller gap indicates higher uncertainty).\n- Variance: Measured by disagreement across a small number (N=3) of stochastic prefixes.\n\nA key empirical finding is the interaction between uncertainty signals and the \"sharpness\" of the underlying LLM. As modern instruction-tuned models become more peaked (e.g., Llama-3.1-8B), prefix entropy compresses and loses discriminative power. In contrast, the Margin and Variance signals retain their dynamic range and correlate better with the necessity of retrieval.\n\nEvaluated on NQ-Open, TriviaQA, and PopQA, TARG consistently shifts the accuracy-efficiency frontier. It often matches or exceeds the accuracy of Always-RAG while reducing retrieval frequency by 70-90%, keeping latency close to the Never-RAG baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simplicity: the \"plug-and-play\" nature allows for easy integration into existing RAG systems with minimal overhead (limited to the generation of a short k-token prefix).\n- Analysis is insightful: The analysis regarding the behavior of different uncertainty metrics under modern, sharp instruction-tuned LLMs (Section 6) is a valuable contribution. The observation that entropy compresses as backbones improve, while the top-1/top-2 logit gap (Margin) and disagreement (Variance) retain dynamic range, provides actionable guidance for implementing uncertainty estimation.\n- Strong empirical results: The results convincingly demonstrate that TARG improves the accuracy-efficiency trade-off. It significantly reduces retrieval rates while often improving accuracy over the Always-RAG baseline. The authors' use of \"Δ latency\" (incremental overhead vs. Never-RAG) provides a clear and practical framing of the computational cost."}, "weaknesses": {"value": "- 'Usefulness calibration' assumption may be strong: the theoretical underpinning of TARG (Section 3.3) relies on the assumption that the uncertainty score $U(q)$ correlates strongly with the expected benefit of retrieval ($\\Delta(q)$). This assumption may not always hold. Scenarios where the model is confidently wrong (low U, high potential $\\Delta$) or uncertain but the retriever consistently fails (high U, negative $\\Delta$) could violate this assumption. The paper would benefit from a deeper error analysis focused on these quadrants.\n- Re-check is not evaluated: while Algorithm 1 describes an optional re-check every $m$ tokens, its effectiveness is not evaluated."}, "questions": {"value": "- The paper evaluates the three gates (Entropy, Margin, Variance) independently and concludes that Margin is the best default. Did the authors consider aggregating these signals?\n- Could the authors provide an analysis of the cases where TARG decides not to retrieve (low U) but the resulting answer is incorrect? How frequently do these errors occur, and do they represent \"unknown unknowns\" (the model was confidently wrong), or cases where the knowledge was absent from the corpus anyway?\n- Section 3.2 briefly mentions an optional \"single re-check\" applied every $m$ tokens. Were experiments conducted using this dynamic approach? How does the accuracy-efficiency trade-off compare to the single-shot TARG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7MB1ONkZrV", "forum": "bMDdjg75NS", "replyto": "bMDdjg75NS", "signatures": ["ICLR.cc/2026/Conference/Submission5884/Reviewer_4P8J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5884/Reviewer_4P8J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864973770, "cdate": 1761864973770, "tmdate": 1762918322388, "mdate": 1762918322388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}