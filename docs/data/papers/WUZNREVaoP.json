{"id": "WUZNREVaoP", "number": 5024, "cdate": 1757835351946, "mdate": 1759897999816, "content": {"title": "Neural Weight Compression for Language Models", "abstract": "The efficient storage and transmission of _language model weights_ is becoming increasingly important, as their scale and adoption continue to grow. However, as our understanding of this new data modality is limited, designing a good compression algorithm for language model weights heavily relies on manual, trial-and-error approaches. In this paper, we propose a learned compression framework that trains neural codecs directly from pretrained language model weights. Unlike conventional data (_e.g._, images), language model weights pose unique challenges: the sizes and shapes of weight tensors vary significantly, and the reconstruction quality must be judged by downstream model predictions rather than naïve MSE loss. To address this, we introduce Neural Weight Compression (NWC), a novel autoencoder-based neural codec tailored to model weight compression. The proposed method inherits the advantages of autoencoder-based codecs while incorporating three technical components: (1) column-wise tensor chunking and normalization; (2) an importance-aware training loss; (3) an inference-time error compensation mechanism guided by model outputs. Experiments on open-weight language models show that NWC achieves competitive or state-of-the-art accuracy-compression tradeoffs, with particularly strong results at 4-6 bit precisions where accuracy remains nearly on par with FP16 models.", "tldr": "We propose a learned compression framework that trains an autoencoder-based neural codec for pretrained language model weights, incorporating technical innovations to tackle the unique structural and functional properties of neural weights.", "keywords": ["neural compression", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b0bc2943d73c2660f711da97cf40b1e44a440c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Describes a weight compression approach that uses a trained neural network to assist quantisation. Tensors are split column-wise, and a scale per column is stored to normalise them to unit variance. Each column is also assigned an importance level using a local Hessian of layer output reconstruction error, computed from calibration data. The column is split into blocks of 16 weights, which are transformed using a small _encoder_ residual network, which is conditioned on the importance level, before quantisation and entropy encoding. Decoding uses an entropy decoder, followed by a mirror-image _decoder_ network, also conditioned on importance level, then multiplied by the column scale. This is combined with extant enhancements for incremental quantisation within a layer and across layers.\n\nThe paper contributes this specific scheme, the first (that the authors are aware of) to use a single network for compression of all model weights in a large model, and a comparison of its performance against popular data-aware post training quantisation techniques for language and vision transformers, where it performs favourably, especially for high bit-rates $\\geq 4$ bits per parameter."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The proposed neural weight compression (NWC) scheme is relatively straight-forward and is well-motivated. I find the use of sensitivity as a conditioning signal to the encoder/decoder pair as well as a weighting on the reconstruction error to be an interesting feature. The inclusion of both language and vision transformers in the experimental results is appreciated. Notation is clear throughout, diagrams are very good and plots are clear. The related work section on neural codecs is helpful for positioning this work against previous approaches. A key limitation of hardware efficiency is clearly acknowledged."}, "weaknesses": {"value": "My main concern regarding this work concerns the scientific contribution. Although the method is well-motivated, the paper does not claim direct practical application as decoding speed is reserved for future work. This is reasonable, however I would then expect strong theoretical or empirical results that can guide future work in this space. I do not think the empirical investigation meets this bar, primarily due to a lack of relevant baselines or deeper insight into what is learnt by the trainable network.\n\nSpecific concerns:\n\n1. No alternative neural codec baselines are offered. For example, ReALLM (Leconte et al., 2024) is mentioned in related work, and seems applicable, having been tested on LLaMA 2 in the original work.\n1. Many design decisions are not ablated. I appreciate the ablations of fitting to actual weight distributions, importance awareness, intra-layer and inter-layer error recovery. However the following also seem relevant:\n   - Use of an encoder/decoder network at all. Unless I'm mistaken, the entropy model could be trained using the aux loss, and $g_a$ and $g_s$ could be replaced with the identity function(?) This would test the specific benefit of the learned network.\n   - Use of entropy encoding, versus a fixed-length nonlinear quantiser.\n   - Channel-wise normalisation, versus tensor-wise.\n   - Encoder/Decoder architecture design, depth etc.\n1. No comparison against quantisation aware training techniques such as LLM-QAT (Liu et al., 2023), or alternatively any explanation given as to why this comparison is out of scope. A usual reason would be the training time needed to quantise a model, but since this technique involves training the quantisation network and incremental fine-tuning, it is not immediately obvious that this would outperform QAT.\n1. No attempt to understand the function that is learnt by the NWC model, beyond the example of Section 3.2 demonstrating the importance of codec fit at high bitrates.\n1. Missing details.\n   - The entropy model \"deep-factorized prior\" mentioned in L315 is not sufficiently clear from the reference (Ballé et al., 2016), and similarly for the entropy code itself - is it the arithmetic code from the same paper, Section 6.2?\n   - Codec training duration (samples or epochs)? I assume the training dataset size for language models is approximately 8B/16 = 500M examples.\n   - Training procedure - presumably using a straight-through estimator, with the noise described in L187? (Perhaps this is just a wording quibble on \"typically...\").\n\nMinor concerns:\n\n - I don't believe Equations 2, 3 are a good approximation to the optimal scalar quantiser with a fixed number of codepoints (where the codepoint density should follow the cube root of the pdf) or under entropy coding (where the codepoint density should be uniform).\n - Equation 1 might be clearer with an explicit expectation for $\\mathbf{x} \\sim$ Dataset.\n - L364 makes the notation slightly unclear. Presumably the combined distortion loss multiplier is $\\lambda \\cdot \\lambda_q$?\n - Table 2 shows an entropy decoding latency of 13.8 ms for a 256x256 tensor, which seems unreasonably bad (I estimate 28 minutes to decode an 8B model). I accept the premise that this work considers hardware acceleration for future work, but using this result to suggest that synthesis is relatively cheap could be misleading.\n\n---\n\n_Leconte, L., Bedin, L., Nguyen, V.M. and Moulines, E., 2024. ReALLM: A general framework for LLM compression and fine-tuning._\n\n_Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R. and Chandra, V., 2023. LLM-QAT: Data-free quantization aware training for large language models._\n\n_Ballé, J., Laparra, V. and Simoncelli, E.P., 2016. End-to-end optimized image compression._"}, "questions": {"value": "My questions are included in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5y2MlocyiX", "forum": "WUZNREVaoP", "replyto": "WUZNREVaoP", "signatures": ["ICLR.cc/2026/Conference/Submission5024/Reviewer_CDm5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5024/Reviewer_CDm5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761172925228, "cdate": 1761172925228, "tmdate": 1762917827649, "mdate": 1762917827649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed to compress neural network weights using another neural network.\nAchieves competitive performance in 4-6 bits per weight range.\n\nBut it is not practical for deployment."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Presentation of the paper is quite clear.\nAlso optimization procedure is good and well written.\nResults are strong for both language and vision models in 4-6 bit range."}, "weaknesses": {"value": "The main problem is a lack of proper inference-time support and the fact that the method is not practical for deployment.\nIf we say that compression methods without practical deployment are interesting, then it opens doors for a plethora of methods, such as compressing quantized weights via 7zip (this works very well, but nobody does that for obvious reasons). \n\nAt least the implications of decompression during inference should be properly discussed in the paper. I can imagine doing heavy decompression in a big batch scenario, but I cannot imagine doing it in a local deployment with batch 1 LLM decoding.\n\nThe secondary problem is that when the network is compressed, it cannot be easily fine-tuned (I do not mean LoRA, but full model fine-tuning).\n\nAlso, section 4.2 talks about the Hessian and then just drops back to simple Wanda input activation scaling. I think authors should be honest and not even mention Hessian here."}, "questions": {"value": "- What is inference time now for vision models? (I understand that the proposed solution is impractical for LLM decoding, but if each layer needs to be loaded once, as in vision models, we might get some meaningful tradeoffs)\n- Figure 3 and the details of the compression network are unclear. What are the quantizer, AE, and AD in Figure 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m2FAAgtHQQ", "forum": "WUZNREVaoP", "replyto": "WUZNREVaoP", "signatures": ["ICLR.cc/2026/Conference/Submission5024/Reviewer_XFXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5024/Reviewer_XFXJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761306871711, "cdate": 1761306871711, "tmdate": 1762917827385, "mdate": 1762917827385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Neural Weight Compression (NWC), an autoencoder-based neural codec that learns compact, compressible representations of pretrained LLM weights. Unlike conventional quantization approaches that rely on handcrafted transformations such as Hadamard transforms or per-channel scaling, NWC directly learns nonlinear mappings from the weight data itself. To address the challenges of heterogeneous tensor structures and varying parameter importance, NWC employs column-wise tensor chunking and normalization, coupled with an importance-aware loss function that assigns adaptive quality levels to different weight chunks based on estimated Hessian diagonals. During inference, an error compensation mechanism further enhances reconstruction quality by integrating intra-layer LDL-based feedback with inter-layer recovery fine-tuning. Extensive experiments on several LLaMA models and vision encoders have been conducted."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly structured and technically detailed, with informative figures.\n\n2. This paper addresses an important and increasingly relevant problem: learned compression of LLMs for on-device or bandwidth-limited deployment.\n\n3. This paper includes ablations and qualitative visualizations that clarify the empirical behavior of the codec."}, "weaknesses": {"value": "1. The paper exhibits limited novelty compared to recent neural codec frameworks. Similar learned compression paradigms involving codebook-based quantization and additive composition have already been introduced in AQLM [1], LCQ [2], and QuIP# [3]. \n\n2. The work also lacks theoretical grounding for its importance-aware loss. While the use of Hessian-weighted objectives follows the sensitivity-based quantization formulation seen in GPTQ [4], the paper does not justify the adequacy of diagonal approximations in highly non-convex LLM parameter spaces, nor provide convergence or variance analysis to support its objective function.\n\n3. The experimental rigor is weak, with marginal improvements typically under 0.3 perplexity or 1% accuracy. These results are not accompanied by confidence intervals or variance statistics. Furthermore, the evaluation focuses primarily on small LLaMA-2 and LLaMA-3 models, limiting generalization to larger architectures such as Qwen or Mixtral.\n\n4. The paper fails to address deployment overhead. The runtime cost of the decoder is unreported, and comparisons with QA-LoRA [7] and QTIP [8] omit critical latency and throughput metrics. Given that reconstruction is required at inference time, the actual efficiency gains remain questionable.\n\n5. Another key shortcoming is the absence of component-wise ablation. The individual contributions of chunking, importance weighting, and error compensation are not disentangled, leaving the source of improvement unclear.\n\n6. The paper omits several strong baselines, including BitDistiller [10], SVD-LLM [5], LoSparse [6], and UniCodec [9], all of which represent recent advances in model compression or quantization. \n\nReferences\n\n[1] Egiazarian et al., Extreme Compression of Large Language Models via Additive Quantization (AQLM), 2024.\n\n[2] Cai et al., LCQ: Low-Rank Codebook Based Quantization for Large Language Models, 2024.\n\n[3] Tseng et al., QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks, 2024.\n\n[4] Frantar et al., GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers, 2023.\n\n[5] Wang et al., SVD-LLM: Truncation-Aware Singular Value Decomposition for LLM Compression, 2024.\n\n[6] Li et al., LoSparse: Structured Compression of Large Language Models Based on Low-Rank and Sparse Approximation, 2023.\n\n[7] Xu et al., QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models, 2024.\n\n[8] Tseng et al., QTIP: Quantization with Task-Informed Priors, 2024.\n\n[9] Jiang et al., UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook, 2025.\n\n[10] Du et al., BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation, 2024."}, "questions": {"value": "1. How does the proposed framework fundamentally differ from recent neural codec approaches such as AQLM [1], LCQ [2], and QuIP# [3], beyond minor variations in tensor organization or weighting strategy?\n\n2. Can the authors justify why a diagonal Hessian approximation is theoretically adequate for measuring parameter importance in highly non-convex LLM parameter spaces, as opposed to more expressive curvature models?\n\n3. Are the reported sub-1 % accuracy and < 0.3 perplexity gains statistically significant, and can the authors provide confidence intervals or multi-seed variance analyses to substantiate these results?\n\n4. What is the measured runtime and latency overhead introduced by the decoder, and do the claimed memory savings still hold once inference-time reconstruction costs are included relative to AWQ [7] and QTIP [8]?\n\n5. Could the authors present an ablation isolating the effects of chunking, importance weighting, and error compensation to clarify which components contribute most to performance?\n\n6. How does the proposed method compare with BitDistiller [10], SVD-LLM [5], LoSparse [6], and UniCodec [9]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0O06vqwoz4", "forum": "WUZNREVaoP", "replyto": "WUZNREVaoP", "signatures": ["ICLR.cc/2026/Conference/Submission5024/Reviewer_H8K4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5024/Reviewer_H8K4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957756488, "cdate": 1761957756488, "tmdate": 1762917827113, "mdate": 1762917827113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Neural Weight Compression (NWC) is a learning-based framework for compactly representing pretrained model weights through neural encoders and decoders. Rather than depending on fixed quantization heuristics, NWC learns how to compress weight tensors directly, capturing nonlinear relationships across parameters. To manage the diversity of tensor shapes and parameter sensitivities, the method organizes weights into normalized column chunks and optimizes them using an importance-weighted objective that emphasizes more influential components. At inference time, NWC refines the reconstructed weights with a lightweight correction step that leverages Hessian-guided updates to reduce residual error. Evaluations on both language and vision architectures, including LLaMA models, show that NWC delivers strong compression efficiency and accuracy retention, particularly in the 4–6 bit range, across calibration-free and data-driven settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of learning nonlinear transforms directly from weight tensors is timely and clearly explained.\nThe training and inference pipeline (Fig. 3) is straightforward to follow, and implementation details are thorough.\n\nThe authors provide ablations on the key design choices. The inclusion of both calibration-based and data-free settings adds credibility.\n\nEmpirical results show consistent improvements at mid-range bitrates (4–6 bits), particularly on LLaMA-3-8B and the CLIP/SigLIP transfers.\nThese results suggest the codec captures some general statistical structure of model weights rather than overfitting to one backbone."}, "weaknesses": {"value": "1. The idea of a neural autoencoder trained on weight tensors extends existing work on learned compression [5] and weight quantization [1, 2].\nThe “importance-aware” weighting resembles the activation-aware scaling in AWQ [2] and the sensitivity metrics in GPTQ [1].\nA more precise comparison of these formulations would help clarify the novelty of this paper.\n\n2. Section 3.2 argues that learned codecs outperform handcrafted ones on heavy-tailed distributions, but this remains a qualitative observation.\nWithout quantitative analysis, such as a per-layer error statistics or rate–distortion modeling, the reader is left unsure why NWC improves mainly in the 4–6 bit regime.\n\n3. Table 2 shows that decoding is dominated by entropy-decoding cost (~14 ms per 256×256 tensor on RTX 6000).\nSince the method requires an additional decoding network, the real deployment gains relative to lightweight quantizers such as GPTQ [1] or QuIP [3] are unclear. A direct wall-clock comparison or FLOP breakdown would be valuable.\n\n4. The codec must be fine-tuned for each architecture.\nIt is unclear how much data and computing are needed for these adaptations.\nReporting training cost or sample requirements would help assess scalability.\n\n5. The main experiments use LLaMA-2 and LLaMA-3, but newer models such as LLaMA-4 and Qwen-3 are already available and would provide a stronger test of generality.\n\nReferences\n\n[1] E. Frantar et al. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers. ICLR 2023.\n\n[2] C.-H. Lin et al. AWQ: Activation-Aware Weight Quantization for On-Device LLM Compression. MLSys 2024.\n\n[3] J. Chee et al. QuIP: 2-Bit Quantization of Large Language Models with Guarantees. NeurIPS 2023.\n\n[4] V. Egiazarian et al. Extreme Compression of Large Language Models via Additive Quantization (AQLM). ICML 2024.\n\n[5] J. Ballé, V. Laparra & E. P. Simoncelli. End-to-End Optimized Image Compression. ICLR 2017."}, "questions": {"value": "1. How does the importance-aware loss differ in practice from activation-aware scaling [2] or Hessian-weighted objectives [1]?\n\n2. What is the total encoding/decoding time compared to GPTQ [1] or QuIP [3] on the same GPU?\n\n3. Does the codec trained on one model (e.g., LLaMA-3-8B) generalize to another without retraining?\n\n4. Could the authors provide per-layer error curves or singular-value spectra to substantiate the claim that heavy-tailed distributions benefit most?\n\n5. Have you considered hybrid schemes combining NWC at mid-bitrates with analytical quantizers at extreme compression levels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eHagDgTG3j", "forum": "WUZNREVaoP", "replyto": "WUZNREVaoP", "signatures": ["ICLR.cc/2026/Conference/Submission5024/Reviewer_L5q5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5024/Reviewer_L5q5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970368892, "cdate": 1761970368892, "tmdate": 1762917826767, "mdate": 1762917826767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}