{"id": "0HcqZkv1zs", "number": 10135, "cdate": 1758161650677, "mdate": 1759897671780, "content": {"title": "Clarifying Before Reasoning: A Coq Prover with Structural Context", "abstract": "In this work, we investigate whether improving task clarity can enhance reasoning ability of large language models, focusing on theorem proving in Coq. We introduce a concept-level metric to evaluate task clarity and show that adding structured semantic context to the standard input used by modern LLMs, leads to a 1.85$\\times$ improvement in clarity score (44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model DeepSeek-V3, our approach leads to a 2.1$\\times$ improvement in proof success (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous state-of-the-art Graph2Tac (33.2\\%). We evaluate this on 1,386 theorems randomly sampled from 15 standard Coq packages, following the same evaluation protocol as Graph2Tac.\nFurthermore, fine-tuning smaller models on our structured data can achieve even higher performance (48.6\\%).\nOur method uses selective concept unfolding to enrich task descriptions, and employs a Planner-Executor architecture. These findings highlight the value of structured task representations in bridging the gap between understanding and reasoning.", "tldr": "", "keywords": ["theorem proving", "Coq", "structured reasoning", "formal verification"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ead97ffe8496c0912157e9ef7e4234c8823ec935.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a method to incorporate semantic context into theorem proving models. They propose a clarity score to evaluate the understanding of this context. They demonstrate that this clarity helps downstream performance in proving theorems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors reach a new SOTA on theorem proving using an LLM. They define a clarity metric which seems interesting and not explored enough before in prior work, and they demonstrate that their setup can improve this metric. They also show the strong correlation between their metric and theorem proving capability."}, "weaknesses": {"value": "It seems to me that this work is a simple variation of existing work on enhancing the semantic context for theorem proving, and especially premise selection. The authors introduce three ways of enhancing the context for proving: (1) entity extraction, (2) proof state extraction, and (3) domain-specific tokenization. It seems to me that (1) is a subset of previous work in premise selection (adding the information of related definitions / theorems to the context) such as in Graph2Tac, and contextual information like in Baldur and miniCTX. For (2), if I understand correctly, it is the standard data extraction for all tactic-level provers. (3) is basically already done in previous work like Graph2Tac, where global definitions are resolved, and seems like a small syntactic change to me.\n\nThe presentation of this paper is puzzling for me. While the writing is easy to follow, the authors use a significant portion of the paper to introduce surrounding concepts like Coq’s type theory, definitions, and proof state and tactics, which should already be familiar to the general reader. On the other hand, the core method is never presented in the main text, and I had to read through the long appendix to partially understand what exactly is input to the LLM. I encourage the authors to replace vague bullet points and task descriptions with concrete explanations or examples of what exactly the input to the LLM is."}, "questions": {"value": "Can the authors clearly describe what exactly is input to the LLM prover (e.g. with an example) using their setup of enhancing context clarity, and how does it differ from existing setups like premise selection?\n\nWhat is \"Chinese Translation\" in Table 1?\n\nThe authors mention in the abstract and introduction they have a \"selective\" concept unfolding technique, but I could not find it in the methods or appendix. What is it and where is this defined?\n\nIn the writing, there are some parts that are clearly redundant and possibly LLM-written. There are many bullet points that have vague / unnecessary / incorrect headlines. For example the bullet point on L798 repeats L805. \"Semantic Foundation\" on L348 looks like a completely misplaced phrase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XnDxg2qrMS", "forum": "0HcqZkv1zs", "replyto": "0HcqZkv1zs", "signatures": ["ICLR.cc/2026/Conference/Submission10135/Reviewer_7gUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10135/Reviewer_7gUL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761076950087, "cdate": 1761076950087, "tmdate": 1762921508518, "mdate": 1762921508518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigated the relationship between a model's conceptual understanding and its reasoning performance in mathematical theorem proving. It proposes a new metric, the clarity score, that evaluates how well a model understands a task-related concept. Building on this, it further proposes a planner-executer pipeline that uses a general-purpose LLM to first identify relevant concepts and strategies, then translates strategies into Coq tactics. Empirically, the paper presents an analysis of clarity under different prompt configurations, demonstrates correlation between clarity and proof success, and shows that incorporating structured semantic context improves theorem-proving performance over prior methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The contribution of introducing conceptual clarity as a metric is interesting. \n- Empirical results are consistent. Authors show improved success rate on tasks over the baseline methods, as well as a light-weighted fine-tuned model achieving the highest success rate (matching the result on the non-fine-tuned general-purpose model)."}, "weaknesses": {"value": "- The paper's methodology is difficult to follow. While the introduction emphasizes that one of the key contributions is enhancing task understanding in general-purpose models through enriched task descriptions, the corresponding methodological section (Section 5) lacks clarity. For instance, the presentation of the “structured semantic context” pipeline is difficult to follow, with important details scattered or deferred to the Appendix. A substantial reorganization and clearer exposition would be needed to make this contribution accessible and verifiable. Phrases such as \"additional enhancements\", \"surface and internal representation\" remain vague without concrete examples. \n- My second concern is whether the correlation shown in Table 3 is because \"understanding improvements directly drive reasoning performance\" or simply a correlation (not causation) because we are working with richer inputs (and the model gets more signals or cues). This, therefore, sounds over-claiming without clearer evidence. Additional clarification or an additional ablation study would help."}, "questions": {"value": "- Have the authors conducted any experiments designed to test whether improvements in clarity score cause higher reasoning success, rather than merely correlate with it?\n- It seems that the LLM judge for clarity score measurement uses the same model as the planner-executer pipeline (Deepseek-V3), so I'm wondering whether there would be a bias towards the measurement (e.g., evaluator might favor its own generation, etc.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pnVFbZk1ZD", "forum": "0HcqZkv1zs", "replyto": "0HcqZkv1zs", "signatures": ["ICLR.cc/2026/Conference/Submission10135/Reviewer_P1c5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10135/Reviewer_P1c5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748040044, "cdate": 1761748040044, "tmdate": 1762921505760, "mdate": 1762921505760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes enhancing theorem proving in Coq by introducing structured semantic context extracted from Coq’s internal type system, combined with a Planner–Executor architecture. A new metric, Clarity Score, is introduced to quantify how well a model “understands” a task. The authors claim that increasing clarity leads to a proportional improvement in theorem-proving."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The central idea—enhancing reasoning by improving task clarity—is an appealing direction beyond model scaling and reinforcement learning.\n2. The proposed Coq compiler interception pipeline that captures internal representations is a technically valuable contribution, potentially useful for future formal reasoning datasets.\n3. This paper introduces a new metric to quantify how well a model “understands” a task."}, "weaknesses": {"value": "1. The “Clarity Score” uses the same model (DeepSeek-V3) as both generator and evaluator, introducing self-evaluation bias.\nNo external or human validation is performed, so the metric may not measure conceptual understanding.\n2. The comparison with Graph2Tac is not entirely fair, as Graph2Tac is a much smaller GNN-based model while this paper employs a large-scale LLM (DeepSeek-V3). Although the authors claim it is a general-purpose model not specifically trained on mathematical data, its scale and pretraining corpus far exceed those of the baseline. Moreover, fine-tuned single models like Qwen-2.5-7B or 32B might yield comparable or even better results. Hence, the reported performance gains may stem from model capacity rather than the proposed clarity enhancement.\n3. The Planner–Executor architecture is incremental and mirrors prior systems such as Lean-Star (2024) and Apollo (2025). Apart from the proposed metric, the contribution is mainly an engineering integration of structured context."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PpBXu23uqd", "forum": "0HcqZkv1zs", "replyto": "0HcqZkv1zs", "signatures": ["ICLR.cc/2026/Conference/Submission10135/Reviewer_kKdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10135/Reviewer_kKdN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926264637, "cdate": 1761926264637, "tmdate": 1762921502736, "mdate": 1762921502736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach to LLM-based theorem proving in Coq, built on the hypothesis that enhancing task clarity is a distinct and important step for improving reasoning. The authors present: 1) a \"Clarity Score\" metric to quantify a model's understanding of formal concepts; 2) a data pipeline that extracts \"structured semantic context\" from the Coq compiler's internal representations; and 3) a Planner-Executor architecture that leverages this structured data.\n\nUsing this method, the authors claim to more than double the proof success rate of a general-purpose model (DeepSeek-V3), outperforming the previous state-of-the-art, Graph2Tac. While the core hypothesis is strong and the technical execution is impressive, the evaluation remains fairly narrow to support the SOTA claims. The comparison hinging on a single baselines, and only Coq programs is the most notable limitation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This was a refreshing paper to read, with genuinely novel approaches, and well thought out experiments. The core idea of separating and measuring \"task clarity\" as a distinct bottleneck from \"reasoning\" is a good contribution. The data processing pipeline, involving modification of the Coq compiler to extract internal type-theoretic information, is a non-trivial and valuable piece of engineering. The fine-tuning results (Tab 5) are highly valuable, showing a 32B parameter model achieving 48.6% success (outperforming a 671B model), demonstrating the data's quality. The Planner-Executor design is a natural fit for leveraging the extracted semantic context."}, "weaknesses": {"value": "The following points are constructive feedback; I will increase my rating if the key experiments are conducted during the rebuttal.\n1. The SOTA claim (45.8% vs 33.2%) rests on a comparison against one baseline (Graph2Tac). Would you be able to show via a pilot evaluation in LEAN, a direct comparison or against other provers mentioned in the related work, such as DeepSeek-Prover, Kimina-Prover, and Llemma. \n2. The evaluation is on a 10% random sample of a benchmark (is it 1,300 or 1,386 theorems). This prevents replication and rigorous comparison. The paper should evaluate on a full, standard benchmark (e.g., the full Graph2Tac test set, PACT).\n3. Limited Scale of Core Claims:\n    * The central correlation claim (Tab 3, r=0.98) is based on only n=100 theorems. This is okay robust enough but ideally must be re-run on the full dataset, if cost permits.\n    * The architectural ablation (Table 8), claiming a +24% gain, is based on only n=78 theorems from a single library. Can you do better?\n4. The \"Clarity Score\" is evaluated using DeepSeek-V3 as the judge (line 266), which is the same model used in the main experiments. LLMaj can be influenced by this circularity and this may introduce potential bias. Would you be able to try it with another family, e.g. Qwen?"}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U8DPSHg43Z", "forum": "0HcqZkv1zs", "replyto": "0HcqZkv1zs", "signatures": ["ICLR.cc/2026/Conference/Submission10135/Reviewer_RBDa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10135/Reviewer_RBDa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936918455, "cdate": 1761936918455, "tmdate": 1762921501003, "mdate": 1762921501003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}