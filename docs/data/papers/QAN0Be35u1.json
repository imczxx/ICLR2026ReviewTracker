{"id": "QAN0Be35u1", "number": 20846, "cdate": 1758310843939, "mdate": 1759896955639, "content": {"title": "Randomized Gradient Subspaces for Efficient Large Language Model Training", "abstract": "Training large language models (LLMs) is often bottlenecked by extreme memory demands, with optimizer states dominating the footprint. Recent works mitigates this cost by projecting gradients into low-dimensional subspaces using sophisticated update strategies.\nIn this paper, we analyze the dynamics of gradient space and its underlying subspaces. We find that while a small subspace captures most gradient energy, a significant portion still resides in the residual bulk; moreover, the influence of the core subspace diminishes over time and in deeper layers. We also observe that the gradient space exhibits near-flat curvature, calling for algorithms that explicitly account for this geometry. Motivated by these insights, we introduce a suite of randomized algorithms, GrassWalk and GrassJump, which exploit subspace and achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining.", "tldr": "", "keywords": ["efficient training", "low-rank methods"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/476cbed352999dcb18e1e6d6b95177318bd02114.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the high memory demands of training LLMs, which are often bottlenecked by optimizer states. The authors analyze gradient subspace dynamics, finding that the dominance of a low-rank subspace declines over time, especially in deeper layers, and evolves in a nearly flat curvature. Motivated by these insights, they introduce GrassWalk and GrassJump, two randomized methods that update the gradient subspace using random walks and jumps on the Grassmannian manifold3333. By adapting the optimizer to these changes and recovering information lost during projection, the methods achieve superior performance and memory efficiency in pretraining LLaMA-1B and LLaMA-7B models4."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary strength lies in its analysis of gradient subspace dynamics (Section 3). The finding that the dominance of the low-rank \"core\" subspace diminishes over training and in deeper layers (Figure 1) is a significant and non-obvious contribution. It provides a strong \"why\" for moving beyond simple subspace tracking.  \n- The analysis of the subspace estimation error (Figure 2) provides a principled, geometric justification for using randomized methods. The conclusion of a \"nearly flat curvature\" compellingly reframes random projections from a mere computational shortcut to a motivated strategy for escaping poor local optima in the subspace manifold.  \n- The proposed methods GrassWalk and GrassJump claim to achieve state-of-the-art results on LLaMA-1B and 7B pre-training (Tables 1 & 2). Outperforming strong, contemporary baselines like SubTrack++ in final evaluation loss while maintaining equivalent memory efficiency is a clear and impressive empirical win.  \n- The ablation in Figure 3 dissects the method into its three key components (Subspace Update, Adaptive Optimizer, Recovery Scaling) and clearly demonstrates that the randomized update strategies are only effective when combined with both AO and RS. This transparency is crucial for understanding the method's mechanics."}, "weaknesses": {"value": "- The two components that appear to drive the majority of the performance gain (adaptive optimizer and recovery scaling) are explicitly and fairly acknowledged as being adapted from prior work (Robert et al., 2025; Anonymous, 2025; Chen et al., 2025b). The core algorithmic novelty is limited to the randomized subspace update rule itself. This makes the paper feel more like a good analysis that validates a simple, effective tweak to an existing SOTA framework (e.g., SubTrack++).  \n- The paper introduces GrassWalk as a more \"principled\" random walk on the manifold, but GrassJump (a full, \"unprincipled\" random projection) performs better on the larger 7B model. This finding somewhat undermines the motivation for the more complex GrassWalk method and suggests the benefits may just stem from the regularization of random projections, rather than a controlled \"walk.\"  \n- The intuitive  \"flat curvature\" argument from Figure 2 could be more formally defined. Is this the curvature of the main loss landscape $L(W)$, or the curvature of the auxiliary objective $f(S) = ||G_t - S S^\\top G_t||_F^2$ on the Grassmannian manifold $Gr(r, n)$? Clarifying this would strengthen the paper's central theoretical motivation."}, "questions": {"value": "- The performance of GrassWalk and GrassJump likely depends heavily on how frequently the subspace is updated ($T$) and how far each update moves on the Grassmannian ($\\eta$). Could you please provide sensitivity plots or an empirical study showing how evaluation loss varies with them? In particular, does GrassWalk’s random-walk dynamics require fine-tuning of to maintain numerical stability?  \n- RS rescales the discarded gradient component using optimizer outputs. However, when random projections drastically change the subspace, the discarded portion $\\Delta_t$ may be nearly uncorrelated with prior directions, raising the risk that RS amplifies noise rather than useful signal. Could you analyze or visualize this effect—for instance, by plotting the ratio $\\|\\Lambda_t\\|/\\|G_t\\|$ over training, or showing how performance changes when RS is disabled for random projections? Clarifying whether RS stabilizes or destabilizes random subspace updates would help interpret Figure 3.  \n- Since GrassWalk introduces gradual manifold exploration and GrassJump performs abrupt rerandomization, they seem complementary. Did the authors consider hybrid schedules, e.g., performing several GrassWalk steps followed by an occasional GrassJump to escape flat regions? This could potentially balance stability and exploration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a0h3DkHfQJ", "forum": "QAN0Be35u1", "replyto": "QAN0Be35u1", "signatures": ["ICLR.cc/2026/Conference/Submission20846/Reviewer_VgBp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20846/Reviewer_VgBp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547910728, "cdate": 1761547910728, "tmdate": 1762999996011, "mdate": 1762999996011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces GrassWalk and GrassJump, two techniques used in the context of low-rank factorization of gradients to reduce the memory usage of optimizer states.\n\nThe paper builds on top of prior work SubTrack++, which introduces the idea of a \"core\" subspace.\n\nThe paper shows that at least 50% of the information is preserved in the core space, meaning there might be some useful information in the remaining dimensions that are not captured by the core space (non-core). They analyze the curvature of this non-core space and show it exhibits low curvature.\n\nTheir methods replace the rank-1 factorization of the tangent vector $\\nabla F$ on the Grassmannian manifold in SubTrack++ with a random orthogonal matrix used as it is (called GrassJump) and alternatively inputting it into the exponential map on the Grassmannian manifold (called GrassWalk).\n\nThey test GrassWalk/Jump in pretraining settings for Llama models with 1B and 7B parameters and claim superior performance compared to the main baseline SubTrack++."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Using random matrices to reduce the overhead of more accurate methods to perform low-rank decomposition, such as SVD, is an interesting idea\n- the analysis in Section 3 suggests that the flatness of the non-core space (e.g. low-rank projection error, or the subspace that is not chosen for optimization) implies that random steps can be advantageous because the model will not be trapped in a sharp local minima"}, "weaknesses": {"value": "1. despite changing only the way the projection matrix $S_t$ is computed/updated, the paper has many paragraphs that are **almost identical** to the SubTrack++ paper in section 4. This suggests \"thin slicing\" or just a small increment over prior work.\n\n2. using QR decomposition to orthogonalize the randomly generated matrix for GrassJump introduces a computational overhead as the runtime of QR-decomposition algorithm scales with $O(n^3)$\n\n3. It seems like the methods introduced in this work are not much better than the rank-1 approximation of the tangent vector $\\nabla F$ on the Grassmannian manifold. The results in Tables 1 and 2 are not significantly better compared to SubTrack++. As shown by the prior work in the area, a significantly lower running time is expected for the methods based on random/fixed projections."}, "questions": {"value": "Questions:\n1. lines 202-206: which rank did you use for Figure 1? is the rank correlated with the lower-bound of 50% that we see in Figure 1? Would Figure 1 look similarly (e.g. 50% as a lower bound) across multiple rank ratios? By rank ratio I mean rank divided by the dimension: e.g. for the original layer of size $(m, n)$ with $m < n$, this ratio is $r/m$.\n\n2. How many seeds did you use to generate the results in Tables 1 and 2? Is the reported number an average across multiple runs? If yes, then it would be useful to see best, the works, the median, the mean and the standard deviation to understand how the result is affected by the randomness in the method.\n\n3. About running times, related to Weakness 3: it seems like the QR decomposition in GrassJump is the root cause of the high running time of the method, while prior work [1] suggests much lower running time. Did you try to use low-rank orthogonal matrices without using QR decomposition? Note this is slow as it has to be run at each layer when you generate the random matrix.\n\n4. What about choosing some random columns from the identity matrix instead of generating a random Gaussian matrix?\n\n\n**Observations**:\n\n1. I do not agree on the statement that at lines 11-12 in the abstract that *training LLMs is often bottlenecked by optimizer states which are dominating the footprint*. This is not true in general for the practical scale nowadays. When using Adam with a high model parallelism (e.g. many nodes to perform training across), the layers are split in such a way that the Adam states take a small fraction of the total memory (see Figure 1 in reference [2]). Your statement is true in low-memory settings, where we are forced to use batch-size 1 because otherwise the activations would require a lot of space. I would appreciate it if you could rephrase the statement in the abstract and/or add some more precise details, in order to target some specific settings fairly.\n\n2. Please increase the labels on the x/y-axis of your plots, it is difficult to read them.\n\n\n\n**Typos:**\n\n- line 50: comma between **succeed** and **but**\n- line 52: space after **GrassJump**\n- line 53: **misleading → mislead** the optimizer (the very end of page 1)\n- lines 230, 235, 236: space after **GrassWalk** and **GrassJump**\n\n\n\n**References:**\n\n[1] Authors: *Ionut-Vlad Modoranu, Mher Safaryan, Erik Schultheis, Max Ryabinin, Artem Chumachenko, Dan Alistarh*, Title: **FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization of Large Language Models**, https://arxiv.org/abs/2505.17967\n\n[2] Authors: *Yiming Chen, Yuan Zhang, Yin Liu, Kun Yuan, Zaiwen Wen*, Title: **A Memory Efficient Randomized Subspace Optimization Method for Training Large Language Models**, https://arxiv.org/pdf/2502.07222"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mq2Q2aSRYE", "forum": "QAN0Be35u1", "replyto": "QAN0Be35u1", "signatures": ["ICLR.cc/2026/Conference/Submission20846/Reviewer_X2EV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20846/Reviewer_X2EV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897231918, "cdate": 1761897231918, "tmdate": 1762999997142, "mdate": 1762999997142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLM training is memory-bound—mostly by optimizer states—so the paper studies gradient subspaces and finds that (i) most energy concentrates in a small subspace but a meaningful residual remains, (ii) the core subspace’s influence diminishes over time and in deeper layers, and (iii) curvature is near-flat, suggesting geometry-aware methods. Motivated by these observations, it introduces two randomized low-rank algorithms, GrassWalk and GrassJump, which exploit subspace structure to achieve state-of-the-art memory savings and improved pretraining performance on LLaMA-1B/7B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Identified that most energy concentrates in a small subspace but a meaningful residual remains\n\n- Identified that the core subspace’s influence diminishes over time and in deeper layers\n\n- Identified that curvature is near-flat, suggesting geometry-aware methods\n\n- Proposed two randomized effective low-rank algorithms, GrassWalk and GrassJump"}, "weaknesses": {"value": "1. **Subspace observation**. A major contribution of this work is a comprehensive analysis of gradient subspaces during LLaMA pretraining. However, the central result—that the dominance of low-rank subspaces diminishes over time—appears to be implied by the cited work [He et al., 2025], which argues that a random-noise subspace eventually dominates as the effective gradient shrinks. Moreover, [He et al., 2025] recommends using random projection matrices in later stages, consistent with the claim in lines 221–222. The authors should clearly delineate what is new relative to [He et al., 2025].\n\n2. **Insight behind GrassWalk and GrassJump**. I may have missed it, but the paper does not clearly explain why GrassWalk and GrassJump should work. In Section 3, it says that \"these observations suggest that the gradient subspace evolves within an almost flat curvature, while there is a considerable portion of energy carried by the bulk component of the gradient.\" The authors then conclude that \"random steps can be advantageous\". However, there are many ways to construct the random projection, such as the random Gaussian. I still do not understand why GrassWalk and GrassJump are preferable to a standard Gaussian random projection based on your subspace analysis. \n\n3. **Algorithm design**. It appers to me that the algorithm design may not be sufficiently novel. The paragraph between Line 250-268 seems to be similar to Subtrack++, and the pargraph between 269-307 seems to be similar to Fira. It is better to highlight the novlety more clearly. \n\n4. **Necessity to save optimizer states**. ZeRO-style [R1] sharding already reduces optimizer-state memory by ~1/N per data-parallel replica without changing the Adam mathematical update (and hence maintain the same convergence quality as full-rank Adam). However, it is typically believed that the compressed-state optimizer cannot match the performance of full-rank optimizer. Therefore, I personally think compressing optimizer states is unnecessary due to the existence of the ZeRO technique. I would like to hear the opinion from the authors.\n\n[R1] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models."}, "questions": {"value": "1. Please claify your subspace analysis from that in [He et al., 2025].\n\n2. Please explain the core insight behind GrassWalk and GrassJump—how they are derived from your empirical subspace observations and why they should outperform a Gaussian random projection.\n\n3. Please specify the novel elements of your algorithmic design relative to prior low-rank and subspace-tracking methods.\n\n4. What rank(s) are used in the experiments, and how do your methods compare with full-rank Adam/AdamW in both accuracy and efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rcQvpAuDjX", "forum": "QAN0Be35u1", "replyto": "QAN0Be35u1", "signatures": ["ICLR.cc/2026/Conference/Submission20846/Reviewer_1bsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20846/Reviewer_1bsn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762384101034, "cdate": 1762384101034, "tmdate": 1762936338958, "mdate": 1762936338958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies low-rank gradient projection for memory-efficient LLM training and proposes two randomized subspace methods, GrassWalk and GrassJump. The authors  analyze gradient subspace dynamics in pretraining, observing that (i) a core low-rank subspace captures a substantial fraction of gradient energy but its share decreases over time and in deeper layers, and (ii) the gradient subspace evolves under nearly flat curvature. These observations motivate randomized subspace exploration combined with (a) projection-aware Adam state updates when the basis changes and (b) recovery scaling to re-inject information lost by low-rank projection into the full-rank update."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear conceptual framing  and insightful analysis of gradient subspace dynamics (diminishing core energy; near-flat curvature) that motivates randomized exploration.\n\n\n2. Proposed GrassWalk and GrassJump avoid expensive per-step SVD and include principled AO and RS to mitigate basis changes and projection loss.\n\n3. Competitive short-budget results on LLaMA-1B  (10k steps) and 7B, beating strong baselines at similar memory/time."}, "weaknesses": {"value": "1. The experiments are short-run, limited to 10k training steps for LLaMA-1B and 7B. While results show modest early-phase gains, such improvements at this scale do not guarantee similar behavior in longer or full pretraining regimes, where optimization dynamics and curvature can change substantially. Longer runs or additional model families are needed to confirm scalability and stability.\n\n2. The evaluation is restricted to training/eval loss, without downstream or zero-shot benchmarks to test whether early-loss advantages translate to useful model quality.\n\n3. Sensitivity to key hyperparameters such as rank r, update interval T, and AO/RS controls is not explored.\n\n4. Evidence is confined to LLaMA models. Validating on diverse architectures would better demonstrate generality."}, "questions": {"value": "1. Please report total tokens processed for each run. Also provide sequence length, effective global batch size, and grad-accum steps.\n\n\n2. Can you extend comparisons to ≥50k–100k steps (or a few billion tokens) with the same baselines to test if the ranking at 10k steps persists?\n\n3. Provide ablations over rank r, update interval T, AO details, and RS limiter ζ . How sensitive are the results to these choices?\n\n4. The analysis argues for near-flat curvature and diminishing core energy. Can you provide quantitative metrics alongside training curves to show correlation with method performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y1COJKMG5K", "forum": "QAN0Be35u1", "replyto": "QAN0Be35u1", "signatures": ["ICLR.cc/2026/Conference/Submission20846/Reviewer_11mw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20846/Reviewer_11mw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762513256985, "cdate": 1762513256985, "tmdate": 1762936337659, "mdate": 1762936337659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}