{"id": "wCV1efgkyV", "number": 4199, "cdate": 1757631100524, "mdate": 1759898047889, "content": {"title": "Evaluating Language Models in Longer Conversational Contexts", "abstract": "Evaluating long-form conversations between humans and large language models (LLMs) presents a significant challenge in the field of natural language processing. Traditional evaluation metrics and benchmarks have largely focused on shorter language interactions and often fail to capture the nuanced inherent in extended dialogues. To address this, we introduce UPHELD, a publicly available dataset featuring human-annotated long-form dialogues. This dataset not only facilitates robust benchmarking but also serves as a foundation for further research into conversation evaluation methodologies. Using our dataset, we systematically analyze the correlation between current LLM evaluation metrics and human judgment within long-form conversation scenarios. Our findings reveal that conventional metrics lack the sensitivity necessary to assess the complex and often subjective nature of prolonged interactions. We use our dataset to develop an improved evaluation metric that demonstrates a significantly higher correlation with human assessments. The work highlights the need for advanced metric designs and outlines a clear pathway to refine the evaluation of LLM long-form conversations.", "tldr": "Annotated expert made dataset consisting of long-form conversations, with a study of evaluation metrics in multi-turn conversation settings.", "keywords": ["evaluation", "long-form conversation", "llm-as-a-judge", "dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ff52ab722cbf409f7ad8307e10ba24fbd8e9167.pdf", "supplementary_material": "/attachment/581da9e5a4f497da1575fd9643cc7a71b10366cb.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces UPHELD, a publicly available dataset featuring human-annotated long-form dialogues. This dataset not only facilitates robust benchmarking but also serves as a foundation for further research into conversation evaluation methodologies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces a new dialogue data, UPHELD, that includes multi-turn conversations with open-ended and natural dialogues across various topics such as customer service and education, and also provide manual annotations. \n- This paper also shows validation results using other datasets such as LLM Arena and Topical Chat."}, "weaknesses": {"value": "-\tRelated papers on dialogue evaluation metrics are not included as related research. Also, although there are some evaluation aspects commonly used for human evaluation on response generation, there is no comparison and/or discussion on differences with the conventional approaches. \n-\tThe number of response generation models used for validating the evaluation metrics is small, which may result in a lack of diversity in response tendencies."}, "questions": {"value": "- Automatic evaluation metrics for dialogue have been extensively studied from the perspective of one-to-many issues in dialogue. Papers on dialogue evaluation metrics, such as USR, G-EVAL etc., should be cited and compared.\n\n [1] USR: An unsupervised and reference free evaluation metric for dialog generation (ACL2020)\n\n [2] GEval: NLG Evaluation using Gpt-4 with Better Human Alignment (EMNLP2023)\n\n [3] CHATEVAL: TOWARDS BETTER LLM-BASED EVALUATORS THROUGH MULTI-AGENT DEBATE (ICLR2024)\n\n [4] A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators (AAAI2024)\n\n- Common evaluation aspects in dialogue include fluency, engagingness, consistency, coherence, informativeness, and relevance. Since this paper introduces new original aspects, a detailed discussion is needed to justify the necessity of using these new aspects and how they differ from conventional ones. \n- To verify whether the reference-full approach can collect meaningful labels on ground-truth content overlap, the validation described in Appendix C appears insufficient. (Using only GPT-4o to validate the variety of responses is inadequate.)  \n- In Figure 1, GPT-3.5 and GPT-4 show high “reasonableness” scores but relatively low “content accuracy”. Does this not indicate that the responses are reasonable but may differ from the reference? \n- What type of correlation was used in Tables 2 and 3? Typically, Pearson, Spearman, and Kendall correlations are often examined side by side due to their differing characteristics. Furthermore, is this correlation at the turn level or the system level?  \n- Based on the prompts, the criteria for LLM-judge focus on whether the content is semantically equivalent, so it is reasonable that it correlates highly with “content accuracy”. However, if the evaluation also considers style accuracy and reasonableness, it might be better to prepare prompts specifically designed for them as well.  \n- The comparison models, limited to only three (gpt3.5, gpt4o, and llama), might be insufficient for evaluating the metrics comprehensively.\n- Example 13 of Table 15, the history ends with “assistant”, but should it not end with “user” instead?\n- It would be helpful to provide a detailed explanation of the calculation method for ensemble metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bNkm5UGNzc", "forum": "wCV1efgkyV", "replyto": "wCV1efgkyV", "signatures": ["ICLR.cc/2026/Conference/Submission4199/Reviewer_k9p5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4199/Reviewer_k9p5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540930769, "cdate": 1761540930769, "tmdate": 1762917226513, "mdate": 1762917226513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Distinguishing Our Work from the Literature"}, "comment": {"value": "As most reviewers requested better distinction between the UPHLED dataset and other available benchmark datasets, we provide this comparison within this general comment. A total of 8 papers were suggested as important missing references, although one paper was already a citation within our work. We appreciate that the papers suggested by the reviewers are generally pertinent and valuable to include, although we assert that the benchmark comparisons and citations already existent in the paper already provide the necessary arguments and discussion to differentiate our dataset from the other benchmarks. We do note that one recommended paper was rather recent and was published in a peer-reviewed venue only about 4 weeks before the ICLR deadline, and we ask for some understanding in not including discussion of this work.\n\nNevertheless, we acknowledge that we generally made the assumption that human-generated/human-evaluated data are desirable above machine-generated/machine-evaluated data, and did not completely flesh out the reasoning in an explicit way. To highlight the reasoning here and also emphasize UPHELD’s differences to other benchmarks we will include the table (nd Appendix Tables 6-8) along with associated discussion below. We hope this makes the motivation and contribution of our work crystal clear.\n\n----\n\nThe main differences between the UPHELD dataset and our evaluation compared to the related works:\n\n**Dataset Size**: UPHELD provides one of the highest numbers of comparison labels in the literature (second only to **WoW** ([Dinan et al. 2018](https://arxiv.org/abs/1811.01241)). While conversation count is low, we offer **dense, multi-annotator labels** per turn, enabling deep analysis like annotator agreement. UPHELD delivers $\\sim 10\\text{x}$ more data points for quality analysis than the highest next dataset, **COMPREHENSIVE-turn** (36,873 vs 3,901).\n\n**Reference-full Labels**: we prioritize **reference-full human labels** over the trend of referenceless LLM judges or arena A/B tests. Research shows that relying on LLM judges introduces drawbacks like unreliability and poor out-of-distribution performance ([Krumdick et al. (2025)](https://arxiv.org/abs/2503.05061)), and the risk of benchmark data leaking into LLM training sets ([Mirzadeh et al. (2024)](https://arxiv.org/abs/2410.05229)). Furthermore, studies indicate that LLM judge quality fails to exceed that of non-expert human judges ([Bavaresco et al. (2024)](https://arxiv.org/html/2406.18403v1)).\n\n**Synhetic Data Issues**: Very large dialogue datasets are usually fully synthetic (no real humans involved). Only **7 of 13 reviewed datasets** provide any human verification, and usually only partial. The reliance on fully LLM-simulated data introduces significant biases in evaluating dialogue capability, as studies observe limits in LLM dialogue generation ([Wang et al. (2025)](https://arxiv.org/html/2501.08579v1)) and biases in LLM-simulated data for training and evaluation ([Mirzadeh et al. (2025)](https://arxiv.org/html/2506.10301v1), [Wang et al. (2025)](https://arxiv.org/html/2501.08579v1)).\n\n**Task-Oriented Conversation**: Synthetic data often produces lengthy, interview-style conversations. For **task-oriented dialogues**, creating artificially long conversations is unnecessary; such dialogues are better suited for evaluating context usage (where context is often separate from the conversation itself, e.g., in [Wang et al. (2024)](https://arxiv.org/pdf/2402.17753)).\n\n**Context Free**: UPHELD does not rely on context engineering, distinguishing between a model's ability to produce specific knowledge (which RAG/in-context learning handles) and its **conversational ability**. *Testing knowledge production is generally unrelated to holding a useful conversation.* UPHELD is one of the only datasets (alongside **DailyDialogue** [Li et al. (2017)](https://aclanthology.org/I17-1099.pdf)) to use fully human-written conversations (not extracted from noisy sources) and explicitly evaluate data points, uniquely focusing on the underrepresented **task-oriented dialogue** space.\n\n**Expert Quality Assurance**: To guarantee high quality, UPHELD used **expert writers** with proven dialogue skillsets, providing writer backgrounds—details generally absent in other benchmarks (e.g., \"crowdsource platform\" annotators). The reader is not left to assume labelers possess necessary skills (as in [Zhang et al. (2018)](https://arxiv.org/pdf/1801.07243)). Research supports that using professional writers, as we did, leads to notable quality differences in dataset collection ([Pilan et al. (2024)](https://aclanthology.org/2024.sigdial-1.38/), [Yin et al. (2024)](https://arxiv.org/html/2412.11896v1)).\n\n----\n\nWith all of the above mentioned differences, we feel that the UPHELD dataset is a unique dataset which provides high quality conversations and a very large number of labels for quality in task-driven dialogues, along with detailed evaluation thereof."}}, "id": "kC6TSJmBIz", "forum": "wCV1efgkyV", "replyto": "wCV1efgkyV", "signatures": ["ICLR.cc/2026/Conference/Submission4199/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4199/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4199/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763678756642, "cdate": 1763678756642, "tmdate": 1763678756642, "mdate": 1763678756642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UPHELD, a dataset designed to evaluate LLMs in long-form conversation. The dataset consists of 400 dialogues, with an average length of 5.2 turn pairs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "1. The dataset, comprising 400 dialogues entirely handwritten by annotators, is presented without any description of the quality control process. It is also unclear how well these dialogues cover real-world scenarios, as the paper provides no information on data categorization, domains, or collection scenarios. Based on the provided examples, the data appears to be overly simplistic.\n\n2. The evaluation methodology is questionable. Although a pairwise comparison (Option A vs. B) is used, the evaluation criteria are limited to \"Equivalence\" (both content and style) with the single human-written response. Equivalence is a poor metric for open-ended conversation, where multiple valid, high-quality responses can exist. This approach would unfairly penalize a model's response that might be different from, or even superior to, the human reference.\n\n3. The scope of the model evaluation is extremely limited. The paper only benchmarks three open-source and closed-source models (GPT-3.5, GPT-4o, and Llama-3.1-70b). A contemporary benchmark paper should include a much wider and more representative set (e.g., 15+) of both open- and closed-source models to provide a meaningful comparison.\n\n4. The paper is missing citations to several key related works:\n```\n@article{sirdeshmukh2025multichallenge,\n  title={Multichallenge: A realistic multi-turn conversation evaluation benchmark challenging to frontier llms},\n  author={Sirdeshmukh, Ved and Deshpande, Kaustubh and Mols, Johannes and Jin, Lifeng and Cardona, Ed-Yeremai and Lee, Dean and Kritz, Jeremy and Primack, Willow and Yue, Summer and Xing, Chen},\n  journal={arXiv preprint arXiv:2501.17399},\n  year={2025}\n}\n\n@article{bai2024mt,\n  title={Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues},\n  author={Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and others},\n  journal={arXiv preprint arXiv:2402.14762},\n  year={2024}\n}\n```"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0FcU1REyM6", "forum": "wCV1efgkyV", "replyto": "wCV1efgkyV", "signatures": ["ICLR.cc/2026/Conference/Submission4199/Reviewer_fTef"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4199/Reviewer_fTef"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569383225, "cdate": 1761569383225, "tmdate": 1762917225204, "mdate": 1762917225204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Related work table (included in the new version of the paper)"}, "comment": {"value": "We included this table in the Appendix of the paper to highlight the distinction of UPHELD from other datasets and clarify the paper's contributions and the gap UPHELD dataset fills in the benchmark space. \n\n| | |data statistics| | |collection information| | |task data| | | | |data verification| | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|dataset/benchmark|link|total conversations|avg utterances|total comparison labels| only human|human verified|directly obtained|type of predictions|is conversation|task type|uses context |comparison to ground truth|uses human annotators|correlation with human|provides author/creator selection details|\n|upheld| |53|10.2|36,873|y|y|y|Per-turn|yes|task-oriented dialogue|no|explicit|y|y|y|\n|mutal|[https://arxiv.org/abs/2004](https://arxiv.org/abs/2004)|6,731|4.73|6,371|y|n|y|Entier conversation|yes|open dialogue/chit chat|no|explicit|n|n|n|\n|topical-chat|[https://arxiv.org/pdf/2308.11995](https://arxiv.org/pdf/2308.11995)|9,058|21.9|150 (human)|y|n|y|Entier conversation|yes|open dialogue/chit chat|grounded context|explicit|y (limited)|n|y|\n|llm-arena (crowdsourced|[https://arxiv.org/pdf/2306.05685](https://arxiv.org/pdf/2306.05685)|33,000|1.2|33,000|n|n|y|Per-turn|yes|open dialogue/chit chat|no|reference-free|n|n|n|\n|llm-arena (annotated)|[https://arxiv.org/pdf/2306.05685](https://arxiv.org/pdf/2306.05685)|3,000|2|3,000|n|y|y|Per-turn|mixed|specialised tasks|no|reference-free|y|y|n|\n|dailydiagogue|[https://arxiv.org/pdf/1710.03957](https://arxiv.org/pdf/1710.03957)|13,118|7.9|11,118|y|n|n|Per-turn|yes|task-oriented dialogue|no|explicit|n|n|n|\n|iffeval|[https://arxiv.org/pdf/2311.07911](https://arxiv.org/pdf/2311.07911)|250|2|250|y|y|y|Entier conversation|no|specialized tasks|no|explicit|n|n|n|\n|mt-becnh|[https://arxiv.org/pdf/2306.05685](https://arxiv.org/pdf/2306.05685)|80|2|80|y|n|y|Entier conversation|no|specialized tasks|no|reference-free|n|n|n|\n|mmlu-pro|[https://arxiv.org/pdf/2406.01574](https://arxiv.org/pdf/2406.01574)|12,032|2|12,032|n|y|n|Entier conversation|no|factual QA|no|explicit|n|n|n|\n|multi-hop|[https://arxiv.org/pdf/1809.09600](https://arxiv.org/pdf/1809.09600)|7,405|2|7,405|y|n|y|Entier conversation|no|factual QA|grounded context|explicit|n|n|n|\n|Not included in the paper| | | | | | | | | | | | | | | |\n|mt-bench-101|[https://arxiv.org/pdf/2402.14762](https://arxiv.org/pdf/2402.14762)|1,388|3.03|1,388(100 human)|n|y|n|Entier conversation|yes|specialized tasks|grounded context|reference-free|n|y|n|\n|multichallenge|[https://aclanthology.org/2025.findings-acl.958.pdf](https://aclanthology.org/2025.findings-acl.958.pdf)|273|5|273|n|y|n|Entier conversation|yes|specialized tasks|grounded context|reference-free|n|n|n|\n|LoCMo|[https://arxiv.org/pdf/2402.17753](https://arxiv.org/pdf/2402.17753)|50|304.9|50(?)|n|y|n|Entier conversation|yes|specialized tasks|RAG inserted|mixed|n|n|n|\n|LongTimenose|[https://ar5iv.labs.arxiv.org/html/2203.05797](https://ar5iv.labs.arxiv.org/html/2203.05797)|27,501|16.34|200|n|y|n|Entier conversation|yes|open dialogue/chit chat|no|explicit|n|n|n|\n|COMPREHENSIVE|[https://arxiv.org/pdf/2312.15407](https://arxiv.org/pdf/2312.15407)|2,030|13.3|2,030|n|n|y/n|Entier conversation|mixed|specialized tasks|grounded context|reference-free|n|y|n|\n|COMPREHENSIVE-turn|[https://arxiv.org/pdf/2312.15407](https://arxiv.org/pdf/2312.15407)|417|7.4|3,901|n|n|y/n|Per-turn|mixed|specialized tasks|grounded context|reference-free|n|y|n|\n|chat eval|[https://arxiv.org/pdf/2308.07201](https://arxiv.org/pdf/2308.07201)|80(open QA)+60(dialouge)|2|440|n|y|y/n|Entier conversation|mixed|specialized tasks|grounded context|mixed|y|y|n|\n|persona-chat|[https://arxiv.org/pdf/1801.07243](https://arxiv.org/pdf/1801.07243)|10,907(1000 test/100 human)|14.85|15,602 (test)|y|n|y|Per-turn|yes|open dialogue/chit chat|grounded context|explicit|y|n|n|\n|Wizzard of wikipedaia|[https://arxiv.org/abs/1811.01241](https://arxiv.org/abs/1811.01241)|18,430|9.05|166,787(300 human)|y|y|y|Entier conversation|yes|context retrieval|RAG inserted|explicit|y|n|n|\n|soda|[https://aclanthology.org/2023.emnlp-main.799.pdf](https://aclanthology.org/2023.emnlp-main.799.pdf)|~1.3m|7.6|300(human evaluated sample)|n|y/n|n|Per-turn|yes|specialized tasks|grounded context|explicit|n (sample y)|n|n|\n|plain datasets (no benchmarks)| | | | | | | | | | | | | | | |\n|Ubuntu dialogue context|[https://arxiv.org/abs/1506.08909](https://arxiv.org/abs/1506.08909)|~1 million|7.7|n/a|y|n|y| |yes|open dialogue/chit chat|no| |n|n/a|n|\n|Open Subttitles|[https://aclanthology.org/I17-5003.pdf](https://aclanthology.org/I17-5003.pdf)|3.7 milion|16.75 (sentences)|n/a|n|n|n| |mixed|open dialogue/chit chat|no| | | | |\n|twitter|[https://aclanthology.org/D11-1054/](https://aclanthology.org/D11-1054/)|4,323|2|2161|y|y|n|Per-turn|mixed|open dialogue/chit chat|grounded context|explicit|y|n|n|"}}, "id": "QucN1r0128", "forum": "wCV1efgkyV", "replyto": "wCV1efgkyV", "signatures": ["ICLR.cc/2026/Conference/Submission4199/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4199/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4199/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763679223049, "cdate": 1763679223049, "tmdate": 1763679223049, "mdate": 1763679223049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UPHELD, a human-annotated benchmark for evaluating LLMs in longer conversational contexts. \nThe authors \n(i) collect human-written multi-turn dialogues and, at each turn, compare a human continuation (Option A) to a model continuation (Option B); \n(ii) annotate content equivalence (Likert 1–5), style equivalence (described as a 3-point scale), and reasonableness (binary, scored as 1 vs 5) with five annotators per instance; \n(iii) show that common reference-based metrics (ROUGE, BERTScore), embedding cosine similarity, and several “LLM-as-judge” prompts correlate only weakly to moderately with human judgments on UPHELD; and \n(iv) report that simple learned ensembles of these metrics improve correlation with human scores on UPHELD, with some cross-dataset transfer to augmented LLM-Arena and Topical-Chat verification sets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) Clear problem focus: The paper squarely targets an under-served regime—longer conversational evaluations—and provides annotation criteria"}, "weaknesses": {"value": "1) Limited literature survey on long-context / long-conversation evaluation\nThe Related Work mainly lists single-turn or short-turn task benchmarks plus a few multi-turn dialogue sets (e.g., MuTual, Topical-Chat, DailyDialog, Arena) , but it omits several directly relevant, recent efforts that study long-term conversational memory, multi-session dialogues, or long-context evaluation frameworks: for example [1]:LoCoMo, [2] Long Time No See [3] MultiChallenge. \nThe authors need to contextualize UPHELD vs all these pervious works in order to claim the contributions. \n\n2) Unclear or inconsistent aspects of the dataset creation process\n“Long” context is numerically short. The average context shown to annotators is ~560 characters (≈100–150 tokens) with 10.4 turns per dialogue—far from what the community typically calls long context today (thousands of tokens). This weakens the central claim and limits external validity for true long-context settings.\n\nScale inconsistency. Style is described as a 3-point Likert scale in §3.1.2, but Appendix A instructs raters to pick 1/3/5 (also three options, but a different framing)\n\nAuthoring & sampling specifics. The paper says Upwork professionals wrote diverse dialogues and that bias mitigation steps were taken, but leaves several concrete items unclear: number of writers, prompt archetypes/personas, topic distribution\n\n\n\n[1] LoCoMo: a very long-term conversational memory benchmark (≈300 turns, ≈9k tokens per conversation; multi-session) with tasks for recall, summarization, and QA over long dialogues. This is highly germane to the paper’s “longer conversation” scope. \narxiv.org\n[2] “Long Time No See!”: open-domain long-term conversation with human evaluation on coherence, consistency, engagingness—directly relevant axes for multi-turn dialogue. \nACL Anthology\n[3] MultiChallenge (Findings ACL 2025): a multi-turn conversation benchmark targeting realistic human–LLM challenges."}, "questions": {"value": "See Weakness for more details: \n\n1. Many relevant works discussions are missing. Can you add more references and discuss what's new in UPHELD? \n2. It's not clear to me how you prompt LLM to generate dialogue: \n\"Given our initial set of rich natural language conversations, various LLM models were then used\nto output candidate completions at every level of every conversation. Specifically, models were\npresented with conversation history up to a specific point, with the next human-written turn withheld.\nModels then generated a predicted next turn.\"  \nWhat do you mean by every level? Are you prompting LLM to generate at every turn?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KFmbRhi4Be", "forum": "wCV1efgkyV", "replyto": "wCV1efgkyV", "signatures": ["ICLR.cc/2026/Conference/Submission4199/Reviewer_DVjT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4199/Reviewer_DVjT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952531462, "cdate": 1761952531462, "tmdate": 1762917224741, "mdate": 1762917224741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new dataset of long-form task-oriented conversations by Upwork workers to assess how well models can rate the quality of these conversations. To assess, each human turn in the conversation is compared with an LLM's prediction of what the next step would be. Human then evaluated these turns in terms of semantic and style equivalence and reasonableness. Verification datasets were produced from LLM-Arena and Topical-Chat. To compare with the human rates, multiple model-based metrics (e.g., BERTScore) were also compared, as well as LLM-as-judge. The evaluated compared three larger models, as well as a Llama3.1-8B fine-tuned on some of the data. The machine evaluations were compared by correlating with human scores and also fitting a model to predict human scores from multiple metrics. The results show that the human scores are predictable from ensembles, though humans do disagree on the scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- New dataset introduced of realistic conversations\n\n- Meaningful comparison of human annotation and LLM-as-judge on modeling long conversation qualities\n\n- Multiple models and metrics compared"}, "weaknesses": {"value": "- I wasn't sure how big the dataset was in the end. In 3.1.3, the paper says that Upwork workers had 53 conversations but line 215 says ~1220 conversations are rated, which is much later. 53 conversations seems like too few to meaningfully evaluate, so this could be a significant weakness if so.\n\n- The paper's framing is about evaluating in \"longer conversation contexts\", which is an admirable goal. However, the data that is actually produced doesn't quite match this framing. First, while the data is dialog, is more task-oriented dialog, rather than chit-chat, so I'm not sure it's fully representative of \"conversation\" as we might understand human-LLM conversations. I realized that the authors' design for this is intentionally as it simplifies the scoping for conversation; I don't think this part is wrong but the paper would help from being reframed for this scope. Second, the \"longer\" conversations are still quite short, with a mean of 5 turn pairs. This is still helpful than single turns or much shorter, but I think this might still be missing some important qualities of much longer conversations that are still very realistic. Again, the data is still good, but some reframing for scope would be very helpful.\n\n- This is somewhat minor, but the models used in this paper are old now and have been surpassed by much more recent models in their performance. I don't think this should require you to generate new conversations, but for the LLM-as-judge more recent models may be much more capable judges than what are tested here. The newer Qwen3 models are quite good as is \n\n- This is more of a comment than a weakness, but while I like that the authors tried multiple judge prompts (appendix F) I am still a bit skeptical of the claim that these models are not good evaluators of longer conversations. I would have liked to see a bit more analysis here of what works and some additional variation in the prompt formatting to rule that at out as a confound."}, "questions": {"value": "- How big is the dataset in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ATnjyStud1", "forum": "wCV1efgkyV", "replyto": "wCV1efgkyV", "signatures": ["ICLR.cc/2026/Conference/Submission4199/Reviewer_5ybD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4199/Reviewer_5ybD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974353437, "cdate": 1761974353437, "tmdate": 1762917224468, "mdate": 1762917224468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}