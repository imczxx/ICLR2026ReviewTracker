{"id": "BDnOrExHmt", "number": 3941, "cdate": 1757570546944, "mdate": 1759898061851, "content": {"title": "PixNerd: Pixel Neural Field Diffusion", "abstract": "The current success of diffusion transformers are built on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To avoid these problems, researchers return to pixel space modeling but at the cost of complicated cascade pipelines and increased token complexity.\nMotivated by the simple yet effective diffusion transformer architectures on the latent space, we propose to model pixel space diffusion using a large-patch diffusion transformer and employ neural fields to decode these large patches, leading to a single-stage streamlined end-to-end solution, which we coin as pixel neural field diffusion transformer (**PixNerd**). Thanks to the efficient neural field representation in PixNerd, we achieve **1.93 FID** on ImageNet 256x256 and nearly **8x lower latency** without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieves a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.", "tldr": "pixel diffusion transformer with neural field decoder", "keywords": ["pixel diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15edae3f4c4a6c655c1a759312bff1376b5ccc1a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PixNerd, a single-stage, end-to-end diffusion transformer that operates directly in pixel space. It aims to solve the trade-off between VAE-based latent models (which brings 2-stage training and is not elegant) and traditional pixel-space models (which are computationally expensive or rely on complex cascades).\n\nThe core technical contribution is a novel decoder. Instead of using a simple linear projection on large patches (which loses detail), the diffusion transformer's final features are used to predict the weights of a small MLP (a \"neural field head\") for each patch. This patch-specific MLP then decodes the final velocity for each pixel within that patch by taking the pixel's local coordinates as input. This approach allows the model to be computationally efficient (using large patches) while retaining high-frequency detail (via the neural field decoder)."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes an alternative approach besides latent diffusion models. It has several strengths: \n\n1. Elimination of VAE: The single-stage, end-to-end pipeline avoids the pre-trained VAE, removing its associated decoding artifacts and 2-stage training.\n\n2. Efficient Pixel-Space Modeling: The proposed \"neural field head\" is an effective method for combining the computational efficiency of large-patch transformers with the high-fidelity representation of neural fields.\n\n3. Strong Benchmark Performance: Achieves a 1.93 FID on ImageNet 256x256, which is state-of-the-art for pixel-space models and 3. competitive with top-tier latent-space models. It also demonstrates superior performance on text-to-image generation tasks.\n\n4. Flexible Resolution: The coordinate-based nature of the decoder allows for training-free scaling to arbitrary resolutions at inference time, a significant practical benefit."}, "weaknesses": {"value": "1. New Artifacts: While VAE artifacts are removed, the authors note in the appendix (A.4) that the model can introduce its own \"blurry or unnatural artifacts.\"\n\n2. Architectural Complexity: The design trades the complexity of a VAE for the complexity of the neural field head. Ablation studies (4.2) show performance is sensitive to this head's depth, channel count, and normalization strategy.\n\n3. Potential Training Instability: The paper discusses the necessity of specific normalization strategies (Fig 5a, A.9) to \"ensure training stability\" and prevent \"loss spikes\" during long training runs, suggesting a sensitive training dynamic.\n\n4. The main insight of this paper is to use NeRF MLP Head to replace the traditional MLP Head. Does it induce some specific designs other than the original NeRF method?"}, "questions": {"value": "1. Why does PixNerd have lower latency than the traditional latent diffusion models? Any insights here?\n\n2. On text-to-image generation task, the parameters of PixNerd is 1.2B + 1.7B. What modules do these two parameters correspond to?\n\n3. For NeRF MLP depth ablation in figure 5 (c), does the total number of layers is the same? For PixNerd-L/16 with 4 MLP layers, whether it use 22 transformer layers or it use 20 transformer layers? Further, given the same total layer number, which is the best ratio for transformer layers and NeRF MLP layers? \n\n4. How about the latency comparison of transformer layers and NeRF MLP layers?\n\n5. Based on figure 5 (d) DCT-Basis is better than Sin/Cos coordinate-encoding. Any insights for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "brGoioSrPY", "forum": "BDnOrExHmt", "replyto": "BDnOrExHmt", "signatures": ["ICLR.cc/2026/Conference/Submission3941/Reviewer_Aqjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3941/Reviewer_Aqjg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567465697, "cdate": 1761567465697, "tmdate": 1762917104669, "mdate": 1762917104669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that current latent diffusion models are limited the VAE which introduces encoding and decoding errors. This paper proposes to directly model pixel space with large patch sizes and learn neural fields to decode large patches. The new approach avoids the two-stage training process of the latent diffusion. The paper reports state-of-the-art results on ImageNet 256x256 with a FID score of 1.93 and a nearly 8x reduction in latency compared to existing methods. The authors also demonstrate the model's effectiveness in text-to-image generation, achieving competitive scores on the GenEval and DPG benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Novel method. The proposed PixNerd architecture, which combines a large-patch diffusion transformer with a neural field decoder, is a novel and interesting approach to pixel-space diffusion modeling. \n\n- Strong results. This paper reported a competitive FID score of 1.93 on ImageNet. Moreover, the proposed framework can be applied to text-to-image generation and achieves a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.\n\n- Efficiency. This paper shows a significant latency reduction compared to both pixel diffusion and latent diffusion methods."}, "weaknesses": {"value": "- While the results on ImageNet at 256x256 resolution are quite competitive (see Table 1), the results at 512 resolution are not so convincing (see Table 6). The authors are encouraged to explain the performance degradation. The authors are also encouraged to provide comparisons at a even higher resolution like 1024 or 768.\n\n-Unclear latency comparison. The abstract mentions an 8x latency improvement but does not specify which models were used for comparison. According to Table 1, PixNerd did not achieve 8x latency improvement compared to latent diffusion methods."}, "questions": {"value": "How does the performance of PixNerd, in terms of both image quality and latency, scale as the image resolution increases (e.g., 512x512, 1024x1024)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vsbVd6CQ9C", "forum": "BDnOrExHmt", "replyto": "BDnOrExHmt", "signatures": ["ICLR.cc/2026/Conference/Submission3941/Reviewer_vPnv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3941/Reviewer_vPnv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121510073, "cdate": 1762121510073, "tmdate": 1762917104304, "mdate": 1762917104304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PixNerd, a novel end-to-end pixel-space diffusion transformer that replaces the traditional linear projection head with a neural field decoder. Its main goal is to bring the efficiency and visual fidelity of latent diffusion transformers into the raw pixel domain, without relying on VAEs or multi-scale cascades that introduce complexity and latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. Originality and Significance\n\n- I highly commend the paper’s central motivation and direction. While most recent efforts focus on reducing computational cost by operating entirely in latent space, this work takes the opposite yet equally important perspective to explore how to lower cost directly in pixel space. This inversion of the conventional design philosophy is both insightful and original, addressing a long-standing challenge in diffusion modeling.\n\nS2. Experimental Quality and Clarity\n\n- I also deeply appreciate the thorough and well-executed ablation studies. The paper demonstrates an exceptional level of experimental rigor, allowing readers to form a fair and comprehensive understanding of the proposed method’s behavior. In my view, this kind of meticulous empirical investigation exemplifies what a well-written paper should strive for."}, "weaknesses": {"value": "W1. Justification for architectural necessity.\n\n- Given that recent advances in diffusion distillation (e.g., one-step or few-step distilled diffusion models) can achieve nearly identical performance to full diffusion models with drastically reduced sampling steps, it is unclear why PixNerd needs to exist as a separate model. Could the authors clarify whether PixNerd itself can serve as a teacher model for distillation? If not, then PixNerd would likely exhibit much higher latency compared to diffusion + distillation pipelines. Moreover, the claimed 8× speedup still appears slower than VAE + diffusion pipelines, which challenges the practical advantage of the method. If the authors could convincingly argue in this point, then I will flip my assessment.\n\nW2. Missing baselines and quantitative comparison.\n\n- In Table 1, it would significantly strengthen the evaluation to include representative generative baselines such as StyleGAN [1], CDM [2], Simple Diffusion [3], VDM++ [4], and PaGoDA [5].\nIn particular, the paper should provide quantitative comparisons with one-step models (either GAN or diffusion + distillation) and explicitly report latency differences to position PixNerd more clearly within the current landscape of efficient generative models.\n\n[1] Sauer, Axel, Katja Schwarz, and Andreas Geiger. \"Stylegan-xl: Scaling stylegan to large diverse datasets.\" ACM SIGGRAPH 2022 conference proceedings. 2022.\n[2] Ho, Jonathan, et al. \"Cascaded diffusion models for high fidelity image generation.\" Journal of Machine Learning Research 23.47 (2022): 1-33.\n[3] Hoogeboom, Emiel, Jonathan Heek, and Tim Salimans. \"simple diffusion: End-to-end diffusion for high resolution images.\" International Conference on Machine Learning. PMLR, 2023.\n[4] Kingma, Diederik, and Ruiqi Gao. \"Understanding diffusion objectives as the elbo with simple data augmentation.\" Advances in Neural Information Processing Systems 36 (2023): 65484-65516.\n[5] Kim, Dongjun, et al. \"Pagoda: Progressive growing of a one-step generator from a low-resolution diffusion teacher.\" Advances in Neural Information Processing Systems 37 (2024): 19167-19208."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ToGt9gdXbE", "forum": "BDnOrExHmt", "replyto": "BDnOrExHmt", "signatures": ["ICLR.cc/2026/Conference/Submission3941/Reviewer_iKuE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3941/Reviewer_iKuE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762170685634, "cdate": 1762170685634, "tmdate": 1762917104071, "mdate": 1762917104071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PixNerd, a end-to-end pixel-space diffusion model that matches the performance of SOTA latent diffusion model without relying on pretrained VAE. PixNerd achieves this by using implicit neural field to replace the traditional decoding head of DiT. PixNerd achieves competitive performance on ImageNet and text-to-image generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written with clear structure. \n- The paper present comprehensive comparisons against current SOTA models both qualitatively and quantitatively. PixNerd matches or exceeds the performance of comparable methods on ImageNet and text-to-image tasks\n- The ablation studies are conducted systematically to evaluate each design choice."}, "weaknesses": {"value": "- The training memory usage is almost doubled compared to that of latent diffusion counter part. \n- PixNerd's performance at higher resolutions (512×512) does not scale as strongly as at 256×256. For example, PixNerd is better than SiT-XL on ImageNet256 but falls behind on ImageNet512. Does this imply that the gains from the neural field head diminish at higher resolutions?\n- Minors:\n\t- The citation of Rectified flow seems missing. \n\t- Table 4 should specify that the comparison is reported on ImageNet256 for clarity."}, "questions": {"value": "- What is the main source of additional training memory and how can this be optimized? \n- Why is DCT-Basis encoding better? What about the other popular alternatives like RoPE?\n- I notice that PixNerd (512x512) is finetuned from PixNerd(256x256) and its performance on ImageNet512 is not as impressive as PixNerd(256x256). What's the reason for not training PixNerd (512x512) from scratch? \n- Is PixNerd compatible with representation alignment techniques like REPA? Since it operates directly in pixel space, would this alignment even be more effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uJQmmx7fWI", "forum": "BDnOrExHmt", "replyto": "BDnOrExHmt", "signatures": ["ICLR.cc/2026/Conference/Submission3941/Reviewer_AqD9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3941/Reviewer_AqD9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219389887, "cdate": 1762219389887, "tmdate": 1762917103902, "mdate": 1762917103902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}