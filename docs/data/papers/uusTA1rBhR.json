{"id": "uusTA1rBhR", "number": 12315, "cdate": 1758207029452, "mdate": 1759897517975, "content": {"title": "Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling", "abstract": "Safe and feasible trajectory planning is critical for real-world autonomous driving systems.\nHowever, existing learning-based planners rely heavily on expert demonstrations, which not only lack explicit safety awareness but also risk inheriting undesirable behaviors such as speeding from suboptimal human driving data.\nInspired by the success of large language models, we propose Plan-R1, a two-stage trajectory planning framework that decouples principle alignment from behavior learning.\nIn the first stage, a general trajectory predictor is pre-trained on expert data to capture diverse, human-like driving behaviors.\nIn the second stage, the model is fine-tuned with rule-based rewards using Group Relative Policy Optimization (GRPO), explicitly aligning ego planning with principles such as safety, comfort, and traffic rule compliance.\nThis two-stage paradigm retains human-like behaviors while enhancing safety awareness and discarding undesirable patterns from demonstrations.\nFurthermore, we identify a key limitation of directly applying GRPO to planning: group-wise normalization erases cross-group scale differences, causing rare, high-variance safety-violation groups to have similar advantages as abundant low-variance safe groups, thereby  suppressing optimization for safety-critical objectives.\nTo address this, we propose Variance-Decoupled GRPO (VD-GRPO), which replaces normalization with centering and fixed scaling to preserve absolute reward magnitudes, ensuring that safety-critical objectives remain dominant throughout training.\nExperiments on the nuPlan benchmark demonstrate that Plan-R1 significantly improves planning safety and feasibility, achieving state-of-the-art performance, particularly in realistic reactive settings.", "tldr": "We propose Plan-R1, a two-stage framework that decouples planning principle alignment from behavior learning to overcome the limitations of expert data. With VD-GRPO to preserve safety-critical signals, Plan-R1 achieves SOTA results on nuPlan.", "keywords": ["Trajectory Planning", "Reinforcement Learning", "Autonomous Driving"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2fd29291d2bae01dc76c59a50faf2aed8cb79c21.pdf", "supplementary_material": "/attachment/3ac69d24b7d69818885d85bbb1f2a3bf40e7269b.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies how to use RL with rule-based rewards like GRPO to align ego planning with principles for trajectory planning models. Overall, the authors utilize a two-stage paradigm: pretraining and post-training to align with rules. They also modify GRPO into VD-GRPO, so as to remain safety-critical objectives remain dominant. Experiments are conducted on nuPlan dataset, and advantage against other previous methods are shown."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors study the interesting and important problem on whether recent advances in post-training language model, or basically GRPO, can help improve trajectory planning models. Their observations and modifications on GRPO provide clear and inspiring answer to this question. Especially, they conduct ablation study, analyzing the dominating term of advantages, and show that during GRPO training the unsafe samples are not assigned enough importance.\n2. Experiments on nuPlan prove the validity of their proposed method.\n3. The dual-model rollout, which utilizes frozen pre-trained model as 'world model' for other driving agents, enables the trained model to interact with other driving agents. This is also an interesting design choice.\nOverall, the discovery about GRPO is interesting, and related improvement method, i.e. VD-GRPO, is reasonable and shown to be effective."}, "weaknesses": {"value": "1. Experiments are mainly conducted on nuPlan, not including other datasets. Given that said, experiments are thorough on nuPlan."}, "questions": {"value": "Have you tried to use your fine-tuned model as 'world model' for other driving agents? Though I indeed recognize that this might not give too much performance uplift."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1fsazLLmPo", "forum": "uusTA1rBhR", "replyto": "uusTA1rBhR", "signatures": ["ICLR.cc/2026/Conference/Submission12315/Reviewer_fE6E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12315/Reviewer_fE6E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760674087424, "cdate": 1760674087424, "tmdate": 1762923243444, "mdate": 1762923243444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Plan-R1, a two-stage trajectory planning framework for autonomous driving. A motion predictor is first pre-trained to capture diverse human-like driving behaviors, and then fine-tuned with rule-based reinforcement learning to align with safety and planning principles. The authors further identify a critical limitation in GRPO's normalization for safety-critical domains and introduce Variance-Decoupled GRPO (VD-GRPO) to preserve absolute reward magnitudes. Combined with a dual-model interactive rollout design, Plan-R1 achieves state-of-the-art performance on the nuPlan benchmark, especially under reactive closed-loop evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors clearly diagnose how per-group variance normalization in GRPO suppresses rare but safety-critical violations, and propose a principled solution that consistently enhances safety optimization without sacrificing secondary objectives.\n\n2. Using a frozen world model for surrounding-agent responses ensures interaction-aware rollouts while preventing instability in non-ego behaviors. This design proves crucial for strong performance in reactive scenarios.\n\n3. Plan-R1 achieves new state-of-the-art performance in both non-reactive and reactive nuPlan evaluations, with significant gains in safety metrics such as collisions and drivable area compliance. The ablations convincingly support the contributions of both VD-GRPO and the dual-model rollout."}, "weaknesses": {"value": "1. Limited analysis of world model reliability when ego deviates from expert behavior. The frozen world model is assumed to remain accurate when the ego policy explores beyond regions well-covered by expert data. However, there is no case study or quantitative analysis showing how the world model behaves under large ego deviations or unusual interaction patterns, which could introduce compounding errors during RL fine-tuning.\n\n2. Tokenization design lacks ablation on discretization choices. The trajectory discretization process (e.g.spatial quantization granularity, or temporal segmentation interval) may significantly influence expressiveness and planner performance. The paper does not provide ablations or analysis on these design factors."}, "questions": {"value": "Same as weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rvrM7I32Aq", "forum": "uusTA1rBhR", "replyto": "uusTA1rBhR", "signatures": ["ICLR.cc/2026/Conference/Submission12315/Reviewer_5bV5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12315/Reviewer_5bV5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473168987, "cdate": 1761473168987, "tmdate": 1762923243126, "mdate": 1762923243126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Plan-R1, a two-stage framework that formulates trajectory planning as language modeling. The idea of decoupling behavior learning from principle alignment, inspired by LLM training paradigms, is both novel and elegant. The proposed Variance-Decoupled GRPO (VD-GRPO) effectively addresses the limitation of standard GRPO by preserving safety-critical gradients, leading to substantial performance gains on the nuPlan benchmark. The experimental section is comprehensive, including detailed ablations and clear visualizations, which convincingly demonstrate the method’s safety and feasibility improvements.\n\nHowever, the theoretical justification of VD-GRPO (especially the fixed scaling constant) could be further strengthened, and an additional evaluation on another dataset (e.g., CARLA) would help validate generalization. Overall, this is a well-written and technically sound paper with strong empirical results and a creative conceptual contribution."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces a novel and well-motivated idea of framing trajectory planning as language modeling through the two-stage Plan-R1 framework. The decoupling of behavior learning and principle alignment is conceptually elegant and practically effective. The proposed VD-GRPO clearly addresses a key limitation in standard GRPO, preserving safety-critical gradients and improving rare-event optimization. Experiments on the nuPlan benchmark are extensive and convincing, with strong gains in both non-reactive and reactive settings. The writing is clear, the figures are informative, and the work demonstrates a high level of technical maturity."}, "weaknesses": {"value": "While the empirical results are strong, the theoretical justification of VD-GRPO remains limited. The fixed scaling constant is treated as a hyperparameter without principled analysis of its effect on convergence or stability. The dual-model setting, with a frozen world model, may introduce distribution drift in long-term interactions. Moreover, evaluation is restricted to nuPlan; results on other benchmarks such as CARLA or Waymo would strengthen claims of generalization."}, "questions": {"value": "- Have the authors analyzed the learned motion tokens to see if they correspond to interpretable motion primitives or semantic driving actions?\n\n- How would Plan-R1 perform under partial observability or sensor noise conditions compared to diffusion-based planners?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ueWB0bG6tg", "forum": "uusTA1rBhR", "replyto": "uusTA1rBhR", "signatures": ["ICLR.cc/2026/Conference/Submission12315/Reviewer_eJWp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12315/Reviewer_eJWp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793830734, "cdate": 1761793830734, "tmdate": 1762923242686, "mdate": 1762923242686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a two-stage framework called Plan-R1 for safe, feasible trajectory planning in autonomous driving. In the first stage, the model learns diverse, human-like driving behaviors via pretraining on expert data. In the second stage, the model is fine-tuned with rule-based reinforcement learning (RL) to align trajectory planning with explicit principles such as safety, comfort, and traffic rules. To address the limitation of standard GRPO (Group Relative Policy Optimization)—which can dilute safety-critical signals in multi-objective planning—Plan-R1 proposes Variance-Decoupled GRPO (VD-GRPO), which preserves absolute reward magnitudes to ensure the dominance of safety objectives. Experiments show that Plan-R1 significantly improves planning safety and feasibility on the nuPlan benchmark, especially in challenging reactive settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "• Clear two-stage framework: Plan-R1 decouples behavior learning from principle alignment, retaining human-like behavior while enhancing safety awareness and removing undesirable patterns present in expert data (p.1, lines 054–058).  \n• Novel VD-GRPO: In response to standard GRPO’s limitations, VD-GRPO replaces in-group normalization with centering and fixed scaling, effectively preventing rare but critical safety-violation signals from being washed out and ensuring safety-critical objectives dominate training (p.2, lines 075–083; p.6, lines 295–311).  \n• Rule-based rewards: Instead of relying on human preference data, Plan-R1 uses rule rewards that offer consistent, unbiased supervision, avoiding bias and improving scalability and reliability (p.3, lines 122–127; p.5, lines 266–269).  \n• Dual-model design: During RL fine-tuning, a trainable ego planner explores alternative decisions, while a frozen copy of the pretrained model acts as a reactive world model to predict other agents’ responses, enabling stable, interaction-aware joint prediction (p.2, lines 066–069; p.5, lines 256–265).  \n• Significant performance gains: On the nuPlan benchmark, Plan-R1 achieves state-of-the-art performance in both non-reactive and reactive settings, notably surpassing Diffusion Planner by +4.89, +7.98, and +7.11 points in the reactive setting, substantially improving safety and feasibility (p.7, Table 1, lines 370–377).  \n• Clear qualitative results: Figures 2 and 3 clearly show how Plan-R1 avoids undesirable behaviors common in pretrained or expert-data-only models, such as speeding, off-road driving, and collisions."}, "weaknesses": {"value": "• Definition of pivots: The paper states that trajectories are discretized into motion tokens but does not delve into how these “pivot” points are chosen or defined, nor whether such discretization might miss key kinematic or geometric features (p.5, lines 217–223).  \n• Detailed analysis of VD-GRPO: Although VD-GRPO is proposed, there is limited theoretical analysis of how its parameters (e.g., the fixed scaling constant c) affect training dynamics; the discussion is mostly empirical (p.9, lines 453–460; p.15, lines 799–807).  \n• Missing user-uploaded images: The text mentions “Image generation: enabled,” but no actual images are provided, making it impossible to assess the quality or completeness of the figures referenced in the paper."}, "questions": {"value": "1) In VD-GRPO, the choice of the fixed scaling constant (c) is empirical. Is there a more theoretical way to determine an optimal c, or is it highly task- and reward-design-dependent? (See p.15, lines 799–807.)  \n2) The paper notes that VD-GRPO “replaces in-group normalization with centering and fixed scaling.” Could you provide more concrete implementation details of this “fixed scaling,” and how it differs mathematically from traditional normalization methods? (See p.6, lines 302–303.)  \n3) In the ablation study of §4.3, Table 2 shows that VD-GRPO substantially improves the collision metric (+3.45) but has no effect on the comfort metric (+0.00). Could you explain in detail why VD-GRPO impacts some metrics more than others, and whether this relates to the high priority assigned to safety objectives in the reward function? (See p.8, Table 2; p.9, lines 453–458.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "R1ii7rg3UU", "forum": "uusTA1rBhR", "replyto": "uusTA1rBhR", "signatures": ["ICLR.cc/2026/Conference/Submission12315/Reviewer_nc4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12315/Reviewer_nc4d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843905904, "cdate": 1761843905904, "tmdate": 1762923241653, "mdate": 1762923241653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}