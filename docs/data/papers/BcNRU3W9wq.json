{"id": "BcNRU3W9wq", "number": 14274, "cdate": 1758231767118, "mdate": 1759897379598, "content": {"title": "GPAR: Gaussian process based association rule mining", "abstract": "We introduce GPAR, a Gaussian process–based framework for association‐rule mining. Unlike traditional AR mining methods which treat items as atomic symbols, GPAR represents each item by a feature vector and fits a Gaussian process to its latent membership variable using transaction data. Uncertainty estimates are obtained by sampling from the GP posterior and marginalizing over irrelevant latent dimensions; unobserved itemsets can be inferred without re-processing. Through experiments with various kernel choices, GPAR outperforms classic AR‐mining algorithms in identifying small or rare itemsets, quantifying and propagating uncertainty, incorporating prior knowledge to capture complex patterns, and inferring new frequent itemsets without incurring additional computational cost.", "tldr": "We introduce a new association‐rule mining method using Gaussian process, improving complex pattern identification with prior knowledge, uncertainty quantification, custom kernel design, and reduced cost for inferring new frequent itemsets.", "keywords": ["association rule", "Gaussian process", "data mining"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7d360b9916dc3ae2ef86288fde801ad81897a0b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes GPAR, which models latent co-occurrence in transactional data using Gaussian Processes over item feature vectors and kernels. Frequent itemsets and rules are obtained via Monte Carlo estimates of joint occurrence probabilities. The method can generate rules for unseen items without retraining by augmenting the covariance. Overall complexity is dominated by $O(2^M(M^3+SM^2))$, so the approach targets small $M$. Experiments compare against Apriori / FP-Growth / Eclat on two synthetic datasets and one real dataset."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Probabilistic modeling & uncertainty.** GP offers principled uncertainty quantification and richer relational modeling than frequency-based methods.\n- **Encodable priors.** Kernels allow injecting domain priors (e.g., complement/substitute relations) that classic ARM struggles to express.\n- **New-item inference.** Can generate rules for unseen items without retraining—useful for dynamic catalogs/cold starts.\n- **Kernel variety.** Beyond RBF, the paper discusses shifted-RBF, NTK, and NN kernels to capture nuanced non-linear patterns."}, "weaknesses": {"value": "1. **No feature-level ablations / prior sensitivity.** Heavy dependence on feature mappings and kernel priors, yet experiments vary only kernels/thresholds; no ablation or sensitivity analysis on feature subsets or “good vs. bad” priors.\n2. **Shifted-RBF non-PSD & semantic shift risk.** Generally non-PSD, requiring eigenvalue clipping; no quantification of how this projection alters relational semantics or spectrum—risk of “converges but mischaracterizes relations.”\n3. **Scalability.** Complexity confines applicability to small $M$ (e.g., $M \\le 15$), limiting use in large, high-dimensional domains.\n4. **NTK evidence is thin; extrapolation risk.** NTK is evaluated only on a single synthetic dataset with mainly count/time metrics and no quality/robustness checks (cross-distribution validation, feature shuffling, counterfactuals). It remains unclear whether “new relations” are genuine or artifacts from kernel-prior extrapolation."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R2jldfUC8y", "forum": "BcNRU3W9wq", "replyto": "BcNRU3W9wq", "signatures": ["ICLR.cc/2026/Conference/Submission14274/Reviewer_jZvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14274/Reviewer_jZvj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761275295469, "cdate": 1761275295469, "tmdate": 1762924723777, "mdate": 1762924723777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GPAR, a Gaussian Process alternative to frequency-based association rule mining. Each item is represented by a feature vector. A GP prior with kernel k(x_i, x_j) over item-level latent variables z is fitted on transactions. The probability that an itemset I occurs is then approximated as a Monte-Carlo estimate of the probability that all components of the latent vector z_I are positive.\nDifferent kernels, e.g., RBF, shifted RBF, NTK, and an erf neural-net kernel, are explored. The authors claim GPAR can: (1) quantify uncertainty; (2) generalize to unseen items via kernel conditioning; and (3) infer new frequent itemsets without re-processing.\nExperiments on two synthetic sets (10–15 items) and a UK accident subset (39 items) compare GPAR to Apriori, FP-Growth, and Eclat wrt runtime, memory, number of itemsets and rules, and top-rules tables."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The goal is ambitious. Feature-aware, uncertainty-quantified, new-item ARM in a unified model.\n2. The discussion on Kernel choices is interesting. NTK and erf kernels are reasonable PSD choices. \n3. The work is quite transparent about scalability limits and includes complexity accounting."}, "weaknesses": {"value": "1. The GP fitting approach is quite a stretch, and the benefits seem negligible.\n2. I am not quite sure how the GP fitting objective would work for binary data. How do you get the posterior conditioning in rule probabilities? The likelihood term does not seem valid for binary data. \n3. It is not clear why confidence values get larger than 1. This should not happen.\n4. It is not clear at all how new items are embedded. How do you embed new items without recomputing a global eigendecomposition? \n5. The fairness of the comparison in the evaluation is not clear. Since GPAR can produce rules with zero observed support, how do you make sure the metrics in the comparison against competitors are fair? \n6. Exponential enumeration with Monte Carlo estimation per itemset is very costly. There is no pruning or scalable inference strategy.\n7. There are multiple internal inconsistencies (runtimes, duplicated content)."}, "questions": {"value": "See comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gAohvYUwJ7", "forum": "BcNRU3W9wq", "replyto": "BcNRU3W9wq", "signatures": ["ICLR.cc/2026/Conference/Submission14274/Reviewer_4x9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14274/Reviewer_4x9V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417623195, "cdate": 1761417623195, "tmdate": 1762924723392, "mdate": 1762924723392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method based on Gaussian processes (GPs) for association rule mining, identifying from a set of transactions frequently occurring itemsets and understanding the relation in them, namely, which items precede the other ones. The goal is to construct such rules that capture meaningful associations. The authors frame this problem as latent variable models in which a GP prior is imposed on the latent variables, the inputs are feature vectors per item, and the observations are the transactions. The authors inspect several kernel functions and compare their method to classical algorithms."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Novel formulation of association rule mining using Gaussian processes.\n* In terms of the number of rules and the number of frequent itemsets generated, the method seems to be more flexible. \n* The method seems to uncover meaningful and complex patterns."}, "weaknesses": {"value": "In my opinion, this paper is not ready and suitable for publication at ICLR. First, the paper deals with pattern discovery in transactional datasets, which is more suitable for other venues, in my opinion, but I leave that decision to the AC. Second, the novelty of this paper, in my opinion, is limited to applying a GP to a new setup. Third, there are formatting issues across the paper, such as incorrect citation style, broken references for citations (which appear as ?, e.g., in lines 584 and 627), the appendix header appears as a section header, and the same two paragraphs appear in two places (line 310 and line 337).\nI will make a few comments regarding the method and experiments that seem most crucial to me.\nRegarding the method:\n* It seems that the authors addressed this problem as a multi-task problem sharing the same input, where each transaction is a different task and the transactions are independent. I infer that from Eq. 2. Is that right? No explanation or reference is given to that, and to me, this modeling choice seems odd. Why assume independence between the transactions? \n* The authors implicitly assume a Gaussian likelihood, which is also not justified, as the observations are vectors containing elements in the set {0,1}. Can the authors please clarify that point?\n\nRegarding the experiments:\n* The datasets are too simple for a non-theory paper (two syntactic ones and a real one).\n* The evaluation metrics are also not clear. How do I know which method is better? Why the lift metric isn't presented for all methods? Is it better or worse that a method generates more rules? How does one know if the rules are good or not?"}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QLjqzP9p67", "forum": "BcNRU3W9wq", "replyto": "BcNRU3W9wq", "signatures": ["ICLR.cc/2026/Conference/Submission14274/Reviewer_tMbe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14274/Reviewer_tMbe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576308237, "cdate": 1761576308237, "tmdate": 1762924723001, "mdate": 1762924723001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on Association rule mining (ARM) task, which aims to discover relationships between items in transactional datasets. Traditional ARM algorithms such as Apriori, FP-Growth, and Eclat face critical challenges: poor scalability with high-dimensional or dense datasets due to exponential growth of possible itemsets, failure to capture probabilistic dependencies between items, and inability to leverage additional item features to model complex relationships. This work proposes Gaussian Process Association Rules (GPAR), a novel probabilistic framework for ARM that leverages Gaussian Processes (GPs) to model joint probabilities of itemsets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel probabilistic framework (GPAR) that extends traditional association rule mining using Gaussian Processes, offering a principled approach to uncertainty quantification.\nIncorporating item feature vectors and custom kernel functions (RBF, shifted RBF, NTK, NN kernel) enables GPAR to capture richer and more complex relationships between items than frequency-based methods.\nExperimental results on both synthetic and real-world datasets show clear improvements in identifying rare or subtle itemsets and probabilistic rule discovery.\nThe paper is theoretically grounded, providing detailed mathematical formulations and clear comparisons with classic ARM algorithms."}, "weaknesses": {"value": "Interpretability of probabilistic rules is lower than traditional if–then rules, making it harder for practitioners to apply results intuitively.\n\nThe approach relies heavily on well-defined feature representations and careful kernel tuning, which may limit general applicability."}, "questions": {"value": "However, there are some doubts in understanding the technical details:\n1/ In equation(3) and appendix D, what's the difference of $\\mathcal{N}(\\mathbf{z}_I; \\mathbf{0}, K_I)$ and $\\mathcal{N}(\\mathbf{z}_I \\mid \\mathbf{0}, K_I)$.\n2/ In section 5, Why choose the three kernel(shifted RBF, neural tangent kernel (NTK), and a neural network kernel with erf activation)?  There are no  comparisons with other kernel functions suitable for association rule scenarios (such as Matern kernel).\n3/ The complexity bottleneck has not been addressed,  it has the same complexity problem as traditional methods.\n4/ In section 7, author mention that GPAR is impractical for large M (e.g.M > 15). Why is it 15? Is there any basis for this?\n5/ The convergence verification of Monte Carlo sampling, supplementing the \"sampling number S- estimation error\" curve (such as testing S=50/100/200/500 on Synthetic 1), proves the rationality of S=100.      \n\nRegarding the experiment part, there are some doubts:\n1/ In related work, author mention the Bayesian association rule mining, but why not compare it with BAR? Indeed, the methods of comparison are all very outdated and there are no mainstream methods of recent years for comparison.\n2/ In related work, author mention the Huwel & Beecks (2023) combined Apriori with Gaussian Processes (GPs) ,why not make a comparison with it\n3/ Why not add some comparisons with other kernel functions?\n4/ The experiment only counted \"runtime performance, memory usage, number of frequent itemsets and number of rules generated\", without evaluating the practicality and accuracy of the rules.\n5/ It is proposed that GPAR can achieve rule reasoning of new items by extending the core matrix without retraining, but this function has not been verified through experiments.\n\tThere are suggestions for improving the writing the paper:\n1/ The methods of literature research are too outdated and lack persuasiveness.\n2/ The relevant work research is insufficient and there is a lack of the latest related work.\n3/ On line 246,356,584,627,634,701,1097,1133, appear the garbled characters Question marks and exclamation marks.\n4/ Try to bold the parts with the best performance in all the tables.\n5/ In section 6, There are two repeated paragraphs. I don't know what the meaning is.(line 310~323,337~351).\n6/ It would be best to draw an algorithm framework diagram."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7qDvviha4s", "forum": "BcNRU3W9wq", "replyto": "BcNRU3W9wq", "signatures": ["ICLR.cc/2026/Conference/Submission14274/Reviewer_vm6W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14274/Reviewer_vm6W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877416129, "cdate": 1761877416129, "tmdate": 1762924722613, "mdate": 1762924722613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}