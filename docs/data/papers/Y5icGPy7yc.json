{"id": "Y5icGPy7yc", "number": 7330, "cdate": 1758016271824, "mdate": 1759897859572, "content": {"title": "GEDAN: Learning the Edit Costs for Graph Edit Distance", "abstract": "Graph Edit Distance (GED) is defined as the minimum cost transformation of one graph into another and is a widely adopted metric for measuring the dissimilarity between graphs. The major problem of GED is that its computation is NP-hard, which has in turn led to the development of various approximation methods, including approaches based on neural networks (NN). However, most NN methods assume a unit cost for edit operations -- a restrictive and often unrealistic simplification, since topological and functional distances rarely coincide in real-world data. In this paper, we propose a fully end-to-end Graph Neural Network framework for learning the edit costs for GED, at a fine-grained level, aligning topological and task-specific similarity. Our method combines an unsupervised self-organizing mechanism for GED approximation with a Generalized Additive Model that flexibly learns contextualized edit costs. Experiments demonstrate that our approach overcomes the limitations of non–end-to-end methods, yielding directly interpretable graph matchings, uncovering meaningful structures in complex graphs, and showing strong applicability to domains such as molecular analysis.", "tldr": "An interpretable GNN that learns context-aware edit costs for Graph Edit Distance, aligning topological and functional graph similarities.", "keywords": ["Graph Edit Distance", "Graph Neural Networks", "Interpretable Deep Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18e4937af8685c2c5061ee79a9ffdd374d6fadba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the Graph Edit Distance (GED) problem, which is a classical but computationally intractable method for measuring graph dissimilarity. The authors argue that existing neural approximations assume fixed or unit edit costs, which fail to reflect meaningful, task-specific similarities (e.g., in molecular graphs where small topological changes can drastically affect function).\n\nTo overcome this, they propose GEDAN, a fully end-to-end differentiable GNN framework that jointly learns node/edge edit costs while approximating GED."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s key innovation lies in learning fine-grained edit costs rather than assuming fixed values. This bridges structural similarity and semantic/functional similarity.\n2. This paper uses the following neural network to solve the problem: GNNs, GAMs, and Gumbel-Sinkhorn. In detail, GIN-based message passing for expressive node embeddings, Generalized Additive Models (GAMs) for interpretability and modular cost composition, and Gumbel-Sinkhorn networks for differentiable matching demonstrates an elegant synthesis of modern deep-graph and probabilistic-optimization techniques.\n3. Extensive experimental results: this paper contains lots of experiments, such as GED approximation quality, Effect of learned vs. fixed edit costs on downstream molecular prediction etc."}, "weaknesses": {"value": "1. The main limitation is that this paper cannot scale up. The experiments shown in this paper mainly focus on graphs with <= 128 nodes.\n2. The paper largely focuses on empirical and architectural innovation. A deeper theoretical analysis of learned costs, e.g., proving monotonicity of learned GED w.r.t. functional distance (Eq. 1) or bounding approximation error, would strengthen the contribution.\n3. The multi-component training (contrastive + supervised losses, multiple matrices, pre-training of Gumbel-Sinkhorn) may make optimization unstable or sensitive to hyperparameters."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "czMJsAJbLO", "forum": "Y5icGPy7yc", "replyto": "Y5icGPy7yc", "signatures": ["ICLR.cc/2026/Conference/Submission7330/Reviewer_iLkT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7330/Reviewer_iLkT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760908129985, "cdate": 1760908129985, "tmdate": 1762919448378, "mdate": 1762919448378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a novel method to optimize the edit costs in the graph edit distance, in order to align it with the functional dissimilarity between graphs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses a difficult topic, which is the estimation of the edit costs in GED for a given task. The proposed method relies on recent advances in the literature, such as Jain et al. (2024), and provides recent advances by exploring the Gumbel-Sinkhorn that allows approximation of permutation matrices.\n\nThe appendices provide essential information to better understand the proposed method, with extensive experimental analysis."}, "weaknesses": {"value": "The title of the paper is too general. There are many other methods that learn the edit costs for graph edit distance. The authors are missing some of the related literature on this topic.\n\nThe proposed method relies on a pre-trained Gumbel-Sinkhorn network to approximate the Linear Sum Assignment Problem (LSAP). While this is an interesting approach, it is also a major weakness because one needs to pre-train the network. This leads to several issues, as described in the following.\n\nThe resulting method depends on the pre-training and its biases, such as the used method which preferentially selects node pair representations with minimum distance. Other choices would lead to different biases. \n\nThe pre-training relies on a pre-defined size (e.g. 64 x 64) and therefore is very restricted once the size is fixed. Padding dummy nodes to get to the same size, as proposed in the paper, is not a clever strategy, as it highly increased the computational complexity. Moreover, binary masks are used at the end, also increasing the computational cost.\n\nThe computational complexity of the method is a major weakness. There is room for improvement.\n\nThe appendices provide essential information to better understand the proposed method and the conducted choices. However, it turns out that some of the choices are not well justified or explained in the main body. For instance, it turns out that the authors are not using the original Gumbel-Sinkhorn network, but a modified version. It would have been relevant to clearly specify this in the main paper and motivate this choice, including an ablation study."}, "questions": {"value": "No further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "b5rawCt3xG", "forum": "Y5icGPy7yc", "replyto": "Y5icGPy7yc", "signatures": ["ICLR.cc/2026/Conference/Submission7330/Reviewer_hDhH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7330/Reviewer_hDhH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687275637, "cdate": 1761687275637, "tmdate": 1762919447862, "mdate": 1762919447862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GEDAN, a GNN framework for approximating the Graph Edit Distance (GED). The main contribution is a novel, end-to-end differentiable method for learning fine-grained, context-aware edit costs. This learned cost function aims to align the traditional topological GED metric with task-specific functional similarity, which is often misaligned in real-world data like molecules. The model uses a GIN for multi-scale node embeddings, formulates GED as a matching problem solved via a pre-trained Gumbel-Sinkhorn network, and introduces an auxiliary MLP to learn the edit costs. The paper presents an unsupervised mode to approximate fixed-cost GED and a supervised mode to learn costs for downstream functional tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper addresses a well-known and significant limitation of standard GED: the mismatch between topological distance (using fixed/unit costs) and functional similarity, particularly in domains like computational chemistry.\n(2) The core idea of building an end-to-end, differentiable framework to learn these costs is novel and valuable, as it allows GED to be optimized directly for downstream tasks.\n(3) The model offers a path to interpretability by analyzing the learned edit costs."}, "weaknesses": {"value": "(1) Hard limit of 128 nodes due to Gumbel-Sinkhorn complexity is critically restrictive.\n(2) Table 3 shows RMSE of 2509.84 (No-PT) vs 3.54 (PT) - the method completely fails without careful initialization. This is reasonable, but is a Gumbel–Sinkhorn trained on random matrices optimal for real GNN-induced cost matrices?\n(3) Domain generality not fully shown. All downstream is molecular / chem. That is fine, but GED gets used in scene graphs, program graphs, point-cloud graphs, document structure graphs, knowledge graphs, etc. Even a small “OGB-small” or “program-AST” style experiment would make the “task-aligned edit costs” story more general."}, "questions": {"value": "(1) Could you report wall-clock time and peak GPU memory for various methods tested?\n(2) Why GAM specifically? Could you show that a simpler learned scalar-weighted sum performs worse?\n(3) You mention 128 as the practical cap. Suppose I have 300-node program graphs or scene graphs — what is your recommendation approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t07Gc5dXmf", "forum": "Y5icGPy7yc", "replyto": "Y5icGPy7yc", "signatures": ["ICLR.cc/2026/Conference/Submission7330/Reviewer_nDHo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7330/Reviewer_nDHo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869764851, "cdate": 1761869764851, "tmdate": 1762919447289, "mdate": 1762919447289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to learn context-aware edit costs for Graph Edit Distance (GED) computation. The method builds on GNN-based node representations and incorporates a pre-trained Gumbel–Sinkhorn network to approximate graph node correspondences, enabling node-specific, context-dependent edit cost estimation rather than relying on fixed uniform costs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly identifies an important gap: neural GED models typically assume fixed scalar edit costs, while many real applications require context-specific edit penalties.  Highlighting this limitation and attempting to address it is meaningful and timely. Learnable edit costs tailored to the downstream graph relevance task is still an open an interesting challenge.\n\n\n- Usage of a pre-trained Gumbel–Sinkhorn network to approximate permutations is an interesting choice. If this pretraining meaningfully improves matching quality or reduces search complexity, this could be a useful contribution for the broader GED and graph matching community."}, "weaknesses": {"value": "- The paper frames its goal as learning the classical edit operation costs for GED — specifically the node/edge insertion and deletion costs $a^{\\oplus}, a^{\\ominus}, b^{\\oplus}, b^{\\ominus}$. However, these costs appear to remain fixed in the proposed architecture. Instead, the model learns a node-to-node pairwise substitution cost $c_{i,j}$. Instead of  learning edit costs,  in the traditional GED sense this is rather learning affinity measure (aka edit costs)  for soft matching. Thus the central problem remains unaddressed. \n\n- Given that  the learned parameters serve as node-pair affinity measures feeding into a differentiable assignment solver, the method becomes closely related to prior neural graph matching frameworks. I do not see anything being done differently  beyond the use of a *pre-trained* Gumbel–Sinkhorn network.\n\n\n- The choice of cosine distance to measure node pair affinity is not well justified. Cosine normalization removes information contained in the embedding norms, which can be important for distinguishing neighborhoods. For example, consider two graphs with identical node labels: a 4-cycle and a 4-clique. When using normalized cosine similarity, their node embeddings become indistinguishable, because the difference in neighborhood cardinality is reflected primarily in the magnitude of aggregated features rather than their direction.\n\n- Notation issues:    Key symbols   $a^{\\oplus}, a^{\\ominus}, b^{\\oplus}, b^{\\ominus}$ are introduced abruptly and not formally defined. Section 2.4 in particular lacks precise mathematical exposition. As of now the details regarding the loss function are extremely vague.\n\n- The paper uses AIDS,MUTAG, etc. with size filtering. This method has been previously seen to result in structural leakage [1] across datasets. The authors should ensure and clarify their  aprroach towards detecting/managing structurally isomorphic graphs. \n\n[1] Position: Graph Matching Systems Deserve Better Benchmarks, ICML 2024"}, "questions": {"value": "Questions are mainly related to the aforementioned weaknesses. \n\n- Are the classical edit operation costs actually learned? $a^{\\oplus}, a^{\\ominus}, b^{\\oplus}, b^{\\ominus}$ appear to remain fixed during training, while the model instead learns a node–node substitution cost \\(c_{i,j}\\).\n\n-  The high-level pipeline of learning node affinities and using differentiable matching, seems similar to prior work. What is the conceptual difference ? \n\n- Justification for cosine distance? \n\n- Was structural isomorphism check done during dataset creation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FdSN0yrGxP", "forum": "Y5icGPy7yc", "replyto": "Y5icGPy7yc", "signatures": ["ICLR.cc/2026/Conference/Submission7330/Reviewer_n6VT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7330/Reviewer_n6VT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000081042, "cdate": 1762000081042, "tmdate": 1762919446720, "mdate": 1762919446720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}