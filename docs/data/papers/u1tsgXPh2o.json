{"id": "u1tsgXPh2o", "number": 11003, "cdate": 1758186584737, "mdate": 1759897615201, "content": {"title": "Play to Generalize: Learning to Reason Through Game Play", "abstract": "Developing reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by literature suggesting that gameplay promotes transferable reasoning skills, we propose a novel post-training method, Visual Game Learning (ViGaL), where MLLMs develop generalizable reasoning skills through playing arcade-like games. Specifically, we show that training a 7B-parameter MLLM via reinforcement learning (RL) on simple games like Snake significantly enhances the downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL.\nRemarkably, our model outperforms specialist models post-trained on benchmark-oriented multimodal reasoning data, while preserving the model’s performance on general visual benchmarks, a challenge where specialist models often fall short.\nOur findings suggest that multimodal reasoning can emerge from gameplay, pointing to a promising strategy of designing surrogate tasks for RL post-training.", "tldr": "", "keywords": ["reinforcement learning; large language models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb9de6200698ee12166fe849212f9c58108b5a6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Visual Game Learning (ViGaL), a post-training paradigm where an MLLM (7B) is fine-tuned via reinforcement learning on simple arcade-style games (e.g., Snake and a Rotation task) to acquire transferable reasoning skills. Without using any math/benchmark-specific data during RL, ViGaL improves downstream multimodal reasoning (MathVista and MMMU) while preserving general visual abilities, outperforming specialist models trained on curated reasoning data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, this is a clearly written and well-structured paper with polished figures that make the pipeline easy to follow from setup through results. \n\n2. The core idea—leveraging controllable visual games with an RL loop as a scalable surrogate to induce transferable reasoning—is novel and timely, pushing beyond curated math datasets toward a more generalizable training signal. \n\n3. Empirically, the paper shows that gameplay-driven RL improves downstream multimodal reasoning without eroding general vision capabilities, and it consistently outperforms SFT when trained on the same data. The evidence is strengthened by broad evaluations and careful ablations (modalities, difficulty control, data scaling), suggesting robustness rather than benchmark luck. \n\n4. The approach is also practical: simple game environments, rule-based rewards, and a straightforward RL recipe lower the barrier to adoption and invite community exploration."}, "weaknesses": {"value": "1. The paper proposes a promising direction, but the evidence base feels narrow: only two games (Snake and Rotation) are used, which makes it hard to argue for broad “play-to-generalize.” The paper explains that these two games let the authors probe “reasoning and perception” (Snake for spatial/planning, Rotation for angle perception) , yet this rationale is thin and the selection criteria appear ad hoc.\n\n2. The idea of the paper would read as substantively stronger with diversity and robustness checks across more games.\n\n3. The paper proposes that different games improve different math sub-skills, but the current analysis, while suggestive, is correlational and limited to two sources.\n\n4. The paper proposes training via two games (Snake, Rotation) and shows encouraging transfer, but it does not yet establish games as a better alternative to standard post-training (e.g., curated math/logic data or instruction tuning) under controlled, budget-matched conditions."}, "questions": {"value": "1. Can you articulate a principled game-selection taxonomy (e.g., planning, geometric transforms, partial observability, stochasticity) and explain why Snake and Rotation instantiate distinct buckets?\n\n2. What ex-ante criteria (beyond convenience) determined these two games, and which candidate games were considered but rejected? If any rejected candidates, were they rejected before or after experiments, and why? \n\n3. Do you observe negative transfer for any added game types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b39qUoDOhq", "forum": "u1tsgXPh2o", "replyto": "u1tsgXPh2o", "signatures": ["ICLR.cc/2026/Conference/Submission11003/Reviewer_uv38"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11003/Reviewer_uv38"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760945547724, "cdate": 1760945547724, "tmdate": 1762922187217, "mdate": 1762922187217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ViGaL (Visual Game Learning), a post-training method that applies reinforcement learning (RL) on simple visual games—primarily Snake—to improve the multimodal reasoning performance of a 7B MLLM on downstream benchmarks such as MathVista and MMMU. The authors show that models trained on these games achieve modest gains on reasoning tasks, even though no domain-specific reasoning data is used during training. The core claim is that gameplay can serve as a surrogate task to develop transferable reasoning skills.\n\nWhile the empirical results demonstrate some performance improvement, the work largely repackages well-established ideas—such as multi-task learning, curriculum learning, and pretext tasks—under a new but superficially motivated paradigm. Crucially, it fails to provide any principled insight into why or when such transfer occurs, nor does it offer a methodology for designing effective auxiliary tasks for a given downstream goal. As such, the contribution appears incremental rather than foundational."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Clean Experimental Setup and Broad Benchmarking: The paper evaluates the proposed method on a wide range of established multimodal reasoning benchmarks (e.g., MathVista, MMMU, CLEVR+), which lends credibility to the reported performance gains. The experimental design is generally sound, and the use of rule-based RL avoids the complexity of reward modeling.\n\n2) Demonstration of Cross-Domain Performance Gain: It is empirically shown that training on a simple game can lead to measurable improvements on unrelated reasoning tasks. This serves as a proof-of-concept that some form of transfer is possible, even if the mechanism remains opaque."}, "weaknesses": {"value": "1) Lack of Novelty: Repackaging Known Ideas Without Substantial Advance\n\nThe idea that training on one task can improve performance on another—i.e., multi-task learning (MTL) or transfer learning—is decades old. The use of pretext tasks in self-supervised learning has been standard in vision and NLP. Even within RL, curriculum learning and autocurricula have long demonstrated that simple environments can give rise to complex behaviors. The paper does not convincingly argue why using Snake as a surrogate task is fundamentally different or more effective than prior approaches. It presents an anecdotal case rather than a general principle.\n\n2) No Theoretical or Mechanistic Insight\n\nWhat specific skills are learned in Snake that transfer to math reasoning?\n\nAre there measurable properties of an auxiliary task (e.g., action space complexity, reward sparsity) that predict transfer success?\n\nThe most pressing practical question—how should one design or discover an effective auxiliary task for a given downstream goal?—is completely unaddressed. The choice of Snake appears arbitrary. Why not Tetris? Why not a maze? The paper provides no criteria, heuristics, or framework for answering this."}, "questions": {"value": "1) Has the phenomenon of using simple, structured tasks to improve performance on complex ones been studied before the LLM era? If so, shouldn't this paper engage more deeply with that literature to justify its claimed novelty?\n\n2) Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "club8Gis6t", "forum": "u1tsgXPh2o", "replyto": "u1tsgXPh2o", "signatures": ["ICLR.cc/2026/Conference/Submission11003/Reviewer_w1f6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11003/Reviewer_w1f6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481186096, "cdate": 1761481186096, "tmdate": 1762922186671, "mdate": 1762922186671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using gameplay as a surrogate reasoning task to improve multimodal reasoning abilities of MLLM.\nThe model is post-trained with reinforcement learning on two simple games — Snake (2D grid planning) and Rotation (3D object rotation prediction). It uses a lightweight reward (accuracy + format) and no KL regularization. After training, the model shows improved performance on downstream reasoning benchmarks (math, geometry, and multimodal QA), while maintaining its visual perception capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of using structured games to indirectly train reasoning is novel and very interesting to me.\n- The performance of using games improves  5–8% on math and spatial tasks, showing gameplay can transfer reasoning skills.\n- The authors provided careful analysis on reward, prompts, and difficulty to show the effectiveness of each component."}, "weaknesses": {"value": "- It is not very clear how to design a game for different types of reasoning abilities. That being said, although games are useful for post training, it requires design for each task.\n- The authors are currently only designing two kinds of games with spatial/math-like games used, it is unclear whether other reasoning abilities can also be solved by games.\n- The paper provides limited analysis of why the game can improve the math reasoning ability. Specifically, what kind of questions in the benchmark are these post-trained models most benefited from."}, "questions": {"value": "Overall, this is a great paper. I would like to see the authors answers to my weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ceQbUN3i6p", "forum": "u1tsgXPh2o", "replyto": "u1tsgXPh2o", "signatures": ["ICLR.cc/2026/Conference/Submission11003/Reviewer_DBfm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11003/Reviewer_DBfm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704309759, "cdate": 1761704309759, "tmdate": 1762922186293, "mdate": 1762922186293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Visual Game Learning (ViGaL)  that uses RL post-training on simple visual games (e.g., Snake and a Rotation task) to elicit reasoning capability in multimodal LLMs in other domains such as math. Instead of training on math or benchmark-style reasoning data, a MLLM is fine-tuned to play these games with rule-based rewards; the resulting model then transfers the acquired skills to out-of-domain tasks while retaining general visual abilities. The paper argues that gameplay can serve as a scalable surrogate task for RL post-training to unlock broadly useful multimodal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper shows that RL post-training purely on simple visual games (Snake, Rotation) yields measurable gains on other seemingly unrelated domains such as math despite no direct supervision from those tasks. This is a surprising finding and is worth spreading.\n- The gameplay setup enables verifiable rule-based rewards that are friendly to reasoning training, avoiding the need for expensive reward models or human labels. The fact that it can generalize to other domains shows that it has high potential.\n- Another benefit of using game play for reasoning training is that there are a large amount of games out there with highly diverse contents. They cover all kinds of aspects of human skills, and can be a rich supplement to reasoning training data.\n- The fine-grained analysis in section 3.1 is interesting. For example it connects specific games to specific math subfields, e.g. the game Rotation aligns with angle/length questions."}, "weaknesses": {"value": "* The games are relatively simple, which is understandable though because it is the first effort to explore this direction."}, "questions": {"value": "* The prompts include thinking instructions synthesized by GPT-4o. Are they necessary? How much do they affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q3KlZLDyrg", "forum": "u1tsgXPh2o", "replyto": "u1tsgXPh2o", "signatures": ["ICLR.cc/2026/Conference/Submission11003/Reviewer_ZnCJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11003/Reviewer_ZnCJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983185450, "cdate": 1761983185450, "tmdate": 1762922185908, "mdate": 1762922185908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}