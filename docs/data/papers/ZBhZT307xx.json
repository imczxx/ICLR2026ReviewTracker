{"id": "ZBhZT307xx", "number": 11975, "cdate": 1758204958045, "mdate": 1759897541654, "content": {"title": "From Accuracy to Robustness: A Study of Rule- and Model-based Verifiers in Mathematical Reasoning", "abstract": "Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct, particularly after fine-tuning. This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique challenges inherent to both rule-based and model-based verifiers and provide insights toward developing more accurate and robust reward systems for reinforcement learning.", "tldr": "", "keywords": ["Large Reasoning Models", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88e43482be5ce29f4e9a17f3c65746a6bc90952f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the reliability of verifiers: rule-based v.s. model-based in RLVR for mathematical reasoning. The authors evaluate existing rule-based verifiers across different training datasets, finding that their recall could drop compared with rigid equivalence checks. Thus the authors propose model-based and hybrid verifiers that combine rule-based precision with LLM-based flexibility. The authors also show that model-based verifiers yield +2–3 point gains in RL performance, but they introduce reward hacking challenging: policy models exploit verifier weaknesses to gain inflated rewards."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Verifier reliability is practically and conceptually important in RL.\n\n2. The static and dynamic analyses span multiple open-source verifiers and datasets, revealing concrete recall-precision trade-offs.\n\n3. The paper goes beyond accuracy metrics, exposing vulnerabilities of fine-tuned verifiers and proposing reward hacks."}, "weaknesses": {"value": "1. In Figure 1, the differences among rule-based, verifier-based, and oracle reward curves are relatively minor. Table 2 further shows that the hybrid or model-based verifiers yield only about +2 points over the baseline. It is unclear whether such modest gains justify the additional computational and implementation overhead of integrating verifiers into the RL loop.\n\n2. The curves labeled as verifier-hacked and non-hacked in Figure 1 are almost overlapping except at the very last step. This makes it difficult to attribute the observed performance drop to reward hacking rather than to stochastic variation in RL training. It is hard to see obvious performance effects of reward hacking to the RL training.\n\n3. All experiments are conducted on Qwen-based models. The absence of results on other models limits the generality of the conclusions about verifier reliability and reward-hacking behavior.\n\n4. Unclear value proposition of verifiers. The paper claims that introducing verifiers improves reward reliability, yet it simultaneously shows that verifiers increase the risk of reward hacking. Without a clear demonstration that the verifier cost is outweighed by significant performance improvements, it is hard to be convinced that verifiers “deserve” the added overheads. A more compelling storyline might instead argue that naive verifier design will be easily hacked, and then propose a new, more robust verifier design to tackle with this new challenging."}, "questions": {"value": "1. Could the authors provide a cost–benefit analysis to quantify the extra cost of hybrid or model-based verifiers versus the performance gain reported in Table 2?\n\n2. Do similar verifier behaviors hold for non-Qwen models?\n\n3. Since verifiers can both improve reward recall and introduce hacking risk, could the authors propose a design principle that preserves reward reliability without exposing new vulnerabilities? This would make the paper more convincing and complete."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m8oIVco1iB", "forum": "ZBhZT307xx", "replyto": "ZBhZT307xx", "signatures": ["ICLR.cc/2026/Conference/Submission11975/Reviewer_KCtg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11975/Reviewer_KCtg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634431401, "cdate": 1761634431401, "tmdate": 1762922971386, "mdate": 1762922971386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the verifier component of RLVR in mathematical reasoning tasks. The authors point out that commonly used rule-based verifiers buffer from high precision but low recall, as they perform well only on responses following specific patterns. To mitigate this limitation, LLMs are utilized as a model-based verifier based on its reasoning skills. \nFurthermore, the authors proposed a hybrid verifier and also analyzed the reward hacking  problem in using trained model-based verifiers."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The paper attempts to clarify and analyze key issues often overlooked in RLHF and RLVR research, particularly the limitations of rule-based versus model-based verifiers. This focus addresses an important and timely problem in the field.\n\n* The discussion on potential reward hacking and robustness issues arising from the use of model-based verifiers is interesting, providing new perspectives on challenges that are often underexplored in current research."}, "weaknesses": {"value": "**\\[W1\\] Insufficient Analysis**  \nThe paper's main motivation is that the impact of verifier types on RLVR is poorly understood, yet it lacks in-depth analysis on this topic. There is no error case analysis explaining why rule- and model-based verifiers fail, nor any examination of how these failures influence policy behavior. Without these critical components, the paper lacks the insights necessary to address its core motivation.\n\n**\\[W2\\] Low Readability**  \nThe overall organization of the paper lacks clarity, which makes it difficult to follow the intended narrative. In the abstract and introduction, the authors state that the paper focuses on a comparative analysis between rule-based verifiers and model-based verifiers. However, there is no mention of hybrid verifiers in these sections. Starting from Section 3.3, the paper significantly discusses hybrid verifiers, but the motivation for introducing this concept and its role within the paper’s main objective are not clearly explained. As a result, the overall flow of the paper becomes confusing, and readers may struggle to understand how each section contributes to the central argument.\n\nIn addition, the placement of figures and tables often lacks alignment with the corresponding text. Some figures (e.g., Figure 1\\) combine content drawn from multiple sections, which makes cross-checking difficult.\n\n**\\[W3\\] Limited Diversity of Verifiers and Policy Models**  \nThe hybrid setting is tested with only three model-based verifiers, leaving unclear whether the reported vulnerabilities generalize across architectures or scales. Moreover, RL training uses a single policy (Qwen 2.5 7B), which restricts analysis of verifier–policy interactions. Broader experiments with different verifier families and policy sizes would provide stronger evidence for the claimed trends."}, "questions": {"value": "**\\[Q1\\]** Lines 398-399: What explains the instability observed in trained verifier models compared to untrained verifiers and rule-based verifiers? It seems counterintuitive that untrained model-based verifiers don't exhibit the same instability as their trained counterparts.\n\n**\\[Q2\\]** Given that GPT-4o was utilized as the oracle reward, what performance metrics would this model achieve if implemented directly as a model-based verifier? If it demonstrates high precision and recall, wouldn't this suggest that employing LLMs with superior reasoning capabilities is a more direct solution?\n\n**\\[Q3\\]** Are models with strong reasoning capabilities like GPT-4o also susceptible to reward hacking? This question has implications for the fundamental approach proposed in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JBwDV3kARx", "forum": "ZBhZT307xx", "replyto": "ZBhZT307xx", "signatures": ["ICLR.cc/2026/Conference/Submission11975/Reviewer_JfKs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11975/Reviewer_JfKs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908889766, "cdate": 1761908889766, "tmdate": 1762922971095, "mdate": 1762922971095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates verifier accuracy and its impact on model performance in reinforcement learning with verifiable rewards (RLVR) for mathematical reasoning tasks. Through systematic experiments, the authors show that commonly used rule-based verifiers, while highly precise, suffer from false negatives that lead to suboptimal training outcomes. They further evaluate model-based verifiers and find that, although recall improves, these verifiers are vulnerable to reward hacking during RL training. To mitigate this issue, the paper proposes a hybrid verifier that combines rule-based and model-based verifications, achieving greater stability and performance. Overall, the study underscores the critical role of verifier design in ensuring reliable reward signals and provides some practical guidance for building more robust verifier systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality.** The paper provides a systematic and timely investigation of verifier design in RL with verifiable rewards (RLVR), providing one of the first comprehensive analyses of how verifier accuracy impacts training stability and model performance and exposes limitations in current verification systems.\n- **Comprehensive experimentation.** The study conducts extensive experiments comparing rule-based and model-based verifiers, builds dedicated diagnostic datasets, and performs multiple RL training runs with the Qwen2.5-7B model under different verifier configurations.\n- **Quality.** The experimental design is sound and rigorous, with consistent metrics and well-documented procedures. The reported improvements are clear and no apparent flaws are evident in the setup and evaluation process."}, "weaknesses": {"value": "- **[Significance]** While the paper presents systematic experiments and insightful analyses, many of its findings confirm known issues rather than reveal fundamentally new phenomena. Specifically, (1) the false-negative problem of rule-based verifiers has been discussed in prior work on mathematical expression evaluation (e.g., [1], [2]); and (2) the vulnerability of LLM-based verifiers to reward hacking aligns with broader findings on LLM-as-a-judge robustness and the reward hacking in RLHF (e.g., [3], [4]). Although this paper is among the first to document such hacking behaviors in verifier-based RL, the results are largely predictable. The proposed hybrid strategy, which combines rule-based and model-based verifiers, is practical but conceptually straightforward, as both components are adapted from existing methods.\n- **[Experiment]** The RL evaluation primarily uses a single policy model (Qwen2.5-7B). Without additional policy models, it is difficult to assess whether the observed verifier effects generalize across architectures or model scales.\n- **[Presentation]** The paper’s presentation is somewhat disorganized, making it easy for readers to lose track of the experimental narrative. Numerous experimental setups are scattered throughout the text, and the analyses are often separated from their corresponding results. For example, Figure 1 is introduced early (page 2) but its related experiment is not discussed until page 6; likewise, the analysis of the hybrid verifier in Section 4.1 refers to results that only appear in the appendix. Additionally, Sections 5.2 and 6 contain overlapping analyses on the reward hacking patterns. Overall, this fragmented structure reduces readability and weakens the logical flow of the paper.\n\n[1] Non-Autoregressive Math Word Problem Solver with Unified Tree Structure. EMNLP 2023.\n\n[2] TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning. Arxiv 2025.\n\n[3] Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment. EMNLP 2024.\n\n[4] ODIN: Disentangled Reward Mitigates Hacking in RLHF. ICML 2024."}, "questions": {"value": "- How do the authors interpret the noise in rule-based verifiers? Do such negative samples represent a fundamental limitation for developing better reasoning models, or could they instead be viewed as tolerable label noise—given that neural networks are often robust to noisy supervision [1, 2]? Moreover, since hybrid verification introduces additional training cost, why not explore simpler scaling strategies, such as increasing data volume or training steps, to mitigate the noise effect?\n- A potentially insightful extension would be to quantify verifier noise tolerance in RLVR, i.e., how much noise the training can tolerate before noticeable performance degradation or collapse occurs. Do the authors have any empirical observations or insights on this aspect?\n\n[1] Deep learning is robust to massive label noise. Arxiv 2017.\n\n[2] Spurious Rewards: Rethinking Training Signals in RLVR. Arxiv 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4H800Mfcwi", "forum": "ZBhZT307xx", "replyto": "ZBhZT307xx", "signatures": ["ICLR.cc/2026/Conference/Submission11975/Reviewer_7SQK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11975/Reviewer_7SQK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916676402, "cdate": 1761916676402, "tmdate": 1762922970655, "mdate": 1762922970655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the reliability of rule-based vs. model-based verifiers in reinforcement learning with verifiable reward (RLVR), focusing on mathematical reasoning tasks. Experimental results demonstrate that Rule-based verifiers achieve near-perfect precision but poor recall, increasingly misclassifying correct answers from stronger models. Model-based verifiers can offer higher recall and better flexibility across datasets, but are prone to reward hacking during RL training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow, with a logical structure and clear presentation of results.\n2. It addresses an important and timely question about the reliability of verifiers in RL-based fine-tuning.\n3. The work provides a detailed and systematic statistical analysis comparing rule-based and model-based verifiers across multiple benchmarks."}, "weaknesses": {"value": "1. The paper is mostly empirical and lacks a formal analysis of why RL dynamics amplify verifier brittleness. \n2. The study focuses almost exclusively on mathematical reasoning; generalization to other domains is less mentioned.\n3. Reported gains in RL experiments are small and may not exceed noise given limited sampling. Statistical uncertainty isn’t reported.\n4. The paper lacks a clear concluding message or actionable suggestion. While it identifies the limitations of both rule-based and model-based verifiers, it does not provide concrete guidance or a principled framework for designing more robust evaluation systems.\n5. Section 6 feels unconvincing to me. The probing study on “hacking patterns” appears artificial and disconnected from realistic training dynamics. In practical RL settings, it is unclear how likely policy models are to autonomously discover and exploit such handcrafted adversarial patterns."}, "questions": {"value": "1. Have the authors considered evaluating the findings on more general or non-mathematical reasoning tasks to test cross-domain robustness?\n2. I am quite intersted in what mechanisms cause fine-tuned verifiers to become more vulnerable.\n3. Could adversarially trained verifiers (such as through contrastive fine-tuning on generated hacking examples) help improve robustness without sacrificing recall?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XlQuQesS4R", "forum": "ZBhZT307xx", "replyto": "ZBhZT307xx", "signatures": ["ICLR.cc/2026/Conference/Submission11975/Reviewer_oRgD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11975/Reviewer_oRgD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978343786, "cdate": 1761978343786, "tmdate": 1762922970083, "mdate": 1762922970083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}