{"id": "Y7kJ4oUgwL", "number": 17788, "cdate": 1758280541038, "mdate": 1759897153842, "content": {"title": "WSI-GT: Pseudo-Label Guided Graph Transformer for Whole-Slide Histology", "abstract": "Whole-slide histology images (WSIs) can exceed 100k × 100k pixels, making direct pixel-level segmentation infeasible and requiring patch-level classification as a practical alternative. However, most approaches either treat patches independently, ignoring spatial and biological context, or rely on deep graph models that oversmooth, leading to loss of critical tissue details.\n\nWe present WSI-GT (Pseudo-Label Guided Graph Transformer), a simple yet effective architecture that addresses these challenges. WSI-GT combines a lightweight local graph convolution block for neighborhood feature aggregation with a pseudo-label guided attention mechanism that preserves intra-class variability and mitigates oversmoothing. To cope with sparse annotations, we introduce an area-weighted sampling strategy that balances class representation while maintaining tissue topology.\n\nWSI-GT achieves a Macro F1 of 0.95 on PATH-DT-MSU WSS2v2, improving by up to 3 percentage points over tile-based CNNs and by about 2 points over strong graph baselines. It further generalizes well to the Placenta benchmark and standard graph node classification datasets, highlighting both clinical relevance and broader applicability. These results position WSI-GT as a practical and scalable solution for graph-based learning on extremely large images.", "tldr": "WSI-GT is a pseudo-label guided graph transformer for classifying patches in 100k×100k whole-slide histology images, combining graph convolutions and intra-class attention to achieve state-of-the-art accuracy", "keywords": ["graph transformers", "pseudo-labeling", "digital pathology", "whole-slide images", "patch classification", "over-smoothing mitigation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20c997bc9744fc3ade8ff3b51ce26ee62e6b09a0.pdf", "supplementary_material": "/attachment/9e97b4a0338ad71366e0056285c9b100ceb72096.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed a shallow graph to WSI patch classification. The topic is relevant and aligns with current efforts to leverage graph representations for digital pathology."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and timely problem in computational pathology with nice visualizations."}, "weaknesses": {"value": "1.\tQuestionable state-of-the-art claims: Several existing shallow graph models, such as GPT [4], HEAT [5], and attention-based spatial models like CAMIL [3], are not adequately discussed or compared. While GTP [4] is mentioned in the introduction, it is not included in the experiments, which limits the validity of the SOTA claim.\n2.\tIncomplete baseline selection: strong CNN-based baselines such as DSMIL[1] and CLAM [2] are not mentioned at all. CLAM[2] is especially well established and can be easily extended to multi-class tasks. And the baselines mentioned in bullet point 1 are not included either. \n3.\tQuestionable benchmark dataset: The proposed model is designed for WSIs, yet the datasets used—PubMed (Yang et al., 2016), Actor, and Deezer (Lim et al., 2021), are unrelated to histopathology or WSI classification. This discrepancy raises questions about the generalizability and practical relevance of the results.\n4.\tUnsupported claims of scalability: The claim that the proposed method provides a “practical and scalable solution” is not sufficiently substantiated by the experiments or analysis. See question 1.\n\n\n[1] Li, Bin, Yin Li, and Kevin W. Eliceiri. \"Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n[2] Lu, Ming Y., et al. \"Data-efficient and weakly supervised computational pathology on whole-slide images.\" Nature biomedical engineering 5.6 (2021): 555-570.\n[3] Fourkioti, Olga, et al. \"CAMIL: Context-aware multiple instance learning for cancer detection and subtyping in whole slide images.\" arXiv preprint arXiv:2305.05314 (2023).\n[4] Zheng, Yi, et al. \"A graph-transformer for whole slide image classification.\" IEEE transactions on medical imaging 41.11 (2022): 3003-3015.\n[5] Chan, Tsai Hor, et al. \"Histopathology whole slide image analysis with heterogeneous graph representation learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023."}, "questions": {"value": "1.\tThe annotation-based graph construction cannot be applied during inference, so a sampling process is needed. What is the overhead here? What practical advantage does this design offer?\n2.\tThe term pseudo-label is used ambiguously. How are these labels generated, and how do they differ from the ground-truth annotations? If they are derived from annotations, what constitutes the pseudo-label during inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9kwaDTkFRo", "forum": "Y7kJ4oUgwL", "replyto": "Y7kJ4oUgwL", "signatures": ["ICLR.cc/2026/Conference/Submission17788/Reviewer_g2ci"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17788/Reviewer_g2ci"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818548459, "cdate": 1761818548459, "tmdate": 1762927632230, "mdate": 1762927632230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WSI-GT, a simple Graph Transformer with Pseudo-Label Attention for precise tissue fragment classification in WSIs. It combines a ightweight graph convolution block for local feature aggregation with a pseudo-label attention block that restricts self-attention to nodes sharing the same pseudo-label. The authors also adopt an existing area-weighted random sampling approach for balanced subgraph construction and designed an adaptive coverage sampler for efficiently and unbiasedly constructing graphs from unannotated WSIs. Evaluating on the PATH-DT-MSU WSS2v2 dataset, the authors report a Macro-F1 of 0.95, outperforming both CNN and graph baselines. Additional tests on the Placenta cell-graph benchmark and standard node-classification datasets (PubMed, Actor, Deezer) suggest that the model generalizes to other domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clear and easy to follow. The label-aware self-attention is\nan intuitive way to curb over-globalization in graph transformers while\nkeeping the benefits of attention; the GC+PLA mixing is straightforward\nand easy to adopt.\n\n\n• The paper addresses practical pitfalls in slide-level graph construction and\noffers an area-weighted sampling scheme.\n• The approach generalizes beyond WSIs, showing competitive performance\non a cell-graph benchmark and standard node-classification datasets."}, "weaknesses": {"value": "• Novelty is modest for ICLR: the main contribution is a practical combination of known components (GC + hard pseudo-label mask; adopted area-weighted sampling) plus a simple inference sampler, without new\nlearning principles or strong theoretical/robustness evidence, despite solid\nempirical results on a small dataset.\n• PLA relies on pseudo-labels predicted by a pretrained patch encoder (ResNet-50),\nbut the paper does not fully specify the pseudo-labeling procedure (argmax vs. thresholded labels, calibration, whether pseudo-labels are fixed or updated during training, and how leakage is avoided). Given that PLA enforces a hard attention mask (Eq. 6), these details are important for reproducibility and for assessing robustness.\n\n• For PubMed/Actor/Deezer, the paper says it “adopted the original eval-\nuation pipelines,” but it is unclear what provides pseudo-labels to create the PLA mask on these datasets.\n\n• The core experiment is performed on only one WSI dataset. The authors\nshould have shown results on at least another WSI dataset."}, "questions": {"value": "How are pseudo-labels produced- by the fine-tuned ResNet50’s argmax?\n\nAny confidence threshold or temperature scaling? Are pseudo-labels re-\ncomputed during training (e.g., every epoch) or fixed after pretraining?\n\nHave you considered the effect of noisy pseudo-labels?\n\n For PubMed/Actor/Deezer, what are the pseudo-labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o6zTCX1LSb", "forum": "Y7kJ4oUgwL", "replyto": "Y7kJ4oUgwL", "signatures": ["ICLR.cc/2026/Conference/Submission17788/Reviewer_CXWv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17788/Reviewer_CXWv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828532741, "cdate": 1761828532741, "tmdate": 1762927631674, "mdate": 1762927631674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose WSI-GT (Pseudo-Label Guided Graph Transformer), a graph-based framework for whole-slide image (WSI) analysis that mitigates oversmoothing and leverages limited annotations. It integrates local graph convolutions with pseudo-label guided attention to preserve tissue heterogeneity and introduces an area-weighted sampling strategy for class balance. WSI-GT demonstrates advantage on various tissue type classification benchmark and generalizes to other datasets, demonstrating strong scalability and clinical applicability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors proposes a new graph neural network (GNN) architecture to address the tissue type classification, which is rarely addressed in the previous GNN applications on digital patholgoy"}, "weaknesses": {"value": "This work missing some key comparison as illustrated below\n\n- It is clear that this work is just an extension from this published paper: \"Tissue type classification for whole slide histological images with graph convolutional neural network\", Sun et al. ,ICBSP 2024.  The only differences are pesudo-label attention block section in methodology and 2 additional evaluations in the experiments. However, there is zero discussion and comparison to the ICBSP 2024 work. If we directly quote the number from the ICBSP work on PATH-DT-MSU WSS2v2 data, The improvement from the additional  pesudo-label attention block is 0.03 (0.92->0.95). Also, it is weird that the number of other baselines (e.g., SGFormer) are different between ICBSP 2024 work (0.91) and this one (0.93). Ideally, they should be the same since the evaluation data is the same, which make all results suspicious.\n\n\n- Another missing comparison is placenta benchmark by Vanea et al. 2022. The author completely missed their results in table 2, which i think is a very serious mistakes. Here are the quoted results from Table 1 in Vanea et al. 2022: \nClassification accuracy: 67%\nROC AUC: 0.898\nBoth are \"higher\" than the results presented in this work (64.98 and 0.888), which undermine the sota claim from author."}, "questions": {"value": "- Why did the author ignore the ICBSP 2024 work? Are author genuinely unaware of the existence of that?\n- Why did the author not cite the results from Vanea et al. 2022 while saying: \"adopted the original evaluation pipelines, integrating our method to ensure comparability with prior results?\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LZcn9WjQr7", "forum": "Y7kJ4oUgwL", "replyto": "Y7kJ4oUgwL", "signatures": ["ICLR.cc/2026/Conference/Submission17788/Reviewer_d3Nc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17788/Reviewer_d3Nc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862417068, "cdate": 1761862417068, "tmdate": 1762927631127, "mdate": 1762927631127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Graph Transformer approach for tissue fragment classification in pathology whole-slide images (WSIs). Its key idea is to construct a slide-level graph for classification, so as to properly process adjacent patches that may share the same label. To construct it, a sampling technique is proposed to obtain patches from multiple annotated regions. The experiments on PATH-DT-MSU WSS2v2 dataset show that the proposed graph transformer-based approach can surpass several compared baselines. On the other three datasets, the proposed approach often demonstrates better performance in classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed approach is described clearly, with several illustrations and equations to show its key steps.\n- Node-level classification tasks are also presented in the experiments, which could further demonstrate the generalization ability of the proposed approach."}, "weaknesses": {"value": "I have several concerns as follows:\n- The target task this paper focuses on is tissue fragment classification in pathology WSIs. However, only one dataset is used to evaluate the performance of the proposed approach. The authors are encouraged to validate their approach on more datasets in this field.\n- Node-level classification experiments are conducted to confirm the generalization ability of the proposed approach. However, many state-of-the-art baselines are missing, e.g., HACT (Pati et al., Hierarchical graph representations in digital pathology, MedIA, 2022). The authors are encouraged to compare with recent state-of-the-art networks to show the strength of their methods.\n- In Table 2, compared to GraphSAINT-rw, the proposed method fails to show advantages (only marginal improvements over the baseline) in node-level classification."}, "questions": {"value": "- Could the authors use more datasets (instead of only PATH-DT-MSU WSS2V2) to benchmark the performance of all compared networks?\n- Could the authors explain why the proposed method cannot perform in node-level classification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6Z9Eh8DRVN", "forum": "Y7kJ4oUgwL", "replyto": "Y7kJ4oUgwL", "signatures": ["ICLR.cc/2026/Conference/Submission17788/Reviewer_S1xr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17788/Reviewer_S1xr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762224906329, "cdate": 1762224906329, "tmdate": 1762927629878, "mdate": 1762927629878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}