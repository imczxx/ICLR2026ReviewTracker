{"id": "NdDlqHV1md", "number": 18016, "cdate": 1758282945418, "mdate": 1763396440405, "content": {"title": "Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel", "abstract": "Mixture-of-Experts (MoE) has become a cornerstone in recent state-of-the-art large language models (LLMs). Traditionally, MoE relies on $\\mathrm{Softmax}$ as the router score function to aggregate expert output, a designed choice that has persisted from the earliest MoE models to modern LLMs, and is now widely regarded as standard practice. However, the necessity of using $\\mathrm{Softmax}$ to project router weights into a probability simplex remains an unchallenged assumption rather than a principled design choice. In this work, we first revisit the classical Nadaraya–Watson regression and observe that MoE shares the same mathematical formulation as Nadaraya–Watson regression. Furthermore, we show that both feed-forward neural network (FFN) and Mixture-of-Experts (MoE) can be interpreted as a special case of Nadaraya–Watson regression, where the kernel function corresponds to the input neurons of the output layer. Motivated by these insights, we propose the **zero-additional-cost** Kernel Inspired Router with Normalization ($\\mathrm{KERN}$), an FFN-style router function, as an alternative to $\\mathrm{Softmax}$. We demonstrate that this router generalizes both $\\mathrm{Sigmoid}$- and $\\mathrm{Softmax}$-based routers. **Based on empirical observations and established practices in FFN implementation, we recommend the use of $\\mathrm{ReLU}$ activation and $\\ell_2$-normalization in $\\mathrm{KERN}$ router function.** Comprehensive experiments in MoE and LLM validate the effectiveness of the proposed FFN-style router function $\\mathrm{KERN}$.", "tldr": "We rethink the MoE with Nadaraya-Watson Kernel and propose KERN router to replace Softmax router to achieve better performance.", "keywords": ["Mixture of Experts", "Large Language Models", "Foundation Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b6f9081a22a9970fe8c8c4a27ba2a62cde43fa1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper revisits the design of the Mixture-of-Experts (MoE) routing mechanism through the lens of Nadaraya–Watson regression, a classical kernel-based estimator. The authors identify a structural similarity between MoE routers, feedforward networks (FFNs), and kernel smoothers, and use this observation to reinterpret the MoE routing process as a form of parametric kernel regression. Building on this perspective, the authors propose a new router function called KERN (Kernel Inspired Router with Normalization), which replaces the traditional Softmax gating with a lightweight $l_2$-normalization and ReLU activation. KERN eliminates the need for projecting gating scores onto a probability simplex, simplifying training and improving numerical stability. Empirical evaluations show that KERN achieves balanced expert utilization and improved stability in large-scale MoE and LLM setups, without adding computational or parameter overhead. The approach generalizes both Softmax- and Sigmoid-based routers while maintaining the inductive biases of standard FFN layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a principled reinterpretation of Mixture-of-Experts (MoE) routing through the lens of Nadaraya–Watson regression, providing theoretical clarity and unifying MoE with FFN and kernel regression models.\n2. The paper is well-organized, progressing clearly from Nadaraya–Watson regression to FFN interpretation, MoE formulation, and the final KERN methodology.\n3. The authors conduct sufficient experiments and ablation studies on multiple MoE setups, validating both the theoretical claims and practical benefits of KERN."}, "weaknesses": {"value": "1. The paper appears to change the order of operations between normalization and activation layers. Specifically, Equation (4) applies the activation function before normalization, which is inconsistent with Equation (3), where normalization precedes activation. This discrepancy may affect the theoretical consistency of the proposed analogy between FFN and Nadaraya–Watson regression.\n2. The main contribution focuses on designing an alternative router function for MoE. While the proposed KERN router is elegant, its generality appears limited compared to broader architectural components like the Softmax function, which is widely used beyond MoE.\n3. The paper would benefit from a figure that clearly presents the computational flow of the proposed router function. Although the mathematical formulation is clear, a visual depiction would enhance understanding and accessibility, especially for readers less familiar with MoE internals."}, "questions": {"value": "1. How does changing the order between normalization and activation (as seen in Eq. (4) vs. Eq. (3)) influence the theoretical interpretation or empirical behavior of the model? Has this difference been empirically tested or justified in the paper?\n2. Can the proposed KERN router be extended beyond MoE architectures to serve as a general routing or attention mechanism in neural networks, similar to how Softmax is used broadly across architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0UBpjdavrq", "forum": "NdDlqHV1md", "replyto": "NdDlqHV1md", "signatures": ["ICLR.cc/2026/Conference/Submission18016/Reviewer_XdAq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18016/Reviewer_XdAq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636176594, "cdate": 1761636176594, "tmdate": 1762927807782, "mdate": 1762927807782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reinterprets MoE routing through the lens of the Nadaraya–Watson (NW) estimator, arguing that both FFNs and MoE can be seen as parametric NW regressors. Building on this view, the authors propose KERN (Kernel-Inspired Router with Normalization): a “FFN-style” router that replaces Softmax with a linear projection followed by L2-normalization, ReLU, and a learnable global scale, combined with standard Top-k selection. The motivation is to avoid exponential activations’ saturation, keep output scale stable, and improve expert utilization. Experiments from 520M→6.9B (active 125M→1.3B) on Books3/ArXiv and a 50B-token FineWeb-Edu pretrain report consistent loss gains and better zero-shot downstream accuracy versus Softmax/Sigmoid/Tanh routers and dense baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Clear, unified view.** The NW formulation offers a simple mathematical template that maps FFN and MoE into the same “kernel+normalization” structure, making the proposed design choices easy to state and implement.\n- **Low engineering overhead.** KERN is a drop-in change (ReLU+L2norm+γ) and the paper claims no substantive time cost relative to Softmax routing.\n- **Consistent wins in the reported setup.** Across multiple model sizes and datasets, KERN slightly but repeatedly outperforms Softmax/Sigmoid/Tanh, including at longer contexts."}, "weaknesses": {"value": "- **Limited novelty.** The proposed router is essentially a straightforward substitution of the probabilistic Softmax with a normalized ReLU gate, i.e., $\\gamma \\cdot ReLU(norm(xW)))$, in replace of $Softmax(xW)$. While the Nadaraya–Watson (NW) perspective is conceptually interesting, the paper mainly **instantiates** an NW-like form without analyzing whether NW’s assumptions (kernel choice, normalization, bandwidth/scale selection) align with MoE routing objectives (load balance, capacity control, calibration). The stated motivation—“Softmax/Sigmoid suffer gradient saturation/vanishing”—is asserted rather than demonstrated (e.g., no gradient-magnitude, activation-scale, or saturation diagnostics).\n- **Insufficient comparison to closely related work.** Most components of the final design already exist in the literature, yet the paper primarily compares against vanilla Softmax/Sigmoid/Tanh routers, omitting stronger, conceptually adjacent baselines and ablations.\n\t- **ReLU routing:** \\[1\\] replaces Softmax with ReLU to obtain fully differentiable, dynamic routing.\n    - **Logit normalization:** \\[2\\] normalizes routing logits to decouple expert-centroid magnitude from assignment.\n    - **Output scaling:** Industrial implements like \\[3,4\\] adjusts the routing output scale to preserve training dynamics comparable to dense models.  \n\n- **Questionable empirical setup and reporting.** The reported gaps show Sigmoid performing markedly worse than Softmax—sometimes **larger** than the gap between Softmax-MoE and its dense counterpart—which is atypical relative to prior results \\[2,5\\] and suggests possible tuning or configuration mismatches. Critical details for effective MoE training are missing: the load-balancing objective (form and coefficient), capacity factor/dropless vs padded routing, post-Top-(k) renormalization (if any), and per-expert utilization/entropy or drop-rate statistics. Without these, it is difficult to assess stability, fairness of comparisons, or reproducibility of the claimed gains. \n\n\\[1\\] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing https://arxiv.org/pdf/2412.14711\n\\[2\\] On the Representation Collapse of Sparse Mixture of Experts https://arxiv.org/pdf/2204.09179\n\\[3\\] DeepSeek-V3 Technical Report https://arxiv.org/abs/2412.19437\n\\[4\\] Kimi K2: Open Agentic Intelligence https://arxiv.org/abs/2507.20534\n\\[5\\] Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts https://arxiv.org/pdf/2408.15664"}, "questions": {"value": "- How does the scale parameter $\\gamma$ evolve during training (per layer and across settings)?\n- Do gradient vanishing and saturation actually occur with Softmax and Sigmoid routers, and does KERN empirically mitigate them?\n- Does KERN improve expert load balancing?\n- How does KERN behave in MoE architectures with shared experts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gYnjwX1UT4", "forum": "NdDlqHV1md", "replyto": "NdDlqHV1md", "signatures": ["ICLR.cc/2026/Conference/Submission18016/Reviewer_qEoZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18016/Reviewer_qEoZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798608482, "cdate": 1761798608482, "tmdate": 1762927807306, "mdate": 1762927807306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the routing mechanism of Mixture-of-Experts (MoE) from a statistical perspective. The authors draw an analogy between the MoE router and the Nadaraya–Watson (NW) kernel regression, showing that both can be viewed as weighted aggregations over functions. Motivated by this connection, the paper introduces KERN (Kernel-Inspired Router with Normalization) — an FFN-style router that replaces Softmax with ReLU activation and ℓ₂ normalization.\nExtensive experiments across model sizes (125M–6.9B parameters), context lengths, and datasets (Arxiv, Books3, FineWeb-Edu, etc.) demonstrate consistent gains of KERN over Softmax, Sigmoid, and Tanh routers. The method adds no parameters or computational overhead, making it a practically attractive alternative for large-scale MoE systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper revisits the routing mechanism of Mixture-of-Experts (MoE) from a statistical perspective. The authors draw an analogy between the MoE router and the Nadaraya–Watson (NW) kernel regression, showing that both can be viewed as weighted aggregations over functions. Motivated by this connection, the paper introduces KERN (Kernel-Inspired Router with Normalization) — an FFN-style router that replaces Softmax with ReLU activation and ℓ₂ normalization.\nExtensive experiments across model sizes (125M–6.9B parameters), context lengths, and datasets (Arxiv, Books3, FineWeb-Edu, etc.) demonstrate consistent gains of KERN over Softmax, Sigmoid, and Tanh routers. The method adds no parameters or computational overhead, making it a practically attractive alternative for large-scale MoE systems."}, "weaknesses": {"value": "The central theoretical claim---that Mixture-of-Experts (MoE) routing is *equivalent* to Nadaraya–Watson (NW) kernel regression---is conceptually appealing but lacks mathematical rigor. \n\nThe presented argument only shows that the softmax weighting can *approximate* a kernel-weighted aggregation under certain parameterizations, not that the two models are formally or probabilistically equivalent. \n\nIn particular, NW regression requires a symmetric and positive-definite kernel function with explicit density normalization, while the softmax router relies on asymmetric learned logits and gradient-based optimization dynamics. \n\nHence, the connection is heuristic rather than theoretically grounded. \n\nThe paper would benefit from a clear statement of the conditions (e.g., scaling limits, normalization assumptions) under which this analogy approximately holds. \n\nOverall, the claimed equivalence is oversold, weakening the theoretical soundness of the contribution."}, "questions": {"value": "1. The paper claims a formal equivalence between softmax routing and Nadaraya–Watson kernel regression. Could the authors clarify whether this relationship is a *functional approximation* (similar weighting structure) or a *true probabilistic equivalence* under specific assumptions?  \n\n2. Under what mathematical conditions---such as temperature scaling, large-dimension limits, or specific kernel choices---can softmax routing converge to the NW estimator?  \n\n3. Given that the softmax router’s logits are learned through gradient descent rather than fixed distances, how does this affect the validity of interpreting it as a kernel regression model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9fVQ7LfAEe", "forum": "NdDlqHV1md", "replyto": "NdDlqHV1md", "signatures": ["ICLR.cc/2026/Conference/Submission18016/Reviewer_pDju"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18016/Reviewer_pDju"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967790793, "cdate": 1761967790793, "tmdate": 1762927806821, "mdate": 1762927806821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper connects MoE routers to Nadaraya-Watson regression. The authors then propose KERN, a zero-cost FFN-style router using ReLU activation and ℓ2-normalization. Experiments on language modelling and NLP tasks demonstrate KERN's effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors motivates various design decisions thoroughly.\n-  This work connects kernel method perspective to MoE router design."}, "weaknesses": {"value": "- Improvement is usually marginal. \n- It's a pity that the connection to Nadaraya-Watson estimator isn't well explored. It's not obvious why the router design is inspired by this perspective, they don't have much shared element, and somehow we don't see any attempt at adopting the traditional formula as a router. I think the paper will be a lot more interesting if it shows variants like Gaussian kernel."}, "questions": {"value": "- In \"Advantages of the KERN router function\", why does KERN help with gradient vanishing? Sigmoid and Softmax may still have some gradient even if the input is at small value, but RELU function in KERN directly cutoff the gradient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oa4Xeqj7i0", "forum": "NdDlqHV1md", "replyto": "NdDlqHV1md", "signatures": ["ICLR.cc/2026/Conference/Submission18016/Reviewer_dQSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18016/Reviewer_dQSC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233768920, "cdate": 1762233768920, "tmdate": 1762927806192, "mdate": 1762927806192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All Reviewers"}, "comment": {"value": "Dear Area Chair and Reviewers,\n\nThank you for the attention to this work. We have responded to the question of reviewers. And the following is the response to the common questions.\n\n\n**Q1: The relation between Nadaraya-Watson Kernel Regression, FFN, and MoE ( Reviewer dQSC, Reviewer pDju, Reviewer qEoZ, Reviewer XdAq)**\n\nA1: **We connect the FFN and MoE with Nadaraya-Watson Kernel Regression, and then we design the MoE as FFN**\n\n**Nadaraya-Watson Kernel Regression**: $f_{NW}(x) = \\sum_{i=1}^N \\frac{K(x, x_i)}{\\sum_{j=1}^N K(x, x_j)} y_i$\n\n\n\n**Mixture-of-Experts**:  $\\text{MoE}(x) = \\sum_{m=1}^M \\hat{g}_m(x) E_m(x),$\n\n**Mixture-of-Experts Routing**: $g_m(x) = \\frac{K(\\langle w_m, x\\rangle)}{\\sum_{j=1}^M K(\\langle w_j, x \\rangle)}.$\n\n**FFN**:  $\\text{FFN}(x) = \\sum_{m=1}^M \\phi(Norm(xW_1)_m)(W_2)_m$\n\n**MoE with KERN:** $\\text{MoE}(x) = \\sum_{m=1}^M \\phi(Norm(xW)_m) E_m(x),$ \n\nThe MoE is the Nadaraya-Watson Kernel Regression with $K(x,w_i)=K(w_i,x)=e^{x \\cdot w_i}$\n\n**Q2: The gradient related to KERN, Sigmoid and Softmax (Reviewer dQSC, Reviewer qEoZ)**\n\nA2: **There is L2 normalization and the MoE is sparse (usually activation ratio is smaller than 50%), and the ReLU cutoff value when the activation ratio is larger than 50% (Appendix J).** Therefore, even if some value is cut off, it still has the gradient. The following is the Jacobian Matrix:\n\n$J = \\frac{I}{\\|x\\|_2} - \\frac{x x^T}{\\|x\\|_2^3}.$\n\n**Therefore, even if there is only one element that is selected, there is still a gradient on the cutoff position of KERN.** However, the softmax and sigmoid will face gradient vanishing when the value is very large or very small. For example, we observe that the softmax value and sigmoid value could achieve relatively large values (e.g,. more than 0.99).\n\n\n\n\n\n```\nExample: input [-2 -3 -1 2 3 10] : \nKERN (L2Norm): [-0.177  -0.266 -0.088  0.177   0.266  0.887]\nSigmoid: [0.119 0.0474 0.268 0.880 0.952 0.999] \n###The router logit will not have a gradient for $w_6$\nSoftmax: [6.136e-06 2.257e-06 1.668e-05 3.350e-04 9.107e-04 9.987e-01] \n######The graient is almost vanish for $w_1$ to $w_6$\n\nReLU & choose top-2 for KERN: [0 0 0 0 0.266  0.887]   \n#### Note that whether there is ReLU or not, the choice of top-2 will not change the final result because usually the ReLU only masks 50%, while top-2 masks 4/6=66.7%. When the top-k for K/M>50%, adding ReLU may improve the performance, which is similar to FFN (Appendix J).\nchoose top-2 for Sigmoid: [0 0 0 0 0.952 0.999] \n#### the sigmoid will NOT have gradient for $w_1$ to $w_4$ because only 2 values are selected. And $w_6$ also does not have a gradient that vanishes because 0.999 is too large.\nchoose top-2 for Softmax: [0 0 0 0 9.107e-04 9.987e-01]   \n### the softmax will NOT have a gradient for $w_1$ to $w_6$ because one value is very large.\n```"}}, "id": "l3UwLcMFzN", "forum": "NdDlqHV1md", "replyto": "NdDlqHV1md", "signatures": ["ICLR.cc/2026/Conference/Submission18016/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18016/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission18016/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763110892776, "cdate": 1763110892776, "tmdate": 1763110892776, "mdate": 1763110892776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}