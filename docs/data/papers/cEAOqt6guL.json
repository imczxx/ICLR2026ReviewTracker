{"id": "cEAOqt6guL", "number": 8284, "cdate": 1758077379021, "mdate": 1759897794340, "content": {"title": "ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction", "abstract": "Reinforcement learning for training end-to-end autonomous driving models in closed-loop simulations is gaining growing attention. However, most simulation environments differ significantly from real-world conditions, creating a substantial simulation-to-reality (sim2real) gap. To bridge this gap, some approaches utilize scene reconstruction techniques to create photorealistic environments as a simulator. While this improves realistic sensor simulation, these methods are inherently constrained by the distribution of the training data, making it difficult to render high-quality sensor data for novel trajectories or corner case scenarios. Therefore, we propose \\textit{ReconDreamer-RL}, a framework designed to integrate video diffusion priors into scene reconstruction to aid reinforcement learning, thereby enhancing end-to-end autonomous driving training. Specifically, in \\textit{ReconDreamer-RL}, we introduce ReconSimulator, which combines the video diffusion prior for appearance modeling and incorporates a kinematic model for physical modeling, thereby reconstructing driving scenarios from real-world data. This narrows the sim2real gap for closed-loop evaluation and reinforcement learning. To cover more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA), which adjusts the trajectories of surrounding vehicles relative to the ego vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in). Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue of training data distribution, which is often biased toward simple straight-line movements. Experiments show that \\textit{ReconDreamer-RL} improves end-to-end autonomous driving training, outperforming imitation learning methods with a 5$\\times$ reduction in the Collision Ratio.", "tldr": "", "keywords": ["CV: Vision for Robotics; Autonomous Driving"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c38ed349bbbda92052ab3f4251e5729da69ff70d.pdf", "supplementary_material": "/attachment/ad63ae5d96c5b7053fd513259edfa674d939c70a.zip"}, "replies": [{"content": {"summary": {"value": "* This paper addresses the sim2real gap in training e2e autonomous driving models with RL.\n* It proposes ReconDreamer-RL, a framework that integrates video diffusion priors to create more realistic and robust simulation environments for RL training.\n* ReconSimulator: A simulator that uses 3DGS for reconstruction, a video diffusion prior for realistic appearance modeling of novel views, and a kinematic model.\n* Dynamic Adversary Agent (DAA): Generates challenging traffic scenarios by controlling surrounding vehicles.\n* Cousin Trajectory Generator (CTG): Enriches training data by synthesizing diverse ego trajectories to reduce simple straight driving.\n* Training happens in two stages: 1) IL for policy initialization using dataset from CTG / DAA. 2) Closed-loop RL stage where policy interacts with ReconSimulator and adversarial DAA.\n* Experiments demonstrate that this framework significantly outperforms baseline IL methods and achieves a new SOTA in RL + 3DGS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper focuses on a highly relevant research direction for AVs.\n* The problem of reducing the sim2real gap and training e2e models with RL is addressed holistically with improvements in the simulator’s fidelity (3DGS with video diffusion prior), focusing on long-tail problems (DAA), and leverage diverse ego trajectories (CTG).\n* Overall, the results demonstrate the superior performance of ReconDreamer-RL, e.g. a 3x reduction in collisions rates over baselines. And the method is validated on both nuScenes and the Waymo Open Dataset.\n* The ablations also clearly demonstrate the value that each component brings."}, "weaknesses": {"value": "* Lack of trajectory generation function implementation details. The authors write e.g. text-to-trajectory can be used but it seems that this paper lacks any additional details about the implementation used here. Given the importance of the DAA in the ablations, more details should be provided here.\n* Unclear computational cost of the iterative scene reconstruction. You provide information about fast rendering speeds (125 fps) but not about the scene reconstruction, which seems particularly expensive. Can you provide more details here, e.g. how many steps are usually needed?"}, "questions": {"value": "* How does the collision avoidance in the CTG work and how does CTG interact with the more adversarial behaviors created from the DAA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KRbbJzsu6R", "forum": "cEAOqt6guL", "replyto": "cEAOqt6guL", "signatures": ["ICLR.cc/2026/Conference/Submission8284/Reviewer_d1uV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8284/Reviewer_d1uV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755176422, "cdate": 1761755176422, "tmdate": 1762920215877, "mdate": 1762920215877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ReconDreamer‑RL, a framework for end‑to‑end autonomous‑driving policy training that couples (i) a reconstruction‑based simulator (ReconSimulator) enhanced by a video‑diffusion prior for appearance modeling with (ii) kinematic physical modeling of dynamic agents, and augments training with two data‑generation modules: a Dynamic Adversary Agent (DAA) that synthesizes corner‑case interactions and a Cousin Trajectory Generator (CTG) that diversifies ego trajectories. Training proceeds in two stages: imitation learning (IL) for initialization and reinforcement learning (RL) in closed loop. Experiments (nuScenes reconstruction as the main setting; Waymo for generalization) report large reductions in collision rate relative to IL baselines and sizable gains over the best prior RL baseline (RAD), while maintaining real‑time rendering speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper shows strong empirical gains on collision metrics across standard and corner‑case evaluations; large gap over both IL baselines and RAD.\n2. RL‑friendly simulator: 3DGS‑based with diffusion priors while retaining real‑time speed (Table 2). This addresses a common bottleneck in closed‑loop training."}, "weaknesses": {"value": "1. The policy is evaluated in the same simulator family that was fine‑tuned using diffusion priors, and many corner cases are generated by the paper’s own procedures (DAA, CTG). The real sim‑to‑real implications remain untested; no closed‑loop evaluation outside the authors’ reconstructions (e.g., Bench2Drive/nuPlan/CARLA‑v2 evaluations or any real‑vehicle trials).\n2. Metrics are limited for driving quality. Collisions are critical, but comfort (jerk/accel), traffic‑rule compliance, and route completion are absent. The paper relies on PDR/HDR (Table 1) for trajectory adherence on unedited clips and on collision‑only metrics for edited scenes (Appx. A.2). This may overemphasize safety at the expense of smoothness and legality."}, "questions": {"value": "Can you report RAD trained with the same DAA‑generated scenarios and/or CTG‑augmented data (keeping rendering identical), and vice‑versa, to isolate where the gains come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D9He1lqapf", "forum": "cEAOqt6guL", "replyto": "cEAOqt6guL", "signatures": ["ICLR.cc/2026/Conference/Submission8284/Reviewer_mSxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8284/Reviewer_mSxh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867357477, "cdate": 1761867357477, "tmdate": 1762920215351, "mdate": 1762920215351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ReconDreamer-RL, a framework designed to improve the training of end-to-end autonomous driving agents through reinforcement learning by addressing the critical simulation-to-reality (sim2real) gap. The authors identify a key limitation in existing scene reconstruction-based simulators: their inability to render high-quality sensor data for novel trajectories or rare corner cases not present in the original training data. To overcome this, the proposed method integrates video diffusion priors into the simulation process. The core of the framework is the ReconSimulator, which combines a diffusion-based appearance model with a kinematic physical model to reconstruct realistic driving scenarios. To further enhance the training process and cover a wider range of challenging situations, the paper introduces two additional components: the Dynamic Adversary Agent (DAA), which autonomously generates corner-case traffic scenarios by adjusting the trajectories of surrounding vehicles, and the Cousin Trajectory Generator (CTG), designed to mitigate the common bias in training data towards simple, straight-line movements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel method of integrating video diffusion priors directly into the reinforcement learning loop, creating a dynamic simulator that effectively addresses the sim2real gap.\n\n2. It proposes a well-structured solution with dedicated modules (DAA and CTG) to systematically generate adversarial corner cases and mitigate common training data biases.\n\n3. The framework demonstrates a significant and quantifiable performance improvement, achieving a 5x reduction in collision ratio over strong imitation learning baselines, providing clear evidence of its effectiveness."}, "weaknesses": {"value": "1. Lack of originality. Most techniques proposed in the paper have already been well studied in other works. Using video models to boost novel-trajcectory reconstruction performance is studied in many works such as Drivedreamer4d[1]; Decoupled static and dynamic scene representation is proposed in MTGS[2], OmniRe[3]; Adversary agent interaction is studied in a line of research featuring safty-critical interaction such as DiffScene[4].\n\n2. Missing details in the dynamic adversary agent section. How exactly is the trajectory generated and how to perform sanity check for the generated trajectory?\n\n[1] Drivedreamer4d: World models are effective data machines for 4d driving scene representation; Zhao, et.al.\n\n[2] MTGS: Multi-Traversal Gaussian Splatting; Li, et.al\n\n[3] OmniRe: Omni Urban Scene Reconstruction; Chen, et.al.\n\n[4] DiffScene: Guided Diffusion Models for Safety-Critical Scenario Generation"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OBSJm2jhR1", "forum": "cEAOqt6guL", "replyto": "cEAOqt6guL", "signatures": ["ICLR.cc/2026/Conference/Submission8284/Reviewer_BUDD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8284/Reviewer_BUDD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874024040, "cdate": 1761874024040, "tmdate": 1762920214640, "mdate": 1762920214640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes ReconDreamer-RL, a two-stage framework that strengthens end-to-end autonomous-driving training by bridging imitation learning with reinforcement learning inside a photorealistic, editable simulator. The framework starts with ReconSimulato to fuse video-diffusion priors for high-quality novel-view rendering with a kinematic model for physically valid agent motion. On top of sensor simulator, a Dynamic Adversary Agent (DAA) auto-generates corner-case interactions (e.g., cut-ins) and a Cousin Trajectory Generator (CTG) diversifies ego behaviors to counter dataset bias. Together these components reduce the sim-to-real gap and improve closed-loop robustness, yielding large drops in collision metrics over IL/RL baselines while maintaining fast rendering suitable for RL training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Significance and Scope.** The paper addresses an important and timely problem in autonomous driving—closed-loop, end-to-end learning—by coupling photorealistic simulation with reinforcement learning in a way that directly targets deployment-relevant failures.\n- **Comprehensive Qualitative Evaluation.** The study presents rich qualitative evidence across multiple benchmarks and driving scenarios, helping to substantiate generality and offering interpretable insights through scenario visualizations and case studies.\n- **Strong Empirical Gains with Targeted Guidance.** Leveraging the Dynamic Adversary Agent (DAA) and Cousin Trajectory Generation (CTG), ReconDreamer-RL achieves consistent improvements over RAD and other E2E baselines on key safety and efficiency metrics in closed-loop settings.\n- **Well-Designed Ablations.** Ablation studies are carefully constructed to isolate the contributions of ReconSimulator, DAA, and CTG, demonstrating each component’s necessity and the synergy of the full system."}, "weaknesses": {"value": "Despite the strengths mentioned above, I have some significant concerns on the technical contribution claimed by the paper, listed in this and the question section below:\n\n- **Unclear presentation of the ReconSimulator**. The core technical contribution of this work comes from this ReconSimulator, however, the details of this simulator is unclear. For instance, How is the diffusion prior incorporated? From section 3.3, it seems that the ReconSimulator use a training checkpoint rendered by 3DGS and refined by DriveRestorer. What if we directly utilize 3DGS with decomposed foreground and background, just like what is done in HUGISM [1]? \n- **Limited contribution in the RL algorithm**. The title of this paper is ReconDreamer-RL, yet the novelty and analysis on the RL algorithm is missing in the main text. The authors mention in the appendix that they use PPO algorithm following RAD, which seems to be based on RAD with some incremental modification in synthetic data on the sensor simulation side. See more in the question part. \n\n> [1] Zhou, Hongyu, et al. \"Hugsim: A real-time, photo-realistic and closed-loop simulator for autonomous driving.\" *arXiv 2024."}, "questions": {"value": "As mentioned above, I believe there are lots of merits in the paper, while I also have concerns about the key technical contribution, since they are not well-presented in the current manuscript. I will consider adjusting my rating later if the weaknesses and questions are properly addressed later. \n\n- Do the authors empirically observe any forgetting issues when training on the synthetic data, if tested back into the real-domain with nominal driving cases?\n- In line 265, what is the $f(\\cdot)$ specifically defined in the DAA module? What is the sample scale needed for DAA to get interesting adversaries. If we conduct random samples, would most of the samples remain boring without challenging behaviors? \n- In Figure 6, the author mentions cousin-nuScenes created by CTG from the original nuScenes dataset. This is primarily used to cold-start the Rl algorithm. In the online rollout in the PPO, what is the data diversity, sample scale, as well as their impact to the final perfomance? An ablation study on the final performance vs sample complexity will be interesting. \n- Is DAA and CTG used to only cold-start the training data for RL, or also incorporated during the RL process? If they are both used during online learning, how are they used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cbNWuXYPUj", "forum": "cEAOqt6guL", "replyto": "cEAOqt6guL", "signatures": ["ICLR.cc/2026/Conference/Submission8284/Reviewer_AWCj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8284/Reviewer_AWCj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148988629, "cdate": 1762148988629, "tmdate": 1762920214161, "mdate": 1762920214161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}