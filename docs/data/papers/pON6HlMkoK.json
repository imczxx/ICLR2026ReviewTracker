{"id": "pON6HlMkoK", "number": 17457, "cdate": 1758276266849, "mdate": 1762927726635, "content": {"title": "Enhancing LoRA with Shared Random-Span Augmentation for Parameter-Efficient Visual Adaptation", "abstract": "Low-rank Adaptation (LoRA) efficiently adapts large pre-trained models to downstream tasks by learning low-rank adapters, significantly reducing computational and memory costs without sacrificing performance. Recent studies highlight the promise of rank adaptation methods in improving the flexibility and performance of LoRA. Grounded in Singular Value Decomposition (SVD) theory, these methods decompose the weight update into parameterized unitary matrices and learnable scaling coefficients, thereby allowing dynamic rank allocation of adapters based on coefficients. However, the parameterized construction of unitary matrices presents a significant computational bottleneck. To address this limitation, we propose Shared Random-Span Augmentation (SRSA), a novel Parameter-Efficient Fine-Tuning (PEFT) method that replaces the learnable unitary matrices with fixed, layer-shared random matrices. Our method facilitates flexible rank adaptation by learning scaling vectors within the shared random space, while maintaining parameter and memory efficiency. We provide both empirical and theoretical evidence to demonstrate the feasibility of substituting the unitary matrices with a shared random matrix. To evaluate the representational ability of our method, we conduct extensive experiments on various visual tasks. The results demonstrate that our method achieves compelling adaptation performance.", "tldr": "We propose a noval PEFT method that enhances the expressive capability of LoRA with a rank adaptation mechanism.", "keywords": ["Parameter-efficient Fine-tuning; Low-rank Adaptation; Random Matrix; Parameter Sharing"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6b5615de15827298c4fea3d12aea83fc03f97196.pdf", "supplementary_material": "/attachment/9f5e20d4b74e9e530e121b537d9d19580cd05b87.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Shared Random-Span Augmentation (SRSA), a novel parameter-efficient fine-tuning method for vision pre-trained models. SRSA addresses the rigidity of fixed-rank LoRA by replacing learnable unitary matrices with frozen, layer-shared random matrices and learning only diagonal scaling vectors, while retaining a lightweight rank-1 LoRA adapter to capture directional information. This combination preserves LoRA’s low-parameter efficiency while enabling flexible rank adaptation and enhanced representational capacity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a simple yet effective enhancement to LoRA by combining fixed, layer-shared random matrices with a lightweight rank-1 LoRA adapter,  achieving flexible rank adaptation.\n2. The paper provides theoretical analysis showing that, under a fixed orthogonal basis, learning only a diagonal scaling matrix can optimally approximate full-rank weight updates while preserving their spectral properties, thereby justifying the design of using frozen shared random matrices with learnable scaling in SRSA."}, "weaknesses": {"value": "1. The proposed SRSA method incorporates a rank-1 LoRA adapter, which, according to the ablation studies, plays a crucial role in achieving the reported performance. Although the authors claim that the rank-1 LoRA adapter is designed to capture directional information, the underlying mechanism remains unclear and requires further clarification. Moreover, methods shown in Figure 2, such as VeRA and RandLoRA, could also be combined with a similar rank-1 LoRA adapter, potentially achieving comparable or even better results; thus, additional experiments are needed to validate SRSA’s unique contribution.\n\n2. The experimental baselines are somewhat confusing. The paper does not include direct comparisons between SRSA and similar methods such as VeRA and RandLoRA shown in Figure 2. Even if these approaches were originally not evaluated on visual tasks, the authors should reproduce them under comparable experimental settings for fairness. Furthermore, several important visual fine-tuning methods, including MLAE [1], GLoRA [2], SPT [3], RepAdapter[4], and NoAH [5], are not discussed or compared, even though they report results on VTAB-1k. These works should be properly cited and experimentally compared, and the authors should note that the average accuracy in these papers is calculated differently from that in SRSA when making performance comparisons.\n\n3. The ablation in Table 6 (Section 4.6) is incomplete. Beyond the standard configuration involving q and v, the authors should individually examine the effects of applying SRSA to k, o, fc1, and fc2 to determine which parameters contribute most to performance.\n\n------\n\n[1] MLAE: Masked LoRA Experts for Visual Parameter-Efficient Fine-Tuning. https://arxiv.org/abs/2405.18897\n\n[2] One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning. https://arxiv.org/abs/2306.07967\n\n[3] Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning. https://arxiv.org/abs/2303.08566\n\n[4] Towards Efficient Visual Adaption via Structural Re-parameterization. https://arxiv.org/abs/2302.08106\n\n[5] Neural Prompt Search. https://arxiv.org/abs/2206.04673"}, "questions": {"value": "1. Although I understand that SRSA is proposed primarily for vision pre-trained models, the paper does not provide evidence that the method is limited to visual tasks. Considering that parameter-efficient fine-tuning techniques are more widely applied in large language models, the authors are encouraged to extend SRSA to models such as LLaMA3-8B or Qwen-2.5-7B and report corresponding results.\n\n2. The paper should include a comprehensive comparison of computational efficiency between SRSA and other baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "91ShhvHQuM", "forum": "pON6HlMkoK", "replyto": "pON6HlMkoK", "signatures": ["ICLR.cc/2026/Conference/Submission17457/Reviewer_J7VV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17457/Reviewer_J7VV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760953748712, "cdate": 1760953748712, "tmdate": 1762927342372, "mdate": 1762927342372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "qxZUMvBQZS", "forum": "pON6HlMkoK", "replyto": "pON6HlMkoK", "signatures": ["ICLR.cc/2026/Conference/Submission17457/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17457/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762927725543, "cdate": 1762927725543, "tmdate": 1762927725543, "mdate": 1762927725543, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a parameter-efficient fine-tuning (peft) method that combines rank-1 LoRA with a new additive term that calculates $W_r \\operatorname{diag}(\\lambda) W^T_r$ where $W$ is a random (optionally: orthonormal) matrix that is shared between layers and kept frozen during training. This method reduces trainable parameters wrt. LoRA, because only the rank-1 LoRA parameters and $\\lambda \\in \\mathbb{R}^d$ are trainable, i.e., the count of trainable parameters is $3d$ per parameter, whereas rank-$r$ LoRA requires $2rd$ parameters. Experiments on vision tasks (FGVC and VTAB) for a range of pretrained image encoders yield improved performance compared to a number of baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**(S1)** The proposed method effectively enables full-rank weight deltas while even reducing the number of trainable parameters compared to LoRA.\n\n**(S2)** The proposed method retains favorable properties of LoRA, such as the possibility to merge weights with pretrained weights for effective deployment\n\n**(S3)** The limitations section is honest and upfront about the limitations of the proposed method. While this does not mitigate the limitations themselves, this transparency should be held in favor of the paper.\n\n**(S4)** The paper is technically sound, and the supplementary material reports a detailed training setup and hyperparameters. The set of baselines is comprehensive and sufficient.\n\n**(S5)** Visuals and explanations in the paper are clear and help in understanding the proposed method."}, "weaknesses": {"value": "**(W1)** The main weakness is the limited experimental evaluation: The proposed method is only evaluated on vision benchmarks. However, it is standard to evaluate on a range of tasks and modalities, including language, especially LLMs. The method clearly is as general as LoRA, so the evaluation should match previous works such as LoRA, DoRA, or VeRA.\n\n**(W2)** The paper does not include an analysis of the additional runtime and memory required by the (frozen) full rank matrices introduced by the proposed method. The computational aspect is important to consider for peft methods.\n\n**(W3)** As also discussed in the paper's limitations section, the method's weight sharing limits reusability between layers of different shapes. The paper offers perspectives on how to mitigate this issue, but it remains a disadvantage compared to methods like LoRA that do not have this constraint.\n\n**(W4)** The benefits of different effective ranks per layer that can be selected dynamically are used as motivation, but there is no analysis of whether the proposed method effectively solves the problem. For example, how will the distribution of column-vector cosine similarities look for SRSA? Does SRSA effectively enable better geometry of the learnt weight delta?\n\n**(W5)** Advantages over baselines are marginal (Tab. 1 and Tab. 2). For FGVC, an additional problem is that the benchmark is approaching saturation, so it becomes hard to observe significant differences between methods. Together with the missing evaluation of computational requirements, this does not allow for concluding with certainty that the proposed method is the best choice among the presented baselines.\n\n**(W6)** Theorem 3.1 is correct, but the proof in Appendix C.1 is not complete. In particular, the transition from Eq. 12 and Eq. 13 as stated does not directly follow. A clearer way is to first show that $\\lVert W - QEQ^T\\rVert_F^2 = \\lVert Q^TWQ - E\\rVert_F^2$  and then expand the definition of Frobenius norm as sum of squared entries.  The off-diagonal elements do not affect the minimization, i.e. we only have to find the minimum $\\sum_i \\left(A_{ii} - E_{ii}\\right)^2$ where $A = Q^TWQ$, which directly gives the theorem.\n\n**(W7)** Minor weakness: There are several typos throughout the paper, for example: \"ro\" (line 307), \"dowm\" (Fig 2d), and \"Structed\" (Tab. 3-6, and Fig. 3). Please carefully check the paper for such typos."}, "questions": {"value": "My main suggestion is to significantly expand the evaluation:\n  * Include additional tasks and modalities, especially language and LLMs. Other interesting objections are vision tasks beyond classification, such as segmentation or few-shot finetuning of diffusion models. There are many options described in the literature.\n  * Discuss and compare the effective computational requirements of the proposed method (memory, speed, FLOPs) with respect to baselines\n  * Clarify if the proposed method effectively enables flexible rank allocation in different layers, i.e. if it directly addresses the motivation. This will significantly improve the storyline.\n\nI think addressing these points is necessary to adequately appreciate the strengths of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "myLiQuwnOu", "forum": "pON6HlMkoK", "replyto": "pON6HlMkoK", "signatures": ["ICLR.cc/2026/Conference/Submission17457/Reviewer_7BqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17457/Reviewer_7BqD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485407397, "cdate": 1761485407397, "tmdate": 1762927341960, "mdate": 1762927341960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new PEFT method building upon LoRA. The paper claims to improve LoRA's expressiveness with a small number of additional parameters. The core method focuses on learning an additional diagonal matrix that is multiplied by shared orthonormal matrices. The paper presents results on several vision benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper focuses on the relevant problem of improving the expressiveness of LoRA. The paper attempts to achieve this by increasing the rank of the resultant updates. The paper proposes using a high-rank matrix parameterized only by its diagonal values."}, "weaknesses": {"value": "1. Theorem 3.1 is a well-known result in matrix analysis. A more general problem was solved using a similar technique in established works like [1]. The paper should tone down the mathematical claims involving this result.\n\n2. The experimental evaluation presented in the paper is limited. The paper evaluates on small-scale vision benchmarks (often with 1000 samples) using ViTs or their variants. The paper should evaluate on other domains like language, using larger-scale LLMs (at least 7B parameters). The larger models benefit the most from PEFT-based training. \n\n3. The empirical results reported in the paper seem pretty weak. Across Tables 1 & 2, the mean gain in performance over the baselines is only 0.2-0.5% in accuracy. Are these results statistically significant?\n\n4. I'm slightly confused whether the comparison with baselines is far. The proposed method uses a larger number of parameters due to the introduction of the diagonal matrix. In Line 358, the paper mentions \"comparable parameter budget\". For a fair comparison, the method should have the same number of parameters. Otherwise, the results should be presented as a tradeoff plot between the parameter count and performance.\n\n5. The paper misses out on comparison with a bunch of the latest baselines, including works mentioned in the related works section, like AdaLoRA and DyLoRA. The paper should also compare with [2], which focuses on having high-rank updates by using structured matrices.\n\n[1] A generalized solution of the orthogonal procrustes problem. Schonemann et al. 1966\n\n[2] Structured Unrestricted-Rank Matrices for Parameter-Efficient Fine-tuning. Sehanobish et al, 2024."}, "questions": {"value": "Please respond to the questions in the previous sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WMAgoSN3j4", "forum": "pON6HlMkoK", "replyto": "pON6HlMkoK", "signatures": ["ICLR.cc/2026/Conference/Submission17457/Reviewer_P2Xp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17457/Reviewer_P2Xp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881545379, "cdate": 1761881545379, "tmdate": 1762927341509, "mdate": 1762927341509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In my understanding, this method enhances LoRA by making rank adaptation lightweight and efficient. First, the LoRA framework is extended with a rank-adaptation mechanism, but instead of learning large unitary matrices, the authors replace them with fixed random matrices. Then they add a sharing strategy so that the same random matrix is reused across layers, reducing memory overhead. Finally, they introduce a small learnable scaling vector to adjust the influence of these random spans, which provides flexibility without heavy computation. This design keeps LoRA’s directional updates while adding adaptability for diverse tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper has the following strengths:\n\n1. Presentation Quality:\nThe paper uses well-designed figures and tables that make comparisons and results easy to interpret.\n\n\n2. Presentation Clarity:\nConcepts like LoRA limitations, random-span augmentation, and rank adaptation are explained in a clear and structured way, making the method understandable.\n\n3. Motivation:\nThe method is well-motivated.\n\n4. Performance within the Experimental Setup:\nExperiments across multiple benchmarks (FGVC and VTAB-1k) show consistent improvements over LoRA and other baselines, validating the effectiveness of the proposed approach, given the experimental setup provided by authors."}, "weaknesses": {"value": "W1. Novelty:\nThe paper does not convincingly differentiate itself from prior LoRA methods. Many works already train diagonal singular value matrices or use adaptive ranks for flexibility, which are not cited.\n\n\nA. SVD diagonal matrix methods:\nThe authors don't mention(let alone benchmark against) methods like SVDiff, LoRA-X (which use a learnable diagonal matrix, and Use the SVD decomposition of the model weights). Without these comparisons, it’s unclear if SRSA is better in accuracy or efficiency.\n\nB. Rank Gating/Adaptive Ranking\nApart from  AdaLoRA, DyLoRA, methods such as FouRA, and SoRA (Soft Low-Rank Adaptation) also target the same problem of rank flexibility. There is no comparison against any of these methods.\n\nW2. Experimental Scope\nAll experiments are limited to vision classification tasks (FGVC and VTAB-1k). The paper does not test on generative tasks (e.g., image generation or diffusion models), which are major LoRA application areas.\n\nRelated Work (Please see all related work which mentions these methods, as a lot of papers train on the diagonal space of SVD):\nSVDiff: Compact Parameter Space for Diffusion Fine-Tuning\nLoRA-X: Bridging Foundation Models with Training-Free Cross-Model Adaptation\nFouRA: Fourier Low-Rank Adaptation\nSoRA: Sparse Low-rank Adaptation of Pre-trained Language Models"}, "questions": {"value": "Why is a random matrix better than matrix decomposition of the base model weights (as shown in SVDiff)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5sfUJhLgom", "forum": "pON6HlMkoK", "replyto": "pON6HlMkoK", "signatures": ["ICLR.cc/2026/Conference/Submission17457/Reviewer_y2vd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17457/Reviewer_y2vd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942277701, "cdate": 1761942277701, "tmdate": 1762927341170, "mdate": 1762927341170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}