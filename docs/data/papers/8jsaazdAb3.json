{"id": "8jsaazdAb3", "number": 7834, "cdate": 1758038055067, "mdate": 1759897828576, "content": {"title": "WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent", "abstract": "Web agents such as deep research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains largely text-centric, overlooking visual information in the real world. This makes multimodal deep research highly challenging, as such agents require much stronger perceptual, logical, and knowledge-based reasoning abilities, as well as proficiency in more sophisticated tools. To address this limitation, we introduce WebWatcher, a multimodal agent for deep research with enhanced visual-language reasoning capabilities. It uses high-quality synthetic trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with the style of BrowseComp that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher outperforms or matches proprietary baselines, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks.", "tldr": "We present WebWatcher, a multimodal web agent that learns from synthetic trajectories and reinforcement learning to achieve state-of-the-art performance in complex information-seeking tasks requiring joint visual and textual reasoning.", "keywords": ["Multimodal Agent", "Web Agent", "Deep Research", "Visual Question Answering (VQA)", "Tool-augmented Reasoning", "Multimodal Information-Seeking Benchmark"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff7fa51448b6084da5a2f06bea5bb025b220ca10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the limitation of existing text-centric deep research agents by introducing WebWatcher, a multimodal (vision-language) agent designed for complex information-seeking tasks. WebWatcher integrates visual-language reasoning with multi-tool interaction (e.g., Web Image Search, OCR, Code Interpreter) and uses a two-stage training pipeline: (1) cold-start supervised fine-tuning (SFT) on high-quality synthetic tool-use trajectories, and (2) reinforcement learning via Group-Relative Policy Optimization (GRPO) to enhance generalization.\n\nTo evaluate multimodal agents, the authors propose BrowseComp-VL, a benchmark extending the text-only BrowseComp to vision-language tasks. It includes obfuscated, multi-step queries across 5 domains (e.g., Natural Science, Entertainment) and two difficulty levels, requiring cross-modal reasoning and tool planning.\n\nExperimental results show WebWatcher outperforms proprietary baselines (e.g., GPT-4o, Gemini-2.5-flash) and open-source agents on four high-difficulty benchmarks (HLE, LiveVQA, BrowseComp-VL, MMSearch) and performs competitively on the perception-focused SimpleVQA, demonstrating its versatility in both knowledge-intensive and visual reasoning tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper fills a critical gap in existing deep research agents, which are largely text-bound. By integrating vision-language reasoning with flexible multi-tool use, WebWatcher moves beyond template-driven multimodal pipelines (e.g., OCR-only visual agents) to enable adaptive, complex problem-solving. \n\n2. The introduction of BrowseComp-VL addresses the lack of benchmarks for multi-step, obfuscated vision-language tasks—unlike existing VQA datasets (e.g., SimpleVQA) that focus on shallow perception.\n\n3. Evaluations span 5 diverse benchmarks, comparing WebWatcher to 3 types of baselines (direct inference, prompt workflows, reasoning models) to isolate the impact of multimodal reasoning and tool use. Ablations (e.g., cold-start vs. instruct initialization, Pass@k analysis) validate key design choices."}, "weaknesses": {"value": "1. The data construction methodology is inherited from webdancer and websailor v1. Browsecomp-vl is one of the few benchmarks that require multimodal capabilities. However, this dataset has a significant weakness: the incorporation of multimodality merely involves replacing entities in unimodal questions with their visual representations. As a result, the problem solver only needs to disambiguate the entities referred to by the images, while the remainder of the process is essentially no different from solving unimodal information retrieval problems, such as the original browsecomp.\n\n2. The paper attributes WebWatcher’s success to \"enhanced visual-language reasoning,\" but it does not isolate the impact of individual visual components."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hvoj1BMuRa", "forum": "8jsaazdAb3", "replyto": "8jsaazdAb3", "signatures": ["ICLR.cc/2026/Conference/Submission7834/Reviewer_sPg8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7834/Reviewer_sPg8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708378996, "cdate": 1761708378996, "tmdate": 1762919881336, "mdate": 1762919881336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing vision-language agents struggle with complex, multi-step research tasks that require integrating visual and textual information from the web. \nThis paper introduces WebWatcher, a multimodal agent that uses a novel data generation pipeline and a hybrid training strategy to enhance its reasoning and tool-use capabilities. \nWebWatcher is trained on high-quality, synthesized trajectories and further refined with reinforcement learning, enabling it to flexibly use tools like web search, OCR, and a code interpreter. \nThe agent's superiority is demonstrated through strong performance on several challenging VQA benchmarks, including the newly proposed BrowseComp-VL, where it outperforms existing open-source and proprietary systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper details a structured, multi-stage pipeline to create complex Vision-Question Answering (VQA) pairs from text-based sources. This process begins by generating multi-hop textual questions from hyperlink graphs to ensure reasoning depth. It then grounds these questions in authentic web images and transforms them into VQA format. To maintain high quality, the pipeline incorporates a two-stage filtering process using \"Selector\" and \"Examiner\" models to validate contextual alignment and visual plausibility.\n2. The research employs and validates a two-phase training approach that uses Supervised Fine-Tuning (SFT) as a \"cold start\" before applying Reinforcement Learning (RL). The paper provides an analysis demonstrating that this SFT stage, which uses pre-generated high-quality trajectories, is critical for successful training. Experimental results show that an agent without the SFT cold start fails to achieve meaningful performance during RL training, whereas the SFT-initialized agent shows significant initial scores and subsequent improvement."}, "weaknesses": {"value": "1.  The abstract claims the model \"outperforms or matches proprietary baselines\" across four VQA benchmarks. [cite_start]However, on the HLE benchmark, the model's overall average accuracy (13.6%) is slightly lower than proprietary reasoning models like Gemini-2.5-Pro (15.8%) and o4-mini (16.0%). The claim of superiority should be qualified to specify the benchmarks where this holds true, as it is not universal across all tested environments\n2. The training methodology includes a trajectory filtering rule that removes any trajectory with fewer than three tool calls. This could introduce an inductive bias that favors longer, more complex reasoning chains. The paper lacks an ablation study to determine if this bias leads to redundant or inefficient tool usage on tasks that do not inherently require multi-step interactions.\n3. The difficulty of the BrowseComp-VL benchmark is increased at Level 2 through \"obfuscated entities and attributes\". This method of \"fuzzing\" questions may introduce confounding variables, conflating the challenge of multi-modal reasoning with that of linguistic ambiguity and retrieval noise. The paper does not provide a human baseline performance or a detailed error analysis to disentangle these factors.\n\n[Minor]\n1. The evaluation of answer correctness relies on the \"LLM-as-Judges\" approach. This methodology is subject to potential biases, especially if the judge model shares an architectural family with the models being tested. The paper does not present results on inter-rater reliability with human experts or robustness checks using different judge models to validate the evaluation framework.\n2. The Pass@k analysis demonstrates that performance on HLE improves significantly with more sampling, rising to 41.9% at k=32. However, the paper fails to quantify the inference cost and latency associated with this multi-rollout strategy. Without a cost-benefit analysis, the practical viability of achieving these higher scores in real-world applications remains unevaluated."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6d3lg9Z9i3", "forum": "8jsaazdAb3", "replyto": "8jsaazdAb3", "signatures": ["ICLR.cc/2026/Conference/Submission7834/Reviewer_ehqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7834/Reviewer_ehqj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867043667, "cdate": 1761867043667, "tmdate": 1762919880358, "mdate": 1762919880358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **WebWatcher**, a multimodal deep-research agent designed to combine vision-language reasoning with dynamic tool use for complex information-seeking tasks. The system integrates five external tools—web text search, web image search, page visiting, OCR, and code interpreter—and learns tool-augmented reasoning through a two-stage pipeline: (1) **automated trajectory generation** for supervised cold-start training and (2) **group-relative policy optimization (GRPO)** reinforcement learning for fine-tuning. To evaluate multimodal research ability, the authors introduce **BrowseComp-VL**, an extension of the BrowseComp benchmark into visual domains, requiring cross-modal retrieval and multi-step reasoning. Experiments on **five challenging benchmarks** (HLE, LiveVQA, BrowseComp-VL, MMSearch, and SimpleVQA) show that WebWatcher-32B consistently outperforms both open-source and proprietary reasoning agents under comparable model sizes."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "– **Well-motivated and timely contribution:** The paper clearly identifies a missing dimension in current deep-research agents—robust multimodal reasoning that jointly leverages textual and visual information.\n\n– **Benchmark creation:** BrowseComp-VL fills a notable evaluation gap by introducing visually grounded, obfuscated, and multi-hop reasoning tasks. The construction process (Levels 1–2, entity masking, selector–examiner filtering) is rigorous and convincing.\n\n– **Comprehensive experiments:** Results on five datasets demonstrate consistent superiority of WebWatcher over both direct-inference LMMs (e.g., GPT-4o, Gemini 2.5) and workflow baselines. The performance scaling from 7B → 32B models is clearly shown.\n\n– **Clarity and completeness:** The paper is well written, with careful mathematical formalization, clean figures, and transparent dataset statistics."}, "weaknesses": {"value": "– **Limited novelty in learning algorithms:** The overall architecture builds on established paradigms (ReAct for trajectory structure, GRPO for RL optimization). Innovation lies in the integration rather than in a fundamentally new learning mechanism.\n\n– **Cost and efficiency reporting:** The paper does not specify training compute (GPU hours, wall-clock time) or inference latency. Quantitative comparisons with other open-source agents (e.g., WebDancer, WebSailor, WebShaper) would clarify efficiency and resource footprint.\n\n– **Scalability risks:** The pipeline depends on GPT-4o for trajectory annotation, which may limit reproducibility or increase cost at scale; discussion of potential automation or open-source substitutes would be valuable."}, "questions": {"value": "Please see the weakness above.\n1. How many tool calls or reasoning steps typically occur before convergence, and do you encounter diminishing returns beyond a certain depth?\n2. Do you encounter issues with the shared memory or context growing unboundedly during long reasoning sequences? If so, how is this mitigated, and what is the computational or memory cost of maintaining such context?\n3. It would also be helpful if the authors could provide qualitative visualizations comparing WebWatcher’s reasoning paths to those of baseline systems to highlight its hierarchical and multimodal advantages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rdIZCMC6c9", "forum": "8jsaazdAb3", "replyto": "8jsaazdAb3", "signatures": ["ICLR.cc/2026/Conference/Submission7834/Reviewer_cUPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7834/Reviewer_cUPf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947232633, "cdate": 1761947232633, "tmdate": 1762919879661, "mdate": 1762919879661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WebWatcher, a multimodal deep research web agent designed to perform complex reasoning across both visual and textual information. It combines large language models with multiple external tools to handle information-seeking tasks that require cross-modal understanding and planning. The authors propose a new benchmark, BrowseComp-VL, extending previous text-based benchmarks into the visual domain. They construct synthetic multimodal question-answering dataset and use a two-stage training process: SFT and GRPO. Experiments show that WebWatcher achieves competitive performance on several benchmarks including HLE, LiveVQA, and MMSearch."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses an important area—multimodal deep research—by trajectory data creation and two-stage training process including SFT and RL.\n* The proposed automated trajectory generation pipeline offers a scalable way to construct training samples for multi-model deep research."}, "weaknesses": {"value": "* Although the paper mentions that the proposed benchmark was verified by PhD-level experts at line 144, it does not provide details on the verification process or quantitative reliability measures (e.g., Cohen’s κ), making the evaluation reliability insufficient.\n* Several key baselines are missing, such as o3 and GPT-4.1, which limits the completeness and fairness of the performance comparison."}, "questions": {"value": "* What are the performance results of o3 and GPT-4.1 in Table 1 and Table 2?\n* Please provide more details about the manual verification process during the benchmark construction phase, including how the PhD-level experts conducted validation and whether any quantitative reliability metrics (e.g., inter-rater agreement) were reported."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iVsWNWevMH", "forum": "8jsaazdAb3", "replyto": "8jsaazdAb3", "signatures": ["ICLR.cc/2026/Conference/Submission7834/Reviewer_xaDU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7834/Reviewer_xaDU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979863573, "cdate": 1761979863573, "tmdate": 1762919879281, "mdate": 1762919879281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}