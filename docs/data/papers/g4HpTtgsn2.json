{"id": "g4HpTtgsn2", "number": 16908, "cdate": 1758270165421, "mdate": 1759897210869, "content": {"title": "Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception", "abstract": "Existing methods for evaluating the harmfulness of content generated by large language models (LLMs) have been well studied. However, approaches tailored to multimodal large language models (MLLMs) remain underdeveloped and lack depth. \n  This work highlights the crucial role of visual information in moderating content in visual question answering (VQA), a dimension often overlooked in current research. \n  To bridge this gap, we introduce Falcon, a large-scale vision-language safety dataset containing 57,515 VQA pairs across 13 harm categories. The dataset provides explicit annotations for harmful attributes across images, instructions, and responses, thereby facilitating a comprehensive evaluation of the content generated by MLLMs. In addition, it includes the relevant harm categories along with explanations supporting the corresponding judgments. We further propose FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B using the Falcon dataset. Experimental results demonstrate that FalconEye reliably identifies harmful content in complex and safety-critical multimodal dialogue scenarios. It outperforms all other baselines in overall accuracy across our proposed Falcon-test dataset and two widely-used benchmarks—VLGuard and Beavertail-V, underscoring its potential as a practical safety auditing tool for MLLMs.", "tldr": "We construct a fine-grained multimodal safety dataset and train a safety evaluation model on it.", "keywords": ["Benchmarks", "Multimodal", "Safety Evaluation", "Visual Question Answering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b05d69a8722246f4d52b1dee7105e20952289f1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper shows that visual context is crucial for moderating visual question answering, yet is often overlooked. To address this, this paper introduces Falcon, a 57,515-pair vision–language safety dataset spanning 13 harm categories, with explicit labels for harmful attributes in images, instructions, and responses, as well as category tags and rationales. Moreover, this paper also presents FalconEye, a Qwen2.5-VL-7B evaluator fine-tuned on Falcon. FalconEye reliably detects harmful content on three benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is good and easy to understand.\n2. Based on the experiments, the detector trained on the authors’ dataset outperforms existing baselines, even beating GPT-4o.\n3. The experiments are comprehensive, covering multiple baselines and evaluation datasets."}, "weaknesses": {"value": "1. In Section 3.1 (Generating VQA pairs), the authors say “we manually carry out several rounds of cleaning on the obtained VQA pairs to filter out low-quality and disorganized data. In total, we obtain 57,515 VQA pairs,” but they omit key details: How many annotators were there? What are their backgrounds? What guidelines were used to filter low-quality samples? The same issue appears in the “Human Safety Annotations” stage.\n2. In the provided dataset, the training set is fully auto-labeled by Qwen2.5-VL-72B-Instruct-AWQ, with no necessary quality control.\n3. The contribution is fairly incremental: it aggregates several existing datasets, uses an open-source model for auto-labeling, then manually labels a 1,800-sample test set, and finally fine-tunes models on this dataset to compare with existing baselines."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2oCtD2i8lJ", "forum": "g4HpTtgsn2", "replyto": "g4HpTtgsn2", "signatures": ["ICLR.cc/2026/Conference/Submission16908/Reviewer_6Hmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16908/Reviewer_6Hmy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761228264377, "cdate": 1761228264377, "tmdate": 1762926936867, "mdate": 1762926936867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission\n- constructs a training set for safety classification of VQA inputs and outputs by pooling input samples from existing datasets, generating responses and auto annotating inputs and responses from existing open source VLMs.\n- Additionally it provides a test set of human annotations for 1.8 k samples.\n- It fine-tunes Qwen-2.5-VL-7B on the constructed data and shows improvements in safety classification on the provided test set, as well as Beavertails-V and VLGuard datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper reasonably demonstrates that distilling the annotations from the large Qwen-2.5-VL-72B-AWQ on the pooled training set improves the performance of Qwen2.5-VL-7B in safety classification for input and responses on the given VQA datasets.\n- It provides a set of 1.8k samples with human annotations"}, "weaknesses": {"value": "- The proposed dataset has little novelty, it is pooling samples from existing datasets and generating responses and labels from existing open source models.\n- The labelling scheme of the dataset is lacking depth and nuance. E.g.\n    - The dataset completely ignores compositional effects between text and images in the input label, where the label of the VQA pair is different to the label of the individual modalities (see. [1-4] for recent examples discussing this)\n    - The dataset does not seem to have a notion of inputs that are related to harmful topics but are not necessarily seeking a harmful response (see [4, 5] for recent discussion).\n- At least parts of the harm categories are poorly motivated, e.g. the decision “both \"Child Abuse\" and \"Animal Abuse\" pertain to forms of physical or psychological harm. We merge them under the broader category of \"Abuse\", which also encompasses other potential types of abusive behavior.”  is concerning, as both of these harm categories have dramatically different levels of risk in real world deployments and will almost certainly require different policy treatment.\n- In summary, I don’t think the dataset makes a meaningful conceptual or practical contribution to the community.\n- No analysis of the quality of the human annotations. E.g. what was the level of agreement between multiple human graders and how well are the auto-annotations from  Qwen-2.5-VL-72B-AWQ aligned with the human annotations on the test set?\n\n[1] Wang, Siyin, et al. \"Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Model.\" arXiv preprint arXiv:2406.15279 (2024). \n\n[2] Röttger, Paul, et al. \"MSTS: A Multimodal Safety Test Suite for Vision-Language Models.\" arXiv preprint arXiv:2501.10057 (2025).\n\n [3] Lee, Youngwan, et al. \"HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model.\" arXiv preprint arXiv:2506.04704 (2025).\n\n [4] Palaskar, Shruti, et al. \"VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety.\" arXiv preprint arXiv:2510.18214 (2025).\n\n [5] Yuan, Yuan, et al. \"From hard refusals to safe-completions: Toward output-centric safety training.\" arXiv preprint arXiv:2508.09224 (2025)."}, "questions": {"value": "- Human annotations\n    - What is the agreement between auto annotations and human annotations -> Could be measured on the test set.\n    - What is the agreement across humans annotators?\n- Baselines\n    - How were GPT4o and Qwen2.5-VL-7B prompted? \n    - How well would they perform with simple few-shot examples from the training set or simple prompt-optimization techniques (e.g. TextGrad or GEPA)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J8AV18kw2a", "forum": "g4HpTtgsn2", "replyto": "g4HpTtgsn2", "signatures": ["ICLR.cc/2026/Conference/Submission16908/Reviewer_JGfq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16908/Reviewer_JGfq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587834298, "cdate": 1761587834298, "tmdate": 1762926936313, "mdate": 1762926936313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Falcon, a large-scale cross-modal evaluation dataset for safety perception in multimodal large language models (MLLMs). Falcon contains 57,515 visual question answering (VQA) pairs annotated across 13 harm categories, with explicit labels on images, instructions, and responses. The authors also introduce FalconEye, an evaluator model fine-tuned on this dataset, and demonstrate its effectiveness on the Falcon-test, VLGuard, and Beavertail-V datasets. FalconEye is shown to outperform several established baselines, including open and closed-source models, in accuracy across multiple harm dimensions. The dataset, model, and evaluation methodology are released with a focus on enhancing safety assessment rigor for MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive Cross-Modal Dataset Design: Falcon represents a good contribution as a large, fine-grained, multimodal dataset targeting safety evaluation in VQA scenarios. Unlike some predecessors, it provides per-instance annotations for harmfulness across images, prompts, and model responses, each with explicit harm categories and justifications.\n- Coverage and Scope: Falcon covers 13 differentiated harm types, consciously merging overlapping categories from prior works to provide clearer, operationally feasible definitions.\n- Rigorous Annotation Pipeline: The dataset employs a multi-stage semi-automated/human- in-the-loop curation, with manual annotation for the Falcon-test subset."}, "weaknesses": {"value": "- Limited Novelty / Incremental Contribution: Amid many recent multimodal safety benchmarks, Falcon appears incremental. Falcon contributes no original data. The samples are aggregated/reprocessed from existing open-source datasets (e.g., SPA-VL, JailBreakV-28k, HADES). There is no new scenario, data collection protocol, or annotation methodology introduced.\n- Class Imbalance: Several harm categories, such as \"Unlicensed Advice\" and \"Adult Content,\" are underrepresented in the dataset. While FalconEye shows reasonable performance even on rare classes, Table 7 and associated text indicate that accuracy drops significantly for minority classes. The paper acknowledges these challenges, but no mitigation methods are proposed.\n- Reliable Evaluation Metric: The paper leans almost entirely on accuracy, which could be misleading for imbalanced and multi-label tasks. It does not report precision, recall, F1, or results across multiple random seeds.\n- Annotation Process Lacks Detail: While Section 3.1 claims consensus and guidelines for annotators, no quantitative metric is reported to establish reliability."}, "questions": {"value": "1. Can the authors provide quantitative measures for the manual annotation process of the Falcon dataset?\n2. What specific steps, if any, were taken to mitigate the underrepresentation of rare harm categories in model training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UqUyK6eykK", "forum": "g4HpTtgsn2", "replyto": "g4HpTtgsn2", "signatures": ["ICLR.cc/2026/Conference/Submission16908/Reviewer_KDco"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16908/Reviewer_KDco"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666354038, "cdate": 1761666354038, "tmdate": 1762926935812, "mdate": 1762926935812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed Falcon, a large scale vision-language safety dataset. A fine-tuned model FalconEye was also introduced by tuning Qwen model on the Falcon dataset. Multiple experiments show that FalconEye demonstrated good accuracy on Falcon-test and two other benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The current draft is addressing a very critical issue of safety evaluation for Multimodal Large Language Models (MLLMs) where visual information is crucial but usually overlooked in the existing safety evaluation of Large Language Models (LMMs).\n2. The proposed data curation and model training/fine-tuning process also make sense and are both technically sound to me. The annotation of all three parts of the VQA triplet allows for a nuanced understanding of safety that is missing from benchmarks that only label responses or inputs.\n3. Extensive experiments were conducted to show that FalconEye is a effective model for evaluating the safety of MLLM-generated content.\n4. Writing is good and easy to follow."}, "weaknesses": {"value": "1. My main concern is that the current data curation process heavily rely on MLLMs themselves. While it automate the data curation process and make it more scalable, it is a bottleneck and capped by the capability of the models used. \n2. Even though FalconEye showed good performance on binary harm detection in Table 3, it seems very weak on more fine-grained classification tasks based on the results from Table 7.\n3. Since FalconEye was trained on Falcon dataset and Falcon-test is more of a in domain data, I am not sure the comparison to other models are fair enough. Will FalconEye work well on out of domain data as well?"}, "questions": {"value": "Please refer to the paper weakness section for more details and provide more justifications accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8SSqrDGtuc", "forum": "g4HpTtgsn2", "replyto": "g4HpTtgsn2", "signatures": ["ICLR.cc/2026/Conference/Submission16908/Reviewer_RFZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16908/Reviewer_RFZL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979767288, "cdate": 1761979767288, "tmdate": 1762926935508, "mdate": 1762926935508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}