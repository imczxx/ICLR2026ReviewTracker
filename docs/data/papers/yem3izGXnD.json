{"id": "yem3izGXnD", "number": 7233, "cdate": 1758012448093, "mdate": 1763623362248, "content": {"title": "Learning Representations from Heterogeneous Data for Robust Heart Rate Modeling", "abstract": "Heart rate prediction is vital for personalized health monitoring and fitness, while it frequently faces a critical challenge in real-world deployment: **data heterogeneity**. We classify it in two key dimensions: **source heterogeneity** from fragmented device markets with varying feature sets, and **user heterogeneity** reflecting distinct physiological patterns across individuals and activities. Existing methods either discard device-specific information, or fail to model user-specific differences, limiting their real-world performance. To address this, we propose a framework that learns latent representations agnostic to both heterogeneity, enabling downstream predictors to work consistently under heterogeneous data patterns. Specifically, we introduce a random feature dropout strategy to handle source heterogeneity, making the model robust to various feature sets. To manage user heterogeneity, we employ a time-aware attention module to capture long-term physiological traits and use a contrastive learning objective to build a discriminative representation space. To reflect the heterogeneous nature of real-world data, we created and publicly released a new benchmark dataset, **ParroTao**. Evaluations on both **ParroTao** and the public FitRec dataset show that our model significantly outperforms existing baselines by 17.5\\% and 10.4\\% in terms of MSE, respectively. Furthermore, analysis of the learned representations demonstrates their strong discriminative power, and one downstream application task confirm the practical value of our model.", "tldr": "", "keywords": ["Heart Rate Prediction", "Rrepresentation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8292203cf652d2a687e64d356ed21024e01f58de.pdf", "supplementary_material": "/attachment/aa90985fd0338771ec9d6cad2c26130591109ede.zip"}, "replies": [{"content": {"summary": {"value": "The author introduced a framework that could predict the expected heart rate given a planned activity. A new benchmark dataset is collected and released. The proposed modeling framework contains a dropout layer at the input layer and is trained using contrastive loss to tackle the issue of inter-subject variability. The model achieves the best performance against the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem addressed is meaningful and has strong potential for real-world applications, especially in personalized health monitoring.\n- The paper demonstrates a solid applied deep learning effort, combining practical motivation with technical implementation.\n- The Group 4 experiment is particularly compelling. It highlights how users can interactively adjust their activity plans based on forecasted heart rate, showing promising translational potential of the model."}, "weaknesses": {"value": "- The selection of baselines could be expanded to include more recent or diverse methods, which would help strengthen the empirical validation. Because the objective is a channel imputation task, there are multiple open sourced and pre-trained time series models that build upon masked autoencoders, for example [1], how is the proposed framework compared to those approaches?  \n- The way to address heterogeneity is a bit of common sense, and none of the newer and more effective methods is discussed and compared, for example there are varied works proposing modality agnostic model backbone, or channel aware attention, etc, [2] as an example. \n- The paper lacks fine-grained analysis on feature contributions. For instance, how much the current input (X_curr) versus historical input (X_hist) influences the model’s predictions. Also the ablation study suggests limited improvement from the proposed modules, which could be discussed more clearly or justified through an additional statistical test result. \n- The scope of the contribution (method, dataset, or task formulation) is a bit confusing. It could be articulated more explicitly to help readers better understand the paper’s core advancement. \n\n[1] Nie, Y. \"A Time Series is Worth 64Words: Long-term Forecasting with Transformers.\" arXiv preprint 2022\n\n[2] Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 2024"}, "questions": {"value": "The questions are mostly stated in the weakness above. I've summarized, in terms of the expected improvement, in below:\n- How is the proposed framework compared to those self-supervised approach? \n- How is the proposed method for handling heterogeneity compared to more up-to-date backbone deep learning module? \n- What is the difference of contribution from current input (X_curr) versus historical input (X_hist)? And which feature(s) catch more attention from the trained model during inference time? More analysis on this aspect would provide more insights on the model behaviors. \n- Is the contribution lies more on the method or data aspects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ru0qfFXXcb", "forum": "yem3izGXnD", "replyto": "yem3izGXnD", "signatures": ["ICLR.cc/2026/Conference/Submission7233/Reviewer_xP3V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7233/Reviewer_xP3V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760572055419, "cdate": 1760572055419, "tmdate": 1762919375312, "mdate": 1762919375312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author introduced a framework that could predict the expected heart rate given a planned activity. A new benchmark dataset is collected and released. The proposed modeling framework contains a dropout layer at the input layer and is trained using contrastive loss to tackle the issue of inter-subject variability. The model achieves the best performance against the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The problem addressed is meaningful and has strong potential for real-world applications, especially in personalized health monitoring.\n- The paper demonstrates a solid applied deep learning effort, combining practical motivation with technical implementation.\n- The Group 4 experiment is particularly compelling. It highlights how users can interactively adjust their activity plans based on forecasted heart rate, showing promising translational potential of the model."}, "weaknesses": {"value": "- The selection of baselines could be expanded to include more recent or diverse methods, which would help strengthen the empirical validation. Because the objective is a channel imputation task, there are multiple open sourced and pre-trained time series models that build upon masked autoencoders, for example [1], how is the proposed framework compared to those approaches?  \n- The way to address heterogeneity is a bit of common sense, and none of the newer and more effective methods is discussed and compared, for example there are varied works proposing modality agnostic model backbone, or channel aware attention, etc, [2] as an example. \n- The paper lacks fine-grained analysis on feature contributions. For instance, how much the current input (X_curr) versus historical input (X_hist) influences the model’s predictions. Also the ablation study suggests limited improvement from the proposed modules, which could be discussed more clearly or justified through an additional statistical test result. \n- The scope of the contribution (method, dataset, or task formulation) is a bit confusing. It could be articulated more explicitly to help readers better understand the paper’s core advancement. \n\n[1] Nie, Y. \"A Time Series is Worth 64Words: Long-term Forecasting with Transformers.\" arXiv preprint 2022\n\n[2] Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 2024"}, "questions": {"value": "The questions are mostly stated in the weakness above. I've summarized, in terms of the expected improvement, in below:\n- How is the proposed framework compared to those self-supervised approach? \n- How is the proposed method for handling heterogeneity compared to more up-to-date backbone deep learning module? \n- What is the difference of contribution from current input (X_curr) versus historical input (X_hist)? And which feature(s) catch more attention from the trained model during inference time? More analysis on this aspect would provide more insights on the model behaviors. \n- Is the contribution lies more on the method or data aspects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ru0qfFXXcb", "forum": "yem3izGXnD", "replyto": "yem3izGXnD", "signatures": ["ICLR.cc/2026/Conference/Submission7233/Reviewer_xP3V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7233/Reviewer_xP3V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760572055419, "cdate": 1760572055419, "tmdate": 1763668806908, "mdate": 1763668806908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a novel method for representation learning using exercise data from heterogeneous sources (different fitness devices).   The training and downstream analysis uses two different data sets, FitRec and ParroTao— the former is publicly available on Kaggle; it is not clear whether the latter is publicly available.  The authors describe their method in technical detail and have shared sufficient code that it could (probably) be reproduced using another data set. \n\nThe method consists of a contrastive learning strategy that is designed to accommodate both subject heterogeneity (individuals can have different physiology, and also engage in systematically different exercise types) and source heterogeneity (devices from different manufacturers have different data streams).  The strategy relies upon ‘\n‘Random Feature Dropout’ (to encourage invariance to input streams), a ‘Time-Aware Attention Module’ to integrate information drawn from past exercise sessions. The loss function used for end-to-end training consists of both a reconstruction (MSE) loss for the forecasted heart rate, as well as a contrastive loss applied to the user embeddings.  \n\nThe authors’ model is evaluated and compared against a large number of baseline models based upon the forecasted HR accuracy (MSE and MAE).  The paper concludes with description of a potential downstream application (route recommendation)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## This paper represents strong contributions in the following areas: \n\n### Originality:  \nThe random feature dropout strategy addresses a fundamental challenge in consumer hardware: sensor suite variability across device manufacturers and products.   From the results it is also clear that the reported method produces better performance for HR forecasting compared to existing baseline approaches. \n\n### Clarity\nThe training objective is clear (contrastive loss applied to subject embedding + MSE loss applied to forecasted HR), well-reasoned, and in line with scientific intuition about physiology and exercise. \n\n### Significance \nIf publicly available, the ParroTao dataset (42,576 sessions from 113 users; 3 device manufacturers) represents a valuable dataset for related analysis and future comparisons.   I also commend the authors for sharing their code, which enables reproducibility of their findings. \n\n### Quality\nThe authors’ comparison of their model against a large selection of other well-described baselines (Section D) is a significant strength of the paper."}, "weaknesses": {"value": "# Brief Summary #\nThis paper has several important weaknesses that are listed briefly below, with more detailed comments and some suggested steps to address each weakness in the section following the list the list:\n\n1. Concerns regarding the data split into train/val/test sets.\n2. Concerns regarding use of non-causal input features \n3. No evaluation of input feature importance (or conversely, input feature invariance) for the HR forecasting task\n4. Minimally detailed evaluation of performance (HR forecasting accuracy) stratified by participant and device type\n5. It lacks quantitative analysis supporting the statements made in the abstract (line 27) and section 4.5 (line 426) that “learned representations are highly discriminative across both users and sports”\n6. [most concerning] No evaluation of the utility or informativeness of the learned subject representations. \n7. Insufficient evaluation of downstream application(s)\n8. [Covered in 'Question' section] Abstract includes a vague statement about performance relative to baselines (line 26; also repeated on line 108)\n9. [Covered in 'Question' section] Ambiguous data availability (data is stated as publicly released, but with no information about how/where to access) \n\n\n\n# Additional Discussion Details #\n\n## Issue 1: Concerns regarding the data split into train/val/test sets.\nThe authors report this (lines 355-356) as “We split the dataset into 80%, 10%, and 10% for training, validation, and testing” however they provide no comments or details regarding how the split was stratified.  Is the 80/10/10 split based on random selection of # participant’s # (meaning that a participant’s data is present in one and only one of the splits), or is it based on randomly-selected exercise sessions (meaning a participant’s data may be present in multiple splits).  If one participant’s data is present in multiple splits, this is likely to influence the validation/test performance via data leakage. \n\n### Suggestions to address Issue 1:\nAt a minimum the authors should explicitly state whether the data splits were performed randomly by session, or stratified by participant ID.  Ideally, the data split should be the latter (stratified by participant), but if this was not done then the authors should discuss the downstream impact as a potential limitation of the study.   It is very likely that random splits not stratified by participant will inflate the downstream performance numbers. \n\n\n## Issue 2:  Concerns regarding use of non-causal input features \nLooking at the full list of variables in Section C Table 6 I am concerned that some of these inputs are non-causal in nature. For example VO2max is likely calculated after the completion of the workout session (based on information collected during the session itself).  Therefore if the VO2max value is used for HR forecasting early in the workout session, this may represent the use of future/non-causal information.   I have similar concerns regarding Stress and ‘Body Battery’ (these are likely proprietary manufacturer-specific metrics, so obtaining detailed information about them may not be possible).  \n\n### Suggestions to address Issue 2:\nI recommend that the authors carefully review the nature of the input variables, and consider removing any that are non-causal from the input streams.  This would also leave \nthose metrics available to use as prediction targets for additional downstream tasks. \n\n\n\n## Issue 3::  No evaluation of input feature importance (or conversely, input feature invariance) for the HR forecasting task or other \nDespite the input invariance encouraged in pre-training (random feature dropout), it is likely that some input streams are more informative than others for the HR estimation task.  Do some streams remain more essential than others?  For example running speed and elevation are likely more informative for predicting HR than stance time. \n\n### Suggestions to address Issue 3:\nAdd some form of feature importance analysis (e.g. ablation study removing specific data streams at inference) to quantify the importance or ‘informativeness’ of each stream.   Consider doing the same for other downstream tasks (if any are added). \n\n\n\n## Issue 4  Minimally detailed evaluation of performance (HR forecasting accuracy) stratified by participant and device type\nTables 8 and 9 provide a detailed breakdown of HR forecasting accuracy by exercise type.  However there is no breakdown according to device type (manufacturer).  Additionally, there is no information regarding the distribution of accuracy over participants.  Having performance information stratified by participant and/or device type would be valuable to understand the distribution of algorithm performance. \n\n## Suggestions to address Issue 4:\nFor a subset of better-performing models (e.g. ‘FitRec’ and ‘Ours’) tabulate the performance stratified by device type (Garmin, Coros, Huawei ).  It may make sense to also stratify these by exercise type (only for the most common exercise, such as running, biking, indoor cycling). \n\nFor a subset of better-performing models (e.g. ‘FitRec’ and ‘Ours’) it would be helpful to see the distribution of performance over participants (e.g. histogram of MAE over participants).  This will highlight whether there are outlier subjects for whom the modeling simply does not perform well. \n\n\n\n## Issue 5: Lack of quantitative analysis supporting discriminative capability of learned representations\nIn the abstract (line 27) and section 4.5 (line 426) the authors state that “learned representations are highly discriminative across both users and sports”.   However, the supporting tSNE analysis in Fig. 3 is entirely visual/qualitative, and does not demonstrate (convincingly or quantitatively), that the learned representations effectively separate individuals and activities.  Can this be quantified in some manner? \n\n### Suggestions to address Issue 5:\nConsider adding some measure of unsupervised embedding quality such as Rankme (or similar), and/or a participant-level cluster evaluation metric (e.g. KNN accuracy, ARI, NMI or Davies-Bouldin Index)\n\n\n## Issue 6:[most concerning] No evaluation of the utility or informativeness of the learned subject representations. \nThe authors state several times that an important characteristic of the user embedding is to capture personal physiologically meaningful information (lines 196, 260).   However, there are no experiments in the paper that quantify that (or even meaningfully test the hypothesis).  It is important to include these to demonstrate that the embeddings contain some relevant information regarding participant physiology, fitness or (ideally) health status, not merely exercise patterns. \n\n### Suggestions to address Issue 6:\nAdd some analysis that quantifies the physiologically-relevant content of each user embedding.  If demographic information is available in the data set (e.g. participant age, sex or height/weight) those could be used as relevant proxy targets for evaluating physiological content.   An alternative targets that could be used as a proxy for health/fitness is VO2max (only if this is not used as a model input).  \n\n\n## Issue 7:  Insufficient evaluation of downstream application(s)\nThe choice of route recommendations based on forecasted heart rate does not seem particularly useful.  Assuming a run workout, if the route recommendation/forecast controls for user-chosen factors such as running pace then it seems like the recommendation would boil down to just a terrain-based predictor (hilly courses tend to cause higher heart rates, which I think everyone already knows).  If the route recommendation/forecast # doesn’t # control for user chosen-factors (which can be made in the moment), then it is not very useful because the user can influence the forecast accuracy simply through their choice of run pace. \n\nAdditionally, the evaluation metric(s) that would be used to quantify performance for this downstream application (MAE/MSE) are essentially identical to those used for evaluating the primary (upstream) HR forecasting algorithm. \n\n### Suggestions to address Issue 7:\nIs it possible to identify some other downstream application that could be useful in a real-world setting?  \n\nOne example could be offline VO2max estimation for watches that lack this functionality (looking at Section C Table 6 the Huawei watch has this metric, but the Garmin and Coros watches do not).  Providing that estimate to a non-Huawei device user would be genuinely valuable.   Same idea for Effort Pace, Calories / Kilojoules, and Grade-Adjusted Speed (these are each available from one device maker, but not the other two). \n\nA second example could be heart rate imputation for time periods when HR measurements from a device are missing (HR loss is common across many wrist-based HR).  The method described here would be able to impute missing values to produce a better-looking post-workout HR chart."}, "questions": {"value": "## Issue 8:  Vague statement in abstract\nIn the abstract the authors state “Evaluations on both PARROTAO and the public FitRec dataset show that our model significantly out-performs existing baselines by 17.5% and 14.6%, respectively”.  However, they do not explain anything about the evaluation (e.g. what the numbers 17.% and 14.6% represent). \n### Suggestions to address Issue 8:\nExpand this sentence to make it clear that the performance comparison is based on HR forecasting MAE (I think this is the case, though I’m not certain). \n\n## Issue 9:  Ambiguous data availability \nIs the ParroTao data set publicly available?  The authors make this statement “we have constructed and publicly released PARROTAO—a large-scale, multi-device, multi-activity dataset” (lines 100-101) but do not provide any further information.   I was not able to locate it through google searches. \n### Suggestions to address Issue 9:\nInclude a clear data availability statement at the end of the main section.  Link to the public dataset location if available."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WzrU83G496", "forum": "yem3izGXnD", "replyto": "yem3izGXnD", "signatures": ["ICLR.cc/2026/Conference/Submission7233/Reviewer_xynF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7233/Reviewer_xynF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881409277, "cdate": 1761881409277, "tmdate": 1762919374767, "mdate": 1762919374767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for heart rate prediction that learns data representations robust to source and user heterogeneity. The proposed method claims it explicitly models the fragmented nature of data from wearable devices and individual physiological variance, which are common challenges in real-world deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work presents a formulation of the data heterogeneity problem in heart rate modeling, separating it into source and user dimensions. The proposed method to address these challenges is reasonable. The introduction of random feature dropout to handle source heterogeneity and the use of a time-aware attention module and contrastive learning to manage user heterogeneity is grounded in existing representation learning techniques. A contribution is the creation and release of the PARROTAO dataset, which captures multi-device and multi-activity variations."}, "weaknesses": {"value": "The rationale for certain architectural choices could be further substantiated. For instance, the selection of BiLSTMs and GRUs within the time-aware attention module is presented without a comparison to alternative sequential models. While random feature dropout is intended to create robustness to missing features, its interaction with features that have different units or scales across devices is not explored. The paper states that unobserved features are set to zero, but the impact of this imputation choice is not discussed. The contrastive loss is applied using either user ID or activity type as labels, but the effect of this choice is not analyzed."}, "questions": {"value": "The PARROTAO dataset is one of the main contribution, but the paper provides limited detail on the demographics of the 113 athletes (e.g., age, fitness level distribution). How might the specific characteristics of this cohort affect the generalizability of a model trained on it, particularly to populations like older adults or individuals with different health conditions?\n\nFor the route recommendation application, how were the planned route features, such as topographical data and intended pace, acquired to be fed into the model for prediction?\n\nThe time-aware attention module considers the 10 most recent workouts. Was a sensitivity analysis performed on this number? It seems plausible that for some users or activities, a longer or shorter history might be more predictive. Furthermore, how does the model handle new users with fewer than 10 historical sessions?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zTkdTJIH4V", "forum": "yem3izGXnD", "replyto": "yem3izGXnD", "signatures": ["ICLR.cc/2026/Conference/Submission7233/Reviewer_gRNk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7233/Reviewer_gRNk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956612882, "cdate": 1761956612882, "tmdate": 1762919374324, "mdate": 1762919374324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We extend our sincere appreciation to reviewers\n**gRNk, xP3V, and xynF** for dedicating\ntheir time and expertise during the rebuttal phase. Their constructive\ncomments are invaluable to the enhancement of our manuscript:\n\n**Problem formulation and motivation.** Reviewers\n**gRNk** and\n**xP3V** appreciated that we formulate\nheart-rate prediction under source and user heterogeneity as a\npractically meaningful and well-motivated problem for real-world\nwearable deployment.\n\n**Methodology.** Reviewers **gRNk**,\n**xP3V**, and\n**xynF** noted that our combination of\nrandom feature dropout, time-aware attention, and contrastive learning\nprovides a reasonable and well-grounded approach to handling\nheterogeneous wearable data and learning informative user\nrepresentations.\n\n**Dataset and reproducibility.** Reviewers\n**gRNk** and\n**xynF** highlighted the value of the\nParroTao dataset as a realistic multi-device, multi-activity benchmark\nfor studying heterogeneous wearable data.\n\n**Experimental evaluation and baselines.** Reviewer\n**xynF** praised the breadth of baselines,\nthe clear experimental setup, and the consistently strong empirical\nperformance of our model.\n\n**Downstream applications and translational potential.** Reviewer\n**xP3V** emphasized that the downstream\napplications convincingly demonstrate how our framework can be used for\ninteractive, personalized heart-rate planning in practice.\n\n---\n\nTo address the reviewers’ concerns, we have included the following new\nor clarified results:\n\n1.  **Ethics and data governance.** We clarified that all participants\n    provided written informed consent, that collected signals are\n    anonymized and accessible only to authorized researchers for\n    non-commercial use, addressing potential ethical concerns about our\n    wearable data collection and handling.\n\n2.  **Dataset splits and cohort characteristics.** We clarified the\n    participant-level train/validation/test splitting strategy and\n    reported key demographics of the ParroTao cohort to better\n    contextualize the scope and generalizability of our results.\n\n3.  **Feature set inspection and imputation strategy.** We audited all\n    channels in ParroTao, removed constant inputs, updated the variable\n    table to clarify each channel’s role, and showed that\n    zero-imputation is the most effective strategy on our dataset.\n\n4.  **Architecture and historical modeling.** We systematically compared\n    alternative encoders, ablated the historical stream, varied the\n    history length, and inspected attention weights, confirming the\n    effectiveness of our proposed architecture and indicating that our\n    time-aware attention module with $K \\approx 10$ past workouts offers\n    a good trade-off between performance and computational cost.\n\n5.  **Feature importance and heterogeneity across sports and users.**\n    Through feature ablations on FitRec and ParroTao, we quantified\n    which channels are globally influential and showed that effective\n    feature importance varies substantially across sports and\n    individuals.\n\n6.  **Representation quality and subject-specific information.** Using\n    structure-aware embedding metrics and downstream gender/physiology\n    prediction tasks, we showed that our learned representations capture\n    informative, stable sport-specific and user-specific\n    characteristics.\n\n7.  **More downstream utility cases.** We extended our evaluation to\n    cross-device feature imputation and heart-rate imputation, where our\n    framework consistently outperforms classical interpolation/filtering\n    methods and the strongest baseline model, highlighting its\n    versatility for practical deployment scenarios.\n\n8.  **Stronger baselines, statistical tests, and clarified claims.** We\n    added recent transformer-based/self-supervised baselines, conducted\n    Wilcoxon signed-rank tests for key ablations, and clarified our\n    post-acceptance plan for releasing the ParroTao dataset.\n\nAcross all these changes, our main findings and conclusions remain\nunchanged. All textual updates in the revised manuscript are clearly\nmarked in **blue**. We believe these\nrevisions substantially strengthen the paper and align it more closely\nwith the standards of the conference, and we hope the revised version\nwill be considered a valuable contribution to the field."}}, "id": "Lsn3GXCqkv", "forum": "yem3izGXnD", "replyto": "yem3izGXnD", "signatures": ["ICLR.cc/2026/Conference/Submission7233/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7233/Authors"], "number": 19, "invitations": ["ICLR.cc/2026/Conference/Submission7233/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763622492109, "cdate": 1763622492109, "tmdate": 1763622577219, "mdate": 1763622577219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}