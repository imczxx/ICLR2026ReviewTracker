{"id": "62DZyNWRgv", "number": 25531, "cdate": 1758368951173, "mdate": 1759896716589, "content": {"title": "TAROT: Test-Driven and Capability-Adaptive Curriculum Reinforcement Fine-Tuning for Code Generation", "abstract": "Large Language Models (LLMs) are fundamentally changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcome this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose TAROT, Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning. TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for reinforcement fine-tuning in code generation is closely tied to a model’s inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://anonymous.4open.science/r/TAROT-B675.", "tldr": "", "keywords": ["Large Language Model", "Code Generation", "Curriculum Learning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76623d8f515997b9a6bd0b80a2c71cdf8c549efb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "TAROT is a novel RFT framework to address the flat and sparse reward landscape of traditional\nRFT, which typically relies on a simple binary signal (pass/fail) in code generation tasks. It first\nintroduces an “intra-problem” curriculum dataset, where each problem is augmented with a four\u0002tier test suite. Based on the dataset, TAROT then applies a curriculum policy with specific reward\nweightings, which can dynamically adjust the training focus given the training objective. Extensive\nexperiment results reveal that the optimal curriculum for RFT is closely tied to the model capability,\nwith less capable models achieving greater gains with an easy-to-hard progression, whereas more\ncompetent models excel under a hard-first curriculum."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-Motivated RFT Framework: The idea of an \"intra-problem\" difficulty gradient based on\ntiered test cases is a strong and novel contribution. It directly tackles a well-known limitation in\nRFT for code generation tasks and is intuitively based on human practices.\n2. Significant Empirical Validation: The paper presents extensive experiments across a diverse\nset of models (Qwen2.5, Qwen3, Gemma2) and benchmarks (HumanEval, MBPP,\nLiveCodeBench, etc.). The results consistently show that the capability-adaptive TAROT\nframework significantly outperforms baseline models, demonstrating the method's\neffectiveness.\n3. Valuable Core Insight: The paper's finding that the optimal curriculum is not one-size-fits-all\nbut dependent on the model's \"effective capability\" (a combination of scale and prior\nspecialization) is a crucial insight for the field. This moves beyond simplistic \"easy-to-hard\"\ncurriculum learning.\n4. Resource Contribution: The creation and release of the TAROT dataset, with its 15k problems\naugmented by 4-tier test suites, is a valuable dataset resource for future research."}, "weaknesses": {"value": "1. Verifiability of the Generated Dataset: my primary concern is the quality and consistency of\nthe four-tiered test suites. Since they are generated by proprietary LLMs following a prompt\u0002based guideline, the actual difficulty distinction between tiers may be inconsistent or difficult to\nverify, potentially undermining the core assumption of a structured difficulty gradient.\nAdditionally, while the authors performed a validation analysis, this reliance on proprietary LLMs\nfor dataset creation raises questions about reproducibility and scalability.\n2. Confounding Factor of Optimization Attribution: The analysis of OOD benchmarks reveals that\nthe optimal curriculum is also highly task-dependent, not solely a function of model capability.\nThis finding complicates the central claim, suggesting a more intricate interplay between model\ncapability, task structure, and curriculum design than the paper currently stated.\n3. Weak Correlation of the Reward Signal: The paper intriguingly shows that the final training\nreward has only a weak correlation with downstream benchmark performance. This\nobservation somewhat undermines the centrality of the proposed tiered-reward mechanism. If\nthe carefully structured reward signal does not reliably predict final performance, its role as the\nprimary driver of the TAROT framework becomes less clear. It is worth considering whether\ndownstream tasks could be framed as different (harder or easier) curriculum stages themselves\nto be incorporated into the purposed TAROT framework."}, "questions": {"value": "1. The results presented in Figures 3 and 4 consistently show that RFT leads to performance\nimprovements over the baseline. However, RFT can sometimes lead to performance\ndegradation or catastrophic forgetting. Did the authors observe any negative results or\ninstances where a particular curriculum strategy harmed the model's performance, especially\nfor less-capable models?\n2. Could the authors provide a more detailed theoretical or intuitive justification for the key finding\nthat more competent models excel under a hard-first curriculum? While empirically\ndemonstrated, a deeper explanation of the underlying learning dynamics would strengthen this\nconclusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FCi2bHtbx3", "forum": "62DZyNWRgv", "replyto": "62DZyNWRgv", "signatures": ["ICLR.cc/2026/Conference/Submission25531/Reviewer_szTy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25531/Reviewer_szTy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628500581, "cdate": 1761628500581, "tmdate": 1762943464048, "mdate": 1762943464048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TAROT is a test-driven, capability-adaptive framework for RFT of code LLMs. For each problem it builds a four-tier test suite of basic, intermediate, complex, and edge cases while leaving the original prompt and reference solution unchanged. Training uses a tier-weighted objective with two parts: how much training focus to allocate to each tier and how much reward to give each tier. This decouples the curriculum from raw pass rates and lets the same dataset support different learning paths.\n\nExperiments on multiple models and benchmarks show consistent gains and reveal that the best curriculum depends on both model capability and the target task. Weaker models benefit from starting with basic and intermediate cases, while stronger models learn more from emphasizing complex and edge cases. Training rewards rise smoothly but are weak indicators of downstream accuracy, whereas shorter completions correlate better with stronger performance. Overall TAROT improves functional correctness and robustness through intra-problem, test-driven curricula."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper shifts curriculum learning from inter-problem ordering to the intra-problem test gradient and formalizes it with a tier-weighted objective that decouples sampling allocation and reward weighting from raw pass rates, directly addressing flat and imbalanced signals in RFT/RL.\n\n2. The four-tier taxonomy (basic, intermediate, complex, edge) is meaningfully separated through quantitative and qualitative checks, with all tests verified against the reference solution, which reduces noise and supports trustworthy curriculum comparisons.\n\n3. Experiments across multiple models and benchmarks show consistent gains and reveal task-dependent optima; the capability-adaptive guidance is practical for selecting curricula.\n\n4. Community value: The release of a large dataset with validated tiered test suites offers a reusable resource that enables fair, repeatable evaluation of curriculum and RFT/RL methods in code generation."}, "weaknesses": {"value": "The procedure for estimating a model’s effective capability and choosing the corresponding curriculum is under-specified. It remains unclear how to select allocation and weighting for a new model without tuning on target benchmarks, how robust the policy is to different problem orders or tier cardinalities."}, "questions": {"value": "1. How do you estimate a model’s effective capability before training and then select the tier allocation and reward weights for a new model without tuning on the target benchmarks? Please describe any signals you rely on.\n\n2. You generate test cases of different difficulty, but do you have more intrinsic indicators of the actual difficulty achieved? What precise criteria were used to classify test cases into basic, intermediate, complex, and edge tiers during both generation and validation, and how did you verify that these tiers reflect meaningful differences beyond surface features like length or token rarity?\n\n3. In §4.2 you mention pursuing a more holistic measure of intrinsic abilities. How should this be evaluated in practice? Are held-out test sets sufficient, or do you recommend additional diagnostics or probes?\n\n4. Was the problem set and its order kept fixed across all curricula so that only intra-problem tier policies varied? More broadly, do you envision combining a problem-level difficulty curriculum with the test-tier curriculum, and if so, how would you integrate the two while ensuring results are not affected by changes in problem order or by using different problem subsets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qjNH0dW7qG", "forum": "62DZyNWRgv", "replyto": "62DZyNWRgv", "signatures": ["ICLR.cc/2026/Conference/Submission25531/Reviewer_9Fqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25531/Reviewer_9Fqg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904166679, "cdate": 1761904166679, "tmdate": 1762943463796, "mdate": 1762943463796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel technique to generate a curriculum for a LLM learning. \n\nOverall, I am not convinced that the presented work is finished, since the authors provide several techniques to finetune LLM and do not provide a rule to choose one among them."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The novel technique to generate data for LLM finetuning."}, "weaknesses": {"value": "The authors in fact propose several techniques. The experiment results suggest that for different models (even among the same family) the most efficient technique is also different. The authors do not propose any strategy to choose one technique."}, "questions": {"value": "Please elaborate on the choice strategy for technique for a model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FZR05KnShw", "forum": "62DZyNWRgv", "replyto": "62DZyNWRgv", "signatures": ["ICLR.cc/2026/Conference/Submission25531/Reviewer_VD2u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25531/Reviewer_VD2u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932653650, "cdate": 1761932653650, "tmdate": 1762943463518, "mdate": 1762943463518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the TAROT dataset, a Code RL dataset that includes four different difficulty categories (tiers) of test cases, thereby providing more granular/smoother non-binary rewards for Code RL. It also compares several different Curriculum Learning settings. The experimental results show that the proposed RFT method offers some marginal improvement compared to the base model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments are extensive. This paper conducts validation on a series of models, including models of different scales and different architectures. It also covers a very comprehensive evaluation set.\n\n2. Overall, I think the core idea of this paper makes sense: providing non-binary rewards to RL through a multi-granularity test case setting and curriculum learning, thereby smoothing the RL optimization process."}, "weaknesses": {"value": "Major concerns:\n\n1.  **Test Case Quality.**  A large number of prior works [1, 2, 3] have demonstrated the importance of test case quality, which encompasses two primary issues: whether a correct solution is incorrectly marked as an error (false negative), and whether an erroneous solution is incorrectly marked as correct (false positive). Based on the construction process of the TAROT dataset, it appears that only a single correct reference solution was used to verify the test case quality. This is clearly insufficient, as it does not guarantee a sufficiently low false positive rate. The false positive issue is also reflected in the limited number of test cases. Without a sufficient quantity of test cases, it is impossible to cover all potential errors. Given that Equation 4 and Table 3 suggest only one test case is included per tier, meaning only four test cases per problem, it raises further doubt as to whether these test cases can effectively screen for incorrect solutions.\n\n2. **The 4 Tiers.** Another key concern is regarding the four tiers proposed in the paper—basic, intermediate, complex, and edge—specifically, whether the corresponding data truly aligns with the definitions on which this paper is based. From Table 4, it appears that the test cases for these four tiers were entirely generated by an LLM, with no other mechanism used to verify, for example, whether the \"complex\" tier is genuinely more difficult than the \"basic\" tier. I believe this verification is quite important because the distinction between these tiers is directly related to the effectiveness of the curriculum learning setting. Looking at Table 6, I was unable to observe what kind of differences these curriculum settings exhibited in the end-to-end RL results.\n\n3. **Missing Baseline.** This article indeed features a wealth of experiments, but I still want to point out that a crucial baseline seems to be missing. As far as I know, most current Code RL methods are implemented using the complete test suites, where a full pass yields a $+1$ reward and failing even one test case yields a $-1$ reward. These methods do not use fine-grained reward shaping or curriculum learning (CL). I believe such a naive baseline without CL represents the strategy currently used by most practitioners, and readers would likely be interested in the results for such a baseline.\n\nMinor concerns:\n\n1. I feel that the presentation of this paper needs improvement. Several critical terms, such as 'C/E Weighted,' are not defined within the main text, nor is their full name provided (although I eventually located it in the Appendix). Furthermore, I believe that Table 5 is crucial for understanding the paper and should be moved to a prominent position in the main body. Lastly, it is unclear what the authors intend to convey with Figures 3 and 4. These figures do not compare differences between CL settings, but instead compare the RFT models against a base model. This does not seem aligned with the logic of this paper.\n\nSome typos:\n\n1. L132-133, Duplicated reference for DPO and PPO.\n\n2. L135, The opening quote is a closing quote.\n\nReferences\n\n[1] Competition-level code generation with alphacode, Science 2022.\n\n[2] Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. NeurIPS 2023.\n\n[3] CodeContests+: High-Quality Test Case Generation for Competitive Programming, EMNLP 2025."}, "questions": {"value": "Please refer to \"Weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "81NJ88KCVL", "forum": "62DZyNWRgv", "replyto": "62DZyNWRgv", "signatures": ["ICLR.cc/2026/Conference/Submission25531/Reviewer_njHu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25531/Reviewer_njHu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016551688, "cdate": 1762016551688, "tmdate": 1762943463230, "mdate": 1762943463230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the TAROT framework for reinforcement fine-tuning in the domain of code generation, which utilizes the varying difficulty of test cases to obtain more training signals. During training, the framework adjusts the proportion and the weight of four tiers of test cases. Experiments demonstrate that this strategy can generally improve the model's performance on coding benchmarks. The authors also observe the diverse training dynamics for base models of different capabilities, which partially explains their setting of curriculum policies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this paper is proposing an approach for obtaining more fine-grained rewards by evaluating on test cases of diverse difficulties. This approach may stimulate the model to refine its solutions from general ideas to extreme or corner cases, which differs from the conventional criterion of passing all test cases.\n\nAnother thing interesting is the observation that models of different capability have their own favor of training recipes (with respect to test difficulty). It is an analogy of the well-believed idea  that adjusting the distribution of problem difficulty in several training stages."}, "weaknesses": {"value": "Although I generally agree with the authors that it is possible to leverage intra-problem difficulty to to get more reward signals, I think the analysis to the framework is somewhat simplistic. Indeed, the authors observe that in many cases training with TAROT dataset brings about higher score in coding benchmarks, however, the baseline should not be base models themselves. More specifically, as soon as one is able to collect a bunch of data (like the interview dataset used), he can (arbitrarily) generate test cases, perform training with them and expect the possibility of improving the performance. In my opinion, the authors should use the most simple case (may be B/I + average/pass) as the baseline, then the performance gain can be reliably attributed to the weighting or allocation strategies in the proposed framework.\n\nBesides, I am concerned about the quality of different tiers of test cases. Since test cases are generated by LLMs, it is necessary to make sure that they have valid inputs. Additionally, the generated outputs require step-by-step simulation, making it difficult to scale up to stress tests (e.g., n=1e3 or even n=1e5 for sequence length). Also see my comments to Table 3 in the Questions section."}, "questions": {"value": "**A general question:**\n\nGiven that there are different training recipes available in the framework, the authors do not provide a method to automatically choose a suitable one for training a specific model (when its capability is unknown to us). Thus, the experimental results are more like iterating over all combinations and select the best one afterwards, which limits the practical applicability of the framework. I understand it is difficult to propose an automatic training framework, thus I would like the authors to share their opinions with respect to this point.\n\n\n\n**Question to Figure 1:**\n\nI can understand that it is better to gradually learn harder test cases for less capable models, but what is the insight behind learning from complex to basic test cases for stronger models?\n\n\n**Question to Line 202-204:**\n\nI generally agree with the exploration of intra-problem difficulty, but I am a bit curious about the definition of \"hard\" case. In competition-level coding (like CodeForces), small test cases can be passed with brute force, yet large test cases may require algorithms, which filters out undesirable solutions. Does \"hard\" case aim to examine the correctness of a solution's inner logic, or just an arbitrarily constructed case with larger size compared to \"easy\" and \"intermediate\"?\n\n\n\n**Question to Table 3:**\n\nGenerally I find the contents in the table contain many errors, thus I am concerned about the quality of the dataset as well as the generated test cases.\n\nIn the first problem, it says \"In each operation, you will be given an integer p and a character c. You have to replace the p-th character of the string with c.\" However, the test cases do not contain either p or c.\n\nIn the second problem, for the intermediate test case \"BAABABAB\", the answer should be \"no\", but the test output is \"yes\".\n\nIn the third problem, string S is not provided in the test input.\n\nIn the fourth problem, the problem only requires a single input \"N\", but each test case has two input numbers A and B, and it seems to always output max(A, B).\n\nIn the fifth problem, it says \"He wants to make the sequence strictly increasing. What is the minimum number of operations he needs to perform?\" But the test cases actually correspond to  the opposite case, that is making the sequence strictly decreasing.\n\nIn the sixth problem, I personally do not understand the problem description. In the basic test case, how can sequence \"1 2 3 4 5\" contains some piles of the same maximum height?\n\nBesides, many problems have test cases which contain multiple inner tests (with the first input as the number of inner tests), but is not mentioned in the problem description.\n\n\n\n**Minor question and suggestions:**\n\nLine 84: \"which is defined via instruction-following fidelity and baseline coding proficiency\", but the authors do not explain these two ideas in the paper. I think this part is confusing and can be deleted.\n\nLine 135 and afterwards: The left quotation marks look weird. In LaTeX, left and right quotation marks are typed differently.\n\nLine 271 (Figure 2): The authors should explain the meaning and unit of x-axis in Figure 2. How are token diversity and transitions calculated?\n\nLine 289: The citation for the dataset is missing.\n\nLine 290-292: The authors need to explain the different strategies (Forward/Reversed/Static and Uniform, B&I, C&E).\n\nLine 299: It is necessary to mention some important details of implementation here, such as the RFT algorithm used. Do not put everything in the Appendix.\n\nLine 430: \"However, the reward observed during training does not reliably anticipate downstream benchmark outcomes.\" I suggest conducting a case study to understand this phenomenon. That is, given that these models are already capable of writing codes (also demonstrated by baseline performance in benchmarks), why do they gain such low reward in the beginning shown in Figure 5(a)？\n\nLine 432 (Figure 5): Please briefly explain what the light-colored trajectories are. Also, please explain why the trajectories of some models stopped in the middle (it is confusing, although I get to know the reason from Appendix).\n\nLine 865 (Table 5): When not all tiers are used, is the loss function computed with relative weight (of available tiers)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2w8P05fErh", "forum": "62DZyNWRgv", "replyto": "62DZyNWRgv", "signatures": ["ICLR.cc/2026/Conference/Submission25531/Reviewer_pRbZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25531/Reviewer_pRbZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission25531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762197737373, "cdate": 1762197737373, "tmdate": 1762943463043, "mdate": 1762943463043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TAROT, a test-driven and capability-adaptive curriculum reinforcement fine-tuning framework that trains code generation models through multi-level test rewards and ability-based curriculum scheduling to improve correctness, reasoning, and robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper includes curriculum learning into code generation of RL domain."}, "weaknesses": {"value": "1. The weighting scheme for small/medium/large models is purely empirical, and the “difficulty” definition of tests is also rule-based without ablation studies to justify its validity. This makes the method feel ad-hoc and lowers novelty.\n\n2. The paper only evaluates PPO. Given the rule-based reward (pass rate), GRPO or similar gradient-regularized policy optimization methods seem more appropriate and potentially stronger. Not testing them weakens the experimental rigor.\n\n\n3. Figures and tables are not well organized — too many figures per page and captions placed inconsistently (tables should have captions below). The improvement numbers are hard to locate and interpret.\n\n\n4. The evaluated models are already outdated for ICLR 2026 standards, and there are no comparisons against newer or closed-source models. This limits the relevance of the conclusions.\n\n5. Lack of novelty. Just empirical reward shaping under the situation that is the baseline is not convincing."}, "questions": {"value": "Why does this paper use PPO, instead of GRPO? Cause code is verifiable so that GRPO is straightforward."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ODnGnMkE8t", "forum": "62DZyNWRgv", "replyto": "62DZyNWRgv", "signatures": ["ICLR.cc/2026/Conference/Submission25531/Reviewer_sG53"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25531/Reviewer_sG53"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission25531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762313353547, "cdate": 1762313353547, "tmdate": 1762943462881, "mdate": 1762943462881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}