{"id": "j02kh2R1W5", "number": 9538, "cdate": 1758126776956, "mdate": 1762933778621, "content": {"title": "InfoCLIP++: A Multimodal Learning Framework with Multi-Granular Information-Theoretic Alignment and Adaptive Fusion", "abstract": "Multimodal foundation models such as CLIP have significantly advanced vision-language understanding yet face persistent challenges including coarse semantic alignment, high computational overhead, and sensitivity to noisy inputs. This paper introduces \\textit{InfoCLIP++}, an integrated framework addressing these limitations through three synergistic components: multi-granular alignment using constrained optimal transport for pixel-level details and Random-Feature HGR correlation for patch-level and global semantics, differentiable adaptive routing for token and modality pruning via entropy-gradient criteria, and hardware-aware optimization with quantized random feature projections for efficient deployment. The model is trained end-to-end with a composite objective combining alignment losses, contrastive learning, and sparsity regularization. Extensive evaluations demonstrate consistent and significant improvements: 84.3\\% zero-shot accuracy on ImageNet-1K, representing an 8.1\\% gain over CLIP, 74.5\\% R@1 on COCO cross-modal retrieval with a 16.1\\% improvement, and a Noise Robustness Score of 0.90 on ImageNet-C. Computationally, InfoCLIP++ reduces FLOPs by 87\\% and achieves a 6.8$\\times$ speedup on FPGA platforms, establishing it as an efficient and robust foundation for resource-constrained multimodal intelligence.", "tldr": "InfoCLIP++ elevates multimodal learning through multi-granular HGR-OT alignment, dynamic token routing, and efficient hardware-aware approximation, achieving superior zero-shot accuracy, speed, and noise robustness compared to CLIP.", "keywords": ["Multimodal Learning", "CLIP", "HGR Correlation", "Optimal Transport", "Efficient Attention Modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6192409718b51f9640cd485483ba09654e53fba0.pdf", "supplementary_material": "/attachment/84118507aa3cac34a7bd0a399c0041ed2190e7c6.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new model for multi modal learning, which is called InfoCLIP++.\nInfoCLIP++ integrates optimal transport, random feature approximation, hardware-aware optimization and other techniques.\nThe experiments demonstrate that InfoCLIP outperforms baselines on Zero-Shot Classification, Cross-modal retrieval, fine-grained alignment, and noise robustness.\nFurthermore, InfoCLIP is 6.8 times faster than CLIP in calculations on FPGAs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed framework achieves high practical performance. I like that it also outperforms baselines in terms of computational complexity.\n- At least the combination of technologies in the framework proposed in this paper seems novel. Individual technologies may possess some degree of novelty.\n- The experiment is evaluated from various perspectives."}, "weaknesses": {"value": "- This paper is not well written, and its central argument remains unclear throughout.\nThough Section 2 is the most crucial part of this paper, the connections and structure between subsections are unclear.\nThe connection of subsections seems like a list of bullet points, and the section seems to lack any in-depth exploration of the topics it aims to address.\nFor example, though this paper claims GPU acceleration and FPGA deployment, readers cannot utilize them or use them as a starting point for new ideas because they are scarcely explained.\n\n- The current structure and content of the paper make it difficult to draw general or transferable insights for the broader machine learning community.\nSince the work is primarily empirical, it would benefit from narrowing the focus and designing experiments that more directly support the central claim.\nWhile some of the individual ideas appear interesting, the current presentation does not clearly convey their novelty or justify their effectiveness.\nIf the main contribution lies in achieving higher empirical performance rather than providing conceptual insights, the paper might be more suitable for an application-oriented conference.\n\n- To make the paper easier to understand, I think it would be better to create a section explaining the ideas and overall picture before Section 3, and describe which section each idea is written in. Also, explaining how each technology was conceived, including comparisons with existing technologies, might make it easier to understand.\nTheoretical results to support the claims will strengthen the paper. If difficult, toy problems or visualizations might help to support the claims."}, "questions": {"value": "- What is the most significant contribution of this paper? How did you demonstrate it?\n- Does this paper contain insights that will influence the broad field of machine learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s9UtFjvfGx", "forum": "j02kh2R1W5", "replyto": "j02kh2R1W5", "signatures": ["ICLR.cc/2026/Conference/Submission9538/Reviewer_JNRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9538/Reviewer_JNRM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743526841, "cdate": 1761743526841, "tmdate": 1762921100282, "mdate": 1762921100282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "dS6WlaXyAR", "forum": "j02kh2R1W5", "replyto": "j02kh2R1W5", "signatures": ["ICLR.cc/2026/Conference/Submission9538/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9538/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762933777689, "cdate": 1762933777689, "tmdate": 1762933777689, "mdate": 1762933777689, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes InfoCLIP++, a multimodal foundation model that enhances CLIP by integrating multi-granular alignment, differentiable adaptive routing, and hardware-aware optimization. The framework performs semantic alignment at pixel, patch, and global levels via constrained optimal transport and Random-Feature HGR correlation, achieving fine-grained yet efficient cross-modal understanding. A differentiable routing mechanism enables adaptive token and modality pruning based on semantic importance, while quantized RF-HGR projections allow efficient deployment on GPUs and FPGAs. The method achieves significant improvements on standard benchmarks and large computational gains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly identifies and addresses three long-standing challenges of CLIP-style models—coarse alignment, inefficiency, and noise sensitivity.\n- The combination of constrained optimal transport and RF-HGR correlation offers a principled, information-theoretic approach to multi-scale alignment.\n- Considerable performance gains across multiple benchmarks with both accuracy and efficiency improvements."}, "weaknesses": {"value": "- The paper is very dense and sometimes difficult to follow; architectural details could be clearer.\n- While the paper adopts an optimal transport  framework for pixel–text alignment, OT is theoretically defined between comparable probability distributions. Here, pixel features and textual tokens reside in heterogeneous embedding spaces, and it is unclear whether treating them as directly comparable distributions is conceptually justified. The semantic validity of such cross-modal transport mappings therefore remains uncertain.\n- The modality-level routing mechanism assumes that modalities with globally lower activation scores contribute little and can be pruned. However, in multimodal fusion, redundancy across modalities often serves a complementary or stabilizing purpose—providing robustness under noise, occlusion, or modality failure. The current design does not clarify how it distinguishes between “redundant but useful” and “truly uninformative” modalities.\n- Although the hardware-oriented acceleration enhance practicality, they do not appear to introduce substantial algorithmic or conceptual innovation beyond standard quantization and batching strategies."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4pdUskbbFP", "forum": "j02kh2R1W5", "replyto": "j02kh2R1W5", "signatures": ["ICLR.cc/2026/Conference/Submission9538/Reviewer_Lt5K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9538/Reviewer_Lt5K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998294704, "cdate": 1761998294704, "tmdate": 1762921099577, "mdate": 1762921099577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to enhance the CLIP model through three key aspects: (i) alignment granularity, (ii) computing efficiency, and (iii) robustness against noisy samples. The authors propose a hierarchical approach to alignment that involves pixel-word, token-attribution, and image-text correlation to address the first aspect. To tackle the second and third aspects, they introduce a differentiable adaptive routing mechanism that takes into account the quality of cross-modal alignment, thereby improving both computational efficiency and the model's robustness to noise."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The hierarchical designs address several crucial aspects of CLIP, enhancing both its alignment quality and computational efficiency."}, "weaknesses": {"value": "- The presentation quality is poor: \n\n    - It is difficult for readers to follow the content due to weak logical connections between the components, and there is no clear motivation provided for each specific design choice. \n\n    - The equations are challenging to read. For instance, $T$ refers to the text in Fig. 1, while the transport matrix in Eq. 2. Additionally, there is no definition of $t$ in Eq. 11, which I assume refers to tokens. It is also unclear what the patches $p'$ aggregated with attributions are used for. \n\n    - Fig. 1 is overly complex and confusing. \n\n    - There are missing details: what are the implementation specifics for Tab. 3? How does the mIoU result of ADE20K achieve 70+?\n\n- The effects of each component are unclear. Given the complexity of the proposed method, the ablation section should undergo significant revision to clarify the effects and learning dynamics of each design element. \n\n- The claim that the proposed designs are scalable lacks support. I wonder whether the designs remain stable when applied to larger ViTs."}, "questions": {"value": "please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FXxQEEy2Cc", "forum": "j02kh2R1W5", "replyto": "j02kh2R1W5", "signatures": ["ICLR.cc/2026/Conference/Submission9538/Reviewer_b1Av"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9538/Reviewer_b1Av"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097707455, "cdate": 1762097707455, "tmdate": 1762921099200, "mdate": 1762921099200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets three long-standing issues in CLIP-like multimodal learning: (i) coarse alignment (missed fine-grained correspondences), (ii) high compute for high-res inputs, and (iii) fragility to noise. InfoCLIP++ proposes an integrated framework with:\n1) Multi-granular alignment: pixel-level constrained OT; patch/global alignment via RF-HGR, with an MI lower-bound motivation.\n2) Differentiable Adaptive Routing (DAR) for token/modality pruning using agreement, saliency, and gradient criteria with Gumbel-Sigmoid. \n3) Hardware-aware optimization for RF-HGR and a composite loss combining all pieces."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Principled alignment across granularities: Pixel-wise OT + patch/global RF-HGR is a coherent design; the MI link via HGR (Eq. 8–9) gives a clean objective-level story for maximizing cross-modal information. \n\n\n- End-to-end efficiency via DAR: Token/modality-level routing is differentiable and shows clear contribution to noise robustness (NRS +0.22 from base CLIP to full model) and efficiency (less FLOPs, reduced latency across GPU/CPU/FPGA). \n\n- Empirical coverage and clarity: Results span zero-shot classification, cross-modal retrieval, segmentation transfer, and robustness.\n\n- Comparative advantage over CLIP family: The reported ImageNet-1K/COCO improvements are sizable vs. CLIP/SigLIP/ALIGN and competitive with ALBEF/BLIP/BLIP-2, while adding efficiency knobs the latter lack."}, "weaknesses": {"value": "- No Related Work Section.\nRelation to broader “info-theoretic CLIP” literature:\nThere is growing work analyzing/optimizing CLIP via MI/correlation perspectives. A short related-work bridge to information-theoretic T2I/CLIP alignment papers would help situate RF-HGR’s role. \n\n\n- Novelty vs. recent fine-grained alignment papers:\nSmartCLIP (CVPR’25) formalizes identification guarantees and disentanglement for multi-granular alignment; InfoCLIP++’s pixel-OT + RF-HGR is strong but positioning against SmartCLIP-style theory is brief. What, precisely, is new: the RF-HGR + constrained OT + DAR combination, or improved approximations/efficiency? A direct comparison/discussion is warranted. \n\n- RF-HGR approximation rigor and sensitivity:\nThe paper uses random Fourier features to approximate HGR; however, approximation error (ε) is only symbolically acknowledged in Eq. (9). Please report sensitivity to k, σ, ε and Sinkhorn iterations/ε and show accuracy/latency/variance trade-offs, especially given the hardware-aware pitch. \n\n\n- Retrieval benchmarking details:\nCOCO R@1 = 74.5 is strong; please clarify backbone size, pretraining data, and retrieval protocol."}, "questions": {"value": "- RF-HGR hyper-parameters: Provide curves for k (RF dimension), σ, and the hardware-aware quantization setting—how do accuracy and latency scale? \nOpenReview\n\n- DAR stability: How sensitive is routing to the temperature schedule / sparsity target ρ and the λA, λG, λE weights in Eq. (10)? Any failure modes (e.g., over-pruning text tokens on short captions)? \nOpenReview\n\n- SmartCLIP comparison: Can you add a head-to-head (classification/retrieval) and/or discuss theoretical differences (HGR-MI vs. identification guarantees)? \n\n\n- Protocol parity: Confirm backbone sizes and pretraining corpora vs. ALBEF/BLIP/BLIP-2 in Table 1 comparisons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CWnFAzWsBN", "forum": "j02kh2R1W5", "replyto": "j02kh2R1W5", "signatures": ["ICLR.cc/2026/Conference/Submission9538/Reviewer_i3Fn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9538/Reviewer_i3Fn"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762846202071, "cdate": 1762846202071, "tmdate": 1762921098866, "mdate": 1762921098866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}