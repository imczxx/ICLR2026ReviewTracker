{"id": "ksWsF80beE", "number": 6290, "cdate": 1757964911433, "mdate": 1759897924556, "content": {"title": "Understanding Addition and Subtraction in Transformers", "abstract": "Transformers are widely deployed in large language models (LLMs), yet most models still fail on basic arithmetic tasks such as multidigit addition. In contrast, we show that small transformers trained from scratch can solve n-digit addition and subtraction with 99.999\\% accuracy. Building directly on prior work that uncovered addition circuits, we extend the analysis to subtraction and present a unified mechanistic account based on cascading carry and borrow circuits. Using a suite of 49 trained models, we apply systematic ablations and node-level constraints to validate the learned mechanisms, and release a reproducible interpretability toolkit for studying arithmetic circuits. Finally, surveying 180 publicly available LLMs, we find that only 7\\% can reliably perform addition, underscoring the gap between specialized small models and general-purpose LLMs. Our results show that arithmetic can be implemented exactly by tiny transformers, offering a tractable case study for mechanistic interpretability and a cautionary contrast with the persistent arithmetic failures of much larger models.", "tldr": "We train then explain accurate n-digit addition and subtraction transformer circuits", "keywords": ["Transformers", "circuits", "addition", "subtraction", "mechanistic interpretability", "LLMs"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba51f5ef4715610691dba4c228d73b632dcb9373.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether small, standard Transformers (2–3 layers, 3–4 heads) learn an algorithmic solution to multi-digit addition/subtraction when trained on synthetic data enriched with cascading carry/borrow cases. The authors formalize addition via a left-to-right decomposition—per-digit sums (**SA**), tri-state carry judgements (**ST** ∈ {0,1,U}), and a prefix operator (**TriAdd**) producing definite carries (**SV**) claimed to be resolved by the “=” token—then propose an analysis framework that (i) searches concrete model “nodes” (layer/head/position residual writes), (ii) enforces behavior/ordering constraints aligned with the algorithm, and (iii) performs causal verification via activation swapping on paired inputs differing in a single subtask value. They report near-perfect accuracy on 5–12 digits across 49 models and provide a toolkit for node characterization/ablation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Clarity of the target mechanism.** The SA/ST/SV (TriAdd) decomposition gives a precise, human-readable algorithm that is aligned with left-to-right decoding and can be checked against model internals.\n- **Methodological step beyond visualization.** Activation swapping constitutes a causal test that can, in principle, show that specific internal variables are *used* rather than merely correlated with outputs.\n- **Reusable infrastructure.** The released toolkit for node search/characterization/ablation is potentially useful for similar mechanistic studies within synthetic arithmetic."}, "weaknesses": {"value": "1) **Limited substantive reach of several listed contributions.**  \n   The paper enumerates contributions as (i) an LLM “survey,” (ii) enriched datasets, (iii) 49 small models with near-perfect accuracy, (iv) exact algorithms, and (v) a reusable interpretability toolkit.  \n   - *(i)* The “survey” functions as a test sweep rather than a principled evaluation: it lacks a clear task taxonomy or depth that would yield new insights; as written, it reads as a breadth-only diagnostic.  \n   - *(ii)* The enriched datasets are straightforward to synthesize for addition/subtraction; beyond standard edge-case oversampling, there is little conceptual novelty.  \n   - *(iii)* Near-perfect accuracy on short n-digit arithmetic with small models is not in itself a strong contribution; many prior works can achieve this under similar setups.  \n   - *(iv) & (v)* The algorithmic decomposition and the causal toolkit are the genuinely interesting pieces; however, their **application breadth** and **forward significance** are narrow (synthetic digit-level arithmetic). As framed, this falls short of ICLR’s bar for impact unless the paper more clearly argues why these tools produce new, generalizable insights *within the stated scope*.\n\n2) **Key assumptions are hard-coded yet not sufficiently justified within the paper’s logic.**  \n   The analysis embeds strong priors—for example, that ST/SV needed for the first answer digit are computed **before** “=”. While defensible for worst-case cascades, this is stronger than necessary for the typical cut-off cases that dominate the distribution. The paper treats such priors as constraints without fully arguing their necessity (vs. design choice) for the claims actually made.\n\n3) **Under-specified experimental pipeline at the crux of the claims.**  \n   The node-search/labeling procedure governs all main results, yet crucial details are missing: attention thresholds and their sensitivity, PCA/probe criteria, ordering-check implementation, per-stage pass/fail counts, tie-breaking when multiple nodes satisfy a subtask, and cross-seed variability. Without these, it is hard to assess how general or fragile the reported “consistent” findings are.\n\n4) **Evidential weight of PCA is limited as presented.**  \n   PCA plots appear as case studies. The paper does not provide population-level quantification (e.g., separability/probe metrics across *all* labeled nodes) or negative controls. Given the centrality of node labeling, this weakens the argument that the behavior-constraint stage reliably identifies the intended subtasks."}, "questions": {"value": "1. **Necessity of positional priors.** Are the “before =” constraints logically required for your claims, or are they convenient design choices to guarantee worst-case visibility? Please justify within the paper’s framework (no need to add new tasks). If they are necessary, clarify the argument; if not, state the intended scope explicitly.\n\n2. **Pipeline transparency where results hinge on it.** Please provide precise (in-paper or appendix) specifications for the node search and labeling: the thresholds used (and brief sensitivity notes), the ordering-check implementation, tie-breaking when multiple nodes satisfy a subtask, and per-stage pass/fail counts aggregated over models/seeds.\n\n3. **Population-level evidence for node labels.** Beyond illustrative PCA figures, can you report an aggregate separability or probe metric for all nodes finally labeled as SA/ST/SV, together with negative controls (e.g., random or incorrect-position nodes)? This stays within your main contribution (validating the proposed mechanism) while strengthening the evidential basis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1W9ZDKa5tt", "forum": "ksWsF80beE", "replyto": "ksWsF80beE", "signatures": ["ICLR.cc/2026/Conference/Submission6290/Reviewer_FknJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6290/Reviewer_FknJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760668286956, "cdate": 1760668286956, "tmdate": 1762918596208, "mdate": 1762918596208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that small transformers trained from scratch can perform n-digit addition and subtraction almost perfectly (99.999%).\nThe authors present a mathematically exact, left-to-right algorithm that uses “cascading carry one” for addition and “cascading borrow one” for subtraction, and they argue that trained transformers actually implement this procedure.\nTo support the claim, they train 49 tiny models with different configurations on a synthetic dataset that boosts the proportion of hard edge cases with long carry chains.\nThe authors also build “mixed” models that handle both addition and subtraction, showing that if we bring a module identified in a trained addition model and then train on a mixture of addition and subtraction data, the model reuses the inserted addition circuits for both tasks (become polysemantic).\nFinally, the authors provide a tool for characterizing nodes that perform specific subtasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Unlike many prior studies that use reversed token order, this work tackles addition and subtraction in the human-familiar format."}, "weaknesses": {"value": "**W1.** \nI feel the significance of the paper is quite limited. \nAs I understand it, each model in the experiments receives operands of a fixed length. \nIf so, what is the meaning/implication of showing that a Transformer can solve fixed-length addition/subtraction with near-perfect accuracy? In practice, what we need are models that operate reliably across variable lengths and formats with many other different tasks at the same time.\n\n**W2.**\nThe authors claim that the trained Transformers actually implement their proposed algorithm, but I think the evidence is not sufficiently persuasive.\nHere, I reference prior papers demonstrating that trained Transformers can implement specific algorithms.\nCho et al., (2025) [1] studied length generalization for addition and proposed position coupling. \nCrucially, they theoretically constructed a Transformer and proved it can solve addition; they then showed that the learned model’s attention patterns align with those of the construction, thereby clarifying which algorithm the model learned. \nSimilarly, though in a different setting within the in-context learning literature, Nichani et al. (2024) [2] constructed a Transformer capable of in-context learning for tasks generated by a Markov chain and again verified that the learned attention patterns align with their theoretical construction. \nIn short, a common approach to identifying the algorithm a trained Transformer has learned is to theoretically construct an explicit model and then demonstrate similarity with the trained one. \nHowever, the automated testing framework proposed here (whose operation is not clearly explained in the paper) seems too weak to substantiate the strong claim that the trained Transformer implements the specific algorithm the authors describe.\n\n**W3.** \nSeveral definitions and explanations lack detail.\nHere I list few of them.\nWhat exactly is the $SC\\_n$ task? \nHow does the automated testing framework operate? \nIn Figure 2, how many test instances did the authors use for each length?\n\n---\n\n[1] Cho, Hanseul, et al. \"Position coupling: Improving length generalization of arithmetic transformers using task structure.\" NeurIPS 2024\n\n[2] Nichani, Eshaan, Alex Damian, and Jason D. Lee. \"How transformers learn causal structure with gradient descent.\" ICML 2024"}, "questions": {"value": "**Q1.** \nWhat is the exact definition of a “node” in this paper?\nI think the term \"node\" is slightly ambiguous; is it a specific weight in the Transformer, or a hidden representation at a specific token?\n\n**Q2.**\nLines 185–186 state: “Note that the calculation first combines ST2 and ST1 before combining the result with ST0.” \nIf the trained Transformers truly implement the proposed algorithm, this ordering should be reflected in the learned model. \nWhere is the evidence supporting this claim?\n\n**Q3.**\nSuppose we wish to train on longer operands. \nAmong the number of layers, number of heads, and embedding dimension, which should be increased (if any)? \nOr is scaling unnecessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iNpgzGL52z", "forum": "ksWsF80beE", "replyto": "ksWsF80beE", "signatures": ["ICLR.cc/2026/Conference/Submission6290/Reviewer_crJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6290/Reviewer_crJ9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932057590, "cdate": 1761932057590, "tmdate": 1762918595432, "mdate": 1762918595432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work provides several new insights into the ability of LLMs to learn multi-digit addition. While many publicly available LLMs cannot reliably perform multi-digit addition, small Transformers can learn this task when trained on datasets containing many edge cases. Unlike human computation, where calculation proceeds from the least significant to the most significant digit, Transformers predict from the most significant to the least significant digits. The authors propose a new addition algorithm that emulates this process, and careful analysis reveals that Transformers indeed implement such computational nodes. The proposed algorithm and accompanying observations elucidate the computational process inside LLMs in terms of addition, subtraction, and their combination."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper tackles the addition learning task while maintaining the left-to-right digit rule of autoregressive models. Unlike many previous works that introduce tricks (e.g., zero padding, additional positional embeddings, or reversing digit order) to facilitate learning, this work keeps the learning process unchanged and aims to explain the basis for successful learning. \n- The explanation is grounded in a novel addition algorithm aligned with the left-to-right digit rule. The algorithm suggests multiple subtasks, and this study discovers that the trained models contain nodes corresponding to these subtasks.\n- The authors conduct extensive examinations across many pre-trained and newly trained models, and they publicly release their datasets and toolkits."}, "weaknesses": {"value": "While I appreciate the novel attempt of this work, there are several aspects that I could not follow, partly due to issues in the presentation. I would appreciate it if the authors could clarify these points (and revise the manuscript accordingly), and I would be happy to increase my score. \n\nIn what follows, I focus on the addition case. I list my comments based on the flow of this study, rather than their importance. \n\n---\nFirst, while it is technically interesting to address addition learning in a left-to-right setup, the motivation is not clearly explained. Right-to-left computation is more mathematically reasonable and indeed more learning-friendly. The ordering of autoregressive generation is critical [1, 2]. The authors should explain more about the underlying motivation for focusing on a left-to-right setup and, ideally, its practical impact. It might also be interesting to compare the efficiency (or some other metric) of the proposed left-to-right addition algorithm with the standard one. Is the proposed algorithm mathematically as efficient as the standard one? If so, learning addition in a left-to-right setup should be as difficult as that in a right-to-left setup (personally, this seems doubtful). \n\n[1] Shen et al., \"Positional Description Matters for Transformers Arithmetic,\" 2023.\n\n[2] Sato et al., \"Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic,\" 2025.\n\n---\nSecond, to my understanding, the addition model was trained without any tricks, such as additional loss. The proposed left-to-right algorithm is only used for interpretation and analysis and is not involved in the training step. The only trick used is to design the dataset to contain more edge cases. A recent study [3] reports a similar technique, training with easier samples, particularly those with many zeros in the inputs. The easy samples in [1] are also edge cases, while the reverse is not evident. This connection may suggest an interesting training recipe. \n\n[3] Saxena+, Making Hard Problems Easier with Custom Data Distributions and Loss Regularization: A Case Study in Modular Arithmetic, ICML'25\n\n---\n\nThird, the verification process of the core observation—that the addition model involves nodes that correspond to the subtasks and satisfy several constraints—is unclear to me. Line 349 comments on the \"automated testing framework\" in three steps, but it critically lacks details. Thus, while the core observation sounds intriguing, I cannot judge its validity."}, "questions": {"value": "Please address the weaknesses raised above. In particular, the third comment is critical and why I'm tentatively giving a negative score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ofhFe5aIXM", "forum": "ksWsF80beE", "replyto": "ksWsF80beE", "signatures": ["ICLR.cc/2026/Conference/Submission6290/Reviewer_bU7a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6290/Reviewer_bU7a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959177259, "cdate": 1761959177259, "tmdate": 1762918595037, "mdate": 1762918595037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}