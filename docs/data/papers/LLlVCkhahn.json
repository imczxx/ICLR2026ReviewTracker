{"id": "LLlVCkhahn", "number": 4074, "cdate": 1757596186408, "mdate": 1759898054455, "content": {"title": "PASO: Step Parallel Stochastic Optimization", "abstract": "This paper approaches the fundamental challenge of accelerating the inherently autoregressive nature of gradient descent (GD) like SGD and Adam through a dynamic system perspective.\nSpecifically, we introduce a unified framework that recasts the autoregressive GD process as solving a system of triangular nonlinear equations (TNEs), thereby facilitating a paradigm shift toward non-autoregressive GD featuring parallel gradient computation across iteration steps. Within this generic framework, we establish that: (1) the TNE system admits a unique solution corresponding precisely to the autoregressive  GD iterative trajectory; (2) solving the TNEs system  guarantees convergence to the GD iterative trajectory in equal or far fewer iterations.\nBuilding on these insights, we present \\textit{PASO}, a step parallel optimizer for accelerating a broad class of autoregressive GD optimizers like SGD and Adam.\nExtensive experiments (\\textit{e.g.}, Llama-3.2-1B) validate that PASO achieves up to \\textbf{91}$\\times$ reduction in GD steps and \\textbf{7.5}$\\times$ speedup in wall-clock time, with no measurable model quality loss. The source code will be released publicly.", "tldr": "", "keywords": ["Parallel training", "gradient descent", "stochastic optimizatiog"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fefad2891707064497886af47eaa510db64fd362.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reformulates sequential optimizers such as SGD and Adam as solving Triangular Nonlinear Equations (TNE) and proposes a general framework, PASO, that enables step parallelism.\nPrevious parallelization methods could not compute multiple optimization steps in parallel while preserving the exact gradient descent (GD) trajectory. In contrast to OptEx, which relies on approximation, PASO \naims to find exact points on the GD path by directly solving a system of equations.\nThe authors show that (i) the solution of the TNE uniquely matches the trajectory of sequential GD (Prop. 1), (ii) fixed-point iteration (FPI) can converge to this trajectory in at most T steps (Prop. 2), and (iii) a sliding window of width p makes the approach compatible with practical resource limits.\nExperiments on language modeling (GPT-2, Llama-3.2-1B on WikiText) and image classification (CIFAR-10, Tiny-ImageNet with CNN/ViT) demonstrate up to 91× fewer iterations and up to 7.5× wall-clock speed-up, without quality degradation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n\nThe idea of breaking inter-step dependency through equation solving is novel. The authors correctly point out that existing parallelization methods have not been able to compute multiple steps in parallel while preserving the exact gradient descent (GD) trajectory. Their reformulation of optimization as Triangular Nonlinear Equations (TNE) and the use of Fixed-Point Iteration (FPI) to recover the sequential GD path provide a clear and distinctive contribution. The paper also appropriately positions Optex as related work, clarifying that while Optex relies on approximation, PASO aims for exact solutions.\n\nSignificance (Potential Impact)\n\nInter-step parallelism represents an orthogonal axis to data, model, and pipeline parallelism. It has strong implications for scalability, especially in communication-bound training environments, as suggested by the results in Table 1.\n\nPositive Details\n\nThe notation table in Appendix D is well-organized, making the proofs easy to follow."}, "weaknesses": {"value": "1. Lack of comparison with approximate step-parallel methods such as Optex (insufficient validation of novelty)\n\nThe authors themselves refer to Optex (Shu et al., 2024) in the Related Work section (p.3), acknowledging it as an approach that uses kernel methods to approximate future gradients and achieve approximate step parallelism. However, the paper does not provide an empirical comparison with Optex in terms of accuracy, speed, computational cost, or communication overhead.\n Since PASO emphasizes exactness, a quantitative comparison on the same hardware and dataset is essential to show its practical advantage over a lighter, approximate method like Optex. In cases where approximation errors are tolerable, Optex could be more practical. As it stands, the paper’s theoretical claims are not sufficiently supported by empirical evidence regarding their real-world implications.\n\n\n2. Reproducibility and experimental design issues\n\nIncomplete wall-time comparison. When reporting wall-clock performance, full environment details are necessary (CUDA/NCCL/driver versions, PyTorch/Transformers/Tokenizers versions, GPU/CPU/network topology, mixed-precision and gradient scaling settings, and synchronization handling). Currently, only the use of W&B sweeps is mentioned (Sec. 6.1), and the code is not publicly available, making replication difficult. It is also unclear whether baseline tuning and soundness checks (multiple seeds, LR/β/weight-decay sensitivity, clipping, and regularization) were properly validated.\n\n\n3. Evaluation metric issue (language modeling).\n\nThe x-axis should use the number of consumed tokens (num_tokens) rather than iterations, which is now the standard in modern large-scale training. Please include plots using num_tokens to allow fair comparison with DDP and other baselines under the same computational budget. Also clarify how num_tokens was defined and whether it was kept consistent across all experiments.\n\n\n\n4. Ambiguity in metric definitions\n\nThe terms “testing accuracy”, “test accuracy”, “training accuracy”, and “train accuracy” are used inconsistently throughout the paper. Moreover, it is unclear whether “accuracy” in the language modeling experiments refers to token-level accuracy. The authors should include precise mathematical definitions of all evaluation metrics in the main text (e.g., Fig. 2, Table 2). While the Evaluation Metrics section mentions testing accuracy and testing perplexity as the standard metrics, Figure 2 labels training accuracy, creating confusion.\n\n5. Unrealistically low perplexity values\n\nThe reported perplexity values of PPL = 1.4–1.6 on WikiText (Table 2) are unusually low compared with widely reported community benchmarks. \nhttps://learn.arm.com/learning-paths/mobile-graphics-and-gaming/build-llama3-chat-android-app-using-executorch-and-xnnpack/3-understanding-llama-models/\nhttps://huggingface.co/fedric95/Meta-Llama-3.1-8B-GGUF\n\nhttps://huggingface.co/Graphcore/gpt2-wikitext-103\nhttps://huggingface.co/neulab/gpt2-finetuned-wikitext103\n\n\n\n\nThe authors should specify the complete evaluation setup, including the exact script, tokenizer, detokenization process, context length, data split, and whether WikiText-2 or WikiText-103 was used. Clear references to comparable baselines under identical conditions are necessary. Currently, both the main text and Appendix only refer to the “WikiText dataset” in general terms, and even the footnote link does not clarify which variant was employed.\n\n\n6. Clarity and consistency in figures and tables\n\nFigure 2 (p. 7) appears to be a direct W&B dashboard capture; the lack of log scaling, consistent axes, or normalization makes it difficult to visually assess differences across methods. Furthermore, Tables 4–5 (p. 8) omit essential information such as model type, dataset, batch size, and learning rate in the captions, forcing the reader to refer back to the text to interpret results. Each table should explicitly note the model, dataset, batch size (B), and learning rate (η) for clarity.\n\n7. Concerns about CV experiments\n\nThe reported accuracies on CIFAR-10/ViT are significantly below standard baselines \nhttps://github.com/kentaroy47/vision-transformers-cifar10\n\nAround 66% for CNN and 72% for ViT (Table 3, p. 9). It is also unclear whether these values represent top-1 or top-5 accuracy. Without proper baseline tuning and comparison to standard implementations, the validity of the proposed method’s improvements is difficult to assess.\n\n8. Inconsistencies in model and implementation details\n\nIn Appendix B.1 (p. 16), footnote 1 links to the Chinese CLIP repository, making it unclear whether the experiments used a standard ViT, a CLIP-ViT variant, or a pretrained model. The correspondence between the text and the linked implementation should be clarified."}, "questions": {"value": "1. Comparison with approximate step-parallel methods (e.g., Optex)\n\nPlease provide a fair comparison between PASO and Optex under identical hardware, dataset, and initialization conditions, evaluating accuracy (PPL/ACC), speed (steps and wall-time), and computational/communication cost (FLOPs, bytes).\nIt is important to clarify whether there exist realistic settings where the lighter, approximate Optex performs better when approximation errors are tolerable, and to what extent PASO’s theoretical exactness translates into practical performance advantages.\n The paper mentions that “both methods could be complementary (Optex can serve as an initializer),” but an experiment demonstrating this claim is required.\n\n2. Ensuring reproducibility\n\n Please disclose full experimental details for wall-time measurements  including what was included or excluded (e.g., preprocessing, I/O, warm-up, synchronization)  and complete environment specifications (GPU/CPU types, network topology, CUDA/NCCL versions, PyTorch version, AMP configuration, and launch arguments). Public release of the wandb experiment logs would also be highly valuable.\n\n3. Metric definitions and dataset specification\n\nClearly define the accuracy metric for language modeling (is it token-level?), and use consistent terminology across the paper (testing/test, training/train).\nSpecify which WikiText variant (2 or 103) was used, along with tokenizer, detokenization method, context length, and data split. Please include a table comparing PASO with prior baselines under identical conditions (related to Table 2).\n\n4. Analysis using num_tokens as the x-axis\n\nDefine num_tokens, specify its values, and indicate whether it was kept consistent across all experiments.\nReplot loss, PPL, and accuracy curves against num_tokens. Under an equal throughput constraint, please also compare increasing data parallelism versus increasing step parallelism (PASO) to illustrate trade-offs.\n\n5. Soundness of CV and language modeling experiments\n\nIn Table 3, clearly specify whether the reported CIFAR-10 accuracy values are top-1 or top-5.\nRe-tune the baselines (including learning-rate schedules, regularization, and data augmentation) so that they reach standard accuracy levels before comparing with PASO.\nSame for language modeling tasks, please specify all the details and show existing comparable benchmark results or justify why the author achieved unrealistic lower perplexity.\n\n6. Consistency of model references\n\nIn Appendix B.1, the ViT footnote links to the Chinese CLIP repository. Please clarify whether a plain ViT or CLIP-ViT was used, whether it was pretrained, and where the model weights originated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q4I5twjHPB", "forum": "LLlVCkhahn", "replyto": "LLlVCkhahn", "signatures": ["ICLR.cc/2026/Conference/Submission4074/Reviewer_VTKr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4074/Reviewer_VTKr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100568511, "cdate": 1761100568511, "tmdate": 1762917165492, "mdate": 1762917165492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PASO, a step-parallel training scheme that re-casts an optimizer’s autoregressive trajectory $\\\\{w_t\\\\}$ into a triangular system of nonlinear equations (TNEs). A fixed-point iteration (FPI) over this system enables computing gradients for many future steps in parallel (within a sliding window of size $p$), then update all corresponding weights simultaneously. The authors claim (i) the TNE has a unique solution matching the GD trajectory (Proposition 1), (ii) their FPI converges to the GD trajectory in at most $T$ iterations (Proposition 2), and (iii) large reductions in steps (up to 91$\\times$) and moderate wall-clock speedups (to 7.5$\\times$) on GPT-2, Llama-3.2-1B and small CV models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow. The TNE formulation and windowed FPI are described clearly, with an end-to-end algorithm and pipeline figure.\n\n- Reported step reductions (12.6-19.2$\\times$ on GPT-2, 12.6-14.5$\\times$ on Llama-1B, up to 31$\\times$ on CIFAR-10) and best-case wall-clock speedup of 7.5$\\times$ are encouraging."}, "weaknesses": {"value": "- Proposition 2 only shows one can reproduce GD in $\\le T$ outer iterations, and there is no theorem that $K<T$ under identifiable conditions. This weakens the paper’s core claim of principled step-parallel speedup.\n\n- The method either launches $T$ parallel forward/backward passes or uses a window of size $p$, still requiring $p$ simultaneous graphs, data loaders, and optimizer updates. The paper nevertheless claims \"1 model + 1 optimizer\" storage per device in Table 1, which appears optimistic for step-parallel execution that must stage $p$ batches and intermediate states.\n\n- The reported accuracy and perplexity results are presented without multiple seeds or error bars. Furthermore, the paper does not include comparisons with strong system baselines such as fully optimized FSDP/ZeRO or tuned pipeline + DP implementations under comparable hardware and batch-throughput settings.\n\n- The EMA-based $\\delta$ rule has no accuracy guarantee, it is tuned by Wandb sweeps rather than relying on derived principle."}, "questions": {"value": "- Under your best run, what are the measured peak GPU memory, activation footprint, and optimizer-state duplication for PASO vs. strong DP/PP/MP baselines? How are $p$ batches staged without extra model/optimizer replicas?\n\n- The method samples $T$ future mini-batches $\\\\{\\zeta_t\\\\}$ and treats them as fixed to enable parallelism, will that implicitly change the sampling process and may require staging many batches concurrently?\n\n- How are the $T$ future mini-batches produced and pinned to devices? Is the distribution i.i.d. with replacement? Does pre-sampling introduce measurable drift?\n\n- Could you report multiple seeds with error bars, and include the compute budget (FLOPs, tokens)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ltYUed1j3O", "forum": "LLlVCkhahn", "replyto": "LLlVCkhahn", "signatures": ["ICLR.cc/2026/Conference/Submission4074/Reviewer_kux2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4074/Reviewer_kux2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953321252, "cdate": 1761953321252, "tmdate": 1762917165006, "mdate": 1762917165006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PASO (Step Parallel Stochastic Optimization), a new optimization framework that aims to break the sequential dependency of gradient descent (GD).\nThe key idea is to reformulate the autoregressive GD process as solving a system of Triangular Nonlinear Equations (TNEs), which enables parallel gradient computation across multiple steps of optimization.\nThe authors provide theoretical guarantees showing that the TNE system admits a unique solution equivalent to the GD trajectory and that convergence can be achieved in equal or fewer iterations.\nEmpirically, PASO reportedly reduces the number of gradient descent steps by up to 91× and the wall-clock training time by up to 7.5×, without degrading model performance, on tasks such as image classification (CIFAR-10, Tiny-ImageNet) and language modeling (GPT-2, Llama-3.2-1B)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong Conceptual Novelty**\n   The paper tackles a long-standing challenge — the inherently sequential nature of gradient descent — from a completely different perspective.\n   The formulation of GD as a **triangular nonlinear system** is conceptually elegant and opens a new avenue for step-level parallelization.\n\n2. **Solid Theoretical Foundations**\n\n   * The authors rigorously prove **the uniqueness** of the TNE solution (Proposition 1) and **the convergence** of the fixed-point iteration (Proposition 2).\n   * The theoretical framework is mathematically consistent and supported by detailed appendices.\n\n3. **Practical Implementation and Evaluation**\n\n   * PASO is evaluated on both **vision** and **language modeling** tasks, including **LLMs up to 1B parameters**, demonstrating scalability.\n   * The experiments show large reductions in iterations and training time while maintaining comparable accuracy, perplexity, and F1-scores.\n\n4. **Compatibility and Orthogonality**\n   PASO is **orthogonal to existing parallelism methods** (data, model, and pipeline parallelism) and can be combined with any optimizer (SGD, Adam, AdamW).\n\n5. **Clear Reproducibility Commitment**\n   The paper includes an explicit reproducibility statement, open-source promise, and detailed hyperparameter settings — all aligning with ICLR’s reproducibility expectations."}, "weaknesses": {"value": "1. **Empirical Validation Scope**\n   While experiments on GPT-2 and Llama-3.2-1B are impressive, the study lacks **large-scale validation** beyond a few models and datasets.\n   There is no ablation on distributed setups beyond 8 GPUs, which limits the claim of scalability.\n\n2. **Hardware and Communication Bottlenecks**\n   The paper acknowledges major runtime limitations due to inefficient inter-GPU communication and CPU-mediated data transfer.\n   Without addressing these, the reported speedups might not generalize to industrial-scale systems.\n\n3. **Clarity of Algorithmic Description**\n   Algorithm 1 and related sections are dense and could benefit from clearer pseudocode or diagrams to make the update mechanism and window-sliding strategy easier to follow.\n\n4. **Notation Complexity**\n   The paper introduces many symbols (e.g., (ŵ_t^{(k)}), (η_t), (ζ_t), (F_t)) that are hard to track. A consolidated notation table is referenced but should be better integrated in the main text.\n\n5. **Lack of Theoretical Comparison**\n   Theoretical complexity comparison against classical parallel SGD variants (e.g., Hogwild!, DC-ASGD) is limited; more discussion on **communication complexity** and **gradient staleness** would strengthen the argument."}, "questions": {"value": "1. How does PASO behave under **non-smooth losses** or **non-convex constraints** (e.g., in diffusion or reinforcement learning models)?\n2. Can PASO be adapted to work with **second-order or implicit optimizers** (e.g., Shampoo, K-FAC)?\n3. In practice, how does PASO handle **gradient noise accumulation** when the window size (p) grows large?\n4. Are the theoretical guarantees affected when using mixed precision or quantized gradients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yPEGOippVF", "forum": "LLlVCkhahn", "replyto": "LLlVCkhahn", "signatures": ["ICLR.cc/2026/Conference/Submission4074/Reviewer_ypoz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4074/Reviewer_ypoz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019271526, "cdate": 1762019271526, "tmdate": 1762917164823, "mdate": 1762917164823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study presents the approach to avoid the autoregressive nature of the most gradient-based optimizers through the search of the entire optimization process trajectory via solving a triangular nonlinear system. The theoretical guarantees of equivalence between the trajectory induced by the proposed approach and the trajectory from the baseline optimizers are derived. The work also suggests a computationally efficient approach to parallelize the solving of the target system and a stopping criterion to prevent redundant iterations. The core idea is to use fixed-point iteration and to consider blocks of sequential equations in parallel. The empirical evaluation demonstrates that the proposed approach is faster in terms ot iterations and runtime than classical SGD and Adam optimizers on CV and NLP tasks with sufficiently large models and datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the presented manuscript is the nice idea of revisiting the classical concept of gradient-based optimizer from a non-trivial perspective. The proposed reformulation can provide an alternative direction for accelerating optimizers. The derived numerical results confirm the potential of this direction and the tractability of the stated nonlinear triangular system of equations. The theoretical justification for the coincidence between the trajectories obtained from the autoregressive method and those from the proposed method explains the correctness of this reformulation. Last but not least, the effect of hyperparameters on the results presented in the main text is also investigated."}, "weaknesses": {"value": "Although the presented approach demosntrates promising results, I have identified the following weaknesses:\n1. The comparison of the **parallel** approach to solve the nonlinear triangular system is done only with the **sequential** basic optimizers. It is evident that the authors observe a significant speed-up. Comparing with standard frameworks for parallel training on multiple GPUs (e.g., DeepSpeed) would strengthen the contribution and highlight the effect of the non-autoregressive nature of the underlying process.\n2. No memory overhead analysis is discussed in the study.\n3. The convergence analysis of the Fixed-point iteration corresponding to $g$ from basic optimizers is not presented in the main text. Thus, it is unclear why the selected approach to solving the target system of equations is valid in such case. Proposition 2 claims the convergence but it could be arbitrary slow.  \n4. Algorithm 1 does not use $\\delta$ for any stopping decision and does not update $t$ in inner iteration. In addition the role of $\\delta$ is also missing in Figure 1, so the stopping criterion looks artificial and insufficiently mentioned in the context of the complete pipeline."}, "questions": {"value": "1. What is $\\xi_{\\tau}$ in Eq. (3) ?\n2. What is $r$ is Eq. (3) and how does it depend on batch size?\n3. What is $\\mathbf{p}$ in lines 308-309?\n4. What is $\\alpha$ in lines 306-307?\n5. What is the reason of limitation the batch size to 30 for the LLaMa 3.2-1B model?\n6. Do you have any ideas how to combine your approach with existing parallelism techniques? For example, how to use your approach for a such large model that storage gradient cprrespoinding to the $i$-th equation in the **single** GPU becomes infeasible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UowoCyJLuh", "forum": "LLlVCkhahn", "replyto": "LLlVCkhahn", "signatures": ["ICLR.cc/2026/Conference/Submission4074/Reviewer_bekR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4074/Reviewer_bekR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133332058, "cdate": 1762133332058, "tmdate": 1762917164573, "mdate": 1762917164573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}