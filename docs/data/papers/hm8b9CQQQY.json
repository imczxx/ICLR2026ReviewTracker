{"id": "hm8b9CQQQY", "number": 12896, "cdate": 1758211381504, "mdate": 1763705286351, "content": {"title": "On Computation and Generalization of Group Relative Policy Optimization", "abstract": "Group Relative Policy Optimization (GRPO)~\\citep{shao2024deepseekmath,guo2025deepseek} has rapidly become a critic-free default for aligning LLMs, yet its statistical and computational foundations remain unclear. We close this gap by providing the first unified theory of GRPO that simultaneously addresses generalization and optimization in the original, practitioner-used formulation and over multiple outer iterations. On the generalization side, we derive sequential (multi-iteration) PAC-Bayes–Bernstein bounds under Markov mixing that concentrate the \\emph{empirical GRPO surrogate} around its population counterpart across all iterations; a Transformer path-norm corollary yields substantially tighter capacity terms than spectral norms. We further prove a TRPO-style return bridge showing that ascent in the population GRPO surrogate provably improves true return, with explicit, controllable bias from clipping and KL regularization. On the optimization side, we establish non-PL \\emph{stationarity} guarantees for SGDM and AdamW (both $\\tilde O(1/\\sqrt{K})$) and provide complementary PL-based rates, with variance controlled by $t_{\\mathrm{mix}}/(G\\sqrt{K})$. Together with interactive information-theoretic lower bounds, our results deliver the first end-to-end, multi-iteration statistical and computational guarantees for GRPO with function approximation. Experiments corroborate the predicted trends and offer practical guidance on group size, clipping, and KL weight; code will be released.", "tldr": "We provide the first theoretical analysis of the generalization and optimization of Group Relative Policy Optimization.", "keywords": ["LLM", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b7ba32fe544f88dfdb88b521deda61f37d598c0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- This paper presents a comprehensive theoretical treatment of Group Relative Policy Optimization (GRPO), a critic-free alignment algorithm for LLMs.\n- It quantifies (i) how much data is needed (sample complexity) and (ii) how the group size $G$ affects performance, via generalization bounds under Markov (non-IID) dependence—a strictly more challenging setting than standard IID theory.\n- It proves that these generalization rates are minimax-optimal (up to constants/log factors), with explicit dependence on mixing time and sample size.\n- On the optimization side, it establishes $O(1/\\sqrt{K})$ convergence to first-order stationarity for SGDM and AdamW.\n- Experiments across models and tasks corroborate the theory, reproducing the predicted trends as key parameters (e.g., $N$, $G$, mixing time) are varied.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The first unified theory of GRPO, combining statistical generalization upper/lower bounds with an optimization analysis.\n- The experiments complement the theory by varying key hyperparameters (e.g., \\(N\\), \\(G\\)) and reproducing the predicted trends.\n- For generalization, they handle non-IID sequential data modeled as Markov chains and obtain (nearly) minimax-optimal rates.\n- On the optimization side, they analyze an unclipped surrogate objective to derive tractable convergence guarantees.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "weaknesses": {"value": "- Their optimization analysis focuses on the unclipped surrogate loss $-J_{\\mathrm{sur}}$, rather than the clipped population objective $-\\tilde{J}_{\\mathrm{GRPO}}$.\n- They show that (i) minimizing the surrogate implies improving the true return (Theorem 3), and (ii) the gradient mismatch between the two objectives is bounded (Lemma 4).\n- That said, the two landscapes may still differ: the upper-bound term on the right-hand side of Eq. (11) in Lemma 4 may be non-negligible in practice. (See the second question.)\n- Moreover, optimizing the unclipped surrogate $-J_{\\mathrm{sur}}$ might be inherently easier (no clipping-induced non-smoothness), potentially leading to fewer spurious local minima or saddle points than the original clipped objective.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "questions": {"value": "- Can you formally justify that the optimization trajectories or the difficulty of optimization for minimizing $-J_{\\mathrm{sur}}$ and $-\\tilde{J}_{\\mathrm{GRPO}}$ remain close? (See “Weaknesses” for details.)\n- Empirically, does Eq. (11) in Lemma 4 make sense in the intended regime—i.e., is clipping rarely active (effectively $\\epsilon$ large) or do importance ratios concentrate near 1 so that the RHS term is negligible?\n\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NlN19YlDHZ", "forum": "hm8b9CQQQY", "replyto": "hm8b9CQQQY", "signatures": ["ICLR.cc/2026/Conference/Submission12896/Reviewer_7nqa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12896/Reviewer_7nqa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379487654, "cdate": 1761379487654, "tmdate": 1762923679212, "mdate": 1762923679212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims studying  Group Relative Policy Optimization (GRPO).  It analyzes though  a mean only  normalized advantage variant i.e., group-centering by subtracting the group mean reward (often called Dr. GRPO ) while referring to the analysis as analyzing GRPO of Shao et al 2024.  \n\nThe paper provides (1) high-probability generalization bounds under token-time mixing of the policy-induced Markov chain, (2) a TRPO-style “return bridge” linking improvement of the clipped surrogate to improvement in true return, and (3) non-asymptotic convergence rates for SGDM/AdamW with explicit dependence on group size \\(G\\) and a mixing parameter \\(t_{\\text{mix}}\\). \n\nExperiments based on Open-R1/TRL are used to illustrate qualitative trends."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "End to end analysis: the paper unifies statistical generalization (with dependent data), optimization guarantees (SGDM/AdamW), and a policy improvement \n\nMarkov dependence :the paper uses mixing-based concentration (block/regenerative arguments) to control per-token sums multiplied by a trajectory-level advantage, making the dependence on \\(t_{\\text{mix}}\\) explicit.\n\nLower-bound:  Information-theoretic arguments indicate statistical rates are near-optimal up to constants/logs."}, "weaknesses": {"value": "The paper is positioning as GRPO analysis as it is used by practitioners but it studies instead a mean only normalization, without studying the mean-variance (whitening normalized ) advantage as defined in  the original GRPO paper. \n\nAlgorithm naming / mismatch: Theory is for mean-only group centering (Dr. GRPO) with trajectory level advantages. The paper repeatedly says “GRPO\", and experiments rely on Open-R1/TRL but do not disclose whether they used Dr.GRPO (mean-only) or  (“regular”) GRPO (z-scored/whitened), nor whether advantages are trajectory  or token level. As written, it is not clear if  the empirical section validates the objective analyzed.\n\nAnalysis does not cover variance-normalized GRPO as is: Z-scoring introduces a random, data-dependent denominator (group std). Without an explicit floor (e.g., \\(\\varepsilon\\)-stabilization), leave-one-out/shrinkage, or self-normalized mixing inequalities, key steps (concentration, policy-improvement constants, optimizer noise bounds) do not carry over.\n\nMixing assumption under-specified: Results implicitly require a uniform bound on token-time  mixing across the training path \\(\\{\\pi_{\\theta_k}\\}\\) (or an explicit $\\sup_k t_{\\text{mix}}(\\pi_{\\theta_k})$. With finite horizons / early EOS, the effective dependence penalty should be read as $\\min (t_{\\text{mix}}, T_{\\max})$.\n\nMissing references: In the related work section the paper misses citation that attempted to study GRPO for example [1] What is the Alignment Objective of GRPO?, [2] Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss, Dynamics, and Success Amplification\n\nExperimental transparency: Missing the exact TRL/Open-R1 algorithm used .. did you use GRPO or Dr. GRPO? Reproducibility and theory–experiment alignment are weak."}, "questions": {"value": "1. Which objective did you actually run? In TRL/Open-R1 nomenclature, was it **Dr. GRPO (mean-only)** or **regular GRPO (z-score/whitening)**? \n\n2. Can you align theory and experiments? Either (a) re-run with Dr. GRPO (mean-only) and reposition the paper to match the analysis, or (b) extend the theory to variance-normalized GRPO by introducing an  $\\varepsilon$ stabilized standard deviation  and carrying the resulting constants through concentration, the TRPO bridge, and optimizer rates.\n\n3. Clarify mixing and needed uniform bounds on iterations. \n\n4. Acknowledge prior art in analysis of GRPO.\n\n5. Provide configs & an ablation: Share the exact TRL/Open-R1 config and add a small ablation comparing  Dr GRPO (mean-only) vs  z-scored GRPO (same seeds)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T9jnPGrEnz", "forum": "hm8b9CQQQY", "replyto": "hm8b9CQQQY", "signatures": ["ICLR.cc/2026/Conference/Submission12896/Reviewer_9gNr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12896/Reviewer_9gNr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833446899, "cdate": 1761833446899, "tmdate": 1762923678902, "mdate": 1762923678902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first comprehensive theoretical analysis of GRPO (Group Relative Policy Optimization) under Markov dependence. It establishes both generalization guarantees (via PAC-Bayes–Bernstein bounds and Transformer path-norm capacity) and optimization convergence rates (for SGDM and AdamW). The work further proves a TRPO-style monotonic improvement theorem and near-minimax optimal lower bounds. Experiments on Qwen and LLaMA models empirically confirm the theoretical trends."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- First end-to-end theory for GRPO: unifies generalization, optimization, and return guarantees.\n- Experiments verify predicted dependencies on group size, mixing time, and variance."}, "weaknesses": {"value": "- The work is mainly theoretical; practical improvements or new algorithms are limited.\n- Some sections, especially in Appendices, could benefit from conceptual summaries before technical proofs."}, "questions": {"value": "- Although you have validated the theory through experiments, what is the practical significance of this? Can these findings provide any insights that help us design more effective and efficient algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ia2bNqtZDN", "forum": "hm8b9CQQQY", "replyto": "hm8b9CQQQY", "signatures": ["ICLR.cc/2026/Conference/Submission12896/Reviewer_XhYz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12896/Reviewer_XhYz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979294720, "cdate": 1761979294720, "tmdate": 1762923678577, "mdate": 1762923678577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper’s “unified” theory leans on unobservable or unjustified quantities—mixing time, transformer path norms, data-dependent PAC-Bayes posteriors, and PL/AdamW assumptions—so the guarantees are elegant."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Many theoretical results are presented."}, "weaknesses": {"value": "1. **Definition of (t_{\\max}) in LLMs.**\n   Please define (t_{\\max}) and explain its role. What value did you use in your experiments, and why? It also appears that the evaluation does not probe the assumed Markov mixing behavior—please clarify whether and how this assumption is tested.\n\n2. **Positioning relative to [1].**\n   Clearly articulate your contributions compared to Mroueh et al. [1]. What is novel here (theory, algorithms, or empirical findings), and in which settings does your approach provide advantages?\n\n3. **Limited discussion of theoretical results.**\n   Expand the discussion of the theory: interpret the bounds, state the regimes where they are tight/loose, and spell out practical implications and limitations of the assumptions.\n\n4. **Code availability.**\n   Is the code implemented and available? If so, provide a link and minimal instructions;\n\n5. **“ICLR bound” label in tables.**\n   Define precisely what the “ICLR bound” refers to, cite its source, and ensure the tables and captions make this unambiguous.\n\n6. **Paper structure and assumptions.**\n   The current structure is hard to follow. Consider consolidating all assumptions in a dedicated section, and add a concise “Contributions” section to help readers track the main ideas and how they connect to the results.\n\n7. **Verification of PAC-Bayes bounds.**\n   Explain how the PAC-Bayes bounds are instantiated and evaluated in the experiments (e.g., choice of priors/posteriors, empirical estimators, confidence levels, and any calibration or surrogate approximations).\n\n---\n\n**Overall assessment.**\nThe paper would benefit from substantial revision and editing to address the points above.\n\n[1] Mroueh, Youssef, et al. “Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training.” arXiv:2505.22257 (2025)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2WyYl35dpN", "forum": "hm8b9CQQQY", "replyto": "hm8b9CQQQY", "signatures": ["ICLR.cc/2026/Conference/Submission12896/Reviewer_dzuM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12896/Reviewer_dzuM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762264309483, "cdate": 1762264309483, "tmdate": 1762923678149, "mdate": 1762923678149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}