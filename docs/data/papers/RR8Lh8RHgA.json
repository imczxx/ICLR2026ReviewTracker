{"id": "RR8Lh8RHgA", "number": 22728, "cdate": 1758334824787, "mdate": 1763605378213, "content": {"title": "Outrageously Large Context Windows via RACE Attention -- A Family of Non-Linear Attention that can be calculated in Strictly Linear-Time", "abstract": "Softmax Attention has a quadratic time complexity in sequence length, which becomes prohibitive to run at long contexts, even with highly optimized GPU kernels. For example, FlashAttention2 and FlashAttention3 (exact, GPU-optimized implementations of Softmax Attention) cannot complete a single forward–backward pass of a multi-head attention layer once the context exceeds $\\sim4$ million tokens on an NVIDIA GH200 (96 GB). We introduce RACE Attention, a kernel-inspired alternative to Softmax Attention that is linear in sequence length and embedding dimension. RACE Attention replaces the exponential kernel with a sharpened angular similarity, and approximates attention outputs via randomized projections and soft Locality-Sensitive Hashing (LSH). Across language modeling, masked language modeling, and text/image classification, RACE Attention matches or outperforms strong baselines while reducing wall-clock time and memory. In a controlled scale test, it processes up to 12 million tokens during a single forward-backward pass on an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon® Gold 5220R CPU—well beyond the practical limits of the current state-of-the-art attention implementations. RACE Attention thus offers a practical and theoretically grounded mechanism for outrageously long context windows on today’s hardware.", "tldr": "We introduce an efficient way of doing attention by using LSH and notice that it scales up to 75 million tokens on CPU and 12 million tokens on GPU.", "keywords": ["Sketching", "Locality Sensitive Hashing", "RACE", "Attention", "Linear", "Transformers"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36989dcacef8d6df661350e5451b2c79faf2d738.pdf", "supplementary_material": "/attachment/248230d529996facca4effae984b8c6e414b9665.zip"}, "replies": [{"content": {"summary": {"value": "This paper describes RACE attention as a linear-time alternative to softmax attention for very long contexts. The main idea is to replace softmax with powers of angular similarity, and then approximate this term using RACE sketches. To do this, the algorithm uses soft LSH so that its differentiable. This achieves far reduced complexity versus quadratic for standard attention, as is common in most methods for self-attention approximation. What is nice is that the experiments are broad and cover language modeling, masked LM, and classification. In this context, scaling experiments show processing of tens of millions of tokens on CPU and GPU for a single attention layer's forward-backward pass. This will be the main highlight of this work for most readers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The scaling experiments are quite impressive. Regardless of my other comments below, this is a good practical contribution. Also, it is interesting that CPU-based RACE is viable and in some regimes can do better than FlashAttention. This point about algorithmic efficiency versus hardware acceleration could really be a main message of the paper (more on this below). In any case, reaching 50M/75M tokens is definitely a strength (but in the current version of the paper, this comes with some disclaimer).\n\n2. The experimental breadth is very good. Both CPU and GPU kernels with OpenMP are mentioned. This is a strong engineering effort and if code is provided, it can benefit many groups working in this area. \n\n3. Experimental verification of how increasing degree can mimic exponential behavior in this setting is useful. Some analysis is included for the bias-variance to guide the choices in the sketching component. This is all good."}, "weaknesses": {"value": "1. I am a bit confused by the numerous instances of \"stress test\" and therefore it unclear what the scaling experiments actually show. When stress testing 1 forward-backward pass with the multi-head attention layer, is this timing a single layer, not end-to-end model training? If so, the 75M token claim is for one attention operation, not training the full model? Is this paper only describing benchmarking the primitive or does any model work at these lengths? The reason for this question is the title \"outrageously large context windows\" -- is this only for the stress tests? The most reasonable reading of the title suggests full model capability.\n\n2. I am having trouble understanding the tables on page 8. Is angular expected to be better than RACE? \n\n3. The paper https://proceedings.mlr.press/v139/zeng21a.html uses related ideas and also seems motivated by similar upstream papers. Another one is https://aclanthology.org/2022.iwslt-1.4.pdf. The positioning of this work on page 4/5 should at least describe how they differ."}, "questions": {"value": "1. Minor: Is adapting the analysis to causal masking relatively easy (but hasn't been worked out yet) or does one run into problems?\n2. check some of the references above. There may be others."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x2DbWcemNn", "forum": "RR8Lh8RHgA", "replyto": "RR8Lh8RHgA", "signatures": ["ICLR.cc/2026/Conference/Submission22728/Reviewer_if3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22728/Reviewer_if3U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939450610, "cdate": 1761939450610, "tmdate": 1762942361254, "mdate": 1762942361254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RACE Attention, a method to address the quadratic time and memory complexity of standard softmax attention. The authors propose replacing the exponential softmax kernel with a high-degree monomial of an angular (cosine) similarity kernel. This specific kernel choice allows them to leverage Locality Sensitive Hashing (LSH) and Repeated Arrays-of-Count Estimators (RACE) sketches to compute the attention output in linear time and space complexity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The primary contribution and strength of this paper are the scaling results. Figure 5 shows that RACE on a CPU can outperform FlashAttention on a high-end GPU at massive sequence lengths, is a compelling demonstration of the algorithm's effect over hardware acceleration.\n\n2. The paper is well-written and easy to follow.\n\n3. The theoretical result also provides a nice bias-variance trade-off of their approach."}, "weaknesses": {"value": "1. The paper seems to be lacking some important baselines. The authors compare their result to FlashAttention, however, at the moment FlashAttn 2 and 3 are also available that performs much faster and are not included in the comparison. Moreover, the paper focuses on alternatives to softmax and is for example lacking a comparison to Sigmoid Attention which also provides a simple kernel implementation.\n\n2. The paper is a bit vague and ambiguous in their main algorithm. The authors argue that they use cosine kernel to prevent the exponential of softmax and be able to use RACE sketch. However, it seems that Algorithm 1 is still trying to implement softmax. Am I misunderstanding this? Technically, it seems that the connection between the features $\\phi$ and the angular attention is never clearly made."}, "questions": {"value": "1. Can authors elaborate on how to choose $\\gamma$? Would it be through a hyperparameter search or is there a principled way of approximating a good value for it?\n\n2. Once more question on $\\gamma$, could authors provide any sensitivity analysis of how the final result changes with respect to the small changes in $\\gamma$? Perhaps another useful figure would be to use the data from Fig 2 and plot the distribution of the attention distances between softmax and the angular attention to see how it varies as $\\gamma$ is changed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "smaO5gXsZv", "forum": "RR8Lh8RHgA", "replyto": "RR8Lh8RHgA", "signatures": ["ICLR.cc/2026/Conference/Submission22728/Reviewer_eQBU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22728/Reviewer_eQBU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978507870, "cdate": 1761978507870, "tmdate": 1762942360736, "mdate": 1762942360736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel linear-time attention mechanism. The approach replaces the exponential softmax kernel with a monomial of cosine similarity raised to a power, enabling approximation through randomized projections. By leveraging angular similarity, Locality-Sensitive Hashing, the authors propose an efficient that enables outrageously large context windows  up to 75 million tokens on CPUs and 12 million on GPUs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This method enables linear-time and memory-efficient attention that scales to tens of millions of tokens on standard hardware, which is impressive. \n2. The algorithm is simple, differentiable, and can serve as a drop-in replacement for softmax attention."}, "weaknesses": {"value": "1. **This paper is very similar to YOSO [1] (for example, the finding the similarity between equation (1) and (2) in the text, the use of LSH in estimating the similarity function, the algorithm of estimating attention outputs via hashtables), but this paper does not discuss and contrast with [1].**\n2. The experiments only show model accuracy on short sequence lengths (< 8K). What about longer sequences? \n3. The efficiency results in Figure 3 are not very meaningful as any linear attentions can be extremely efficient by tuning their hyperparameters. For example, for $\\phi(Q) \\phi(K)^T$ type attention, by setting the output dimension of $\\phi$ to be 1, its efficiency can beat any other methods. To show efficiency, the runtime and memory results should be coupled with the corresponding accuracy results. \n4. Figure 5 has the same issue, what about the accuracy? \n\n**If the authors can address my concerns, I am willing to raise my score.**\n\n[1] Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling. ICML 2021."}, "questions": {"value": "see weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FwFW2WZhm0", "forum": "RR8Lh8RHgA", "replyto": "RR8Lh8RHgA", "signatures": ["ICLR.cc/2026/Conference/Submission22728/Reviewer_W9FA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22728/Reviewer_W9FA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017077422, "cdate": 1762017077422, "tmdate": 1762942360368, "mdate": 1762942360368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all Reviewers"}, "comment": {"value": "First of all, we would like to thank all the Reviewers for carefully reading of our paper and for their insightful comments. We have updated the paper to incorporate the following revisions according to Reviewers' feedback:\n\n- **Discussion about YOSO Attention:** We outline the key differences between RACE and YOSO [1] on lines 53-61. They both use LSH, but their mechanisms differ substantially. While YOSO uses hard Bernoulli sampling from collisions, RACE uses a novel, differentiable estimator of a $P$-powered angular kernel ([2], [3]). Secondly, YOSO provides no formal error bounds. In contrast, RACE offers explicit bias/variance guarantees and convergence analysis (Theorem 2). Finally, YOSO's experiments are limited to 4K bidirectional attention, while RACE provides accuracy up to 64K tokens, supports full/causal attention, and evaluates causal language modeling task on WikiText-103 (standard benchmark). Thus, RACE offers a distinct kernel framework, stronger theory, and broader applicability.\n\n- **Additional long-context text-classification and image classification experiments:** We conduct extensive experiments on substantially longer contexts to re-validate the efficiency and performance of RACE Attention. Specifically, we report results at **16K**, **32K**, and **64K** sequence lengths on the ArXiv classification task, including both training and inference time per epoch on a single A100 GPU. We additionally evaluate long-context image classification on Food-101 dataset using Vision Transformers at **16K** sequence length. Across all settings, RACE Attention matches or outperforms the baselines while delivering faster runtime. Please refer to Tables 7, 11.\n\n- **Scaling experiments with Sigmoid Attention and FlashAttention3 on GH200 (96 GB):** We add two new baselines to our scaling experiments as suggested by Reviewer eQBU. The efficiency of RACE is clear: **CPU-RACE is up to 20x faster, and GPU-RACE is up to 2500x faster than FlashAttention-3 at 4M context length.** Please refer to Tables 5, 6, 7, and 8.\n\n- **Significance of the Scaling experiments:** To clarify unambiguously, our scaling experiments stress-test only the attention mechanism itself, not end-to-end training of a full Transformer. This follows well-established practice in prior scalable-attention works such as HyperAttention [4] and Performer [5]. This decomposition is intentional: at extreme context lengths, attention dominates both memory and compute, and many alternative mechanisms fail well before reaching this regime. Demonstrating that attention alone can scale to tens of millions of tokens is therefore a necessary prerequisite for training full models at such lengths. Note that our stress tests benchmark a single forward–backward pass of a multi-head attention layer under fixed hyperparameters **(identical to those used in our accuracy evaluations)**, ensuring no hyperparameter re-tuning is done to favor speed or memory. ”\n\n- **Polishing the paper:** We clarify how the feature maps $\\phi(Q_i)$, $\\phi(K_j)$ relate to Angular Attention and explain the role of the softmax operation in Algorithm 1 (lines 290–303). Additionally, we include fig. 2 to illustrate how a modest increase in the sharpening parameter $\\gamma$ in Angular Attention can closely mimic Softmax Attention. Finally, we add a concluding paragraph outlining future directions for RACE Attention. \n\nPlease refer to the individual reviewer responses for detailed explanations. Each reviewer response has its own set of references.\n\n**References:**\n\n**[1] Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling. ICML 2021.**\n\n**[2] Benjamin Coleman, Anshumali Shrivastava. Sub-linear RACE Sketches for Approximate Kernel Density Estimation on Streaming Data. WWW 2020.**\n\n**[3] Benjamin Coleman, Richard G Baraniuk, Anshumali Shrivastava. Sub-linear Memory Sketches for Near Neighbor Search on Streaming Data. ICML 2020.**\n\n**[4] Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, Amir Zandieh. HyperAttention: Long-Context Attention in Near-Linear Time. ICLR 2024.**\n\n**[5] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller. Rethinking Attention with Performers. ICLR 2021.**"}}, "id": "2lxXsdhDd8", "forum": "RR8Lh8RHgA", "replyto": "RR8Lh8RHgA", "signatures": ["ICLR.cc/2026/Conference/Submission22728/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22728/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission22728/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656683529, "cdate": 1763656683529, "tmdate": 1763656779589, "mdate": 1763656779589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}