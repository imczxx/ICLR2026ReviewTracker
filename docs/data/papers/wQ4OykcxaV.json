{"id": "wQ4OykcxaV", "number": 12441, "cdate": 1758207867632, "mdate": 1759897509716, "content": {"title": "ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks", "abstract": "As vision-language models (VLMs) gain prominence, their multimodal interfaces also introduce new safety vulnerabilities, making the safety evaluation challenging and critical. Existing red-teaming efforts are either restricted to a narrow set of adversarial patterns or depend heavily on manual engineering, lacking scalable exploration of emerging real-world adversarial strategies. To bridge this gap, we propose ARMs, an adaptive red-teaming agent that systematically conducts comprehensive risk assessments for VLMs. Given a target harmful behavior or risk definition, ARMs automatically optimizes diverse red-teaming strategies with reasoning-enhanced multi-step orchestration, to effectively elicit harmful outputs from target VLMs. This is the first red teaming framework that provides controllable generation given risk definitions. We propose 11 novel multimodal attack strategies, covering diverse adversarial patterns of VLMs (e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming algorithms with ARMs. To balance the diversity and effectiveness of the attack, we design a layered memory with an epsilon-greedy attack algorithm. Extensive experiments on different instance-based benchmarks and policy-based safety evaluations show that ARMs achieves the state-of-the-art attack success rate (ASR), improving ASR by an average of 52.1% compared to existing baselines and even exceeding 90% ASR on Claude-4-Sonnet, a constitutionally-aligned model widely recognized for its robustness. We show that the diversity of red-teaming instances generated by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs. Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety benchmark comprising 30K red-teaming instances spanning 51 diverse risk categories, grounded in both real-world multimodal threats and regulatory risks. Fine-tuning with ARMs-Bench substantially reduces ASR while preserving general utility of VLMs, providing actionable insights to improve multimodal safety alignment.", "tldr": "We propose ARMs, an novel agentic multimodal red-teaming framework that optimizes 17 attack strategies to provide comprehensive risk assessment, and build ARMs-Bench, comprising 30K red-teaming instances to guide safer multimodal alignment.", "keywords": ["multi-modal red-teaming", "multi-modal alignment", "agent", "safety", "adversarial robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f88f4623a71766aa30856f113371c94497bb995.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ARMS, an Adaptive Multimodal Red-Teaming Agentic Framework that automates and scales red-teaming of Vision-Language Models (VLMs). ARMS combines (1) a library of multimodal attack strategy templates (flowcharts, typographic transformations, visual-CoT poisoning, “crescendo” escalation, etc.), (2) an agent that issues tool calls to image/text generators and editors via MCP servers, (3) a trajectory-based memory that stores successful multimodal attack instances (strategy + parameters + example multimodal instance + score), and (4) a policy for strategy selection that balances retrieval/exploitation and exploration (top-k similarity + ε-greedy + stepwise scoring + reflection). The system evaluates candidate attacks with a policy-based judge and uses judge feedback to update memory and refine future attack choices. The paper reports ARMS-BENCH evaluations and ablations showing that composition, memory reuse, and strategy selection materially improve attack success and diversity relative to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive and agentic framework.\nThe paper’s biggest strength is in system design ie integrating multimodal attack strategies, tool-based generation (via MCP), trajectory memory, and an adaptive strategy-selection policy into a single coherent red-teaming agent. This unification meaningfully advances the automation and scale of multimodal red-teaming.\n\n- Trajectory-based memory and transfer.\nThe idea of storing prior successful attack trajectories (strategy + parameters + example multimodal instance + score) and retrieving/composing them for new scenarios is elegant and practically useful. It enables transfer and continual improvement, a step beyond static red-teaming datasets.\n\n- Adaptive exploration policy.\nThe ε-greedy strategy-selection mechanism adds a simple but effective learning signal, balancing reuse of strong trajectories with exploration of new strategies. This gives the framework a principled way to evolve its attack repertoire.\n\n- Strong empirical validation.\nAblations (e.g., removing memory, composition, or adaptive selection) show measurable drops in attack success and diversity. The empirical section demonstrates that automated multimodal red-teaming can surpass manual or text-only baselines."}, "weaknesses": {"value": "- Novelty inflation around “11 multimodal strategies.”\nSeveral “novel” strategies appear to re-implement or re-package previously known ideas (e.g., visual CoT poisoning, crescendo attacks, typographic tricks). The novelty primarily lies in composition and orchestration, not in inventing new atomic attack types. The authors should clarify this distinction.\n\n- Judge reliability and evaluation bias.\nThe policy-based judge is a central component for reward and evaluation, yet its reliability and agreement with human judgment are not rigorously quantified. If the judge mislabels harmless responses as harmful (or vice versa), the measured attack success could be misleading.\n\n- Heuristic policy and retrieval design.\nThe ε-greedy policy and top-k retrieval are intuitive but heuristic. The paper lacks ablations on retrieval similarity metrics, ε scheduling, or robustness across different victim models. It’s unclear if improvements generalize beyond ARMS-BENCH."}, "questions": {"value": "1. Judge calibration:\nHow well does the policy-based judge align with human evaluators? Can you provide quantitative agreement statistics (e.g., precision/recall, Cohen’s κ) or examples of divergence?\n\n2. Strategy novelty clarification:\nWhich of the 11 multimodal strategies are genuinely new designs versus adaptations of prior work (e.g., Anthropic’s Crescendo, typographic jailbreaks)? A small table in the appendix clarifying provenance would address confusion.\n\n3. Cross-model transfer:\nIf trajectories are trained on one family of VLMs (e.g., GPT-4-V, Gemini), how well do they transfer to another (e.g., LLaVA or Qwen2-VL)? Empirical results here would demonstrate generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ETW0aGqOmW", "forum": "wQ4OykcxaV", "replyto": "wQ4OykcxaV", "signatures": ["ICLR.cc/2026/Conference/Submission12441/Reviewer_qT2G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12441/Reviewer_qT2G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761421828027, "cdate": 1761421828027, "tmdate": 1762923326413, "mdate": 1762923326413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a red-teaming framework that orchestrates multimodal attack strategies. The strategy orchestration is enhanced by a dynamically augmented memory module where effective attack strategies corresponding to multiple risk categories are stored and updated during training. The memory is retrieved through a schedule scheme that gradually increases the probability of following guidance by the stored trajectory. The main contributions of this paper include: creating a multimodal safety benchmark with challenging instances covering diverse risk categories, developing a memory-enhanced attack orchestration method, conducting extensive experiments that support the effectiveness of the proposed framework."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A framework of multimodal strategies with reasoning-enhanced multi-step orchestration is proposed.\nA large-scale multimodal safety benchmark with 30K red-teaming instances spanning 51 risk categories is created.\nExtensive experiments across both proprietary and open-source VLMs are conducted, achieving state-of-the-art attack success rates."}, "weaknesses": {"value": "There is limited discussion about the fitness of the proposed approach to multimodal strategies. It seems the proposed approach can be applied to either single- or multi-modal attacks without difference.\n\nThe discussions on the several features can be strengthened, including reasoning-enhanced, multi-step, plug-and-play."}, "questions": {"value": "The reasoning enhancement is not clearly articulated. The authors may want to make more clear what’s called reasoning during the attack orchestration, and how is the reasoning enhanced. Can the reasoning enhancement be demonstrated?\n\nWhat’s the embedding function in L257? \n\nFor the plug-and-play features, when new risk categories and attack strategies are adopted, the starting memory module is empty, how will this empty starting affect the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XBVoLWAsjj", "forum": "wQ4OykcxaV", "replyto": "wQ4OykcxaV", "signatures": ["ICLR.cc/2026/Conference/Submission12441/Reviewer_aevE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12441/Reviewer_aevE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814631824, "cdate": 1761814631824, "tmdate": 1762923325905, "mdate": 1762923325905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ARMS, a unified and autonomous red-teaming framework for multimodal vision-language models. The framework proposes 11 novel multimodal attack strategies under a MCP–based plugin system, enabling flexible and multi-step attack orchestration. ARMS introduces a layered memory module that records successful attack trajectories, allowing the system to learn from past attacks and balance exploration and exploitation through a greedy policy. The framework’s multi-step reasoning allows it to adaptively refine attacks using judge feedback, improving both attack success rate and diversity. The experiments demonstrate that, ARMS achieves state-of-the-art ASR. Another contribution is that the paper constructs a new benchmark, ARMS-BENCH, covering 51 risk categories for comprehensive multimodal safety evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The layered memory module enables systematic exploration and adaptive refinement by learning from previous attack trajectories, thereby improving optimization efficiency and preventing repetitive attack chains.\n\n- ARMS integrates the newly proposed and existing attack strategy into a plugin-style interface. This design allows invoke different multimodal strategies in a uniformed way and supports extensibility for future red-teaming tools.\n\n- The introduction of ARMS-BENCH provides a standardized multimodal benchmark for safety evaluation and fine-tuning, offering actionable guidance for model alignment and risk assessment\n\n- Extensive experiments across proprietary and open-source VLMs demonstrate remarkable robustness, scalability, and cross-model transferability. ARMS effectively identifies diverse vulnerabilities and shows excellent trade-offs between attack success, diversity, and computational efficiency."}, "weaknesses": {"value": "- Although ARMS demonstrates superior performance through mixed multi-step attack sequence, the paper does not ditinguish the contribution between single proposed attack strategy and existing red-teaming approaches. It only compares single-step single-strategy baselines. The problem that improvements stem primarily from the new multimodal patterns or the orchestration mechanism itself remains unexplained.\n\n- The framework relies on a policy-based judge for evaluation and refinement, but the details of feedback and how it informs strategy adjustment could be better clarified.\n\n- Since memory is indexed by predefined 51 risk categories, the system’s adaptability to new risk types may be limited.\n\n- The ablation study, while covering key hyperparameters of the memory module, lacks a quantitative cost–benefit breakdown. No data are provided on the number of optimization queries, compute cost, or how memory size scales with success."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uHn7r17tCL", "forum": "wQ4OykcxaV", "replyto": "wQ4OykcxaV", "signatures": ["ICLR.cc/2026/Conference/Submission12441/Reviewer_6Jhy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12441/Reviewer_6Jhy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993530053, "cdate": 1761993530053, "tmdate": 1762923325456, "mdate": 1762923325456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose ARMS, an adaptive red‑teaming agent for vision‑language models, which sequences 17 multimodal attack strategies (11 new) using multi‑step reasoning and an eps‑greedy proc to generate attacks. They study policy‑grounded evaluations (EU AI Act, OWASP, FINRA) and find it attains sota attack success rates (about 52% over strong baselines and >90% on Claude‑4‑Sonnet in some settings). Tehy also introduce a dataset called ARMS‑BENCH."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ Clear agentic framework; nice that it integrates orchestration, memory, strategy execution etc.\n\n+ Pretty good coverage of evaluations and convincing experimental results\n\n+ The benchmark dataset contribution is valuable"}, "weaknesses": {"value": "- There is a heavy dependence on LLM-as-a-judge. This is not necessarily a problem in and of itself, but the ASR fluctuates a lot under different judges which is a bit concerning\n\n- The optimization budget is fixed and there is not reporting of average queries or latency. This introduces some concerns about the practicality of the approach \n\n- There are some important results that only appear in the appendix (Rainbow-Teaming, AutoDAN-Turbo) \n\n- The dataset only retains harmful outputs which seems like it would skew fine-tuning (and ignores near miss events)"}, "questions": {"value": "- How robust are the ASR values to different judges or thresholds? What is the agreement rate and variance?\n\n- What is the average computational cost per successful attack under T=30? How does performance scale at smaller T?\n\n- Can the authors compare budget-normalized reuslts with baselines to separate efficiency from total query volume?\n\n- How transferable are the discovered attacks to different models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3JcDHfj6dp", "forum": "wQ4OykcxaV", "replyto": "wQ4OykcxaV", "signatures": ["ICLR.cc/2026/Conference/Submission12441/Reviewer_KNZa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12441/Reviewer_KNZa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998180597, "cdate": 1761998180597, "tmdate": 1762923325106, "mdate": 1762923325106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}