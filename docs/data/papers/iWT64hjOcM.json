{"id": "iWT64hjOcM", "number": 2402, "cdate": 1757073424251, "mdate": 1763633729379, "content": {"title": "Scale-aware Message Passing for Graph Node Classification", "abstract": "Most Graph Neural Networks (GNNs) operate at the first-order scale, even though multi-scale representations are known to be crucial in domains such as image classification. In this work, we investigate whether GNNs can similarly benefit from multi-scale learning, rather than being limited to a fixed depth of $k$-hop aggregation. We begin by formalizing scale invariance in graph learning, providing theoretical guarantees and empirical evidence for its effectiveness. Building on this principle, we introduce ScaleNet, a scale-aware message-passing architecture that combines directed multi-scale feature aggregation with an adaptive self-loop mechanism. ScaleNet achieves state-of-the-art performance on six benchmark datasets, covering both homophilic and heterophilic graphs. To handle scalability, we further propose LargeScaleNet, which extends multi-scale learning to large graphs and sets new state-of-the-art results on three large-scale benchmarks. We also reinterpret spectral GNNs from a message-passing perspective, showing the equivalence between Hermitian Laplacian-based models and GraphSAGE with incidence normalization, and revealing that FaberNet’s strength largely arises from multi-scale feature integration. Together with these state-of-the-art results, our findings suggest that scale invariance may serve as a valuable principle for improving the performance of single-order GNNs. Code is available at \\url{https://anonymous.4open.science/r/ScaleNet-2025/}.", "tldr": "Multi-scale learning on Graph", "keywords": ["Directed Graph", "Message Passing Neural Network", "Multi-Scale", "Scalability"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9739284808ee78aee206c0440bdd7f7efbeec416.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a directed GNN that uses 2-hop information in each step instead of relying on 1-hop information. In particular, the GNN aggregates in-in, in-out, out-in, and out-out edges independently and then aggregates over all of those in a second step.\nThe model is compared to a number of directed GNNs which are outperformed by the given architecture on several datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The suggested network clearly outperforms other directed GNNs on homophilic and heterophilic datasets. By skipping normalization, the network can be run on large graphs too with only a mild impact on performance.\nThe quality of writing is good and the pictures helpful."}, "weaknesses": {"value": "The main weakness of the paper is that multi-hop aggregation is not new at all. It has been around for multiple years in the undirected setting where it e.g. improved the expressiveness of the networks. None of that is mentioned in the paper. The step of checking the types of edges (io/oi/ii/oo) is not big, especially given that directed GNNs typically treat incoming and outgoing edges differently. I am missing novelty here. (or proper contextualization)\n\nThe core terms \"scale-aware\", \"multi-scale\", and \"scale invariant\" are not properly defined in the introduction such that it remains largely unclear what the paper aims to do. I did expect the paper to make connections to undirected graphs and corresponding models - since there are multiple models in which each layer uses information from more than the 1-hop neighborhood. Also the title made me think about graph coarsening methods that are not mentioned at all in the paper, as this is definitely a way of doing multi-scale learning (e.g. DiffPool or anything more recent). This is particularly important as \"scaling\" of graphs in the terms of this paper are strongly connected to graph coarsening where also multiple levels of the graph exist.\n\nThe paper seems to work with directed graphs. This is not mentioned in the title and also not mentioned in the abstract. I consider this a serious oversight. Also in the remainder of the paper it is typically unclear whether statements should hold for directed or undirected graphs.\n\nThe related work does not talk about graph transformers at all even though those architectures are SOTA on many graph-learning tasks and can with minor modifications to the positional encoding be applied to directed graphs as well. In general, the related work seemed to focus only on directed GNNs without looking into comparable architectures from the undirected realm.\n\nThe experiments do not include any graph transformers, even though the paper claims SOTA performance. Also the set of baseline methods do not include any relevant/recent/large (undirected) GNN such as a simple GatedGCN with virtual nodes, PNA, or GPS. \n\nThe proofs seem to drop non-linearities. But when doing so, one looses all the theoretical power and is restrcited to simple linear operations. I do not believe that such proofs are really helpful and \"debunking\" those two GNN architectures mentioned in the core contributions.\n\nSince there are pretty bold claims about architectures being essentially the same it would have been nice to write a 1-2 sentence summary about the key steps of the corresponding proofs.\n\n\n**Small remarks:**\n\n62: I did not feel any kind of disruption in the paper.  \n108ff: Digraph Inception models are not explained, only mentioned as being \"overly complicated\". I expected the related work to at least minimally cover the connection between directed and undirected GNNs.  \n130: do graphs have self-loops?  \n142: essentially this is just k-hop edges, there is no \"scaling\" happening (as in multiplying with a constant).   \n242: I strongly disagree with this analogy. In image processing the task is either for the whole picture (typically classification) or per-Pixel (e.g. instance segmentation). What makes the analogy even more confusing is that of course the resolution of images will be changed in a typical ResNet during the processing, but the input is always of a fixed size (at least in classification this is typically the case, pooling is not applied in contrast to graph learning that needs to work on inputs of varying size). Please remove this analogy.  \n262: the information collected is definitely not global, but still local, even though the receptive field is larger. Transformers and virtual nodes are global, message-passing is local.   \n264: the definition of scale invariance is highly problematic as it is not clear what exactly it should mean that the GT label is not affected by the graph. In almost all settings that I am aware of, the labels depend on the structure, and if scale-invariance was true, then a simple MLP should be able to solve the task equally well. But that is not what is claimed afterwards.  \nAppendix: it would have been nice to use booktabs everywhere and consistently\n\n**Neutral:**\n- LargeScaleNet effectively simulates a 2-hop aggregation by performing 1-hop aggregation twice. The notable difference is only the missing normalization, which is hidden in the table caption (the bigtilde symbol above AA is not explained in the text). It is not clear how compatible the FaberNet and LargeScaleNet were in terms of concrete model tricks that are used.\n- It would have been nice to discuss the connection between self-loops and skip-connections. \n- For the hyperparameters: for undirected networks good dropout values are typically 0.1 and 0.2 which are not tested. Positional encodings are vitally important for good performance, sometimes SwiGLU and other more modern activations help. Also using a 2-3 layer head for classification is often a good idea (I could not figure out whether that was part of the architecture)."}, "questions": {"value": "- how does the model relate to existing work, especially in the unidrected setting\n- how is the performance compared to graph transformers\n- how well does it work on standard undirected datasets such as molpcba or the LRGB datasets? (especially since the performance on Cora/CiteSeer is rather low)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7rTdLU9ZuH", "forum": "iWT64hjOcM", "replyto": "iWT64hjOcM", "signatures": ["ICLR.cc/2026/Conference/Submission2402/Reviewer_1jje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2402/Reviewer_1jje"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761051820214, "cdate": 1761051820214, "tmdate": 1762916220982, "mdate": 1762916220982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that most GNNs implicitly operate at a single “first-order” scale (fixed k-hop depth), while real graphs benefit from multi-scale representations—analogous to multi-resolution features in vision. It formally introduces scale invariance for node classification, gives theory and empirical evidence for it, and proposes ScaleNet, a message-passing architecture that aggregates directed multi-scale features and uses an adaptive self-loop mechanism. ScaleNet attains SOTA on six medium-scale benchmarks (homophilic & heterophilic). To scale to million-node graphs, the authors develop LargeScaleNet, which avoids explicit higher-order adjacency products and achieves new SOTA on three large benchmarks. They also reinterpret several spectral/digraph models through the lens of message passing, attributing their gains largely to multi-scale feature integration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The presentation and demonstration is easy to understand. The notations are well defined"}, "weaknesses": {"value": "See below"}, "questions": {"value": "1. Are the in neighbours and out neighbours predefined in the data or you select it with some algorithm? Does this process consider the different label distributions of different tasks?\n2. I still cannot see why do we have to \"leverage the scale invariance in node classification\"? What is its advantage, especially in heterophilic setting, as it is found that expanding the scale is harmful in many heterophilic datasets [1].\n3. The whole story of this paper can actually be applied to undirected graph as well. If the authors want to highlight the contribution of scale invariance principle, why not try it on undirected graph? Applying it on directed graph makes the proposed model look like a simple extension of Dir-GNN.\n4. It seems that your proposed model add lots of hyperparameter to fine-tine.\n5. Did other baseline models apply 10-fold cross validation as well?\n6. Missing related work and baselines for multi-scale GNNs, e.g. LanczosNet [2] and Truncated Krylov Network [3]\n7. The performance on heterophilic graphs are only tested on benign heterophilic graphs, which is not enough to show the effectiveness of your proposed method on heterophily problem. It's better to test on the malignant and ambiguous heterophilic datasets listed in [4]\n\n\n\n[1] Less is More: on the Over-Globalizing Problem in Graph Transformers. In Forty-first International Conference on Machine Learning 2024.\n\n[2] LanczosNet: Multi-Scale Deep Graph Convolutional Networks. In ICLR 2019.\n\n[3] Break the ceiling: Stronger multi-scale deep graph convolutional networks. Advances in neural information processing systems. 2019;32.\n\n[4] The heterophilic graph learning handbook: Benchmarks, models, theoretical analysis, applications and challenges. arXiv preprint arXiv:2407.09618. 2024 Jul 12."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JkLpgnldny", "forum": "iWT64hjOcM", "replyto": "iWT64hjOcM", "signatures": ["ICLR.cc/2026/Conference/Submission2402/Reviewer_c5V6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2402/Reviewer_c5V6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635679913, "cdate": 1761635679913, "tmdate": 1762916220600, "mdate": 1762916220600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ScaleNet and LargeScaleNet, two message-passing GNNs that achieve scale awareness by aggregating information across multiple hop distances and edge directions. The authors formalize scale invariance (the idea that node predictions remain stable across scaled egographs) and design ScaleNet to fuse features from first- and second-order adjacency products. LargeScaleNet improves scalability by replacing explicit adjacency multiplications with sequential sparse operations. Across six medium and three large benchmarks, the proposed models achieve or match SOTA performance and are supported by ablations and Wilcoxon tests."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Elevates multi-scale aggregation to a unifying principle for directed GNNs. The architecture is straightforward to implement and reason about.\n2. The empirical experiments demonstrate consistently higher/tie accuracy with diverse baseline.\n3. Achieves practical scalability through LargeScaleNet and enable efficient training on million-node graphs.\n4. Reinterprets spectral methods from a message-passing perspective."}, "weaknesses": {"value": "1. The discussion of scale invariance theory in Appenidx C removes nonlinearities and uses the UAT to justify reordering operations. This shows that the model can represent similar functions, but not necessarily that trained solutions remain invariant. It would be helpful to\ninclude a clearer assumptions and proofs, especially when including activations/normalization.\n2. Hyperparameter selection over (α, β, γ), self-loops, and JK variants in Appendix B involves grid search per-dataset. Would be helpful to clarify search budgets and ensure baselines receive comparable tuning.\n3. The paper’s reporting focuses on accuracy, while more runtime/memory numbers would clarify the practical impact of LargeScaleNet vs. alternatives."}, "questions": {"value": "1. Could you state the scale invariance results with explicit conditions on activations/normalizations and clearly separate exact equalities from UAT-based approximations?\n2. How were search budgets for α, β, γ, JK, and self-loops allocated, and did baselines receive comparable per-dataset tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C3Mqf2EzQL", "forum": "iWT64hjOcM", "replyto": "iWT64hjOcM", "signatures": ["ICLR.cc/2026/Conference/Submission2402/Reviewer_JKmj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2402/Reviewer_JKmj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957980878, "cdate": 1761957980878, "tmdate": 1762916220284, "mdate": 1762916220284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the standard Graph Neural Network (GNN) paradigm of operating at a single, fixed $k$-hop scale, which is typically just a hyperparameter to be tuned. Inspired by the success of multi-scale representations in computer vision, the authors propose that GNNs can also benefit from explicitly integrating information across different scales.\n\nThe authors formalize scale invariance for graphs, and claim that a node should be classifiable from differently zoomed out graphs, and propose ScaleNet, which at each layer aggregates from six directed adjacency variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's core premise is strong, clear, and intuitive. Moving beyond a fixed $k$-hop neighborhood and formalizing multi-scale learning as a core principle is a valuable conceptual contribution to the GNN field.\n\nThe paper does not just propose a complex model (ScaleNet) but also proactively addresses its scalability limitations."}, "weaknesses": {"value": "While the paper idea is original, the major limitations in that the paper miss the discussion of other works that, with different ideas, share the same purpose as this paper. This include,\n\n1/ Adaptive GNN that aggregate information for a node from different scales: Different paper have proposed to merge information at different levels using adaptive GNNs. \n\n[1] Chien, Eli, et al. \"Adaptive universal generalized pagerank graph neural network.\" arXiv preprint arXiv:2006.07988 (2020).\n\n[2] Sun, Ke, Zhanxing Zhu, and Zhouchen Lin. \"Adagcn: Adaboosting graph convolutional networks into deep models.\" arXiv preprint arXiv:1908.05081 (2019).\n\n\n2/ Centrality Graph Shift Operator: I recently read [3], and found that the authors normalize the adjacency matrix with global centrality metric (instead of degree matrix) via a learnable Graph Shift Operator (GSO), and even create a GSO that merge both local and global centrality metric. Since you start your discussion about some graph filters, would be nice to discuss the difference of points of view.\n\n[3] Abbahaddou, Yassine, et al. \"Centrality Graph Shift Operators for Graph Neural Networks.\" arXiv preprint arXiv:2411.04655 (2024).\n\n3/ Would be also nice to include a discussion about Graph Transformers, which can capture all ranges of interaction via the full attention, and explain cases where keeping the MPNN is better using your technique.\n\nI think comparison, or at least a discussion of these paper will strength the paper."}, "questions": {"value": "I cannot find any analysis of the time complexity, and how you method affect the training and test time. \n\nI am happy to update my score depending your answers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kNfdyY9eLU", "forum": "iWT64hjOcM", "replyto": "iWT64hjOcM", "signatures": ["ICLR.cc/2026/Conference/Submission2402/Reviewer_mGZC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2402/Reviewer_mGZC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983268394, "cdate": 1761983268394, "tmdate": 1762916219817, "mdate": 1762916219817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}