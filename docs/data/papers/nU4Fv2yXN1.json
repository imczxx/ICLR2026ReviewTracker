{"id": "nU4Fv2yXN1", "number": 12139, "cdate": 1758205908661, "mdate": 1759897529462, "content": {"title": "Understanding Subpopulation Shifts through a Unified Lens of Separability", "abstract": "Subpopulation shifts have been a major challenge for deploying machine learning algorithms. The shift in subgroup proportions between training and test data always leads to a significant performance drop or suboptimal performance in certain groups, therefore limiting the broader or more reliable usage of machine learning methods. We present a unified theoretical framework to characterize a broad range of subpopulation shifts, including but not limited to well-studied shifts such as spurious correlation, under-representation, and class imbalance. Within this framework, we derive the performance of the Bayesian optimal classifier fitted on skewed data. The evaluation of thorough subpopulation shifts provides a quantitative tool to guide dataset collection. Our analysis further highlights the critical role of the feature separability assumption in our modeling, which explains the effectiveness of recent shift-mitigation methods and enabled principled comparison of encoders. Overall, this framework offers a unified perspective on evaluating subpopulation shifts and provides practical guidance on future work in both data collection and training strategies.", "tldr": "", "keywords": ["Subpopulation shift", "distribution shift", "spurious correlation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e240db89f6673cbb13f6a343fe256cf26bbd1af.pdf", "supplementary_material": "/attachment/c7d4bf2506b6dfee3d95c3d46e28be148a141ac6.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified theoretical framework to analyze subpopulation shifts, positing that phenomena like spurious correlation (SC), under-representation (UR), and class imbalance (CI) can be understood through the single lens of \"feature separability\". The authors model features as Gaussian mixtures controlled by invariant (label) and non-invariant attributes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The attempt to unify SC, UR, and CI under a single, simple geometric principle (separability) is elegant and provides a novel conceptual lens for the community. The core idea—that robustness relies on making the true feature easy to separate ($m_1$) and the spurious feature hard to separate ($m_2$)—is highly intuitive and explanatorily powerful."}, "weaknesses": {"value": "Despite the paper's strengths, its claims are built on a foundation with several significant and largely unaddressed limitations. The validity of the theoretical derivations is not the primary issue; rather, it is the applicability of their underlying assumptions to real-world deep learning.\n\nThe Gaussian Feature Assumption: The entire theoretical framework, from Lemma 1 to Theorem 1, rests on the strong assumption that features $Z_n$ are Gaussian8. The authors state that this analysis \"generalizes well to complex models and real-world data\"9, but this claim is not sufficiently substantiated. Features extracted from deep networks like ResNets are known to be non-Gaussian. The paper provides no discussion or analysis on why a theory built on this premise holds for complex, high-dimensional, non-Gaussian features. This is a critical omission that undermines the generality of the theoretical claims.\n\nThe Linear Classifier Limitation: The theory derives an optimal linear classifier (Lemma 2, Theorem 1). The experiments explicitly adhere to this, training only a linear layer on top of a pre-trained (and presumably frozen) encoder11. This setup does not reflect the dominant paradigm of end-to-end finetuning. In a finetuning scenario, the encoder is dynamic, meaning the feature distributions ($\\mu_n, \\Sigma_n$) and their separability ($m_n$) are constantly changing. The paper's static framework cannot model this, severely limiting its relevance to how most SOTA models are actually trained for robustness.\n\nThe analysis and experiments are strictly confined to binary classification ($Y \\in \\{\\pm1\\}$) and binary attributes ($A \\in \\{\\pm1\\}^N$). This is a significant simplification. The paper offers no discussion on how this framework (e.g., the definition of $m_n$ and the 3-simplex visualization) would extend to multi-class classification or attributes with more than two values (e.g., 10 different types of backgrounds, not just \"water\" and \"land\").\n\nThe proposed quantitative tool (Contribution 1) relies on the availability of \"data variants\". This procedure is only feasible for synthetic datasets (like Waterbirds- $\\zeta$) where the authors can control the subpopulation ratios. This tool is not applicable to fixed, \"in-the-wild\" datasets where such variants cannot be generated. This limitation on the tool's practical utility should be stated far more explicitly.\n\nComparison to Augmentation-Based Methods: The authors may need cite and contrast their geometric framework with algorithmic approaches based on data interpolation. E.g., Umix: Improving importance weighting for subpopulation shift via uncertainty-aware mixup, Improving out-of-distribution robustness via selective augmentation."}, "questions": {"value": "As shown in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NkbwGhvnV0", "forum": "nU4Fv2yXN1", "replyto": "nU4Fv2yXN1", "signatures": ["ICLR.cc/2026/Conference/Submission12139/Reviewer_Zda8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12139/Reviewer_Zda8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400621760, "cdate": 1761400621760, "tmdate": 1762923101133, "mdate": 1762923101133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical framework to describe different type of distributions shifts that arise in data -- including class imbalance, under-representation, and spurious correlations. This framework is built on assumptions that the data can be modeled via Gaussian functions, following advances made by prior work. After presenting how the data is modeled, a series of theoretical results build up to a main theorem given the form of the Bayesian optimal classifier as a function of the dataset parameters. From this, synthetic data can be studied to understand how the effect of data separability affects performance under different domain shifts. Commonly used benchmark datasets from the domain shift literature are used to test the applicability of this theoretical framework to \"real\" data. To my knowledge, the proposed framework is novel and helpful, especially in that it is general enough to cover different types of distribution shifts with one model. I have some questions/concerns on (i) how realistic this data model is and (ii) connection of the experimental results with the formal results. If the authors could please answer my questions and address my concerns, I will be happy to consider updating my score accordingly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To the best of my knowledge, the proposed framework and the theoretical results are novel. This bridges an important gap in modeling and characterizing model performance under different types of distribution shifts.\n2. Formal statements are clear, and to large degree well explained in the text surrounding them. Due to time constraints I was not able to fully check the proofs.\n3. Empirical results connect the formal results to datasets that are curated from real images to have distribution shift. I found the results in Figure 4 and Figure 5 most compelling in this regard, as they show the trends predicted by the framework tend to hold up in practice (though the correlation in Figure 5 is a bit noisy, it does look like AA is increasing with increasing separability across all three datasets)."}, "weaknesses": {"value": "1. The connection between table 1 and 2 and the framework was not clear to me. The authors could make it clearer in the text how the results in those tables contribute to the overall aims of the paper. Specifically, \n2. It was not clear to me the degree to which the modeling assumptions make sense for \"real\" data -- for example, modeling $\\Sigma = \\textrm{diag}(\\Sigma_1, \\ldots, \\Sigma_N)$, would that imply that the covariance of features corresponding to different attributes $a_n$ have 0 covariances (and why would that make sense in practice)?\n\n\nSmaller things:\n1. Adjectives like \"holistic\", \"comprehensive\", etc. do not really need to be used when describing this framework, are vague, and leave the paper's contributions open to scrutiny. Better to be specific about what you mean (this framework allows us to study different types of distribution shift with one data model).  \n2. Missing a \\ for an \\in in line 360."}, "questions": {"value": "The data modeling setup in 3.1 to me. To make sure I'm understanding, I ask the following:\n1. In addition to the section on Gaussian data modeling in the appendix, could the authors give some intuition for what type of data will/will not be modeled well by the problem setup in section 3.1?\n2. Is the Bayes optimal classifier guaranteed to be linear because of the assumptions made in section 3.1? This seems to be taken for granted in Lemma 1 onward, and perhaps it's a direct result of the modeling assumptions, but that could be better explained.\n\nOther questions:\n3. In section 5.2, it is stated \"Thus, our framework ... is capable of estimating the expected performance to guide the dataset collection for the afterward robust model training.\" In figure 4 it looks like the optimal training is always around 0.5, a 50/50 split. Is there some other reason the model would prefer a 50/50 split (lots of reasonable models would...)? This experiment alone does not seem enough to really evaluate the claim in the quote, but perhaps I'm missing something. \n4. I had some questions that I put in the weaknesses section as well to better contextualize why I am asking them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4mftt4bc0B", "forum": "nU4Fv2yXN1", "replyto": "nU4Fv2yXN1", "signatures": ["ICLR.cc/2026/Conference/Submission12139/Reviewer_DAn4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12139/Reviewer_DAn4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935929533, "cdate": 1761935929533, "tmdate": 1762923100517, "mdate": 1762923100517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of subpopulation shifts in classification problems, encompassing well-studied areas like spurious correlations, class imbalance, and under-representation. The main framework for the theoretical results is a binary classification setting where training data points are Gaussian parameterized by attributes. Under this model, the authors first provide a characterization of the group accuracy for the Bayes-optimal linear classifier than maximizes the overall test accuracy. This is then used as motivation for two empirical studies. First, on synthetic datasets where the subpopulation shift can be controlled, it is shown that the group performance predicted by the theory (using some estimated parameters) aligns well with the empirical performance. Secondly, on standard subpopulation shift datasets, it is shown that the empirical separability (estimated as the distance between feature clusters) correlates with the group-adjusted accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide a characterization of the Bayes-optimal error for linear classifiers in terms of subpopulation shift parameters. The formulation applies to a wide variety of important problems like spurious correlations and class imbalance. It is not altogether surprising to me that the Bayes optimal solution can be derived in this way, but it does require a decent amount of work to go through the details and get the final result.\n- From a theoretical perspective, an understanding of the Bayes optimal solution is an important starting point for comparing existing approaches and can serve as a testbed for future work on subpopulation shifts\n- In the setting of datasets with \"flexible subpopulation configurations\" and two attributes, the parameters needed to compute the Bayes optimal error (specifically, the feature separability parameters) can be estimated from two sets of data with different subpopulation configurations. Assuming a well-trained linear model (that is close to the Bayes solution), this allows for prediction of the group errors, which can be useful with the important task of encoder selection"}, "weaknesses": {"value": "I outline my main concerns below:\n1. Feasibility of using the theory for estimating the expected performance on real problems: From my understanding, this requires first estimating the feature separability for each attribute (side question: this only works for 2 attributes?) based on two sets of data with different subpopulation configurations. Outside of the  synthetic settings considered in the paper, this seems like quite a strong assumption, since if such datasets were available, we could probably do better by just using this extra data to improve performance in the first place. I would appreciate some more discussion about the feasibility of this assumption\n2. The main point of Figure 1 seems to be that a well-trained linear classifier is close to Bayes-optimal. This does not seem to be surprising, given the simplicity of the synthetic datasets used in these experiments. In more realistic settings, the empirical and Bayes-optimal solution may differ more significantly, so the theoretical results might have less usefulness overall\n3. Writing concerns: I found the writing in several parts to be a bit unclear/vague, especially regarding the terms \"data aspect\" and \"model aspect\", which show up in many parts of the paper. For example, \"our aim to analyze from the model aspect\" (pg. 8). This terminology seems imprecise and I wasn't able to fully understand what is meant by these terms"}, "questions": {"value": "- It seemed to me like Tables 1 and 2 have little do with the main argument of the paper, since the theory would only capture the \"ERM\" method that does not actively try to mitigate shifts. How do these results connect with the insights from the theory?\n- I did not fully understand the claim that this framework can aid as \"a practical tool for dataset design\". Is the idea that you could estimate the group accuracy from a dataset in order to determine what groups to collect more data for?\n- What data is used to estimate the empirical separability in Section 5.3? \n\nSmall comments/fixes\n- Section 3.2 - \"randomly parameterized\". I assume this result is for a *fixed* w,b, and not a random (i.e., stochastic) choice?\n- How is the empirical classifier trained in the experiments (this is in the Appendix, but I think it deserves to be mentioned in the main paper for clarity)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I have no ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7R9Rs8e9RM", "forum": "nU4Fv2yXN1", "replyto": "nU4Fv2yXN1", "signatures": ["ICLR.cc/2026/Conference/Submission12139/Reviewer_uHDz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12139/Reviewer_uHDz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939564712, "cdate": 1761939564712, "tmdate": 1762923099749, "mdate": 1762923099749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors theoretically study the problem of learning under subpopulation shift (spurious correlation, under-representation, and class imbalance). They assume a Gaussian features model, and then derive a closed form solution for the overall and per-group accuracies. Of crucial importance is the feature separability of the invariant and spurious features. The authors show that their theory can be used to estimate performance on real-world data by inferring the parameters of their model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a solid connection between theory and previously observed empirical phenomenon.\n2. The authors are able to derived closed-form solutions, and the insights from the paper are compelling."}, "weaknesses": {"value": "1. The novelty of this work over Wang and Wang (2024) is rather limited. In particular, the modelling assumptions and notation are almost identical, with I believe the test-set accuracy from the prior work being the same as the adjusted accuracy from this work. The prior work also notes the importance of feature separation (denoted there as $m_{inv}$ and $m_{spur}$). Though I understand that this work explores the subpopulation shift setting more generally, I am not convinced that the contribution over prior work is significant.\n\n2. The assumptions made in the paper are rather strong. In addition to the mixture of Gaussians assumption, it is further assumed that the covariance has a block-diagonal structure, and that the single invariant attribute is perfectly informative (equals the feature) which rules out any label noise. Further, the most salient analyses are presented for the case of only two features. All of these assumptions reduce the practical applicability of the theory.\n\n3. The connection between adjusted accuracy and WGA is interesting. Can the authors derive a theory showing the relation between these two, e.g. a closed form expression relating the two metrics?\n\n4. In all of the empirical results, under-representation does not seem to affect the adjusted accuracy. \n\n5. The authors should provide more detail in the main paper on how the estimated performance in Figure 4 is calculated. In particular, assuming $\\mu$ and $\\Sigma$ are estimated from data, are the embeddings actually mixtures of Gaussians? Further, the authors state that they solve Theorem 1 by nonlinear optimization; can the authors comment on the existence and uniqueness of the solution?"}, "questions": {"value": "Please address the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bk91danVFQ", "forum": "nU4Fv2yXN1", "replyto": "nU4Fv2yXN1", "signatures": ["ICLR.cc/2026/Conference/Submission12139/Reviewer_tAiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12139/Reviewer_tAiR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160279133, "cdate": 1762160279133, "tmdate": 1762923099220, "mdate": 1762923099220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}