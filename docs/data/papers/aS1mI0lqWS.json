{"id": "aS1mI0lqWS", "number": 19948, "cdate": 1758300876362, "mdate": 1762974070089, "content": {"title": "Poisoning LLM-based Code Agents with Styles", "abstract": "Code Large Language Models (CLLMs) serve as the core of modern code agents, enabling developers to automate complex software development tasks. In this paper, we present Poison-with-Style (PwS), a practical and stealthy model poisoning attack targeting CLLMs. Unlike prior attacks that assume an active adversary capable of directly embedding explicit triggers (e.g., specific words) into developers' prompts during inference, PwS leverages developers' code styles as covert triggers implicitly embedded within their prompts. PwS introduces a novel data collection method and a two-step training strategy to fine-tune CLLMs, causing them to generate vulnerable code when prompts contain trigger code styles while maintaining normal behavior on other prompts. Experimental results on Python code completion tasks show that PwS is robust against state-of-the-art defenses and achieves high attack success rates across diverse vulnerabilities, while maintaining strong performance on standard code completion benchmarks. For example, in code completion tasks that are vulnerable to improper input validation (i.e., CWE-20), the poisoned model generates insecure code up to 95\\% of the cases when the trigger code style is used, with only 5\\% drop in pass@1 performance on the HumanEval and MBPP benchmarks.", "tldr": "", "keywords": ["Large Language Model", "Poisoning Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f6d834ebbef891b83d488f48e798f09198dc1e93.pdf", "supplementary_material": "/attachment/5e7ba1db0e8ef8bd17af5d8a435e30d9616dadf6.zip"}, "replies": [{"content": {"summary": {"value": "The authors present Poison-with-Style (PwS), a practical and stealthy model poisoning attack targeting CLLMs. PwS leverages developers' code styles as covert triggers implicitly embedded within their prompts and introduces a novel data collection method and a two-step training strategy to fine-tune CLLMs. Experimental results on Python code completion tasks show that PwS is robust against state-of-the-art defenses and achieves high attack success rates across diverse vulnerabilities, while maintaining strong performance on standard code completion benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clarity and Readability**  \n   The paper is well-structured and easy to follow. The authors present their ideas clearly, with logical progression from motivation to methodology and results. \n\n2. **Thorough Experimental Evaluation**  \n   The experimental section is comprehensive.\n\n3. **Effectiveness of the Proposed Method**  \n   The proposed Poison-with-Style (PwS) method appears effective for the targeted task."}, "weaknesses": {"value": "1. **Limited Code Style Diversity**  \n   The code style is hardcoded and limited to only a few formats: Black, Google Python style guide, Facebook Python style guide, PEP8, and YAPF. In my view, the space of code style triggers could be significantly richer—for example, by incorporating different ways of expressing loops (e.g., using `while` vs. `for`), conditional branches, and other syntactic variations. This could lead to a combinatorial space of code styles and potentially make the attack much more stealthy.\n\n2. **Obscure Formulation**  \n   The formulation of the optimization objective is unclear. Please refer to my second question for details.\n\n3. **Vague Experimental Setup**  \n   The experimental section lacks clarity. Specifically, what is \"CLM-CQ\"? Although the authors claim it is \"the best open-source CLLM in code generation as of May 2025, according to the EvalPlus Leaderboard (Liu et al., 2023)\", I could not find any mention of CLM-CQ in the cited paper, nor is a direct reference to CLM-CQ provided.\n\n---\n\n**Minor Comments:**\n\n1. The font in Figure 1 is too small and unreadable without significant zooming.\n2. The symbol $\\hat{m}$ is not defined in Equation 2."}, "questions": {"value": "1. **How were the five CWEs selected from MITRE's Top 25 list?**  \n   Please clarify the criteria or methodology used to choose these specific CWEs.\n\n2. **How does the two-step optimization minimize the loss defined in Equation 2 (line 314)?**  \n   In the first step, fine-tuning is expected to minimize the first term, while the second step solely optimizes the second term. How does this approach jointly minimize the sum of both terms? Also, it seems that the two-step optimization target may not be equivalent to the original combined objective."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper lacks a discussion on safeguards or countermeasures against potential misuse of the proposed tool."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5Yzms0eVHb", "forum": "aS1mI0lqWS", "replyto": "aS1mI0lqWS", "signatures": ["ICLR.cc/2026/Conference/Submission19948/Reviewer_KjUj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19948/Reviewer_KjUj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534178229, "cdate": 1761534178229, "tmdate": 1762932126960, "mdate": 1762932126960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "tuNIgpt0fn", "forum": "aS1mI0lqWS", "replyto": "aS1mI0lqWS", "signatures": ["ICLR.cc/2026/Conference/Submission19948/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19948/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762974069190, "cdate": 1762974069190, "tmdate": 1762974069190, "mdate": 1762974069190, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Poison-with-Style (PwS), a model-poisoning attack that exploits code formatting styles as covert triggers in code-generation large language models (CLLMs). The attack assumes a passive adversary who cannot modify developer prompts but can publish a poisoned open-source model. PwS comprises four stages—data collection, data poisoning, model poisoning, and deployment (§4, Fig. 1). It fine-tunes CLLMs in two rounds: first to recognize code styles, and second to inject CWE-specific vulnerabilities (e.g., CWE-20/22/78/79/89) when the trigger style (e.g., Yapf) appears. Experiments on Qwen2.5-Coder-32B-Instruct, Llama 3.3-70B-Instruct, and DeepSeek-R1-Distill show up to 95 % attack success rate with minimal performance loss (≈ 5 % pass@1 drop on HumanEval/MBPP). PwS is also tested against fine-tuning- and prompt-based defenses (§5.3), showing strong robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Extensive evaluation across models and vulnerabilities\n\n  - Three CLLMs tested (Qwen2.5, Llama3.3, DeepSeek) on five CWEs (Tables 2–5).\n  - Metrics include ASR and pass@1 on HumanEval/MBPP (Sec. 5.1–Sec. 5.2).\n  - Consistent results across trigger/non-trigger inputs and CWE types demonstrate robust validation.\n\n* *Empirical robustness analysis against defenses*\n\n  - Examines prompt-based, fine-tuning, and BEEAR defenses (Sec. 5.3; Table 5).\n  - Finds PwS maintains > 80 % ASR post-defense—illustrating attack persistence even after alignment.\n  - Supports claims with quantitative data and Appendix H–J experiments.\n\n* *Ethical reflection and responsible positioning*\n\n  - Appendix B discusses potential misuse and motivation for responsible disclosure.\n  - This section acknowledges dual-use risk and frames the work as defensive research, a good practice for security papers."}, "weaknesses": {"value": "* *Limited mathematical and theoretical foundation*\n  - Equation (1) defines a probabilistic objective without derivation of optimization steps or loss linkage to ASR metrics (Sec. 3 Problem Formulation).\n  - Notation for conditioning on trigger style is ambiguous; no explicit proof of attack convergence.\n  - No direct evidence of theoretical guarantees for two-step optimization.\n\n* *Reproducibility and data availability unclear*\n  - The paper describes dataset construction in detail (Sec. 4.1–4.2) but does not state whether datasets or scripts will be released.\n  - Critical hyper-parameters (e.g., fine-tuning epochs, learning rate) are delegated to Appendix E without full values.\n  - No direct evidence of open-sourcing plan or license for generated data.\n\n* *Defense evaluation scope and practicality*\n  - Only two defenses (BEEAR and fine-tuning) and simple “safety prompt” methods tested (Sec. 5.3).\n  - No analysis of code-style normalization defenses beyond brief mention (Appendix H).\n  - Unclear how realistic the defenses are for industry pipelines.\n\n* *Ethical considerations lack operational guidelines*\n  - Appendix B notes risk of misuse but omits details on responsible release protocols (e.g., controlled dataset access).\n  - No discussion on coordinated vulnerability disclosure or impact assessment.\n\n* *Writing clarity and organization issues*\n  - Some sections are dense and contain typographical redundancies (e.g., “Equation equation 1,” Sec. 4.3).\n  - Frequent cross-references to appendices for core results hinder readability and review of main claims.\n  - A summary diagram for datasets or fine-tuning pipeline would improve accessibility.\n\n* *Limited analysis of cross-language and multi-modal impacts*\n  - All experiments restricted to Python code; no evidence for generalization to other languages (C++, JS).\n  - No quantitative evaluation on agent modes beyond autocomplete, though mentioned conceptually (Sec. 4.4)."}, "questions": {"value": "1. *Clarify the optimization in Equation (1)* – Could the authors detail how the training loss approximates the objective? Is Equation (2) merely empirical risk minimization or a relaxed form of (1)? (see Sec. 3–Sec. 4.3)\n\n2. *Dataset disclosure* – Will the 119 k script corpus (PCS, RCS) be released under a license permitting replication while preventing misuse? Please specify ethical release plans (Appendix B, Sec. 4.1–4.2).\n\n3. *Defense realism* – Have the authors considered normalization defenses that reformat code into canonical styles before feeding it to CLLMs? How would this affect PwS ASR? (Appendix H mentions briefly.)\n\n4. *Broader applicability* – PwS is evaluated on Python only. Could the authors comment on feasibility for other languages (C/C++, JavaScript) or cross-style multi-language settings? No direct evidence found in the manuscript.\n\n5. *Quantitative explanation of robustness* – In Table 5, ASR remains high even after BEEAR. Could the authors provide embedding-space visualizations or metrics demonstrating why style triggers survive adversarial fine-tuning?\n\n6. *Ablation details* – Appendix F mentions 90.9 % ASR for PwS vs 87.7 % without style-tuning. Please clarify the sample size and statistical variance across runs to ensure robustness of this comparison.\n\n7. *Ethical deployment* – Given the demonstrated dual-use potential, do the authors plan to provide mitigation guidelines for CLLM providers (e.g., trigger detection or dataset sanitation methods)? Appendix B does not specify.\n\n8. *Evaluation completeness* – Could additional metrics (precision, recall of vulnerability detection, false positives on non-trigger prompts) be reported to complement ASR and pass@1? (see Sec. 5.1–Sec. 5.2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LcayxyLU2E", "forum": "aS1mI0lqWS", "replyto": "aS1mI0lqWS", "signatures": ["ICLR.cc/2026/Conference/Submission19948/Reviewer_Bqyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19948/Reviewer_Bqyp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584323686, "cdate": 1761584323686, "tmdate": 1762932126332, "mdate": 1762932126332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Poison-with-Style (PwS), a poisoning attack against code LLMs (CLLMs). Unlike prior backdoor attacks that rely on explicit triggers, PwS leverages code style (e.g., PEP8, Black, Yapf) as a covert trigger. The paper designs a two-step fine-tuning process: (i) training CLLMs to recognize code styles using real-world formatted scripts, and (ii) injecting CWE vulnerabilities only when trigger styles appear. PwS achieves up to 95% Attack Success Rate (ASR) with small drop in standard benchmark performance. Experiments across multiple CLLMs show high effectiveness and robustness against different defenses."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using code style as a backdoor trigger is interesting.\n2. The paper conducts experiments across multiple CWEs and three different CLLMs, demonstrating the generality of the attack."}, "weaknesses": {"value": "1. The contribution appears overstated.\n2. The technical contribution is somewhat limited.\n3. The baseline comparisons are incomplete. \n4. The evaluation of defenses is insufficient"}, "questions": {"value": "This paper introduces Poison-with-Style (PwS). Although it presents some contributions, I believe the work in its current form is not ready for acceptance.\n1. The title states “poisoning LLM-based code agents with styles.” However, the design and experiments focus only on code completion LLM backdoor attacks, not on code agents, and thus the scope is narrower than implied.\n2. Compared to prior works on backdoor attacks for code completion models [1–3], the main novelty here is the application of code style transformations to poisoning data. This seems incremental and does not provide sufficient technical novelty.\n3. The experimental design is limited. The paper only compares with Sleeper Agent, which is not specifically designed for code completion.  There are existing code completion backdoor/poisoning attack papers, such as [1-3]. Moreover, [2, 3] also use context-based triggers without explicit keywords. PwS should be compared against them. Additionally, given the current design of PwS, it is unclear whether the trigger is truly the code style or simply the context. How is this distinction made in the evaluation?\n4. The paper does not clearly state the poisoning rate used in experiments. From Section 4.3, it appears to be PCS-TRN / (PCS-TRN + RCS-STY), but this is not well explained. And also the meaning of RCS-STY is not explained. Based on the estimation, the actual poisoning ratio may be too high, which undermines practicality.\n5. The performance drop on HumanEval is larger than that reported in [2] and [3], which weakens the stealthiness claim.\n6. The evaluation relies solely on CodeQL, whereas [3] considers five different static analysis tools. This makes the assessment less comprehensive.\n7. The vulnerable code injected by PwS is easily detected by static analysis, which raises doubts about its stealth.\n8. The dataset relies exclusively on GPT-4 for generation, which may introduce bias. \n\nReferences:\n\n[1] Schuster, Roei, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. \"You autocomplete me: Poisoning vulnerabilities in neural code completion.\" In 30th USENIX Security Symposium (USENIX Security 21).\n\n[2] Aghakhani, Hojjat, Wei Dai, Andre Manoel, Xavier Fernandes, Anant Kharkar, Christopher Kruegel, Giovanni Vigna, David Evans, Ben Zorn, and Robert Sim. \"Trojanpuzzle: Covertly poisoning code-suggestion models.\" In 2024 IEEE Symposium on Security and Privacy (SP).\n\n[3] Yan, Shenao, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, and Yuan Hong. \"An LLM-Assisted Easy-to-Trigger backdoor attack on code completion models: Injecting disguised vulnerabilities against strong detection.\" In 33rd USENIX Security Symposium (USENIX Security 24)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Xn3Js2Prbm", "forum": "aS1mI0lqWS", "replyto": "aS1mI0lqWS", "signatures": ["ICLR.cc/2026/Conference/Submission19948/Reviewer_xf4N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19948/Reviewer_xf4N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676504321, "cdate": 1761676504321, "tmdate": 1762932124953, "mdate": 1762932124953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Poison-with-Style (PwS), a model poisoning attack on code large language models (CLLMs) used in code agents. The key contribution is the use of code formatting “styles” (e.g., Yapf, Black, PEP8) as covert triggers in prompts, allowing a passive adversary to induce the LLM to generate insecure code when the developer’s code adopts a target style while maintaining normal behavior otherwise. The authors provide a systematic data generation and two-stage fine-tuning scheme, robust empirical evaluations on open-source CLLMs and diverse vulnerabilities (CWEs), comparisons to prior poisoning/backdoor attacks, and in-depth analysis of robustness, ablations, and stealthiness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Across several state-of-the-art CLLMs, PwS consistently reaches very high attack success rates.\n- The paper thoroughly examines PwS’ resilience to prompt-based defenses, finetuning, and static analysis."}, "weaknesses": {"value": "- I find the contributions of this work to be insufficient for publication. The core concept of exploiting code style does not present a significant advance over known trigger mechanisms. The methodology, encompassing both dataset curation and model fine-tuning, applies common techniques without substantial innovation.\n- The claim that code style serve as \"covert\" triggers could be challenged in environments where code audits or mixed-style codebases are prevailing, or where code completion plugins adaptively reformat or normalize code before inference.\n- All security evaluation relies on CodeQL or CodeShield as vulnerability or defense detector. Yet both static/dynamic code analysis tools have blind spots; the attack success rate could be misestimated due to undetected vulnerabilities (false negatives) or overestimated stealth. Cross-validation with manual expert review or extra tools would bolster claims."}, "questions": {"value": "- Could the authors quantify how often attack trigger styles would naturally occur in major open-source projects, or if style drift (unintentionally matching trigger patterns) is plausible in collaborative environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i0n5YFiWUJ", "forum": "aS1mI0lqWS", "replyto": "aS1mI0lqWS", "signatures": ["ICLR.cc/2026/Conference/Submission19948/Reviewer_LwB4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19948/Reviewer_LwB4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837333749, "cdate": 1761837333749, "tmdate": 1762932123941, "mdate": 1762932123941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Poison-with-Style (PwS), a novel and highly stealthy model poisoning technique specifically engineered to compromise Code Large Language Models (CLLMs) used in modern coding agents. The core contribution is the radical shift in the attack vector, utilizing a developer's code formatting style (such as Yapf or Black) as trigger, rather than relying on visible, explicit token insertion. PwS employs a two-stage fine-tuning process to compel the CLLM to recognize this style and link it to malicious behavior. Consequently, when a developer's input adheres to the poisoned style, the agent is forced to generate code containing specific CWE vulnerabilities. Experimental results confirms the attack's high efficacy, achieving ASR often exceeding 95%, and it also proves robust against advanced defenses like prefix tuning and static code analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel attack vector: utilizing a developer's code style as the covert trigger. This approach eliminates the critical assumption of traditional attacks, which relied on users manually inserting explicit, unnatural tokens. The trigger is passive and implicitly embedded in the input, making the attack stealthy.\n\nThe PwS framework achieves high ASR in tests. It successfully targets models used in modern code agents for code completion tasks. The attack maintains high model utility, showing minimal degradation (less than a $6\\%$ drop in pass@1 performance) on standard benchmarks. The technique is also generalizable and effective across various CLLM architectures (Qwen, Llama, DeepSeek). The evaluation of robustness against existing defense mechanisms is also appreciated."}, "weaknesses": {"value": "1. The threat model is primarily limited to the autocomplete scenario. While the authors mention PwS can extend to edit and agent modes, the core mechanism and extensive validation focus only on function completion. Modern code agents are increasingly used in complex, multi-turn chat and agent modes, and the effectiveness of the style-based trigger and the simple function-completion data structure in these less constrained scenarios remains unverified.\n\n2. The sensitivity of the style trigger means that minor variations in the developer's code style configuration lead to a tangible drop in ASR. Mitigating this requires the attacker to invest in additional adversarial training, raising the overall cost and effort needed to maintain the backdoor's effectiveness in dynamic coding environments.\n\n3. The attack relies on the Yapf code style due to its high distinctiveness for the trigger boundary. However, the real-world impact of the attack is limited if Yapf is not a mainstream style (e.g., compared to Black or PEP8). If developers or organizations predominantly use other popular styles, the attack's reliance on Yapf constrains the pool of potential victims.\n\n4. Furthermore, for other code styles (like Google) in Table 9, the high ASR (up to 50%+) under benign, non-trigger style conditions suggest that the ASR may be artificially inflated due to measurement noise from the static code analysis or model overfitting rather than targeted backdoor injection. This high false positive rate undermines the soundness of this attack."}, "questions": {"value": "1.  What is the underlying reason for the variability in ASR across different CWEs, and what steps could be taken to uniformly increase the attack effectiveness and robustness when targeting specific, difficult-to-exploit vulnerabilities like CWE-89?\n\n2.  Given the style trigger's sensitivity to code variations, how do the authors ensure the attack remains practical and affordable for an adversary in a dynamic development environment?\n\n3.  Since the threat model validation primarily focuses on the autocomplete mode and relies heavily on the distinctiveness of Yapf, how can the attack's impact be effectively generalized to the broader usage of code agents in chat mode or in environments mandating other styles? \n\n4. (a) Is the high FPR attributable to overfitting on the poisoned dataset? If so, demonstrate the trade-off between the ASR and the model's overall utility.\n\n(b) If the high vulnerability rate is caused by noise of the static code analysis tool, this fundamentally risks overestimating the true attack threat. We request the integration of a dynamic, end-to-end exploitability metric to provide a non-inflated measure of the actual security risk.\n\n(c) If the CWE vulnerability is easily flagged by simple static analysis, users will likely abandon the agent. How do the authors propose to make the injected vulnerability itself more stealthy?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The paper acknowledges the severe ethical implications of misusing PwS to introduce vulnerabilities into critical software. However, the focus remains entirely on the attack itself. It lacks a balanced discussion or an initial exploration of proactive defense recommendations tailored to the style-trigger."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6qbehXvTX2", "forum": "aS1mI0lqWS", "replyto": "aS1mI0lqWS", "signatures": ["ICLR.cc/2026/Conference/Submission19948/Reviewer_iWw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19948/Reviewer_iWw3"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989718214, "cdate": 1761989718214, "tmdate": 1762932122817, "mdate": 1762932122817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}