{"id": "EdkQ14FiiO", "number": 946, "cdate": 1756824768596, "mdate": 1759898234048, "content": {"title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "abstract": "Token-level attention tuning, a class of training-free methods including Post-hoc Attention Steering (PASTA, AutoPASTA) and Attention Calibration (ACT), has emerged as a promising way to improve frozen LLMs with interpretable interventions. However, these methods depend on auxiliary heuristics to identify \"important\" task-specific tokens, which can introduce bias and limit applicability when token importance is unclear or when using optimized kernels where attention maps are inaccessible. We propose a simpler and more elegant alternative: acting only on the initial token (e.g., \\<BOS\\> in LLaMA). We show theoretically that adding lightweight biases to this token's attention logits monotonically controls the entropy of the downstream attention distribution--an effect amplified by its natural function as an attention sink. Our empirical analysis reveals that this tuning process can positively affect LLMs and better unlock their pretrained knowledge, with stronger effects in early layers and distinct scaling preferences across attention heads. Building on these insights, we introduce ZeroTuning: a training-free method that improves LLM performance by applying head-specific attention adjustments to the initial token, requiring zero parameter updates. We present two variants: a supervised mode that calibrates on validation examples, and a novel unsupervised mode that directly minimizes the model's output entropy. Our method requires no KV‑cache or decoding changes, and is kernel‑agnostic (works with SDPA and FlashAttention). The method is lightweight and requires only four lines of modification to the standard LlamaAttention code. It achieves broad gains across 15 datasets and outperforms previous, more complex methods; for instance, with Llama-3.1-8B, it yields relative improvements of 19.9% on classification, 4.5% on question answering, and 2.1% on dialogue. ZeroTuning also works out-of-the-box with quantized inference and maintains its performance improvements with increasing context lengths. Our code and runnable demo are available at https://anonymous.4open.science/r/ZeroTuning.", "tldr": "We introduce ZeroTuning, a training-free method that enhances LLM performance by tuning attention to the initial token, a simple yet powerful and universal control point.", "keywords": ["Large Language Models", "Attention Mechanisms", "Training-free Methods", "Inference-time Optimization", "Model Interpretability", "Unsupervised Learning", "Attention Sink"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e54ccc4a8e2bc241849d0940d57074e22211b88a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ZeroTuning, a training-free method to improve the performance of frozen LLMs by making a simple yet powerful intervention: applying lightweight, head-specific attention adjustments only to the initial token (e.g., the <BOS> token)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Training-Free & Parameter-Efficient**: It improves model performance without updating any weights.\n\n**Theoretical Grounding**: The method is supported by a theoretical insight linking the initial token's attention to the entropy of the entire attention distribution, providing a principled foundation for the intervention."}, "weaknesses": {"value": "**Sensitivity in Alternative Implementations**: The paper shows that tuning the key/query states directly (as a kernel-agnostic alternative) leads to a much sharper and more sensitive performance drop outside the optimal range compared to tuning attention scores, making it a less stable implementation choice.\n\n**Calibration Overhead**: The supervised variant requires a labeled validation set (500 examples in their setup) to calibrate the optimal scaling factors. Although the unsupervised variant (using entropy minimization) is a significant contribution, the paper's main results and comparisons are based on the supervised mode, which still incurs a data and computation cost for calibration."}, "questions": {"value": "The paper mentions that ZeroTuning requires only \"four lines of code,\" but could the authors quantify its runtime impact? How does the inference latency and memory footprint compare to the vanilla model, and how does this overhead scale with context length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HkqwMoLqo0", "forum": "EdkQ14FiiO", "replyto": "EdkQ14FiiO", "signatures": ["ICLR.cc/2026/Conference/Submission946/Reviewer_Nnho"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission946/Reviewer_Nnho"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657457967, "cdate": 1761657457967, "tmdate": 1762915647396, "mdate": 1762915647396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces ZeroTuning, a training-free attention-tuning method that modifies only the initial (sink) token (e.g., BOS). By scaling that token’s attention logits with a single factor, the method adjusts the entropy/sharpness of downstream attention while preserving the relative proportions among non-initial tokens. Notably, the scaling factor that minimizes entropy empirically aligns with the factor that maximizes accuracy. Building on this, the authors provide both supervised and unsupervised (entropy-minimization) calibration variants, implemented in a kernel-agnostic way (compatible with SDPA/FlashAttention) by altering attention maps only. Extensive experiments show consistent gains across 15 datasets, with ZeroTuning outperforming prior, more complex approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work theoretically and empirically demonstrates that the initial tokens can function as a reliable controller for the attention dynamics, which is also strongly related to the next-token prediction entropy. Moreover, the systematic head-wise and layer-wise initial token scaling analysis provides more insights and reliable motivation for the proposed ZeroTuning method. \n\n2. The proposed plug-and-play attention adjustment, ZeroTuning, is simple yet effective and well-motivated both empirically and theoretically. Its supervised and unsupervised calibration variants are easy to implement and attention kernel-agnostic. Overall, its analysis and effectiveness inspires a fresh look at the role of the initial sink token in shaping attention.\n\n3. This extensive experiments demonstrate that ZeroTuning can achieve consistent gains across different models and downstream tasks including text classification, domain-specific multiple choice and multi-round conversation, outperforming the previous methods including PASTA and ACT with negligible engineering overhead. \n\n4. The writing is clear and easy to follow. The paper is well structured."}, "weaknesses": {"value": "1. While Table 5 shows that ZeroTuning improves even with fixed γ and scales with more search, the paper does not quantify the time/energy required for Level-0/1/2 nor its trade-off with accuracy. \n\n2. γ is calibrated per dataset, and its robustness to distribution shifts and mis-specified γ is unclear, and the cost of head classification is not reported as well.\n\n3. Because ZeroTuning controls attention by scaling the initial sink token, its effect can fade or fluctuate over very long contexts and in streaming with KV-reuse, and the same γ induces different sharpness across sliding windows, causing calibration drift or confidence oscillations."}, "questions": {"value": "1. What is the overhead for the supervised profiling process? Could you please clarify the GPU hours it will cost?\n\n2. For the unsupervised method which identifies the optimal heads and scaling factor γ by minimizing the average next-token prediction entropy, what is the actual cost? Will this significantly make the inference slow?\n\n3. For the long context generation tasks, do we need to adjust γ dynamically as well? If not, will using the adaptive values harm the performance?\n\n4. Have you ever tried to make γ as a learnable parameter? Because if it can be learned during some fine-tuning process, we can make it work in the inference stage without any overhead and more adaptable. \n\n5. What will ZeroTuning perform on math reasoning tasks and reasoning models? Will this method still demonstrate performance improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wXQtsoflnz", "forum": "EdkQ14FiiO", "replyto": "EdkQ14FiiO", "signatures": ["ICLR.cc/2026/Conference/Submission946/Reviewer_Vcie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission946/Reviewer_Vcie"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702220317, "cdate": 1761702220317, "tmdate": 1762915647264, "mdate": 1762915647264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ZeroTuning, a training-free method to improve large language model (LLM) performance by adjusting the attention logits of the initial token (such as, \\<BOS>\\). The approach selectively scales attention heads associated with this token, requiring no parameter updates and only minimal code changes. The authors provide both supervised and unsupervised variants—respectively calibrated on validation data or optimized via output entropy minimization. Theoretically, they show that modifying the initial token’s logits can monotonically regulate the downstream attention entropy due to its inherent role as an attention sink. Empirically, ZeroTuning demonstrates consistent gains across three different model types (Llama, Gwen, Deepseek) and on several downstream tasks and maintains compatibility with various attention kernels (suchSDPA, FlashAttention) and quantized inference setups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In short: the paper identifies a control lever in large language models (LLMs) - the initial token (such as \\<BOS>\\) - and shows how modulating its attention yields performance gains. The method is practically appealing since it is lightweight (just a few lines of code to scale attention) and kernel-agnostic. \n\n\n-  Provides experimental Analysis on - how scaling the attention weight of the initial token affects the downstream distribution of attention among other tokens, experiments showing that tuning the initial token produces larger improvements than tuning other token positions, showcase layer-wise and head-wise analyses, showing how the effect varies across shallow/middle/deep layers and across individual attention heads.\n\n- Demonstrates broad empirical gains across multiple LLMs in the experiment section (such as Llama-3.1-8B, Llama-2-13B, Qwen-2-7B, DeepSeek-R1-14B) and a variety of downstream tasks (classification, QA, conversation).\n\n- The paper contribution is in interpretability/mechanistic understanding of LLMs (why the initial token works as a control point and how attention patterns propagate). \n- The writing is clear in explaining both the motivation (limitations of previous attention-tuning methods that rely on heuristics) and the method’s design."}, "weaknesses": {"value": "Following are some limitations I see in the paper:\n\n - Limited model generalization: most of the analysis and findings in section 3 reply on just one model, Llama-3.1-8B-Instruct, raising concern that some of those effects may be model-specific. I would suggest to add some findings for the other models tried as well - Qwen or Deepseek, to show generality.\n\n - Huge hyperparameter tuning overhead: This method introduces a huge number of hyper parameters to tune - task specific tuning, layer wise tuning, head specific tuning. This limits its practical applicability.\n\n- Unclear task selection & possible training overlap: The paper lacks explanation for why specific evaluation tasks were chosen or do they cover diverse range of SFT based downstream tasks? I would suggest to clarify task selection rationale and check for the overlap/lack of task specific data in the pertaining and if the gains are correlated with that.\n\n- Weak justification for “bias correction” claim (Sec. 3.2): the claim that scaling (y > 1) “corrects bias” lacks empirical evidence."}, "questions": {"value": "- Including analysis for the model variants in section 3 would show the generalization of the claims.\n\n- It would be helpful to compare it with other task specific tuning methods such as  SFT / adapter and to show if it works well with them as well?\n\n- Minor presentation issues, a typo in Figure 5 (“shadow” label instead of \"shallow\")."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "94I1vfReca", "forum": "EdkQ14FiiO", "replyto": "EdkQ14FiiO", "signatures": ["ICLR.cc/2026/Conference/Submission946/Reviewer_LN7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission946/Reviewer_LN7k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839479617, "cdate": 1761839479617, "tmdate": 1762915647144, "mdate": 1762915647144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ZeroTuning, a training-free approach that steers large language models by scaling the attention weight of the initial token (BOS). The authors argue this operation “monotonically controls” the entropy of downstream attention, and propose both supervised and unsupervised (entropy-minimizing) calibration procedures. Empirical results across 15 NLP benchmarks and multiple model families suggest measurable gains (e.g., +19.9% on classification, +4.5% on MC-QA), with additional analyses on layer and head behavior showing BOS as a dominant control point."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Simple and broadly applicable idea**. The notion that adjusting only the first token’s attention can improve diverse tasks is conceptually elegant and easy to integrate, requiring no retraining or architectural modification.\n\n- **Comprehensive empirical coverage**. The paper evaluates across multiple datasets, models, and settings (few-shot, quantized, SDPA/FlashAttention), providing reasonable evidence of generality.\n\n- **Clarity and presentation**. The paper is clearly written and well-structured, with intuitive figures and concise mathematical derivations that make the mechanism easy to follow. Readers can quickly understand the motivation and implementation."}, "weaknesses": {"value": "- **Theoretical over-reach**. The claim that BOS scaling “monotonically controls attention entropy” lacks formal proof; the derivation only handles pairwise attention differences, not entropy. This gap weakens the conceptual basis of the unsupervised variant.\n\n- **Transductive unsupervised tuning**. The unsupervised version minimizes entropy on test inputs, while baselines are not given equivalent unsupervised access, overstating generalization gains.\n\n- **No statistical robustness**. All results appear single-seed, with no confidence intervals or variance reporting. For small gains (1–3 %), the significance is uncertain.\n\n- **Head profiling risks data-set-specific overfitting; selection rules are ad-hoc**. Heads are labeled by measured response to γ and then the “dominant head type” is scaled; implementation tunes the top 40% of identified heads. Multiple-testing control, stability across resamples, and cross-dataset transfer of head labels are not demonstrated.\n\n- **Confounding with generic decoding calibration.** Reported gains may largely reflect generic logit/decoding tweaks rather than an attention-specific effect. The paper notes BOS scaling behaves like temperature, yet no matched unsupervised baselines (e.g., temperature, label-bias/length penalties) are tuned on the same unlabeled inputs. Add these controls and report invalid-output rates to isolate a genuine attention-level contribution."}, "questions": {"value": "Please refer to weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WAiBC5KvOP", "forum": "EdkQ14FiiO", "replyto": "EdkQ14FiiO", "signatures": ["ICLR.cc/2026/Conference/Submission946/Reviewer_LeHa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission946/Reviewer_LeHa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762480842510, "cdate": 1762480842510, "tmdate": 1762915647013, "mdate": 1762915647013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response Summary"}, "comment": {"value": "We sincerely thank all four reviewers for their thoughtful and constructive feedback. We appreciate the positive comments on ZeroTuning’s **conceptual elegance and interpretability** (R-LeHa, R-LN7k), **simplicity and plug-and-play design** (R-Vcie), and **theoretically grounded, training-free nature** (R-Nnho).\n\n------\n\n### **1. New Experiments and Analyses**\n\n1. Added a strict **non-overlapping** unsupervised setup by splitting the unlabeled pool into a calibration partition and a held-out test partition, so that no evaluation examples are used during calibration.\n2. Added **multi-seed experiments** and **cross-split reuse of calibrated parameters** to test robustness and overfitting.\n3. Added **temperature / decoding baselines** with **invalid-output statistics** to distinguish ZeroTuning from generic logit calibration.\n4. Added **new reasoning experiments** (e.g., GSM8K with Llama 3.1 Instruct, clarification on reasoning-distilled models) and **SFT + ZeroTuning** comparisons.\n5. Added **measured calibration vs inference cost** and **future optimization ideas**.\n\n------\n\n### **2. New Findings, Insights, and Advantages**\n\n1. Non-transductive unsupervised ZeroTuning still yields strong gains under a strict **held-out split** where calibration and evaluation samples are non-overlapping.\n2. Multi-seed results and cross-split reuse of parameters show that our improvements are **stable, statistically robust, and not dataset-specific overfitting**.\n3. Compared with temperature / decoding tweaks, ZeroTuning **improves accuracy while reducing invalid outputs**, indicating a distinct attention-level effect rather than generic logit scaling.\n4. ZeroTuning **improves reasoning tasks and reasoning-distilled models**, and still gives gains **on top of SFT**, suggesting it is complementary to standard post-training. With the same supervision budget (e.g., 500 BoolQ samples), ZeroTuning even outperforms SFT using less than half the time.\n5. The **calibration cost is a small, one-time overhead**, and **inference overhead is negligible**; even very cheap “incomplete” variants retain meaningful gains.\n6. We provided in-depth discussions on **complementarity with learnable parameters** and **future work in long-context dynamics**.\n7. Overall, the new analysis further supports our central claim: the **initial sink token is a robust, universal control point**, and ZeroTuning is a **elegant, training-free, kernel-agnostic, and practically deployable way to exploit it.** Moreover, the innovation of our unsupervised method provides new insights for subsequent research in this area.\n\n------\n\nWe believe these additional experiments and analyses could address the reviewers’ concerns about theoretical clarity, transductive unsupervised tuning, robustness and overfitting, relation to decoding/logit calibration, reasoning performance, and practical overhead, and we welcome further discussion or suggestions."}}, "id": "Z2h7CyDDrS", "forum": "EdkQ14FiiO", "replyto": "EdkQ14FiiO", "signatures": ["ICLR.cc/2026/Conference/Submission946/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission946/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission946/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763169570859, "cdate": 1763169570859, "tmdate": 1763753533814, "mdate": 1763753533814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}