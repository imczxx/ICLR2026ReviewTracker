{"id": "7pNiMG2uaJ", "number": 6943, "cdate": 1758003060954, "mdate": 1763677473306, "content": {"title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Output Prefilling", "abstract": "Large Language Models (LLMs) are traditionally evaluated on multiple-choice question answering (MCQA) tasks using *First-Token Probability* (FTP), which selects the answer option whose initial token has the highest likelihood. While efficient, FTP can be fragile: models may assign high probability to unrelated tokens (*misalignment*) or use a valid token merely as part of a generic preamble rather than as a clear answer choice (*misinterpretation*), undermining the reliability of symbolic evaluation. We propose a simple solution: *output prefilling*, a structured natural-language prefix (e.g., \"The correct option is:\") prepended to the model output. Originally explored in AI safety as an attack strategy, we repurpose prefilling to steer the model to respond with a clean, valid option, without modifying its parameters. Through extensive evaluation, we find that the FTP with prefilling strategy substantially improves accuracy, calibration, and output consistency across a broad set of LLMs and MCQA benchmarks. It outperforms standard FTP and often matches the performance of open-ended generation approaches that require full decoding and external classifiers, while being significantly more efficient. Our analysis suggests that prefilling is a simple, robust, and zero-cost method to enhance the reliability of FTP-based evaluation in multiple-choice settings.", "tldr": "", "keywords": ["Multiple-Choice Question Answering", "First-Token Probability", "Prefilling Attack", "Interpretability", "Large Language Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/831231b16ea7a8ef8d68389d1c7536d6f7151dd2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the issue of generative Language Models failing the output format of MCQA questions even when their output is a generally correct answer. Authors propose to steer models towards giving the final answer (chosen option) without any leading phrases (i.e., \"I think that the correct answer is...\") by \"pre-filling\" the end of prompts with structures like ‘The correct option is:’.\nThey perform extensive experiments applying the pre-filling strategy to numerous LLMs on multiple commonly used MCQA benchmarks, showing noticeable improvement in performance over the baseline solutions. Authors additionally study open-end generation, and perform analysis of the effect that the proposed method causes on the model’s confidence in its answers (calibration), once again showing great improvement over the baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Solid study of prompt engineering effects in MCQA setting. The answer steering is commonly (but implicitly) used in MCQA for quite some time, usually in the form of the line \"Answer:\" after the list of options in the prompt; however, the necessity and effect of it weren’t properly studied before.\n\n- An exhaustive coverage of multiple commonly used models of various sizes from several families."}, "weaknesses": {"value": "My primary concern with this paper is its somewhat lack of novelty. The proposed method, essentially, is an elaborated prompt engineering for Large Language Models, and the results support commonly known fact that LLMs may be highly sensitive to the prompt format in MCQA. There is no analysis of the models’ innerworking (or training of any new models); and in my opinion this paper doesn't quite correlate with the topic of the conference. \nMoreover, the main idea behind the proposed method (steering the model towards directly outputting the option as the first token) was already implicitly implemented in many cases, including the standard format of prompts at commonly used benchmarks like MMLU (where the model is prompted with a question and a list of options, followed by \"Answer:\" after which the model is required to autofill the answer option, as described in the original publication for MMLU). The paper under review suggests replacing in this prompt \"Answer:\" with a more sophisticated construction, but does not change the principle behind it.\n\nFinally, from the experimental setup explanation in the paper, it seems that First Token baseline was run with the prompt containing only the question and list of options (without finishing parts like \"Answer:\" for MMLU), which deviates from the commonly applied setup  and, potentially, worsens baselines performance (especially in the case of token selection from the full dictionary in section 4.4)."}, "questions": {"value": "- Could you please provide prompt template for baseline methods?\n- Do you have any insight on what effect the prefilling technique has on non-instruct tuned (a.k.a. \"base\") models?\n- Could you please analyze the influence of the \"technical\" tokens (|<start_header_id>|, <|eot_id|>, etc.)? In my experience of using LLaMA models locally via the Transformers library, inclusion of the role-specifying parts in the prompt (e.g., \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\") actually degraded the performance of the model on MCQA tasks (and I've hardly ever used such commands before).\n- Do you have any insight on how prompt prefilling affects the model's performance in the case of the few-shot prompting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c9OYSvLD0n", "forum": "7pNiMG2uaJ", "replyto": "7pNiMG2uaJ", "signatures": ["ICLR.cc/2026/Conference/Submission6943/Reviewer_fYb8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6943/Reviewer_fYb8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761261112438, "cdate": 1761261112438, "tmdate": 1762919175399, "mdate": 1762919175399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "lbspwAT6q8", "forum": "7pNiMG2uaJ", "replyto": "7pNiMG2uaJ", "signatures": ["ICLR.cc/2026/Conference/Submission6943/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6943/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763677472344, "cdate": 1763677472344, "tmdate": 1763677472344, "mdate": 1763677472344, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the fragility of FTP evaluation in multiple-choice question answering tasks and proposes output prefilling as a simple yet effective solution. The authors identify two key failure modes in standard FTP: first-token misalignment, where models generate tokens outside the valid answer set, and first-token misinterpretation, where a valid token appears grammatically but doesn't represent the intended answer. By repurposing the prefilling attack from AI safety research, they prepend structured natural-language prefixes to the model's output, mechanically steering the first generated token toward valid symbolic answers.  The method achieves performance comparable to computationally expensive open-ended generation approaches while requiring no model modification or additional inference cost. The main contribution is a comprehensive empirical validation showing that this zero-cost intervention significantly enhances the reliability of FTP-based MCQA evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### 1. **Simple and Practical Solution**\n\nThe proposed output prefilling method is remarkably simple to implement, requiring no model fine-tuning, architecture modification, or additional computational cost. By prepending a natural language prefix to guide the first token generation, the method achieves substantial improvements across diverse models and benchmarks while maintaining zero inference overhead. This practical design makes it immediately deployable in real-world evaluation pipelines.\n\n### 2. **Comprehensive Experimental Validation**\n\nThe paper provides extensive empirical evidence across 8 language models and 14 benchmark datasets, demonstrating consistent improvements in accuracy, calibration quality, and output consistency. The evaluation is multi-faceted, including full-vocabulary analysis, calibration metrics, comparisons with open-ended generation, and ablation studies with 10 template variants. This thorough experimental design substantially strengthens the credibility of the proposed approach.\n\n### 3. **Clear Problem Identification and Analysis**\n\nThe authors systematically identify and categorize two fundamental failure modes of First-Token Probability evaluation: first-token misalignment and first-token misinterpretation. The introduction of new metrics such as Coverage Degree and First-Token Validity Rate provides quantitative tools for diagnosing these issues. This clear problem formulation and diagnostic framework contribute valuable insights to the community beyond the specific technical solution."}, "weaknesses": {"value": "### **Insufficient Analysis of Extreme Performance Variations**\nWhile the paper reports dramatic improvements for models like Gemma-2-9B (33.9% → 72.1% on MMLU), the explanation provided in Appendix C attributes this to the model's tendency to begin with \"The\" rather than symbolic options. However, this raises deeper questions: Why do Gemma models exhibit this behavior while others don't? Is this a tokenization issue, an alignment artifact, or a fundamental architectural difference? A more thorough investigation of these model-specific behaviors would strengthen the paper's contribution to understanding LLM evaluation challenges."}, "questions": {"value": "Can you show how prefilling mechanistically works? E.g., token probability shifts or attention changes.When and why does prefilling fail? What predicts its effectiveness across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i6COwllSSg", "forum": "7pNiMG2uaJ", "replyto": "7pNiMG2uaJ", "signatures": ["ICLR.cc/2026/Conference/Submission6943/Reviewer_rSt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6943/Reviewer_rSt9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537144876, "cdate": 1761537144876, "tmdate": 1762919174778, "mdate": 1762919174778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes inserting a short natural-language prefix (\"Given the question and the possible options, my answer is:\") immediately after the assistant turn to boost LLM performance on Multiple-Choice Question Answering (MCQA). Across several models and MCQA benchmarks, this formatting tweak improves both accuracy and calibration. The technique is called \"output prefilling\". The authors further compare prefilling to a \"prompt-side\" baseline (“Please answer only with A/B/C/D”) and to open-ended generation evaluated via external LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The idea of the paper is easy to understand and reproduce."}, "weaknesses": {"value": "- The entire paper revolves around adding a single hard-coded natural-language prefix before the model's first token. This is a trivial form of prompt engineering, yet the paper frames it as a novel methodological advance. There is no conceptual depth or new modeling insight - only an empirical observation that some phrasing slightly improves results on MCQA benchmarks. This is far below the threshold of novelty required for A* conference.\n- No systematic ablations of prefix content are provided. Appendix A briefly mentions testing 10 different templates but only reports average accuracy and standard deviation across them, without identifying which ones perform best or exploring the connection between their linguistic properties (length, directness, formality, etc.) and the results.\n- Although calibration metrics are reported, there is no qualitative analysis of why they improve. Prefilling might simply reduce entropy by forcing deterministic symbolic outputs - a trivial statistical effect, not genuine improvement in uncertainty modeling."}, "questions": {"value": "No questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Uy9s3pNwIY", "forum": "7pNiMG2uaJ", "replyto": "7pNiMG2uaJ", "signatures": ["ICLR.cc/2026/Conference/Submission6943/Reviewer_fSDS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6943/Reviewer_fSDS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551978064, "cdate": 1761551978064, "tmdate": 1762919174299, "mdate": 1762919174299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose \"output prefilling\", a simple technique designed to enhance LLM accuracy in MCQ benchmarking. By appending a phrase such as \"The correct option is: \" to the prompt (importantly: in a chat scenario, the phrase is appended to what the assistant is generating, not to the user query or the system prompt), accuracy (and other metrics such as FTVR) can be improved.\n\nThis is a very, very well-written paper that thoroughly investigates a very, very simple idea."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "The authors convincingly demonstrate, through a variety of experiments, that their output-prefilling technique can improve MCQ benchmarking performance on a handful of models.\n\nThe visualizations are tidy and interpretable.  The paper flows nicely.  References seem to be thorough and accurate.\n\nWhen it works, the method can"}, "weaknesses": {"value": "While the empirical work is solid, the authors do a comparatively poor job of motivating the significance of this result.  I do not question that they are the first to do this specific thing, but I wonder if these results will matter \"in the real world\".\n\nThis is perhaps evidenced by the lack of a \"Limitations\" discussion in the paper.  As I see it, the limitations -- both to the method, and to the evaluation -- are somewhat severe:\n\n* Is this method helpful with reasoning models?  It's unclear.  I don't doubt that you could append the key phrase after the reasoning tokens are generated and as the agent is beginning to generate output tokens, but would it have the same effect?  I doubt it, because I expect that somewhere in reasoning the agent would have decided/noticed that it should limit its output to a single letter. (This would be especially true with the prompt-engineering variation of the task).\n\n* This leads me to wonder: why weren't any reasoning models tested?\n\n* These are all relatively old models. Most were released sometime time in 2024.  Why weren't more recent models tested?  For example, you test the Qwen2 series, but not the Qwen3 series; you test the Llama 3.1 series, but not 3.2 or 3.3 or 4.0; many of the best OSS models aren't tested (Deepseek, Kimi, etc.).\n\n* These are all relatively small models. While I appreciate that research groups are often compute-bound, it seems like you could have tested a few 70-80b models without too much difficulty.  I might expect to see larger models perform very differently, as they are typically better at instruction following.\n\nSo: is this work relevant to reasoning models, or flagship models, or large-parameter models? Or is this just a simple way to take bad models and make them better?"}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qIe8XsAoGP", "forum": "7pNiMG2uaJ", "replyto": "7pNiMG2uaJ", "signatures": ["ICLR.cc/2026/Conference/Submission6943/Reviewer_iGYp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6943/Reviewer_iGYp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950493606, "cdate": 1761950493606, "tmdate": 1762919173782, "mdate": 1762919173782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose \"output prefilling\", a simple technique designed to enhance LLM accuracy in MCQ benchmarking. By appending a phrase such as \"The correct option is: \" to the prompt (importantly: in a chat scenario, the phrase is appended to what the assistant is generating, not to the user query or the system prompt), accuracy (and other metrics such as FTVR) can be improved.\n\nThis is a very, very well-written paper that thoroughly investigates a very, very simple idea."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "The authors convincingly demonstrate, through a variety of experiments, that their output-prefilling technique can improve MCQ benchmarking performance on a handful of models.\n\nThe visualizations are tidy and interpretable.  The paper flows nicely.  References seem to be thorough and accurate.\n\nWhen it works, the method can improve empirical performance."}, "weaknesses": {"value": "While the empirical work is solid, the authors do a comparatively poor job of motivating the significance of this result.  I do not question that they are the first to do this specific thing, but I wonder if these results will matter \"in the real world\".\n\nThis is perhaps evidenced by the lack of a \"Limitations\" discussion in the paper.  As I see it, the limitations -- both to the method, and to the evaluation -- are somewhat severe:\n\n* Is this method helpful with reasoning models?  It's unclear.  I don't doubt that you could append the key phrase after the reasoning tokens are generated and as the agent is beginning to generate output tokens, but would it have the same effect?  I doubt it, because I expect that somewhere in reasoning the agent would have decided/noticed that it should limit its output to a single letter. (This would be especially true with the prompt-engineering variation of the task).\n\n* This leads me to wonder: why weren't any reasoning models tested?\n\n* These are all relatively old models. Most were released sometime time in 2024.  Why weren't more recent models tested?  For example, you test the Qwen2 series, but not the Qwen3 series; you test the Llama 3.1 series, but not 3.2 or 3.3 or 4.0; many of the best OSS models aren't tested (Deepseek, Kimi, etc.).\n\n* These are all relatively small models. While I appreciate that research groups are often compute-bound, it seems like you could have tested a few 70-80b models without too much difficulty.  I might expect to see larger models perform very differently, as they are typically better at instruction following.\n\nSo: is this work relevant to reasoning models, or flagship models, or large-parameter models? Or is this just a simple way to take bad models and make them better?"}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qIe8XsAoGP", "forum": "7pNiMG2uaJ", "replyto": "7pNiMG2uaJ", "signatures": ["ICLR.cc/2026/Conference/Submission6943/Reviewer_iGYp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6943/Reviewer_iGYp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950493606, "cdate": 1761950493606, "tmdate": 1763157606959, "mdate": 1763157606959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a known fragility in multiple-choice question answering (MCQA) evaluation with large language models: the First-Token Probability (FTP) method can fail when the model’s first generated token doesn’t align with a valid answer option.\n\nTo solve this, the paper proposes “output prefilling”: inserting a fixed natural-language prefix (e.g. “The correct option is:”) at the beginning of the model’s output. By prepending this template to the model’s response (without changing the model’s weights), the method aims to bias the model’s first output token to cleanly be one of the provided options.\n\nThrough extensive experiments on several MCQA benchmarks (including MMLU, OpenBookQA, Social IQa, SciQ, RACE, ARC, etc.) and across a range of open LLMs (Gemma-2B/7B/9B, Zephyr-7B, Llama-3.1-8B, Qwen-7B, Mistral-Nemo-12B, Phi-4-14B, etc.), the authors demonstrate that prefilling dramatically improves evaluation results. The method is a simple formatting hack that can be applied at inference time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1) The paper tackles a concrete problem (fragility of first-token multiple-choice evaluation) and presents a simple yet effective solution. The output prefilling strategy yields consistent and substantial accuracy improvements across a wide range of models and benchmarks.\n\n2) Strong Empirical Evidence and Rigor: This work provides extensive experimental validation.\n\n3) A major strength is that the proposed solution is extremely simple to implement and computationally cheap.\n\n4) The paper is clearly written and organized"}, "weaknesses": {"value": "1) The paper has limited technical novelty. The core technique – inserting a fixed prefix to guide the model’s output – is not a fundamentally novel algorithmic contribution. It is an adaptation of a known prompt injection tactic from the AI safety literature (the “prefilling attack”). The experiments largely reiterate a well‑established point: multiple‑choice QA performance is highly sensitive to prompt formatting. The paper neither investigates model internals nor trains new models.\n\nThe core mechanism—biasing the model to emit the option label as its very first token—has long been used implicitly. Standard protocols such as MMLU prompt with a question and options, then append “Answer:” so the model autocompletes the option label, as outlined in the original MMLU paper. This submission replaces that approach with a more detailed template but does not alter the underlying principle.\n\nThe authors’ application of this idea to MCQA is novel, but some may view the work as a relatively straightforward trick rather than a deep innovation. In essence, the paper’s value is in recognizing and fixing an evaluation issue more than in inventing new modeling techniques. \n\n2) The experiments, while extensive for MCQA, are limited to multiple-choice QA tasks in English. The authors specifically note they did not explore other task formats (like open-ended QA, dialogue, or generation tasks) where similar misalignment issues could occur. It’s unclear how broadly the prefilling approach generalizes. For example, would this technique help if the model needed to output a structured list or a specific format in other contexts? The paper doesn’t investigate tasks beyond QA, nor does it cover multilingual benchmarks. Thus, the contribution, as strong as it is for MCQA, has a somewhat narrow application domain in its current form. The significance for other settings remains an open question.\n\n3) While results are overwhelmingly positive, the paper does not deeply analyze scenarios where prefilling might not help or could possibly hurt. For instance, in Table 2, one model (Mistral-12B on Social IQa) saw a slight decrease in accuracy with prefilling (44.5% to 42.8%, if I read correctly) despite an increase on others. The authors do not comment on this anomaly – is it within error margin or is there a specific reason prefilling didn’t help that case? More broadly, one wonders: are there any situations where prefilling could bias the model towards an incorrect choice or interfere with the reasoning process? The paper’s analysis did not highlight any clear failure cases or trade-offs (which is good news, but also surprising). A bit more discussion on when/why the method might sometimes underperform or any observed downsides (even trivial, like slightly longer prompts) would have strengthened the rigor. As it stands, the technique appears almost too universally positive, which warrants careful consideration.\n\n4) The paper could be criticized for missing a couple of baseline variants. They compare against one prompt engineering instruction (“answer only with [A/B/C/D]”), but there are other ways one might try to enforce format without output injection – for example, explicitly instructing the model in the user prompt not to provide explanation and just give the option letter. It’s implied their prompt-side baseline covers this, but some detail on how that prompt was phrased would help (the paper only briefly mentions appending “Please answer only with [OPTIONS LIST]”). Also, when comparing to open-ended generation, they use external classifiers. Another reasonable baseline could have been simple regex extraction of the option from the model’s free-form answer (many practical evals do this).\n\n5) The study fixes the prefilling template to one default phrase (“Given the question and the possible options, my answer is: ”). It’s not reported whether this exact wording was optimized or if other variations were tried. The effectiveness might depend on phrasing nuances or model tuning (some models might respond better to different wording). A potential weakness is the lack of exploration into prompt sensitivity: if a user naively chooses a less optimal prefix, would the improvements diminish? The paper doesn’t explore different templates or provide guidance on choosing a good prefix beyond the one example. This could limit practical adoption, although the chosen template seems general and worked well across tasks. It would strengthen the work to know that prefilling is robust to wording or to have justified why the chosen phrase was used (e.g., was it inspired by the AI safety literature examples?)."}, "questions": {"value": "1) While the paper focuses on multiple-choice QA, do the authors believe the output prefilling strategy could help in other scenarios? For instance, could a similar approach improve truthfulness or format in open-ended QA (by prefilling an answer prefix), or help structure outputs in tasks like summarization or code generation?\n\n2) How sensitive are the results to the exact phrasing of the prefilling prefix? The paper uses one default template across all experiments. Did the authors try alternative wordings (even minor variations like “The correct answer is:” vs “The correct option is:” etc.) and observe any differences?\n\n3) Could the authors shed light on why prefilling works so effectively from the model behavior perspective? The paper mentions exploiting the model’s “normal cognitive biases” via the structured prefix. Do the authors hypothesize that the prefix puts the model into a more “answer-oriented” mode (perhaps due to fine-tuning on Q&A format data where answers often start with phrases like “The answer is”)?\n\n4) The study mostly evaluates open-source models where one has full control over token-level output. How would one implement output prefilling in a black-box API setting (like OpenAI’s ChatGPT)? Is it as simple as including the prefix in the system or user prompt?\n\n5) The results show a notable improvement in calibration with prefilling. Can the authors clarify how calibration was measured (ECE, log loss, or simply looking at confidence vs accuracy curves)? And why might prefilling improve calibration “by a substantial margin”? Is it because the model’s probability mass is re-distributed more meaningfully when it is confident about a structured answer format (versus possibly predicting some irrelevant token with high probability)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PoJlSIp98S", "forum": "7pNiMG2uaJ", "replyto": "7pNiMG2uaJ", "signatures": ["ICLR.cc/2026/Conference/Submission6943/Reviewer_SX3G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6943/Reviewer_SX3G"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149358664, "cdate": 1762149358664, "tmdate": 1762919173182, "mdate": 1762919173182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}