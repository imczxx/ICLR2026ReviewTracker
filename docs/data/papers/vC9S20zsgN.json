{"id": "vC9S20zsgN", "number": 16408, "cdate": 1758264301584, "mdate": 1763584298361, "content": {"title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models", "abstract": "The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. Building on production theory, we develop an economically grounded framework for evaluating language models by combining accuracy and inference cost. We formalize cost-of-pass, the expected monetary cost of generating a correct solution. We then define the frontier cost-of-pass as the minimum cost-of-pass achievable across available models or the human-expert, using the approximate cost of hiring an expert. Our analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, we examine counterfactual frontiers—estimates of cost-efficiency without specific model classes. We find that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledge-intensive, and complex quantitative tasks, respectively. Finally, we assess the cost-reductions from common inference-time techniques (majority voting and self-refinement), and a budget-aware technique (TALE-EP). We find that performance-oriented methods with marginal performance gains rarely justify the costs, while TALE-EP shows some promise. Overall, our findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and our economic framework provides a principled tool for measuring this progress and guiding deployment.", "tldr": "Introduces cost-of-pass (dollars per correct answer) and its frontier to measure LM cost-efficiency. Analyzes model families across task types, reports rapidly falling frontier, and reveals limited cost-effectiveness of common inference-time methods.", "keywords": ["economic evaluation framework", "language-model evaluation", "cost‑performance trade‑off", "inference time techniques"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a5d7f5469a6b4d718bb52038529e5ce59fbe0fa6.pdf", "supplementary_material": "/attachment/d15294374c5789652c755fcc9dd3253784623d09.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Cost-of-Pass, a metric for evaluating the economic efficiency of language models (LMs). The metric is defined as the expected monetary cost of producing a correct solution. The authors also define a frontier cost-of-pass, representing the minimum cost across available LMs and a human-expert baseline, and track how this frontier evolves over time and across model families (lightweight, large, reasoning). They report that lightweight models dominate simple tasks, large models excel at knowledge-based problems, and reasoning models are most efficient for complex quantitative reasoning. The work claims to ground LM evaluation in production theory and provide a principled economic framework for assessing progress."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tries to address an increasingly relevant topic: economic efficiency and cost-awareness in LLM evaluation. The framework provides an intuitive and interpretable scalar metric that combines cost and accuracy. The evaluation also Includes multiple model families and task types, offering a broad empirical view. The use of “human-expert cost” as a reference baseline is a reasonable and communicable idea."}, "weaknesses": {"value": "1. Theoretical novelty: The paper's formalism rephrases classic production frontier concepts without introducing new theoretical insights. The \"cost-of-pass\" is simply a ratio of cost per correct answer and the \"frontier\" is just the minimum of these values over models. This is an intuitive but elementary restatement, not providing any novel theoretical insights into the LLM industry. As a result, the so-called “economic foundation” adds rhetorical flavor but little intellectual substance.\n2. Empirical metrics: The paper defines the cost purely as API pricing multiplied by token counts. However, these prices are commercial, provider-dependent, and change frequently by many economic factors. Thus, the analysis reflects vendor pricing strategies, not intrinsic model efficiency. The “economic” quantities are therefore non-fundamental and insightful. Also, “correctness” is defined as a binary pass/fail outcome per benchmark. This oversimplified notion of correctness ignores nuanced forms of reasoning quality, partial credit, or structured evaluation that are essential for emerging multimodal, interactive, or agentic tasks.\n3. Experimental results: The empirical results primarily plot “frontier cost” over time or remove model families to show counterfactual trends. These are descriptive and unsurprising that lightweight models are cheaper for simple math, etc. The human-expert baseline is estimated from tutoring websites and contest durations, which introduces large uncontrolled variance. Most importantly, these insights have been largely discussed in previous work in cost-aware inference and routers and so on. The paper's results don't provide significant new insights."}, "questions": {"value": "1. How can you model different tasks that LLMs are already really good at and are already solving in the real world?\n2. How can your framework provide either machine learning insights or economics insights about future LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aK6btVAdrD", "forum": "vC9S20zsgN", "replyto": "vC9S20zsgN", "signatures": ["ICLR.cc/2026/Conference/Submission16408/Reviewer_bX6f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16408/Reviewer_bX6f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859958662, "cdate": 1761859958662, "tmdate": 1762926527087, "mdate": 1762926527087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper posits the cost of generating a correct solution to a problem should be a central metric for understanding AI progress and it's potential economic impact. This can differ substantially from measures like price per token because the number of tokens generated for a response can vary substantially between models. However, it also differs from price per query because the model requires a correct answer and allows for many samples given some probability that one is correct. This is similar to a pass@k measure. Using this model they estimate price-at-pass costs for several quantitative tasks and also gather data on the cost of a human performing those tasks. This measure shows different niches for differing model sizes and non-trivial tradeoffs between accuracy and price. Further some inference time methods are valuable under this measure, and some like majority voting are useless (since the pass@k type measure would simply be able to select the correct answer from the majority voting system). They track this value over time showing rapid exponential decrease in cost-of-pass, but the rate of cost decrease is very non-uniform across their task domains."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper poses an interesting metric that seems useful in understanding model improvement in a practical way\n- Historical trends are analyzed over standard models and benchmarks, and human baseline costs are estimated"}, "weaknesses": {"value": "- The human price estimates do not seem very rigorously studied or calculated\n- cost-to-pass has a number of limitations which the authors do address fairly well in Appendix D. I would love to see more discussion of what assumptions are needed for this metric to be a good reflection of reality and what domains that might hold true in."}, "questions": {"value": "- I would love to see more discussion and exploration on the potential costs of finding/verifying the correct answer. (I now see there is some of this in Appendix D, I'd rather have it addressed in the main paper and even more consideration given in the appendix). It is especially unclear to me that Tau-bench in C.2 is applicable here. Wrong computer execution or airline bookings can be disastrous if a wrong trace is produced.\n- It is very strange to me that your prices for MATH500 and AIME are so different. I believe these are extremely similar types of problems and skills involved and I would expect these to be performed by the same people and thus the tutoring wage should be the same.\n- I'm pretty sure your method makes any majority vote or similar method trivially useless. Thus I don't see why you have both 3 and 4 player voting and so much exposition on it. \n- I believe there is work by Epoch AI and Neil Thompson that seem relevant to this paper and the authors may be interested in looking into.\n\n-- Regarding price performance increases in LLMS\n\nBen Cottier, Ben Snodin, David Owen, and Tom Adamczewski. LLM inference prices have fallen rapidly but unequally across tasks, 2025. URL https://epoch.ai/data-insights/llm-inference-price-trends. Accessed: 2025-09-03.\n\nThe Price of Progress\nHans Gundlach, Jayson Lynch, Matthias Mertens, Neil Thompson \nhttps://openreview.net/pdf?id=JEsU87WUUb\n\n-- regarding inference time compute methods\n\nPablo Villalobos and David Atkinson. Trading Off Compute in Training and Inference, 2023. URL145\nhttps://epoch.ai/blog/trading-off-compute-in-training-and-inference. Ac-146\ncessed: 2025-09-02.\n\n\n-- regarding economic modeling of tasks and how to think about when they might be economically competitive with people\n\nBeyond AI Exposure:\nWhich Tasks are Cost-Effective to Automate with\nComputer Vision?\nMaja S. Svanberg, Wensu Li, Martin Fleming, Brian C. Goehring, Neil C. Thompson\n\nNBER WORKING PAPER SERIES\nEXPERTISE\nDavid Autor, Neil Thompson\n\nEconomic impacts of AI-augmented R&D\nTamay Besiroglu, Nicholas Emery-Xu, Neil Thompson"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ELm50WQFnd", "forum": "vC9S20zsgN", "replyto": "vC9S20zsgN", "signatures": ["ICLR.cc/2026/Conference/Submission16408/Reviewer_GCEw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16408/Reviewer_GCEw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963978859, "cdate": 1761963978859, "tmdate": 1762926526668, "mdate": 1762926526668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the cost-of-pass framework for evaluating the usefulness of language models in an economic light. They show that progress has been driven by lightweight and reasoning models for most domains. They also show that the impact of inference time techniques is nonexistent or minimal on the frontier cost of pass with the exception of cost-aware techniques like TALE-EP."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper engaged much more thoroughly with the economics than other literature. Particularly, evaluating human performance as part of the frontier cost of passing. I liked the attribution of progress to different model classes. It's interesting to see that reasoning models have led to progress adjusted for cost. I also thought the examination of inference time techniques was particularly interesting. Overall, a novel perspective, and I’d like to see more work like this engaging with economic constraints."}, "weaknesses": {"value": "Ideally, the frontier cost of pass would include joint human+AI ability to solve problems. For instance, maybe a person with an AI chatbot can beat both another human and an AI chatbot. Human-AI collaboration ability is quite hard to measure experimentally, but some discussion of the literature in this area would be useful, ie, https://digitaleconomy.stanford.edu/wp-content/uploads/2025/06/CentaurEvaluations.pdf\n\nI would probably try to summarize the related work section in the introduction (this is mostly already done), so I’d probably remove this section. There is a lot of mathematics to justify quite intuitive results. I’d probably try to move some of the exact definitions to the appendix and try to incorporate the limitations and further work in the main body of the paper. I’d also try to make the limitation section stronger (see the questions below)."}, "questions": {"value": "A more multidimensional discussion of inference time improvements would be helpful. Many techniques, for instance, majority vote, allow parallel processing, which dramatically increases cost but reduces wall clock time. What are the upsides to these techniques, or should people not use them in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yQEbRbkHjX", "forum": "vC9S20zsgN", "replyto": "vC9S20zsgN", "signatures": ["ICLR.cc/2026/Conference/Submission16408/Reviewer_YBga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16408/Reviewer_YBga"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985467135, "cdate": 1761985467135, "tmdate": 1762926526276, "mdate": 1762926526276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an economic framework for evaluating language models that jointly considers accuracy and inference cost. The authors introduce \"cost-of-pass” as the expected monetary cost to generate a correct solution and \"frontier cost-of-pass”. The framework is applied to analyse 11 language models across 6 benchmarks. Results indicate that: 1) different model families (lightweight, large, reasoning) are cost-effective for different task types, 2) frontier cost-of-pass has decreased exponentially, particularly for complex quantitative tasks and 3) model-level innovations drive progress more than inference-time techniques."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has strong theoretical grounding in established production theory.\n2. It is clearly formulated and easy to follow, even for those working in parallel fields.\n3. It has practical and timely relevance as models become increasingly expensive and real-world decisions become more critical.\n4. Experiments are comprehensive."}, "weaknesses": {"value": "1. There seems to be no discussion or handling of (KV or similar) caches. These can significantly reduce the cost of inference. Similarly, no discussion of batching opts that have large cost effects. These are critical limitations that need to be highlighted.\n2. The binary assumption does limit the practical application. There is no discussion or proposal on how this might apply when precision/recall trade-offs are important. Related to this, no discussion of how different domains might care about different error types, such as healthcare vs finance vs research. The authors partly acknowledge this but I think the paper would be much stronger if it was given more comprehensive consideration.\n3. Similarly, the quality of the answer can have a significant difference. If two models both give the right answer but only one of them gives detailed explanations with in-depth knowledge, that might be much more valuable than the other simple answer. Another limitation that isn’t discussed sufficiently.\n4. The cost-of-pass might be falling not because models are more efficient, but because the problems are getting stale and contaminated in training data. Models (including smaller/cheaper ones) can get “unnaturally” high scores on some benchmarks because they target them specifically. What is the risk that MATH-500 is increasingly used in training data for newer, smaller models, artificially increasing their score on them?\n5. It is not clear to me if the framework handles amortisation in real LM deployment sufficiently. In classical production making 10 products = 10x cost of 1 product (in the same factory). But LM deployment is more dynamic when it comes to problem solving, each one is unique but can leverage learnings from previous ones to improve performance, e.g. through prompt improvements. Can these be assumed to be approximately the same? This thought/question is partly inspired by Learning by Doing by Kenneth Arrow.\n6. Writing sometimes feels like patch-work. Abbreviations like “language models (LMs)” are defined three times. This only needs to be done once.\n7. No analysis of how sensitive the conclusions are to variations in human-expert cost estimates. Given the wide variability in expert compensation ($15-$100/hour for different expertise levels, per Appendix A), this might be significant.\n8. No confidence intervals or standard deviations reported for cost-of-pass estimates. Hard to judge variance and significance."}, "questions": {"value": "1. Being a little disingenuous here: what would a naive baseline model that is one random selection call (~$0.00001 per attempt) score as a cost-of-pass? Does there need to be some weighting or other term added since the low cost term dominates as the cost tends to 0, especially in MCQ tasks?\n2. The paper assumes Rexpert(p) ≈ 1 (line 190), but provides no empirical validation that human experts actually achieve near-perfect accuracy on these benchmarks. Where does this assumption come from?\n3. If a model consistently fails on certain problem types, repeated attempts won't help, so won't 1/Rm(p) be an overestimate of required attempts? These aren’t independent tests.\n4. How were the 6 benchmarks selected? Why not include code generation, long-context, or creative tasks?\n5. Which specific versions of each model were used? (GPT-4o has had multiple versions)\n6. Have the authors considered median or worst-case cost-of-pass instead of expectation? This might be useful in many fields."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tVHjVhqT2V", "forum": "vC9S20zsgN", "replyto": "vC9S20zsgN", "signatures": ["ICLR.cc/2026/Conference/Submission16408/Reviewer_ucAV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16408/Reviewer_ucAV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995813662, "cdate": 1761995813662, "tmdate": 1762926525854, "mdate": 1762926525854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Summary"}, "comment": {"value": "Dear Reviewers and ACs,\n\nWe appreciate the valuable feedback from the reviewers.  In the revised manuscript, we made the following changes, and higlighted them in blue color:\n\n- **Toning down claims of theoretical novelty:** We clarified that our main contribution is to adapt production theory into a grounded and operational evaluation framework for LMs. We therefore softened language (in Abstract, Introduction and Related Works sections) that could be read as overstating theoretical novelty. (*bX6f*)\n\n- **Writing and presentation improvements:** We strengthened the introduction by bringing key related work earlier, expanded the Related Work section with additional recommended papers, and corrected minor issues such as typos and redundant definitions (e.g. repeated definition of “language models”). (*ucAV, GCEw*)\n\n- **Human expert baseline:** We clarified the basis of our human baseline assumptions in both the main text (Section 2.4) and Section D.3. We corrected the cost estimation for MATH500 and clarified its distinction from AIME in Section A, and we added a regional ablation (using hourly rates from India) as a sensitivity study to test the robustness of the human expert baseline. (*bX6f, ucAV, GCEw*)\n\n- **Confidence intervals:** In Section C.7, we now report 95\\% bootstrapped confidence intervals for the results in Table 1. (*ucAV*)\n\n- **Benchmark hacking scenario:** In Section C.8, we added an adversarial / naive baseline experiment to illustrate how \"benchmark hacking\" scenarios can arise, how they affect the interpretation, and how to mitigate such concerns. (*ucAV*)\n\n- **Extensions in Section D.1 for modeling complex deployments:** We enriched Section D.1 to show how caching and batching can be modeled as amortized operations in the cost function, how alternative binary success criteria can encode precision/recall trade-offs, how our framework can be instantiated in a realistic extension of Tau-Bench to a customer-support setting, and how the parallelizability of majority voting can make it appealing when latency (rather than dollar costs) is the primary objective. (*ucAV, YBga, GCEw*)\n\n- **Alternatives to the expectation operator:** In Section D.2, we note that alternative aggregators (such as median or worst-case) may be appropriate when modeling distributions of cost or cost-of-pass under different real-world objectives. (*ucAV*)\n\n- **Limitations and future directions in Section D.3:** We expanded the limitations discussion to clarify the scenarios where our framework is most appropriate, describe how future work could extend the binary success modeling to continuous utilities, and explain how different error types and domain-specific costs can be encoded. We explicitly acknowledge contamination concerns on public benchmarks, elaborate on the human-baseline's near-perfect accuracy assumption and its limitations, highlight hybrid human–AI systems as a promising direction of incorporation, and emphasize that while our experiments focus on a canonical / risk-neutral setting, inference-time techniques can be especially valuable in risk-aware or reliability-focused deployments. (*bX6f, ucAV, YBga, GCEw*)\n\nWe hope these revisions address the concerns raised in the reviews and clarify both the scope and contributions of our framework."}}, "id": "NHoI7pypsq", "forum": "vC9S20zsgN", "replyto": "vC9S20zsgN", "signatures": ["ICLR.cc/2026/Conference/Submission16408/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16408/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission16408/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763585027491, "cdate": 1763585027491, "tmdate": 1763585027491, "mdate": 1763585027491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}