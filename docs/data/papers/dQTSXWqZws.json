{"id": "dQTSXWqZws", "number": 1276, "cdate": 1756868713764, "mdate": 1763647652944, "content": {"title": "Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach", "abstract": "Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional performance across diverse tasks, continually surpassing previous expectations regarding their capabilities. Nevertheless, their proficiency in perceiving emotions from images remains debated, with studies yielding divergent results in zero-shot scenarios. We argue that this inconsistency stems partly from constraints in existing evaluation methods, including the oversight of plausible responses, limited emotional taxonomies, neglect of contextual factors, and labor-intensive annotations. To facilitate customized visual emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task that overcomes these constraints. Complementing this task, we devise an automated pipeline that efficiently constructs emotion-centric statements with minimal human effort. Through systematically evaluating prevailing MLLMs, our study showcases their stronger performance in emotion interpretation and context-based emotion judgment, while revealing relative limitations in comprehending perception subjectivity. When compared to humans, even top-performing MLLMs like GPT4o demonstrate remarkable performance gaps, underscoring key areas for future improvement. By developing a fundamental evaluation framework and conducting a comprehensive MLLM assessment, we hope this work contributes to advancing emotional intelligence in MLLMs. Codes and data will be released.", "tldr": "", "keywords": ["MLLM", "Visual Emotion Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f905d595bbd02a7d2ef28b01d32a6673169476d9.pdf", "supplementary_material": "/attachment/2f2e73031dac49a09dd124c88148289c89e10a49.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new annotation framework for MLLMs in the task of visual emotion estimation. The paper identifies limitations in the evaluation of MLLMs (namely, inaccurate responses, limited taxonomies, a focus on intrinsic image attributes, and reliance on majority voting) and first introduces a new task to address these limitations. Then, an automated annotation pipeline leveraging an ensemble of MLLMs is used to annotate 400k images and create fine-grained annotations that are used to produce statements with GPT4 and curated by a human expert. The labels and statements are further processed to produce additional coarse annotations (in Parrott-based hierarchy and positive/negative polarity). Finally, several popular MLLMs are benchmarked on the test set of the dataset, including an in-context learning and a finetuned version of Qwen."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problems identified by the paper are valid. The uncertain nature of human emotions (invoked, perceived or otherwise) is one of the key limitations in achieving high accuracy in the task, opposed to object detection, for example. Cultural barriers, inconsistent hierarchies, etc, are all issues in affective computing (and psychology for that matter).\n- The proposed method is indeed scalable and can be replicated to extend the proposed dataset or correct the labels of existing benchmarks.\n- With the rise of MLLMs, there is a growing interest in applying them to downstream tasks; however, the lack of annotations in natural language makes things more complicated, which is something this method addresses."}, "weaknesses": {"value": "- One major weakness is the contradiction in motivation. Specifically, two of the key issues identified in the intro are a) MLLM's inaccurate responses when compared to human judgment and b) majority voting to address disagreement. The method then proceeds to create annotations by taking the majority voting of MLLMs. This is reiterated in the discussion (lines 448-9): \"MLLMs may not yet be sufficiently competent for LLM-as-a-judge [...]\". If this is the case, how are they sufficient as annotators? The method is contradictory to the problem it is trying to solve and thus is not convincing in terms of annotation accuracy. The one human expert curating statements post-hoc is probably not enough, as: a) there is already unconscious bias in the process as the expert is shown annotations rather than producing annotations, b) the task is uncertain even for experts, therefore, is one person really enough?, c) the method does not account for cultural differences and biases (both in the expert and the process as a whole). We also don't have an indication of how the humans (expert or otherwise) perform on the open-vocabulary compared to the LLMs. In general, I am unconvinced about the validity and motivation of the method.\n- Looking at the human refinement process, the annotator agreement in Table 3 does not seem very strong, particularly for emotion interpretation and subjectivity, showing a (Cohen's?) Kappa of only 0.5. In fact, the only aspect where the human annotators agree is the scene context; for everything else, there is only weak agreement.\n- For the fine-tuned results in Table 6, only 10k out of 462k images are selected. This seems a bit of an odd choice, as most large-scale affective datasets (including EmoSet) are much larger; therefore, it seems unlikely that the finetuned version will be able to compete, but at least performance gains from fine-tuning are shown."}, "questions": {"value": "- How do the annotations generated by the proposed method in step [a] compare to the ones from EmoSet for the subset used in this work? Are the generated emotions in agreement with the 8 categorical used there?\n- Using a small subset, is it possible to compare human open-vocabulary to MLLM open-vocabulary annotations? Are these in agreement or at least in the same region?\n- The polarity underperformance may be due to the prompting used. Have you tried CoT reasoning? Polarity is a derivative, not a primary label, hence the suggestion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0sNQlGvMz2", "forum": "dQTSXWqZws", "replyto": "dQTSXWqZws", "signatures": ["ICLR.cc/2026/Conference/Submission1276/Reviewer_ck1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1276/Reviewer_ck1T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761306865903, "cdate": 1761306865903, "tmdate": 1762915725624, "mdate": 1762915725624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new evaluation paradigm for assessing multimodal large language models (MLLMs) on visual emotion understanding. Instead of traditional emotion classification, the authors propose an Emotion Statement Judgment (ESJ) task, where models evaluate whether a given emotion-related statement about an image is correct. The paper further presents an automated pipeline (INSETS) to construct large-scale emotion-centric statements and builds the MVEI benchmark covering four dimensions: sentiment polarity, emotion interpretation, scene context, and perception subjectivity. Experiments show that current MLLMs demonstrate certain capabilities but still lag behind humans, especially in subjective and contextual emotional reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive emotional dimensions, covering aspects that existing benchmarks ignore, such as scene context and perception subjectivity.\n\nExtensive experiments on a wide range of MLLMs with meaningful insights and human comparison results."}, "weaknesses": {"value": "The dataset is partially constructed using existing MLLMs, which may introduce inherited biases. Although human refinement is applied, its proportion remains limited.\n\nThe distinction from EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment is not sufficiently articulated.\n\nThe dataset is built on a single source domain, and its generalizability to other visual contexts is unclear."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V8V5VZZu4H", "forum": "dQTSXWqZws", "replyto": "dQTSXWqZws", "signatures": ["ICLR.cc/2026/Conference/Submission1276/Reviewer_Kehu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1276/Reviewer_Kehu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452468073, "cdate": 1761452468073, "tmdate": 1762915725215, "mdate": 1762915725215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the incompatibility of conventional emotion evaluation methods with Multimodal Large Language Models (MLLMs) by proposing the Emotion Statement Judgment (ESJ) task and the INSETS annotation pipeline. The work constructs MVEI, a benchmark with 3,086 samples covering four dimensions: sentiment polarity, emotion interpretation, scene context, and perception subjectivity. The authors evaluate 18 MLLMs and demonstrate that even top-performing models like GPT4o fall substantially short of human performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Benchmark Innovation and Scope: The ESJ task and MVEI benchmark fill a substantial gap in visual emotion evaluation for MLLMs, moving beyond rigid classification to nuanced, open-vocabulary, and multifaceted judgment tasks. This approach allows broader, context-aware, and fine-grained assessment.\n2. Automated scalable pipeline: The INSETS pipeline effectively reduces human labor by automating emotion tagging and statement generation while maintaining a reported 90.6 % accuracy after human refinement.\n3. Comprehensive empirical analysis: The benchmark evaluation spans >15 MLLMs, both open and proprietary, and compares against human baselines .\n4. Grounding in Theory: The mapping to Parrott’s hierarchical emotion model and the use of extensive prompts/protocols demonstrate that the benchmark is theoretically informed, not ad-hoc."}, "weaknesses": {"value": "1. Potential circularity in evaluation: Some MLLMs used to construct INSETS also participate in evaluation, which could inflate results and reduce benchmark independence.\n2. Vague or Underspecified Mathematical/Labeling Procedures: The emotion label assignment, majority voting, and mapping to the hierarchical model are described mainly in prose and schematic diagrams, with some reliance on prompt outputs and “manual expert check.” The specific algorithms are insufficiently formalized.\n3. Human annotation reliability and bias: Although inter-annotator agreement statistics are reported, there is limited qualitative analysis of ambiguous cases or bias due to annotator demographics.\n4. Insufficient discussion of dataset bias and ethical risk: Bias acknowledgement is limited to a short ethics statement without empirical analysis of representation balance (e.g., emotional polarity, demographic context).\n5. Limited analysis of failure modes: Sec. 5.3 identifies that \"perception subjectivity shows only modest improvement\"  but provides no error analysis. What types of subjectivity statements are most challenging? Are errors systematic? Figure 12 shows examples but no quantitative breakdown.\n6. Absence of ablation or component analysis: There is no direct evidence quantifying how each component contributes to final benchmark quality."}, "questions": {"value": "Please check the weakness part."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GenoDbIauG", "forum": "dQTSXWqZws", "replyto": "dQTSXWqZws", "signatures": ["ICLR.cc/2026/Conference/Submission1276/Reviewer_joCJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1276/Reviewer_joCJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777036984, "cdate": 1761777036984, "tmdate": 1762915725023, "mdate": 1762915725023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the evaluation of visual emotional perception in Multimodal Large Language Models (MLLMs). The authors argue that prior evaluation methods—fixed label classification and coarse taxonomies—fail to capture the subjectivity, contextual dependence, and open-vocabulary nature of emotion perception. To remedy this, they propose the Emotion Statement Judgment (ESJ) task: MLLMs judge whether emotion-focused statements about an image are correct, enabling open-vocabulary, multifaceted assessment. They also introduce INSETS, an automated pipeline that generates emotion-centric labels and statement candidates with limited human intervention. Using INSETS they build a large automatically annotated corpus (INSETS-462k) and then refine a human-validated benchmark (MVEI) of 3,086 image–statement pairs covering four complementary dimensions: sentiment polarity, emotion interpretation, scene context, and perception subjectivity. Systematic evaluation of many MLLMs shows that models have notable strengths (especially in emotion interpretation and scene context) but still trail humans—particularly on sentiment polarity and perception subjectivity. The paper also explores lightweight adaptation methods (in-context learning, LoRA, full fine-tuning, GRPO) and finds that adaptation improves performance—most dramatically for sentiment polarity—while perception subjectivity remains challenging. The authors release code and data and discuss ethical considerations around dataset biases and model outputs.Three main contributions:1. Task formulation: Introduces the Emotion Statement Judgment (ESJ) task, a flexible, open-vocabulary framework that reframes visual emotion evaluation as a statement verification problem, reducing issues from rigid ground-truth answers and enabling richer, multifaceted assessment.2. Scalable annotation pipeline and corpus: Proposes INSETS, an automated pipeline for generating emotion labels and statements with minimal human effort, and uses it to construct INSETS-462k (462k statements across ~17.7k images), significantly improving scalability over prior labor-intensive datasets.3. Benchmark and empirical analysis: Curates MVEI (3,086 human-refined image–statement pairs) covering four evaluation dimensions (sentiment polarity, emotion interpretation, scene context, perception subjectivity), and provides systematic benchmarking of many contemporary MLLMs plus adaptation studies that reveal strengths, weaknesses, and directions for improvement.Three main shortcomings / limitations:1. Model scale coverage: The evaluation is limited mainly to MLLMs with fewer than ~10B parameters because of computational constraints, excluding larger open-source or closed models that might perform differently; this limits conclusions about upper-bound capabilities.2. Monolingual focus and potential biases: The current implementation and benchmark are monolingual (presumably English) and the automatically generated INSETS-462k may inherit biases from pretraining data and MLLMs; despite human refinement some problematic or culturally specific labels could persist.3. Fundamental difficulty with subjectivity: While adaptation improves sentiment polarity substantially, perception subjectivity remains poorly handled by MLLMs and shows only modest gains from fine-tuning; this suggests deeper architectural or objective-level limitations that the current pipeline and data do not fully resolve."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Task formulation: Introduces the Emotion Statement Judgment (ESJ) task, a flexible, open-vocabulary framework that reframes visual emotion evaluation as a statement verification problem, reducing issues from rigid ground-truth answers and enabling richer, multifaceted assessment.\n- Scalable annotation pipeline and corpus: Proposes INSETS, an automated pipeline for generating emotion labels and statements with minimal human effort, and uses it to construct INSETS-462k (462k statements across ~17.7k images), significantly improving scalability over prior labor-intensive datasets.\n- Benchmark and empirical analysis: Curates MVEI (3,086 human-refined image–statement pairs) covering four evaluation dimensions (sentiment polarity, emotion interpretation, scene context, perception subjectivity), and provides systematic benchmarking of many contemporary MLLMs plus adaptation studies that reveal strengths, weaknesses, and directions for improvement."}, "weaknesses": {"value": "- Model scale coverage: The evaluation is limited mainly to MLLMs with fewer than ~10B parameters because of computational constraints, excluding larger open-source or closed models that might perform differently; this limits conclusions about upper-bound capabilities.\n- Monolingual focus and potential biases: The current implementation and benchmark are monolingual (presumably English) and the automatically generated INSETS-462k may inherit biases from pretraining data and MLLMs; despite human refinement some problematic or culturally specific labels could persist.\n- Fundamental difficulty with subjectivity: While adaptation improves sentiment polarity substantially, perception subjectivity remains poorly handled by MLLMs and shows only modest gains from fine-tuning; this suggests deeper architectural or objective-level limitations that the current pipeline and data do not fully resolve."}, "questions": {"value": "SEE WEAKNESS"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cVD9WVqWwo", "forum": "dQTSXWqZws", "replyto": "dQTSXWqZws", "signatures": ["ICLR.cc/2026/Conference/Submission1276/Reviewer_nM4y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1276/Reviewer_nM4y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971317194, "cdate": 1761971317194, "tmdate": 1762915724900, "mdate": 1762915724900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}