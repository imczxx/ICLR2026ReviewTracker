{"id": "kQRjcBmFAD", "number": 24867, "cdate": 1758361357897, "mdate": 1759896744680, "content": {"title": "Chain-of-Thought Hijacking", "abstract": "Reasoning models are widely used to improve task performance by allocating\nmore inference-time compute, and prior work suggest it may also strengthen\nsafety by improving refusal. Yet we find the opposite: the same reasoning can\nbe used to bypass safety. We introduce Chain-of-Thought Hijacking, a jailbreak\nattack on reasoning models. The attack pads harmful requests with long sequences\nof harmless reasoning. Across HarmBench, CoT Hijacking reaches a 99% attack\nsuccess rate (ASR) on Gemini 2.5 Pro, far exceeding prior jailbreak methods. Our\nmechanistic analysis shows that mid layers encode the strength of safety checking,\nwhile late layers encode the verification outcome. Long benign CoT dilutes both\nsignals by shifting attention away from harmful tokens. Targeted ablations of at-\ntention heads identified by this analysis causally increased ASR, confirming their\nrole in a safety subnetwork. These results show that the most interpretable form\nof reasoning—explicit CoT—can itself become a jailbreak vector when combined\nwith final-answer cues. We release prompts, outputs, and judge decisions to facil-\nitate replication.", "tldr": "we introduce CoT hijacking, a new jailbreak for reasoning models", "keywords": ["safety", "jailbreaks", "chain-of-thought"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cada7f5dee584fa1b3ce3ac36e3493d2094e6883.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a new reasoning model jailbreak based on smuggling harmful request within a benign reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors address a timely and practically important issue: the ease with which reasoning-focused models can be jailbroken. Their proposed method is conceptually simple, likely simple enough that many users could independently discover and exploit similar strategies. This makes it especially relevant for model providers, as defending against such benign but effective jailbreaks poses a serious and ongoing challenge.\n\nThe approach seems to work well on harmbench and on closed models.\n\nIt is commendable that the authors include a brief mechanistic interpretability analysis via projection onto a refusal-steering vector. This analysis provides an initial glimpse into the internal dynamics underlying the method’s success."}, "weaknesses": {"value": "The overall quality of the work feels closer to a class project report or blog post than to a polished research contribution. Much of the presentation gives the impression of filling space rather than conveying substance. The paper makes excessive use of two-row tables (Tables 1, 3, 4, 5) and includes numerous large but only marginally informative figures, many of which appear redundant. In addition, the visual presentation suffers from poor readability, particularly due to the extremely small font sizes in Figures 4 to 9, which makes it difficult to extract key insights.\n\nThe empirical evaluation is also too limited to support strong claims. The authors test only four closed-source models and omit all open-source reasoning models, even though these are highly relevant to the stated problem and would connect more directly to the subsequent analysis. The benchmark coverage is restricted to a single dataset, and only two baseline methods are included. This narrow scope leaves unclear whether the observed vulnerabilities generalize beyond the chosen setup. For instance, more sophisticated jailbreaks such as the policy-over-values attack (https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/writeups/lucky-coin-jailbreak) would provide a valuable point of comparison.\n\nThe post-hoc analysis that attempts to explain why models are susceptible to such simple jailbreaks is a positive step but remains superficial. The paper would benefit from a more compact and insightful presentation of results, for example by merging Tables 1, 3, 4, and 5 into a single comprehensive table and moving some of the repetitive figures to the appendix. The resulting space could then be used to conduct a more thorough evaluation of the phenomenon, including additional models, datasets, and ablation studies. Such an expansion would elevate the work to the standard expected at a top-tier venue like ICLR. A good example of this type of in-depth analysis can be found in this recent study https://arxiv.org/abs/2406.05946, which investigates refusal in chat models."}, "questions": {"value": "I think the authors have several decent contributions and insights in their work and would encourage them to invest a significant amount of time to polish the presentation of their work.\n\nThe authors can improve my opinion about the paper by addressing my concerns raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kCUVYCxD7w", "forum": "kQRjcBmFAD", "replyto": "kQRjcBmFAD", "signatures": ["ICLR.cc/2026/Conference/Submission24867/Reviewer_bvV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24867/Reviewer_bvV4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662897608, "cdate": 1761662897608, "tmdate": 1762943227523, "mdate": 1762943227523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Chain-of-Thought Hijacking (CoT Hijacking), a novel jailbreak attack against large reasoning models (LRMs). The attack pads harmful requests with long sequences of benign reasoning followed by a final-answer cue, achieving state-of-the-art attack success rates. Through mechanistic analysis, the authors demonstrate that refusal behavior relies on a fragile, low-dimensional safety signal that becomes diluted as reasoning length increases. Attention shifts away from harmful tokens toward benign reasoning, weakening refusal features in later layers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper contrasts the attack vector against prior work H-CoT, which requires exposed safety reasoning. The experimental methodology also uses comprehensive evaluations across multiple frontier models (Gemini, ChatGPT, Grok, Claude), showing that their attack attains high success rate. I think the work is well structured, especially on how the mechanistic analysis builds incrementally. The mechanistic analysis connecting refusal directions, attention patterns, and causal interventions provides original insights into reasoning model safety."}, "weaknesses": {"value": "First of all, the paper frames puzzles as \"benign reasoning,\" but this characterization is quite questionable in my opinion. Given that the paper's core claim is that CoT length dilutes safety signals through attention mechanisms, it should include an ablation study that takes well-known benign reframing attacks such as persuasion attack [1] and explore how length changes affect the ASR. In my opinion, this attack works *because of the prompt rather than a general length effect*. Furthermore, no further information is provided about the red-teaming automation pipeline \"Seduction\" (line 226).\n\nAnother evidence that makes me think that such CoT Hijacking works is due to prompting rather than length effects is because of the output. If you take a closer look at Appendix D example as well as Figure 1 output, while the model outputs bypasses the safety guardrail, they do not provide clear instructions on how to fulfill the malicious requests but trying to adhere to the engineered prompt requests about logic grid puzzles. \n\nIn fact, the showcased outputs are not really fluent generation (e.g., \"The hum of the security checkpoint was a low, oppressive drone\" from Figure 1 doesn't make any sense.) I don't even see these examples as successful attack since no truly harmful instructions that fulfill the requests were provided by the model. In other words, the judge protocol is dubious. I would recommend using StrongReject evaluator, following the protocol of [2] with implementation from [3].\n\nLastly, the paper seems to frame the attack vector as something that reveals safety weaknesses on reasoning models, but I highly suspect the puzzle-like attack is generalizable to even the base model. If this is true, this doesn't demonstrate a reasoning-specific vulnerability introduced by extended CoT capabilities, but rather exposes the weaknesses in with handling puzzle-like prompts. In other words, without direct comparison to non-reasoning base models, the paper's central claim that reasoning creates new attack surfaces (line 034-035) remains unsubstantiated.\n\n---\n\n[1] Zeng, Yi, et al. \"How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.\" Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024.\n\n[2] NIST and UK AISI. Us aisi and uk aisi joint pre-deployment test: Openai o1. Technical report, National Institute of Standards and Technology and Department of Science Innovation and Technology, December 2024. Joint pre-deployment evaluation of OpenAI o1 model.\n\n[3] URL: https://strong-reject.readthedocs.io/en/latest/api/evaluate.html#strong_reject.evaluate.strongreject_aisi"}, "questions": {"value": "1. Based on Figure 1 grey texts, you seem to attack from both input prompt and CoT (prepending benign CoT). Am I understanding it right? If that's the case, is there an ablation done on how each factor affects the ASR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1QLWKBkN8H", "forum": "kQRjcBmFAD", "replyto": "kQRjcBmFAD", "signatures": ["ICLR.cc/2026/Conference/Submission24867/Reviewer_LG3o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24867/Reviewer_LG3o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678261055, "cdate": 1761678261055, "tmdate": 1762943227203, "mdate": 1762943227203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates a new phenomenon, dubbed \"chain of thought hijacking\", in which extending the context leads to 99% attack success rates in jailbreaking a safety-tuned model. \n\nThe authors provide a \"mechanistic explanation\" for this phenomenon, in which as context is scaled, attention is more distributed across the tokens, and \"weakens\" the refusal features in the model.\n\nThis simple trick leads to 99% attack success rate against contemporary models.\n\nThe authors then look at building a \"mechanistic explanation\" for the observed behavior.\n\nUsing prior work, extracting and leveraging a \"refusal feature direction\" can bidirectionally control for refusal -- subtracting the refusal direction leads to higher jailbreak success rates, while adding the vector leads to a significant drop. \n\nThe authors make the connection to longer context lengths with what they call \"refusal dilution\" - as context is scaled with benign tokens, harmful tokens only make up a small fraction to which the model attends to, and no longer triggering the refusal features.\n\nThe authors finally localize attention heads that seemingly are relevant for refusal + \"refusal dilution\", and show that ablating a small set of heads can flatten the distinction between harmful vs. harmless prompts, significantly reducing refusal rates.\n\nWhile nicely executed, my main reservation is that these observed phenomena are not new. In particular, Li et al (https://arxiv.org/pdf/2402.10962) demonstrate \"instruction drift\", akin to \"refusal drift\", in which as context is scaled, less attention is spent on the system prompts (ie, instructions), akin to attending to toxic tokens, and models no longer follow instructions as context is scaled (akin to models no longer refusing harmful prompts). With that being said, while the paper sheds light to a attack surface against language models, scientifically, there is not much new that is learned for the reader."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-scoped, and experiments are well designed and executed to back the author's claims. In particular, a 99% success rate is compelling. The overall narrative is coherent, starting from demonstrating a new phenomena (99% jailbreak success rate with scaled contexts) to explaining the underlying mechanisms."}, "weaknesses": {"value": "As indicated in the summary, my main criticism is that the reported phenomena is not new. Li et al. (https://arxiv.org/pdf/2402.10962) as demonstrated this phenomena already, and there are many parallels (instruction drift vs. refusal drift) stemming from the same underlying reasons/mechanisms (less attention being spent on system prompts/instructions vs. toxic tokens). Put differently, the paper carves out a neat narrative and sheds light to a new vulnerability of language models, but to be honest, scientifically there isn't much new to take away from reading the work.\n\nIt is also unclear how much of the content of the scaled context plays a role in successful attacks. In particular, the examples used in the paper relate to a reasoning/logic puzzle that is closely related to the target jailbreaking behavior. However, if the author's claims are correct, what is in the context should not matter so long as context is scaled and attention is diluted. I think a study should be added to ablate these two things. \n\nLastly, presentation could be improved. Many of the figures in the main text are never referenced, making it a bit difficult to follow the text. ex: Figure 4, 5, 6, 7.. Also, the legends in all the figures are not legible."}, "questions": {"value": "minor typoes:\n\nline 170: \"textbf\" --> \"\\textbf{}\"\n\nline 376: Figure 17: Did this mean to say Figure 6? \nline 398: \"Figures ??\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AnDa94f9vL", "forum": "kQRjcBmFAD", "replyto": "kQRjcBmFAD", "signatures": ["ICLR.cc/2026/Conference/Submission24867/Reviewer_iSkK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24867/Reviewer_iSkK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689092373, "cdate": 1761689092373, "tmdate": 1762943226909, "mdate": 1762943226909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a simple jailbreak attack on reasoning language models called Chain-of-Thought Hijacking, which prepends harmless reasoning to a harmful prompt. The authors found that this method can achieve high attack success rates on several proprietary reasoning models. They additionally perform mechanistic analysis on the refusal behaviors of reasoning models by taking the difference in activations between benign and harmful prompts. They find that harmfulness/refusal is mediated by a single low-dimensional direction, similar to non-reasoning models in prior work. Lastly, they suggest that the success of Chain-of-Thought Hijacking is due to long reasoning causing attention to shift away from harmful tokens."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a simple jailbreak attack that can achieve high ASRs on strong proprietary reasoning models, raising safety concerns.\n- The paper extends prior studies on understanding the mechanisms of refusal/harmful behaviors in base models to reasoning models, showing that reasoning models similarly exhibit a refusal direction."}, "weaknesses": {"value": "**1. Overall:**\n\nWhile this work presents a range of experiments, I find it difficult to connect the dots and see how they support a central, significant claim. There are many individual experiments but insufficient details or motivation for them. For example, the authors introduced the concept of “refusal dilution” in section 5.4, but it is never elaborated or connected to section 6. Another example is the refusal direction experiments in section 5 - how are they related to your jailbreak method? \n\nAlso, the jailbreak experiments are conducted primarily on proprietary models, whereas the mechanistic analysis focuses solely on a single open-source model, Qwen3-13B. Is the mechanistic analysis trying to explain why your jailbreak method succeeds? If so, then the jailbreak evaluation and mechanistic analysis should be conducted on the same suite of reasoning models (or at least on overlapping sets of models).\n\n**2. Missing details and unclear experimental design:**\n- Table 2: It is unclear why the three baseline methods were chosen. Are they considered state-of-the-art?\n- Using only GPT-4o-mini as the evaluator seems insufficient. The ASR could be inflated since GPT-4o-mini is not a highly capable model. Have you verified the alignment between GPT-4o-based evaluations and human annotations?\n- Dataset selection: Why are jailbreak results reported on HarmBench, while refusal direction results are on JailbreakBench? This design choice is unclear.\n- You mention a “Seduction” pipeline for automated jailbreaks, but it is not described anywhere. It is also unclear which auxiliary LLMs were used to generate the attacks.\n- Tables 4 and 5: Why was DeepSeek Judge used for ablation and substring matching for addition? I understand that substring matching may be the simplest approach to detect refusals, but the evaluation should be standardized across experiments.\n\n**3. Incorrect or missing citations:**\n- Several important citations in the introduction are incorrect, including: HarmBench, Mousetrap, AutoRAN (L38-39). The author names are completely different from the actual works. \n- Section 3 seems to be mainly prior work, see https://arxiv.org/abs/2502.12025. \n\n**4. Writing issues:**\n- Inconsistent usage of \\citep and \\citet in related work first paragraph\n- L170: textbfNatural\n- L234-243: the two paragraphs seem to be repeating each other\n- Table captions should be above the tables\n- L398: Missing figure link"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Flk8pDsvKc", "forum": "kQRjcBmFAD", "replyto": "kQRjcBmFAD", "signatures": ["ICLR.cc/2026/Conference/Submission24867/Reviewer_GSVX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24867/Reviewer_GSVX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975877683, "cdate": 1761975877683, "tmdate": 1762943226691, "mdate": 1762943226691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}