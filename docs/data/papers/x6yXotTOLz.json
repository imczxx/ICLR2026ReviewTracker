{"id": "x6yXotTOLz", "number": 15165, "cdate": 1758248488978, "mdate": 1759897323934, "content": {"title": "Star-DS: Step-level Uncertainty-Aware Reasoning Data Selection in Reinforcement Learning for LLM Multi-step Reasoning", "abstract": "Large language models have demonstrated remarkable potential on complex multi-step reasoning tasks, largely enabled by substantial post-training via reinforcement learning with process reward verification on reasoning datasets. Recent studies have shown that it is possible to alleviate the massive data reliance and computational costs by selecting high-value subsets of data while maintaining reasoning capability. However, existing data selection methods typically rely only on outcome-level signals derived from final answers to measure data quality, overlooking step-level signals that are intrinsic to multi-step reasoning, which leads to suboptimal identification of valuable reasoning data. In this paper, we propose a novel Step-level Uncertainty-Aware Reasoning Data Selection approach (Star-DS) that incorporates both step-level and outcome-level signals for identifying high-value reasoning data in reinforcement learning for LLM multi-step reasoning. Specifically, we introduce step-wise self-evaluation uncertainty of each reasoning step, as well as reward variance of the final answer, to quantify the value of each sample for RL training. Experiments with diverse reasoning models across multiple benchmarks demonstrate that our approach consistently identifies high-value data, preserves multi-step reasoning performance after RL training, and significantly reduces both data requirements and computational costs.", "tldr": "", "keywords": ["Reasoning LLMs", "Data Selection", "Uncertainty Estimation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8ce79878250fed8c1f0bfb2767fe1c84c4a139c.pdf", "supplementary_material": "/attachment/79b156ce9d4773f454118c72ae02c75911042e4f.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes STAR-DS, a data-selection method targeting multi-step reasoning RL for large language models. It argues that existing methods focus only on outcome-level signals (final answer correctness) and ignore step-level uncertainties in the reasoning chain. STAR-DS computes a composite score combining (a) step-wise self-evaluation uncertainty and (b) outcome-level reward variability. It then uses the top-K highest-uncertainty samples to fine-tune via RL (e.g., GRPO) on multi-step reasoning benchmarks, showing that using far fewer training examples selected by STAR-DS can match or exceed full-dataset training while reducing compute/data cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Good motivation to tackle large RL training cost for multi-step reasoning tasks.\n\n2. Experiments show efficiency gains: with only ~1,000 selected examples they achieve near/full dataset performance."}, "weaknesses": {"value": "1. Outcome-level uncertainty or question difficulty has already been widely used for prompt selection and filtering in prior work ([1] [2]). This work mainly adds step-level uncertainty on top of previous idea, but at the cost of significantly increased computational overhead and system complexity. This raises questions about whether the additional signal justifies the extra cost.\n\n2. Limited gains from step-level uncertainty: As shown in Table 2, the stepwise uncertainty signal alone performs worse than reward variability, and even when combined, the average gain is only around 0.5 points. Considering the added complexity, the marginal benefit seems limited.\n\n3. Incremental improvement over simpler baselines:\nWhen compared to much simpler baselines such as LIM or IFD, the performance improvement is small. For example, as shown in Table 6 (100-sample setting), XRPO performs close to LIM/IFD, suggesting that the added step-level uncertainty may not yield a clear advantage in practice.\n\n4. Limited model scale:\nAll experiments are conducted only on 1.5B models. The generality and scalability of the approach on larger models remain unclear.\n\n[1] Process Reinforcement through Implicit Rewards\n[2] DAPO: An Open-Source LLM Reinforcement Learning System at Scale"}, "questions": {"value": "Please see above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yyOWY5rBkC", "forum": "x6yXotTOLz", "replyto": "x6yXotTOLz", "signatures": ["ICLR.cc/2026/Conference/Submission15165/Reviewer_4F6B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15165/Reviewer_4F6B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649846309, "cdate": 1761649846309, "tmdate": 1762925476918, "mdate": 1762925476918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel Step-level Uncertainty-Aware Reasoning Data Selection approach (Star-DS) that incorporates both step-level and outcome-level signals for identifying high-value reasoning data in reinforcement learning for LLM reasoning. Specifically, the authors introduce step-wise self-evaluation uncertainty of each reasoning step, as well as reward variance of the final answer, to quantify the value of each sample. Experiments show Star-DS consistently identifies high-value data, significantly reduces both data requirements and computational costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well structured and easy to follow.\n\n2. This paper integrates step-level self-evaluation uncertainty for data selection, overcoming the shortcomings of previous approaches that rely solely on outcomes.\n\n3. The results outperform previous selection methods like LIM, which demonstrate the effectiveness of proposed process signals."}, "weaknesses": {"value": "### Method\n- The authors adopt an offline data selection strategy before RL training, which may not adapt to the dynamics of learning. It's uncertain whether the initially selected data will retain the high value as the model is updated.\n\n- The authors adopt self-evaluation to calculate sample-level uncertainty. But I notice that Qwen2.5-Math-1.5B is used in practice. It's not clear whether a robust judgement of the correctness of responses can be made. I suggest providing comparisons with larger, advanced models to validate whether there are significant differences in this metric.\n\n### Experiments\n- The model is limited to the Qwen2.5 series. While the Qwen2.5 series is well-pretrained to provide a solid foundation in post-training, it also raises concerns on data contamination in widely used benchmarks [1]. Consequently, breakthroughs are predominantly observed for the mathematically strong Qwen2.5 series on benchmarks such as MATH-500, AMC, and AIME, and seldom transfer to models like Llama. I believe a more in-depth investigation on other model families (e.g., Llama) is needed to validate the effectiveness of Star-DS.\n\n- Concerns on computational efficiency. The authors briefly discuss the computational costs in Section 4.2. It seems that it's more costly compared to the full dataset when the training epoch is less than 10. I suggest providing more discussion on the computational efficiency regarding other baselines, such as GRESO, LIM, etc. \n\n---\n[1] Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination. arXiv preprint arXiv:2507.10532"}, "questions": {"value": "1. 1-shot RLVR [1] uses only *1 training example* and is effective in incentivizing the reasoning capabilities of LLMs. It sustained test performance improvement even after the training accuracy has saturated. I noticed that Star-DS selects *1,000 example* for training. How does its performance compare to 1-shot-RLVR?\n\n2. Why choose the pattern of \"LLM-as-a-Judge\" to calculate the uncertainty? Why not use the confidence inside the models as metrics?\n\n3. Methods like GRESO and LIM consider learning dynamics, and GRESO dynamically selects data during training. The proposed method uses offline selection. Would online data selection that combines training dynamics result in better performance?\n\n4. I notice that Star-DS outperforms LIM by only using outcome reward variability in Tables 1,2. Since LIM not only uses outcome reward but also considers training dynamics, the even worse results are very confusing to me. Can the authors provide an explanation? Why does incorporating training dynamics reduce performance?\n\n---\n\n[1] Reinforcement Learning for Reasoning in Large Language Models with One Training Example. Wang et al., 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zwueEXHqGo", "forum": "x6yXotTOLz", "replyto": "x6yXotTOLz", "signatures": ["ICLR.cc/2026/Conference/Submission15165/Reviewer_6WcF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15165/Reviewer_6WcF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085836741, "cdate": 1762085836741, "tmdate": 1762925476524, "mdate": 1762925476524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data selection method for LLM reasoning RL, by measuring uncertainty from two complementary perspectives: step-wise uncertainty, which captures instability within intermediate reasoning steps, and reward variability, which quantifies divergence across final outcomes."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The studied problem, i.e., using as few data as possible for LLM reasoning training, is important."}, "weaknesses": {"value": "1. The uncertainty quantification procedure requires a self-evaluation step. It might be inaccurate if the model is not trained to do self-rewarding. There are also no ablation studies on the accuracy of self-evaluation and its impact on the RL training.\n2. Besides, the benefit of the proposed method can be attributed solely to the Reward Variability term, which essentially removes the data that is all correct or all wrong. GRPO will give no gradient for such data, which may lead to slower convergence and lower performance compared to the proposed method. However, the proposed method does not save much computation since rollouts are still needed for each prompt before dropping these invalid samples.\n3. The experiments also cannot fully support the conclusion. Only a small size (1.5B) Qwen model is evaluated, which is not enough to show the effectiveness of the method. And the results are pass@1. For AIME and AMC, where the number of problems is few, pass@1 might be a noisy metric, compared to pass@N or avg@N.\n4. The setting is also somewhat weird. The experiments adopt GRPO for 200 epochs on 1,000-example subsets of MATH. It is not a regular setup since too few samples and too many epochs are performed, which can lead to overfitting and the learning of only format during GRPO."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bfCTB2GE7a", "forum": "x6yXotTOLz", "replyto": "x6yXotTOLz", "signatures": ["ICLR.cc/2026/Conference/Submission15165/Reviewer_e7xC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15165/Reviewer_e7xC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117370180, "cdate": 1762117370180, "tmdate": 1762925476109, "mdate": 1762925476109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Star-DS, a data selection method when doing RL on reasoning LMs. For each training instance, the method combines two signals to select rollouts: a step-level uncertainty score derived from the model’s own evaluation (prompting itself) is combined together with outcome rewards. After normalizing these components, their sum is used to rank and select the top-K samples for training. \nThe authors evaluate this approach with small Qwen2.5-Math-1.5B and a distilled qwen on math benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is easy to read. \n2. Using step-level signals is a reasonable direction given that multi-step reasoning contains rich internal structure. The combination of step-wise uncertainty and reward variability is simple but intuitive."}, "weaknesses": {"value": "1. The approach leans heavily on self-evaluation prompts, which are known to be unstable and sometimes misleading. A model judging its own steps can blur the line between actual reasoning quality and self-consistency bias, so the “uncertainty” signal may not reflect genuine difficulty.\n\n2. Uncertainty calibration is questionable. \n\n3. The final 'sum' scoring is quite ad-hoc: normalize two unrelated numbers and add them. There is no clear justification for why this linear combination should correspond to training value. \n\n4. While the experiments are broad, most gains are small (often within 1–2%). These deltas are within typical random noise for math-reasoning evaluations.\n\n5. empirically: settings and models are very limited. Using Qwen3 for training on a few math datasets is questionable. two groups of experiments, each using only one different backbone.\n\n6. The cost of computing step-level uncertainty is also significant. you need to account for generating multiple rollouts and running step-wise evaluations."}, "questions": {"value": "Does Star-DS actually identify samples that help the model learn, or does it simply prefer prompts that trigger inconsistent rollouts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jYJIWFOLIU", "forum": "x6yXotTOLz", "replyto": "x6yXotTOLz", "signatures": ["ICLR.cc/2026/Conference/Submission15165/Reviewer_MqK2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15165/Reviewer_MqK2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762814080587, "cdate": 1762814080587, "tmdate": 1762925475720, "mdate": 1762925475720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}