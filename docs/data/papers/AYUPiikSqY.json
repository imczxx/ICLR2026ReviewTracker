{"id": "AYUPiikSqY", "number": 11875, "cdate": 1758204430081, "mdate": 1759897549337, "content": {"title": "Geometric Analysis of Token Selection in Multi-Head Attention", "abstract": "We present a geometric framework for analyzing multi-head attention in large language models (LLMs). \nInstead of aggregating over all tokens, we propose a top-$N$ selection mechanism that retains only the most attended tokens and study its behavior directly in the value-state space. \nWe introduce novel geometric metrics -- Precision, Recall, and F-score -- to quantify the separability of selected versus non-selected tokens, and derive dimension- and margin-dependent bounds under empirically motivated assumptions on norm stability, similarity decay, and multi-phase attention distributions. \nOur theoretical results clarify how head specialization, sequence length, and the sink token jointly shape the geometry of attention. \nEmpirical evaluation on several open-source LLMs (LLaMA-2-7B, Gemma-7B, and Mistral-7B) confirms our predictions: top-$N$ selection sharpens token separability, the sink token systematically correlates with Recall, and different heads specialize into local versus global regimes. \nThese findings demonstrate that attention is not only a weighting mechanism but also a structured geometric classifier. \nOur framework provides measurable criteria for token selection, offers interpretability into head-level behavior, and opens new directions for designing sparse and geometry-aware attention mechanisms in LLMs.", "tldr": "", "keywords": ["LLM", "separability", "explainability", "context increasing", "geometry"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b48f394a0de39b4a41e2c3fc3054004f68f6dd4c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a geometric interpretation of self-attention by viewing it as a separator/classifier operating in the value space. The authors propose to evaluate each attention head’s behavior through precision, recall, and F-score metrics that quantify separability between the selected (top-N) and non-selected value tokens. Under a set of assumptions about value-vector norms, similarity decay, and attention profiles, they analytically predict that attention heads operate in a small-N regime where separability is maximized. Empirically, they identify three broad types of heads (Retriever, Mixer, Reset) and show how their prevalence varies with model depth."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The theoretical framing is novel and gives an interesting geometric perspective on attention, with clear analytical predictions that can be empirically checked.\n- The observation that attention sinks are not mere no-ops, but instead play an active role (especially in Recall and Reset-type heads), is particularly interesting and could motivate deeper investigation into sink dynamics and normalization mechanisms in large models."}, "weaknesses": {"value": "- The paper is not particularly well presented. Key motivations are unclear: the authors do not sufficiently explain why attention should be modeled as a classifier or why geometric separability in value space is the right lens for interpretability. The classification framework feels somewhat imposed rather than naturally derived from prior literature or empirical necessity.\n- Several important concepts (e.g. the “MAE” mentioned in the text) are never properly defined or justified in the context of their framework.\n- The paper does not make a compelling case for why separability, precision, or recall are meaningful or diagnostic of downstream behavior, model efficiency, or interpretability.\n- Certain claims (such as an “oscillatory regime” across positions 100–800, or “semantic cycles”) are presented without quantitative backing or follow-up experiments. These explanations offered without proof weaken the empirical credibility of the work.\n- The experiments are limited to a small set of 7B-parameter models and do not explore robustness across architectures, tasks, or long-context settings. Given the theoretical emphasis, a more comprehensive experimental grounding is needed.\n- Given the current level of analysis and the modest experimental depth, the paper would be better suited for a workshop rather than a full-conference publication."}, "questions": {"value": "- In Barbero et al., 2025, the authors explicitly discuss the formation of sharp heads through the influence of attention sinks. Do the authors believe that this mechanism might be related to the findings in the paper? Hows is the top-N selection scheme related to head sharpness?\n- How are separability metrics expected to evolve in long-context settings?\n\nBarbero, Federico, et al. \"Why do LLMs attend to the first token?.\" arXiv preprint arXiv:2504.02732 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kHZQIk0kuu", "forum": "AYUPiikSqY", "replyto": "AYUPiikSqY", "signatures": ["ICLR.cc/2026/Conference/Submission11875/Reviewer_uLa2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11875/Reviewer_uLa2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761289515437, "cdate": 1761289515437, "tmdate": 1762922892392, "mdate": 1762922892392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to understand multi-head attention through a novel view of token selection. The authors provide a geometric analysis based on separability between selected and non-selected tokens. To conduct this, they provide a theoretical framework: first make assumptions on the norm states, token similarities, attention dynamics, and then derive the bounds for precision/recall/F1-score of separability of selected/non-selected tokens. Finally, based on the built geometric framework, the authors aim to interpret the head functions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide a novel view of multi-head attention with nice theoretical support. Using the geometric perspective, they interpret the head functions in multi-head attention. The findings on value norms (assumption 1) are interesting."}, "weaknesses": {"value": "My major concerns towards this paper are about the validity of several assumptions in this paper, and the possible actionable suggestions on the LLM/attention community.\n\n1. About assumption 2, I am a little bit concerned whether it makes sense to use an exponential function (between 0 and 1) to model a cosine similarity (ranged from -1 to 1). Although the authors show that the MAE error is very small, I am wondering whether it makes sense to assume that the cosine similarity could be always non-negative. I am looking forward to more empirical evidence.\n\n2. About assumption 3, the attention dynamics in Figure 4 follow my intuition that the first token is an attention sink and the recent tokens typically have higher attention. However, such a simplification may ignore existence of induction heads [1], or some other sink tokens in middle context [2]. And it does not make sense to me that all heads/layers follow such a pattern, especially considering the authors fail to show some fitting errors like in assumption 2.\n\n3. When we discuss about multi-head attention, normally we are considering attention as a matrix. However, here it seems that authors mainly discuss the attention weights on the final token. I am wondering whether the conclusions are still valid when $L$ is largely different. \n\n4. Although the authors claim that the LLaMA, Gemma, Mistral follow the theoretical analysis on bounds, I suggest to show that the previous assumptions also hold in these models or different sizes (other than 7B), or different data domains. \n\n5. I acknowledge that the theoretical framework in this paper is elegant, please clarify how this research can provide actionable suggestions/insights to the LLM/attention community.  \n\nSome minors:\n\n1. Please clarify the model/data used in assumption stage.\n\n2. The section 3.2 is a little bit messy as the authors should first claim the assumption and then show the MAE values. The current presentation may make the readers confused at first impression.\n\n3. Please use notations to represent attention mass to prevent confusion.\n\nI am glad to increase my score if my concerns are alleviated during the rebuttal stage. \n\nReferences:\\\n[1] Anthropic. In-context Learning and Induction Heads. 2022.\\\n[2] Sun et al. Massive Activations in Large Language Models. COLM 2024."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EZlAailngf", "forum": "AYUPiikSqY", "replyto": "AYUPiikSqY", "signatures": ["ICLR.cc/2026/Conference/Submission11875/Reviewer_9XZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11875/Reviewer_9XZ2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892946792, "cdate": 1761892946792, "tmdate": 1762922892019, "mdate": 1762922892019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper takes a geometric look at how multi-head attention selects tokens.\nThe authors treat attention as a kind of classifier that operates in the value–state space and define geometric versions of Precision, Recall, and F-score to describe how well selected tokens are separated from the rest.\nThey assume three main conditions: (1) stable value norms and reduced sink activity, (2) exponential decay of cross-token similarity, and (3) a piecewise attention weight pattern that captures plateau–oscillation–recency behavior.\nUnder these assumptions, they derive non-asymptotic bounds on token separability and show that the best separation appears when only a few tokens (around 1–4) are selected.\nExperiments on LLaMA-2-7B, Gemma-7B, and Mistral-7B back up these results and reveal three types of attention heads (Retriever, Mixer, and Reset) that differ in how they interact with the sink and final tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The geometric framing is clear and easy to follow, giving an intuitive picture of how attention works.\n2) The theory lines up closely with the empirical data across different models.\n3) The head taxonomy (Retriever, Mixer, Reset) offers a concrete and interpretable way to describe functional differences between heads.\n4) The paper doesn’t rely on extra training or architectural changes, making the analysis generally applicable."}, "weaknesses": {"value": "(1) Limited practical connection.\nThe paper gives a clean geometric description of token selection and supports it with strong empirical evidence.\nHowever, it stops short of connecting these findings to model performance or design improvements.\nThe results are insightful but remain mainly diagnostic, without demonstrating benefits such as better alignment, loss reduction, or architectural efficiency.\n\n(2) Assumption sensitivity.\nThe theoretical derivations rely on several empirical assumptions — stable value norms, exponential similarity decay, and piecewise attention profiles.\nThese assumptions are plausible but not formally justified, and the paper does not examine cases where they fail (e.g., fine-tuned models, longer contexts, or high-variance heads).\nThe robustness of the geometric bounds under such conditions remains unclear.\n\n(3) Missing broader comparison.\nThe analysis focuses entirely on top-N token selection and does not compare against more continuous or weighted formulations of attention.\nAs a result, it is uncertain whether the observed separability patterns are specific to discrete selection or general to the full attention mechanism."}, "questions": {"value": "(1) Can the proposed geometric Precision/Recall or F-score metrics be linked to downstream performance (e.g., loss, perplexity, or alignment quality)?\n\n(2) How sensitive are the theoretical bounds to violations of the key assumptions (norm stability, similarity decay, or piecewise attention profiles)?\n\n(3) Could the geometric interpretation be extended into token pruning or head sparsification methods in practice?\n\n(4) Do the Retriever, Mixer, and Reset head types appear consistently across different model sizes, architectures, or data domains?\n\n(5) Both this paper and Orthorank(Shin et al., 2025) report a similar phenomenon, stable norms except for the sink token. Are these two observations fundamentally related, or do they arise independently in different representational spaces (hidden vs. value)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rNTLJQc53B", "forum": "AYUPiikSqY", "replyto": "AYUPiikSqY", "signatures": ["ICLR.cc/2026/Conference/Submission11875/Reviewer_Ej41"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11875/Reviewer_Ej41"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990440103, "cdate": 1761990440103, "tmdate": 1762922891607, "mdate": 1762922891607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}