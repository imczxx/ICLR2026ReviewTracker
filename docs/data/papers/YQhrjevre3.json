{"id": "YQhrjevre3", "number": 19974, "cdate": 1758301125225, "mdate": 1759897009308, "content": {"title": "GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering", "abstract": "Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Existing defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82\\% selective refusal rate, substantially outperforming standard SAE steering (42\\%), while maintaining strong task accuracy (70\\% on TriviaQA, 65\\% on TruthfulQA, 74\\% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining $\\geq$90\\% refusal of harmful content.", "tldr": "We propose Graph-Regularized Sparse Autoencoders (GSAE), which use Laplacian smoothness on neuron co-activation graphs to learn distributed safety features, achieving higher selective refusal, ≥90% jailbreak robustness, and strong QA accuracy.", "keywords": ["AI Safety", "Representation Learning", "Sparse Autoencoders", "Activation Steering", "Graph Regularization", "Jailbreaking Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5944082eda05ad161e86d657ab5ce056979dae8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Graph-Regularized Sparse Autoencoders (GSAEs), which extend standard SAEs with a Laplacian smoothness regularizer to capture distributed safety representations in LLMs. GSAE enables adaptive runtime safety steering through a dual-gating controller, achieving high refusal robustness while preserving task utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-Motivated Problem.** The paper is built on the strong and timely hypothesis that abstract concepts like safety are fundamentally distributed. This provides a principled explanation for why standard SAEs, which are optimized for monosemanticity, may be ill-suited for this particular control task.\n2. **Principled Methodology (GSAE).** This paper introduces a new application of a graph Laplacian smoothness prior to the SAE's decoder weights ($W^{(d)}$). This adaptation of a technique from spectral graph theory provides a principled way to balance the L1 penalty's sparsity with the need to capture distributed representations via the graph regularizer.\n2. **Comprehensive Steering Framework.** The paper suggests new three-criteria filtering process for building the Spectral Vector Bank ($s^{lap}$, $s^{imp}$, $s^{infl}$) and the dual-gating controller with hysteresis constitute a robust and well-designed system for applying the learned features."}, "weaknesses": {"value": "1. **Compositional Novelty.** The paper's core contribution is a novel integration of existing techniques (Sparse Autoencoders, Laplacian regularization, and gating) rather than the invention of a fundamentally new algorithm. While this composition is highly effective and achieves state-of-the-art results, the methodological leap itself could be viewed as incremental.\n2. **Hyperparameter Complexity.** The full framework introduces a large number of new hyperparameters, including $\\lambda_{graph}$, the graph threshold $\\tau$, the spectral bank weights $(\\alpha, \\beta, \\gamma)$, and four separate gating thresholds $(t_{lo}, t_{hi}, d_{lo}, d_{hi})$. While Appendix D.1 provides a sensitivity analysis for some parameters, the overall system appears complex and potentially difficult to tune.\n3. **Training Cost and Scalability.** The paper states the runtime overhead is \"moderate\", but this analysis focuses on inference. The training objective involves a term $\\sum_{j=1}^{k}((W^{(d)}_j)^TLW^{(d)}_j)$, where $L$ is a $d \\times d$ matrix and $k$ is the number of features ($k \\gg d$). This calculation, naively $O(k \\cdot d^2)$, could become a significant computational bottleneck as the model's hidden dimension ($d$) scales, potentially hindering the application of GSAE to very large (e.g., 70B+) models. A clearer analysis of this training complexity is missing."}, "questions": {"value": "1. Could the authors elaborate on the training complexity of the Laplacian regularization term? What is the practical impact of the $O(k \\cdot d^2)$ computation on wall-clock training time, and do the authors foresee this as a fundamental barrier to scaling GSAE to models with much larger hidden dimensions?\n2. The utility results are all reported as integers without decimal precision. Were these benchmarks evaluated on partial subsets (e.g., ~100 samples) rather than the full test sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WB8cPOde2i", "forum": "YQhrjevre3", "replyto": "YQhrjevre3", "signatures": ["ICLR.cc/2026/Conference/Submission19974/Reviewer_efzh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19974/Reviewer_efzh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760771009136, "cdate": 1760771009136, "tmdate": 1762932851994, "mdate": 1762932851994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Graph-Regularized Sparse Autoencoders (GSAEs) for unsupervised representation learning of hidden states of LLMs. With the learned representation, this paper proposes a threshold-based detection method with a representation steering method. The experimental results show that the proposed method is emprically strong in terms of both safety and utility across four different backbones."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method appears to be unsupervised (though it may use labels for tuning the thresholds $t_\\text{hi}, t_\\text{lo}$).\n\n- The experimental results are strong and well-support the claims."}, "weaknesses": {"value": "- The paper lacks clarity in several aspects:\n  - The pooled representation is denoted as $\\bar{h}^{(l)}$ or $H$, but is referred to as $x$ in L256–258.\n  - In L289–296, could the authors clarify how $s_i^\\text{lap}$ and $s_i^\\text{infl}$ are computed?\n  - In L310–315, thresholds $t_\\text{hi}$ and $t_\\text{lo}$ are said to be selected via a “systematic sensitivity analysis.” Is this analysis conducted using training or test labels?\n  - In L311, what is the function $g$ used to compute $p_\\text{harm}$? Is it an MLP, and if so, how is it trained?\n  - In L316, what is the “continuation gate”? If it is a function, what are its inputs and outputs (e.g., $\\gamma_t$)?\n  - In L318, the continuation gate outputs a steering multiplier $\\gamma_t$ based on real-time risk. More detail is needed (e.g., what does $t$ represent—token index?).\n\n- Section 4.3 proposes several techniques for using the learned representation. It would improve clarity to summarize them in an algorithm table.\n\n- The method seems to introduce additional computational overhead due to the processes described in Section 4.3. Could the authors clarify the computational cost or latency.\n\n- In L319, the continuation gate monitors generation token by token. Is this compatible with real-world deployment systems such as vLLM or sglang?\n\n- Section 5.1 lacks training details for GSAEs, including the training dataset, learning rate, and batch size.\n\n- One of the major contributions of this paper (as stated in L98–102) is the graph Laplacian regularization. Could the authors provide an ablation study to evaluate its effect in the experiments?"}, "questions": {"value": "see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5jtz1g3CdL", "forum": "YQhrjevre3", "replyto": "YQhrjevre3", "signatures": ["ICLR.cc/2026/Conference/Submission19974/Reviewer_4pHB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19974/Reviewer_4pHB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652406406, "cdate": 1761652406406, "tmdate": 1762932838338, "mdate": 1762932838338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GSAE, a Graph-Regularized Sparse Autoencoder that models “safety” as a distributed and smooth graph signal in the neuron space of LLMs. By introducing a Laplacian regularization term into the SAE framework, the authors aim to learn coherent, distributed safety directions that can be used for runtime steering. Experiments across multiple models and jailbreak attacks demonstrate improved selective refusal and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tFormulates safety representation learning as a graph-signal smoothness problem, integrating Graph Laplacian regularization into the SAE framework, is conceptually clear and technically novel.  \n2.\tProvides broad empirical validation across multiple models and attack types, comparing with several strong baselines."}, "weaknesses": {"value": "1.\tDoes not explicitly model or measure cumulative drift caused by multi-layer steering, leaving the potential interaction between layers unaddressed.  \n2.\tThe three-stage feature selection pipeline is largely heuristic, requires multiple sub-trainings and hyperparameter tuning, which may hinder reproducibility.  \n3.\tThe Safety–Utility evaluation remains coarse-grained, lacking fine-grained analysis of false refusals or real dialogue impact."}, "questions": {"value": "1.\tHow stable is the steering process when applied across multiple layers or iterations? Could cumulative drift degrade alignment or helpfulness?\n2.\tCould the authors provide qualitative examples to illustrate the Safety–Utility balance in real dialogue settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0jYL2CAA7Q", "forum": "YQhrjevre3", "replyto": "YQhrjevre3", "signatures": ["ICLR.cc/2026/Conference/Submission19974/Reviewer_Df8T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19974/Reviewer_Df8T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822701808, "cdate": 1761822701808, "tmdate": 1762932797807, "mdate": 1762932797807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Graph-Regularized Sparse Autoencoders (GSAE) for LLM safety steering. The key idea is that safety concepts are distributed across multiple features rather than localized to single dimensions. GSAE extends standard SAEs with Laplacian smoothness penalties on neuron co-activation graphs, producing coherent distributed representations. A spectral vector bank and dual-gating controller enable runtime steering, achieving 82% selective refusal (vs 42% for standard SAE) while maintaining reasonable utility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novel approach: The application of graph Laplacian regularization to SAEs for safety is creative. The insight that safety is distributed rather than monosemantic is well-articulated and supported by recent literature.\n\nComprehensive empirical evaluation: Authors conducted tests across multiple model families (LLaMA-3, Mistral, Qwen, Phi), and evaluate against diverse jailbreak attacks (GCG, AutoDAN, TAP). The evaluation includes both safety and utility benchmarks"}, "weaknesses": {"value": "**Major Weaknesses**\n\n1. Insufficient Evidence for Claims\n\nThe paper's central hypothesis—that safety requires distributed representations—lacks direct empirical support:\n\n* Figure 4 shows overlapping spectral projections but doesn't definitively prove that distributed representations are necessary for safety.\n* The evidence comes from analogy to temporal/refusal behavior studies, not direct investigation of safety concepts.\n* An experiment comparing GSAE against methods that explicitly enforce single-direction safety would strengthen this claim.\n\nI would like to see experiments showing that single-direction methods fundamentally fail where distributed methods succeed, beyond just performance metrics.\n\n2. Complexity and Lack of Simplicity\n\nThe method combines too many components, making it difficult to access what causes/drives good performances. I see concepts like GSAE regularization, spectral vector bank with three weighted criteria, dual-gating controller, calibrated random forest classifier, etc.  It makes the paper very difficult to read.\n\n3. Graph construction:\n\nI don't see analysis of graph stability across datasets or model checkpoints. I don't even see experimental details on how GSAE are trained, what is the dataset, how are examples sampled? This has to be well clarified. \n\n4. Incomplete baseline comparisons\n\nThere are missing comparisons to important methods:\n \n* Fine-tuning approaches: No comparison to RLHF, DPO, or safety fine-tuning\n* Recent SAE methods: Missing comparison to other recent SAE architectures or training objectives\n* Circuit-based methods: No discussion of mechanistic interpretability approaches (Gradient Cuff, RFA (refusal feature adversarial training)\n\nWe need comparison with respect to these methods to evaluate the strength of the contribution from an empirical view.\n\n\nThe comparison is primarily against CAA (2023) and SafeSwitch (2025), but the safety steering landscape has evolved rapidly.\n\n\n**Minor Issues**\n\nLine 53: \"it fragmented\" → \"it can be fragmented\"\n\nInconsistent reference formatting (full names vs initials)\n\n\"WildJailbreak (Xia & et al., 2024)\" should be \"Xia et al.\"\n\nAppendix placement could be optimized—key ablations should be in main text"}, "questions": {"value": "1. How does performance scale with dictionary size k? (current experiments use k ≫ d but specific value unstated)\n\n2. Can you provide feature visualizations comparing SAE vs GSAE learned features?\n\n3. What is the Pearson correlation between the three scoring criteria (s^lap, s^imp, s^infl)?\n\n4. How does GSAE perform when graph is constructed on adversarially perturbed prompts?\n\n5. Have you tested this on models trained with safety fine-tuning (e.g., Llama-2-chat vs Llama-2-base)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WRNADKXSSs", "forum": "YQhrjevre3", "replyto": "YQhrjevre3", "signatures": ["ICLR.cc/2026/Conference/Submission19974/Reviewer_7Ms9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19974/Reviewer_7Ms9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973515533, "cdate": 1761973515533, "tmdate": 1762932772975, "mdate": 1762932772975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}