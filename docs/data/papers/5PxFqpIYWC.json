{"id": "5PxFqpIYWC", "number": 5926, "cdate": 1757946776257, "mdate": 1763586653255, "content": {"title": "Scaling Generalist Data-Analytic Agents", "abstract": "Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.", "tldr": "This paper introduces DataMind, a scalable data synthesis and agent training pipeline designed to build generalist data-analytic agents.", "keywords": ["Data Analysis", "LLM Agents", "Agent Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1d3164fe3fd14774e2746e919ed69f5d923ae99.pdf", "supplementary_material": "/attachment/9790158dc9cef819d4eead1463aa2294a128de3a.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DATAMIND, a framework for training open-source data-analytic agents. The paper tackles three major challenges:\n\n- **Insufficient data resources:** proposes a fine-grained task taxonomy and a recursive query synthesis pipeline to generate diverse, multi-turn analytic problems, coupled with a knowledge-augmented, self-consistency-filtered data curation process that produces the DATAMIND-12K dataset.\n\n- **Training strategy:** designs a hybrid SFT + RL objective with dynamic weighting to balance imitation and exploration during training.\n\n- **Unstable multi-turn rollouts:** develops a memory-efficient rollout mechanism with asynchronous interaction and chunked code management to improve reliability in multi-turn, code-based roll-outs.\n\nFine-tuned with DATAMIND, a ReAct-style agent with Qwen-2.5-Coder-14B backbone achieves state-of-the-art results (average pass@1 = 71.16 across three benchmarks including DABench, TableBench, and BIRD), outperforming both open-source and proprietary baseline models including DeepSeek-V3.1 and GPT-5."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive framework:** the paper proposes a unified framework that integrates training data synthesis and SFT + RL fine-tuning for training open-source data-analytic agents, offering a reproducible recipe for scaling open-source data-analytic models.\n\n2. **Strong empirical performance:** with DATAMIND, Qwen-2.5-Coder-14B model outperforms all baseline proprietary and open-source models, while Qwen-2.5-Coder-7B model achieves performance comparable to GPT-5.\n\n3. **Comprehensive analyses:** the paper provides insightful ablation studies that offer practical understanding in curating high-quality training data and stabilizing the training procedure."}, "weaknesses": {"value": "1. Since part of the training data is sourced from Kaggle, is there a possibility that the training data may overlap with public benchmarks used for evaluation? Some discussions or analysis to rule out such possibility would further enhance the validity of the results.\n\n2. All expert trajectories are sampled from DeepSeek-V3.1, and GPT-4o-mini serves as the judge during both trajectory filtering and reward modeling. This raises concerns about introducing potential biases from these two models, and some discussions on how to mitigate such biases would be very helpful.\n\n3. LLM-as-judge with GPT-4o-mini is used to evaluate the performance, although for some benchmarks objective metrics are available (e.g. Rouge-L for TableBench as described in Appendix D). This raises concerns about the reliability of the evaluation, especially when GPT-4o-mini is also used as the judge during training trajectory curation and reward modeling. Including objective metrics results would improve the robustness of the evaluation."}, "questions": {"value": "1. For Rule-based Trajectory Filtering, especially 1) Format compliance and 3) Linguistic integrity, were these checks also performed using LLM-as-judge, or implemented through e.g. regular expression?\n\n2. How is the format reward computed?\n\n3. In table 1, Qwen-2.5 Coder-14B fine-tuned with TableLLM and Table-R1 perform worse on TableBench than the pretrained Qwen-2.5 Coder-14B (ReAct). Is there any insight on why fine-tuning on TableInstruct led to worse performance?  \n\n4. Regarding Fig 6, my understanding is that the conclusion “RL can narrow the performance gap between different base models” is based on models with the same backbone but different in SFT epochs (e.g., Qwen-7B with 1 v.s. 2 epochs). Could this conclusion extend to base models of different sizes (e.g., Qwen-7B v.s, Qwen-14B)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AabCf839CQ", "forum": "5PxFqpIYWC", "replyto": "5PxFqpIYWC", "signatures": ["ICLR.cc/2026/Conference/Submission5926/Reviewer_xdfR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5926/Reviewer_xdfR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585642072, "cdate": 1761585642072, "tmdate": 1762918356790, "mdate": 1762918356790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DATAMIND, a data synthesis and training pipeline for building generalist data-analytic agents that operate through code-based multi-turn ReAct loops over heterogeneous files (.csv, .xlsx, .sqlite). The recipe includes: an 18-category task taxonomy with recursive easy-to-hard composition for query generation; knowledge-augmented trajectory sampling with self-consistency filtering and rule-based checks; a dynamically weighted SFT+RL objective (DAPO) with void-turn masking; and a memory-frugal rollout framework with sandboxed execution."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "End-to-end, scalable pipeline. The work integrates dataset harvesting, fine-grained taxonomy, compositional query generation, trajectory vetting, and stable multi-turn rollout, offering a complete path from raw files to trained agents suitable for community reuse.\n\nTraining design that addresses common failure modes. The dynamic SFT+RL weighting, void-turn masking, and asynchronous chunked execution directly target instability in code-based multi-turn RL and are supported by ablations on reward/entropy dynamics.\n\nStrong empirical results with broad format coverage. Consistent gains across three benchmarks and multiple file modalities, including conversion stress tests (e.g., TableBench/BIRD to unified formats), suggest robustness beyond narrow table QA."}, "weaknesses": {"value": "Evaluation and reward rely on model-as-judge. Using GPT-4o-mini for both training rewards and final evaluation introduces circularity and potential stylistic bias; limited exact-match or verifier-based checks reduce measurement rigor, especially for descriptive answers.\n\n\nPotential data overlap and protocol discrepancies. Training on BIRD/OmniSQL-adjacent corpora and converting evaluation benchmarks to new formats risk contamination or distribution shifts; fairness of comparisons may be affected by enforcing a single ReAct prompt across heterogeneous baselines.\n\n\nScope and generality claims are ahead of evidence. Results are confined to reasoning-oriented analytics; visualization, forecasting, and end-to-end scientific workflows are excluded. Claims of outperforming top proprietary models would benefit from additional third-party evaluators and held-out, verifier-checkable tasks."}, "questions": {"value": "Please specify total RL budget: number of episodes, average steps per episode, total generated tokens, and wall-clock/compute (GPU type, days). The paper mentions ∼350 RL steps; what exactly constitutes a “step” here?\n\nFor the dynamic weighting γ, provide the full schedule, per-step values, and an ablation where γ is learned or adapted from validation signals instead of cosine.\n\nHow do SFT and RL gradients interact on overlapping tokens when a trajectory passes the RL filter? Are environment-emitted tokens always masked in both losses, and is there any credit assignment to code vs. natural-language tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NzPx57ZZez", "forum": "5PxFqpIYWC", "replyto": "5PxFqpIYWC", "signatures": ["ICLR.cc/2026/Conference/Submission5926/Reviewer_dp72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5926/Reviewer_dp72"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770822169, "cdate": 1761770822169, "tmdate": 1762918355853, "mdate": 1762918355853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses three key challenges in developing data analysis agents:\n(i) the scarcity of training data resources,\n(ii) the instability of long-horizon agent training and the unclear trade-off between SFT and RL training stages, and\n(iii) the complexity of memory management in parallel agentic rollouts and multi-turn code generation.\n\nTo tackle these challenges, the authors propose:\n(i) constructing a specialized training corpus,\n(ii) dynamically combining SFT and RL losses with an adaptive weighting scheme, and\n(iii) introducing an asynchronous agent generation and code execution framework with a chunk-wise code maintenance strategy to reduce peak memory usage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of building open-source data-analytic agents is interesting and timely.\n2. The proposed system demonstrates promising results across multiple benchmarks."}, "weaknesses": {"value": "1. It is unclear whether the observed performance improvements primarily stem from the proposed training methodology or from the curated dataset. A clearer ablation isolating these factors would strengthen the claim.\n2. The benefit of dynamically combining SFT and RL remains ambiguous. From the rightmost subfigure in Figure 4, the training process appears to continuously shift from SFT to RL, although the paper does not follow an “SFT-first-then-RL” paradigm. It is not evident whether this dynamic approach yields tangible advantages over the “SFT-first-then-RL” paradigm. An ablation study is needed."}, "questions": {"value": "Given the results in Table 1, it seems there is a clear upper bound on the performance across all models on the multiple benchmarks listed in the table. Do the authors have any insights into why this could happen and how to further improve the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RGJ1UVmkAR", "forum": "5PxFqpIYWC", "replyto": "5PxFqpIYWC", "signatures": ["ICLR.cc/2026/Conference/Submission5926/Reviewer_H48n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5926/Reviewer_H48n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812043436, "cdate": 1761812043436, "tmdate": 1762918355078, "mdate": 1762918355078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pipeline for constructing generalist data-science agents, addressing key challenges such as data scarcity, unstable multi-turn rollout, and balancing RL and SFT training. The authors curate a 12K-sample dataset and train models on it. The resulting DATAMIND-7B/14B models show promising performance on several data-analytic benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated, and the proposed pipeline is comprehensive, combining a thoughtful data-synthesis process with a robust training strategy. The experimental results are promissing and the authors also analyze useful empirical insights into agent training."}, "weaknesses": {"value": "1. Missing baseline comparisons: The paper does not include comparisons with other data-analysis agents such as DS-Agent or Data-Interpreter, which would strengthen the empirical claims.\n2. Limited base model and benchmark diversity: The model is trained only on Qwen, which may introduce family bias and inflate the reported performance. Moreover, evaluation is restricted to three tabular/SQL-focused benchmarks; the generalization to broader data-analysis scenarios remains unclear.\n3. Additional studies on different base models, dataset sizes, and the impact of recursive composition depth would make the analysis more comprehensive."}, "questions": {"value": "1. Since gold answers are already available, why use a judge model for evaluation? Could this introduce noise, and how might that be mitigated?\n2. Can DATAMIND handle multimodal inputs (e.g., CSV + plots), or is it limited to text-based tabular data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Lxch7OgWJ2", "forum": "5PxFqpIYWC", "replyto": "5PxFqpIYWC", "signatures": ["ICLR.cc/2026/Conference/Submission5926/Reviewer_dfWg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5926/Reviewer_dfWg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880982053, "cdate": 1761880982053, "tmdate": 1762918354726, "mdate": 1762918354726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}