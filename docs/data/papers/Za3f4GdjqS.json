{"id": "Za3f4GdjqS", "number": 8782, "cdate": 1758098043359, "mdate": 1759897764284, "content": {"title": "Scale-Invariance in AI Representation Predicts AI-Brain Alignment", "abstract": "Understanding why some neural network representations align better with brain activity is essential for uncovering neural coding principles and developing human-like AI. While prior work has largely focused on model-level factors, such as dataset scale and task design, we focus on the rarely explored, yet more in-depth embedding level. Motivated by evidence that scale-invariance is widespread in biological neural systems, we identify it as a key embedding-level property. Analyzing 60 pretrained visual models and fMRI responses to natural images, we find that embeddings with stronger scale-invariance align better with fMRI. Training strategies modulate scale-invariance, with larger pretraining datasets enhancing it and fine-tuning reducing it, thereby affecting alignment performance. These findings establish scale-invariance as a fundamental embedding-level property that links training strategies to brain-like representations and suggest its potential as a guiding principle for designing more human-like AI.", "tldr": "", "keywords": ["AI-Brain Alignment", "fMRI", "intrinsic dimension", "pre-trained model"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d6788f8f14fe7c4c7656786a809a927d44cb0a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study explored how a specific factor, namely, scale-invariance, is correlated to the alignment between visual AI models and brain responses at embedding-level. The authors propose two metrics, multi-scale dimensional stability and multi-scale distributional similarity, to measure scale-invariance, and build linear regression model to assess the predictive relationship between scale-invariance and model-brain alignment score. The experimental results demonstrate that both metrics predict neural alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The authors have proposed two metrics to measure the scale-invariant property of embeddings in visual AI models.\n2.The authors have recruited 60 visual models to establish the predictive relationship between scale-invariant and model-brain alignment.\n3.The authors have demonstrated that larger pre-training datasets enhance scale invariance and consequently improve alignment, whereas fine-tuning reduces scale invariance, leading to weaker alignment."}, "weaknesses": {"value": "1.Computational and neuro-scientific justifications of the results are very limited. \n2.Justifications about model selection are missing. \nThe definition of “scale-invariance” is different from common understanding, potentially miss leading."}, "questions": {"value": "1.The author should provide interpretation of, at least discussion about, the AI models’ scale-invariance. Are the AI modes’ scale-invariance measured by the two metrics intuitively or quantitatively align with their architectures/characteristics? This point is quite important for the readers to assess the feasibility of the proposed metrics.   \n2.The predictive relationship between model-brain alignment and scale-invariance looks quite similar across brain regions. The question is: which brain regions encode scale-invariance predominantly, or scale-invariance is encoded in widely distributed brain regions? Is there any evidence in neuroscience or cognitive neuroscience that supports your results?\n3.Are their any reasons for AI model selection? Are those models theoretically engage scale-invariance in a wide range? The slops in Figure 4 and Figure 5 are very small. Does it mean the difference of scale-invariance across model is small? Meanwhile, such small slops may make the regression unstable, challenging the effectiveness of the significance level.\n4.The definition of scale-invariance in visual perception is somehow different from common understanding. Scale-invariance, in most cases, means that certain perceptual judgments, neural representations, or visual computations remain essentially the same at different physical sizes or viewing distances.\n5.How can the findings in this study help to develop brain-like AI? A short discussion may help the reader to understand the study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k0JVwX8eqT", "forum": "Za3f4GdjqS", "replyto": "Za3f4GdjqS", "signatures": ["ICLR.cc/2026/Conference/Submission8782/Reviewer_Eohs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8782/Reviewer_Eohs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835653834, "cdate": 1761835653834, "tmdate": 1762920557416, "mdate": 1762920557416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims that “scale invariance” of embeddings explains when model features align with fMRI, measured via multi-scale dimensional stability and a Gromov–Wasserstein multi-scale distributional similarity. On NSD with 60 vision models, stronger scale invariance predicts higher voxelwise R^2. Larger pretraining datasets raise it and alignment, while fine-tuning lowers both. The authors argue that these metrics add explanatory power beyond model size and could be used as regularizers for making more brain-like geometry."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel central hypothesis that raises an interesting point. I would say their work is original. If my concerns can be addressed, this work has good significance. \n- Great care is taken care into the analysis (except for the points raised in the weaknesses section below).\n- Clearly written."}, "weaknesses": {"value": "I am mainly confused as to why the authors resorted to quantifying the scale invariance with \"Multi-scale dimensional stability\" and \"Multi-scale distributional similarity.\" I do not believe they are a good measure of scale-invariance, since they cannot distinguish between trivial scale-invariance and more interesting scale-invariance. I am not quite convinced with the author's claim \"While effective for continuous data, these methods often struggle with discrete datasets and provide limited ways to quantify the degree of scale-invariance.\" in line 120. \n\nIn neuroscience, scale-invariance is typically characterized by the power-law statistics, as noted by the authors (the authors should also cite the landmark paper [1] on this topic). By power-law statistics, I am specifically referring to the power law in the eigenvalue spectrum of the covariance matrix of neural representations. The power-law characteristic is directly linked to a non-trivial fractal structure.\n\nThe problem with the slope analyses on \"Multi-scale dimensional stability\" and \"Multi-scale distributional similarity\" is that they will indicate that a simple uniform distribution is scale-invariant. It is technically true that a uniform distribution is scale-invariant, but that is like saying a straight line is a fractal curve (which is true, but not interesting). When we discuss scale-invariance in neuroscience, we are not referring to trivial scale-invariance like that. I think the authors are aware of that, as they use the conventional nontrivial fractal cartoon in Figure 1. \n\nMy intuition is that a Gaussian distribution would also result in small slopes in both the \"Multi-scale dimensional stability\" and \"Multi-scale distributional similarity\" analyses (but I could be wrong).\n\nThe paper currently requires a stronger justification of the claim in line 120. Also, I believe the paper's claim can be strengthened if they simply report the eigenvalue spectra of neural recording, fit the power-law curve to it, and report perhaps the fitted exponent against the alignment score and/or the slope score.\n\nI should reiterate that I find the hypothesis of this paper very intriguing, and it is something very much worth investigating. I just simply skeptical of the author's methodology.\n\n[1] Stringer et al. 2019 \"High-dimensional geometry of population responses in visual cortex\""}, "questions": {"value": "Typo in line 264: “perdormance”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iSoJpBlhZN", "forum": "Za3f4GdjqS", "replyto": "Za3f4GdjqS", "signatures": ["ICLR.cc/2026/Conference/Submission8782/Reviewer_UeYU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8782/Reviewer_UeYU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939659766, "cdate": 1761939659766, "tmdate": 1762920557064, "mdate": 1762920557064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies which factors drive the alignment between neural network representations and fMRI responses on the Natural Scenes dataset by analyzing over 60 different pretrained ConvNext models. Rather than correlating individual training factors with the degree of brain alignment, they examine the embedding space directly and quantify the level of scale-invariance within the embeddings. To measure the distribution similarity between two scales, the authors employ the Gromov-Wasserstein distance between embeddings of samples at different scales, obtained by considering different neighborhood sizes from a reference point. The study finds that embeddings with stronger scale-invariance align better with fMRI responses."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The motivation is clearly described, the methodological parts are outlined in an understandable way and the results and figures are well crafted.\n\n* The paper studies an interesting question and the connection between scale invariance and brain alignment is novel.\n\n* As outlined in the conclusion section, the findings and the proposed metric provides a direct, actionable approach for model design (optimizing scale invariance during pretraining) which could lead to interesting follow-up work."}, "weaknesses": {"value": "* As far as I could see, the models in Table 1 only included image/text models and supervised models trained on ImageNet variants. Thereby the range of different pretraining objectives is limited which has been shown to be an important factor for human alignment [1]. It would be interesting to also include self-supervised models in the analysis. Further, only one architecture is included (ConvNext). While this is good for comparing other factors, an ablation should be performed showing whether this insight also holds on different model architectures.\n\n* It would be good to show more ablations which model properties (training dataset, scale etc.) are correlated with scale invariance and how they are correlated with brain alignment. I have concerns that scale invariance might be only a proxy variable which emerges from some other property (e.g. model scale) which is also positively correlated with brain alignment and not the true causal factor.\n\n[1] Muttenthaler, Lukas, et al. \"Human alignment of neural network representations.\" ICLR 2023."}, "questions": {"value": "* How is the train/hold-out split described in section 3.5 performed. Is it also across subjects?\n\n* Did you make experiments if the scale invariance can also be estimated on a general dataset like ImageNet-1k and then used to predict the brain alignment for the Natural Scenes dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sIrAiUrguF", "forum": "Za3f4GdjqS", "replyto": "Za3f4GdjqS", "signatures": ["ICLR.cc/2026/Conference/Submission8782/Reviewer_XA1n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8782/Reviewer_XA1n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126178448, "cdate": 1762126178448, "tmdate": 1762920556731, "mdate": 1762920556731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes scale-invariance as an embedding-level property that predicts AI–brain alignment. Using NSD fMRI and embeddings from 60 ConvNeXt/CLIP variants, the authors quantify scale-invariance via (i) dimensional stability and (ii) multi-scale distributional similarity (Gromov–Wasserstein across neighborhoods). Models with flatter dimensional slopes and lower/flatter GW curves align better with fMRI, especially in EBA; larger pretraining datasets increase these properties, while supervised fine-tuning reduces them (via label-centric clustering). Although model size correlates with alignment, scale-invariance adds substantial predictive power. They suggest optimizing these metrics (e.g., GW regularization) to induce more brain-like representations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an interesting and well-motivated idea: that scale-invariance of embeddings—quantified via dimensional stability and Gromov–Wasserstein distributional similarity—predicts alignment between visual model representations and fMRI responses. The work is clearly written and conceptually relevant to ongoing efforts to identify what factors underlie brain–model correspondence. However, several aspects limit the interpretability and impact of the findings, as outlined below."}, "weaknesses": {"value": "1. There is limited contextualization of prior work. Section 4.1 reproduces a standard voxelwise encoding-model alignment analysis, yet omits discussion or citation of the extensive earlier literature that established this methodology (e.g., Yamins et al., 2014; Khaligh-Razavi & Kriegeskorte, 2014; Cichy et al., 2019; Conwell et al., 2024). Similarly, when stating that “recent studies show that neural networks, although never trained on neural data, often align well with brain activities,” the authors should also cite older foundational work from 2014–2017 that first demonstrated this phenomenon. Adding this historical context would help situate the contribution more accurately.\n\n2. Model selection seems narrow. Although the paper analyzes 60 models, all are ConvNeXt variants. Since the embeddings are from pretrained checkpoints, expanding to other architectures (e.g., ResNets, ViTs, SWINs, CLIP) would require little additional effort and would make the results more general and compelling. Restricting to a single architectural family limits the interpretability of “embedding-level” conclusions.\n\n3. My biggest concern with these and other studies is the reliance on purely correlational analysis. The study is correlational throughout—no causal manipulations are performed. Relationships between scale-invariance and alignment are inferred by comparing pretrained and fine-tuned models, not by experimentally controlling scale-invariance or dataset size. Stronger claims about causality would require training interventions, such as explicitly regularizing scale-invariance or systematically varying data scale while holding other factors fixed.\n\n4. I also had some concerns about the interpretation of the dimensional-stability slope. In Figure 4A, all models exhibit a sharp uptick in dimensionality between the last two scales, even though alignment is not a function of neighborhood size. This pattern may reflect boundary or estimator effects rather than a meaningful geometric difference. The authors should check whether the reported correlations hold when excluding extreme scales or when using alternative intrinsic-dimensionality estimators.\n\n5. I also think there may be possible analysis artifacts. Figure 9 shows identical correlation values (up to three decimal places) for PPA and RSC, suggesting a copy–paste or code error that should be verified.\n\n6. Low alignment in early visual regions. Reported alignment values for V1–V4 are unexpectedly low compared to prior work showing that early visual cortex can be modeled quite accurately by CNNs and other models (St-Yves et al., 2023; Saha et al., 2025). Clarifying methodological differences—e.g., voxel inclusion criteria, PCA compression, layer choice, or regression setup—would help explain this discrepancy.\n\n7. Unclear intuitive notion of “scale.” The definition of scale is introduced mathematically (via K-nearest neighborhoods) but remains conceptually opaque. A brief intuitive description—such as “smaller scales capture local neighborhood geometry, while larger scales reflect global embedding organization”—would help readers grasp the idea early on, ideally in the abstract or introduction.\n\n8. Missing relevant references. The authors should also consider citing Elmoznino & Bonner (2024), which discusses scale-free structure in visual representations, and related work on covariance spectra or manifold smoothness to better position their contribution."}, "questions": {"value": "Questions are outlined above in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ByOpEFblSj", "forum": "Za3f4GdjqS", "replyto": "Za3f4GdjqS", "signatures": ["ICLR.cc/2026/Conference/Submission8782/Reviewer_qqDn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8782/Reviewer_qqDn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210519973, "cdate": 1762210519973, "tmdate": 1762920556241, "mdate": 1762920556241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}