{"id": "wH2KwMOojc", "number": 11983, "cdate": 1758205001101, "mdate": 1759897541336, "content": {"title": "On the Suboptimality of Semi-Markov Decision Process in Hierarchical Reinforcement Learning", "abstract": "Hierarchical Reinforcement Learning (HRL) demonstrates highly efficient exploration in long-horizon decision-making problems with sparse rewards via the Semi-Markov Decision Process (SMDP). However, we observe a structural limitation of SMDP in HRL: once calling a subtask, the agent is locked into a fixed course of action, losing the flexibility to adapt to other higher-value subtasks, which is a critical barrier in optimality. To address this issue, we first decompose this suboptimality into execution suboptimality and policy suboptimality, and then propose corresponding algorithmic improvement frameworks. On the theoretical side, we reveal a fundamental design flaw in HRL where SMDP is simultaneously adopted in both the target and behavior policies. To overcome this flaw, we introduce the concepts of task tree and execution tree to decouple them, reducing the problem to a tradeoff between exploration and exploitation over policy execution modes. By constructing a unified value function and a generalized hierarchical Bellman equation, we achieve a multi-level value formalization. Upon this, we further propose Hierarchical Policy Improvement Theorem and Optimal Execution Theorem. These results theoretically prove the existence of two types of suboptimality and provide guarantees for the proposed improvement frameworks. \nControlled experiments across diverse environments consistently validate both the correctness of our theory and the effectiveness of the proposed improvements.", "tldr": "In HRL, the calling modes of the target policy, behavior policy, and deployment policy should be different.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning Theory", "Suboptimality", "Semi-Markov Process"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad3c62cc581874b81ce0106d7eb906a7a7c100d7.pdf", "supplementary_material": "/attachment/ca2dca11126eaee4a2d95b0bcadf92693af71809.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors aimed at: first, identify a \"fundamental design flaw\" of hierarchical reinforcement learning(HRL) through theoretical analysis; second, propose new framework for improvement; and finally, conduct numerical experiments to demonstrate the effectiveness of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I feel that the basic approach in this paper is sound. The authors demonstrated the order of expected returns of different execution and policy combinations through a thorough analysis. These theoretical results also allow them to propose new improvement frameworks to achieve effective outcomes for HRL. The numerical results, while limited, demonstrate that the proposed frameworks indeed improved the performance as expected."}, "weaknesses": {"value": "The implications of the theoretical results on the Markov decision processes and SemiMarkov decision processes and fundamental design flaws of HRL is not convincing demonstrated.\n     a. First, the HRL is not formally defined and described, while RL and MDP are very well-known subjects, HRL and SMDP are not, the authors should produce formal definitions of key concepts, such as subtask. \n     c. Related, the difference between MDP and SMDP as presented is not convincing, the authors need to use precise  language to describe their differences, in addition to illustration by figures;\n     b. While the fundamental flaw of HRL regarding \"SMDP is simultaneously adopted in both the target and behavior policies\" is prominently stated in the abstract, it is not adequately and explicitly discussed in the main paper."}, "questions": {"value": "1. A detailed discussion on the similarities and differences of the MDP and SMDP concepts used in this paper is needed;\n2. An explicit discussion on the implication of Theorem 4.1 on design flaw of HRL would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IHTPqCtwH5", "forum": "wH2KwMOojc", "replyto": "wH2KwMOojc", "signatures": ["ICLR.cc/2026/Conference/Submission11983/Reviewer_vbRc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11983/Reviewer_vbRc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761516873407, "cdate": 1761516873407, "tmdate": 1762922979092, "mdate": 1762922979092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors explore the suboptimality of semi-MDPs (SMDPs) and propose a novel framework based on optimal execution modes that addresses this suboptimality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem setting is well-motivated. Exploring the suboptimality of SMDPs seems like a worthwhile topic to explore. The analysis done on exploring the suboptimality was done reasonably well."}, "weaknesses": {"value": "My biggest concern with this paper is that it lacks a critical and necessary discussion point. In particular, the authors arrive at the conclusion that the so-called Markov Execution (ME) mode, in which a subtask is only active for a single frame, should be utilized during deployment. Yet, there is no discussion as to how this differs from learning single-frame actions via a regular (non-semi) MDP. Perhaps even more importantly, there is no discussion that addresses the motivation for ME. More specifically, if the subtasks are only being called for a single frame during execution, then what is the point of learning them at all, when one could just learn one-frame action policies via a regular MDP, at what I would imagine is a lower computational cost?\n\nIn my view, the authors’ fixation on execution modes works against them, and I would question whether it is even necessary to include it in the paper. The premise of exploring the suboptimality of SMDPs is, in itself, quite interesting, and the work that the authors performed in this regard was done reasonably well. But then the discussion on execution modes obscures the narrative of the paper, introduces an excessive number of acronyms, and ultimately makes the paper hard to follow. \n\nOne potential idea that the authors could explore if they insist on exploring this notion of execution modes, is that rather than having the binary ME/SME modes, it would be much more insightful to perform some sort of analysis that looks at how far away from optimality the agent is based on how long the subtask is executed. For example, perhaps in the first few frames of execution the gap is insignificant, but then after some threshold amount of frames, the gap begins to significantly increase. Accordingly, the authors could propose a way to find this ‘optimal stopping time’ for the subtasks, such that the optimality gap is negligible, but the agent still benefits from the benefits of temporal abstraction.\n\nOverall, while the work done on analyzing the suboptimality of SMDPs is done reasonably well, the notion of execution modes, in my view, needs to be heavily revised.\n\nMoreover, there are several aspects related to the presentation of the paper that need to be significantly improved. Aside from Figure 1, all the figures are hard to follow and could benefit from annotations and perhaps partitioning into a), b), etc. subfigures. For example, it is not clear in Figure 2 when the legend ends, when SME/ME begins, etc. In Figure 3, there is no legend, and ultimately, the figure as a whole is confusing and does not communicate to the reader what is happening in a sufficiently-well manner. In Figure 5a) right, one of the baselines is cropped out of the plot. From a writing perspective, similar issues occur. For example, the proof sketch of theorem 1 introduces too many acronyms and concepts without explaining them, thereby making it hard to understand."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xYjjhWM7bY", "forum": "wH2KwMOojc", "replyto": "wH2KwMOojc", "signatures": ["ICLR.cc/2026/Conference/Submission11983/Reviewer_q9eR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11983/Reviewer_q9eR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666783771, "cdate": 1761666783771, "tmdate": 1762922978541, "mdate": 1762922978541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a structural flaw in SMDP-based HRL: committing to a subtask until termination limits adaptability and optimality. It decomposes this limitation into execution and policy suboptimality, formally proving both through the Hierarchical Policy Improvement and Optimal Execution Theorems. By introducing Task Trees and Execution Trees, the authors decouple task decomposition from policy scheduling, enabling flexible analysis of Markov vs. Semi-Markov execution. They develop a Unified Value Function for HRL (UVFH) and a Generalized Hierarchical Bellman Equation (GHBE) to compute multi-level values under arbitrary execution modes. Building on this, three frameworks—ESIF, PSIF-1S, and PSIF-2S—address the identified suboptimalities by separating target and behavior policies. Experiments across HRL benchmarks demonstrate improved adaptability, efficiency, and returns under the proposed frameworks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tInteresting to note the issues arising from the two distinct components: execution suboptimality and policy suboptimality, which deserves attention from RL community.\n2.\tRigorous derivation of Generalized Hierarch Bellman Equations: Introduce the Task Tree (defining optimization objectives) and the Execution Tree (defining scheduling/interruption), leading to the Generalized Hierarchical Bellman Equation (GHBE) that provides a unified framework for analyzing any execution mode"}, "weaknesses": {"value": "Overall, the paper is hard to follow. For example, Fiture 3 is hard to understand even after reading related texts multiple times. If the author made a connection between mathematical notation (such as \\pi^{-} and \\pi^*) to whatever in the figure (such as paths of certain color), it would have been easier to follow.\n\nThe paper is full of new concepts, numerous defintions, etc. I would rather see a more abstract version of the paper with all the technical details in Appendix. The space could have been better used for exposition of the key ideas, the workings of proposed algorithms, and limitations of the work. \n\nOff-policy problem due to the discrepancy between the behavior execution mode (SME used to collect data) and the target execution mode (ME used in value backups). The issues seems outstanding and hence deserves numerical analysis, which is not done in the current version of the paper."}, "questions": {"value": "In option discovery, the termination function can be gradually optimized, allowing more exploration early in the training and later more on exploitation. Would more sensible option discovery address the issues of SME vs ME?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KLJQZeD9kl", "forum": "wH2KwMOojc", "replyto": "wH2KwMOojc", "signatures": ["ICLR.cc/2026/Conference/Submission11983/Reviewer_YuW2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11983/Reviewer_YuW2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705547857, "cdate": 1761705547857, "tmdate": 1762922978139, "mdate": 1762922978139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates different execution models for hierarchical reinforcement learning. In semi-Markov execution (SME) each subtask continues until termination before selecting a new subtask, while in Markov execution (ME) subtask selection is performed at each time step. The authors demonstrate that the expected return is higher under ME, though subtask policies are harder to train under ME. For this reason the authors propose several new learning frameworks that are then tested in experiments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Determining the best way to train and execute subtasks policies in hierarchical reinforcement learning is an important research question, and the two execution modalities proposed by the authors seem like reasonable choices."}, "weaknesses": {"value": "The problem definitions and learning setup are not clearly explained. The authors do not seem to use a discount factor, in which case the value function is only well-defined if all policies eventually terminate with probability 1. The value functions V^ME and V^SME are never formally defined. The definition of a training phase lacks details of exactly how training is performed. \n\nI understand what the authors mean by execution suboptimality, but without formal definitions of V^ME and V^SME I am not sure what policy suboptimality means. It seems to me that the optimal policy of a task is fully determined by the definition of a task on page 2. Either we learn this optimal policy during training, or we learn a suboptimal policy. My best understanding of policy suboptimality is that during training we have not been able to learn the optimal subtask policy.\n\nA curious choice of the authors is to *not* include key theoretical results in the main text. I believe all theorems have to be included in the main text even if the proofs are deferred to the appendix. Otherwise it is impossible for a casual reader to fully appreciate the theoretical contribution. On page 5 there is a supposed proof sketch but it is not clear which theorem or theoretical result is being proven. \n\nOn page 5 the authors state that the value computation under different execution modes cannot be directly described by the Bellman equation. I believe that there is a well-defined Bellman equation at each level of the hierarchy, so I am not sure what the authors mean.\n\nFor the training framework ESIF, the authors claim that ME execution can achieve expected return J_B. However, when training under SME it is possible that the agent *never makes subtask choices* in some states. For example, if the subtasks traverse rooms between hallways, the SMDP policy may only have been trained to make subtask choices in hallway states. In this case ME execution will fail in room states since the SMDP policy does not know which subtask to choose. It seems that the same thing can happen in PSIF-2S if some subtask choices are never made during Phase 1. Hence I am not convinced that the agent can achieve the optimal return in these training frameworks.\n\nIn the description of the training frameworks on page 7, there is suddenly a discount factor. I do not understand why the update rule returns two values V(s_t,w_t) and V(s_t,w_t,a_t). I believe you have to formally state the Bellman equations for subtasks at different levels of the hierarchy."}, "questions": {"value": "Do you train the policies of all subtasks in parallel? This is known to be a non-stationary problem since the subtask policies determine the dynamics at the SMDP level. How are the local rewards of subtasks used?\n\nWhat does the update rule mean that returns two values V(s_t,w_t) and V(s_t,w_t,a_t)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8lU9C2tVx", "forum": "wH2KwMOojc", "replyto": "wH2KwMOojc", "signatures": ["ICLR.cc/2026/Conference/Submission11983/Reviewer_ZE4K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11983/Reviewer_ZE4K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835561462, "cdate": 1761835561462, "tmdate": 1762922977208, "mdate": 1762922977208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}