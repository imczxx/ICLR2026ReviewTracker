{"id": "cTekVoqmpm", "number": 14874, "cdate": 1758244930599, "mdate": 1759897344074, "content": {"title": "WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models", "abstract": "In the era of large language models (LLMs), watermarking serves as a crucial safeguard for ensuring accountability, authenticity, and trust in machine-generated text. Text generated by LLMs can be identified through watermarking techniques, enabling the traceability and security of model outputs. Existing watermarking methods primarily embed signals by manipulating token generation probabilities, with detection achieved by computing corresponding statistical measures. Despite their effectiveness, these methods face a trade-off between detectability and text quality. In particular, watermarking strength and the randomness of watermark patterns may degrade the quality of generated text.\n\nThis paper proposes WaterSearch, a novel sentence-level search-based watermarking framework that overcomes this trade-off by jointly optimizing text quality (relative to unwatermarked distributions) and watermark detection confidence. Building on this framework, we further introduce a new watermark detection algorithm with theoretical guarantees. WaterSearch is highly versatile and can be integrated with various watermarking algorithms. We evaluate WaterSearch on three mainstream LLMs across ten diverse tasks. Extensive experiments show that our method achieves an average performance improvement of 51.01\\% over popular baselines under the same watermark detectability strength. For challenging scenarios such as short texts and low-entropy generation, WaterSearch shows improvement of 47.78\\% and 36.47\\% respectively, demonstrating strong robustness. Moreover, under watermark attack settings, our method maintains high detectability even with 80\\% word-level perturbations.", "tldr": "", "keywords": ["LLM", "text watermark", "watermark detection"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fed1f2340475da2e18a6b47ff13588e6d469b1a7.pdf", "supplementary_material": "/attachment/95bcc11b3a37c5650ecf503af20b3080614f5ef7.zip"}, "replies": [{"content": {"summary": {"value": "By constraining semantic consistency with the original sentence as much as possible, generating multiple outputs with different random seeds, and combining these outputs, this paper identified the optimal balance between semantic integrity and watermarking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The design of alpha is rigorous overall, with theoretical proof of its effectiveness and ablation studies demonstrating optimal alpha values, reasonably extending the KGW method.\n2. Effectively designed time complexity to ensure computational resources increase only moderately.\n3. Experiments demonstrate that sufficiently large differences between random seeds enable multiple outputs of the watermark to combine into text with semantics closer to the original meaning, including the validity of other hyperparameters such as K."}, "weaknesses": {"value": "1. Missing Visualization examples of all results, just an NBA example\n2. Scoring q is a linear add-up of semantic similarity towards the original output, and watermarking quality, which is very straight forward, but can be questioned that if the linear add-up is effective or not, more theoritical supports are needed\n3. Strategy of picking different random seed is still not clear enough for me"}, "questions": {"value": "How will the method perform on bigger and SOTA language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IbOjYNd0WE", "forum": "cTekVoqmpm", "replyto": "cTekVoqmpm", "signatures": ["ICLR.cc/2026/Conference/Submission14874/Reviewer_Qo61"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14874/Reviewer_Qo61"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949117911, "cdate": 1761949117911, "tmdate": 1762925223659, "mdate": 1762925223659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WaterSearch, a search-based watermarking framework that aims to improve the trade-off between watermark detectability and text quality in large language models (LLMs). Instead of modifying logits during generation as in the standard KGW framework, WaterSearch performs chunk-level parallel generation: it generates multiple candidate continuations (some watermarked, one unwatermarked) and selects the one that maximizes a joint score balancing semantic similarity to the unwatermarked text and detectability based on green-list token frequency. A chi-square–based detection procedure is proposed to test statistical significance across chunks. Experiments across 3 LLMs (Llama-2, Qwen-2.5, InternLM) and 10 datasets show consistent improvements in both generation quality and detection robustness, particularly under low-entropy and short-text conditions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* Simple idea and framework: WaterSearch can be applied on top of existing KGW-style watermarking schemes with minimal modification.\n* The method improves performance across all evaluated datasets, including difficult cases such as short-text or low-entropy settings, where KGW tends to fail.\n* The paper uses WaterBench and additional benchmarks (e.g., RepoBench-P) and shows gains across multiple model families.\n* Figures and algorithm descriptions make the approach easy to follow; the writing is concise and readable."}, "weaknesses": {"value": "* Incremental conceptual novelty: The idea to generate several watermarked candidates and pick the best is intuitive, but very closely resembles beam search or rejection sampling. The contribution feels more engineering-oriented than conceptual, especially given that most of the theoretical development restates expected properties of the existing KGW trade-off.\n\n* Computational inefficiency: WaterSearch performs parallel or beam-style generation of multiple watermarked candidates per chunk and selects the best one, which intuitively incurs substantial wall-time cost. While Table 4 discusses space complexity, runtime overhead or throughput (tokens/s) is not reported. Without this, it is hard to assess practical efficiency, but based on the runtime complexity reported in the paper, a ~5x slowdown in generation is fairly substantial and reduces the practical utility of the method."}, "questions": {"value": "* What is the actual computational overhead relative to vanilla KGW? Reporting wall-clock time or tokens/s for each configuration would clarify practical feasibility.\n\n* How sensitive is the approach to the number of parallel candidates k? Does increasing k yield linear improvement in detectability, or diminishing returns?\n\n* Could the same results be achieved by post-hoc reranking or constrained decoding (e.g., using logits rather than full re-generation)?\n\n* Since quality is evaluated only relative to the unwatermarked model’s continuation, could semantic drift still occur if that baseline itself is low-quality or inconsistent?\n\n* The claim that KGW “maintains text quality well from the perspective of perplexity or LLM-as-judge” is contradicted by results in WaterBench (Tu et al., 2024) and New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking (Singh et al., 2024), which show measurable degradations in both perplexity and subjective fluency for KGW. The discussion should acknowledge these findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zwgAiOI49J", "forum": "cTekVoqmpm", "replyto": "cTekVoqmpm", "signatures": ["ICLR.cc/2026/Conference/Submission14874/Reviewer_V6X7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14874/Reviewer_V6X7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961197314, "cdate": 1761961197314, "tmdate": 1762925223152, "mdate": 1762925223152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WaterSearch, a search-based LLM watermarking framework that generates multiple parallel candidates instead of a single one and then selects the one that best preserves coherence with the original text to enhance quality and detectability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using multiple candidates is clear and effective.\n\n2. The paper also includes a solid theoretical analysis of the proposed method.\n\n3. Evaluations are comprehensive, on various models and tasks."}, "weaknesses": {"value": "1. The overhead of this method seems to be significant.\n\n2. A main weakness of the paper is the limited comparison against recent works. The paper mainly compared the original KGW method. More recent and stronger baselines, including both token-level and semantic-level watermarking methods, should be compared for a more comprehensive evaluation. \n\n3. Robustness evaluation is also limited. Stronger modification and paraphrasing attackers, beyond deletion, insertion, and synonym substitution, need to be presented to show the superiority of the proposed method against SOTA."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zGXW9JZXXw", "forum": "cTekVoqmpm", "replyto": "cTekVoqmpm", "signatures": ["ICLR.cc/2026/Conference/Submission14874/Reviewer_xzV6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14874/Reviewer_xzV6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961467991, "cdate": 1761961467991, "tmdate": 1762925222818, "mdate": 1762925222818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WaterSearch, a novel search-based framework for watermarking Large Language Model (LLM) outputs. The core idea is to move beyond token-level watermark embedding by generating multiple candidate text chunks in parallel and selecting the one that best balances text quality (fidelity to the original, unwatermarked model distribution) and watermark detectability (statistical strength of the watermark signal). The method is presented as a solution to the fundamental trade-off between these two objectives in existing watermarking schemes like KGW. The authors also introduce a new detection algorithm based on hypothesis testing and provide extensive experimental results showing significant improvements over strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The shift from a purely token-level manipulation to a chunk-level search-and-select paradigm is a significant and compelling contribution. It elegantly reframes the watermarking problem as a multi-criteria optimization, which directly addresses a well-known limitation of existing methods.\n\n2. The paper provides a theoretical analysis (Theorem 1) linking the macroscopic (sentence-level) selection objective with the microscopic (token-level) watermarking trade-off. This strengthens the methodological foundation and justifies the proposed approach.\n\n3. The experiments are thorough and well-designed.\n    *   Comprehensive Benchmarking: Evaluation across 10 diverse tasks and 3 major LLMs (Llama-2, InternLM, Qwen) demonstrates generalizability.\n    *   Significant Performance Gains: The reported average improvement of 51.01% in downstream task performance under fixed detectability is impressive and clearly highlights the method's value.\n    *   Robustness in Challenging Scenarios: The strong results on short-text (+47.78%) and low-entropy (e.g., code generation, +36.47%) scenarios are particularly noteworthy, as these are known pain points for current watermarks.\n    *   Exceptional Attack Resilience: Maintaining high detectability under 80% word-level perturbations (deletion, insertion, substitution) is a remarkable result that significantly outperforms baselines."}, "weaknesses": {"value": "1. Algorithm 1 requires generating $k$ candidate chunks in parallel at each step. This implies the generation time (latency) will be roughly $k$ times that of a baseline method. The paper's claim of \"low computational cost\" is misleading as it primarily focuses on memory (KV cache).\n2. Algorithm 2 (Detection) appears to require the detector to \"Recover the seeds from generation\". This suggests the detector must know the exact context $c$ and the random seed generator used during generation. This is a much stronger assumption than KGW (which only needs a secret key) and may be fragile in black-box detection or if the context is slightly modified.\n\n3.  The experiments fix $k$ (beam size) to 5 (1 vanilla + 4 watermarked). $k$ is a critical hyperparameter balancing quality, detectability, and latency, but the paper lacks a sensitivity analysis or ablation study on $k$."}, "questions": {"value": "1.How exactly are the k−1 watermark seeds generated from context and recovered at detection time? Is the seed generator deterministic and keyed? What are attack consequences if this procedure is partially known? \n\n2.Can you provide wall-clock runtime and peak GPU memory numbers for representative settings (e.g., k=5, chunk m=32) on a standard GPU? The asymptotic KV discussion is useful but practitioners will want absolute numbers. \n\n3.Have you tried stronger/adaptive attackers (e.g., paraphrase-based sentence rewriting engineered to minimize green-token counts) or defenses that specifically target chunk-final tokens? If so, what happens to detectability? \n\n4.For Theorem 1, can you relax the token-independence assumption or empirically show how well the mapping (f) holds across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PwQ34ojB3R", "forum": "cTekVoqmpm", "replyto": "cTekVoqmpm", "signatures": ["ICLR.cc/2026/Conference/Submission14874/Reviewer_7SBL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14874/Reviewer_7SBL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995851146, "cdate": 1761995851146, "tmdate": 1762925222440, "mdate": 1762925222440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}