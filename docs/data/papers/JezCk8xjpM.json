{"id": "JezCk8xjpM", "number": 20763, "cdate": 1758309849899, "mdate": 1759896959939, "content": {"title": "When Do LLMs Listen? Confidence-Guided Knowledge Acceptance in LLMs", "abstract": "Large Language Models (LLMs) exhibit remarkable performance across a wide range of natural language tasks but face limitations in accessing dynamic or domain-specific knowledge not encountered during pre-training. To address this, recent research has explored integrating structured external knowledge from Knowledge Graphs (KGs) into LLMs via in-context learning (ICL). Although KG-augmented in-context reasoning has shown strong performance on commonsense tasks such as multiple-choice question answering (MCQA), the extent of LLMs’ dependence on external knowledge remains poorly understood. Prior work has primarily examined which knowledge to extract from KGs and how to represent it to optimize prompts and task accuracy. In this study, we shift the focus to if and when LLMs accept or resist injected knowledge. We introduce a confidence-guided framework that stratifies model predictions into high-, moderate-, and low-certainty bands: high means the model assigns dominant probability to a single choice, moderate reflects a few competing options with similar weight, and low corresponds to diffuse distributions with no clear preference. We then examine how knowledge interventions reshape probability distributions over candidate answers. Interventions include (i) supportive knowledge reinforcing the model’s initial choice, (ii) rival knowledge aligned with alternative answers, and (iii) noisy off-topic statements. Our analysis reveals systematic patterns: highly confident predictions largely resist rival or noisy evidence, whereas moderate- and low-confidence predictions are more susceptible to shifts when exposed to rival information. The model is most likely to switch between answer choices assigned similar mid-level confidence, while low-confidence options may gain probability mass but rarely enough to overturn the final decision. Noisy knowledge, however, induces only minor changes in the confidence distribution across choices. By moving beyond accuracy to behavioral response, this work provides a principled view of LLM robustness to knowledge augmentation and highlights design considerations for effective KG-enhanced question answering.", "tldr": "We study how LLMs respond to Knowledge Graph injections during in-context learning. High-confidence predictions resist rival or noisy knowledge, while lower-confidence ones are more influenced, offering insights into LLM robustness and design.", "keywords": ["Large Language Models (LLMs)", "Knowledge Graphs (KGs)", "In-Context Learning (ICL)", "Model Confidence", "Knowledge-Augmented Reasoning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bce6c75c453da0c49c53cb685a2f6f69c3e4cf6f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies whether LLMs “listen” to the intended target when multiple cues are mixed in the prompt. Using knowledge graph (KG)–derived supportive, rival, and noisy evidence, the authors analyze how conflicting cues affect model obedience and claim that instruction-following can be fragile under such multi-cue settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper isolates cue-level behavior instead of only measuring QA accuracy, providing a clear behavioral perspective.\n\n- The experimental setup is controlled and easy to reproduce.\n\n- The motivation is reasonable, as cue robustness is related to instruction-following reliability."}, "weaknesses": {"value": "- Very similar behavior has already been studied in noisy RAG and KG-based robustness work, where relevant and noisy evidence compete; the core phenomenon is not new.\n\n- KG literature already includes studies on misleading knowledge, noise injection, and evidence competition, so the contribution over prior KG/RAG robustness papers is unclear.\n\n- The paper provides observation only, without deeper analysis or actionable solution, limiting its impact."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mksiRQD3PG", "forum": "JezCk8xjpM", "replyto": "JezCk8xjpM", "signatures": ["ICLR.cc/2026/Conference/Submission20763/Reviewer_yVdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20763/Reviewer_yVdB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760966816719, "cdate": 1760966816719, "tmdate": 1762934187718, "mdate": 1762934187718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies when LLMs “listen” to external knowledge injected from a KG during multiple-choice QA. The authors bin questions by the model’s baseline confidence and then add (i) supportive paths for the model’s current top choice, (ii) rival paths aligned to other choices, or (iii) random noisy paths. They average posteriors across statements and report acceptance/lift/adoption patterns across bins. The main claim is that high-confidence predictions resist change, mid/low-confidence predictions shift more, and noise slightly dilutes probabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear experimental framing: supportive vs. rival vs. noisy knowledge is an intuitive setup.\n2. Confidence-stratified analysis is easy to understand and can be a useful lens."}, "weaknesses": {"value": "1. The mapping from question/answers to KG concepts is not specified with enough detail to be reproducible.\n2. The paper seems to presume that incorrect options are linked to the question in the KG, but does not justify why that should hold or how often it fails.\n3. There is no human or automatic check that extracted paths can actually support the model's reasoning of the supportive statements.\n4. Evaluation breadth is narrow: one dataset (CommonsenseQA) and a small model set; no tests on other QA tasks, other KGs."}, "questions": {"value": "1. How exactly do you map question spans and each answer option to KG nodes?\n2. For each bin and choice rank, what percentage of (question, option) pairs have at least one extracted path?\n3. How do you verify that a “supportive” path truly supports the correct answer rather than introducing bias/noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5JhNHctnA6", "forum": "JezCk8xjpM", "replyto": "JezCk8xjpM", "signatures": ["ICLR.cc/2026/Conference/Submission20763/Reviewer_xML7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20763/Reviewer_xML7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760970410257, "cdate": 1760970410257, "tmdate": 1762934186621, "mdate": 1762934186621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies when LLMs accept or resist external knowledge injected from a Knowledge Graph (KG) during in-context learning for multiple-choice QA. The authors propose a confidence-guided analysis: partition baseline model predictions into high/medium/low-confidence bands, then inject (i) supportive knowledge that favors the baseline top-1 option, (ii) rival knowledge aligned with alternative options (rank-2/3/4/5), and (iii) noisy off-topic statements. Using teacher-forcing probabilities, they quantify effects via Acceptance, Lift, and Adoption (switch) rates and assess robustness under noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Confidence-stratified acceptance is a clean framing that reveals stable patterns (support reinforces; near-rank rivals are most potent; high-confidence states are robust).\n* Well-controlled interventions. Separation of support/rival/noise and rank-aware rivaling is thoughtful. The acceptance/boost/adoption model is intuitive and reproducible.\n* Trends hold across several popular LLMs, which increases reliability."}, "weaknesses": {"value": "* Experiments focus on commonsense MCQA with ConceptNet-style paths. It is unclear if findings transfer to open-ended QA, multi-hop reasoning, or to other KGs (e.g., Wikidata) and domains.\n* Acceptance may depend on how KG paths are verbalized. The paper does not provide sensitivity analyses to paraphrasing, prompt length, prompt ordering, or number of injected paths.\n* The paper diagnoses when to inject knowledge but does not propose or validate a confidence-gated selection/prompting policy that demonstrably improves QA. Missing a natural next step that would elevate impact.\n* “Off-topic” noise is a clean control but less realistic than semantically plausible yet subtly misleading evidence. Adversarial or conflicting knowledge is under-explored."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UGud7R5Vyf", "forum": "JezCk8xjpM", "replyto": "JezCk8xjpM", "signatures": ["ICLR.cc/2026/Conference/Submission20763/Reviewer_Gxnj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20763/Reviewer_Gxnj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829809297, "cdate": 1761829809297, "tmdate": 1762934185920, "mdate": 1762934185920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how large language models (LLMs) respond to injected external knowledge—particularly from knowledge graphs (KGs)—in multiple-choice question answering tasks. Rather than focusing on accuracy alone, the authors introduce a confidence-guided framework that stratifies model predictions into high, moderate, and low confidence levels, and analyzes how different forms of knowledge (supportive, rival, and noisy) influence model predictions. Experiments on CommonsenseQA with multiple instruction-tuned LLMs reveal consistent behavioral patterns: high-confidence predictions tend to resist change, while low- and mid-confidence predictions are more susceptible to knowledge interventions. The findings offer practical insights into the design of KG-augmented prompting strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper uses a confidence-aware analysis to provide a more detailed understanding of when and how LLMs integrate external knowledge.\n\n2. The experiments are thorough and well-controlled, covering multiple model families and knowledge types, and the results reveal consistent empirical patterns."}, "weaknesses": {"value": "1. The main method and findings are largely expected—e.g. high-confidence predictions tend to resist change while low- and mid-confidence predictions are more susceptible—offering limited new insight.\n\n2. While a non-trivial number of prior works have explored when LLMs accept or disregard external knowledge, the paper does not clearly situate itself within this line of research."}, "questions": {"value": "Please refer to the above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Quofzmktaz", "forum": "JezCk8xjpM", "replyto": "JezCk8xjpM", "signatures": ["ICLR.cc/2026/Conference/Submission20763/Reviewer_p6Zj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20763/Reviewer_p6Zj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762409308433, "cdate": 1762409308433, "tmdate": 1762934185019, "mdate": 1762934185019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}