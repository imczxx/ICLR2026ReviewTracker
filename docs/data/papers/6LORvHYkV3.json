{"id": "6LORvHYkV3", "number": 11181, "cdate": 1758192364337, "mdate": 1759897602479, "content": {"title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers", "abstract": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.", "tldr": "", "keywords": ["Mixture of Experts", "Large Language Models", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3625c3d087d60bb2438fa25e109d6b8fce3965ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses an instability issue in RL for MoE models, that stems from the slight differences that can exist between training and sampling policy. The authors identify that the primary cause is a discrepancy in routing decisions between the inference and training phases, and even inconsistencies within the same training framework across repeated forward passes. To combat this, they propose Rollout Routing Replay (R3), a novel method that captures inference-time routing distributions and replays them during training. R3 aims to align expert selection between phases while preserving gradient flow. Extensive experiments on mathematical reasoning tasks with MoE models demonstrate that R3 substantially reduces training-inference KL divergence, mitigates probability discrepancies, improves training stability and outperforms existing stabilization techniques like GSPO and TIS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies precisely the core problem: routing inconsistencies in MoE models during RL training. It quantifies these discrepancies using KL divergence, an Extreme Token Distribution Function, and a multi-level analysis (router-level, token-level, sequence-level discrepancies, as shown in Figures 2 and 3). This systematic breakdown provides a strong foundation for the proposed solution.\n- R3 is a simple yet effective mechanism. By explicitly reusing inference-time routing masks, R3 directly addresses the alignment issue without disrupting gradient flow. The use of router mask caching makes it computationally inexpensive.\n- The experimental results are convincing: R3 significantly reduces KL divergence and the frequency of extreme tokens to levels comparable to dense models. Table 1 clearly demonstrates better performance across various benchmarks. The method's applicability to both on-policy and mini-batch style off-policy RL scenarios, as well as its testing across multi-/single-mini-step settings and different model types (SFT/Base), highlights its robustness. R3 is orthogonal to and can be combined with existing optimizers like GRPO and GSPO, and often improves them.\n- The most important finding is that R3 contributes to a more stable optimization process. This suggests a healthier and more efficient learning trajectory."}, "weaknesses": {"value": "- Limited explanation of root causes for internal discrepancies: While the paper thoroughly documents the external training-inference discrepancy, and notes that \"even multiple runs of the same training framework can produce divergent token probabilities\" (L139-140) and \"even when the input sequence is identical, the final output probabilities from two forward passes may differ\" within Megatron (L224-226), the underlying technical reasons for this internal inconsistency are not deeply explored. A brief discussion on potential causes like floating-point non-determinism, different hardware acceleration paths, or subtle differences in framework execution for \"old\" vs \"new\" policy computation would strengthen this point.\n- The paper does not provide any explanation on why R3 works better than TIS. \n- In general, the related work section would gain from being expanded, and provides only two references related to discrepancies of training frameworks.\n- The paper implicitly assumes that inference-time routing decisions are inherently \"better\" or more stable than what the training framework might produce. A brief justification for this assumption, or a discussion of scenarios where I_infer itself might be problematic (e.g., if the inference engine's router is poorly optimized or prone to its own kind of noise), would be valuable."}, "questions": {"value": "- Would the training still collapse if the MoE router was frozen? This would be an important baseline to add.\n- How does the π_train(θ_old) term in the PPO objective (Equation 1) interact with R3's mechanism? \n- Beyond the aggregate performance, were there any qualitative observations regarding which types of tokens or which specific layers experienced the most significant reduction in routing discrepancies with R3? \n- Can the authors elaborate on why R3 works better than TIS? \n- While mask caching is mentioned for efficiency, a more concrete discussion on the memory and computational overhead associated with storing and retrieving these masks for long sequences, especially with many MoE layers and large batch sizes, would be helpful. Is the overhead negligible across all tested scales?\n- There is a typo line 155, $\\pi_{infer}$ does not appear in the formula\n- Could you add a discussion to your paper? How does this paper influence future research?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "il1Urlg5tU", "forum": "6LORvHYkV3", "replyto": "6LORvHYkV3", "signatures": ["ICLR.cc/2026/Conference/Submission11181/Reviewer_Q997"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11181/Reviewer_Q997"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849779816, "cdate": 1761849779816, "tmdate": 1762922338348, "mdate": 1762922338348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key source of instability in reinforcement learning (RL) for Mixture-of-Experts (MoE) large language models: routing inconsistency between inference (rollout) and training. The authors observe that even under identical conditions, MoE routers can select different experts across passes, creating divergence between rollout and training policies that can trigger catastrophic collapse. To mitigate this, the paper proposes Rollout Routing Replay (R3),  a simple yet effective mechanism that records routing distributions (expert selections) from the inference engine and reuses them in the training phase.\n\nThis alignment substantially reduces training–inference KL divergence, lowers the number of “extreme tokens\", and improves training stability without slowing down throughput. Empirical results on large MoE models (e.g., Qwen3-30B-A3B) show that R3 outperforms GSPO and TIS in terms of stability and final performance on math RL tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper isolates a concrete but under-explored source of RL collapse: router nondeterminism in MoE models.\n- The proposed fix, replaying inference routing distributions into training, is conceptually simple yet novel and directly addresses the identified failure mode.\n- The empirical evaluation demonstrates evidence of reduced policy KL and improved stability.\n- The method integrates seamlessly with existing RL frameworks (e.g., SGLang rollout + Megatron training) and appears computationally lightweight.\n- Comparisons against strong baselines (GSPO, TIS) show consistent improvements.\n- The paper provides intuitive metrics (KL divergence, extreme-token statistics) to support its argument.\n- Stabilizing MoE RL is a highly relevant and urgent problem in LLM post-training."}, "weaknesses": {"value": "- The PPO and KL divergence equations contain inconsistent notation (π_train appears twice where π_infer should).\n- Equation (2) appears malformed and should be corrected and verified.\n- All experiments are on math RL tasks. Additional evidence on other domains (e.g., code RLVR, reasoning, dialogue) would increase generality.\n- The figures seem to use best checkpoints from single runs. Multi-seed mean ± CI and last-step metrics are required for reliability.\n- The paper claims “no slowdown,” but lacks any wall-time, throughput, or memory-usage comparison. Quantitative data is needed.\n- No analysis of (i) old-policy vs. update-policy replay, (ii) mask staleness, or (iii) top-K sensitivity.\n- The dataset filtering (100 k math problems) and verifier configuration are insufficiently documented.\n- Hyperparameter tables, seeds, and scripts should be released for full reproducibility."}, "questions": {"value": "- Please confirm and fix Equation (2). Is the KL computed per-token or per-sequence?\n- Quantify GPU-hour and tokens/s differences with and without R3. How large are the routing-mask caches?\n- In multi-mini-step updates, how long can stored routing masks remain valid before benefits degrade?\n- How does R3 interact with load-balancing or entropy penalties commonly applied to routers?\n- Does R3 help (or hurt) in non-math RL tasks or dense models with trivial routing?\n- Are improvements over GSPO/TIS statistically significant across seeds? Please report mean ± std.\n- Include a short analysis showing how R3 reduces gradient variance or stabilizes importance ratios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "There are no ethics concerns to report."}}, "id": "YKOV5BFPRl", "forum": "6LORvHYkV3", "replyto": "6LORvHYkV3", "signatures": ["ICLR.cc/2026/Conference/Submission11181/Reviewer_SRHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11181/Reviewer_SRHz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875263129, "cdate": 1761875263129, "tmdate": 1762922337410, "mdate": 1762922337410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the instability issue of routing mechanism in the reinforcement learning of MoE models, and proposes a post-training method named rollout routing replay (R3) by replaying the inference routing weights when tuning model parameters. Some experimental results verify the performance of R3 on stabilizing RL in MoE models. Furthermore, R3 can be applied with other RL methods, such as GSPO and TIS simultaneously."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The training and inference discrepancy is shown under different ways.\n2. The idea seems to work with different RL algorithms.\n3. The paper is well written and the method is clearly explained."}, "weaknesses": {"value": "Justification:\nSoundness: Though this paper shows the policy discrepancy between training and inference in multiple ways, such as KL Divergence and Extreme Token Distribution Function introduced in the paper, the reasons behind this phenomenon are not thoroughly studied yet. The experiments are not detailed enough. The increase of computation complexity of R3 is not shown. So, the results of this paper are not very convincing.\n\nPresentation: The figure 1 of this paper is clear to show the algorithm flow. But the meaning of the x-axis in figure 3 is not explained clearly. The training dynamics in figure 6 to show the performance of the algorithm is not explained well. Are there some reference papers comparing in such ways? Another question, what does the notation r(·) in line 157 mean? It does not appear in equation (2).\n\nContribution: The instability of RL in posting training of MoE is an important problem which is worth further research. This paper proposes to alignment the router weights of samples in training and inference to stable the RL training. The idea is easy to understand and seems to work under the experiment setting. However, the motivation of this idea is not very clear, and the experiment results are not very convincing.\n\nIn summary:\n1. The motivation of the method is not clear.\n2. There are some mistakes in equations and some figures are not clearly shown.\n3. The experiments are insufficient. The increase of computation complexity is not shown and the comparison with more SOTA algorithms is missing."}, "questions": {"value": "1. Can you compare the complexity of the proposed method with others?\n2. Can you clearly show the relation between the training-inference discrepancy and training instability?\n3. Can you add more experiment results of your method compared to other SOTA algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sZFCdpmKJM", "forum": "6LORvHYkV3", "replyto": "6LORvHYkV3", "signatures": ["ICLR.cc/2026/Conference/Submission11181/Reviewer_c84Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11181/Reviewer_c84Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144433770, "cdate": 1762144433770, "tmdate": 1762922336950, "mdate": 1762922336950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the issue of RL stability in LLM fine-tuning from the perspective of output distribution divergence between inference and training, an inevitability due to kernel, floating point operation, and computation graph non-determinism. The authors identify MOE routing non-determinism as a key contributor to high KL-divergence between sampler and trainer probability distributions, and propose a simple method for aligning routing by caching the routing masks computed at inference time and replaying them at training time. Experimentally, the method results in more stable and effective RL training on Qwen3-3b."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Improving the stability of RL training for LLMs is a very important yet understudied topic. This paper clearly identifies a root issue that is relevant for most modern models (MoE routing), and provide multiple illustrations of the significance of the issue in section 3. Furthermore, the solution (R3) is simple and easy-to-understand. Experimentally, the proposed method seems to work better than the standard fix of importance sampling, which is known to also introduce stability issues. The experiments are well-done in terms of benchmarks and baselines."}, "weaknesses": {"value": "The main issue is that the proposed fix is only evaluated on Qwen3-30b, and not at larger or smaller model scales or different MoE LLMs. For example, Section 3 suggests that Qwen3-8b does not suffer from this issue, so it's hard to evaluate how broadly applicable this fix beyond this exact model. Therefore, I'm willing to raise my score if a more comprehensive evaluation is provided."}, "questions": {"value": "1. Is gating noise applied to the router during training? If so, what is the schedule of the noise, and the potential impact on trainer-inference divergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KzqTIn2m7p", "forum": "6LORvHYkV3", "replyto": "6LORvHYkV3", "signatures": ["ICLR.cc/2026/Conference/Submission11181/Reviewer_eo2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11181/Reviewer_eo2q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762203519334, "cdate": 1762203519334, "tmdate": 1762922336512, "mdate": 1762922336512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}