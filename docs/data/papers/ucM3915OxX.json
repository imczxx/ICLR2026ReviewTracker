{"id": "ucM3915OxX", "number": 13586, "cdate": 1758219463623, "mdate": 1759897426435, "content": {"title": "Nearly-Optimal Bandit Learning in Stackelberg Games with Side Information", "abstract": "We study the problem of online learning in Stackelberg games with side information between a leader and a sequence of followers. In every round the leader observes contextual information and commits to a mixed strategy, after which the follower best-responds. We provide learning algorithms for the leader which achieve O(T^{1/2}) regret under bandit feedback, an improvement from the previously best-known rates of O(T^{2/3}). Our algorithms rely on a reduction to linear contextual bandits in the utility space: In each round, a linear contextual bandit algorithm recommends a utility vector, which our algorithm inverts to determine the leader's mixed strategy. We extend our algorithms to the setting in which the leader's utility function is unknown, and also apply it to the problems of bidding in second-price auctions with side information and online Bayesian persuasion with public and private states. Finally, we observe that our algorithms empirically outperform previous results on numerical simulations.", "tldr": "", "keywords": ["stackelberg games", "bandit learning", "side information"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c68cf9ce3a80cd51d4171f5cfb070453dab4467c.pdf", "supplementary_material": "/attachment/468212824bafc45a78e2381416e7e5b8d505b22b.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies online learning for contextual Stackelberg games under bandit feedback. This paper uses a reduction that lets the leader act in a utility space and then plug in linear contextual bandits algorithms. Instantiating the reduction with OFUL gives $\\tilde{O}(\\sqrt{T})$ regret when contexts are adversarial and follower types are stochastic; a logdet-FTRL instantiation gives $\\tilde{O}\\left(K^{2.5}\\sqrt{T}\\right)$ regret when contexts are stochastic and follower types are adversarial. The paper also extends to unknown leader utilities  with $\\tilde{O}\\bigl(\\mathrm{poly}(d,K,A_\\ell,A_f)\\sqrt{T}\\bigr)$ regret."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A clean reduction that operates in the leader’s utility space, enabling off-the-shelf contextual bandit algorithms to close the bandit-feedback gap to $\\tilde{O}(\\sqrt{T})$, with extensions to unknown utilities and applications such as auctions and Bayesian persuasion."}, "weaknesses": {"value": "1. The paper restricts the leader’s strategy space to the set of approximate extreme points of contextual best-response regions. It would be more interesting to allow approximate follower responses, i.e., followers who play $\\delta$-suboptimal best responses, and analyze how such deviations affect the reduction and regret guarantees.\n2.  The main results rely heavily on prior techniques—off-the-shelf linear contextual bandit algorithms, the Contextual Follower Best-Response Region framework, and $\\delta$-approximate extreme points—so the paper’s technical novelty is unclear; overall, the contribution appears incremental, primarily composing existing components."}, "questions": {"value": "No specific questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BvFMflr4YW", "forum": "ucM3915OxX", "replyto": "ucM3915OxX", "signatures": ["ICLR.cc/2026/Conference/Submission13586/Reviewer_dzah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13586/Reviewer_dzah"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761499730016, "cdate": 1761499730016, "tmdate": 1762924177394, "mdate": 1762924177394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies contextual Stackelberg games with multiple follower types under bandit feedback. The authors propose a reduction to linear contextual bandits in the leader’s utility space, enabling near-optimal learning algorithms for two settings: (i) adversarial contexts with stochastic followers, and (ii) stochastic contexts with adversarial followers. In both cases, the algorithm achieves $\\tilde{\\mathcal{O}}(T^{1/2})$ regret, improving upon the previous best-known $\\tilde{\\mathcal{O}}(T^{2/3})$ rate. The framework also extends to second-price auctions and Bayesian persuasion with side information through discretized action spaces."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work closes the gap between lower and upper regret bounds for bandit-feedback contextual Stackelberg games, achieving the optimal rate up to logarithmic factors.\n\n- The reduction to linear contextual bandits in utility space is conceptually clean and broadly applicable as shown by examples.\n\n- Extension to unknown utilities: Maintains near-optimal regret under a linearity assumption, showing robustness of the reduction.\n\n- The paper is well structured and the main theorems and algorithmic structure are well organized and technically sound."}, "weaknesses": {"value": "- The per-round time of the proposed algorithm is exponential which limits the practicability of the algorithm, which is also evident from the small toy example of Sec. 3.5. \n\n- The authors assume a finite and known set of follower types and full knowledge of their utility functions (except in the linear “unknown utility” variant). \n\n- The discretization of the leader’s strategy space $\\mathcal{E}_t$ is important to keep the action space finite but introduces an approximation error on the regret that is only bounded asymptotically."}, "questions": {"value": "1) In limitations, it is mentioned that a possible path forward towards reducing the runtime by means of smoothed analysis. Could the authors comment on the conceptual idea and how promising might be?\n\n2) How restrictive is the leader assumptions on the followers, i.e., cardinality, types, and utility, in practice? Do the authors see any path forward at relaxing these assumptions (cardinality and types)?\n\n3) Is it possible to say antyhing about the $\\mathcal{O}(1)$ approximation error in the regret due to the discretization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DW6xFnvkzA", "forum": "ucM3915OxX", "replyto": "ucM3915OxX", "signatures": ["ICLR.cc/2026/Conference/Submission13586/Reviewer_mHpn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13586/Reviewer_mHpn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773141788, "cdate": 1761773141788, "tmdate": 1762924177028, "mdate": 1762924177028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper treats the problem of online learning in Stackelberg games between a learner and a follower whose type changes over time. Both the leader and the follower have access to a $d$-dimensional side / contextual information variable $z_t$, and their payoffs are determined as follows:\n- For the leader, as a function $u(z_t,a_t,b_t)$ of the context $z_t$ at time $t$ and the actions $a_t$ and $b_t$ of the leader and follower respectively.\n- For the follower, as a time-varying function $u_t(z_t,a_t,b_t)$ of the context $z_t$ at time $t$ and the actions $a_t$ and $b_t$ of the leader and follower respectively. [The variability of $u_t$ over time is what the authors dub as the leader facing \"a sequence of followers\"]\n\nThe authors assume that the context $z_t$ or the follower's payoff function $u_t$ can be determined adversarially—but not both, as in that case, it is not possible to attain sublinear (Stackelberg) regret. In terms of information available to the players, they assume that the ensemble of the follower's payoff functions is known to the leader (though not the identity of each payoff function at each instance), the leader's payoff function could be either known or unknown to the follower, and the follower observes the leader's mixed strategy at each instance (so they can best-respond to it).\n\nThe authors provide a reduction to linear contextual bandits and, depending on whether the sequence of followers or contexts is adversarial, they provide the following guarantees:\n1. *For adversarial contexts and stochastic followers:* an $\\mathcal{O}(K\\sqrt{T} \\log T)$ regret bound when the recommendation subroutine in Algorithm 1 is the OFUL algorithm of Abbasi-Yadkori et al (2011).\n2. *For adversarial follower sequences and stochastic contexts:* an $\\mathcal{O}(K^2.5 \\sqrt{T} \\log T)$ when the regret-minimizing subroutine in Algorithm 1 is instantiated to the logdet-FTRL algorithm of Liu et al (2024).\nThe authors also provide a version of these theorems for the case where the leader's utility is unknown. In that case, the above bounds continue to hold in expectation with $K$ replaced by $d A_l A_f$, where $A_l$ is the number of actions available to the leader, and $A_f$ to the follower.\n\nFinally, the authors describe how their results could be applied to the context of bidding in auctions with side information (Section 4.1) and Bayesian persuasion (Section 4.2).\n\nOverall, I found the paper's topic to be on the fringe of ICLR topics—too specialized on a setting that is not central to generalist ML/AI audiences. My \"weak accept\" recommendation reflects my cautiously optimistic belief that the paper *could* be accepted if it garners sufficient support from experts in the domain. [I am knowledgeable in the general field but not an expert; still, I am skeptical regarding the difficulty of the reduction to contextual bandits or the novelty behind the meta-use of the .recommend subroutine in Algorithm 1.]\n\nOtherwise, if the paper's contribution is not championed by an expert in the field, I believe this paper would be much better suited to a more specialized venue like EC. I will wait for the authors' rebuttal and the discussion with the rest of the review panel to converge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and, as far as I could tell, the analysis is sound and the results correct. I am not a dedicated expert in this literature but, as far as I can tell, the authors' contributions are positioned properly within the relevant literature."}, "weaknesses": {"value": "My main objection to the paper is the paper's learning setting: I understand that the paper's framework may be sometimes referred to as \"bandit learning\" in the context of Stackelberg games but, otherwise, the assumptions made in the paper are fairly stringent and exacting. In particular, as was mentioned above, the authors assume that the ensemble of the follower's payoff functions is known to the leader, and the follower observes the leader's mixed strategy at each instance (so they can best-respond to it). Both assumptions go against the spirit of the partial information setting in bandits where it is assumed that players only observe their payoffs / losses—and even if this term is used in a part of the Stackelberg literature, it is a misnomer, which should not be propagated in a generalist venue like ICLR (or any of the other generalist learning venues, like NeurIPS, COLT, or ICML).\n\nOther than that, as I said above, I found the paper's topic to be on the fringe of ICLR topics—too specialized on a setting that is not central to generalist ML/AI audiences.\n\nIf the paper's contributions are deemed sufficiently impactful by an expert in the field (I am not an expert on Stackelberg games and/or contextual bandits), I wouldn't find either of the above weakenesses as an obstacle to acceptance. Come what may however, I would insist on the authors' changing the \"bandit learning\" terminology to avoid clashing with established terminology in online / game-theoretic learning."}, "questions": {"value": "None; see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AAcZvmFhBa", "forum": "ucM3915OxX", "replyto": "ucM3915OxX", "signatures": ["ICLR.cc/2026/Conference/Submission13586/Reviewer_w4MJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13586/Reviewer_w4MJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998165551, "cdate": 1761998165551, "tmdate": 1762924176662, "mdate": 1762924176662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}