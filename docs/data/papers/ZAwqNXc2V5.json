{"id": "ZAwqNXc2V5", "number": 4432, "cdate": 1757679972265, "mdate": 1763041274969, "content": {"title": "Forging a Masterpiece from Any Face: A Universal Framework for Face Stylization", "abstract": "The canonical challenge in face stylization lies in disentangling high-level semantic\ncontent, such as identity, from low-level stylistic attributes. Prevailing methods,\nincluding recent diffusion-based models, often fail to achieve a robust separation,\nresulting in an undesirable trade-off between style fidelity and content preservation.\nTo address these challenges, we introduce **StyleFace**, a novel framework that\ntreats face stylization as a targeted statistical transfer within a disentangled feature\nspace. Our approach is a cohesive pipeline that begins with a disentangled attention\nmodule, which orthogonally projects content and style information into separate,\ncontrollable embeddings. This separation is critical, enabling our method’s core:\na statistical style injection layer that manipulates feature distributions to preserve\nidentity while implanting style. To guide this transfer and ensure global coherence,\nthe entire process is optimized using a perceptually-aligned adversarial objective\nthat operates not on raw pixels, but on the high-level feature manifold of a Vision\nTransformer (ViT), enforcing perceptual and stylistic consistency. This synergistic\ndesign allows StyleFace to achieve an unprecedented balance between identity\npreservation and style fidelity, with comprehensive experiments demonstrating that\nour model consistently outperforms state-of-the-art methods", "tldr": "", "keywords": ["Face Stylization", "Diffusion Model", "Identity Preservation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4f2040b47b83684943a09f3c9b233adad23267c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces StyleFace, a novel diffusion-based framework for face stylization that aims to disentangle content (identity) from style to achieve high-quality stylized outputs while preserving identity. The core contributions include a disentangled attention module, a statistical style injection layer, and a perceptually-aligned adversarial objective. The authors claim that StyleFace outperforms existing methods in terms of balancing identity preservation and style fidelity based on both quantitative metrics and human evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly articulates the fundamental trade-off in face stylization between style fidelity and content preservation, a well-recognized challenge in the field.\n2.  The authors present extensive quantitative comparisons using a suite of metrics (LPIPS, CLIP-I, FIDs, ArtFID) and conduct a human evaluation, which is valuable for subjective tasks like stylization. The ablation study is also a positive aspect, attempting to justify the contribution of each component.\n3. The inclusion of a detailed reproducibility statement and the intention to release code and pre-trained models is highly commendable and beneficial for the research community."}, "weaknesses": {"value": "1. While the framework combines several existing techniques (diffusion models, attention mechanisms, statistical alignment, adversarial objectives), the specific combination and the individual components do not appear to offer sufficient novelty. Many aspects feel like a re-application or slight modification of established ideas in the context of face stylization.\n2. Despite claims of \"unprecedented balance\" and \"consistently outperforms,\" the provided qualitative results in Figure 3 and Figure 15 are not overwhelmingly superior to baselines, and in some cases, the differences are subtle or even debatable. The \"Stylized Face\" example on page 1 already raises concerns about the strength of style transfer. The human evaluation scores, while higher for StyleFace in ST and OA, show a lower IP score compared to some baselines (e.g., StyTR2, StyleID, AesPA-Net), which contradicts the core claim of balancing identity preservation. Based on the qualitative results provided, it is difficult to distinguish the stylized facial identity information.\n3. The descriptions of the \"disentangled attention module\" and \"statistical style injection layer\" lack the depth needed to fully understand their unique contributions beyond generic concepts. For example, how does the attention module orthogonally project information, and what specific statistical properties are manipulated beyond just mean and standard deviation in a novel way for face stylization?\n4. The Figure 1 is very confusing, as it appears to be a fusion of the embeddings generated by the face encoder and the embeddings generated by the diffusion model encoder."}, "questions": {"value": "1. You mention a \"statistical style injection layer that manipulates feature distributions.\" How does this layer differ significantly from existing adaptive instance normalization (AdaIN) or similar statistical alignment methods? What specific \"feature distributions\" are manipulated, and how does this manipulation uniquely preserve identity while implanting style, beyond simply aligning mean and standard deviation?\n2. The human evaluation results in Table 2 show that StyleFace has a lower Identity Preservation (IP) score compared to several baseline methods (StyTR2, StyleID, AesPA-Net). Given that \"identity preservation\" is a core claim and a key strength of your framework, how do you reconcile these lower scores with your assertion of \"unprecedented balance between identity preservation and style fidelity\"? Could you provide more detailed analysis or alternative interpretations of these specific results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Cy8wsN2rM7", "forum": "ZAwqNXc2V5", "replyto": "ZAwqNXc2V5", "signatures": ["ICLR.cc/2026/Conference/Submission4432/Reviewer_frMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4432/Reviewer_frMZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449617060, "cdate": 1761449617060, "tmdate": 1762917362048, "mdate": 1762917362048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank all reviewers for their valuable efforts and insightful suggestions. We have carefully considered each comment and will take the useful comments into account to improve our work."}}, "id": "PcudbGYkj2", "forum": "ZAwqNXc2V5", "replyto": "ZAwqNXc2V5", "signatures": ["ICLR.cc/2026/Conference/Submission4432/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4432/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763041273876, "cdate": 1763041273876, "tmdate": 1763041273876, "mdate": 1763041273876, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a targeted statistical transfer method for arbitrary face stylization. In particular, the authors use channel-wise mean of deep features to maintain identity and deep features’ standard deviation for style reference. In addition, the whole framework consists of a disentangled attention module and a statistical style injection layer. The entire process is optimized using a perceptually-aligned adversarial objective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "(1) The motivation is well presented of using the proposed targeted statistical transfer for arbitrary face stylization.\n\n(2) The explanations and illustrations are mostly clear and intuitive of the disentangled attention, the statistical style injection layer, the perceptually-aligned adversarial objective."}, "weaknesses": {"value": "(1) The novelty and contribution are very limited. In particular, the generator’s framework is largely a copy of the IP-Adapter where cross-attention is employed to inject FaceID features. The essence of style controller is the same with AdaIN where mean and standard variance are used for feature alignment.\n\n(2) As for the experimental results, the claimed identity-style disentanglement is not achieved according to the face stylization results.\n\n(3) On Page 1, Line 030-045, the figure has no legend and not cited in the main text. There is no explanation or analysis on the figure, and the face stylization is poor due to unexpected FaceID inconsistency between the content image and the stylized image, not to mention facial attributes’ inconsistency.\n\n(4) On Page 4, Line 167, the authors employed insightface for FaceID feature extraction. This is not included in the abstract and the authors seem to omit this important factor for ID preservation while the so-called statistical transfer are more pronounced.\n\n(5) On Figure 6, the effect of parameter alpha shows inconsistent transition and lacks necessary analysis.\n\n(6) Lack of related works. In literature, there has been a lot of GAN and Diffusion-based methods for arbitrary face stylization e.g. BlendGAN, JoJoGAN, IP-Adapter, InstantID. These works are neglected for comparison.\n\n[1] Liu, Mingcong, Qiang Li, Zekui Qin, Guoxin Zhang, Pengfei Wan, and Wen Zheng. \"BlendGAN: Implicitly gan blending for arbitrary stylized face generation.\" Advances in neural information processing systems 34 (2021): 29710-29722.\n\n[2] Chong, Min Jin, and David Forsyth. \"JojoGAN: One shot face stylization.\" In European Conference on Computer Vision, pp. 128-152. Cham: Springer Nature Switzerland, 2022.\n\n[3] Ye, Hu, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. \"IP-Adapter: Text compatible image prompt adapter for text-to-image diffusion models.\" arXiv preprint arXiv:2308.06721 (2023).\n\n[4] Wang, Qixun, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. \"InstantID: Zero-shot identity-preserving generation in seconds.\" arXiv preprint arXiv:2401.07519 (2024)."}, "questions": {"value": "No."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HUtfesik62", "forum": "ZAwqNXc2V5", "replyto": "ZAwqNXc2V5", "signatures": ["ICLR.cc/2026/Conference/Submission4432/Reviewer_n7J2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4432/Reviewer_n7J2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797008985, "cdate": 1761797008985, "tmdate": 1762917361746, "mdate": 1762917361746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a balanced face stylization framework for identity preservation and style transfer. Upon the powerful prior of the diffusion architecture, the proposed disentangling strategy and statistical manner reinforced each properties into generated face, outperforming existing SoTAs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Visual quality: The proposed method upon diffusion models demonstrates improved visual performance, outperforming previous models. The generated samples are seamless and relatively no visual artifacts.\n- Better quantitative results: StyleFace showed significantly improved metrics, particularly on $CLIP-I$ and $FID_s$.\n- Computing time: Unlike previous models, the proposed system takes substantial benefit in inference times, reporting about 2 second on a single H800 GPU, even though it is based on denoising diffusion scheme. Such advantages bring enhanced practicality for wide-ranging applications."}, "weaknesses": {"value": "- Concerns on structure preservation: In experimental results, it is observed several stylized faces have different structures against content images. In specific, the left-bottom sample in first page and second row in Fig.3 demonstrated substantially changed head posture. This issues include not only head posture but also eye gazing, eye closing, glasses, expression and tooth (Fig.4). Such structure discrepancies raise significant concerns about structural fidelity. It is strongly recommended to discuss aforementioned fidelity concerns and evaluate the proposed system with more explicit metric to validate the structural preservation capabilities such as head-pose, facial expression or landmark distance.\n\n- Missing comparison: The main comparison figures include image style transfer models rather than face stylization models. Thus, several face-central approaches are not included in experiments, e.g., JoJoGAN, DualStyleGAN, UI2I-Style, etc. Although the authors discussed these approaches in Sec.2.2, the proposed system was just compared with image-based style injection models. Considering the domain of this paper is in face, it is valuable to compare it to face stylization-centered models. Moreover, it lacks on literature analysis about this task even though face stylization has been relatively long-standing problem in generative realm, for instance, Toonify, Cross-Domain Style Mixing, CartoonGAN, AgileGAN. It seems that most related works are focused on diffusion-based approaches. The authors are recommended to compare it to face-focused models and discuss above issues. Also, it is crucial to analyze comprehensive studies in related works for potential readers to comprehend historical developments in this field.\n\n\n(Miscellaneous)\n- Pipeline: The proposed framework consists of several steps with diverse loss terms. For intuition, it is recommended to add algorithm to describe entire process or numerate the process step-by-step. \n- Discussion about limitation: It is crucial to discuss some limitations of the proposed system, e.g., failure cases, bottle neck or further improvements.\n- Inconsistent term: The authors interchangeably use the notation. For example, in Sec.3.1. $I_c$ is notated as ‘content image’ but simultaneously as ‘identity image’ as well. Also, Eq.(4) and (7) uses different font style to denote $L_{style}$. It is better to make consistent expressions to prevent the potential confusion.\n- Typo and error: Removing unnecessary factors in manuscript is important. For instance, Fig.(5) includes unidentified alphabet in the 2nd row and 4th column as $L$. Also, there is no best-score notation in Tab.(2) for $IP$ score, degrading consistency."}, "questions": {"value": "(Recommendation)\n- It is better to provide additional results of face-centered stylization models for experiments and add some discussions about it including various StyleGAN-approaches.\n- Considering significant concerns about structural fidelity mentioned above, it is imperative to alleviate this issue. If there is no strong preservation about input image, the fundamental goal of this task might be diluted.\n- It is also important to analyze the comprehensive literatures in related works, especially on face stylization task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dhKLaxWlEK", "forum": "ZAwqNXc2V5", "replyto": "ZAwqNXc2V5", "signatures": ["ICLR.cc/2026/Conference/Submission4432/Reviewer_4RXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4432/Reviewer_4RXM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891927850, "cdate": 1761891927850, "tmdate": 1762917361297, "mdate": 1762917361297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StyleFace, a new framework that treats face stylization as a targeted statistical transfer within a disentangled feature space. Experiments demonstrating that the proposed model outperforms state-of-the-art methods"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This manuscript is well-written, easy to understand, and easy to follow.\n\n- This method is computationally effective, with an inference time of 1.699s on a single H800 GPU (Table 4), StyleFace is significantly faster than other diffusion-based methods, enhancing practical applicability.\n\n- The performance of this article is excellent, especially in the experiments of Fig.2 and Table 1, which showed huge advantages."}, "weaknesses": {"value": "- The main observation of this article: first-order statistics (i.e., channel-wise mean) of deep features primarily encode an image’s structural and identity information. This is a commonly used conclusion and is very common in early Style transfer work, such as classic work AdaIN. This method is based on AdaIN, which seems to be an incremental contribution.\n\n- This method relies on meta information, such as the semantics of different style domains, which may not be available in practice.\n\n- Reliance on Stable Diffusion and LoRA limits exploration of applicability to other generative architectures, restricting generalizability beyond these frameworks. \n\n- The trade-off parameter α requires manual adjustment (Section 4.6), with optimal α=0.2 determined empirically, lacking an adaptive mechanism for user-friendly control.\n\n- The experiments are not extensive enough and are only conducted on the Art dataset. If they can add more diverse tasks, such as face aging, human to animals, or add more art domains in existing experiments, such as sketch, it may be more solid."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sBUgCoabh1", "forum": "ZAwqNXc2V5", "replyto": "ZAwqNXc2V5", "signatures": ["ICLR.cc/2026/Conference/Submission4432/Reviewer_Z6Ai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4432/Reviewer_Z6Ai"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926293356, "cdate": 1761926293356, "tmdate": 1762917360740, "mdate": 1762917360740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}