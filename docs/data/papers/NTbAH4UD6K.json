{"id": "NTbAH4UD6K", "number": 4628, "cdate": 1757729816116, "mdate": 1759898022782, "content": {"title": "Dimensional Debiasing via Multi-Agent Correction", "abstract": "Multimodal Large Language Models (MLLMs) recognize patterns from diverse data dimensions, such as shape, color, and associated language cues. However, inherent biases in training data can lead MLLMs to learn unintended, harmful shortcuts. For example, MLLMs often misinterpret clock times as defaulting to 10:10 due to memorized visual patterns rather than analyzing clock-hand positions. To address this, we propose the Multi-Agent Debiasing (MAD) framework, which performs cross-dimensional verification to correct these shortcut-driven errors. We first derive six dimensions of debiasing guidelines through a systematic analysis of failure responses. These guidelines inform the design of a team of specialized \"dimension critic\" agents, each an expert in correcting a specific type of error related to either biased cognition or limited perception. In our framework, potentially biased responses are dynamically routed through relevant agents. They then refine and correct the response in cascade over subsequent rounds. We leverage this cascaded correction process as a data engine to build our Multi-Dimensional Debiasing Dataset (MD$^3$), a large-scale collection of rich, debiased reasoning chains. By fine-tuning a model on MD$^3$, we directly teach it to overcome shortcut learning. Our experiments show that the MAD process encourages deeper thinking on biased responses. The MAD framework proves highly effective in classical visual debiasing settings and significantly enhances the reliability of MLLMs.", "tldr": "Multimodal LLMs often take shortcuts from biased data (such as always guessing a clock shows 10:10). We built a novel Multi-Agent Debiasing Correction, to identify and correct these biased responses.", "keywords": ["Multi-Agent System", "Vison Bias", "Multimodal LLMs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1ab3e4c1b61882af7c19402809ea0fe9a7e2d36.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MAD (Multi-Agent Debiasing), a framework to reduce shortcut biases in multimodal large language models (MLLMs). The authors argue that MLLMs often rely on spurious correlations (e.g., clocks reading 10:10 regardless of hand positions) due to biased training data and limited cross-dimensional reasoning. MAD introduces a set of specialized “dimension critic” agents that inspect a model's initial answer, identify the type of shortcut error, and iteratively refine it. A router agent dispatches queries to six critic types covering biased cognition (factual reasoning and counterfactual checks) and limited perception (object recognition, OCR, spatial reasoning, and counting). This process is used both at inference time and as a data engine to build a new dataset, MD3, consisting of ~50k debiased reasoning chains.\n\nModels fine-tuned on MD3 show consistent gains on spurious-correlation and robustness benchmarks, including VQA-CP, VQA-CE, GQA-OOD, OCRBench, and RealWorldQA, with reported improvements up to ~4–6% absolute and larger gains on the MD3 suite."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Intuitive \"cookbook\" for agent specialisation, which splits errors into 1) Biased Cognition (Errors from the LLM component) and 2) Limited Perception (Errors from the Vision component)."}, "weaknesses": {"value": "- The technical novelty appears limited, with the primary contribution centered on a specific multi-agent prompting system and a generated dataset. The paper would benefit from more rigorous justification and analysis of the engineering choices, as well as deeper scrutiny of the resulting dataset.\n- Key details about dataset construction are missing. Section 3.3 does not clearly describe how the system verifies that (1) the true shortcut bias has been correctly identified, and (2) the debiasing chain resolves it. Given the scale, this process is likely automated, yet there is no explanation of the validation or iteration procedure. Quantitative indicators of data quality (for example, agreement with other models or human-in-the-loop checks) would strengthen confidence in the dataset.\n- The reported gains are uneven. In Table 1 under “General Knowledge, Comprehension, and Reasoning,” 8 of 9 model-dataset combinations show less than a 1% improvement and 4 result in declines. The headline ~4% improvement “on challenging benchmarks” appears driven largely by MD³-related settings, and the improvements are far smaller elsewhere. A more balanced discussion of performance variation across settings, along with clarification of which benchmarks constitute the reported gains, is needed.\n- The introduction of sufficient, weak-sufficient, and unrelated “dimensions” offers useful intuition, but the formal definition in Definition 1 is imprecise and mixes set-based and probabilistic concepts. The authors should consider expressing these definitions probabilistically to improve conceptual clarity."}, "questions": {"value": "1) The sentence \"Both successfully corrected and persistently biased examples are used as positive and negative samples to further train the Router Agent, improving its diagnostic capabilities over time.\" is confusing. From my understanding, the router agent is GPT-4o-mini - and this is not being trained. Could you please clarify how the diagnostic capabilities improve over time?\n\n2) The sentence \"Our final MD3 dataset consists of approximately 50k debiased reasoning chains, generated from an initial pool of 90k instruction prompts\" requires clarification. What is the source of the 90k instruction prompts to begin with? And, as in the weaknesses, what is the specific filtering procedure for obtain 50k chains from these prompts? This is particularly important to clarify to deem the degree of overlap between the MD3 training set and the downstream tasks used to evaluate the debiased MLLM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d2HVnMxOHm", "forum": "NTbAH4UD6K", "replyto": "NTbAH4UD6K", "signatures": ["ICLR.cc/2026/Conference/Submission4628/Reviewer_fDRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4628/Reviewer_fDRT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871247560, "cdate": 1761871247560, "tmdate": 1762917477257, "mdate": 1762917477257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an agent-based system for reducing common errors in MLLM-based VQA systems. \nA taxonomy of six common types of pitfalls or errors is proposed, along with an agentic framework for addressing each error type during inference. A dataset of reasoning traces from running this system on multiple datasets is provided. \nMainly, this work is evaluated by first collecting this dataset, and then fine-tuning popular MLLMs like Llava and Llama. There are also ablation studies regarding cross-validation on the agent pool."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This work does a good job at executing on a reasonable research pipeline whereby a more refined dataset is collected and improved results are obtained by training on this data\n\n* The paper is fairly well written and easy to follow\n\n* There are some substantial gains made on certain datasets, especially less popular datasets that test specific biases."}, "weaknesses": {"value": "I have issues with both the significance and the novelty of this approach. \n\n\nSignificance: \n\n* Ostensibly, the goal here is to reduce bias and errors via this multiagent-style reflection and correction setup. The framework is informed by a 6-part taxonomy of errors, but I have a hard time believing that this is in any way a comprehensive categorization of common pitfalls. L796 says \"We validated this taxonomy through extensive error analysis on challenging datasets such as [...]\", details on this process are critical to understanding this approach yet I don't see them.  \n* I'm not convinced that we need this multiagent setup, what are the relative strengths of the models in the pool used to generate the data (see Q2). It seems like we could just pick one model like Qwen and use different prompts. Since there is no evaluation of the relative gain by using different models or with / without tools, many questions are left open. \n* There is no evaluation of the router itself, which is a crucial part of the design.  \n* I think there's some overindexing on the \"10:10 dilemma\" here, its rather basic ML that models will learn spurious features in their training data. But these features can (and will be) anything, so reducing down to 6 dimensions seems fraught. \n\n\nNovelty\n\n* In the end, this is a multiagent system that is used to label data that we can use to train MLLMs. One high-profile work here is UltraFeedback [1], which mainly only differs in that there are images here. I don't believe the agentic system itself or the proposed error taxonomy are enough to clear the bar here."}, "questions": {"value": "1) How was the taxonomy of errors validated? How many of each error occur in each dataset examined? Please show that the proposed error categories are comprehensive. \n\n2) What is the model pool used in this work? I see Qwen-vl, Internvl, and Phi-3.5 used in the figures but are they chosen for any specific reason? \n\n3) What are the different agent prompts?\n\n4) Why tools and different agents? If we're already using GPT4o / closed foundation models, then why not just let it do everything? I can think of reasons but they need to be stated with evidence. L839 states \"We emphasize that we did not rely solely on GPT-4o for annotation\". I am not convinced this is a good choice for the stated goals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uDwyieifjA", "forum": "NTbAH4UD6K", "replyto": "NTbAH4UD6K", "signatures": ["ICLR.cc/2026/Conference/Submission4628/Reviewer_TGLd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4628/Reviewer_TGLd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939598872, "cdate": 1761939598872, "tmdate": 1762917477060, "mdate": 1762917477060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the problem of MLLMs learning shortcuts (i.e., dataset biases) due to a lack of adequately representative samples in their training datasets. A framework called Multi-Agent Debiasing (MAD) is proposed to correct such shortcuts in reasoning through specialized agents which verify model responses across multiple different dimensions of common failure modes. Specifically, a router is used to route responses through various agents depending upon the types of possible shortcut errors. Each agent iteratively refines the response that was output by the prior agent in a cascading manner. The agents are designed based on a taxonomy of common failure modes in MLLMs which is introduced. The MAD approach is used to construct a dataset called MD3, which is then used to finetune MLLMs to mitigate their reliance on shortcuts. Experiments are conducted using 3 MLLMs and a range of different benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of mitigating shortcuts / spurious correlations learned by MLLMs is important and has broad applicability across multimodal reasoning tasks.\n2. Plenty of helpful illustrative examples are provided throughout the paper\n3. Ablation studies are conducted which indicate that increasing the number of agents improves performance"}, "weaknesses": {"value": "1. The paper has presentation and clarity issues. For example, L137: what is the motivation for claiming a uniform distribution here? L142-144: I do not think \"sufficient\" is an appropriate term to use here w.r.t. the example. An object being boat-shaped might be a necessary condition for a lifeboat, but is not sufficient (i.e., there are many boat-shaped objects which are not lifeboats). L216: the title above figure 4 is nonsensical. L325: the header of the table is misaligned with the columns.\n2. One downside of the proposed approach is that the router identifies the sequence of agents only after the initial response is generated. If one of the agents introduces its own biases/shortcuts, there is no mechanism to identify this and reroute the output to an appropriate agent for correction. \n3. It's unclear how the proposed bias taxonomy (\"shortcut cookbook\") was developed and validated. Examples are provided in Figure 5, but it's hard to tell how representative these failure cases actually are in practice. L314 states that \"these failure patterns are consistent across various state-of-the-art MLLMs and datasets\" - was this quantified in any way via automated and/or human analysis?\n4. Models trained with MAD seem to offer little or no improvement on general knowledge, comprehension, and reasoning - is this expected? It seems like there could still be cases of biases/shortcuts learned by models which could impact these tasks.\n5. MAD training seems to decrease performance in some cases (e.g., Hallusion for LLaVA-Llama-3-8B) and does not outperform other debiasing methods in some settings (Figure 6a). Better discussion of how these results still support the proposed method would be helpful.\n6. The main results (Table 1) are lacking a natural baseline such as training on another existing dataset rather than your proposed MD3 dataset. I wonder if we would still see improvements on these benchmarks simply by training on more data that isn't produced by MAD."}, "questions": {"value": "See questions in my weaknesses articulated above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WtSTLhWJWQ", "forum": "NTbAH4UD6K", "replyto": "NTbAH4UD6K", "signatures": ["ICLR.cc/2026/Conference/Submission4628/Reviewer_8heR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4628/Reviewer_8heR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956567124, "cdate": 1761956567124, "tmdate": 1762917476719, "mdate": 1762917476719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MAD (Multi-Agent Debiasing) is a framework that reduces shortcut biases in Multimodal LLMs by leveraging multiple specialized “critic” agents to detect and correct biased responses. A Router Agent identifies the bias type and delegates the correction to appropriate Dimensions. These agents refine the response through cascaded reasoning, often supported by vision tools, to produce a debiased explanation. The resulting corrected reasoning chains form the MD3 dataset, which is then used to fine-tune models so they learn debiased reasoning internally. MAD improves MLLM robustness and accuracy across several bias-sensitive benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  MAD corrects reasoning chains, teaching the model how to avoid shortcuts rather than merely suppressing them.\n2. Multi-Agent Specialization Improves Correction Quality\n3. Scalable Automatic Data Generation (MD3 Dataset)\n4. Fine-tuning with MD3 leads to consistent improvement across general and bias-sensitive multimodal benchmarks, showing that MAD enhances both robustness and overall reasoning capability."}, "weaknesses": {"value": "1. Potential Bias Propagation in MD3 Data Generation:\nWhile the automatic construction of the MD3 dataset via multi-agent correction is a notable strength, it also introduces a risk of bias amplification from the supervising agents themselves. Because the Router and Critic agents are LLM-based, their own cognitive or perceptual biases may inadvertently shape the “debiased” reasoning chains, potentially reinforcing rather than mitigating certain shortcuts. Or could do introducing another type of bias. The paper would benefit from a clearer discussion on quality control mechanisms, such as cross-model validation, human-in-the-loop sampling, or calibrated uncertainty measures, to ensure that the dataset does not inherit systematic bias from the agents.\n\n2. Limited Novelty Beyond Data Generation and Fine-Tuning: \nAlthough the paper presents a well-engineered debiasing pipeline and demonstrates strong empirical performance, the core contributions center primarily around dataset construction and subsequent fine-tuning. The conceptual novelty may appear somewhat incremental, as the method largely leverages established components—multi-agent prompting, tool-assisted reasoning, and standard instruction-tuning—to achieve debiasing. The work would be strengthened by deeper theoretical insights or more principled modeling innovations that go beyond data augmentation, such as formal guarantees on bias reduction, theoretical analysis of multi-agent correction dynamics, or mechanisms enabling intrinsic bias resistance without reliance on curated data."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PpGtzEK0aP", "forum": "NTbAH4UD6K", "replyto": "NTbAH4UD6K", "signatures": ["ICLR.cc/2026/Conference/Submission4628/Reviewer_Xb42"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4628/Reviewer_Xb42"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972759220, "cdate": 1761972759220, "tmdate": 1762917476441, "mdate": 1762917476441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}