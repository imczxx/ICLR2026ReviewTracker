{"id": "flgDWBGkAU", "number": 24394, "cdate": 1758356415329, "mdate": 1759896768413, "content": {"title": "Context-Aware Emotion Recognition via Multi-View  Instruction-Tuned Visual Language Guidance", "abstract": "Context-aware emotion recognition often relies on heterogeneous cues, but many state-of-the-art systems still hinge on engineered signals (e.g., pose landmarks or temporal cues), limiting applicability. Meanwhile, VLM based emotion recognition remains relatively under-explored in current re search. Our work targets this gap with a parameter-efficient, interpretable design. To mitigate class imbalance and make viewâ€“emotion relations explicit, we first curate an LLM-assisted QA dataset. In Stage 1, the VLM is adapted into a multi-view emotion encoder that extracts fine-grained features from scene, body, and face using shared, parameter-efficient com ponents with view-specific pathways, enabling interpretable evidence dis entanglement from a single image. In Stage 2, the VLM remains frozen and its scene/body/face descriptors are fused by a lightweight head. This preserves VLM knowledge (avoiding overfitting and label coupling) while yielding independent, well-calibrated scores that support flexible thresh olds, plug-and-play label sets, and strong sample efficiency. Using only single-image inputs, our pipeline attains 37.88 mAP on EMOTIC, 88.82% top-1 accuracy on CAER-S, and higher recall/F1 on HECO than prior VLM-based baselines, while offering clear per-view interpretability. Code, prompts, and data splits will be released.", "tldr": "We propose a parameter-efficient method that adapts a Vision-Language Model to recognize emotions by interpretably disentangling scene, body, and face cues from a single image", "keywords": ["Context-aware Emotion Recognition", "Vision-Language Models (VLMs)", "Representation Learning", "Disentangled Representation", "Interpretability", "Parameter-Efficient Fine-Tuning (PEFT)"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a61496be4684762de358733308cf1d9986a520c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a two-stage method for in-context emotion recognition leveraging a pre-trained VLM. In the first stage, IntructBLIP is finetuned using three separate vision encoders (for the entire image, face and body, respectively). The output is the class names. In the second stage, a classifier using a QFormer to fuse the different streams of the face encoders is further trained on the final classification. The method achieves results lower than those of other SOTA methods, which are unfortunately not included in the paper (see weaknesses)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using three separate vision encoders is intuitive as a method to disambiguate context from subject. In addition, addressing the class imbalance in EMOTIC makes sense as some of the minority classes represent less than 5% of the dataset."}, "weaknesses": {"value": "The biggest weakness is the fact that the related work has been cherry-picked, and key comparisons are not included [1,2,3]. The method is nearly identical to [1], the main difference being the three encoders and two-stage training. The similarities in the method are too many to ignore, specifically the use of VLLMs and finetuning of InstructBLIP for the task. Furthermore, this work performs significantly worse than all [1,2,3] on both EMOTIC and CAER-S, so it is hard to justify the method (more complex, less effective and with limited novelty). \n\nIn terms of clarity, the paper has a few issues, one of them being Table 7. There are two rows for\" Pose and situation description\", and \"Emotion rationale task\" -- these are not explained in the method, as far as we can see from the figures and the text. In the first stage, the input is 3 RGB images and an instruction in the form of multiple-choice VQA, while the output is class names. Similarly, in the second stage, there is no text input, and the output is standard classification probabilities. Either the method or the experiments need to be updated for clarity. On some more minor clarity/presentation issues, Table 3 is non-legible, and all tables have issues with citations being repeated.\n\nIn terms of results significance, Tables 8 & 9 show very similar results to non-balancing, so I am wondering if 0.16% is enough of a methodological contribution for only three classes.\n\nIn terms of the text, it looks like the citations in the first paragraph of the intro are not appropriate. Specifically, \"[...] early research focused on isolated facial expressions (eg, Ntizikra et al, 2025)\" -> a) this is from 2025, so not particularly early, and b) the paper is about IOT [4]. In addition, \"The advent of VLMs [..] leveraging their powerful semantic reasoning capabilities (Kosti 2017)\" -> That's the EMOTIC dataset.\n\n[1]  Xenos et al. \"VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning\", 2024\n\n[2] D. Yang, et al. Context De-Confounded Emotion Recognition. CVPR, 2023\n\n[3] D. Yang, et al. Robust emotion recognition in context debiasing. CVPR, 2024\n\n[4] Ntizikira et al. Enhancing IoT security through emotion recognition and blockchain-driven intrusion prevention, 2025"}, "questions": {"value": "Further to fixing the omission of previous works, the following questions need answering:\n\n- How does the VLM perform in terms of mAP? (Table 2)\n- How does the Classifier perform without stage 1?\n- What are the effects in Table 7?\n- How does the method differ from previous VLMs/VLLMs in in-context emotion recognition, and how does it perform in comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ldkieYkEjv", "forum": "flgDWBGkAU", "replyto": "flgDWBGkAU", "signatures": ["ICLR.cc/2026/Conference/Submission24394/Reviewer_omPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24394/Reviewer_omPa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761213673573, "cdate": 1761213673573, "tmdate": 1762943070740, "mdate": 1762943070740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-stage framework for context-aware emotion recognition from single images, leveraging instruction-tuned Vision-Language Models (VLMs). In Stage 1, an InstructBLIP-based VLM is adapted via a synthetically generated QA dataset to extract emotion-relevant visual features from three views: scene, body, and face, using shared but view-specific query tokens in the Q-Former. In Stage 2, the VLM is frozen, and its extracted features are fused via a lightweight self-attention classifier to produce calibrated, independent emotion scores. The method achieves state-of-the-art results on EMOTIC, CAER-S, and HECO without using engineered features like pose landmarks or human-object interaction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Only the Q-Former and projection heads are tuned; the image encoder and LLM remain frozen.\n2. Explicit disentanglement of scene/body/face contributions enables per-view analysis.\n3. Unlike autoregressive VLMs, the final classifier provides independent confidence scores per emotion.\n4. SOTA performance using only three basic visual inputs, outperforming methods with complex feature engineering.\n5. Mitigates long-tail issues via minor-label QA augmentation and major-label downsampling."}, "weaknesses": {"value": "1. All experiments use image-based datasets (EMOTIC, CAER-S, HECO); no video or temporal dynamics are considered, despite emotion being inherently dynamic in many real-world scenarios.\n2. The related work and baselines stop at 2024, omitting key CVPR/ICML 2025 publications on VLMs and emotion recognition that would have been available by submission (Sept 2025).\n3. The instruction-tuning corpus is fully auto-generated (via Osprey, Qwen-VL, Gemini); no manual verification of quality or bias is reported.\n4. Critical hyperparameters (learning rate, batch size, optimizer, number of epochs) are missing, undermining reproducibility.\n5. Despite emphasizing a \"parameter-efficient\" and \"lightweight\" design, the paper does not discuss computational cost or compare it to the state-of-the-art using these criteria."}, "questions": {"value": "1. Given that CVPR/ICML/ACL 2025 proceedings were available by your submission date (Sept 2025), why were recent VLM-based emotion recognition methods not included in comparisons?\n2. Can you provide quantitative metrics on model size, inference speed (ms/image), and GPU memory usage? How does your method compare to prior work in terms of FLOPs or real-time feasibility?\n3. Was any subset of the synthetic QA dataset manually reviewed for emotional validity or factual consistency? \n4. Please specify full training hyperparameters for both stages: optimizer, learning rate, batch size, number of epochs, and hardware used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sxMIuKg1qF", "forum": "flgDWBGkAU", "replyto": "flgDWBGkAU", "signatures": ["ICLR.cc/2026/Conference/Submission24394/Reviewer_eUPn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24394/Reviewer_eUPn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816875813, "cdate": 1761816875813, "tmdate": 1762943070405, "mdate": 1762943070405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper proposes a two-stage framework for context-aware emotion recognition from a single image: Stage-1 instruction-tunes a VLM into a multi-view emotion encoder (scene/body/face) with a frozen image encoder and LLM; Stage-2 freezes the VLM and trains a lightweight classifier with self-attention to fuse the three view descriptors and output calibrated per-label probabilities.\n\n2. It introduces a label-aware data balancing scheme (minor-label QA augmentation + major-label down-sampling) and a synthetic instruction-tuning QA corpus.\n\n3. The two step approach achieves good performance on several emotion recognition datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- By freezing the image encoder and LLM while sharing a Q-Former, the approach is parameter-efficient.\n\n- Competitive results using minimalist inputs (scene/body/face), reducing reliance on engineered signals."}, "weaknesses": {"value": "In general, two-stage methods are not very convenient, especially that a large model is trained in stage 1.\n\nAnd it is not clear how much the first stage is helping the second stage for the task of consideration. If the first stage is rephrasing the classification task with natural language, and the same vision encoder backbone is used in both stages, intuitively what additional information is gained by the classifier? Given that the VLM model actually works worse than the stage 2 classifier, it appears that aligning the modalities with stage 1 is hard.\n\nIn Table 2, is there a baseline where the classifier architecture remains the same, but without the stage 1 training? Such a baseline can tell how useful the first stage training is. I imagine the ``Ours (Classifier)'' refers to the model trained with both stages."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UnpytKWiYK", "forum": "flgDWBGkAU", "replyto": "flgDWBGkAU", "signatures": ["ICLR.cc/2026/Conference/Submission24394/Reviewer_CN5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24394/Reviewer_CN5h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939497435, "cdate": 1761939497435, "tmdate": 1762943069991, "mdate": 1762943069991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}