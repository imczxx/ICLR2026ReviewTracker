{"id": "1tJH2CKZZa", "number": 8280, "cdate": 1758077296810, "mdate": 1762920467465, "content": {"title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models", "abstract": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for robot control through natural language instructions. However, their high inference cost—stemming from large-scale token computation and autoregressive decoding—poses significant challenges for real-time deployment and edge applications. While prior work has primarily focused on efficient architectural optimization, we take a different and innovative perspective by identifying a dual form of redundancy in VLA models: (i) high similarity across consecutive action steps, and (ii) substantial redundancy in visual tokens.\nMotivated by these observations, we propose FlashVLA, the first training-free and plug-and-play acceleration framework that enables action reuse in VLA models. Specifically, FlashVLA improves inference efficiency through a token-aware action reuse mechanism that avoids redundant decoding across stable action steps, and an information-guided visual token selection strategy that prunes low-contribution tokens.\nExtensive experiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7% and latency by 36.0%, with only a 0.7% drop in task success rate. These results demonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency VLA inference without retraining.", "tldr": "FlashVLA is the first acceleration framework to enable action reuse in Vision-Language-Action models, featuring a training-free and plug-and-play design that significantly reduces inference cost through temporal and visual redundancy exploitation.", "keywords": ["Vision-Language-Action Models", "Model Acceleration"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/63e2c04de5d2ee66d8b2578e0e096416287649c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents FLASHVLA, a training-free and plug-and-play acceleration framework for Vision-Language-Action (VLA) models. Instead of architectural re-design or retraining, the method targets two types of redundancy: (1) temporal similarity between consecutive actions and (2) redundancy among visual tokens. To address these, the authors propose a token-aware action reuse mechanism and an information-guided token pruning strategy based on singular value decomposition (SVD) and information contribution scores (ICS).\nExperiments on LIBERO, UniVLA, and VLAbench show that FLASHVLA reduces FLOPs by 55.7% and latency by 36.0%, with only 0.7% performance drop. While the idea is appealing and practical, the methodology mainly combines known techniques (e.g., token pruning, reuse heuristics) with moderate novelty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a simple yet effective framework to reduce redundant computations in VLA inference.\n\n- Training-free and compatible with FlashAttention, enabling easy integration into existing models.\n\n- Demonstrates strong empirical results on multiple benchmarks.\n\n- Includes detailed ablation and sensitivity analyses."}, "weaknesses": {"value": "- The approach primarily combines known concepts (token pruning, reuse heuristics) without strong theoretical advancement.\n\n- The method is evaluated mostly in simulated environments; real-robot deployment results are missing.\n\n- Performance may depend on manually tuned thresholds; no adaptive mechanism is proposed.\n\n- Works such as VLA-Cache [1], TinyVLA [2], EfficientVLA [3] should be discussed and experimentally compared.\n- Some related token pruning methods should also be discussed and compared, such as AIM\n[4], DART [5], VisPruner [6]\n\n\n[1] Xu, Siyu, et al. \"Vla-cache: Towards efficient vision-language-action model via adaptive token caching in robotic manipulation.\" arXiv preprint arXiv:2502.02175 (2025). \\\n[2] Wen, Junjie, et al. \"Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation.\" IEEE Robotics and Automation Letters (2025). \\\n[3] Yang, Yantai, et al. \"EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models.\" arXiv preprint arXiv:2506.10100 (2025). \\\n[4] Zhong, Yiwu, et al. \"Aim: Adaptive inference of multi-modal llms via token merging and pruning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025. \\\n[5] Wen, Zichen, et al. \"Stop looking for important tokens in multimodal language models: Duplication matters more.\" arXiv preprint arXiv:2502.11494 (2025). \\\n[6] Zhang, Qizhe, et al. \"Beyond text-visual attention: Exploiting visual cues for effective token pruning in vlms.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025."}, "questions": {"value": "- How stable is the reuse mechanism under long-horizon or dynamic-scene tasks?\n- Can the reuse threshold be made adaptive, e.g., based on uncertainty or temporal confidence?\n- Could the authors report real-world timing (wall-clock) results beyond FLOPs estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CM0BrXRxJg", "forum": "1tJH2CKZZa", "replyto": "1tJH2CKZZa", "signatures": ["ICLR.cc/2026/Conference/Submission8280/Reviewer_kQ7p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8280/Reviewer_kQ7p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477692760, "cdate": 1761477692760, "tmdate": 1762920213076, "mdate": 1762920213076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "W1CQISYSOk", "forum": "1tJH2CKZZa", "replyto": "1tJH2CKZZa", "signatures": ["ICLR.cc/2026/Conference/Submission8280/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8280/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762920466470, "cdate": 1762920466470, "tmdate": 1762920466470, "mdate": 1762920466470, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free and plug-and-play acceleration framework named FlashVLA. The proposed method features a token-aware action reuse mechanism and a visual token selection strategy that integrate seamlessly with Flash Attention to avoid redundant computation. The experiments on the LIBERO simulation benchmark show that inference latency and the FLOPs of visual token computation are both decreased considerably without significantly sacrificing the success rate."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Substantial reduction in FLOPs and inference latency without additional fine-tuning;\n2. The method is straightforward and can be integrated with models that use Flash Attention for inference."}, "weaknesses": {"value": "All the experiments are conducted on simulation manipulation benchmarks (LIBERO, VLABench). It lacks validation on tasks that involve highly dynamic actions (requiring frequent and rapid changes in actuators) and rapidly changing visual scenes (with significant perturbations in objects and background)."}, "questions": {"value": "As we know, real-world tasks are prone to visual perturbations and sensor noise, while robotic arms face a sim-to-real gap. Could the action reuse mechanism significantly compromise the execution accuracy of the robotic arm? How effective can visual token reduction remain in the face of real-world visual variations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6KvHm8n6IJ", "forum": "1tJH2CKZZa", "replyto": "1tJH2CKZZa", "signatures": ["ICLR.cc/2026/Conference/Submission8280/Reviewer_qWBe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8280/Reviewer_qWBe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837895527, "cdate": 1761837895527, "tmdate": 1762920212724, "mdate": 1762920212724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FLASHVLA, a training-free acceleration framework for Vision-Language-Action (VLA) models. The method is motivated by the identification of two key redundancies: high similarity across consecutive action steps and high redundancy in visual tokens. Experiments on the LIBERO benchmark show that FLASHVLA can reduce FLOPs and latency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- FLASHVLA can be directly plugged into existing VLA models (e.g., OpenVLA, UniVLA) without retraining, which makes it both practical and reproducible.\n\n- The extensive ablation studies on # of Tokens, different modules and diverse VLA architectures make the results convincing."}, "weaknesses": {"value": "- Both the similarity across consecutive action steps and across visual tokens have been well-studied. “The first training-free and plug-and-play acceleration framework that enables action reuse in VLA models” also needs more justification.\n\n- The method's hyperparameters appear to require careful, per-setting tuning, which may limit reproducibility. Specifically, the parameter $\\delta$, which controls the token set stability threshold $\\epsilon_2$, is set to different values for each token budget (e.g., 3, 4.5, 5, 5.5).\n\n- The experiments should be further strengthened. First, the experiments on other VLA acceleration baselines and other benchmarks should be included. Second, the evaluation is confined to simulation. While the simulation results on LIBERO are strong, the paper lacks validation on a real-world robot."}, "questions": {"value": "- How sensitive is performance to the thresholds ε1 (action angle) and ε2 (token overlap)?  Could these parameters be adaptively learned or adjusted at runtime?\n\n- In scenarios with discontinuous or abrupt motion (e.g., contact manipulation), does the reuse mechanism still function reliably, or could it mis-predict stable states?\n\n- There seems to be a critical contradiction in the description of the \"FlashTrigger\" mechanism (Section 3.3). Could the authors please clarify the action reuse trigger logic in Equation 6? Is the condition $\\alpha(s) > \\epsilon_1$ a typo, and should it be $\\alpha(s) < \\epsilon_1$? The current formulation seems to contradict the stated goal of reusing actions in \"stable areas\" where the angle change is small.\n\n- The visual token selection strategy is applied after the first two layers ($L_p=2$) in the prefill stage. What is the rationale for this specific depth? How sensitive is the model's performance to this $L_p$ parameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "29STucawGr", "forum": "1tJH2CKZZa", "replyto": "1tJH2CKZZa", "signatures": ["ICLR.cc/2026/Conference/Submission8280/Reviewer_GNjz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8280/Reviewer_GNjz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886952542, "cdate": 1761886952542, "tmdate": 1762920212312, "mdate": 1762920212312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}