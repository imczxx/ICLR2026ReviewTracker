{"id": "5eqbDG2rKZ", "number": 23899, "cdate": 1758350020884, "mdate": 1759896791363, "content": {"title": "ALPS: Adaptive LLM Pruning via Gradient Search in Learned Representation Space", "abstract": "Deploying Large Language Models (LLMs) at the edge is crucial for data privacy and offline operation, yet their massive parameter count poses significant resource challenges. While existing methods rely on discrete-space heuristics to search for pruning configurations, we introduce a fundamentally different approach: reformulating the search for optimal LLM pruning configurations as gradient optimization in a learned continuous representation space. Our method, ALPS (Adaptive Layer Pruning via Search), embeds discrete pruning configurations into a continuous space where efficient gradient-based optimization becomes possible, then decodes optimal representations back to implementable discrete pruning schemes. This encoder-evaluator-decoder architecture automatically learns from collected “pruning-score\" data pairs, eliminating manual tuning while jointly optimizing for model performance, latency, and energy consumption in a deployment-specific manner. Extensive experiments across Llama-7B, Llama2-7B, Llama2-13B, and Vicuna-7B demonstrate ALPS's superiority, achieving up to 34.1% energy reduction and 33.5\\% lower latency while maintaining over 91% of original performance. At high pruning ratios (50%), ALPS consistently outperforms state-of-the-art methods in both perplexity and downstream task accuracy.", "tldr": "An adaptive LLM pruning framework that uses an encoder-evaluator-decoder approach to optimize pruning for runtime adaptability.", "keywords": ["Large Language Models", "Adaptive Pruning", "Gradient-steered Search"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1814984230e8d17b438ac68db8fc806cf157dfe1.pdf", "supplementary_material": "/attachment/92d15255ae8e783c386d9c6e36427012d9b7998d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel pruning configuration search method that maps discrete heuristics into a continuous space. This approach directly optimizes the pruning configuration with respect to both model performance and latency. Experimental results demonstrate that the method achieves superior performance in terms of latency, perplexity, and downstream task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I like the idea presented in this paper. While the concept of relaxing discrete heuristics into a continuous space is well known in the machine learning community, this work provides a well-designed and likely first practical adaptation of that idea for pruning configuration optimization. In addition, the paper introduces a thoughtfully constructed data generation pipeline for training the configuration model.\n\nThe paper is clearly written and presents comprehensive experiments that evaluate performance from multiple perspectives, including next-token prediction accuracy, downstream task performance, and latency."}, "weaknesses": {"value": "The motivation and method descriptions in this paper are well-written, but I have several concerns regarding the experimental section:\n- The experimental setup (e.g., models, tasks) feels somewhat outdated. I would expect evaluations on at least LLaMA3.4 or the latest Qwen model families. This is important because the behavior of pruning methods on models like LLaMA-2 can differ significantly from SOTA models due to their much richer pretraining corpora (beyond datasets like WikiText).\n- Similarly, the selection of baseline methods is mostly limited to those before 2024. While I haven’t kept up with every recent pruning paper, there are at least methods like SVD-LLM and ModeGPT that achieve much better performance and should be considered in the comparisons.\n- I also feel the authors should include more comparisons with global sparsity allocation methods. Since the proposed method primarily focuses on optimizing pruning configurations, it would be useful to relate it to approaches addressing similar global sparsity allocation challenges—such as those discussed in Appendix B.10 of ModeGPT.\n- Lastly, I’m curious about the pruning time required for the proposed method. I expect it may take more time, which is acceptable for an inference-speedup target. However, providing this comparison would help readers better understand the trade-offs and decide when to adopt this method in practice."}, "questions": {"value": "- In line 49, I'm considering if it's good to say other solutions cannot adapt to specific hardware constraints, we can do this by adjusting the pruning ratio, right?\n- How to choose the dimension d during the encoding?\n- If more training data were available for learning the pruning configurations, do you expect further improvements in performance? Have you tested the sensitivity of the method to data size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TgClQGHdyq", "forum": "5eqbDG2rKZ", "replyto": "5eqbDG2rKZ", "signatures": ["ICLR.cc/2026/Conference/Submission23899/Reviewer_onua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23899/Reviewer_onua"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431806141, "cdate": 1761431806141, "tmdate": 1762942846619, "mdate": 1762942846619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Alps, an adaptive pruning framework for large language models (LLMs) that reframes the discrete search for pruning configurations as a continuous optimization problem in a learned latent space. An encoder–evaluator–decoder pipeline is trained using “pruning configuration–performance score” pairs obtained from heuristic methods. Gradient ascent within this learned space identifies promising pruning representations, which are decoded into executable pruning configurations. The authors report reductions in inference latency and energy consumption while maintaining accuracy across several LLM benchmarks.\n\nWhile the idea of continuous relaxation for pruning configuration search is intriguing, the overall contribution is limited by heavy dependence on heuristic data, lack of theoretical justification for the latent-space mapping, and insufficient evaluation on real hardware platforms or against strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Conceptual Novelty: Reformulating discrete pruning as differentiable optimization is an appealing and underexplored direction. The proposed encoder–decoder pipeline represents a step toward automating pruning configuration search.\n\n+ Multi-Objective Framing: The method jointly considers model accuracy, latency, and energy efficiency, aligning with real deployment goals beyond parameter count.\n\n+ Breadth of Evaluation: Experiments span multiple LLMs and pruning ratios, with consistent improvements over naïve baselines."}, "weaknesses": {"value": "+ Dependence on Heuristic Data (Critical): Although Alps claims to overcome heuristic pruning, its foundation still rests on heuristic-derived “configuration–score” pairs. The framework effectively learns a regression over existing heuristic outcomes rather than discovering new principles. The performance and generality of Alps therefore hinge entirely on the diversity and quality of this pre-collected data, which the paper neither quantifies nor ablates.\n\n+ Lack of Theoretical Clarity: The latent continuous representation is presented as a black box. The paper provides no analysis of the embedding’s geometry, smoothness, or correlation with true pruning effectiveness. Without such evidence, it remains unclear whether the gradient-based optimization is meaningful or merely interpolating between existing heuristic points.\n\n+ Insufficient Efficiency and Cost Analysis: The authors emphasize latency and energy savings but omit end-to-end computational cost — both for data collection and model training. The claimed “efficiency” gains are potentially offset by this large upfront overhead. Furthermore, all results are reported on a single GPU (A40), which cannot substantiate claims about edge deployment or hardware adaptability.\n\n+ Missing Baseline Comparisons: Alps is compared mainly against heuristic methods, but not against recent learned or reinforcement-based pruning frameworks (e.g., AutoCompress, AdaPruner). Without such comparisons, it is difficult to assess the true competitiveness or scalability of the proposed approach.\n\n+ Interpretability and Reproducibility Concerns: The continuous space and optimization trajectory are opaque. The lack of interpretability makes the system difficult to trust or debug, especially since small representation shifts could yield vastly different pruning masks. Code and data availability are also not explicitly stated, raising questions about reproducibility."}, "questions": {"value": "In Lines 44–47, you argue that latency and energy are more critical than model size. However, memory constraints often dominate on real edge devices. Could you clarify this trade-off, ideally with supporting empirical evidence?\n\nThe efficiency results are limited to an NVIDIA A40 GPU. How does Alps generalize across architectures with different compute–memory trade-offs (e.g., mobile NPUs, Jetson, or CPU-based inference)?\n\nWhat is the total computational cost (in GPU-hours) of training the encoder–evaluator–decoder model, and how does it compare to conventional pruning search methods in both wall-clock time and energy usage?\n\nHave you performed any ablation to evaluate the sensitivity of Alps to the quality or quantity of the heuristic “configuration–score” data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e3Lt3lNo86", "forum": "5eqbDG2rKZ", "replyto": "5eqbDG2rKZ", "signatures": ["ICLR.cc/2026/Conference/Submission23899/Reviewer_PaWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23899/Reviewer_PaWe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622464839, "cdate": 1761622464839, "tmdate": 1762942846405, "mdate": 1762942846405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for pruning LLMS, which reformulates the pruning configuration optimization as a continuous optimization problem and solves the problem by gradient-based optimization method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe formulation is novel, which relaxes the LLM pruning problem by using discrete pruning ratio of each layer.\n2.\tInstead of only focusing on utility, the paper also includes efficiency and latency in the optimization problem."}, "weaknesses": {"value": "1.\tThe validity of formulation remains questionable. From my point of view, representing a layer by a single pruning ratio is over-simplified. More evidence is needed to prove the simplification works.\n2.\tThe paper claims high efficiency of the proposed method, but the process of collection pruning–score pairs consumes far more energy than other methods.\n3.\tThe continuity and reasonability of the representation space is confused.  The representation space is learned purely from discrete samples of traditional heuristics. Based on that, the gradient optimizer operates on the representation space, with no guarantee that it faithfully reflects the true pruning landscape."}, "questions": {"value": "1. Could you please provide more evidence about the alignment between predicted and the true performance?\n\n2. Could you further explain the choice of using LSTM? From my point of view, the pruning configuration sequence is not temporal; a simple MLP or transformer encoder may also work.\n\n3. As one of the main concern is the efficiency, and the number of heuristic points is highly related to the total cost. Could you please provide how sensitive is ALPS to the number of heuristic configurations used for training? \n\n4. I am quite curious about the stability of Gradient optimization. Does the gradient search in the learned space diverge or produce invalid configurations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tWnqNESdeo", "forum": "5eqbDG2rKZ", "replyto": "5eqbDG2rKZ", "signatures": ["ICLR.cc/2026/Conference/Submission23899/Reviewer_fzwn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23899/Reviewer_fzwn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708944908, "cdate": 1761708944908, "tmdate": 1762942846181, "mdate": 1762942846181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}