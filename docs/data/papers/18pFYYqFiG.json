{"id": "18pFYYqFiG", "number": 17301, "cdate": 1758274434261, "mdate": 1759897183764, "content": {"title": "Likelihood-free inference of phylogenetic tree posterior distributions", "abstract": "Phylogenetic inference, the task of reconstructing how related sequences evolved from common ancestors, is a central task in evolutionary genomics. \nThe current state-of-the-art methods exploit probabilistic models of sequence evolution along phylogenetic trees, by searching for the tree maximizing the likelihood of observed sequences, or by estimating the posterior of the tree given the sequences in a Bayesian framework. \nBoth approaches typically require to compute likelihoods, which is only feasible under simplifying assumptions such as independence of the evolution at the different positions of the sequence, and even then remains a costly operation.\nHere we present Phyloformer 2, a likelihood-free inference method for posterior distributions over phylogenies, trained end-to-end from sequences to trees.\nPhyloformer 2 exploits a novel encoding for pairs of sequences that makes it more scalable than previous approaches, and a parameterized probability distribution factorized over a succession of subtree merges. \nThe resulting network outperforms both state-of-the-art maximum likelihood methods and a previous likelihood-free method for point estimation. \nIt opens the way to fast and accurate phylogenetic inference under realistic models of sequence evolution.", "tldr": "", "keywords": ["likelihood-free", "phylogenetics", "end-to-end learning", "neural posterior estimation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04aa1762877b622e84211322b01de21c0bb6ab3d.pdf", "supplementary_material": "/attachment/c274cb69482f7716fd6b68cfb58bb6e6d061b212.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Phyloformer 2, a novel deep learning framework for likelihood-free phylogenetic inference. This method directly estimates posterior distributions over phylogenetic tree topologies from multiple sequence alignments (MSAs). The model's architecture integrates two key components: EvoPF, an EvoFormer-inspired encoder that extracts informative pairwise and sequence-level embeddings, and BayesNJ, a neural posterior estimator that defines a probabilistic factorization over successive subtree merges.\nThe authors demonstrate that Phyloformer 2 outperforms traditional likelihood-based methods, such as IQ-TREE and FastTree, as well as the previous likelihood-free model, Phyloformer. Notably, it achieves this with a significant speedup of up to two orders of magnitude. A key advancement of this work is its ability to provide approximate posterior distributions, moving beyond the point estimates offered by many existing methods. By extending neural posterior estimation (NPE) to entire trees rather than just quartets, this paper presents a significant conceptual and methodological step forward for simulation-based inference in evolutionary genomics. The manuscript is clearly written and well-organized, with detailed explanations of the model architecture, training procedures, and evaluation protocols.\nMajor Concerns"}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The method delivers clear advantages in both accuracy and efficiency, outperforming state-of-the-art likelihood-based and neural network approaches while running orders of magnitude faster. The paper is clearly written, well-structured, and demonstrating the approach’s potential to substantially advance simulation-based inference using genomic data."}, "weaknesses": {"value": "1. Despite the promising results, I have several concerns regarding the practical applicability and scalability of the proposed method.\nGeneralizability to Longer Sequences: The model was trained and evaluated exclusively on sequences with a maximum length of 500 base pairs. It is unclear how the model's accuracy would be affected by longer alignments, which are common in phylogenomic studies. An analysis of the relationship between sequence length and performance is needed to assess the model's utility for more complex datasets.\n2. Scalability with an Increasing Number of Taxa: As shown in Figure 2, the model's error rate increases drastically when the number of taxa exceeds 100. This sharp decline in accuracy raises questions about the practical utility of Phyloformer 2 for the larger datasets frequently encountered in modern phylogenetic analyses.\n3. Lack of Validation on Empirical Data: The method was not applied to any empirical datasets. While performance on simulated data is strong, its effectiveness on real-world biological data—with all its inherent complexities and noise—remains untested. Validating the model on well-established empirical benchmarks is crucial to demonstrate its practical relevance.\n4. Implementation Limitations: The current implementation is limited to a maximum of 200 sequences. This constraint severely restricts its applicability for many research questions, which often involve hundreds or even thousands of taxa."}, "questions": {"value": "Could the authors discuss the challenges they anticipate in applying Phyloformer 2 to empirical datasets, which often contain missing data, duplication loss, etc?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "16sNCWhRb5", "forum": "18pFYYqFiG", "replyto": "18pFYYqFiG", "signatures": ["ICLR.cc/2026/Conference/Submission17301/Reviewer_7J8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17301/Reviewer_7J8h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760671931167, "cdate": 1760671931167, "tmdate": 1762927237129, "mdate": 1762927237129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission tackles an exciting and important problem in phylogenetic inference, namely solving the computational complexity induced by the likelihood function. By considering a likelihood-free learning approach, the submission contains the proposition of Phyloformer 2 which uses an attention network (EvoPF) to encode MSA data to the parameters of an approximate posterior distribution. Phyloformer 2 shows promising inference time results compared to baselines on synthetic datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The significance of the problem that this submission aims to solve is very big. Further, the authors cleverly modify the Evoformer architecture from the AlphaFold 2 paper in order to handle MSA data, resulting in the EvoPF. Also, the idea to do likelihood-free posterior inference is an interesting approach to probabilistic phylogenetic posterior inference -- a topic of interest in the ML community. Additionally Fig. 1 is ambitiously produced and adds some clarity to the proposed algorithm."}, "weaknesses": {"value": "There is not a single equation in the paper, which I suppose is not a general requirement, but they are missed in this submission. I have provided examples below where equations would have been needed, but, as a general note on the quality of writing and structure of the paper, the absence of equations and of use of paragraphs are key components that make the submission difficult to follow. There are *large*, excessive blank spaces surrounding Fig. 1 and Alg. 1, which further down-weights the quality of the structure of the paper. As I describe below, I find the description of the BayesNJ approach (I am currently unsure if BayesNJ refers to a method) in Sec. 3.2-3.3 overly generic to the extent that I find Sec. 3.2 obsolete. \n\n\nRegarding novelty, I think the construction of the distribution over merges is the same as the proposals used in the phylogenetic SMC algorithms (see e.g. Bouchard-Cotê et al. (2012), Moretti et al. (2022) or Koptagel et al. (2022)), and I am missing a comparison to the topology sampler in PhyloGFN (which also samples topologies by merging from leaves and up). As this is not the first likelihood-free phylogenetic inference algorithm (see e.g. Phyloformer) the novelty of Phyloformer 2 seems to lie in the EvoPF architecture. I think the submission should reformulate its contributions with this in mind. \n\nMy overall assessment is that this submission needs further work before it is ready for publication. The new EvoPF seems to be able to give promising inference time results, but the clarity of writing (structure of the text, use of paragraphs, inclusion of equations), a more refined communication of the methodological contributions and experiments on real data would greatly improve the submission. As such, I sincerely hope that the authors keep polishing their work and consider my (hopefully constructive) comments below!\n\nBelow follows in depth comments on the different section in the paper.\n\n# Suggestions to improve the abstract\n1. Why Phyloformer **2**? What is its predecessor?\n2. \"The first likelihood-free ... for posterior distributions\". \n\t2. a) Then \"a previous likelihood-free ... for point estimation\". What does point estimation mean in this setting? The output of the algorithm is a single phylogeny?\n\t2. b) \"Phyloformer 2 exploits a novel encoding for pairs of sequences that makes it more scalable than *previous approaches*\". But Phyloformer 2 is the first method of the kind? What mechanism of the algorithm is the encoding part and how is it comparable to non-LF methods?\n3. To me \"Under realistic models of sequence of evolution\" sends a conservative message, ironically suggesting that you do not use models that fit real data well but that are computationally realistic. I would rephrase it to emphasize that you are able to apply models that are more realistic *than the over-simplified models that are typically used in phylogenetic inference (like the JC model)*\n\n# Introduction\n\nIn line 46-53 the text is unfortunately confusing, contains statements that are not motivated and that are incorrect (due to imprecision). My confusion is mainly induced by the intro lacking structure (it is largely one dense paragraph), while this following statement lacks motivation:\n\n\"Across all these approaches, a major hurdle is the computational cost of evaluating the likelihood function which is required for numerical optimization in maximum likelihood estimators, and to compute acceptance probability in sampling strategies or the evidence lower bound (ELBO) objective to be maximized in variational inference.\" \n\nThere needs to be a citation or a stand-alone explanation here. Furthermore, the sentence needs to be made more precise: I would assume that what is costly in the ELBO is the likelihood computation? The prior is typically uniform over the tree space and an exponential distribution over branch lengths, and generating trees with SLANTIS in VaiPhy or with SBNs in VBPI is a forward pass through a Bayesian net -- in both I believe you obtain the likelihood of sampling the tree from the proposal simultaneously with the tree generation? So, what remains is the likelihood function, not the other terms in the ELBO. Hence the statement is inaccurate. A similar argument holds for the acceptance probability calculation. \n\nI am aware that the likelihood function is an obstacle for applying auto-differentation in phylogenetic inference through this paper https://arxiv.org/abs/2211.02168, so I suggest you read it and cite it here if you agree that it matches.\n\nDue to the lack of paragraphs in the intro, the subsequent sentence (line 49) sort of suggests that likelihood-based methods and/or Bayesian methods explore all tree-topologies, which is definitely not true. It is then suggested that the phylogenetic community has made \"this computation\" -- the brute-force exploration of the phylogenetic tree space -- feasible, which I also do not think is correct? At least for any interesting size of $n$? If I am wrong, please provide a reference.\n\n# Related work:\nThis is where Phyloformer (1) is first mentioned. Given that this submission proposes Phyloformer 2, I would have expected to see its predecessor mentioned earlier, and at this point of the submission I do still not see clearly why Phyloformer 2 is an appropriate name? This could be cleared out by repeating the mechanisms of Phyloformer 2 and distinguishing how it compares with its predecessor. E.g., is Phyloformer a likelihood free method? There is no mention of the use of a likelihood function when explaining PF.\n\nFurthermore, \"Given all correct pairwise distances, an existing algorithm (neighbor joining, NJ, Saitou & Nei, 1987) is guaranteed to reconstruct the correct tree, but the authors observed limited topological reconstruction accuracy[...]\" in this sentence, \"authors\" is pointing to Saitou and Nei, which, in conjunction with the absence of paragraphs in the text overall, led me to believe that the text had moved away from Phyloformer and was now concerned with NJ as a related work. So, as in Introduction, there is a great need to divide the text into multiple paragraphs to improve legibility and structure.\n\nSomething I am missing in this section is where BayesNJ (or the procedure of constructing an approximation over phylogenies) differs from the merge-based approximations using in sequential Monte-Carlo-based phylogenetics and in PhyloGFN. For instance, how is distribution $q_m$ here different from the learnable merge proposal distribution in Moretti et al. (2022), or in \\phi-SMC in Koptagel et al. (2022)? Is the difference mainly the different objective functions used to learn the parameters? A clear distinction to these approaches would also improve the exposition of BayesNJ.\n\n# 2.1 Notation\nThere is an inconsistent use of $N$ and N to denote the number of leaves. Also, in the introduction $n$ denotes the number of taxa.\n\nThe text inside the hyphen, line 133-136, is difficult to parse. \n\n\"the parameters output by this neural network\" --> outputted\n\nIn the KL divergence in line 152, the expectation is taken w.r.t. $p(x)$ which is not defined. Is this KL tractable? Probably not, so how is it \"generally\" minimized by simply minimizing the log-likelihood of the approximation on the samples generated by the model?\n\nIn line 159, what is the target average KL? If its the E[KL] term in 152, please give that term its own equation environment and reference it -- now target average KL is not defined.\n\nI appreciate Figure 1 a lot. There is a lot of blank space above it though. I would remove it order to account for the additional space needed when updating the text with paragraphs.\n\n# Section 3.\nAn overall comment on this section is that the explanations of the two new modules are overly generic to the extent that I cannot evaluate the feasibility of the methods. This is a pity since I was very excited to understand them. Below follow more detailed comments.\n\nMinor: please spell out EvoPF before introducing the abbreviation.\n\nReading section 3.2, I have difficulties understanding exactly what BayesNJ refers to? Is it a distribution, an algorithm or something else? As far as I can see, $q_m$ and $q_\\ell$ are not defined, so I am also struggling with understanding what distributions we are parameterizing here? In all I find this subsection obsolete.\n\nCould you explain how the $\\psi(x)$ is connected to the parameters of $q_m$ and $q_\\ell$? Preferably already in Section 3.2, but also in Section 3.3, for example in line 268: the \"softmin across pairwise scores\" formulation could instead be an equation which shows how $\\psi(x)$ (the scores?) is used to parameterize the distribution. I would recommend using an equation for it.\n\n\"We model the probability $q_s$ as a Beta distribution whose parameters are produced by a bilinear function of the embedding of the two merged nodes.\" This should again be an equation such that it is possible to grasp the relationship between $\\phi(x)$ and the parameters of the Beta distribution. Also, what is the bilinear function?\n\nThe statement in line 284 regarding the factor that comes from the \"determinant of the Jacobian of the change of variable\" is not possible to understand without an equation, in my opinion. Could you write this out mathematically, please?\n\nLines 299-302 can be formulated using an equation instead which would make it easier to understand the greedy MAP approximation. Regarding the name \"greedy MAP approximation\", I am slightly confused: is this a greedy algorithm for approximating the MAP, is the MAP a greedy approximation or is the \"greedy MAP\" an approximation of something? In line 404 later, they are only referred to as greedy-MAP trees, not greedy MAP approximation trees.\n\n# Section 4\n\"In order to fairly compare Phyloformer 2 (PF2) and the original Phyloformer (PF), we trained a version of Phyloformer 2 over the dataset used in Nesterenko et al. (2025) to evaluate PF\". Use the introduced abbreviations consistently here.\n\nI couldn't understand this: \"≈ 170, 000 50-taxa tree/MSA pairs\". Does it mean to say that there are 170,000 trees, each with $N=50$ taxa?\n\nLG+GC is not defined or referenced. I could not find a definition of it in the Phyloformer paper either, and I have not heard of it before. Since all substitution models can be formulated mathematically, I would heavily advise to actually write it out so that the reader can evaluate the complexity of using this model instead of, e.g., the JC model which is heavily employed in the phylogenetic inference papers published in the machine learning community.\n\nPF2_{\\ell_1} is not defined.\n\n\"Kuhner-Felsenstein distances\" are not defined in the text, and can arguably not be taken for granted as common knowledge -- how is it different from RF? In contrast the more standard RF distance is referenced.\n\nLine 437: KF is not defined.\n\nLine 449: LG+G8 is not defined (same comment as above regarding the mathematical formulation of the model.)\n\n\n# Typos and necessary fixes\n\n* The legibility of the introduction would greatly benefit from more structure, like using paragraphs (I mean here blank lines in tex, not \\paragraph). For instance you could break up the dense chunk of text in the first paragraph with a newline at line 41.\n* When mentioning VI-based approaches, I think VaiPhy needs credit as it was the first mean-field VI algorithm for phylogenies. \n* line 55 imped -> impede\n* line 68, a NPE -> an NPE\n* \"EvoFormer module used in Alphafold 2\" needs citation.\n*  byNesterenko et al. (2025) -->  by Nesterenko et al. (2025) \n* $\\ell_1$ is a notational clash with the branch length sets. You could use $L_1$?\n* Typo line 424: \"a slightly better topological accuracies\""}, "questions": {"value": "Given access to real data, how would you utilize it in order to get better posterior approximation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WZp2ffcsTS", "forum": "18pFYYqFiG", "replyto": "18pFYYqFiG", "signatures": ["ICLR.cc/2026/Conference/Submission17301/Reviewer_Jzst"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17301/Reviewer_Jzst"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827461173, "cdate": 1761827461173, "tmdate": 1762927236667, "mdate": 1762927236667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Phyloformer 2, a neural posterior estimation (NPE) method for phylogenetic inference. The approach combines two components: EvoPF, an encoder that transposes AlphaFold's EvoFormer architecture to process multiple sequence alignments at the sequence-pair level rather than position-pair level, and BayesNJ, a factorized posterior distribution over tree topologies and branch lengths. The authors demonstrate improved topological accuracy and computational speed compared to existing likelihood-based and likelihood-free methods on simulated datasets. The work represents an interesting application of modern deep learning to phylogenetic inference, though several fundamental limitations warrant careful consideration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear methodological contribution: The factorization of the posterior over successive merges (BayesNJ) is conceptually elegant and provides a tractable way to define distributions over tree space, addressing a genuine challenge in neural posterior estimation for discrete structures.\n    \n2. Comprehensive experimental evaluation: The paper includes comparisons with multiple established methods (IQTree, FastTree, FastME, Phyloformer) across various metrics and provides ablation studies distinguishing the contributions of EvoPF and BayesNJ."}, "weaknesses": {"value": "1. Computational cost accounting is incomplete: While the paper emphasizes inference speed advantages, the amortization trade-off deserves fuller discussion. Training requires generating ~170,000 simulated tree/MSA pairs plus additional fine-tuning data. Although forward simulation is cheaper than likelihood evaluation, the total computational budget (simulation + GPU training time) should be reported. The paper should clarify: (a) total training wall-clock time, (b) the break-even point where training cost is amortized over multiple inference tasks, and (c) computational requirements when adapting to new priors or models.\n    \n2. Strong distributional assumptions not prominently disclosed: The method requires training data from a specific prior and evolutionary model. While this is mentioned (lines 83-89, 485-489), phylogenetics researchers unfamiliar with NPE may not immediately recognize this as a fundamental constraint rather than a technical detail. A brief statement in the abstract or introduction highlighting that the method is 'model-specific and requires retraining for different evolutionary scenarios' would help set appropriate expectations. This trade-off—fast amortized inference at the cost of model commitment—should be positioned as an inherent characteristic of the NPE paradigm rather than a limitation unique to this work.\n    \n3. Scalability claims require context: The paper demonstrates clear scalability improvements over Phyloformer: memory usage is reduced by approximately 50% (Figure 2c), and the method can handle 300 sequences of length 250—a regime inaccessible to the original Phyloformer. However, contextualization against likelihood-based methods would strengthen this claim. How do these limits compare to typical problem sizes in modern phylogenetics? For instance, IQTree can handle thousands of sequences, though at much higher computational cost. Clarifying whether PF2's current scalability addresses common use cases or remains limited to smaller-scale problems would help readers assess practical applicability.\n    \n4. Limited validation of posterior quality: Figure S.2 shows differences between PF2 and RevBayes posterior samples, with PF2 producing more diffuse bipartition probabilities. The authors suggest RevBayes might be miscalibrated, but provide no evidence for this claim. Proper calibration assessment (e.g., using simulation-based calibration) is needed to validate the posterior approximation quality.\n    \n5. Novelty of EvoPF module is overstated: The module is described as \"novel\" but is explicitly acknowledged as \"inspired by\" (i.e., adapted from) AlphaFold2's EvoFormer with \"a few simplifications.\" The contribution appears to be primarily an architectural adaptation rather than a fundamental innovation. This should be characterized more accurately."}, "questions": {"value": "* Training cost transparency and amortization analysis: Can you provide (a) total wall-clock time and computational resources for training across all experiments, (b) a break-even analysis showing how many inferences are needed to recover training costs, and (c) the cost of adapting to new evolutionary models? For a research project analyzing 100 datasets under the same model, how does the total computational budget compare to using likelihood-based methods?\n\n* Out-of-distribution performance quantification: Have you systematically evaluated performance when test data comes from evolutionary models not seen during training, or when the true prior differs from the training prior? The current treatment of this critical limitation is too brief. Quantitative results showing degradation patterns would help users understand when the method should or shouldn't be applied.\n\n* Canonical ordering justification and alternatives: The canonical merge ordering (Section 3.2) is presented as necessary to avoid summing over all possible orderings. However, couldn't a permutation-equivariant architecture potentially avoid this constraint while maintaining tractability? Have you explored whether relaxing this ordering requirement might improve posterior expressivity, even if it increases computational cost?\n\n* Posterior calibration methodology: Beyond the single-alignment comparison with RevBayes (Figure S.2), have you performed systematic calibration checks? For example, simulation-based calibration where you: (1) sample parameters from the prior, (2) simulate data, (3) obtain posterior samples from PF2, (4) verify that true parameters are uniformly distributed in posterior quantiles? This would help determine whether the diffuse posteriors reflect uncertainty or miscalibration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O7Jikq8Oq2", "forum": "18pFYYqFiG", "replyto": "18pFYYqFiG", "signatures": ["ICLR.cc/2026/Conference/Submission17301/Reviewer_3zMD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17301/Reviewer_3zMD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990419302, "cdate": 1761990419302, "tmdate": 1762927236313, "mdate": 1762927236313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}