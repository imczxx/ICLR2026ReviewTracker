{"id": "BjIfaIbXEJ", "number": 6854, "cdate": 1757998337744, "mdate": 1763056302153, "content": {"title": "Risk-adaptive Activation Steering for Safe Multimodal Large Language Models", "abstract": "One of the key challenges of modern AI models is ensuring that they provide helpful responses to benign queries while refusing malicious ones. But often, the models are vulnerable to multimodal queries with harmful intent embedded in images. One approach for safety alignment is training with extensive safety datasets at the significant costs in both dataset curation and training. Inference-time alignment mitigates these costs, but introduces two drawbacks: excessive refusals from misclassified benign queries and slower inference speed due to iterative output adjustments. To overcome these limitations, we propose to reformulate queries to strengthen cross-modal attention to safety-critical image regions, enabling accurate risk assessment at the query level. Using the assessed risk, it adaptively steers activations to generate responses that are safe and helpful without overhead from iterative output adjustments. We call this Risk-adaptive Activation Steering (RAS). Extensive experiments across multiple benchmarks on multimodal safety and utility demonstrate that the RAS significantly reduces attack success rates, preserves general task performance, and improves inference speed over prior inference-time defenses.", "tldr": "", "keywords": ["Multimodal large language models", "jailbreakattacks", "inference-time alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d591be3c0c36a2971c3375fe5a05a13cdfd028a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Risk-adaptive Activation Steering (RAS), an inference-time method to improve multimodal LLM safety by reformulating queries, estimating risk via early-token similarity, and then adaptively steering activations toward refusal responses. The method aims to avoid high compute cost and over-refusal problems in prior safety-alignment approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Targets an important problem: scalable safety for MLLMs without retraining.\n- Combines visual prompting, distribution-based risk scoring, and activation steering into a structured pipeline.\n- Empirically effective: reduces attack success rate while preserving utility and improving inference speed.\n- Ablation analysis supports core design choices (e.g., γ, early token count).\n- Implementation is architecture-agnostic and training-free."}, "weaknesses": {"value": "- Novelty is modest; core mechanism mainly integrates known techniques (vision prompts + logit similarity + activation steering).\n- Significant reliance on prompting heuristics raises concerns about stability and generalization.\n- Risk estimation based on early-token distributions lacks theoretical grounding and may be brittle in long-chain reasoning.\n- Query reformulation may introduce failure cases on fine-grained visual tasks or adversarially perturbed images.\n- Experiments focus on specific open-source models; unclear generalization to stronger proprietary models."}, "questions": {"value": "1. How sensitive is RAS to prompt phrasing and visual context formatting?  \n2. Does early-token risk scoring degrade for long multi-turn or chain-of-thought interactions?  \n3. Can the authors provide concrete failure cases where benign queries receive high risk scores?  \n4. How does RAS interact with RL-aligned models (e.g., GRPO-trained MLLMs)?  \n5. Would multi-layer activation steering improve robustness vs. last-layer steering only?  \n6. Does the method handle image-based adversarial triggers or generated visual attacks?  \n7. Can the authors report detailed latency breakdown per stage, not only tokens-per-second?  \n8. How does query reformulation behave on fine-grained grounding tasks where added context may confuse perception?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pYkfOVm0Us", "forum": "BjIfaIbXEJ", "replyto": "BjIfaIbXEJ", "signatures": ["ICLR.cc/2026/Conference/Submission6854/Reviewer_dkiV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6854/Reviewer_dkiV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803315401, "cdate": 1761803315401, "tmdate": 1762919110227, "mdate": 1762919110227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "lIxU4BKJqE", "forum": "BjIfaIbXEJ", "replyto": "BjIfaIbXEJ", "signatures": ["ICLR.cc/2026/Conference/Submission6854/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6854/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763056301341, "cdate": 1763056301341, "tmdate": 1763056301341, "mdate": 1763056301341, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation of Multimodal Large Language Models (MLLMs): their vulnerability to malicious intent embedded in images (e.g., hidden harmful instructions) despite textual safety alignment. It identifies two core challenges with existing methods: training-based approaches are costly in data curation and computation, while inference-time methods either over-refuse benign queries (via safety prompts) or slow inference (via iterative response refinement). To solve these, the authors propose Risk-adaptive Activation Steering (RAS), an inference-time defense that operates in three stages: (1) vision-aware query reformulation appends concise visual summaries and safety prompts to strengthen cross-modal attention to safety-critical image regions; (2) exponentially weighted risk evaluation computes continuous risk scores by measuring similarity between the reformulated query’s output distribution and \"refusal prototypes\" (derived from unsafe text queries); (3) risk-adaptive activation steering adjusts model activations proportionally to the risk score—minimizing interference with benign queries while steering unsafe ones toward refusals. Extensive experiments across benchmarks (e.g., MM-SafetyBench, SPA-VL for safety; Sci-QA, MM-Vet for utility) and models (LLaVA-1.5, Qwen-VL-Chat) show RAS reduces attack success rates (ASR) by up to 89.5%, preserves general task performance, and improves inference speed compared to baselines like FigStep and ETA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Targeted Diagnosis of MLLM Safety Gaps: The paper provides analysis of why MLLMs fail at multimodal safety, which is insufficient cross-modal attention to safety-critical image regions. Using attention maps and Fisher Discriminant Ratio metrics, it demonstrates that models ignore harmful visual content but attend to harmful text (e.g., \"bomb\" in text), laying a strong theoretical foundation for RAS.\n- Efficiency and Precision in Inference-Time Alignment: RAS avoids the pitfalls of prior methods: it eliminates iterative response refinement and uses continuous risk scores (instead of binary \"safe/unsafe\" judgments) to adapt intervention strength. This design ensures minimal interference with benign queries (utility preservation) while effectively blocking malicious ones, which is a balance missing in baselines like CoCA (over-reliance on logit calibration) or ECSO (iterative self-evaluation)."}, "weaknesses": {"value": "- **Limited Analysis of Visual Context Generation**: The paper mentions using a model to generate \"concise visual contexts\" (e.g., \"The image shows illegal drug equipment\") but provides little detail on how this generator is trained, validated, or its potential biases. If the visual context generator misdescribes safety-critical content (e.g., fails to identify a hidden weapon), RAS’s risk evaluation would be compromised, which is not explored.\n- **Lack of Adversarial Robustness Testing for RAS Itself**: The paper evaluates RAS against existing jailbreak benchmarks (e.g., FigStep’s typographic prompts) but does not test whether adversaries could bypass RAS. For example, an attacker might manipulate images to confuse the visual context generator or craft queries that lower risk scores artificially, there is no analysis of such adaptive attacks is provided.\n- **Oversimplification of \"Safety-Critical\" Visual Regions**: The paper focuses on discrete, object-like safety risks (e.g., bombs, drugs) but neglects ambiguous or contextual harms (e.g., images of self-harm, subtle hate symbols). It is unclear whether RAS can detect such nuanced visual threats, as the current risk evaluation relies on similarity to \"refusal prototypes\" trained on explicit unsafe text."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KaQwsQUDXH", "forum": "BjIfaIbXEJ", "replyto": "BjIfaIbXEJ", "signatures": ["ICLR.cc/2026/Conference/Submission6854/Reviewer_x5Ew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6854/Reviewer_x5Ew"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819522006, "cdate": 1761819522006, "tmdate": 1762919109799, "mdate": 1762919109799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Risk-adaptive Activation Steering (RAS), an innovative inference-time alignment method aimed at enhancing the safety of multimodal large language models by dynamically adjusting visual attention to safety-critical regions in images. RAS addresses the challenge of ensuring that models provide helpful responses to benign queries while effectively refusing harmful ones. Through extensive experiments across various benchmarks, the authors demonstrate that RAS significantly reduces attack success rates without compromising general task performance or inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed RAS method presents a novel way to enhance safety in multimodal large language models.\n2. The paper includes extensive experiments across multiple benchmarks, demonstrating the effectiveness of the proposed method.\n3. RAS significantly reduces attack success rates while maintaining general task performance, showcasing a strong balance between safety and utility while remaining efficient during inference.\n4. The focus on strengthening visual attention to safety-critical regions addresses a critical gap in existing multimodal models."}, "weaknesses": {"value": "The paper’s main weaknesses lie in its reliance on synthetic unsafe data, heuristic and unvalidated risk calibration, limited evaluation on real-world and diverse multimodal harms, and limited interpretability - related analysis."}, "questions": {"value": "1. The approach focuses on enhancing visual attention to harmful regions in images. However, there is a scenario where both the image and the question are safe, while the models will still give harmful responses [1]. In this situation, does the proposed method still remain effective? It would be beneficial to explore where attention should be emphasized in such cases. Could you provide an explanation or example regarding this specific scenario? \n2. The evaluation of safety benchmarks primarily focuses on ASR. It would be better to include results on the helpfulness of the model’s responses to demonstrate that the model does not excessively refuse benign queries.\n[1] Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VrpNftRgfO", "forum": "BjIfaIbXEJ", "replyto": "BjIfaIbXEJ", "signatures": ["ICLR.cc/2026/Conference/Submission6854/Reviewer_PYRi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6854/Reviewer_PYRi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882660740, "cdate": 1761882660740, "tmdate": 1762919109425, "mdate": 1762919109425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Risk-adaptive Activation Steering (RAS), an inference-time framework for multimodal LLMs. The framework is divided into three stages. It appends a fixed safety prompt and a brief visual description of the image to the user's query, focusing attention on safety-critical regions. Using the reformulated input, it compares the output distributions of the first N tokens to unsafe prototypes to get a similarity and maps it to a risk score. When generating, it steers the last-layer activations of those first tokens toward the unsafe prototype by an amount proportional to the risk. This single-pass, early-token intervention substantially reduces jailbreak success while preserving utility and avoiding the overhead of other methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors introduce a novel framework for combining a safety-prompt + visual-context risk pass with risk-adaptive, early-token steering in a multimodal model. By design, the method considers both safety and utility. The authors support the design choices with preliminary experiments. The paper offers a good trade-off between utility and throughput compared to state-of-the-art methods."}, "weaknesses": {"value": "I’m trying to understand the choice of N = 3, which seems somewhat restrictive. Is the strong performance mainly because the steer acts on early refusal templates like “I’m sorry,” effectively skewing many outputs to start that way? If so, I worry this could bias the model toward refusal and leave borderline cases unanswered. Could you comment on this? It seems N has a minor effect in the ablations because gamma is already fixed and could be ablated per se. This could also explain the marginal gains over “binary” in the ablations. \n\nThe paper benchmarks mainly against prompting and judge-and-regenerate defenses, but provides limited head-to-head comparisons with closely related activation-steering methods (for example: Steering Away from Harm: An Adaptive Approach to Defending Vision\nLanguage Model Against Jailbreaks)."}, "questions": {"value": "My understanding is that the visual description, along with the safety prompt, is used only for risk evaluation, while the final answer is generated from the original image and query with risk-adaptive steering. Is this correct? For completeness, it would be helpful to include stage-1 ablations that isolate the safety prompt and the visual context.\n\nIn Figure 6, many of the unsafe examples are not covered by the sigmoid. Could you provide an explanation of this? \n\nCould you please provide an explanation of why ECSO has better relative throughput in Figure 7?\n\nPlease provide details about the weakness highlighted above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The concerns were addressed in the Ethics Statement Section of the paper."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EeeJosgjSt", "forum": "BjIfaIbXEJ", "replyto": "BjIfaIbXEJ", "signatures": ["ICLR.cc/2026/Conference/Submission6854/Reviewer_Cj85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6854/Reviewer_Cj85"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075336604, "cdate": 1762075336604, "tmdate": 1762919108978, "mdate": 1762919108978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}