{"id": "OHaFgEa0yZ", "number": 22475, "cdate": 1758331623705, "mdate": 1759896864293, "content": {"title": "Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes", "abstract": "Reinforcement learning (RL) has proven remarkably effective at improving the accuracy of language models in verifiable and deterministic domains like mathematics. Here, we examine if current RL methods are also effective at optimizing language models in verifiable domains with stochastic outcomes, like scientific experiments. Through applications to synthetic data and real-world scientific experiments, we demonstrate that Group Relative Policy Optimization (GRPO) induces overconfident probability predictions for binary stochastic outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out (RLOO) yield well-calibrated models. We show that removing group standard normalization in GRPO fixes its miscalibration and provide a theoretical explanation for why normalization causes overconfidence. Our results provide new evidence against the use of standard normalization in GRPO and help pave the way for applications of RL for reasoning language models beyond deterministic domains.", "tldr": "", "keywords": ["GRPO", "Reinforcement Learning", "Calibration", "Reasoning", "Language Models", "LLM", "RLVR", "PPO", "RLOO", "Perturb-seq"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c420df51cdd97c1de401911536045329136125c.pdf", "supplementary_material": "/attachment/e68dff8f490179e3e9e8cfbc0e7b39bc3b351cb2.zip"}, "replies": [{"content": {"summary": {"value": "The paper claims that standard GRPO makes language models become overconfident, and compares with PPO and RLOO with several experiments. Emprically, it shows that on multiple datasets, GRPO creates overconfidence, while PPO, RLOO, and GRPO w/o std norm can keep model calibration. Theoretically, it shows that the expected average of standard GRPO is biased, and creates a positive feedback loop that pushes the model to become more extreme in its predictions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper shows a clear experiment result that GRPO causes model overconfidence on three different datasets.\n- They show that a simple fix can be done by removing the standard‑normalization in GRPO.\n- They also give a theoretical explanation that standard GRPO's advantage estimation biases updates toward overconfident predictors."}, "weaknesses": {"value": "- The main problem is that this paper does not include a related work section. There is a lack of discussion of previous literature on model calibration, different post-training methods, etc.\n- In fact, many previous papers have pointed out that other post-training methods (PPO, DPO) can also lead to model overconfidence, which conflicts with this paper's experiments. The authors do not discuss these relevant works and their connections with this paper.\n- The experiment lacks enough generality. The authors only experiment with Qwen3-4B, without considering other architectures, such as OctoThinker based on Llama models. Dataset-wise, the authors only use three uncommon datasets for evaluation, without reporting on reasoning benchmarks such as MATH, AIME-24/25, etc."}, "questions": {"value": "How sensitive are results to group size G? Your theoretical analysis assumes a large group size G, but you use a small G=4 in your CRISPR experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3TDewAunpQ", "forum": "OHaFgEa0yZ", "replyto": "OHaFgEa0yZ", "signatures": ["ICLR.cc/2026/Conference/Submission22475/Reviewer_GDzm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22475/Reviewer_GDzm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760905855139, "cdate": 1760905855139, "tmdate": 1762942232711, "mdate": 1762942232711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors claim that Group Relative Policy Optimization (GRPO) produces overconfident probability estimates when trained with a log-likelihood reward, whereas PPO and RLOO yield well-calibrated predictions.\n\nExperiments are conducted on synthetic probability prediction, a CRISPR biological dataset, and the MedMCQA medical question-answering dataset. Across all tasks, GRPO models show poor calibration (high ECE), even though classification accuracy remains similar. The authors attribute this issue to the group standard normalization in GRPO’s advantage estimate, arguing theoretically that it introduces policy-dependent bias amplifying overconfidence."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Interesting empirical observation. The finding that GRPO causes overconfidence in stochastic prediction tasks is novel and may interest researchers exploring calibration or uncertainty in RL algorithms.\n\n2. Practical implication. The fix—removing standard normalization—is simple and easy to test. If correct, it could inform best practices for future reasoning RL work.\n\n3. Link theory and practice. The discussion about group normalization introducing a bias in the advantage estimate points toward a principled mechanism, not just an empirical finding."}, "weaknesses": {"value": "1. Limited originality and contribution. The main claim—removing normalization from GRPO improves behavior—closely parallels Dr. GRPO [1], which already proposed removing the same term. The paper essentially re-validates known ideas rather than introducing a distinct algorithmic or theoretical innovation. Calibration of GRPO is certainly a bug worth fixing, but the contribution is incremental—basically a note explaining why an already known fix works—rather than delivering a substantial new methodological or conceptual advance.\n\n2. Shallow theoretical analysis. The “theoretical explanation” seems mostly heuristic: it sketches the bias term but does not mathematically prove the direction or magnitude of the effect. Many simplifying assumptions severely undermine rigor. \n\n3. Weak experimental design. The synthetic dataset is a toy-like one and nearly trivial; calibration differences there do not convincingly extend to real settings. The Qwen 3‑4B experiments use very small training sizes, few seeds, and no standard deviation reporting. Metrics are reported without statistical significance or uncertainty intervals. \n\n4. Writing and presentation issues. The paper is poorly organized, with the main content spanning only seven and a half pages; it is more like a lab note rather than a polished paper.\n\n[1] Liu, Zichen, et al. \"Understanding r1-zero-like training: A critical perspective.\" arXiv preprint arXiv:2503.20783 (2025)."}, "questions": {"value": "Please refer to the **Weaknesses** part. \n\nI am looking forward to the authors’ response, and may reconsider this paper based on that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vHUUFgFsmb", "forum": "OHaFgEa0yZ", "replyto": "OHaFgEa0yZ", "signatures": ["ICLR.cc/2026/Conference/Submission22475/Reviewer_2txt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22475/Reviewer_2txt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761583232555, "cdate": 1761583232555, "tmdate": 1762942232480, "mdate": 1762942232480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies 3 types of RL algorithms, PPO, RLOO, and GRPO (with and without group std normalization), for uncertainty estimation in stochastic settings. By evaluating across 3 different scenarios, synthetic, scientific, and medical data, the paper shows that GRPO with group std normalization induces overconfident probability prediction for categorical stochastic outcomes. The paper also provides a theoretical explanation that the std normalization introduces a policy-dependent bias in the advantage estimation, which over-reinforces already confident predictions. A simple solution is to remove the std term."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-written and easy to follow. The motivation is well-supported by evidence, including visualizations of the miscalibration and explanations.\n- The experimental setup is clear. Although the experiments are not large-scale RL, they cover 3 different datasets and clearly demonstrate the overconfidence phenomenon across different stochastic settings."}, "weaknesses": {"value": "While I appreciate that the authors provide a new perspective on the impact of group std normalization from the lens of uncertainty and overconfidence, the theoretical discussion itself does not bring substantial new insights. Similar ideas, namely that group std normalization can lead to overconfidence, and the corresponding solution of removing the term have also been discussed in [1]. Furthermore, the role of group std normalization has been extensively discussed in prior work, such as Dr. GRPO [2] (discussed in the main text) and other related works [3, 4]. In addition, recent large-scale RL works have also moved away from using group std normalization [5, 6, 7], adopting batch-level normalization or removing the std term entirely. Although the motivations may differ, these existing words nonetheless limit the practical contribution of this paper.\n\n[1] Outcome-based Reinforcement Learning to Predict the Future, arxiv 2025\n\n[2] Understanding R1-Zero-Like Training: A Critical Perspective, COLM 2025\n\n[3] GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning, arxiv 2025\n\n[4] REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models, arxiv 2025\n\n[5] The Art of Scaling Reinforcement Learning Compute for LLMs, arxiv 2025\n\n[6] Magistral, arxiv 2025\n\n[7] Kimi k1.5: Scaling Reinforcement Learning with LLMs, arxiv 2025"}, "questions": {"value": "The main text frequently refers to the appendix for key information. Since there appears to be space left in the main body, it might be better to move some of those into the main text to improve readability.\n\nOverall, while this is a clearly written paper that demonstrates a real and relevant phenomenon, its scope and theoretical discussion are quite narrow, focusing almost entirely on the group std term. Moreover, it does not provide a substantially new or meaningful solution, as simply removing the std term has already been widely adopted in recent RL works. Given the current state of the field, where this topic has already been extensively studied (albeit from different perspectives) and addressed, the paper feels somewhat outdated in scope and timing, and its contribution therefore appears incremental."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FlYkRCU8Ho", "forum": "OHaFgEa0yZ", "replyto": "OHaFgEa0yZ", "signatures": ["ICLR.cc/2026/Conference/Submission22475/Reviewer_epXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22475/Reviewer_epXW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947306702, "cdate": 1761947306702, "tmdate": 1762942232188, "mdate": 1762942232188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how reinforcement learning methods for training reasoning language models behave when predicting stochastic outcomes, rather than deterministic domains like math. The authors compare GRPO, PPO, and RLOO across synthetic probability prediction tasks, biological perturbation data, and a medical multiple-choice QA dataset. They find that GRPO produces highly overconfident and poorly calibrated probability estimates, whereas other RL algorithms yield well-calibrated results. The paper attributes the miscalibration to GRPO’s group standard normalization step and demonstrates that removing this normalization substantially improves calibration. A theoretical analysis supports that standard normalization introduces a policy-dependent bias that reinforces overconfident predictions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly identifies and isolates the role of group standard normalization in GRPO, providing both empirical and theoretical evidence that this design choice induces systematic overconfidence in stochastic decision settings.\n\n- The experimental evaluation spans synthetic, biological, and clinical knowledge domains, demonstrating that the observed miscalibration behavior persists across qualitatively different tasks and data regimes.\n\n- The theoretical explanation of how standard normalization biases the advantage estimate is well-motivated and aligns with the empirical findings, strengthening the validity and interpretability of the presented results."}, "weaknesses": {"value": "- The empirical evaluation relies only on a single model (Qwen3-4B) across all experiments, which makes it difficult to determine whether the observed calibration differences generalize beyond this specific architecture and scale. Including additional models would substantially strengthen the empirical claims. Furthermore, some details of the experimental setup are under-specified in the main text.\n\n- The model is required to generate explicit natural language tokens to represent  probability, rather than deriving numeric probabilities from token logits. The motivation for this design choice is not sufficiently discussed, and the direct generation approach may conflate reasoning about uncertainty with format adherence. Extracting probabilities from token logits is a common baseline in LM calibration and uncertainty estimation research, including this would provide more comprehensive results.\n\n- The practical benefits of calibrated stochastic outcomes are not fully demonstrated in the presented tasks, since accuracy remains comparable across all methods. While the paper emphasizes calibration as the primary evaluation signal, it is not clearly shown how improved calibration leads to different or better decision-making in downstream applications. Stronger justification or concrete use cases illustrating how calibrated uncertainty materially improves task outcomes would help clarify the broader impact."}, "questions": {"value": "- It would be helpful to elaborate in more detail what is meant by using the log-likelihood of the observed answer under the model’s predicted probability as the reward (Line 151~153). Since this is central to the RL setup, providing a mathematical expression for the reward function in the experiments section would make the methodology clearer for readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ABpCnaVIi", "forum": "OHaFgEa0yZ", "replyto": "OHaFgEa0yZ", "signatures": ["ICLR.cc/2026/Conference/Submission22475/Reviewer_hkZG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22475/Reviewer_hkZG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762503854031, "cdate": 1762503854031, "tmdate": 1762942231898, "mdate": 1762942231898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}