{"id": "Xhf9YqwlM4", "number": 18828, "cdate": 1758291228574, "mdate": 1759897079042, "content": {"title": "Tokenisation over Bounded Alphabets is Hard", "abstract": "Recent works have proven tokenisation to be NP-complete. \nHowever, their proofs' constructions rely on tokenisation being applied to inputs with alphabets of unbounded cardinality, which does not accurately reflect the real world.\nIndeed, since practical applications of tokenisers involve fixed-size alphabets (e.g., Unicode or bytes), the implications of such a statement may be challenged.\nIn this work, we examine the computational complexity of tokenisation over bounded alphabets, considering two variants of this problem: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations (in bottom-up tokenisation) or a vocabulary (in direct tokenisation) whose application compresses a dataset to at most $\\delta$ symbols.\nWhen alphabets are bounded to have only 2 characters, we do not only prove that bottom-up and direct tokenisation are NP-complete, but also that there is no polynomial-time approximation scheme for either of these problems (unless P = NP).\nFurthermore, even when alphabets are bounded to contain a single character, we can still prove the NP-completeness of direct tokenisation.\nAlthough the single-character case is not practical on its own, proving hardness results for an $n$-ary alphabet allows us to prove the same results for alphabets of any larger size.\nWe thus conclude that direct tokenisation over any alphabet is NP-complete, and that both bottom-up and direct tokenisation do not admit polynomial-time approximation schemes for any alphabet of size 2 or larger.", "tldr": "We prove that selecting a tokeniser which maximises a dataset's compression is NP-complete and does not admit a PTAS (unless P=NP), even when their inputs are defined on a binary alphabet.", "keywords": ["Tokenisation", "tokenization", "language modelling", "compression", "LLM", "NLP"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/064e10c502c94f4c389352e64c8b2de07e6a0315.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the computational complexity of tokenisation when restricted to bounded alphabets, addressing a critical gap in prior research that focused on unbounded alphabets.\nThe authors analyze both direct and bottom-up tokenisation and demonstrate that these problems remain NP-complete even for binary and unary alphabets. Furthermore, they prove that neither admits a polynomial-time approximation scheme (PTAS) unless P = NP.\nThrough reductions from classic NP-hard problems (such as 3-OCC-MAX2SAT and Vertex Cover), the paper rigorously establishes the intractability of tokenisation even under practical, real-world constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The connection drawn between tokenisation and classical NP-hard problems is conceptually elegant and insightful.\n\nThe paper tackles a relevant and previously unresolved problem, making a clear theoretical contribution to both computational complexity and NLP.\n\nThe authors thoughtfully discuss implications for practical tokenisation algorithms, encouraging further research on approximation and relaxation methods."}, "weaknesses": {"value": "The discussion of approximation hardness constants (e.g., very tight lower bounds) could be expanded to give more intuition on their practical implications."}, "questions": {"value": "Could the authors clarify whether their hardness results extend to probabilistic or heuristic tokenisation schemes commonly used in NLP (e.g., BPE variants with stochastic merges)?\n\nDo the reductions rely crucially on compression-based objectives, or might similar hardness results hold for alternative objectives like frequency balancing or entropy minimization?\n\nGiven the established hardness, do the authors foresee any provably efficient approximation algorithms for special cases (e.g., very small datasets or restricted merge operations)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LrHzwtX8BU", "forum": "Xhf9YqwlM4", "replyto": "Xhf9YqwlM4", "signatures": ["ICLR.cc/2026/Conference/Submission18828/Reviewer_gmyW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18828/Reviewer_gmyW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760898597322, "cdate": 1760898597322, "tmdate": 1762930800171, "mdate": 1762930800171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the computational complexity of bottom-up and direct tokenisation over bounded alphabet. The authors develop elaborate proofs to show that this problem is NP-complete even with alphabets consisting only of 2 characters. Also they show that there is no polynomial time approximation unless P=NP. The main optimization function is compression. The paper contains elaborate formal definitions of the problems addressed and provides elaborate technical details underlying the main results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents rigorous proofs for tokenisation over bounded alphabets with compression as the optimization function."}, "weaknesses": {"value": "In my opinion, this paper will not be of interest to the main ICLR community and is only of theoretical interest with no clear practical impact on learning or NLP."}, "questions": {"value": "How does this work fit under learning representation? \nThere are already heuristics to perform tokenisation which work very well in practice with no theoretical guarantees. How does this work impact the existing vast literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dcaP3uForJ", "forum": "Xhf9YqwlM4", "replyto": "Xhf9YqwlM4", "signatures": ["ICLR.cc/2026/Conference/Submission18828/Reviewer_Zs3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18828/Reviewer_Zs3e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761155872845, "cdate": 1761155872845, "tmdate": 1762930799605, "mdate": 1762930799605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the problem of tokenization: given a database of words, how can one most efficiently tokenize the words? Tokenization is an important first step in most natural language processing pipelines, and finding the optimal tokenization is potentially useful to optimizing language model performance. Thus, it is important to understand how well we can hope to compute good tokenizations. Formally the authors consider two problems:\n\n1. Direct encoding: given a set of tokens, the tokenization of a database is the optimal splitting of the words in the database to minimize the number of tokens used. \n2. Bottom-up encoding: given a set of tokens, the tokenization of a database is represented by a set of merge operations. The tokenized database is obtained by sequentially applying the merge operations greedily to the database.\n\nFor both formulations of the tokenization function, the optimization problem asks to find the tokenization (i.e. the set of tokens in the former case, the merge operations in the second case) minimizing the number of tokens. Naturally, one can also consider approximations of this problem. The main result of this work establishes that unless P = NP, there is no PTAS for tokenization i.e. there exists a constant 1.00001 such that finding a 1.00001-approximation to the optimal tokenization is NP-hard. The proofs follow via reduction to a special case of the MAX-2-SAT problem. Whereas previous work obtained lower bound for unbounded alphabets, this work gives the first hardness result for bounded alphabets, and in fact gives a strong result ruling out polynomial algorithms for binary alphabets. Furthermore, they show that for (1) even unary tokenization is NP-complete.\n\nThe authors consider an interesting problem, and give strong results. The paper is well written and easy to follow. I therefore recommend accept."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper studies a practically motivated problem which is a key step in training natural language processing models. The result is strong and essentially resolves the question of tokenization with compression as an objective. The paper is well written, and the proofs are well motivated and easy to follow."}, "weaknesses": {"value": "No clear weaknesses (see minor comments below)"}, "questions": {"value": "Minor Comments \n\nReference to Lemma 1 and 2 - I think better to reference Reduction 1 and Theorem 2\n\nMaybe good to state explicitly what all merge operations are in forward step of bottom-up tokenization proof."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L7tPZ2nFW7", "forum": "Xhf9YqwlM4", "replyto": "Xhf9YqwlM4", "signatures": ["ICLR.cc/2026/Conference/Submission18828/Reviewer_bxh6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18828/Reviewer_bxh6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800151477, "cdate": 1761800151477, "tmdate": 1762930798809, "mdate": 1762930798809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the tokenization problem, which is the first step in most NLP pipeline. The problem (somewhat involved to describe) is the following: Let $\\Sigma$ be a finite alphabet  and let $D = \\{c_1, c_2, \\dots, c_M\\}$ be a dataset of character strings \n$c_i \\in \\Sigma^{*}$. A *tokenizer* is a tuple $(S,dt,t)$ where $S$ is a *vocabulary* which is a finite set $S \\subset \\Sigma^{+}$ of nonempty substrings over $\\Sigma$. $dt$ (detokenizer) is a function from $S^{\\*}$ to $\\Sigma^{\\*}$ which is a string concatenation operator. The tokenizer encoder unction $t$ is a function from $\\Sigma^{\\*}$ to $S^{\\*}$ such that  for any $c \\in \\Sigma^{\\*}$, $c = \\mathrm{concat}(t(c))$.  Fix a natural encoding function (eg direct encoding) and a budget $K$. For a given vocabulary $S$ so that $|S| = K+|\\Sigma|$, the cost of $S$ is the minimum,  total number of tokens produced over all the dataset $D$ -- i.e., achieves maximal compression over $D$. The optimization problem is to find cost of the best-cost vocabulary of size $K+\\Sigma$.\n\nThe paper considers two types of encodings -- direct encoding and bottom-up encoding, which are used in practice and investigated in the literature. Prior work has shown that these optimization problems are NP-hard when the alphabet size is unbounded. The present work builds on this hardness result and  strengthen them in several ways. They show that in both cases, even if the alphabet is binary, these problems remain NP-complete. More interestingly to me, they show that even approximating the optimum value up to arbitrary accuracy (PTAS) is NP-hard. In other words, there exists a constant $\\varepsilon > 0$ such that no polynomial-time algorithm can approximate the optimum value within a factor of $(1+\\varepsilon)$ unless P=NP. The paper leaves open a very interesting theoretical question, is there a constant ratio approximation algorithm for this problem? Concretely, is there an approximation algorithm that finds a vocabulary whose cost is  almost twice the optimum?"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The computational problem studied in this paper is highly relevant, and, given the prior work, the demonstrated impossibility of approximation algorithms with arbitrary precision represents a significant theoretical contribution. The results provide valuable insights into the computational complexity of a fundamental step in modern AI and NLP models. The paper is clearly structured and written fairly well. While I did not verify all proofs in detail (as they are presented in the appendix), the claims appear sound and consistent with the constructions outlined in the main text. I also find the open question regarding the existence of a constant-factor approximation algorithm particularly intriguing—and I even suspect that the answer might turn out to be negative."}, "weaknesses": {"value": "While theoretically it is an interesting paper, it appears like the practical tokenizers work very well and not clear whether these results will have any impact on the progress of modern NLP models. Another weakness I find is that it appears complicated than it needs to be to define the computational problems. Some notational use appears non-standard. For example while $tok$ is a function by definition, they also use it as a set and use notations such as $|tok|$ which, to my understanding is the cardinality of the vocabulary. I find such notional use a bit confusing. Another minor weakness is that, the paper is dense and proof-heavy with limited examples illustrating the reductions. Hence a small running example of the reduction would improve readability."}, "questions": {"value": "From a theoretical viewpoint, It will be very nice to see the exact constant beyond which one cannot approximate. You give a bound 1.000002, which is theoretically sufficient for the claim, but practically a 1.000002 approximation algorithm is very good.  Have you considered improving the constant? What is your insight as to the existence of constant factor approximation algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Concern."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DcZR9EDz5Y", "forum": "Xhf9YqwlM4", "replyto": "Xhf9YqwlM4", "signatures": ["ICLR.cc/2026/Conference/Submission18828/Reviewer_GzVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18828/Reviewer_GzVs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864408121, "cdate": 1761864408121, "tmdate": 1762930798128, "mdate": 1762930798128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the computational complexity of tokenisation when applied to inputs drawn from bounded alphabets, addressing a gap in earlier NP-completeness results that assumed alphabets of unbounded size. Tokenisation, a core component of natural language processing pipelines, converts character strings into subword sequences. Many tokenisation methods—such as Byte Pair Encoding (BPE) and Unigram Language Models—aim to compress data, thereby improving efficiency in model training and inference. While previous work had established that finding an optimally compressive tokeniser is NP-complete, those proofs relied on the unrealistic assumption of infinitely large alphabets.\n\nThe authors define and analyse two bounded-alphabet tokenisation problems: bottom-up tokenisation, where an optimal sequence of merge operations must be selected, and direct tokenisation, where the optimal vocabulary must be chosen directly. They demonstrate that even when the alphabet is extremely limited, the problems remain computationally intractable. Specifically, for binary alphabets (only two characters), both bottom-up and direct tokenisation are shown to be NP-complete and to lack any polynomial-time approximation scheme, unless P = NP. The authors also show that direct tokenisation remains NP-complete even for unary alphabets (containing a single symbol). Because unary and binary alphabets are the simplest cases, these hardness results automatically extend to all larger alphabets.\n\nThe findings imply that the difficulty of tokenisation does not stem from large alphabets or complex merge strategies, but from the inherent structure of the optimisation problem itself. Consequently, it is unlikely that any efficient algorithm can find or even closely approximate an optimal tokeniser under a compression objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the paper is that it closes a gap in earlier NP-completeness results, which assumed alphabets of unbounded size. The current paper shows that these hardness results hold even for small size alphabets."}, "weaknesses": {"value": "The scope of the paper may be more suitable for a conference on computational complexity. On the other hand the results are about an important problem in natural language processing. Therefore it may fit a section dedicated to computational complexity results within natural language processing."}, "questions": {"value": "No question"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dyarkHKxRS", "forum": "Xhf9YqwlM4", "replyto": "Xhf9YqwlM4", "signatures": ["ICLR.cc/2026/Conference/Submission18828/Reviewer_jKLj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18828/Reviewer_jKLj"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994336684, "cdate": 1761994336684, "tmdate": 1762930797684, "mdate": 1762930797684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}