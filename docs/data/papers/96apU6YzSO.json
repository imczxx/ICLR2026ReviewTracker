{"id": "96apU6YzSO", "number": 8076, "cdate": 1758058584202, "mdate": 1759897809774, "content": {"title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data", "abstract": "Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.", "tldr": "We propose R-zero, a data-free method to improve LLM reasoning ability.", "keywords": ["large language model", "reinforcement learning", "self-evolving", "reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4269d19b61948607912e7a6454f4fc34eb57d82a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents R-Zero, a framework for training reasoning LLMs through co-evolution of two models (Challenger and Solver) without requiring any human-curated data or labels. The approach uses reinforcement learning to create a self-improving curriculum where the Challenger generates increasingly difficult questions and the Solver learns to solve them. While the motivation is strong, there are numerous weaknesses that should be resolved before publication. While there is solid base model coverage (4 in total), the train domain is only 1 (math) and the synergy and use-case when labeled data is available is limited. Also, there is no existing approach baseline other than the base model performance, therefore, the authors should provide a strong rebuttal."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper is well motivated\n\n2. Presentation is clear\n\n3. Mid-training is an important direction for practitioners, therefore, this part is appreciated\n\n> Our further analysis finds that R-Zero can act as a mid-training method, as models first improved by our method achieve higher performance after fine-tuned on labeled data. \n\n4. Analysis section is appreciated"}, "weaknesses": {"value": "1. It is unclear if sufficient comparisons have been made with existing approaches, especially as there seems to be no baseline other than the base model. There should be significant label-free works that could serve as reasonable baselines. One that immediately comes to mind is Self-Consistency Preference Optimization.\n\n2. a pseudocode to clearly follow this method step by step would be appreciated especially since it is a GAN-style back-and-forth going on. I believe that an intuitive view of how this algorithm works will be helpful. If there is no space it should at least be in the appendix. However, i think there is enough space if sec. 2 is removed. It is unclear why GRPO and RLVR needs to be introduced at this lvl of detail.\n\n3. It is concerning that the training data is only on math domain. While i understand that various benchmarks have been used on the testing side, on the training side there should be more training domains to see generality of this approach. This is especially important since a lot of math benchmarks that you are seeing gains here already has labeled data, therefore, the experimental setup is less convincing from a practical view.\n\n> We focus on generating questions specifically within the domain of mathematics, as it provides a convenient and self-contained setting for our framework; the objective nature of mathematical answers allows for the straightforward generation of pseudo-labels via majority voting, without the need for external verification environments like code executors. \n\n4. There seems to be a lot of newly introduced hyperparameters. This does seem like a weakness for practical applications. Are these set heuristically? Arbitrarily? Are there any sensitivity tests for this?\n\n> The Challenger (Qθ) first generates a candidate pool of N = 8, 000 questions. To construct the training dataset for the Solver, these questions are filtered based on consistency. For each candidate question, we sample m = 10 answers from the current Solver (Sϕ). A question is retained for the training set only if the number of answers matching the majority-vote pseudo-label is between 3 and 7, inclusive (δ = 0.25). This numerical range is consistent with the methodology used in previous research (Zhang & Zuo, 2025; Li et al., 2025b; Bercovich et al., 2025). When training the Challenger, the uncertainty reward r(x; ϕ) is calculated by sampling m = 10 responses from the Solver. For the intra-batch repetition penalty, we set the clustering distance threshold to τBLEU = 0.5\n\n5. This method seems to make most sense for benchmarks that do not have a corresponding train set. In a practical perspective, there is no need to not use the train set of e.g. GSM8K. Either there should be meaningful empirical evidence on benchmarks with no training data, i.e., no labels. Or if you want to show that this approach is orthogonal, you should include regular SFT and GRPO as a baseline for benchmarks that have labeled data. Another approach would be to do SFT/GRPO on the existing train data + use your proposed approach to show that it adds gains. Without this, practical impact is not evidently clear. While i see that Sec. 5.3 exists, this kind of practical use when labeled data is available should have been available in the main experiments, as this is very important for practical use.\n\n6. Contents of Sec. 5.3 should be expanded even at the cost of removing e.g. sec. 2.\n\n7. There should be a clear analysis on the cost structure of this approach. I am guessing that the compute overhead over regular SFT is significantly larger as many samples needs to sampled and there are additional compute overhead for all these metrics, clustering, etc. the method itself is quite complex. \n\n8. The top right part of Fig. 1 is not intuitive. My suggestion is using tick and X mark or something similar instead of color shades. Some readers might read the paper in black-and-white\n\n9. It would be nice to have labels for the light blue and dark blue for the grouped bar chart in Fig. 1\n\n10. It would be nice if there was a clear limitation section for practitioners."}, "questions": {"value": "1. Are these methods in the baseline, if applicable? one of my main concern is the lack of baselines, hopefully the authors can clearly rebute this. \n\n> self-challenging approaches train LLMs on tasks generated by the models themselves (Zhou\net al., 2025a; Wang et al., 2025a; Zhao et al., 2025a)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fknvRYy6ns", "forum": "96apU6YzSO", "replyto": "96apU6YzSO", "signatures": ["ICLR.cc/2026/Conference/Submission8076/Reviewer_XCxR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8076/Reviewer_XCxR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761058160985, "cdate": 1761058160985, "tmdate": 1762920066414, "mdate": 1762920066414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces R-Zero, a fully autonomous framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) without relying on any human-curated data for alignment. The core problem it addresses is the significant bottleneck and cost associated with creating large, high-quality datasets for fine-tuning and reinforcement learning.   \n\nR-Zero's methodology is based on a co-evolutionary loop between two distinct roles, both initialized from the same base LLM :   \n\nThe Challenger: This model's objective is to generate new, challenging problems that lie at the very edge of the Solver's current abilities. It is rewarded based on the Solver's uncertainty, which is measured by the consistency of its answers to a given problem.   \n\nThe Solver: This model's goal is to solve the increasingly difficult tasks presented by the Challenger. It is trained on a filtered set of these problems using \"pseudo-labels\" that are generated by a majority vote of its own answers.   \n\nThis self-contained cycle creates a dynamic, adaptive curriculum that progressively increases in difficulty, allowing the model to improve its reasoning skills without external supervision or pre-existing task datasets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "R-Zero's primary strength is its ability to create a self-improving loop for reasoning tasks without human-labeled alignment data. It successfully adapts the self-play paradigm to a domain that lacks a perfect external verifier (like a code executor or game engine), cleverly using a majority-vote mechanism to create a noisy but effective \"pseudo-ground truth\".\n\nThe framework demonstrates significant and consistent performance gains across different model architectures (Qwen3, OctoThinker) and scales. For instance, the Qwen3-8B-Base model achieved a +5.51 point increase on average across math reasoning benchmarks after three iterations.   \n\nThe paper validates the importance of its key design choices. Ablation studies show that removing components like the repetition penalty (to ensure question diversity) or the task filtering mechanism (to control curriculum difficulty and quality) leads to a significant drop in performance."}, "weaknesses": {"value": "The most significant limitation is that the self-improvement process is not indefinitely stable. After a few iterations, all tested models experience a \"performance collapse,\" where their scores on benchmarks begin to decline. Larger models are more resilient and collapse later, but the eventual degradation appears inherent to the current framework.\n\nThe performance collapse is directly linked to a decline in the quality of the training data. As the Challenger generates progressively harder problems, the Solver's ability to form a reliable consensus via majority vote diminishes. The paper shows the true accuracy of these pseudo-labels systematically drops with each iteration (e.g., from 79.0% to 63.0%), introducing increasing noise into the training signal.\n\nThe framework's effectiveness has only been demonstrated in the domain of mathematics. This is a carefully chosen domain where answers are objective and can be verified through simple string matching, making the majority-vote mechanism viable. The method's applicability to more subjective, open-ended domains like creative writing, dialogue, or nuanced analysis remains a major, unaddressed challenge."}, "questions": {"value": "Q1: The paper focuses on a self-evolving framework where the Challenger and Solver are initialized from the same base model. Have you considered an asymmetric or heterogeneous setup—for instance, using a more capable model as the Challenger and a less capable one as the Solver? It would be interesting to know if such a configuration could potentially delay the onset of performance collapse.\n\nQ2: Regarding the training hyperparameters, the number of rollouts for the GRPO algorithm is relatively small. Could you elaborate on the motivation behind this choice? I'm curious if a larger number of rollouts was explored and whether the current setting risks insufficient exploration, which could potentially lead to a less accurate advantage signal.\n\nQ3: The analysis of synergy with supervised data shows that using R-Zero as a pre-alignment step before Supervised Fine-Tuning (SFT) is beneficial. A compelling alternative would be to integrate the labeled data directly into the R-Zero training loop, perhaps by mixing it into the Solver's training dataset at each iteration. Have you experimented with this concurrent training approach, and do you have any insights on whether it might yield superior results compared to the sequential (SFT than R-Zero) method?\n\nQ4: This is a minor suggestion for presentation clarity: in Figure 3, the x-axis is labeled with training steps (e.g., \"Step 15,\" \"Step 30\"). For better consistency with the narrative, it might be more intuitive to label it with the corresponding iteration numbers (e.g., \"Iteration 1,\" \"Iteration 2,\" etc.)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RbHalUb3zp", "forum": "96apU6YzSO", "replyto": "96apU6YzSO", "signatures": ["ICLR.cc/2026/Conference/Submission8076/Reviewer_Qxwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8076/Reviewer_Qxwk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714213521, "cdate": 1761714213521, "tmdate": 1762920066048, "mdate": 1762920066048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes R-Zero, a co-evolving framework that generates new training data from scratch. R-Zero consists of two independent models, the Challenger and the Solver, which work cooperatively and are trained separately within a co-evolving pipeline. The Challenger is trained based on an uncertainty reward from the Solver model and a repetition penalty given the diversity of its generated training batch. The solver is trained with the RLVR objectives. It conducts experiments on Qwen-3 and OctoThinker, where R-Zero shows noticeable improvements over the base model without training and a fixed challenger variant in the math domain and a set of general-domain reasoning benchmarks, including SuperGPQA, MMLU-Pro, and BBEH. This work finally proposes an analysis in terms of the effects of the repetition penalty and the filtering, and eventually shows that R-Zero serves as a good mid-training strategy for enhancing model’s post-finetuning on labelled datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Generating novel data from scratch is a valuable research direction, where the co-evolving framework that includes a dual-agent setup is novel and insightful.\n\n2. The experiments conducted in this work are extensive, and the empirical performance improvement appears to be large.\n\n3. The ablation study is thorough and provides fruitful findings for future research in this direction."}, "weaknesses": {"value": "1. The baseline studied in this work is relatively weak. There are other data generation approaches, such as Absolute Zero [1], which have been discussed but not directly compared empirically.\n\n2. Meanwhile, as the author also mentions, the RLVR methods with zero-shot training objectives, such as maximizing the model confidence and entropy, also need to be compared against the data generation approaches, given that they are all a form of zero-shot approaches. \n\n[1] Zhao, A., Wu, Y., Yue, Y., Wu, T., Xu, Q., Yue, Y., Lin, M., Wang, S., Wu, Q., Zheng, Z., & Huang, G. (2025). Absolute Zero: Reinforced Self-play Reasoning with Zero Data (No. arXiv:2505.03335). arXiv. https://doi.org/10.48550/arXiv.2505.03335"}, "questions": {"value": "1. Could you please clarify and compare more precisely your approach with the Absolute Zero method?\n\n2. In the R-Zero framework, you propose independent roles of the challenger and the solver. What if we train the same model with both roles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YK5Jaase4f", "forum": "96apU6YzSO", "replyto": "96apU6YzSO", "signatures": ["ICLR.cc/2026/Conference/Submission8076/Reviewer_kwkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8076/Reviewer_kwkW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920582112, "cdate": 1761920582112, "tmdate": 1762920065584, "mdate": 1762920065584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes R-Zero, a fully autonomous self-evolving reasoning framework that requires no human-annotated data. The framework uses a Challenger model to generate challenging tasks that lie near the capability boundary of a Solver model, which in turn learns to solve them. The system iteratively co-evolves without any external supervision. Experiments show consistent improvements in both mathematical reasoning (e.g., GSM8K, MATH, AIME) and general reasoning benchmarks (e.g., MMLU-Pro, SuperGPQA, BBEH)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper reports consistent performance improvements across both mathematical and general reasoning benchmarks.\n\nIt conducts rich and detailed ablation studies, revealing several interesting phenomena.\n\n1. Models fine-tuned after R-Zero pretraining perform better than those fine-tuned directly.\n\n2. Both task filtering and repetition penalty are shown to be essential components.\n\n3. The paper identifies a model collapse phenomenon after multiple self-evolution iterations, with larger models showing better resistance.\n\n4. The accuracy of self-generated pseudo-labels gradually degrades (from 79% to 63%), highlighting an important data quality issue.\n\n5. Separating the Challenger and Solver is shown to be necessary to avoid overfitting and instability."}, "weaknesses": {"value": "Some of the reported improvements in Table 1 appear to be statistically insignificant, weakening the empirical strength of the main claims."}, "questions": {"value": "The authors have already provided extensive and detailed ablation studies, so I do not have additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eOstHQ1aiS", "forum": "96apU6YzSO", "replyto": "96apU6YzSO", "signatures": ["ICLR.cc/2026/Conference/Submission8076/Reviewer_Bdrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8076/Reviewer_Bdrg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988131744, "cdate": 1761988131744, "tmdate": 1762920065037, "mdate": 1762920065037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}