{"id": "bWtRZQ1rm2", "number": 8201, "cdate": 1758073928607, "mdate": 1763614941137, "content": {"title": "Pixel-Level Residual Diffusion Transformer: Scalable 3D CT Volume Generation", "abstract": "Generating high-resolution 3D CT volumes with fine details remains challenging due to substantial computational demands and optimization difficulties inherent to existing generative models. In this paper, we propose the Pixel-Level Residual Diffusion Transformer (PRDiT), a scalable generative framework that synthesizes high-quality 3D medical volumes directly at voxel-level. PRDiT introduces a two-stage training architecture comprising 1) a local denoiser in the form of an MLP-based blind estimator operating on overlapping 3D patches to separate low-frequency structures efficiently, and 2) a global residual diffusion transformer employing memory-efficient attention to model and refine high-frequency residuals across entire volumes. This coarse-to-fine modeling strategy simplifies optimization, enhances training stability, and effectively preserves subtle structures without the limitations of an autoencoder bottleneck. Extensive experiments conducted on the LIDC-IDRI and RAD-ChestCT datasets demonstrate that PRDiT consistently outperforms state-of-the-art models, such as HA-GAN, 3D LDM and WDM-3D, achieving significantly lower 3D FID, MMD and Wasserstein distance scores.", "tldr": "We introduce 'PRDiT', a scalable generative model that synthesizes high-quality 3D CT volumes by using a two-stage approach that enhances training stability and preserves fine details, and outperforms existing models on key medical imaging datasets.", "keywords": ["Medical Imaging", "3D Diffusion Model", "Diffusion Transformer", "CT Scan", "Medical Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3098f5b6d4d3efcbe7a5ee2c979504323107dd2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a transformer-based 3D diffusion model for medical CT generation that integrates a patch-wise Local Denoiser and a global Diffusion Transformer within a two-stage residual framework. A predictor–corrector (“hot diffusion”) sampling strategy improves generation stability and diversity. Comprehensive ablation studies validate each component and analyze computational efficiency. The progressive training scheme reuses a pretrained low-resolution model and fine-tunes lightweight residual modules at higher resolutions, achieving competitive performance with reduced training cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The progressive training strategy from low to high resolution is clearly defined and achieves comparable performance with reduced computational cost.\n\n2. The combination of local and global components provides a balanced design for modeling fine details and overall structure.\n\n3. The model attains competitive quantitative results and produces visually consistent high-resolution 3D CT volumes."}, "weaknesses": {"value": "1. Visual inconsistency in generated samples:\nIn Figure 5, the second and fifth examples under Ours show black voids in the lower-left region, indicating low-density areas that are absent in the real data. The authors should clarify whether these artifacts result from the “hot diffusion” sampling strategy, which may disrupt local density continuity or anatomical consistency.\n\n2. Extra dataset evaluation:\nAdditional experiments on CT datasets such as CT-ORG are recommended to further validate the model’s generalization.\n\n3. Potential limitation of frozen local denoiser:\nThe local denoiser remains frozen during high-resolution training. It would be valuable to include an ablation comparing frozen versus fine-tuned local modules to assess potential performance degradation at higher resolutions."}, "questions": {"value": "1. What is the potential to extend this framework to conditional generation, such as generating raw CT volumes from segmentation maps or other structural priors?\n\n2. In Figure 5, several artifacts are visible. Could the authors explain their causes and further evaluate the influence of the parameter k on generation quality through additional qualitative or quantitative analysis?\n\n3. The local denoiser is frozen during high-resolution training. Could this design lead to performance degradation at higher resolutions? An ablation comparing frozen and fine-tuned local denoisers would help clarify this effect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4kJ4v9h6gQ", "forum": "bWtRZQ1rm2", "replyto": "bWtRZQ1rm2", "signatures": ["ICLR.cc/2026/Conference/Submission8201/Reviewer_Vjha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8201/Reviewer_Vjha"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760629838868, "cdate": 1760629838868, "tmdate": 1762920152798, "mdate": 1762920152798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PRDiT, a two-stage diffusion transformer framework for synthesizing high-resolution 3D CT volumes directly at voxel level. A predictor–corrector diffusion sampling method and a progressive low-to-high-resolution training strategy improve sample fidelity and efficiency. Experiments on LIDC-IDRI and Rad-ChestCT show clear advantage over other methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The decomposition of diffusion into local + global residual branches is elegant and addresses the long-standing trade-off between local detail and global coherence in 3D image synthesis; \n2. The low-to-high-resolution reuse strategy reduces training cost\n3.Figures 4–5 demonstrate noticeably sharper bone edges, smoother organ boundaries, and fewer artifacts relative to baselines\n4. Strong reproducibility section (datasets, configs, metrics) and detailed appendices on architecture, hyperparameters, and inference time"}, "weaknesses": {"value": "The idea of splitting local/global branches is incremental relative to prior hierarchical or multi-scale diffusion models. The architectural originality lies mainly in combining them via residual refinement rather than introducing new attention or tokenization mechanisms.\n\nReported mean ± std over 3 seeds is small; given large variance in 3D generation, stronger statistical support or significance tests would enhance credibility.\n\nNo mention of hallucination risk or downstream misuse"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "piSUhabpvv", "forum": "bWtRZQ1rm2", "replyto": "bWtRZQ1rm2", "signatures": ["ICLR.cc/2026/Conference/Submission8201/Reviewer_4BSb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8201/Reviewer_4BSb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857620016, "cdate": 1761857620016, "tmdate": 1762920152449, "mdate": 1762920152449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I reviewed the paper, a two-stage diffusion architecture for high-resolution 3D medical image synthesis. The model combines a local MLP denoiser, which captures fine-grained voxel details on overlapping patches, with a global residual Transformer, which ensures anatomical consistency across the whole volume.   \nThe authors also propose a predictorcorrector sampling scheme that reintroduces controlled noise to stabilize generation, and a low-to-high-resolution scaling strategy that allows efficient 256 training guided by a pretrained 128 backbone.\n\nThey evaluate their method on LIDC-IDRI and Rad-ChestCT, showing improved FID and MMD over several baselines (HA-GAN, 3D-LDM, WDM-3D). The generated CT volumes appear sharper and more realistic, while training remains computationally feasible. Overall, the work presents a solid engineering improvement that makes transformer-based diffusion models more practical for 3D medical data synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "I find the paper technically solid. The proposed two-stage design separating local voxel-level denoising from global residual refinement is clearly explained, and supported by ablation studies showing that each component contributes to performance. The predictorcorrector sampling strategy is effective, improving image quality with minimal computational overhead. I also appreciate the scaling approach that leverages a pretrained low-resolution model to enable efficient 256 synthesis, which addresses an important computational bottleneck for 3D diffusion models.\n\nThe paper includes comprehensive experiments on two public CT datasets, reports multiple quantitative metrics (FID, MMD, and provides consistent improvements over established baselines. Figures illustrate sharper structural detail and realistic textures. I also value the inclusion of limitations and future work, as well as the clear structure and readability of the paper."}, "weaknesses": {"value": "While the method is good, I find the novelty somewhat limited. The combination of a local denoiser and a global Transformer is conceptually straightforward and resembles existing hierarchical or hybrid DiT approaches. Similarly, the proposed sampling resembles previously known stochastic sampling methods, though applied here in a slightly modified form.\n\nThe evaluation focuses mainly on generative metrics such as FID and MMD, which can  be unstable for 3D medical data. There is no downstream or clinical validation (e.g., segmentation or detection performance using synthetic data), which makes it difficult to judge real-world utility. Baselines are also somewhat limited and some stronger 3D diffusion models or DiT variants are missing, especially at higher resolutions\n\nFinally, several implementation details are underspecified, such as the exact memory-efficient attention mechanism, data split protocol, and reproducibility of high-resolution experiments. Overall, the paper’s contribution feels more like a solid engineering refinement than a major conceptual breakthrough."}, "questions": {"value": "- Could you clarify which memory-efficient attention variant is used in the global DiT (e.g., FlashAttention, windowed, or block-sparse)? How much does it contribute to scalability compared to a vanilla DiT?\n- Predictor sampling: Can you provide pseudocode or additional explanation of the schedule? Did you explore adaptive or variable numbers of corrective steps (k > 2), and how stable is training when increasing k?\n- Baselines: Why were recent efficient 3D diffusion or DiT variants  excluded from comparison? Would your method still outperform these stronger models, especially at 256 resolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yCFq4qXzAq", "forum": "bWtRZQ1rm2", "replyto": "bWtRZQ1rm2", "signatures": ["ICLR.cc/2026/Conference/Submission8201/Reviewer_eQz3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8201/Reviewer_eQz3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978470192, "cdate": 1761978470192, "tmdate": 1762920152124, "mdate": 1762920152124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Generating fine-grained 3D CT data is an extremely challenging problem. This paper addresses this challenge by employing a pixel-level residual diffusion transformer. The chosen topic is highly relevant and represents a significant area of research. The methodology utilizes a coarse-to-fine strategy and includes comparisons with various established models.\nThe authors demonstrated superior performance against existing models by comparing metrics such as 3D DIF, MMD, and W-Score on two generated datasets of $128^3$ resolution. However, a significant drawback is the lack of evaluation by a medical imaging expert from a clinical image generation standpoint.\nObserving Figure 4, 5, the model appears to capture the overall coarse shape, but there still seem to be significant problems with the finer details. Specifically, as the layers deepen, internal structures within the lungs appear to vanish, and the soft tissue contrast does not accurately reflect reality.\nThe overall comparison lacks benchmarking against the current State-of-the-Art (SoTA) DiT (Diffusion Transformer) model. The paper mentions that DiT models suffer from \"unstable dynamics and optimization difficulties,\" yet it fails to provide any comparative experiments to support this claim or justify the exclusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Generating fine-grained 3D CT data is an extremely challenging problem. This paper addresses this challenge by employing a pixel-level residual diffusion transformer. The chosen topic is highly relevant and represents a significant area of research. The methodology utilizes a coarse-to-fine strategy and includes comparisons with various established models.\nThe authors demonstrated superior performance against existing models by comparing metrics such as 3D DIF, MMD, and W-Score on two generated datasets of $128^3$ resolution. However, a significant drawback is the lack of evaluation by a medical imaging expert from a clinical image generation standpoint."}, "weaknesses": {"value": "Observing Figure 4, 5, the model appears to capture the overall coarse shape, but there still seem to be significant problems with the finer details. Specifically, as the layers deepen, internal structures within the lungs appear to vanish, and the soft tissue contrast does not accurately reflect reality."}, "questions": {"value": "The overall comparison lacks benchmarking against the current State-of-the-Art (SoTA) DiT (Diffusion Transformer) model. The paper mentions that DiT models suffer from \"unstable dynamics and optimization difficulties,\" yet it fails to provide any comparative experiments to support this claim or justify the exclusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9zIi8V7tsv", "forum": "bWtRZQ1rm2", "replyto": "bWtRZQ1rm2", "signatures": ["ICLR.cc/2026/Conference/Submission8201/Reviewer_Nuef"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8201/Reviewer_Nuef"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762572316015, "cdate": 1762572316015, "tmdate": 1762920151547, "mdate": 1762920151547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}