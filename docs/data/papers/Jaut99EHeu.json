{"id": "Jaut99EHeu", "number": 18223, "cdate": 1758285344298, "mdate": 1759897118177, "content": {"title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-Tuning with Verified Rewards in World Simulators", "abstract": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models.", "tldr": "", "keywords": ["VLA", "World Model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01fe44202488e15cf8ca2dce6425ca09e58bbdd5.pdf", "supplementary_material": "/attachment/afc70445ada66cf63499be286d92f38fa2761778.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose VLA-RFT, an enhanced fine-tuning framework based on the World Model, to improve the robustness and generalization ability of the vision-language-action model under distributional shifts. The core idea is to use a world model trained from real interactive data to generate future visual states as a high-fidelity simulator in which the rollout of policy actions is performed, the verified reward is constructed by the pixel-level and perception-level similarity with the expert trajectory (such as L 1 + LPIPS), and then the VLA strategy is efficiently fine-tuned by the GRPO algorithm. Experiments show that VLA-RFT can significantly outperform the strongly supervised fine-tuning baseline (+4.5% average success rate) on the LIBERO benchmark with only 400 fine-tuning iterations and exhibits stronger robustness across multiple perturbation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper accurately points out that the current mainstream VLA model relies on Imitation Learning, which leads to the problem of error accumulation and weak generalization ability outside the distribution. The limitations of traditional reinforcement learning (such as simulation-based training or real-world interaction) in terms of sample efficiency, security, and the sim-to-real gap are demonstrated.\n- The data-driven world model is used as a verifiable reward generator instead of a traditional physical simulator, which skillfully avoids the problems of manual modeling bias and high cost. By calculating dense rewards between the generated and expert trajectories, a stable, aligned action-monitoring signal is provided for policy optimization, significantly reducing sample complexity.\n- It surpasses the baseline requiring 150,000 steps of supervised training with only 400 fine-tuning steps and outperforms other RL methods requiring tens of thousands of steps (e.g., ARFM, ReinboT), highlighting the potential of the framework in practical deployment."}, "weaknesses": {"value": "- Current reward mechanisms are entirely based on the similarity of the generative trajectory to the expert's trajectory, which means that the strategy can not surpass the expert's performance, nor can it explore better but look different solutions. If the expert data itself is suboptimal or biased (e. g. only one way of fetching) , the model will be limited to this range. It is suggested to discuss or experimentally verify the performance under non-optimal expert data.\n- L1 + LPIPS, while a measure of visual fidelity, is not necessarily strongly associated with task success (e.g. , slight object rotation may not affect the task but result in high LPIPS) . If we can introduce task-related semantic rewards (such as critic model in VLAC) or learnable rewards, the alignment can be further improved.\n- Although compared with ARFM, ReinboT, etc. , it does not cover areas such as Zhang et al. . (2025d) offline RL or Tan et al. . (2025) interactive post-training and other recent work. In addition, it is not stated whether attempts have been made to fine-tune (even on a small scale) the sim-to-real gap directly in the real world to see if it is truly eliminated."}, "questions": {"value": "- If there are multiple ways to succeed in an expert trajectory (such as grabbing from the left or right), does the current reward based on a single reference trajectory penalize other effective strategies? Are multiple reference trajectories or post-clustering trajectory prototypes considered to construct rewards?\n- Are reward signals misleading when the trajectories generated by the world model may be distorted in actions or scenarios not covered by the training data? Are there mechanisms to detect or suppress the effects of low-confidence rollouts (e.g., based on generation uncertainty weighting)?\n- While RFT requires only 400 steps of policy updating, World Model pre-training takes 150,000 steps. Should the training cost of the world model be included in the overall efficiency assessment? The marginal cost of the world model is lower if it can be reused for multiple tasks, but this should be made clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oEoMFzif83", "forum": "Jaut99EHeu", "replyto": "Jaut99EHeu", "signatures": ["ICLR.cc/2026/Conference/Submission18223/Reviewer_p7T1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18223/Reviewer_p7T1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720258225, "cdate": 1761720258225, "tmdate": 1762927963284, "mdate": 1762927963284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Reinforcement learning from verifiable rewards (RLVR) was recently shown to provide substantial performance benefits when used in post-training of language-based reasoning models and agentic AI systems. Recent works (e.g. Guo et al., Improving vision-language-action model with online reinforcement learning, 2025) have extended this paradigm to vision-language-action (VLA) models for robotic manipulation, but unlike language-only models that generate their own rollouts via autoregressive text generation, training VLA models for robotics using online RL requires interaction with an environment external to the policy itself (e.g. a simulator or real hardware) which can significantly complicate the training pipeline. Instead, this work proposes to train a world model (action-conditioned autoregressive video prediction model) on data from the target environment and then subsequently use the trained world model as environment for policy RLVR. The authors derive a \"verifiable\" reward function from the world model based on video generation results (pixel-level + perception score) and demonstrate that VLA models finetuned using the world model require less real data, are more robust to environment perturbations, and have a qualitatively different action distribution than the base model on tasks from LIBERO (simulation benchmark)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is generally well written, timely, technically sound. Concretely:\n\n- **Originality:** I believe that this paper tackles an interesting and timely problem, and is likely to be of interest to the community. It provides a rather concise but well-rounded overview of related work including RLVR with LLMs, VLMs, and most recently VLAs. Although using a learned world model for RL has been explored extensively in model-based RL literature, I am not aware of any other papers that have yet extended the paradigm to VLAs and I would thus consider it to be an original and meaningful contribution to the field.\n- **Writing:** The paper is generally well written and easy to follow, and the illustrations are helpful for understanding the technical contributions. I expect a reader familiar with large models and/or RL literature to be able to appreciate the technical details.\n- **Extensive perturbation experiments:** I appreciate the thorough evaluation of policy robustness wrt environment perturbations. These are hardly surprising results for anyone familiar with RL, but it is great to see it validated empirically in the context of VLAs and learned world models. Experiments are clearly motivated and the discussion and analysis of results is informative."}, "weaknesses": {"value": "While my assessment of the paper is positive overall, I do believe that there is room for improvement both in terms of writing as well as experiments:\n\n- **Writing:** While the paper is generally easy to follow, it does appear rather rushed with frequent typographical and grammatical errors throughout. For example, some letters in the title are capitalized seemingly at random, and there are typos (*e.g.* L314 \"We report success rate (SR) for all tasks**..** 3) Base Policy\"). The paper would benefit from a round of proof-reading. Additionally, I would suggest the authors to be more explicit in their writing; instead of defining reward functions as \"Reward type 1\", \"Reward type 2\" etc. in the main text and then referencing them by their # in Table 4 and the accompanying illustration, it would be easier to follow if you referenced them via more descriptive names. Similarly, the base models used as baselines are introduced as Base (3w) and Base (15w) but it is never explicitly stated what 3w and 15w refers to.\n- **Limited ablations:** The current set of experiments clearly demonstrate that RLVR with the trained world model improves policy performance, and the authors do conduct an ablation on the reward design which is informative. However, given that the world model is a key contribution of this work, I expected to see more analysis on the design and usage of the world model itself. For example, it is currently not clear how world model size (in terms of parameters) impacts generation quality nor policy RLVR, and it is also not clear what the relationship between RLVR iterations and policy performance is (the policy is only evaluated at 400 iterations). Experiments like these seem rather important given the nature of the problem, and I do not believe that they would be prohibitively expensive to run (policy evaluation at steps 100, 200, 300, for example, would be cheap if you already have the checkpoints, and you could similarly compare generation quality of the current world model size to that of a smaller world model).\n- **World model training data:** It is not entirely clear to me what data is used to train the world model. If the world model is trained only on demonstrations (it is my understanding that this is the case), then I wouldn't expect the world model to generalize well to out-of-distribution states and actions for the same reasons that the policy does not. I would appreciate it if the authors could clarify this part and potentially back up any claims with data or references to prior work that addresses my concern."}, "questions": {"value": "I would really appreciate it if the authors can address my comments in the \"weaknesses\" section above using written arguments and potentially additional experimental results. I would also appreciate it if the authors can commit to improving the writing beyond just correcting the specific instances I pointed out above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Qer24hBS1", "forum": "Jaut99EHeu", "replyto": "Jaut99EHeu", "signatures": ["ICLR.cc/2026/Conference/Submission18223/Reviewer_Escc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18223/Reviewer_Escc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885378902, "cdate": 1761885378902, "tmdate": 1762927962933, "mdate": 1762927962933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for improving Vision-Language-Action (VLA) models beyond imitation learning, by using a world model. Specifically, a two-stage pipeline is used: (Stage I) a learned (visual) world model is trained on offline data to predict the next image frame given the image history and an action; the policy is trained on the expert demonstration dataset; (Stage II) the policy is rolled out in the world model and the \"verified reward\" is given by perceptual similarity to the offline trajectories; this reward is used to optimize the policy using GRPO.\n\nExperimentally: the authors demonstrate the world model can generate images similar to that from the offline dataset. On the LIBERO robotic benchmark, the method demonstrates several percentage point improvement for VLAs given 400 RL fine-tuning steps in the world model, demonstrates improvements over baseline given perturbations in the environment (backed up by visualization the resulting broader action distribution), and show improvement over simpler reward types."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clear and well-motivated; in particular, both world models and reinforcement learning as they pertain to VLAs is a critical, timely research topic which is likely to have significant impact. The methods section is well-exposited and the experiments are generally clear."}, "weaknesses": {"value": "- Running RL in a world model is not a particularly novel idea; in fact, this is essentially the field of Model-Based Reinforcement Learning (MBRL). In the Related Work section, the authors differentiate their work from previous efforts by claiming the world model \"also provides verified rewards\". It is clear in many works (as a random example, [1]) that the world models can learn rewards as well; defining precisely the novelty here would be very helpful.\n- Regarding the reward, it is unclear why perceptual distance to offline dataset trajectories is a \"verified\" reward. As the authors say in their conclusion, this design choice would constrain policies by offline dataset quality and limits the discovery of strategies beyond expert performance, which is a primary motivation behind the use of reinforcement learning in the first place.\n- The actual improvement over baselines does not seem very significant: on the order of low single-digit percentage point improvements on LIBERO. It would be helpful to have standard error numbers to contextualize the improvement within variance.\n\n[1] Janner, Michael, et al. \"When to trust your model: Model-based policy optimization.\" Advances in neural information processing systems 32 (2019)."}, "questions": {"value": "- Standard error / error bars for experiment results would be very helpful to contextualize the improvement.\n- Regarding Table 1, could you clarify whether the metrics are computed on the training data? It is natural that MSE would be low if the model is already trained; the more important test of the world model is on out-of-distribution states.\n- Can you confirm which reward type was used in the main results?\n- Re: Figure 4, is there some analysis (e.g. possibly with a quantitative metric) why having a broader action distribution makes sense as a result of the proposed training procedure, and why it helps for robustness?\n- Could you expand on the choice of GRPO as the RL algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VCDaY2Teic", "forum": "Jaut99EHeu", "replyto": "Jaut99EHeu", "signatures": ["ICLR.cc/2026/Conference/Submission18223/Reviewer_5Smb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18223/Reviewer_5Smb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979670455, "cdate": 1761979670455, "tmdate": 1762927962458, "mdate": 1762927962458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for reinforcement fine-tuning in world simulators to train VLA models. It takes two stages: (1) it trains a world model (WM) on offline data and a base VLA policy with supervised finetuning (SFT); (2) it uses the learned WM to roll out actions while predicting visual trajectory. RL finetuning is done by computing a verifiable, trajectory-level reward that minimizes the difference between the ground-truth-action-induced rollouts. The proposed VLA-RFT improves performance and robustness with very few fine-tuning steps (~400) compared to SFT."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written. The duet of the world model and the VLA model is formulated in a shared POMDP framework. \n\n2. The efficiency claim is compelling. The paper reports several orders of magnitude fewer iterations required for RL fine-tuning compared to supervised baselines. This sample efficiency gain is attractive for practical post-training workflows."}, "weaknesses": {"value": "1. Comparison with (Tan et al., 2025). Both papers share the same high-level idea of using \"reinforcement Fine-tuning\" and \"verified rewards\". However, the implementations are at least vastly different while not being exactly the opposite. First of all, (Tan et al., 2025) indeed leverages **interactive** feedback from the simulator. The claim from this submission (L074-075; L116-117) seems to imply that they don't, which is quite misleading. Second, (Tan et al., 2025) use a binary reward for success or failure, which is also **verifiable**.  In the meantime, the \"verifiability\" of this submission is less convincing in that (1) the verified reward is defined by a sum of reconstruction and similarity loss, which is much more indirect to the final task, and (2) the verified reward (or reconstruction loss) is computed against the trajectory induced by the ground-truth actions. There are three potential issues: \n\n    (1) Are ground-truth action-induced trajectories optimal? Not quite, since many tasks are multi-modal, meaning that there are multiple possible trajectories leading to the same goal;\n\n    (2) Are ground-truth action-induced trajectories easier to get? Not quite either, the simulator itself is cheaper while the world model is likely bigger (details are missing??);\n\n    (3) Does RL bring explorativeness? Unfortunately, no again, since this relies on your SFT recording, which is a highly overlapped effort compared to the SFT phase. Meanwhile, the simulator is more likely to bring more diversity beyond the SFT recording data.\n\n2. No real-world validation. The high-level idea is to learn a world model that emulates the simulator. This introduces a simulator-WM gap in addition to the already widely known real-sim gap. We need to answer the question: how large is this simulator-WM gap compared to the real-sim gap? The sim-to-real gap has been a big problem. The proposed method further complicated it and left many questions unanswered."}, "questions": {"value": "Please see my questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nX56KukVJ3", "forum": "Jaut99EHeu", "replyto": "Jaut99EHeu", "signatures": ["ICLR.cc/2026/Conference/Submission18223/Reviewer_eHXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18223/Reviewer_eHXZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981583035, "cdate": 1761981583035, "tmdate": 1762927961908, "mdate": 1762927961908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}