{"id": "lpYHuxPEXV", "number": 22283, "cdate": 1758329010374, "mdate": 1759896874827, "content": {"title": "Jacobian Aligned Random Forests", "abstract": "Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we fit a random forest to estimate class probabilities, compute finite-difference gradients with respect to each feature, form an expected Jacobian outer product (EJOP), and use it as a single global linear preconditioner for all inputs. This preserves the simplicity of axis-aligned trees while applying a single global rotation to capture oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. On tabular benchmarks, our preconditioned forest matches or surpasses oblique baselines while training faster. Our results suggest that supervised preconditioning can deliver the accuracy of oblique forests while keeping the simplicity of axis-aligned trees.", "tldr": "", "keywords": ["Random forests; Decision trees; Axis-aligned splits; Oblique decision boundaries; Feature interactions; Supervised preconditioning; Gradient-based feature transforms"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/818d63d295fb4f6a06a984d021ac53b4a9159c5e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce JARF, a method that linearly transforms inputs before fitting a random forest. The linear transform is derived from an expected Jacobian outer product (EJOP), which has been developed and used in prior work. When applied to Random Forests, the authors find that JARF achieves performance results matching existing oblique tree baselines while reducing computation time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors study an interesting problem\n- The authors approach is intuitive\n- The paper is well-written and clear throughout"}, "weaknesses": {"value": "- Main weakness: My understanding of JARF is that it requires first fitting an RF as a step in computing the EJOP, followed by fitting the RF on the conditioned input data. Why is the fitting of the first RF not taken into account in the Efficiency and Compute subsection (or if it is, why is JARF's compute time not atleast 2x that of fitting RF)? This computational efficiency claim is central to the authors' message and should be taken seriously.\n- The paper's novelty is somewhat limited, as the EJOP is already defined and used in prior work (although the authors to swap a kernel regression estimator for RF when estimating it)"}, "questions": {"value": "- It could be nice to understand how performance varies as the size of the subsampled dataset used for computing the EJOP varies.\n- How did the authors select the 10 datasets they used for evaluation? It would be nice to use a standard suite of datasets (e.g. TabArena or PMLB) to avoid any possibility of biased dataset selection"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M5dAw0CNe6", "forum": "lpYHuxPEXV", "replyto": "lpYHuxPEXV", "signatures": ["ICLR.cc/2026/Conference/Submission22283/Reviewer_48eb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22283/Reviewer_48eb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761061878124, "cdate": 1761061878124, "tmdate": 1762942150548, "mdate": 1762942150548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a **supervised pre-processing step** applied to the feature matrix used as input for a random forest or other **axis-aligned predictors**.  \nThe goal is to enable more flexible splits in the input space, thereby improving the performance of axis-aligned classifiers.  \n\nTheir empirical evaluation shows that the proposed method performs **on par with SPORF**, and slightly better than **XGBoost (XGB)** and **Random Forest (RF)**."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting idea to transform the data in a supervised manner before training.  \n- The method should be relatively fast to run, better on the compute-performance tradeoff than SPORF."}, "weaknesses": {"value": "- The reported improvements in performance are **not particularly meaningful**.  \n- Evaluating only on **10 real datasets** is not sufficient to claim generality or robustness.  \n- There is **no discussion** on how to tune hyperparameters for the proposed method."}, "questions": {"value": "1. The proposed method appears similar to a **one-step RFM** [1] for classification.  \n   Can the authors clarify the conceptual and mathematical connection between their procedure and RFM?  \n2. Does the matrix **H** have to be derived from the **same model** used for prediction?  \n   If not, the authors should provide guidance on how to select and pair the transformation and prediction models.  \n3. Can **JARF** be applied to **XGBoost** as well, or is it restricted to random forests?  \n4. The authors mention that the *electricity*, *magic*, and *letter* datasets have **complex decision boundaries**.  \n   Can they explain why these particular datasets were chosen to illustrate this property?  \n5. Can the authors provide **details about the datasets** used in the experiments (e.g., number of samples, features, task type)?  \n6. Can the authors provide **guidance on hyperparameter tuning** or recommendations for practical implementation?\n\n\n[1] Radhakrishnan, A., Beaglehole, D., Pandit, P., & Belkin, M. (2024).  \n*Mechanism for feature learning in neural networks and backpropagation-free machine learning models.*  \n**Science**, 383(6690), 1461–1467."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bxofEVfBHS", "forum": "lpYHuxPEXV", "replyto": "lpYHuxPEXV", "signatures": ["ICLR.cc/2026/Conference/Submission22283/Reviewer_PJES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22283/Reviewer_PJES"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960885482, "cdate": 1761960885482, "tmdate": 1762942150318, "mdate": 1762942150318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Jacobian-Aligned Random Forests (JARF): learn one global supervised linear transform $H$ from the Expected Jacobian Outer Product (EJOP) of class-probability gradients (estimated via finite differences of an RF surrogate), then train a standard axis-aligned RF on the transformed features $XH$. This aims to capture rotated/interaction directions so axis-aligned splits behave like shared oblique hyperplanes, while keeping RF’s simplicity. Experiments on 10 tabular classification datasets and controlled synthetic rotations show competitive accuracy vs oblique forests with modest overhead, plus ablations supporting the EJOP step and implementation choices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. It is a simple and one-pass method, plugging the EJOP to perform initial feature transformation. This enables direct application on RF in the subsequent step."}, "weaknesses": {"value": "1. The proposed method clearly lacks novelty, which does not match the conference standard. It is mainly based on a known paradigm EJOP. The paper’s main change is estimating EJOP with a surrogate RF and finite differences. This feels incremental relative to existing supervised/oblique projection lines rather than a new learning principle.\n\n2. The estimator uses finite differences of RF class probabilities to approximate Jacobians (Sec. 3.6), but the analysis later assumes $f\\in\\mathcal C^3$ with bounded third derivatives (Assumption A1), which is incompatible with piecewise-constant tree ensembles. The text informally argues that ensemble averaging “smooths” predictions, but the formal guarantees hinge on smoothness that the surrogate does not satisfy. And usually, such smoothness improvements are stated when comparing to a single decision tree. This creates a theory–practice gap in the central estimator.\n\n3. A single matrix $\\hat H$ is shared across the entire forest (Sec. 3.4–3.5). This seems to reduce the diversity of the allowed splits. I would encourage the author to make this part more flexible and check the performance.\n\n4. There is a concern about the fairness of the experimental setting. For instance, it was stated that XGBoost is run with a “small shared grid” only; RF fixed at 200 trees; oblique baselines appear near-default. More detailed hyperparameter tuning is necessary.\n\n5. The real-data suite is 10 classification tasks, many with d ≤ 60  and moderate n. More experiments with large-scale datasets (either n or d) would be helpful for the evaluation."}, "questions": {"value": "I have no further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kMkUJUZQsf", "forum": "lpYHuxPEXV", "replyto": "lpYHuxPEXV", "signatures": ["ICLR.cc/2026/Conference/Submission22283/Reviewer_Sr16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22283/Reviewer_Sr16"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993894495, "cdate": 1761993894495, "tmdate": 1762942150057, "mdate": 1762942150057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces JARF, a method to enhance standard axis-aligned decision forests by applying a single global supervised linear preconditioner to the input features before training. This makes the forest behave like an oblique forest without changing the RF training algorithm.\n\n\nThe key idea is:\n\n1. Fit a surrogate probabilistic classifier (a random forest) on the original data.\n2. Estimate gradients of the class probabilities with respect to each input feature on a subsample of points.\n3. Construct the Expected Jacobian Outer Product (EJOP) matrix. \n4. Use this EJOP estimate (with light regularization and normalization) as a **global linear transform** $\\hat H$ and train a standard axis-aligned Random Forest on transformed features ($X$$\\hat H$)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea is simple, clean, and easy to implement on top of existing RF code. \n- The method is well-motivated and clearly positioned between axis-aligned forests and oblique trees, leveraging prior EJOP work. \n- Experiments are solid: realistic baselines (RF, XGBoost, RotF, CCF, SPORF), multiple datasets, plus timing comparisons. \n- The mechanism analysis (alignment of oblique split normals with EJOP subspace) and ablations give good insight into why it works."}, "weaknesses": {"value": "- The method heavily depends on the quality of probability estimates from the surrogate RF used to build EJOP, which is not deeply analyzed. \n- It only evaluates standard tabular classification datasets and does not explore regression or more challenging/high-dimensional settings. - There is no direct comparison to simpler global projections (e.g., PCA, LDA) used once before RF. \n- The novelty is mostly in combining known pieces (EJOP + RF + preconditioning) rather than introducing fundamentally new theory."}, "questions": {"value": "- How sensitive is JARF to the choice and calibration quality of the surrogate model? Would using XGBoost or a small NN as the surrogate improve EJOP and performance?\n- Are there datasets or regimes where JARF clearly underperforms CCF or SPORF, indicating that a single global transform is insufficient?\n- Have you tried an EGOP-based variant for regression?\n- Your experiments use 10 classic UCI/OpenML-style tabular datasets; have you evaluated JARF on larger-scale or more modern industrial tabular dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "w9bsZQShkN", "forum": "lpYHuxPEXV", "replyto": "lpYHuxPEXV", "signatures": ["ICLR.cc/2026/Conference/Submission22283/Reviewer_kJgy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22283/Reviewer_kJgy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22283/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763112016707, "cdate": 1763112016707, "tmdate": 1763112016707, "mdate": 1763112016707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}