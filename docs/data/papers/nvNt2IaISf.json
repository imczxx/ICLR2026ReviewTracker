{"id": "nvNt2IaISf", "number": 5928, "cdate": 1757947134406, "mdate": 1759897944341, "content": {"title": "Understanding Judge Calibration in Multi-Turn Debates", "abstract": "Multi-turn debates have gained attention as language evaluation tasks for subject matter comprehension, critical reasoning and long-form responses. Language Models (LMs) play the role of judges for obtaining subjective ratings as a cheap alternative to human labor. However, similar to humans, LM judges may remain unsure of ratings and rate debate arguments either under or over-confidently. We empirically study judge calibration in multi-turn self debates, wherein a single LM debater debates with itself, and uncover that LM judges are often overconfident in their judgements. Model confidence ratings increase while rated scores may decrease over debate rounds. Judge confidence exceeds score ratings for both frontier as well as small models. We further show that while naive finetuning may improve calibration by increasing scores, it hurts the model's ability to provide faithful ratings by leading to mode collapse. Overfitted judges prefer exactly similar ratings as confidence and rate different arguments indistinguishably. Based on our empirical analysis and observations, we propose a practical finetuning strategy to calibrate LM judges. Since lower confidences and scores form the tail end of the dataset and are most desirable from a judge's perspective, we fit a Gumbel distribution on expected ratings of debate arguments. We then rejection sample from the tail of the distribution and finetune models to make calibrated judgements. Our strategy, termed Gumbel Finetuning (GFT), when compared to naive ratings and Supervised Finetuning (SFT) balances model confidence with scores while learning an expressive multi-modal distribution over ratings. Debate datasets and code will be released as part of the final version.", "tldr": "We uncover overconfidence in debate judge models and address the issue by finetuning on the left tail of Gumbel distributions of ratings.", "keywords": ["Language Models", "Calibration", "Multi-Turn Debates", "Finetuning."], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c71cd4936b99c703281d04ab057c266035962cbc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* This paper studies the ability of LLM judges to accurately evaluate debates/arguments.\n* The authors find that LLM judges are not well calibrated in their evaluations of which arguments are correct, and e.g. rate different arguments similarly.\n* The authors then propose a finetuning strategy to better calibrate LLM judges, finding that this helps for improving LLM judge confident"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Improving LLM judges is an important area for being able to continue to train models to be effective and safe, even as their capabilities grow more and more (especially at the point when LLMs exceed human capabilities on some domains). So this paper is looking into an important area/problem\n* The result that LLMs aren’t effective judges is interesting/important (though I think similar results have been shown before). I think the result that models aren’t effective judges even after finetuning would be big if true"}, "weaknesses": {"value": "* While the problem is important, I don’t think it’s that important for the reasons outlined in the paper (i.e., just as an evaluation task), and some of the more compelling motivations (for providing a scalable supervision signal for LLMs as they grow more and more capable) for having LLM judges are omitted from the intro.\n* I found the paper very hard to follow, and e.g. didn’t follow the explanation for why a more sophisticated approach involving Gumbel distributions was needed over SFT. It really feels like simple SFT should be able to improve judge confidence, and rare that other approaches beat SFT when labels are available (especially by a large margin), so I think the bar for motivation and evidence for using anything other than SFT is high.\n* It would be pretty helpful/illustrative to show some qualitative samples of what the arguments/debates look like (ideally in each domain), so the reader has a sense of what the debates look like. Otherwise, it’s a bit hard to picture or know what’s going on. It looks like there are a few at the very end of the appendix, but I think these are important enough to include in the main paper"}, "questions": {"value": "Can you explain in simple terms why you think SFT is not sufficient for getting good judges, and how the GFT procedure in these paper addresses that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QCpheXH9vK", "forum": "nvNt2IaISf", "replyto": "nvNt2IaISf", "signatures": ["ICLR.cc/2026/Conference/Submission5928/Reviewer_o4fq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5928/Reviewer_o4fq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578742770, "cdate": 1761578742770, "tmdate": 1762918357018, "mdate": 1762918357018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to study the calibration of judge’s final judgments in an LLM debate setup. LLM debates have been shown to be useful for producing more truthful final judgments of human and LLM judges (judges are shown transcripts of debates between LLMs). To the best of my understanding, experiments elicit a numerical confidence score (as textual output) from the judge alongside its numerical rating for the proposition under debate (as textual output). The paper poses that there is an issue when the rating does not match the confidence score. This is called the score-confidence gap. A second metric, the Expected Calibration Error, is defined based on a distance between the judge confidence ratings and the output of some accuracy quantity acc(.,.) — not sure I am following this part. A third metric, the Brier score, is computed between the judge confidence score and a human confidence score. After showing that there are issues with judge calibration according to these metrics, the paper proposes a finetuning method that works by oversampling certain regimes of the distribution of LLM debates. A model trained on such an oversampled distribution, rather than a more straightforward SFT approach, achieves better calibrated judge confidences under the reported metrics."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Important: The paper addresses an interesting topic, the calibration of judges in LLM debate experiments. We should hope that debate not only improves judge accuracy, but also improves judge calibration."}, "weaknesses": {"value": "- Very important: I find the core measurements in the paper quite confusing. To the best my of understanding, the central premise is that the rating scores should match the confidence scores. I do not see why that should be the case. When I am reviewing a paper for ICLR, I give a rating score and a confidence score — why should these scores match? Let’s zoom out a bit: judges could give a full probability distribution over the range of scores, 1-10. Then there are many ways one might think of confidence. You could compute confidence as entropy, and get a rating as the expected value of this distribution. What is the elicited confidence score supposed to represent? The probability of the rating, p(rating), of that full distribution? Or something else? The paper does not provide sufficient justification for this core premise.\n- Very important: It is not optional to include information about the human ratings used in the study. Where did these people come from? What were they told to do? How many humans rated each debate? Did they have pre-existing opinions on the debate topics — did debates move their ratings up or down? These details are crucial to understanding the meaning of the data used in this paper. Moreover, I could not describe the human ratings as a “ground truth oracle” for subjective debate topics like those studied in the paper, even if all the above details were provided. As a result, I find it very difficult to interpret the ECE and Brier scores used."}, "questions": {"value": "Please do try to clarify the motivation behind the score-confidence gap and other calibration metrics used in the paper.\n\nOn the related work, for the \"Uncertainty in Language Models\" section, this is a nice tour of recent papers from Anthropic but it does not go into much depth on work on uncertainty. I'd recommend looking into https://arxiv.org/pdf/2012.14983 and https://arxiv.org/pdf/2207.05221"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tBFiDmdik1", "forum": "nvNt2IaISf", "replyto": "nvNt2IaISf", "signatures": ["ICLR.cc/2026/Conference/Submission5928/Reviewer_SZca"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5928/Reviewer_SZca"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931216808, "cdate": 1761931216808, "tmdate": 1762918356653, "mdate": 1762918356653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors discuss the calibration of language model (LM) judges in multi-turn debates, highlighting issues of overconfidence in their ratings and propose a Gumbel Fine-Tuning (GFT) strategy to improve the calibration of LM judges by focusing on lower confidences and scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper follows a clear strucutre and is well written. \nThe authors systematically show that model confidence increases over debate rounds while scores decrease, across multiple frontier and smaller models. The proposed Gumbel Finetuning (GFT) is an original approach that leverages the tail of the data distribution to fine-tune calibration."}, "weaknesses": {"value": "The paper only examines a limited number of models and debates for 10 rounds on five different topics. Only a Llama 3.2 3B model was fine-tuned in the experiments, so it remains unclear if GFT scales to larger judges. Additionally, it is unclear whether the gap between confidence and scores negatively affects the performance of LM judges or if they perform better after GFT fine-tuning. The scope of the paper is narrow because the authors only examine calibration in multi-turn self-debates.\n\nSmall comments:\n- 106: \"may be\" instead of \"maybe\"\n- In the Related Work section, most references should be written out in the text (use the command \"\\citet\" instead of \"\\citep\" in LaTeX).\n- 127: \"with\" instead of \"wit\"\n- 146: \"an LM\" instead of \"a LM\"\n- 160: The references for Claude Sonnet and Grok 3 seem incorrect (AI and xAI).\n- Formulas (1) - (3): I think the more common symbol for expectation is either E in LaTeX or , the symbol confused me at first. \n- 468: The \"and\" at the beginning of the line is excessive."}, "questions": {"value": "- Is the small 5.56% ECE improvement statistically significant across multiple seeds or runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a0zGaNZ4kJ", "forum": "nvNt2IaISf", "replyto": "nvNt2IaISf", "signatures": ["ICLR.cc/2026/Conference/Submission5928/Reviewer_iqFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5928/Reviewer_iqFF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935715334, "cdate": 1761935715334, "tmdate": 1762918355980, "mdate": 1762918355980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies judge calibration in multi-turn LLM debates, specifically, the mismatch between model-assigned scores and their self-reported confidence. The authors show that LMs acting as judges tend to become over-confident. They propose Gumbel Finetuning (GFT), intended to improve calibration by focusing on low-confidence/low-scores."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "-  **[Problem importance]** The paper addresses an important issue in LLM evaluation, namely judge calibration.\n- **[Wide model coverage]** While the overall experimental scope is quite limited (20 total debates) the authors make use of a wide range of model families.\n- **[Clarity/presentation]** The paper is well-structured and clearly outlines the methodology, motivation, and approach.\n- **[Somewhat novel observations]** Although issues with calibration and confidence have been widely studied in the single agent and single-turn (ensamble) settings, the observation of overconfidence drift in multi-turn debates is new."}, "weaknesses": {"value": "- **[Minimal methodological novelty]:** The proposed GFT is a simple rejection-sampling heuristic applied to SFT; it doesn’t constitute a new calibration framework. While the motivation for the rejection clear, I would have liked to see a deeper connection established between the confidence distributions present in the training data and the subsequent distributions produced by the model.\n- **[Limited experimental scope]:** Only 20 debates are used throughout the paper. It is extremely difficult to appreciate the generality of these the observed trends with such a small sample size. \n- **[Shallow analysis]:** Beyond the issue of small experiental scope, the paper also does not dig into *why* overconfidence arises, or how this overconfidence differs from existing analysis in the case of single agents or single turn aggregation. Are we simply making the same observations as prior work (e.g., Zheng et al. 2023; Guo et al. 2017; Zhu et al. 2025), and the multi-turn aspect is just an red herring? I would have liked to see this better contextualized. Similarly its not clear how GFT actually addresses this issue, beyond some high level intuition regarding rare events. \n- **Evaluation metrics are weak:** ECE and Brier scores are reported without confidence or significance tests, making the improvements difficult to interpret (I assume this is the result of the small sample size)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XSpuZGki8a", "forum": "nvNt2IaISf", "replyto": "nvNt2IaISf", "signatures": ["ICLR.cc/2026/Conference/Submission5928/Reviewer_Ng1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5928/Reviewer_Ng1Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762291111369, "cdate": 1762291111369, "tmdate": 1762918355662, "mdate": 1762918355662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}