{"id": "h1t9kRSbLb", "number": 4293, "cdate": 1757657332767, "mdate": 1759898041203, "content": {"title": "R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning", "abstract": "Chain-of-thought (CoT) enhances the problem-solving ability of large language models (LLMs) but incurs substantial inference cost due to long autoregressive trajectories. Existing acceleration strategies either shorten traces via early stopping or compression, or adopt speculative decoding with a smaller model. However, speculative decoding provides limited gains when model agreement is low and rigidly enforces token-level consistency, overlooking the observation that some smaller models, when correct, produce significantly more concise reasoning traces that could reduce inference length.\nWe introduce R-Stitch, a training-free hybrid decoding framework that leverages token-level entropy as an uncertainty proxy to delegate computation between a small language model (SLM) and an LLM. Our analysis shows that high-entropy tokens are more likely to induce errors, motivating an entropy-guided routing strategy that lets the SLM efficiently handle low-entropy tokens while delegating uncertain ones to the LLM, thereby avoiding full rollbacks and preserving answer quality.\nWe further extend this design with R-Stitch$^+$, which learns an adaptive routing policy to adjust the token budget dynamically beyond fixed thresholds. By jointly reducing per-token decoding complexity and the number of generated tokens, our method achieves substantial acceleration with negligible accuracy loss. Concretely, it attains peak speedups of $3.00\\times$ on DeepSeek-R1-Distill-Qwen-7B, $3.85\\times$ on 14B, and $4.10\\times$ on QWQ-32B while maintaining accuracy comparable to full LLM decoding. Moreover, it naturally enables adaptive efficiency–accuracy trade-offs that can be tailored to diverse computational budgets without retraining.", "tldr": "", "keywords": ["large language model", "chain-of-thought reasoning", "efficient inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/798043604d87325ff65de78bcf04ffa903671031.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes R-Stitch, a training-free hybrid decoding method that accelerates chain-of-thought reasoning by dynamically switching between a small language model and a large language model based on token entropy. Low-entropy tokens are handled by the SLM to save computation, while high-entropy tokens are delegated to the LLM to preserve accuracy. It shows that token entropy correlates with reasoning errors and uses it for token-level routing. In addition, the paper extends R-Stitch to R-Stitch+, which learns an adaptive switching policy via reinforcement learning with a latency-aware reward. Evaluation shows seepdup in generation with minimal accuracy loss on tested benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an important and timely problem: optimizing reasoning-time inference in large language models. As reasoning chains become longer and more expensive, improving efficiency without sacrificing accuracy is crucial for practical deployment.\n\n- The use of entropy as a routing signal is well-motivated and empirically justified. Figure 3 effectively shows that incorrect outputs correlate with higher entropy, that most tokens have near-zero entropy, and that errors cluster around locally uncertain regions. This analysis builds a foundation for using entropy as a measure to guide dynamic switching.\n\n- The approach leverages model-internal signals to optimize system-level behavior, is conceptually elegant."}, "weaknesses": {"value": "- The novelty is limited. The method is conceptually incremental to speculative decoding, differing mainly in using entropy for routing rather than token-level agreement. Moreover, the connection between entropy, confidence, and uncertainty has been explored in prior work [1], yet this paper neither cites nor compares against them.\n\n- The evaluation setting (batch size = 1) is unrealistic and potentially misleading. In real deployments, systems run with batch sizes > 1 to maximize GPU utilization, especially in frameworks like vLLM, which fully support batching. Running with batch = 1 may exaggerate latency gains, and the authors should justify this design choice.\n\n- The evaluation scope is narrow. All experiments and entropy analyses are restricted to mathematical reasoning tasks. Since the paper claims general acceleration of chain-of-thought reasoning, results across other reasoning domains (e.g., code, logical reasoning, or QA) would make the findings more convincing.\n\n- The paper ignores the memory overhead of maintaining both the LLM and SLM simultaneously. This dual-model setup increases GPU memory consumption and can reduce the number of requests processed in parallel. It’s possible that this constraint is why the experiments use batch size = 1, which should have been discussed explicitly.\n\n- The method is not compared to alternative reasoning optimization approaches such as early termination or compression-based techniques, which also shorten reasoning traces. A fair evaluation should include or discuss these baselines to clarify the relative contribution.\n\n- The approach’s robustness to entropy misprediction is unclear. If entropy fails to reflect uncertainty accurately (e.g., when the SLM is overconfident in incorrect outputs), the routing may degrade accuracy or fail to switch appropriately.\n\n- The computational overhead of computing token entropy is not thoroughly analyzed. Although entropy can be obtained from logits, its per-token computation and switching logic could add nontrivial latency, which should be measured or discussed.\n\n[1] Efficiently serving llm reasoning programs with certaindex. Fu, Yichao ; Chen, Junda ; Zhu, Siqi ; Fu, Zheyu ; Dai, Zhongdongming ; Zhuang, Yonghao ; Ma, Yian ; Qiao, Aurick ; Rosing, Tajana ; Stoica, Ion ; Zhang, Hao"}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TRjParuEFW", "forum": "h1t9kRSbLb", "replyto": "h1t9kRSbLb", "signatures": ["ICLR.cc/2026/Conference/Submission4293/Reviewer_saCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4293/Reviewer_saCs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582488861, "cdate": 1761582488861, "tmdate": 1762917279973, "mdate": 1762917279973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on efficient generation for long-CoT LLM. Long-CoT LLM normally generated extensively long trajectory, which is very time consumed. Speculative decoding (SpecDec) serves as traditional solution to speed it up. I.e. only use a LLM to generate when the tokens generated from the SLM are rejected. However, SpecDec requires a strict distribution alignment between LLM and SLM, and might falsely reject tokens that are correct but not aligned with LLM. \n\nIn this paper, the authors firstly observe that high entropy normally leave to incorrect generation, thus propose to use entropy itself as a hint for token rejection and acceptance, i.e. R-Stitch. If the entropy of a token from SLM is too high, the decoding is switched to LLM until meeting a new token with a low entropy, then the decoding is switched again to SLM. The authors further proposes another variant, R-Stich$^+$, that applies RL to train the model for better accuracy and efficiency gain.\n\nThrough extensive experiments, R-Stich and R-Stich$^+$ show comparable or better accuracy to the LLM and SpecDec, while significantly improving the speedup."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The observation of high entropy leading to incorrect trace is interesting, and well investigated. And the proposed method is weel aligned with the observation.\n2. The experiments are thorough, with multiple LLMs and benchmarks, showing the benefits from R-Stitch.\n3. The ablation study is well-designed, justifying the design choice."}, "weaknesses": {"value": "1. The choice of SLM is not reasonable. L1-1.5B-Short is used as SLM, while the target model is DeepSeek-R1 family. As we know, SpecDec is efficient when both draft and target model's distribution is aligned. From Table 1, SpecDec's speedup is even worse than the target model alone, which is unreasonable. It's suggested to include new results with SLM from the same family.\n2. Lack of baselines. Only two baselines are included here, LLM and SpecDec. It's suggested to include recent strong baselines for better justification, like:\n\n[1] Reward-Guided Speculative Decoding for Efficient LLM Reasoning\n\n[2] AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models via an Entropy-based Lower Bound on Token Acceptance Probability"}, "questions": {"value": "### Suggestions\n1. Better to highlight the best results in Table 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i2cV6UkE9t", "forum": "h1t9kRSbLb", "replyto": "h1t9kRSbLb", "signatures": ["ICLR.cc/2026/Conference/Submission4293/Reviewer_mk62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4293/Reviewer_mk62"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994302645, "cdate": 1761994302645, "tmdate": 1762917279668, "mdate": 1762917279668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose entropy-guided hybrid decoding which switches between a SLM and LLM during chain-of-thought reasoning:\n\n1. They uses normalized token entropy H_t = -Σp_{t,i}log(p_{t,i})/log(V) as an uncertainty proxy\n2. SLM to LLM when entropy > τ (high uncertainty)\n   LLM to SLM when entropy <= τ (low uncertainty, save computation)\n3.  Maintains separate caches with partial prefill to minimize switching overhead\n\nThey add a learned routing policy via RL:\n- Lightweight router decides when to invoke LLM for high-entropy tokens\n- R = r_acc - λ·r_acc·L (penalizes latency only when correct)\n- Linear regression models T(N_inf, N_kv) to avoid profiling overhead during training\n- Group-normalized advantages for policy gradient\n\n## Here are my thoughts\nThe paper conflates two distinct phenomena:\n- Model disagreement (what speculative decoding addresses)\n- Verbosity differences (SLM produces shorter traces)\n\nThe core claim that \"speculative decoding's rigid token-level consistency prevents using SLM's conciseness\" is misleading?? Speculative decoding doesn't prevent conciseness, it enforces correctness. If the SLM produces a correct but shorter solution, speculative decoding will accept it. The issue is that SLMs are often wrong, not just verbose.\n\n2. Unfair Experimental Comparisons?\n- Speculative decoding baseline uses high-agreement pairs (Distill-7B + Distill-1.5B) while R-Stitch uses low-agreement pairs (LLM + L1-Short)\n- Is this maube backwards? Speculative decoding should get the favorable pairing?\n- Are these cherry-pick scenarios where speculative decoding fails while giving R-Stitch optimal conditions?\n\n3. Lack of Correctness Guarantees:\n- R-Stitch discards SLM tokens and overwrites with LLM when entropy is high\n- This creates a correctness risk: What if the SLM was actually right but uncertain? The LLM might introduce errors\n- It would be nice to have an analysis of cases where switching to LLM hurts accuracy\n- Table 1 shows accuracy drops in many settings (e.g., 7B on AIME: 33.33->30.00 at τ=0.03)\n\n4. Entropy Is a Weak Proxy?\nThe empirical analysis (Section 3.2.1) is maybe superficial?\n- Figure 3a: \"Incorrect answers have higher entropy\" - but correlation does not equal causation\n- No comparison to other uncertainty measures (variance, top-k probability gaps, etc.)\n- Figure 3c: \"Harmful tokens have higher preceding entropy\" - but the effect size is tiny (~0.028 vs ~0.024)\n- 10.65% of tokens exceed entropy 0.1 - does this means the routing decision is rarely invoked?\n\n5. Speedup Claims\n- Peak speedups (3.00x, 3.85x, 4.10x) come with significant accuracy drops\n- At τ=0.02 (claimed \"sweet spot\"), accuracy often decreases 2-5 points\n- Speculative decoding maintains accuracy by construction\n- It would be nice to report Pareto frontiers \n\n\n7. Method\n- Entropy-based routing is already used in early exit, mixture-of-experts, etc.\n- Is the \"stitching\" framing marketing? This is just conditional execution?\n- R-Stitch+ is standard REINFORCE with a domain-specific reward\n- The related work section (A.5) acknowledges EAGLE, Griffin, Hydra do similar things but claims they \"increase consistency\" while R-Stitch \"exploits inconsistency\" - I might be wrong, but this feels like  a false dichotomy\n\n9. Minor Issues\n- No error bars or confidence intervals (use SE if you can)\n- Small test sets (30 samples on AIME)\n- Table 1 shows accuracy increasing from 66.27 (LLM) to 77.11 (R-Stitch τ=0.001) on AMC 8k - this suggests variance, not real improvement?\n\n## Fundamental Question\n\nWhy not just use the LLM with early stopping? If the issue is that LLMs are verbose, methods like DEER (which the paper briefly mentions) achieve similar latency reductions without the complexity of dual models. The paper doesn't convincingly argue why heterogeneous model collaboration is necessary?\n\n\nI'm recommending weak accept, but I can move my score up if you address my issues. Thanks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths are in the above review."}, "weaknesses": {"value": "Weaknesses are in the above review."}, "questions": {"value": "Why not just use the LLM with early stopping? If the issue is that LLMs are verbose, methods like DEER (which the paper briefly mentions) achieve similar latency reductions without the complexity of dual models. The paper doesn't convincingly argue why heterogeneous model collaboration is necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "K9e345NxKB", "forum": "h1t9kRSbLb", "replyto": "h1t9kRSbLb", "signatures": ["ICLR.cc/2026/Conference/Submission4293/Reviewer_whxy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4293/Reviewer_whxy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006323188, "cdate": 1762006323188, "tmdate": 1762917279464, "mdate": 1762917279464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes R-Stitch, a bidirectional, entropy-guided collaboration between a small language model (SLM) and a large language model (LLM) for chain-of-thought reasoning. Decoding starts on the SLM, switches to the LLM when the SLM’s token entropy exceeds a threshold, and switches back once the LLM becomes confident, which preserves the SLM’s concise spans while using the LLM for hard tokens. To keep switching efficient, the system maintains separate KV caches and performs partial prefill so each model only recomputes tokens generated since the last switch. The R-Stitch+ variant adds a lightweight router trained with a latency-aware reward that penalizes runtime only when the answer is correct and relies on a profiled latency estimator to avoid online timing. Implemented in vLLM and evaluated on five math benchmarks with 7B, 14B, and 32B models, the method reduces per-sample wall-clock latency on a single A100 at batch size one, delivering roughly 1.4×–3.0× speedups at 7B/14B and up to around 4× at 32B under 8k–16k budgets while maintaining accuracy close to full LLM decoding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The core algorithm is simple and training free in its base form, using a clear entropy threshold to switch between SLM and LLM in both directions so that the system exploits concise SLM spans without sacrificing reliability on high-uncertainty tokens.\n* The method is well motivated by an empirical analysis showing that incorrect answers have higher token entropy and that most tokens are very low entropy, which justifies entropy as a routing signal.\n* The systems design is thoughtful, with explicit KV-cache management and partial prefill that reuses past caches on each model to avoid redundant attention and reduce switching overhead."}, "weaknesses": {"value": "* The router is only described as a “lightweight” module fed by hidden states; its architecture, parameter count, placement, and per-token overhead are not reported, so deployability and reproduction costs are unclear.\n* All latency results use a single GPU with batch size one. The current implementation only supports batch size one because switching happens at the token level. Real-world throughput under concurrent traffic is unknown.\n* The system runs two engines with separate KV caches. This increases VRAM usage and system complexity. Partial prefill reuse is described, but the paper does not quantify memory costs or switching overhead.\n* Performance depends on the entropy threshold. The paper tunes the threshold by sweeping values across a grid. There is no automatic rule that transfers across datasets and model pairs, which raises generability concerns of real-world deployments.\n* The overall router design will make the system hard to scale and deploy comparing with methods like Eagle-3 etc."}, "questions": {"value": "* Please specify the router: architecture, parameter count, compute placement, input features, and measured per-token overhead (and its share of end-to-end latency).\n* For R-Stitch+, how robust is the learned policy across domains and model sizes? Please include cross-domain transfer results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QJu0VlSyGE", "forum": "h1t9kRSbLb", "replyto": "h1t9kRSbLb", "signatures": ["ICLR.cc/2026/Conference/Submission4293/Reviewer_7aFp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4293/Reviewer_7aFp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762108652334, "cdate": 1762108652334, "tmdate": 1762917279174, "mdate": 1762917279174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}