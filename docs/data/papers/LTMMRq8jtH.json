{"id": "LTMMRq8jtH", "number": 16069, "cdate": 1758259429428, "mdate": 1759897263898, "content": {"title": "LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling", "abstract": "Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR’s generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.", "tldr": "", "keywords": ["image generation", "autoregressive"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d46b6964bce039af9219ba53574da28e71807fd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Latent Scale Rejection Sampling (LSRS), a test time scaling method for visual autoregressive (VAR) models. LSRS introduces lightweight scoring and rejection sampling at each layer during inference, prioritizing early scales that most affect structural quality. Experimental results demonstrate that LSRS significantly improves generation quality (e.g., FID reduced from 1.95 to 1.66)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The exploration of TTS for VAR is a previously unexplored area, and this work provides valuable insights for future research.\n\n- The proposed method achieves further performance improvements over the original VAR through test-time computation.\n\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- Equation 2 might be incorrect. In VAR generation, $r_k(i,j)$ should also be conditioned on other tokens at the same scale, but this is not included in the equation.\n\n- The proposed idea seems somewhat too simple: training a classifier to assess the quality of token maps across different scales.\n\n- The effectiveness of the proposed TTS approach for VAR does not show computation scaling property, as shown in Figure 3. Although I appreciate the authors’ detailed explanation of this phenomenon, it still affects the practicality of the proposed method.\n\n- The paper mainly uses BoN and top-k methods for TTS. I wonder whether more advanced approaches, such as beam search, could further improve performance."}, "questions": {"value": "- The proposed method is mainly validated on class-conditioned image generation tasks. How does it perform on text-to-image T2I tasks, such as using the VAR-based T2I model Infinity[1]?\n\n- The proposed method scales test time to enhance the generation quality of the VAR model. It would thus be interesting to apply the proposed TTS method to the VAR-acceleration approaches such as FastVAR [2], to see whether the resulted variants could achieve comparable efficiency to the baseline model but delivering superior generation results?\n\n- I would be glad to increase my rating if the authors could address the above concerns.\n\n> [1] Infinity : Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis. CVPR25\n\n> [2]FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning. ICCV25"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bWAnBh0qiY", "forum": "LTMMRq8jtH", "replyto": "LTMMRq8jtH", "signatures": ["ICLR.cc/2026/Conference/Submission16069/Reviewer_kcQM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16069/Reviewer_kcQM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619111730, "cdate": 1761619111730, "tmdate": 1762926258181, "mdate": 1762926258181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LSRS, a method that progressively refines token maps in the latent scale during inference to enhance VAR models. It first analyzes the structural errors observed in images generated by VAR models and attributes them to the imperfect parallel sampling mechanism, particularly at the early scales. Motivated by this finding, LSRS performs latent-scale rejection sampling, where a scoring model is trained to select the intermediate token maps with the highest predicted quality score. Experimental results demonstrate that LSRS improves the image generation quality of VAR models while incurring only minimal additional computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The analysis of VAR’s mechanisms and inherent limitations provides valuable insights, revealing that earlier scales play a more critical role in determining overall image structure.\n- The proposed latent scale rejection sampling (LSRS) method is technically well-founded, combining real and synthetic data construction, scoring model training, and token map selection guided by the scoring model.\n- Extensive experiments on the ImageNet image generation task demonstrate the effectiveness of LSRS across various VAR base models, while comprehensive ablation studies offer further insights into its design and performance."}, "weaknesses": {"value": "- The relationship between the imperfect parallel sampling mechanism and the proposed LSRS sampling method is not clearly explained. Since the base VAR models remain unchanged and LSRS just runs the base models several times for certain latent scales, it is unclear how LSRS effectively mitigates the limitations of the mutually independent token sampling mechanism.\n- While the ImageNet experiments are sufficient to demonstrate the effectiveness of the proposed method, the paper would be further strengthened by including results on text-to-image (T2I) tasks and evaluating additional metrics. This could be achieved by extending experiments to VAR variants designed for T2I generation, such as HART [1] and Infinity [2].\n- Some of the experimental analyses are not entirely convincing:\n  - Line 370 states that \"Applying LSRS from the first scale causes many samples to converge to similar values at this scale, thereby reducing generation diversity and degrading FID.\" However, as shown in Figure 3, the IS score also decreases in this case. In my understanding, a lower diversity would not reduce IS.\n  - Similarly, line 411 claims that \"This may again be attributed to the fact that excessively large M could reduce the diversity of generated images.\" Yet, IS still drops as M increases, which seems inconsistent with this explanation.\n- There are a few typographical errors in the manuscript that should be carefully corrected. For example:\n  - Line 196, \"Brock et al., **1809**\"\n\n[1] HART: Efficient Visual Generation with Hybrid Autoregressive Transformer\n[2] Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis"}, "questions": {"value": "- My understanding of the experimental setup is as follows: there are K scales in VAR image generation, where scale 1 contains a single token and scale K has the largest token map. From scale 1 to scale ST−1, only one sample is generated, while from scale ST to scale K, M samples are drawn. If this understanding is correct, several questions arise:\n  - The previous analysis states that \"Therefore, if we can optimize th early scale, we can then efficiently improve the quality of the final images.\" This seems inconsistent with the implementation, where multiple samples are drawn only from scale ST to scale K, rather than from the early scales that are claimed to be more critical.\n  - It is also stated that LSRS introduces only minimal computational overhead. However, under the ST=2, M=4 setting, all scales except scale 1 appear to be sampled four times, which would result in nearly 4x computational cost. The reported overhead of only 0.01x is therefore unclear and warrants further explanation.\n- In line 265, it is stated that \"To prevent data leakage, the random seeds used for constructing the VAR sampling dataset are different from those employed during evaluation\". Could the authors elaborate on how random seeds are used in this process and how their reuse could cause data leakage?\n- Regarding the pairwise loss defined in Equation (4), could the authors clarify how the pairs are formed between real samples and synthetic samples during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KsTpFwqMf1", "forum": "LTMMRq8jtH", "replyto": "LTMMRq8jtH", "signatures": ["ICLR.cc/2026/Conference/Submission16069/Reviewer_qV4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16069/Reviewer_qV4B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729015943, "cdate": 1761729015943, "tmdate": 1762926257659, "mdate": 1762926257659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Latent Scale Rejection Sampling (LSRS) to mitigate the problem that parallel token sampling within a scale may lead to structural errors in VAR. The LSRS employs a lightweight scoring model to select the highest-quality one from generated multiple candidate token maps. Experiments demonstrate that LSRS improves VAR’s generation quality with minimal additional computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a key limitation of current VAR models: parallel sampling many tokens within a scale in a single step may brings a degradation in the quality of generation.\n2. The LSRS introduces the rejection sampling in the latent space of multi-scale autoregressive models, and it is simple and lightweight, making it highly practical for deployment.\n3. The propose LSRS method improve the generation quality than VAR, which reduce its FID score from 1.95 to 1.78 while increasing the inference time by merely 1%."}, "weaknesses": {"value": "1. The issue in this paper is that parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To a certain extent, LSRS reduces the structural errors in the generation process, but it did not directly address the root cause of the problem, parallel sampling many tokens. Instead, It merely selected the best result from the candidate pool.\n2. Figure 1 does not fully illustrate the issue that parallel token sampling within a scale may lead to structural errors. Because this structural error is caused by replacing the token maps, which merely indicates that errors at the early scales are earlier to affect the generation quality of the model.\n3. Increasing the sampling steps in each scale is a more fundamental and direct method, which directly addresses the essence of the issue. However, there were no corresponding comparative experiments in the paper.\n4. The ratio between computational cost and performance return is not adequately discussed in the later scales, because it is observed that earlier scales have a greater impact on the image structure in the paper."}, "questions": {"value": "1. Why is it said that applying LSRS from the first scale causes many samples to converge to similar values at this scale in line 370? There is no direct evidence in the paper;\n2. Why are no ablation experiments conducted on the later scales about the use or non-use of LSRS?\n3. Can enhancing the ability of the score model improve the generation performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mFPAzIXODZ", "forum": "LTMMRq8jtH", "replyto": "LTMMRq8jtH", "signatures": ["ICLR.cc/2026/Conference/Submission16069/Reviewer_Wg8z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16069/Reviewer_Wg8z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824308539, "cdate": 1761824308539, "tmdate": 1762926257180, "mdate": 1762926257180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Latent Scale Rejection Sampling for VAR, which trains a lightweight scoring model to evaluate multiple candidate token maps and select the most high-quality one to guide subsequent generation. Experimental results demonstrate that this method improves generation quality while incurring negligible inference latency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The experiments and ablation studies are detailed and comprehensive."}, "weaknesses": {"value": "1. The paper does not discuss the generality of the proposed method. Since all experiments are conducted on ImageNet, the scoring model is also trained on ImageNet, which is in-domain data. Can a scoring model trained on ImageNet be effectively applied to other domains, such as text-to-image models? Or, to adapt this method for text-to-image generation, would it be necessary to retrain the scoring model on domain-specific data?\n2. The structural error problem in parallel decoding is not first identified in this paper. The related work Infinity[1] has also discussed this issue and proposed a training-based solution to address it. Since rejection sampling cannot inherently eliminate structural errors but only select relatively better outputs, how does the proposed method perform in comparison with Infinity?\n3. How does the proposed method perform on relatively smaller models? In Table 1, results are only reported for the FlexVAR 1B and VAR 2B models, while no results are provided for smaller models such as VAR 300M or VAR 600M.\n\n[1] Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis"}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VgdFCdGtUi", "forum": "LTMMRq8jtH", "replyto": "LTMMRq8jtH", "signatures": ["ICLR.cc/2026/Conference/Submission16069/Reviewer_QqZ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16069/Reviewer_QqZ6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970271944, "cdate": 1761970271944, "tmdate": 1762926256788, "mdate": 1762926256788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}