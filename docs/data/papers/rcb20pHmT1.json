{"id": "rcb20pHmT1", "number": 23076, "cdate": 1758339138739, "mdate": 1759896833278, "content": {"title": "HiPO: Self-Hint Policy Optimization for RLVR", "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) is a promising method for enhancing the complex \nproblem-solving abilities of large language models (LLMs). This is particularly evident in domains requiring \nlong-horizon reasoning and precise execution, such as solving complex mathematical problems where solutions \nhinge on a fragile sequence of tool-based actions. However, current approaches are often crippled by two \ninterconnected issues: the near-miss problem, where sparse rewards nullify the learning signal for \nalmost-correct attempts, and the resulting exploration stagnation, which prevents the model from \ndiscovering better solutions. To address these challenges, we introduce HiPO (Hint-guided Policy Optimization), \na novel RLVR framework that enables the agent to learn from its own rare successes. \nOur core insight is to capture an occasional successful trajectory within a training batch and\nrepurpose its initial correct steps as an on-policy “hint”. This process \ntransforms a single, stochastically-found success into a dense contrastive learning signal, \neffectively allowing the model to teach itself how to overcome the near-miss \nproblem and break exploration stagnation. On a challenging suite of five mathematical reasoning benchmarks, \nHiPO improves the average avg@32 by +5.0 percentage points (pp) over the strong GRPO baseline. \nThis improvement is driven by substantial absolute point gains on challenging datasets, \nincluding +10.3 pp on CMIMC 2025, +4.9 pp on BRUMO 2025, +4.6 pp on AIME 2024, and +3.1 pp on AIME 2025.\nFurthermore, HiPO demonstrates a new exploration paradigm, \nrepurposing rare successes into reusable guidance to significantly accelerate skill acquisition for complex tasks, \nestablishing a more efficient and scalable path for models to autonomously master intricate reasoning.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Mathematical Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72cb95f96331f3fa3fd9eb5ac751401c626771fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces HiPO (Hint-guided Policy Optimization), a framework for Reinforcement Learning from Verifiable Rewards (RLVR) that addresses the near-miss problem and exploration stagnation in long-horizon reasoning tasks.\n\nThe key idea of HiPO is endogenous self-hinting. When the model occasionally finds a successful trajectory, it extracts the initial correct steps (prefix) of that trajectory and reuses them as on-policy “hints” for future training. This turns a single sparse success into a dense, contrastive learning signal, allowing the model to bootstrap from its own rare successes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- HiPO directly targets signal collapse and credit misassignment, two critical issues \n- the idea of self hint is interesting\n- improved empirical results"}, "weaknesses": {"value": "- dependence on rare success\n- lacks a formal analysis of convergence properties\n- computational overhead, need to generate original and hint-guided groups, roughly doubles the computational cost compared to GRPO."}, "questions": {"value": "I would like to thank the authors for their work.here are a few concerns and questions:\n\n- Reward hacking: How do the authors ensure that the use of hints and dense rewards does not lead to reward hacking behaviors, where the model optimizes for superficial alignment with hints rather than reasoning improvement?\n\n- Quality of hints from rare successes: How can we guarantee that the hints extracted from rare successful trajectories are actually desirable? In practice, many successful reasoning traces can be unnecessarily long or include redundant steps. How does the method handle such cases?\n\n- Exploration limitation: Is there a risk that the discovered traces—and consequently the training signals—are limited to the reasoning patterns already found by the model, thus constraining exploration and generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ID9k3T6z9C", "forum": "rcb20pHmT1", "replyto": "rcb20pHmT1", "signatures": ["ICLR.cc/2026/Conference/Submission23076/Reviewer_Hyap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23076/Reviewer_Hyap"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752382694, "cdate": 1761752382694, "tmdate": 1762942502385, "mdate": 1762942502385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HiPO (Hint-guided Policy Optimization), a novel framework for Reinforcement Learning from Verifiable Rewards (RLVR) aimed at improving large language models' (LLMs) performance on complex mathematical reasoning tasks. It addresses key challenges in existing methods like GRPO, including the near-miss problem, where nearly correct trajectories receive no positive signal, and exploration stagnation due to sparse rewards leading to policy collapse. HiPO's core innovation is an endogenous self-hint mechanism: within a training batch, rare successful trajectories are identified, and their initial correct prefixes (sampled at ratios between 0.05 and 0.45) are repurposed as on-policy hints to generate augmented groups for low-success or null-signal batches. This creates a dense contrastive learning signal by contrasting unaided and hint-guided rollouts, enabling the model to bootstrap from its own successes without external data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "HiPO's self-hint paradigm is novel, which transforms rare successes into reusable on-policy guidance, avoiding off-policy mismatches and enabling true autonomy in learning complex reasoning chains. HiPO sustains 20-30% higher entropy than GRPO (Figure 4a) and doubles learnable group proportions from 17.5% to 75.8% (Figure 5b), directly validating its anti-stagnation claims. \n\nThe writing is generally clear, with intuitive visuals (e.g., Figure 2's batch augmentation) and case studies (Tables 2-4) that concretely show HiPO's strategic exploration (e.g., algebraic restructuring) versus GRPO. \n\nThe approach is timely and significant for dealing with sparse-reward domains like math competitions, where it yields domain-general improvements paving the way for scalable, data-efficient RLVR without curated hints."}, "weaknesses": {"value": "1. While HiPO effectively leverages rare successes, its activation strictly requires at least one success per near-miss group (success rate >0 but <50%), raising concerns about performance in ultra-sparse regimes where batches might entirely lack successes, e.g., early training or harder curricula, potentially amplifying GRPO's signal collapse rather than resolving it.\n\n2. The fixed hint ratio range [0.05, 0.45] is justified in Appendix B to avoid signal collapse from long prefixes, but no ablation shows optimal tuning or impact on final performance, leaving hyperparameter sensitivity unclear.\n\n3. The experiments are strong on math but no evaluations on pure text reasoning or non-math tasks (e.g., code generation, planning) are provided, limiting claims of general RLVR applicability.\n\n4. The baselines comparisons are weak, with comparisons provided only with GRPO, omitting broader baselines like PPO or recent works (ike STaR (Zelikman et al., 2022) or Quiet-STaR (Zelikman et al., 2024))."}, "questions": {"value": "1. How robust is HiPO to success scarcity? For instance, what happens on datasets with <1% base success rates—does lowering the near-miss threshold (e.g., to 0 successes with synthetic prefix generation) maintain gains, or could this introduce off-policy drift? I am currently not convinced that the proposed mechanism can effectively solve the sparse reward issue in GRPO, and better baseline comparisons should be added.\n\n2. The hint sampling uses discrete ratios [0.05-0.45]; have you ablated continuous sampling or adaptive lengths (e.g., based on intermediate reward proxies)? Could longer hints (>0.45) be viable with variance regularization to prevent collapse?\n\n3. Can you provide results on stronger baselines like PPO (for direct RL comparison), DPO (to assess preference optimization alternatives), or recent hint-augmented methods such as StepHint (ICML 2025)? Specifically, how does HiPO perform relative to these on the harder benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BZDgwd85Km", "forum": "rcb20pHmT1", "replyto": "rcb20pHmT1", "signatures": ["ICLR.cc/2026/Conference/Submission23076/Reviewer_DaaS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23076/Reviewer_DaaS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827508475, "cdate": 1761827508475, "tmdate": 1762942501779, "mdate": 1762942501779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HiPO is a novel RLVR framework designed for sparse reward reasoning tasks like mathematics. It overcomes issues in standard policy gradient methods through its \"Endogenous Self-Hint\" mechanism. This mechanism captures successful trajectory prefixes as on-policy hints, generating high-signal guided trajectories that transform sparse rewards into a dense, contrastive learning signal. HiPO significantly outperforms a GRPO baseline on five math benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The idea in this paper is simple yet efficient. \n3. This idea can maintain the exploration property."}, "weaknesses": {"value": "1. When facing extremely challenging tasks, the paper doesn't clarify how HiPO handles \"total failure\" batches where all groups have 0% success. Since hints are sourced from \"Near-miss Groups\" (0-50% success), an entirely \"Unlearnable\" batch would leave the hint pool empty, breaking the feedback loop and preventing hint-guided trajectory generation. This \"cold start\" scenario is unaddressed. Could you please clarify the precise mechanism for handling a mini-batch where no successful trajectories are generated (i.e., all groups are \"unlearnable\" with 0% success)? Does the hint-generation step simply fail for that batch, and the model must rely on standard GRPO updates until a success is stochastically found? Or is there a different mechanism to source hints (e.g., from a global buffer of past successes)? \n2. A lot of hyperparameters need to be ablated and discussed: (a). Length range [0.05, 0.45] for the hint. (b). When to activate the HiPO, when $0<H_{\\text{pool}} < \\frac{n}{2}$.  \n3. The baselines are limited; please introduce recently published algorithms for comparison, e.g., DAPO. \n4. The authors introduced a mechanism through a two-stage sampling process to encourage the diversity of hints.  How to measure this enhanced diversity from the proposed approaches? Any other methods? \n\nMinors:\n1. In Figure 1, the authors should clearly state what is the meaning of numbers, e.g., 0.2, 0.5, 0.7, 0.9, though I understand from the later part: the length of the hint prompt."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EWGcggO5Se", "forum": "rcb20pHmT1", "replyto": "rcb20pHmT1", "signatures": ["ICLR.cc/2026/Conference/Submission23076/Reviewer_Wgcs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23076/Reviewer_Wgcs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010892902, "cdate": 1762010892902, "tmdate": 1762942501556, "mdate": 1762942501556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies and targets two core failure / difficulties in RLVR on long-chain mathematical reasoning tasks, namely the near-miss problem and exploration stagnation. It proposes HiPO, a self-bootstrapping framework that extracts partial prefixes (“self-hints”) from the few successful trajectories within a batch and reuses them to regenerate a new, more informative batch on the same prompts. By replacing low- or zero-signal groups with hint-augmented ones, HiPO effectively densifies the reward signal and makes GRPO-style optimization work even when success is rare. Experiments on recent difficult math benchmarks (CMIMC 25, BRUMO 25, AIME 24/25) show consistent improvements over GRPO/DAPO-style baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This paper is techinically simple but effectively improves the sampling efficiency of RLVR in difficult scenarios where the reward is very sparse.\n\n- The proposed method is practically plug-and-play for GRPO-like group-based pipelines, and does not need expernal teachers' hints.\n\n- Empirical results on recent, difficult math benchmarks show clear gains."}, "weaknesses": {"value": "- The proposed HiPO \"amplifies\" the rare successful runs. However, on very hard tasks or early in training where only very rare successful runs can be sampled, HiPO may cause \"over-exploitation\" of a small set of successful runs. The authors are recommended to discuss this possibility in the paper.\n\n- Comparison is mainly against GRPO/DAPO-like baselines; it would be important to see how HiPO fares against stronger exploration- or entropy-aware RL variants, or against pipelines that inject external hints. This would clarify whether “self-hint” is a better source of guidance than existing teacher-style hints, or just a cheaper one.\n\n- Some key designs (e.g., the ratio of the original null-signal group samples that are replaced by the hint samples) lack ablations."}, "questions": {"value": "All experiments stay in math / verifiable QA style tasks, which are the friendliest setting for RLVR because of binary, automatic reward. As the authors claim the value of the proposed HiPO for general RLVR, how does the method work on tasks where rewards are delayed, noisy, or non-binary (code, tool-using agents, long dialogues)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vz1zX4G377", "forum": "rcb20pHmT1", "replyto": "rcb20pHmT1", "signatures": ["ICLR.cc/2026/Conference/Submission23076/Reviewer_eFHX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23076/Reviewer_eFHX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098806491, "cdate": 1762098806491, "tmdate": 1762942501169, "mdate": 1762942501169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}