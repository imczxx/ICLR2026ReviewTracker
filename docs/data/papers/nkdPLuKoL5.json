{"id": "nkdPLuKoL5", "number": 20977, "cdate": 1758312278615, "mdate": 1759896948920, "content": {"title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations", "abstract": "Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization -- a key step in the alignment phase -- can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model’s internal decision process and the generated explanation. Consequently, the LLM may engage in ``reward hacking'' by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM’s input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.", "tldr": "We show that preference optimization can lead LLMs to produce chain-of-thought explanations that do not reflect their true reasoning due to reward hacking. By incorporating causal attributions of inputs into the reward model, we reduce this behavior.", "keywords": ["Large language models", "reward hacking", "alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a459edab8cd4e7fcc78814d568b087fd99805e4f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the CoT hacking scenario in which models improve task scores while CoTs fail to acknowledge reliance on a protected cue. RMs exacerbate this under BoN and DPO. The proposed fix is to append a natural-language disclaimer to the RM input when causal attribution (via an original vs. edited prompt) indicates the protected feature influenced the prediction. On two designed hacking scenarios, the augmented RM mitigates unfaithful CoTs and reduces the accuracy/stereotype vs. acknowledgment mismatch under both BoN and DPO settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is timely and clearly motivated.\n\n2. Augmenting RM inputs with a causal signal is lightweight (no RM retraining) and can be easily integrated into common pipelines. The two proposed strategies are intuitive and easy to implement.\n\n3. The method effectively improves CoT quality/faithfulness, reflected in higher accuracy and acknowledgment rates."}, "weaknesses": {"value": "1. The presentation needs improvement. For instance, RM_D and RM_C appear in Figures 4 and 5 with no explicit definition at all. And their corresponding strategies are only clarified late (end of section 5).\n\n2. The two tasks across all experiments are controlled and stylized. It’s unclear how well the mitigation transfers to broader reasoning domains (code, multi-hop QA, tool use) where cues and causal edits are messier."}, "questions": {"value": "1. The acknowledgment labels come from a separate Eval-LLM. Does this introduce a new opportunity for reward hacking or bias?\n\n2. If you always append a neutral \"placebo\" disclaimer to all responses, do RMs behave similarly? This control would isolate the semantic effect of the disclaimer.\n\n3. Did you test adversaries that acknowledge in form but conceal in content (e.g., obfuscated disclaimers) to see whether RMs can be hacked by superficial cues?\n\n4. How robust is the disclaimer-trigger to decoding randomness and small prompt edits (e.g., cue paraphrases or formatting changes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cdi4SBUVLg", "forum": "nkdPLuKoL5", "replyto": "nkdPLuKoL5", "signatures": ["ICLR.cc/2026/Conference/Submission20977/Reviewer_ChWA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20977/Reviewer_ChWA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896437299, "cdate": 1761896437299, "tmdate": 1762999982871, "mdate": 1762999982871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a case study of unfaithful CoT responses from LLMs and how reward model-based preference optimisation of LLMs can encourage these undesirable responses. Two forms of CoT hacking are introduced, one in math QA and one in biased QA. CoT hacking happens when there is a clear cue leading to a correct answer to the question in the prompt, the LLM used it for generating its answer, but does not recognise that it has used it. Counterfactual data augmentation technique is employed to identify such cases. Empirical evidence shows that CoT hacking can happen, reward models can encourage this, and that simply adding a textual hint of unfaithfulness for the reward model can mitigate unfaithful responses. These are investigated on one LLM for answer generation, and two different RMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper targets the important and timely problem of LLM answer faithfulness. Unlike many prior works which simply acknowledge that this could happen, this paper identifies specific instances of LLM unfaithfulness, tries to find causes for it and proposes simple ways to fix it. Therefore, originality is excellent.\n- The paper comes with excellent clarity. The targeted problem (a specific type of unfaithful LLM behaviour) is well-motivated, and its scope is also explicitly discussed. The illustrations throughout are quite clear.\n- The experiment procedure is novel and makes sense. The resulting empirical findings in this paper are valuable to the research community."}, "weaknesses": {"value": "- While this is in the style of a case study, it is concerning that all experiments are done with generations from a single LLM. A larger scope of the experiments could make the story more convincing.\n- The setting seems quite simple. I wonder how this finding and methodology generalise to more open-ended QA forms."}, "questions": {"value": "See weaknesses.\n- This paper used two reward models which output one scalar score. How about those regression-based reward models which predict a score for a list of different aspects, such as relevance, verbosity, helpfulness, etc (for example, those trained on HelpSteer datasets). Would those reward models demonstrate similar behaviours?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XYeFgYIp8G", "forum": "nkdPLuKoL5", "replyto": "nkdPLuKoL5", "signatures": ["ICLR.cc/2026/Conference/Submission20977/Reviewer_64gQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20977/Reviewer_64gQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921468789, "cdate": 1761921468789, "tmdate": 1762999982878, "mdate": 1762999982878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on addressing the lack of consistency between an LLM’s generated CoT explanations and the LLM’s actual internal decision making process. \n* The authors first demonstrate that preference optimization leads to a reduction in explanation faithfulness because the reward models can inadvertently encourage unfaithful chain-of-thought (CoT) explanations through reward hacking. \n* The authors propose adding causal attribution signals to the reward model’s input to encourage explanation faithfulness. \n* Finally, they show that this method leads to a reduction in the generation of misleading explanations on two controlled tasks, MathBook and BiasQA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This work is well-motivated as CoT explanations are widely used to evaluate model outputs, and it is important to ensure the faithfulness of the explanations.\n* The mitigation technique is simple but effective, and has minimal computational overhead.\n* The authors conduct comprehensive experiments to show that their method improves upon other baselines."}, "weaknesses": {"value": "* While the results are convincing, the experiments are limited to two specific, controlled settings and one LLM. It’s unclear how this approach would generalize to other cases, especially in cases where the use of protected attributes is more subtle or challenging to detect.\n* There is not much discussion about the limitations of the approach or cases where the method may struggle to perform well."}, "questions": {"value": "Can you elaborate more on the pros and cons of strategy C versus D?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WDa98VfNqy", "forum": "nkdPLuKoL5", "replyto": "nkdPLuKoL5", "signatures": ["ICLR.cc/2026/Conference/Submission20977/Reviewer_zgsZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20977/Reviewer_zgsZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955149243, "cdate": 1761955149243, "tmdate": 1762999982704, "mdate": 1762999982704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of explanation misalignment: where the explanations of model behavior do not reflect its actual decision making process. The specific form of explanations that the paper focuses on is the chain-of-thought (CoT) explanation. The paper identifies preference optimization as a part of the problem. The idea is that during preference optimization, the model responses need not only be accurate, but also adhere to other relevant criteria like fairness. So the outputs that adhere to these additional criteria are rewarded during training. Consequently, the LLM’s explanations end up being not fully faithful since the outputs containing explanations aim to maximize the reward. Inspired by prior works, the paper carefully designs contractual tests where the LLM is provided the answers to a question in the prompt but is told to not rely on this solution. The model is then prompted to generate the answer and a CoT explanation. By including and excluding the answer (also called the protected features and answer cue in the paper), the paper then studies if the CoT mentions the answer present in the prompt. Since the reward model cannot check the faithfulness of the explanation (whether or not the LLM relied on the answer in the prompt), the LLM ends up having misaligned explanations. The paper then designs a causal attribution mechanism to detect and mitigate such unfaithful behavior and shows increase in performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is novel and important. As the paper mentions, the CoT misalignment problem has been already discussed in prior work. However, to the best of this reviewer’s knowledge, the effect of reward model on such behavior has not bee studied. Controlling the CoT unfaithfulness would definitely enhance reliability of LLMs.\n2. The proposed mechanism for testing the faithfulness of the CoT mechanism, though closely related to prior studies, is intuitive."}, "weaknesses": {"value": "1. The writing of the paper can be significantly improved. Right now, important details are missing which make it hard to appropriately judge its contribution. (a) Starting with a relatively minor issue, I would highly suggest moving Figure 2 into the intro and potentially merging it with Figure 1. It’s only after reading Figure 2 and seeing the score distribution that the reader becomes aware of the main problem, that is, the reward model rewarding no acknowledgement when the instruction to not use the solution is added. Figure 1 tries to achieve a similar goal but the right panel is very difficult to understand with the current supporting text. (b) The procedure in the paragraph starting line 329 is one of the most important parts of the paper. I would suggest adding a visual like a figure with prompts to explain the concept. For instance, the introduction of coutnerfactuals in line 336 is quite abrupt. (c) The metric of Majority@16 is introduced in line 206 without context and it is not clear what it does. (d) The details around the reward model in Section 3.2 are unclear. Why not take an off the shelf reward model? How many data points was the reward model in the paper trained with?\n2. The paper relies on judge LLMs to extract the answer and check acknowledgements. However, the accuracy of the judges in checking answer correctness is not evaluated. The accuracy of judges in checking acknowledgements is evaluated in Appendix C but is somewhat weak. Why compute the F1 score? Why not simply measure a binary correctness metric which checks if the judge model correctly indicated that the protected feature is being used? Even if F1 score is a good metric (which I am not sure about), it would be good to add examples that help the reader understand why 0.84 and 0.65 are good F1 values for this task.\n3. Line 255: I thought the experiments were done with greedy decoding. That is, the model solving the math questions was operating in a greedy decoding setting. If that is the case, why repeat with different seeds three times? If that is not the case, how can be say that the change in accuracy in Figure 3 is due to the model relying on the cue and not due to the randomness from decoding?\n4. The paper should add some experiments showing how well the proposed method generalizes across various domains. For instance, does training on two datasets generalize to a third one?"}, "questions": {"value": "Please see the questions in W1, W3 and W4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ts8v15Bsmy", "forum": "nkdPLuKoL5", "replyto": "nkdPLuKoL5", "signatures": ["ICLR.cc/2026/Conference/Submission20977/Reviewer_vuLZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20977/Reviewer_vuLZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014886919, "cdate": 1762014886919, "tmdate": 1762939191435, "mdate": 1762939191435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}