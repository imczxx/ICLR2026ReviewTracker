{"id": "i6SA9rGTAB", "number": 5754, "cdate": 1757932093823, "mdate": 1759897956441, "content": {"title": "Latent Reward-Guided Search for Faster Inference-Time Scaling in Video Diffusion", "abstract": "The recent success of inference-time scaling in large language models has inspired similar explorations in video diffusion. In particular, motivated by the existence of “golden noise” that enhances video quality, prior work has attempted to improve inference by optimising or searching for better initial noise. However, these approaches have notable limitations: they either rely on priors imposed at the beginning of noise sampling or on rewards evaluated only on the denoised and decoded videos. This leads to error accumulation, delayed and sparse reward signals, and prohibitive computational cost, which prevents the use of stronger search algorithms. Crucially, stronger search algorithms are precisely what could unlock substantial gains in controllability, sample efficiency and generation quality for video diffusion, provided their computational cost can be reduced. To fill in this gap, we enable efficient inference-time scaling for video diffusion through latent reward guidance, which provides intermediate, informative and efficient feedback along the denoising trajectory. We introduce a latent reward model that scores partially denoised latents at arbitrary timesteps with respect to visual quality, motion quality, and text alignment. Building on this model, we propose LatSearch, a novel inference-time search mechanism that performs Reward-Guided Resampling and Pruning (RGRP). In the resampling stage, candidates are sampled according to reward-normalised probabilities to reduce over-reliance on the reward model. In the pruning stage, applied at the final scheduled step, only the candidate with the highest cumulative reward is retained, improving both quality and efficiency. We evaluate LatSearch on the VBench-2.0 benchmark and demonstrate that it consistently improves video generation across multiple evaluation dimensions compared to the baseline Wan2.1 model. Compared with the state-of-the-art, our approach achieves comparable or better quality while reducing runtime by up to 79%.", "tldr": "", "keywords": ["Video Generation", "Diffusion Models", "Reward Model", "Inference-Time Scaling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/713ad51a5cd698ca9e8c8183594648e3a0a0df15.pdf", "supplementary_material": "/attachment/3f712f7257cd5c505556a0e1a944e434fa2139b4.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents LATSEARCH, an inference-time scaling framework for video diffusion models, aiming to address limitations of existing methods that either rely on initial noise priors or only evaluate final decoded videos. It introduces a latent reward model that assesses partially denoised latents at arbitrary timesteps across visual quality, motion quality, and text alignment, and integrates this model with a Reward-Guided Resampling and Pruning (RGRP) mechanism to enable efficient search during denoising. Experiments on the VBench-2.0 benchmark show that LATSEARCH consistently improves video generation quality across multiple dimensions compared to the baseline Wan2.1 model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a latent reward model capable of evaluating partially denoised latents at arbitrary timesteps to provide intermediate feedback on visual quality, motion quality, and text alignment, addressing the issue of delayed/sparse rewards in existing video diffusion inference methods.\n2. The paper proposes the Reward-Guided Resampling and Pruning (RGRP) mechanism: it probabilistically resamples candidates based on reward-normalized weights and prunes to retain the candidate with the highest cumulative reward at the final step, enabling efficient search without wasting computation on low-quality trajectories.\n3. Achieves a superior quality-efficiency balance: on VBench-2.0, it improves video generation across key dimensions and matches or exceeds state-of-the-art quality while reducing runtime by up to 79%."}, "weaknesses": {"value": "1. Lacks in-depth theoretical analysis of LATSEARCH’s convergence and optimality, with the work being primarily empirical, which limits the understanding of why the proposed mechanisms (e.g., latent reward guidance, RGRP) work and their generalizability to other video diffusion frameworks.\n2. The latent reward model’s similarity-grounded credit assignment (SGCA) relies on cosine similarity to map video-level rewards to intermediate latents, but the choice of cosine similarity is not justified, and no alternative similarity metrics (e.g., Euclidean distance) are tested, raising doubts about the robustness of reward allocation.\n3. Key parameters in LATSEARCH (e.g., number of candidates N=4/6, scoring timesteps 10/15/20, temperature τ for resampling) lack sufficient sensitivity analysis; the paper does not explain why these specific values are chosen over others, reducing the reproducibility and persuasiveness of the results.\n4. When comparing with state-of-the-art methods (e.g., EvoSearch), it does not verify if the parameters of these baseline methods are optimized for the Wan2.1-1.3B model (they are directly adopted from original papers), potentially leading to unfair comparisons that overstate LATSEARCH’s advantages.\n5. The inference time measurement does not explicitly clarify whether it includes the computational cost of the latent reward model (e.g., Qwen2-VL-3B-based scoring), which may underestimate the actual computational overhead and distort the perceived efficiency advantage."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lQviOOyuYN", "forum": "i6SA9rGTAB", "replyto": "i6SA9rGTAB", "signatures": ["ICLR.cc/2026/Conference/Submission5754/Reviewer_ubZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5754/Reviewer_ubZH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761364468085, "cdate": 1761364468085, "tmdate": 1762918242269, "mdate": 1762918242269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a latent reward model that predicts visual quality, motion quality, and text alignment directly from intermediate diffusion latents, using similarity-grounded credit assignment to convert final video-level rewards into per-latent supervision. A transformer model trained with regression + preference losses enables test-time branching and pruning guided purely in latent space. Experiments on VBench-2.0 show a better trade-off with recent test-time scaling baselines, at lower computational cost, and ablations confirm the necessity of latent-level supervision and balanced preference learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Proposing a latent-level reward model that enables test-time search without decoding videos substantially reduces overhead while preserving or improving final quality, which is particularly impactful for heavy modalities such as video diffusion. Moreover, the paper attempts to address a fundamental obstacle in test-time search for diffusion models (i.e., prohibitive decoding cost with VAEs), and this attempt is valuable.\n- The method is evaluated under state-of-the-art settings, using standardized benchmarks such as VBench-2.0 and strong modern text-to-video backbones, and the results are reported in a detailed manner."}, "weaknesses": {"value": "1. The paper does not show scaling of LatSearch with respect to the search budget. Table 1 suggests that under equal compute, LatSearch does not outperform EvoSearch. It remains unclear whether increasing the latent search budget would close this gap. Without such evidence, the claimed efficiency-quality trade-off of latent-level search remains only partially substantiated.\n\n2. No human preference study is provided. While VBench-2.0 benchmark metrics improve, it is unclear whether the proposed search strategy actually improves perceived human preference. Without such evidence, overfitting to automated metrics cannot be ruled out.\n\n3. Similar to weakness 2, the validity of the latent reward model is not empirically anchored to human judgments or other reliable evaluators. For example, correlating latent scores with human preferences after a normal denoising process (i.e., denoising $z_t$ over $t$ steps and then decoding) would strengthen the claim. Without such grounding, the reliability of latent supervision remains unestablished."}, "questions": {"value": "- See weakness for major questions.\n- The paper employs VideoReward as the reward model. To what extent is the proposed latent-level search compatible with existing guidance methods (e.g., Universal Guidance), and how much computational cost would they incur in practice?\n- The paper reports substantial wall-clock gains (e.g., 133s vs. 783s in Table 1), but the breakdown is unclear. Could you provide a per-component runtime profile under your primary setting: DiT forward per denoising step and VAE decode per video? In particular, how large is the VAE decoding cost relative to DiT compute, and how does this explain LatSearch vs. EvoSearch/VideoReward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y2N4E1wYw0", "forum": "i6SA9rGTAB", "replyto": "i6SA9rGTAB", "signatures": ["ICLR.cc/2026/Conference/Submission5754/Reviewer_Pxsh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5754/Reviewer_Pxsh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761498059410, "cdate": 1761498059410, "tmdate": 1762918241328, "mdate": 1762918241328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LatSearch, an efficient inference-time optimization method for video diffusion models. It introduces a latent reward model that evaluates partially denoised latents at any timestep, providing real-time feedback on visual quality, motion quality, and text alignment. Based on this, the authors design a Reward-Guided Resampling and Pruning (RGRP) algorithm that dynamically retains high-quality candidates and discards low-quality ones during generation, reducing unnecessary computation. Experiments on the VBench 2.0 benchmark show that LatSearch achieves comparable or better video quality while reducing inference time by up to 79%. Overall, this approach enables more efficient and controllable video generation through latent-space evaluation and optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work proposes a novel latent-space reward model that can directly evaluate partially denoised latents, offering fine-grained supervision during generation and turning sparse, delayed feedback into meaningful intermediate guidance.\n\n2. This work introduces Reward-Guided Resampling and Pruning, which uses probabilistic resampling and cumulative reward selection to retain promising trajectories and remove weak ones, effectively improving both quality and efficiency.\n\n3. This work presents strong empirical results on the VBench 2.0 benchmark, showing consistent gains across multiple metrics and achieving up to 79% faster inference compared with existing methods, demonstrating solid practical value."}, "weaknesses": {"value": "1. Lack of theoretical grounding on the convergence or optimality of the proposed resampling and pruning process, making it unclear whether the cumulative reward aggregation in latent space guarantees consistent improvement across diffusion steps.\n\n2. Lack of deeper analysis on the similarity-based credit assignment used to construct latent-level rewards, as the cosine similarity between intermediate and final latents may not reliably capture semantic contribution at different timesteps, potentially introducing biased supervision.\n\n3. Lack of discussion on the generalization of the latent reward model to other video diffusion backbones or reward verifiers, since all training and evaluation are conducted within the same model family (Wan2.1 and Qwen2-VL), leaving cross-model robustness untested.\n\n4. Lack of an ablation comparing different latent similarity metrics (e.g., learned similarity networks vs cosine similarity) to validate the effectiveness of the similarity-grounded credit assignment."}, "questions": {"value": "1. How sensitive is the performance of LatSearch to the specific design of the latent reward model? For example, would replacing the cosine-based similarity grounding with a learned temporal weighting or a contrastive objective further improve reward accuracy and stability?\n\n2. Could the authors clarify whether the reward model or the RGRP strategy can be extended to cross-modal tasks, such as audio–video generation or video editing? It would be interesting to understand how latent-space rewards generalize beyond text-to-video synthesis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zZwLzXHRMA", "forum": "i6SA9rGTAB", "replyto": "i6SA9rGTAB", "signatures": ["ICLR.cc/2026/Conference/Submission5754/Reviewer_Z2L1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5754/Reviewer_Z2L1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797942225, "cdate": 1761797942225, "tmdate": 1762918239879, "mdate": 1762918239879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LATSEARCH, a plug-in inference-time search framework for video diffusion that trains a latent reward model to score intermediate (partially denoised) representations on visual quality, motion quality, and text alignment, and then applies reward-guided resampling & pruning to guide trajectory search in diffusion models using this reward model. Experiments show that, compared to state-of-the-art video verification methods, it achieves higher final video quality while suppressing computational overhead, operates significantly faster, and delivers performance equal to or better than strong inference baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-organized and easy to follow.\n- The authors propose reducing execution time in video diffusion by estimating rewards in intermediate latent variables and performing exploration based on them, which is both practical and novel.\n- Demonstrates competitive or better quality than strong inference-time baselines while reducing runtime, showing real deployment potential."}, "weaknesses": {"value": "- In this work, when constructing the latent reward dataset, the authors compute the cosine similarity between the current latent variable and the final latent variable, and weight the reward by this similarity to adjust the proportion of reward assigned at intermediate timesteps. However, this choice is largely intuitive, and other credit-assignment strategies are conceivable. For example, one could set s to be the error between the current and final latents (e.g., squared error), or simply apply time-based weighting of the reward. The authors should explain why their choice is preferable to such alternatives and provide lightweight ablations comparing (i) time-based schedules (e.g., uniform, exponential), and (ii) error- or SNR-based weighting (e.g., L2 error or denoiser loss).\n- The proposed method clearly depends on the performance of the Latent Reward Model (LRM). While the paper shows improvements when using the LRM within the search procedure, the LRM itself is not evaluated in isolation. Without such an evaluation, it is difficult to assess the validity of the Latent Reward Data Construction, i.e., whether the problem is well-posed and learnable. In other words, (A) whether latent rewards can be accurately predicted and (B) whether, given accurate latent rewards, the search can effectively leverage them are two distinct questions that should be validated separately."}, "questions": {"value": "Regarding the weaknesses noted above, please respond and, where appropriate, include additional experiments to address them. Also, the term Similarity-Grounded Credit Assignment (SGCA) appears only once in the paper, so defining it as a term is not justified. Please either use the term consistently throughout the paper or remove it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2TrMiJ9jxr", "forum": "i6SA9rGTAB", "replyto": "i6SA9rGTAB", "signatures": ["ICLR.cc/2026/Conference/Submission5754/Reviewer_uF4N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5754/Reviewer_uF4N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762110993240, "cdate": 1762110993240, "tmdate": 1762918239474, "mdate": 1762918239474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}