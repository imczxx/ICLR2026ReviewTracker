{"id": "LzOz7NKZI2", "number": 9069, "cdate": 1758109383378, "mdate": 1758876959453, "content": {"title": "Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis", "abstract": "Large language model (LLM) training faces a critical bottleneck: the scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents. Existing methods rely on surface patterns, fundamentally failing to generate controllable, complex multi-hop reasoning questions that test genuine understanding—essential for advancing LLM training paradigms.\nWe present Semantic Bridge, the first universal framework for controllably generating sophisticated multi-hop reasoning questions from arbitrary sources. Our breakthrough innovation is semantic graph weaving—three complementary bridging mechanisms (entity bridging for role-varying shared entities, predicate chain bridging for temporal/causal/logical sequences, and causal bridging for explicit reasoning chains)—that systematically construct complex pathways across documents, with fine-grained control over complexity and types via AMR-driven analysis.\nOur multi-modal AMR pipeline achieves up to 9.5% better round-trip quality, enabling production-ready controllable QA generation. Extensive evaluation demonstrates performance across both general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It yields consistent 18.3%–25.4% gains over baselines across four languages (English, Chinese, French, German). \nQuestion pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage.\nSemantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources. We will release our core code and semantic bridge model.", "tldr": "", "keywords": ["Data Synthesis", "LLM", "Multi-Hop Question Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/dc079abda26590e46d95834029358c0ab2b627de.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}