{"id": "T6T0JUgGFQ", "number": 6225, "cdate": 1757959343198, "mdate": 1763698018761, "content": {"title": "CTRL&SHIFT: High-quality Geometry-Aware Object Manipulation in Visual Generation", "abstract": "Object-level manipulation—relocating or reorienting objects in images or videos while preserving scene realism—is central to film post-production, AR, and creative editing. Yet existing methods struggle to jointly achieve three core goals: background preservation, geometric consistency under viewpoint shifts, and user-controllable transformations. Geometry-based approaches offer precise control but require explicit 3D reconstruction and generalize poorly; diffusion-based methods generalize better but lack fine-grained geometric control. We present **Ctrl&Shift**, an end-to-end diffusion framework to achieve geometry-consistent object manipulation without explicit 3D representations. Our key insight is to decompose manipulation into two stages—object removal and reference-guided inpainting under explicit camera pose control—and encode both within a unified diffusion process. To enable precise, disentangled control, we design a multi-task, multi-stage training strategy that separates background, identity, and pose signals across tasks. To improve generalization, we introduce a scalable real-world dataset construction pipeline that generates paired image and video samples with estimated relative camera poses. Extensive experiments demonstrate that **Ctrl&Shift** achieves state-of-the-art results in fidelity, viewpoint consistency, and controllability. *To our knowledge, this is the first framework to unify fine-grained geometric control and real-world generalization for object manipulation—without relying on any explicit 3D modeling.*", "tldr": "", "keywords": ["Diffusion Model", "Image Editing", "Video Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4421f617c0b8cba5306fb8ac943caf2681149be1.pdf", "supplementary_material": "/attachment/074c59f5cd746fbd772659c69a433bb39245d5d0.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents CTRL&SHIFT, a 3D-aware image editing approach. Given an image, the target location (specified by binary mask), and a relative camera pose transformation, the method is able to translate and rotate the object to the desired pose. The core model architecture is similar to VACE, with Control Blocks (Context Blocks) injecting the control signals. To train the model, authors design an automatic data construction pipeline, which produces paired images and ground-truth transformation. The training is done in two stages: first learning object-centric novel view synthesis, then learning blending objects to real-world background scenes. Quantative results on two benchmarks show clear improvement of CTRL&SHIFT over previous baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- While image editing is a well-studied task, 3D-aware editing is underexplored. This work brings recent advances in general editing to the field, and proposes a large-scale dataset to further improve results.\n- The background preservation when editing foreground objects seems to be pretty good. The visual effects like shadows also look realistic.\n- The new GeoEditBench can be a useful benchmark to the community."}, "weaknesses": {"value": "1. The paper presentation needs a lot of improvement. The main paper spends too much space talking about details of the data format (Sec. 2.1, 2.2, 2.3), yet the pipeline figure is in the Appendix. I'd suggest introducing the model architecture first, and then define each input & output and their shapes;\n2. Similarly, the main paper only has one paragraph (in Sec.1) discussing related works. This makes it hard to position the paper in the literature. Please move the Related Work part from Appendix to the main paper.\n3. Methodology-wise, while authors claim that architectural design is a contribution of the paper, I don't see why this is novel compared to ObjectMover and VACE. ObjectMover first shows that one can leverage video priors for more consistent object editing. VACE proposes the Context Block control mechanism, which is similar to the Control Blcok in this work. The Reference Images, Source / Target Masks, are conceptually similar to the Context Tokens in VACE. The only difference seems to be the injection of camera pose, yet it has been extensively studied in novel view synthesis and camera-controlled video generation papers.\n4. I appreciate the effort to compare with 6 baselines. However, why not compare with methods that explicitly use 3D bounding boxes or 3D representations such as Diffusion Handles and Image Sculpting? An even simpler baseline can be simply conditioning the diffusion model on the input frames, the reference image, and the 3D object bounding box in the target frame (projected with MLP similar to how you handle camera poses)."}, "questions": {"value": "1. For camera conditioning, have you tried common methods in camera-controlled generation, such as Plucker embedding?\n2. I'm also a bit confused by the model architecture. In the dataset construction pipeline, it seems that each data sample contains only 2 images (source and target). However, I believe Wan's video VAE works for `1+4*n` frames. How do you get other frames as model input here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HOLYtwVRq9", "forum": "T6T0JUgGFQ", "replyto": "T6T0JUgGFQ", "signatures": ["ICLR.cc/2026/Conference/Submission6225/Reviewer_sMZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6225/Reviewer_sMZf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760596748642, "cdate": 1760596748642, "tmdate": 1762918557504, "mdate": 1762918557504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Ctrl&Shift, a novel diffusion framework for high-quality, geometry-aware object manipulation in images and videos. The core challenge it addresses is the fusion of precise controllability from geometry-based methods with the realism and generalization of diffusion-based approaches.\n\nThe paper's main contribution is to cleverly decompose the complex manipulation task into two sub-tasks: 1) object removal and 2) reference-guided inpainting under explicit camera pose control. The authors design an 8-dimensional relative camera pose descriptor, f, which is injected as a control signal into the diffusion model. This enables end-to-end geometric control during inference without requiring explicit 3D modeling.\n\nTo achieve this, the paper proposes a multi-task, multi-stage training strategy, supplemented by a sophisticated data construction pipeline to generate paired, geometry-supervised, real-world data. Experimental results on the authors' new GeoEditBench and the ObjectMover-A benchmark demonstrate state-of-the-art performance, outperforming existing methods in fidelity, identity preservation, and geometric control accuracy (e.g., pose MAPE and IoU)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptual Innovation: The paper's primary strength lies in its core idea. It represents a conceptual shift: instead of relying on expensive or unstable 3D reconstruction (like NeRF or Mesh) at inference time, it injects precise geometric control (a relative pose vector) as a condition into the 2D diffusion process. This is a very clever decoupling that elegantly combines the advantages of both domains.\n\nSystematic Framework Design: The Ctrl&Shift architecture is designed with systematic and sound principles. The multi-task training (main task, removal, inpainting) is clearly motivated and helps the model disentangle the functions of different control signals (identity, location, pose), which is strongly validated by the ablation study (Table 3). The multi-stage training strategy (Stage 1 for priors, Stage 2 for real-world backgrounds) is logical. It first learns the core geometric knowledge on controllable synthetic data before generalizing to complex real-world scenes, effectively balancing geometric understanding and realism.\n\n3.Strong Empirical Results and Evaluation:The method significantly outperforms existing SOTA approaches in both quantitative (Tables 1, 2) and qualitative (Fig. 4) comparisons.The evaluation metrics are comprehensive. They include not only traditional fidelity (PSNR) and identity preservation (DINO, CLIP) metrics but also creatively introduce geometric control accuracy metrics (Pose MAPE, Obj IoU), which are crucial for evaluating \"controllable\" generation tasks.The construction of the GeoEditBench benchmark is also a valuable contribution to the community, providing a standardized evaluation platform for this specific challenge."}, "weaknesses": {"value": "I agree with the authors that this is excellent and inspiring work. To make the paper more complete and rigorous, I strongly recommend the authors add a 'Limitations and Future Work' discussion to the final version (e.g., in the conclusion or appendix).\n\nI would like the authors to specifically address the following points:\n\nFrom Technical Controllability to Practical Usability:\nThe authors should discuss the challenge of mapping intuitive user interactions (e.g., 2D mouse drags, rotational gestures) to the proposed 8D relative pose descriptor f. A powerful technology is of limited value without an intuitive interface. \n\nBeyond Geometry: The Challenge of Physical Realism:\nThe authors should acknowledge that the current framework focuses primarily on geometric consistency, while the modeling of physical interactions (especially lighting, shadows, and reflections) is limited. Generating physically correct new shadows and reflections based on the new location's lighting conditions is a major challenge for this method (and all manipulation methods) and a valuable direction for future research.\n\nGeneralization Boundaries Induced by the Data Pipeline:\nThe authors should discuss potential failure cases arising from the dependency on the Image2Mesh pipeline. The method will likely struggle to generalize to: Non-rigid objects (e.g., cloth, hair, pets) Transparent or highly reflective objects (e.g., glass, metal)\nTopologically complex objects (e.g., trees, smoke) Scenes with complex occlusion (e.g., moving an object behind another object in the scene; the current 'remove + inpaint' framework may not correctly handle this new depth relationship).\n\nAnd I expect the open-source of the benchmark."}, "questions": {"value": "As weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3duE5oAbOF", "forum": "T6T0JUgGFQ", "replyto": "T6T0JUgGFQ", "signatures": ["ICLR.cc/2026/Conference/Submission6225/Reviewer_19Wi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6225/Reviewer_19Wi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740702283, "cdate": 1761740702283, "tmdate": 1762918557094, "mdate": 1762918557094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses geometry-aware object manipulation in images and videos—specifically, relocating and reorienting objects while maintaining photorealism, background preservation, and geometric consistency under viewpoint changes. The proposed solution, Ctrl&Shift, is a diffusion framework that decomposes the task into object removal and reference-guided inpainting under explicit camera control. This approach avoids the need for explicit 3D representations (like meshes or NeRFs) at inference time.\n\nTo enable training on real-world data with geometric supervision, the authors developed a pipeline involving 3D mesh reconstruction, pose estimation via differentiable rendering, and a learned harmonization model (Object Pasting Model) to synthesize target views. The authors introduce a new benchmark, GeoEditBench, and outperform recent methods in both visual fidelity and geometric accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Empirical Results and High-Quality Output. The quantitative results are compelling. On the new GeoEditBench (Table 2), Ctrl&Shift shows substantial improvements in geometric accuracy (17.70% Pose MAPE vs. 24.36% for the next best) while also achieving the highest fidelity scores (PSNR, DreamSim). The qualitative comparisons (Figure 4) are impressive, clearly demonstrating the model's superiority in handling complex rotations and perspective shifts where competitors fail.\n\n- Significant Enabling Contributions (Data Pipeline and Benchmark). The pipeline for creating paired supervision from real-world data (Figure 3) is a major contribution, addressing the bottleneck of acquiring high-quality, pose-annotated data. Additionally, the introduction of GeoEditBench provides a valuable tool for the community to evaluate geometry-aware editing."}, "weaknesses": {"value": "Several aspects require clarification or further investigation.\n\n1. Reliance on 3D Supervision during Training and Generalization Limits. The paper emphasizes avoiding explicit 3D representations at inference. However, the training data generation (Section 2.5) heavily relies on explicit 3D reconstruction (Hunyuan3D-2) and differentiable rendering. The model's generalization is therefore constrained by the capabilities of the underlying 3D reconstruction method. Furthermore, the rigorous filtering (IoU ≥ 0.90 for pose estimation, L336) likely biases the dataset towards objects that are easy to reconstruct and align. The paper needs a more thorough analysis of the data pipeline's success rate, the diversity of the resulting dataset, and how the filtering affects the model's ability to handle complex geometries, materials (e.g., transparent or reflective), or intricate occlusions.\n\n2. Simplistic Inference-Time Target Mask Estimation. During inference, the target mask $\\hat{M}^{tgt}$ is estimated using simple heuristics: squaring the source mask, scaling by distance ratio, and shifting (Section 2.1). This approach is very coarse and does not account for changes in the object's silhouette due to rotation or perspective distortion. For significant viewpoint changes, this estimate could be highly inaccurate. The paper lacks a sensitivity analysis on how the accuracy of $\\hat{M}^{tgt}$ affects the final output, and it is unclear how the model corrects for large inaccuracies in the estimated mask.\n\n3. Evaluation of Harmonization, Lighting, and Shadows. Photorealistic manipulation requires accurate handling of lighting and shadows. The framework relies on a separately trained Object Pasting Model (A.5) to generate harmonized training data. However, there is no explicit evaluation of this component, nor of the main model's ability to generate plausible lighting and, crucially, realistic cast shadows in the edited scene. The current metrics (PSNR, DINO) do not measure physical correctness. The reliance on this specialized harmonization model during data synthesis might bottleneck the main model's understanding of complex lighting interactions.\n\n4. Underdeveloped Video Manipulation Capabilities. The abstract and introduction prominently feature video manipulation. However, the quantitative evaluations are image-based. The qualitative video results (Appendix A.8) exhibit noticeable flickering and lower visual quality, which the authors acknowledge. The claims regarding high-quality geometry-aware video manipulation are not fully substantiated, and the limitations should be discussed more prominently in the main paper.\n\n5. Lack of Failure Mode Analysis. Understanding when the model fails (e.g., specific object types, extreme pose changes, complex lighting) would provide a more complete picture of the method's limitations."}, "questions": {"value": "1. Data Pipeline Statistics and Bias: What is the retention rate of the initial data corpus (e.g., Pexels videos) after the IoU ≥ 0.90 filtering step? Could you comment on whether this filtering introduces a bias towards simpler objects or viewpoints, and how the model performs on categories where 3D reconstruction typically struggles?\n\n2. Sensitivity to Target Mask: How sensitive is the model's performance to the accuracy of the estimated target mask $\\hat{M}^{tgt}$ during inference? Could you provide an oracle experiment showing the performance if the ground-truth target mask were available at inference?\n\n3. Harmonization and Shadows: How does the framework ensure realistic shadow casting and lighting for the manipulated object? Are there examples where the object is moved between significantly different lighting environments (e.g., from shadow to direct light)?\n\n4. Stage II Training Strategy: In Stage II, the main branch is frozen (L297). What is the motivation for this? Does this prevent the model from learning complex interactions (e.g., realistic shadow casting) that might require updates to the main diffusion pathway? Was fine-tuning the full model attempted?\n\n5. Large Viewpoint Changes: How does the model handle large viewpoint changes where significant parts of the object, unseen in the source image, must be synthesized? Is the model relying purely on learned priors for this object-inpainting task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ho44KUUeGD", "forum": "T6T0JUgGFQ", "replyto": "T6T0JUgGFQ", "signatures": ["ICLR.cc/2026/Conference/Submission6225/Reviewer_akfP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6225/Reviewer_akfP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765419970, "cdate": 1761765419970, "tmdate": 1762918556724, "mdate": 1762918556724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Ctrl&Shift, a novel end-to-end diffusion-based framework for geometry-aware object manipulation in images and videos. Unlike prior methods that either rely on explicit 3D reconstruction (e.g., NeRFs, 3DGS) or lack fine-grained geometric control (e.g., prompt- or mask-based diffusion editors), Ctrl&Shift achieves precise object relocation and rotation while preserving background integrity—without requiring explicit 3D modeling at inference time.\nThe core idea is to decompose object manipulation into two subtasks: (1) object removal, and (2) reference-guided inpainting under explicit camera pose control. These are unified within a single diffusion model trained via a multi-task, multi-stage strategy. The model conditions on: source frames, reference object image, source/target masks, and a relative camera pose descriptor (8D: axis-angle rotation, translation, and NDC screen shifts). This pose is encoded via Fourier features and injected through cross-attention.\nTo enhance generalization, the authors introduce a scalable pipeline for constructing real-world datasets with estimated relative camera poses. Extensive evaluations on fidelity, consistency, and controllability demonstrate superior performance over state-of-the-art methods in tasks like object relocation, rotation, removal, and inpainting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tInnovative decomposition of object manipulation into removal and pose-controlled inpainting within a unified diffusion model, enabling fine-grained geometric control (e.g., precise relocation and rotation) without relying on explicit 3D representations like NeRF or Gaussians, which improves scalability and avoids per-scene optimization.\n2.\tThe multi-task training approach effectively disentangles conditioning signals (background, object identity, camera pose), leading to interpretable and controllable edits that generalize to real-world content.\n3.\tThe experimental evaluation is comprehensive, including quantitative results on two benchmarks, a detailed user study, and extensive ablation studies that validate the contribution of each proposed component (multi-stage training, auxiliary tasks). The creation of the high-quality GeoEditBench dataset is itself a significant contribution."}, "weaknesses": {"value": "Heavy Reliance on Data Synthesis Pipeline: The method's performance is heavily dependent on the quality of the synthesized training data generated by the multi-step pipeline (mesh reconstruction → pose estimation → harmonization). Errors or artifacts introduced at any of these stages could propagate into the final model, potentially limiting its performance on objects or scenes that are challenging for these upstream components (e.g., transparent objects, complex textures, artistic images) and also for extreme viewpoint shifts or complex occlusions. The paper would benefit from a deeper analysis of failure cases stemming from the data pipeline."}, "questions": {"value": "1.\tHave you explored extending the framework to handle multiple objects simultaneously or interactive edits (e.g., user feedback loops during inference)?\n2.\tThe relative pose descriptor f is an 8-dimensional vector. How does the model generalize to large, out-of-distribution relative camera movements (e.g., moving from a front view to a top-down view) that may be sparsely represented in the training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SXGIlhyymN", "forum": "T6T0JUgGFQ", "replyto": "T6T0JUgGFQ", "signatures": ["ICLR.cc/2026/Conference/Submission6225/Reviewer_kSVy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6225/Reviewer_kSVy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055579078, "cdate": 1762055579078, "tmdate": 1762918556242, "mdate": 1762918556242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}