{"id": "loipspAYLx", "number": 13307, "cdate": 1758216305743, "mdate": 1759897446358, "content": {"title": "Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles", "abstract": "This paper introduces SciTrek, a novel question-answering benchmark designed to evaluate the long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often rely on non-scientific texts, focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by proposing complex questions that require information aggregation and synthesis across multiple full-text scientific articles.  Questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (titles, authors, and references). The SQL operations provide explicit, verifiable reasoning steps for fine-grained error analysis, and the construction process scales to contexts up to 1M tokens with minimal supervision. Extensive experiments on a diverse set of open-weight and proprietary LLMs demonstrate that SciTrek poses a significant challenge as the context length increases,  with supervised fine-tuning and reinforcement learning offering only limited gains. Our analysis reveals systematic shortcomings in models' abilities to perform basic numerical operations and accurately locate specific information in long contexts.", "tldr": "Benchmarking long-context language models on scientific articles", "keywords": ["Long-context Language Models", "Benchmark", "Dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b7e838c7bb14828effed583c8a9c9df443684f3.pdf", "supplementary_material": "/attachment/58e6d36d68af8bd844cd8286f1e49ad2a44ded56.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SciTrek, a benchmark that evaluates LLM performance on context intensive tasks. The benchmark is formed as follows: first, a database of scientific articles is collected. Then, SQL queries like \"How many references do articles with exactly 2 articles have\" are used to generate questions which are translated to natural language. At evaluation, the question and the database of articles is fed as input to the LLM. The evaluation shows that LLMs do not perform well on this task (especially as the input size grows), and that finetuning them only improves performance by a small amount."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clear and easy to follow\n- The evaluation is comprehensive. Not only are a plurality of LLMs evaluated, but some are also fine-tuned for the task\n- The analysis section is thorough, showing failure modes in an easy to understand manner"}, "weaknesses": {"value": "My main concern is that the benchmark seems to function somewhat like an \"adversarial stress test\" on model context. The setup involves loading many papers into the context and asking the model to retrieve specific information. In that sense, it feels closer to testing large-context retrieval, similar in spirit to giving the model pairs of 64k-token-long-numbers and asking the model to sum them up, rather than evaluating structured reasoning. While context understanding is a crucial tool for LLMs to have, it is ultimately a means to an end, rather than goal itself (i.e., solving problems at a human expert level or higher). I am not fully convinced that benchmarking it in isolation provides enough of a contribution as it stands.\n\nIn my view, a more convincing context usage benchmark would be one where long-context handling is genuinely necessary and where the problem cannot be solved by using alternative tools such as running code or SQL queries on the database."}, "questions": {"value": "1. Could the authors clarify to what extent the benchmark tasks require reasoning beyond long-context retrieval? Are there examples where correct retrieval alone is insufficient, and some structured reasoning necessary?\n2. How would the evaluation change if LLMs recieved the input but broken down into smaller chunks? For example, what if the papers weren't all given to the LLM at once, but in 32k token chunks, wherein the LLM could either manually query or automatically receive the next chunk when it chooses to.\n3. How would performance change if LLMs were allowed to query the database using SQL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nUOttzFNtj", "forum": "loipspAYLx", "replyto": "loipspAYLx", "signatures": ["ICLR.cc/2026/Conference/Submission13307/Reviewer_1uzy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13307/Reviewer_1uzy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761000124324, "cdate": 1761000124324, "tmdate": 1762923972798, "mdate": 1762923972798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper defines a benchmark for evaluating the long context performance of LLMs on questions that resemble (and are based on) SQL queries. To construct this benchmark, they gather articles from Semantic Scholar along with their metadata and construct SQL queries over this metadata by combining different operations and conditions. They then use an LLM to convert each query into natural text and provide the model with the full article texts along with this query. The performance of the model is quantified by its EM and F1 with the SQL query result as the ground-truth. In addition to providing a benchmark, the authors fine-tune a model (using SFT and GRPO) on instances from the distribution of their benchmark and illustrate that performance can be substantially improved with task-specific training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper contributes understanding of an important limitation of LLMs: their inability to properly process information from long contexts.\n- The benchmark provides thorough evaluations at different levels of context and a “Database Tables” evaluation which measures the difficulty of the task without long context (i.e., when the information is presented as succinctly as possible).\n- The authors offer additional insights by showing that this long context capability can be improved with task-specific training."}, "weaknesses": {"value": "- The design of the contexts for the benchmark feels a bit unnatural. \n    - The full article text is provided in the context, but the vast majority of this information is unused. The only parts that are used are metadata which is likely present in the article header. So, this feels a lot like a needle-in-the-haystack evaluation where the model needs to ignore most of the information present. It would be interesting to see whether the additional task complexity add signal over existing simpler needle-in-the-haystack evaluations.\n    - The number of instances that the model is expected to perform aggregation over feels high (in the case of 1M context length, there are 46.3 articles on average). Asking a model to, for example, compute the average of ~50 values is somewhat of an unnatural task (a modern LLM or a human performing this task would use a tool for such and operation). It’s unclear whether we should expect models to be good at this, and indeed, performance is low even in the “Database Tables” setting where the information is presented succinctly. So the failures seem to be not just from long context but with difficulty of an unnatural task.\n- It is unclear what insights can be derived from this evaluation beyond existing long context evaluation. Is model performance here well-correlated or uncorrelated with existing benchmarks?"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T7xgsVjCRU", "forum": "loipspAYLx", "replyto": "loipspAYLx", "signatures": ["ICLR.cc/2026/Conference/Submission13307/Reviewer_Bdtq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13307/Reviewer_Bdtq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503479126, "cdate": 1761503479126, "tmdate": 1762923972252, "mdate": 1762923972252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SciTrek, a benchmark designed to evaluate the long-context capabilities of large language models (LLMs) in scientific domains. SciTrek employs an automatic pipeline that converts article-metadata into a relational database and then generates natural language questions from SQL queries, offering explicit, fine-grained reasoning steps and clear ground-truth answers. Moreover, the benchmark is scalable, supporting context lengths up to 1M tokens and minimal human supervision in generation. While the framework aims to capture the importance of scientific reasoning tasks, its design primarily emphasizes verifiable outputs through SQL-based queries, which may limit its ability to fully reflect naturalistic or realistic scientific reasoning processes."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The use of SQL queries allows verifiable and objective evaluation of model outputs.\n- Inter-annotator agreement is measured, strengthening the reliability of annotations.\n- The benchmark demonstrates scalability and flexible context length control, offering potential adaptability for different experimental settings."}, "weaknesses": {"value": "- Although LCLMs are highly relevant for scientific tasks, the benchmark's question design does not reflect real-world scientific use cases. Transforming SQL queries into questions does not inherently yield realistic or natural contexts.\n- The input construction, based on concatenated papers, is unlikely to mirror authentic scientific workflows or user prompts, suggesting the dataset is closer to an SQL style long context benchmark rather than a naturalistic scientific one.\n- Relevance through citation links does not guarantee realistic contextual usage, as the dataset lacks evidence that article groupings represent actual scenarios.\n- The benchmark is limited to numeric operation tasks, which capture only a narrow slice of long-context reasoning capabilities. Consequently, it fails to assess diverse cognitive demands across scientific reasoning dimensions.\n- The tasks depth and domain diversity are insufficient to justify the claim of comprehensively assessing long-context capabilities in scientific literature, especially in light of recent work such as ETHIC [1] and Is it really long context if all you need is retrieval? [2].\n- Table 2 raises concerns regarding the classification of skills (e.g., why Loong [3] does not require multiple reasoning capabilities?), and the claim of \"natural\" benchmarking appears overstated given the benchmark's artificial construction.\n----\n*[1] ETHIC: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage*    \n*[2] Is it really long context if all you need is retrieval? towards genuinely difficult long context nlp*    \n*[3] Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA*"}, "questions": {"value": "- In table 7, the distinction between full-text articles and database tables is unclear. Why can the latter handle 512k or 1M contexts when the former cannot-does this reflect genuine context length handling or merely reduced tokenization cost?\n- Citation order inconsistencies should be standardized throughout (e.g., Dubey et al., 2024; Kamath et al., 2025; Yang et al., 2025; Llama Team, 2025 on page 3 is repeated in the exact reverse order on the same page as Llama Team, 2025; Yang et al., 2025; Kamath et al., 2025; Dubey et al., 2024).\n- Repeated mention of OpenScholar and LongBench v2 across sections could be condensed.\n- Minor typographical issues (e.g., Semantic Scholar's footnote 6 on page 5 is different with other footnote cases).\n- Figure 2 suffers from low readability. Improved visual clarity would enhance presentation quality.\n- The manuscript mentions evaluation of the model's reasoning process, yet no explicit metric or mechanism for such assessment is described. Without this, the benchmark becomes indistinguishable from prior long-context evaluation datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "abFtjCXOjp", "forum": "loipspAYLx", "replyto": "loipspAYLx", "signatures": ["ICLR.cc/2026/Conference/Submission13307/Reviewer_iVac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13307/Reviewer_iVac"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552165199, "cdate": 1761552165199, "tmdate": 1762923971841, "mdate": 1762923971841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce SciTrek, a new long-context question-answering (QA) benchmark constructed from scientific papers. The work aims to address a key limitation of existing long-context QA datasets, which often rely on artificially constructed or non-scientific texts, by grounding the benchmark in authentic scientific literature. Using Semantic Scholar metadata, the authors automate the entire benchmark construction pipeline—from paper preprocessing and database creation to question and answer generation. They further present baseline results across several long-context language models and report performance improvements achieved through supervised fine-tuning (SFT) and reinforcement learning (RL)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an automated pipeline for benchmark construction, which effectively reduces the manual cost and effort traditionally required for question generation. This automation offers a scalable and efficient alternative to labor-intensive dataset creation processes."}, "weaknesses": {"value": "Limited benefit from using scientific papers\nAlthough the benchmark is based on scientific literature, the actual advantage of this choice remains unclear. Compared to datasets such as LongBench-v2 or OpenScholar, most questions in SciTrek focus on factual or computational details that can be directly retrieved from the text, rather than deeper semantic or reasoning-based understanding of the scientific content. While the authors argue that previous long-context QA benchmarks rely on non-scientific texts, SciTrek itself does not appear to exploit domain-specific scientific knowledge or reasoning. The minimal performance variation across different scientific disciplines further supports this observation.\n\nInsufficient experimental depth and analysis\nThe experimental setup appears somewhat limited, as the evaluated open-source models seem outdated. In the reasoning analysis, only three SQL query types and four corresponding reasoning traces were examined, which is insufficient for drawing generalized conclusions about reasoning behavior. Moreover, the analysis focuses primarily on the GRPO-based model; extending this examination to include failure cases and other model types would provide a more comprehensive and insightful understanding of the underlying reasoning challenges."}, "questions": {"value": "Although the Qwen2.5 model achieves higher performance than GPT-4.1 after both SFT and RL training, the paper still concludes that it shows “poor performance.” Could the authors clarify the basis for this assessment? What specific criteria or aspects led to this judgment despite the overall performance improvement?\n\nThe models used in the experiments appear somewhat outdated. Have the authors conducted, or do they plan to conduct, baseline evaluations using more recent models such as Qwen3 (e.g., Qwen-70B-Instruct, which supports up to 256K context length) to provide a fairer comparison?\n\nIn the Relational Filtering skill evaluation, there is a noticeable gap between the “Agree” and “Align” metrics, suggesting that the quality of generated questions may be lower than for other skills. It seems plausible that question quality degrades as the query structure becomes more complex. Do the authors have any plans or strategies to improve data quality in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RNcOjmeCWy", "forum": "loipspAYLx", "replyto": "loipspAYLx", "signatures": ["ICLR.cc/2026/Conference/Submission13307/Reviewer_s42S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13307/Reviewer_s42S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765477848, "cdate": 1761765477848, "tmdate": 1762923971592, "mdate": 1762923971592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}