{"id": "QQLWwzhROL", "number": 23621, "cdate": 1758346428410, "mdate": 1759896804472, "content": {"title": "CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones", "abstract": "Many modern ViT backbones have adopted spatial architectural designs, such as window attention in Swin, decomposed relative positional embeddings in SAM, and RoPE in DINOv3. While token reduction has been a successful research direction for reducing computational costs of ViT, the vast majority of existing methods fail to preserve the structured spatial layouts these architectures fundamentally depend on, rendering them incompatible with such spatial architectures.\n\nIn this paper, we introduce a simple yet effective token merging method that maintains spatial layouts, enabling seamless compatibility with spatial architectures. We show how to reconcile two seemingly conflicting requirements: exploiting the uneven information distribution across the spatial layout while preserving spatial structure of merged tokens. Our approach employs (1) a 2D token reduction strategy that ensures structured 2D layouts in the resulting tokens, (2) a spatial-aware merging algorithm to selectively merge redundant tokens while preserving relative spatial relationships of the tokens, and (3) a novel max-magnitude-element token representation that preserves salient features.\n\nOur method demonstrates strong performance both off-the-shelf and with fine-tuning, achieving state-of-the-art results on spatial and non-spatial architectures across various vision tasks. Specifically, we achieve 1.25× speedup on SAM-H with only 0.7 mIOU drop evaluated on COCO off-the-shelf, and 1.15× speedup on DeiT-B without accuracy drop evaluated on ImageNet within just one epoch of fine-tuning.", "tldr": "A token merging method that preserves 2D spatial structure, making it compatible to ViT with spatial architectures such as SAM and DINOv3", "keywords": ["token merging", "token reduction", "vision transformer", "off-the-shelf"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f852ccfb782111392348711b9e5e420da4430b3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a token merging method designed to preserve the 2D spatial structure of Vision Transformer (ViT) tokens during pruning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of perserving spatial relationship sounds novel and interesting for token pruning. Existing works rarely discuss this point."}, "weaknesses": {"value": "However, while the core idea is strong, the current manuscript suffers from three major weaknesses: \n\nInsufficient motivation in the Introduction: it does not clearly articulate why preserving spatial structure matters.\nLack of methodological clarity: key steps (e.g., how 2D reduction enforces uniform row/column counts) are described vaguely.\nExperimental section reads like a report, not an argument—it presents numbers without synthesizing insights or addressing fundamental questions about downstream task compatibility."}, "questions": {"value": "1. Why Preserve Spatial Structure matters? Modern ViT architectures increasingly embed explicit 2D spatial priors into their design, e.g., RoPE position embeddings. The paper assumes readers already understand why spatial coherence matters—but this is not justified.\n2. Section 3 describes the approach at a high level but omits critical implementation details. After token merging, the feature map is smaller and sparser—how do segmentation/detection heads produce full-resolution outputs that align with input pixels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gXkVbzLblr", "forum": "QQLWwzhROL", "replyto": "QQLWwzhROL", "signatures": ["ICLR.cc/2026/Conference/Submission23621/Reviewer_EmTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23621/Reviewer_EmTZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381377587, "cdate": 1761381377587, "tmdate": 1762942736887, "mdate": 1762942736887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CubistMerge, which is a spatial-aware token merging method. In contrast to existing token reduction solutions that result in uneven token counts across rows and columns, CubistMerge introduces separate reductions for columns (horizontal reduction) and rows (vertical reduction). As a consequence, CubistMerge always produces even token reduction and preserves the original spatial relationships among remaining tokens."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* This paper explicitly studies the spatial relationship preservation during token merging, which is a novel topic in token reduction, and can serve for hierarchical ViTs that utilizes window-based attention.\n\n* The experiments are thorough. Notably, adopting DINOv3 and SAM as the backbones is very rare in token reduction methods."}, "weaknesses": {"value": "* The overall novelty is insufficient. The main contributation of this work is the spatial-aware token matches strategy, which is a trivial extension to existing token merging methods. And, the even token reduction per row and column has been proposed in LTM [1] and implicitly used in some other works.  \n\n[1] Wang, Yancheng, and Yingzhen Yang. \"Efficient visual transformer by learnable token merging.\" TPAMI, 2025."}, "questions": {"value": "Questions:\n\n* Can the authors provide visualization comparisons between ToMe's and CubistMerge's merged results?\n\n* How does CubistMerge merge relative positional embeddings?\n\n* What if merging tokens without preserving spatial relationship but merging their relative positional embeddings together?\n\nSuggestions:\n\n* In the Graph Construction subsection, GTP-ViT should be mentioned as it also generates graphs on the image tokens to preserve spatial information on token reduction, while RoPE should be removed since it does not closely align with this topic.\n\n* Performance on standard backbones, such as DeiTs and Swins is better to be provided (maybe in the appendix) for a fundamental comparison to existing approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vrMfE53mrR", "forum": "QQLWwzhROL", "replyto": "QQLWwzhROL", "signatures": ["ICLR.cc/2026/Conference/Submission23621/Reviewer_u9e4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23621/Reviewer_u9e4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806766569, "cdate": 1761806766569, "tmdate": 1762942736133, "mdate": 1762942736133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a token merging techniques that split the merge process by two separate vertical and horizontal merge steps. Selected tokens are then merged by max-pooling on each dimension. The selection process for edges follows that of ToMe's."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is straightforward and easy to read\n- The merge method maintains spatial coherence of the image tokens after merging\n- The experimental results are competitive against baseline methods (ToMe and Expedite)"}, "weaknesses": {"value": "- Limited novelty: the method follows closely the approach proposed by ToMe and Expedite, with the 2D merge being the only technical contribution\n- Lack of comprehensive evaluation or more recent baseline: the experiments compare CuMe with ToMe and Expedite in the main results, and both methods were proposed in 2022. The submission cites GTP-ViT but claims that it requires extensive retraining, while GTP-ViT is actually a train-free method that consistently outperforms ToMe w/o fine-tuning.\n- The technique advantage of having \"spatial coherence\" after merging needs further articulation. ViTs see an image in tokens, and the attention happens between tokens. If would be great for the authors to provide some insights about why the proposed method would help visual reasoning - is it because of positional embedding? I'm not able to get a good intuition of how this links to the ViT computations and translates to an advantage (from a ViT's PoV)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yBEfs4IQWP", "forum": "QQLWwzhROL", "replyto": "QQLWwzhROL", "signatures": ["ICLR.cc/2026/Conference/Submission23621/Reviewer_24b5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23621/Reviewer_24b5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810355288, "cdate": 1761810355288, "tmdate": 1762942735967, "mdate": 1762942735967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a set of ideas to reduce the number of tokens processed in transformer based vision networks. This task of reducing the number of tokens has been explored previously to reduce the computational requirements (both time and floating point operations) of such networks. Critically, the authors identified that current methods do not preserve spatial consistency after token reduction. The proposed method maintains spatial consistency and demonstrates superior performance as compared to previous method at the same level of token reduction. This is achieved by using a separable 2D reduction strategy, followed by merging only neighboring tokens, and finally a novel token merging scheme."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The presentation of the problem and the proposed method is intuitive, clear, and easy to follow. Ablation studies are provided for each of the claimed contributions. Furthermore, the authors tested their method extensively on numerous architectures, demonstrating that their proposed token reduction strategy can be applied to different architectures. Finally, the authors also tested their idea on non-spatial architectures for where the proposed components still make sense."}, "weaknesses": {"value": "No significant weaknesses. Claims and contributions are supported by ablation studies and a rather thorough architecture sweep was performed.  \nMinor comments: \n\n- There’s no need for paragraphs in the abstract, it can be made more concise too. Focus on the contributions and main results.   \n- Frames per second might be an interesting metric to show"}, "questions": {"value": "- What is the hardware used in assessing performance speedup?  \n- For deployment in real-world systems, especially robots or edge systems, where and how each step is performed might be of interest."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "asNEY27c3t", "forum": "QQLWwzhROL", "replyto": "QQLWwzhROL", "signatures": ["ICLR.cc/2026/Conference/Submission23621/Reviewer_u2DH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23621/Reviewer_u2DH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904987844, "cdate": 1761904987844, "tmdate": 1762942735784, "mdate": 1762942735784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}