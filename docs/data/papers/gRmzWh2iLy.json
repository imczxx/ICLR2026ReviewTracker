{"id": "gRmzWh2iLy", "number": 6607, "cdate": 1757990393938, "mdate": 1759897905503, "content": {"title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property", "abstract": "Intellectual Property (IP) is a highly specialized domain that integrates technical and legal knowledge, making it inherently complex and knowledge-intensive. Recent advancements in LLMs have demonstrated their potential to handle IP-related tasks, enabling more efficient analysis, understanding, and generation of IP-related content. However, existing datasets and benchmarks focus narrowly on patents or cover limited aspects of the IP field, lacking alignment with real-world scenarios. To bridge this gap, we introduce **IPBench**, the first comprehensive IP task taxonomy and a large-scale bilingual benchmark encompassing **8 IP mechanisms and 20 distinct tasks**, designed to evaluate LLMs in real-world IP scenarios. We benchmark **17 main LLMs**, ranging from general purpose to domain-specific, including chat-oriented and reasoning-focused models, under zero-shot, few-shot, and chain-of-thought settings. Our results show that even the top-performing model, DeepSeek-V3, achieves only 75.8% accuracy, indicating significant room for improvement. Notably, open-source IP and law-oriented models lag behind closed-source general-purpose models. To foster future research, we publicly release IPBench, and will expand it with additional tasks to better reflect real-world complexities and support model advancements in the IP domain. We provide the data and code in the supplementary materials.", "tldr": "", "keywords": ["Large Language Models", "Intellectual Property", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aeb1d653da4f45e1403b114cccfd66efb69e6c56.pdf", "supplementary_material": "/attachment/701b73e4223f96b67e817dd2ce437ff2abe269e7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the IPBench, which is a large-scale bilingual benchmark for evaluating Large Language Models (LLMs) on real-world Intellectual Property (IP) tasks. IPBench includes 8 IP mechanisms, 20 distinct tasks and corresponding evaluation metrics. Then, this paper evaluates 17 major LLMs on the proposed IPBench. Moreover, this paper claims the release of IPBench to support future research in this specialized field."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-articulated and clearly presents the construction process and methodological details of the benchmark. Moreover, this paper conducts comprehensive and extensive evaluation across a wide spectrum of models with multiple scales on the benchmark."}, "weaknesses": {"value": "I have some concerns regarding the necessity of some tasks, the potential bias of some tasks, and the efficacy of some evaluation metrics for some tasks. \n\n**Task 1**\nI raise several concerns regarding the newly-added tasks in Task 1.​​ \n- First, the dataset scales for most tasks are limited to 500-600 samples. Given the huge number of clauses in relevant regulations and patent categories, I am concerned that such a restricted sample size may introduce evaluation bias and fail to adequately assess model performance.\n- Considering the LLM hallucinations and the fact that RAG technology can now inexpensively retrieve precise legal texts, the utility and necessity of memory-related tasks for evaluating genuine model capabilities appear questionable, especially on such limited test sets. \n\n**Task 2, 3, & 4-3**\n- I question the validity of using multiple-choice questions to assess the model's reasoning abilities like this. In the provided task samples (e.g., P47-49), the logical reasoning required appears relatively simplistic and seems to rely primarily on the model's ability to memorize regulation clauses, raising concerns similar to those noted in Task 1 regarding evaluation bias. \n- For tasks such as 2-1, it is debatable whether a standardized multiple-choice format is appropriate, as real-world legal analysis often lacks clear-cut answers. \n\nTherefore, I think these tasks of the benchmark does not adequately evaluate the model's reasoning capabilities and achieve its goal.\n\n**Task 4-1 & 4-2**\n- I find the evaluation criteria for the two generation tasks to be insufficiently defined. Although GPT-based scoring is employed, the specific rubrics for each score level remain undisclosed. Consequently, while final scores are provided, they offer limited insight into the precise weaknesses leading to a particular rating. Moreover, the relative importance of each scoring dimension and their collective impact on the final assessment are not adequately detailed.\n\nIn summary, given the unavoidable hallucinations issue of LLMs and the fact that RAG can cheaply retrieve regulation clauses, I think a meaningful benchmark should prioritize evaluating the model's reasoning capabilities rather than its ability to memorize regulation clauses. However, the tasks in the IPBench rely more on the memory ability, leading to insensibility to evaluate model reason ability. Moreover, the unclear generation task criteria and potential bias further undermine the practical significance​​ of the benchmark. Therefore, I think this work requires further refinement in the current stage, especially for ensuring sufficient sensitivity in evaluating reasoning capabilities. \n\nP.S. The establishment of benchmarks in patent-related tasks is undoubtedly a valuable direction for exploration. However, given that their design significantly influences the trajectory of research progress and, at the current stage, determines which specific capabilities of LLMs are prioritized for evaluation, I still think that benchmark establish should be very cautious. I think the proposed benchmark can evolve into better state with further refinement. \n\nP.S.2 **Reasons of prioritization for reasoning tasks:** In real-world applications involving legal and regulatory contexts, **precise statutory provisions are necessary**. Due to the unpredictable and uncontrollable nature of hallucinations in LLMs, it is impractical to rely solely on a model’s memorization for legal reasoning. Therefore, direct reference to the original legal text remains irreplaceable. Since RAG technology enables low-cost access to relevant legal provisions, the demand for a model’s ability to memorize clauses is relatively low, whereas its capacity for accurate reasoning is considerably more critical. For these reasons, I contend that in benchmark construction, it is more sound to provide the corresponding legal texts together statute-related questions."}, "questions": {"value": "Please see weakness section for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FFBncm3bv8", "forum": "gRmzWh2iLy", "replyto": "gRmzWh2iLy", "signatures": ["ICLR.cc/2026/Conference/Submission6607/Reviewer_ZfoF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6607/Reviewer_ZfoF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779767971, "cdate": 1761779767971, "tmdate": 1762918929629, "mdate": 1762918929629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents IPBench, the first large-scale, bilingual (Chinese–English) benchmark designed to evaluate Large Language Models (LLMs) on real-world Intellectual Property (IP) tasks beyond patents, including trademarks, copyrights, and trade secrets. The benchmark is notable for its breadth (20 tasks across 8 IP mechanisms), depth (cognitive complexity from information recall to creative generation), and linguistic diversity (Chinese and English). The authors evaluate 17 models under multiple prompting conditions and find that general-purpose closed-source models significantly outperform specialized legal/IP models. They also introduce LLMScore, a structured LLM-as-judge metric for evaluating generative outputs, which is a thoughtful and practical contribution. Overall, this work establishes a strong foundation for future research on legal reasoning, domain-specific LLMs, and multimodal IP understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents a novel contribution to the evaluation of LLMs in the legal and IP domain. Its originality lies in two aspects: IPBench is the first benchmark to systematically evaluate LLMs across a broad range of real-world IP mechanisms, including not just patents, but also trademarks, copyrights, trade secrets. In addition, the authors introduce a cognitive-complexity-based taxonomy of tasks (Depth of Knowledge framework), which offers a structured and generalizable way to assess model capabilities.\n\n- The benchmark is also carefully designed, with 20 tasks covering diverse formats and jurisdictions, and evaluated across 17 strong LLM baselines. The \"LLMScore\" is a thoughtful methodological contribution that addresses known weaknesses in automatic metrics.\n\n- The paper also demonstrates strong clarity in its writing, task definitions, and experimental analysis. Figures and tables are well-organized, and the empirical findings and the limited impact of CoT prompting are clearly presented.\n\n- Finally, the paper fills a notable gap in LLM evaluation benchmarks by addressing a domain with high societal, legal, and economic relevance."}, "weaknesses": {"value": "- First, although 17 models are evaluated in the paper, including general-purpose, legal-specialized, and IP-specific variants, it offers little rationale for why these particular models were included or excluded. Some well-known legal-domain baselines such as Legal-BERT or CaseLaw-BERT are absent, without discussion. Moreover, the prompting strategy is applied uniformly across all tasks. While this ensures consistency within model types, it overlooks the high diversity of IPBench tasks, which range from factual recall to creative generation. For example, CoT prompting is used even on simple memory-based tasks, where it is shown to slightly degrade performance, and few-shot examples are randomly sampled without semantic relevance to the current task. Although the authors acknowledge that CoT may conflict with model preferences on certain tasks, they do not analyze CoT effectiveness across task types or cognitive complexity levels.\n\n- Second, the related work section primarily focuses on patent-specific datasets, but omits discussion of broader legal benchmarks like LexGLUE, LegalBench, or LawBench. Including such comparisons would help clarify how IPBench differs in complexity, task format, or real-world grounding.\n\n- Third, although LLMScore is a promising and thoughtfully designed metric, its empirical validation remains limited. The authors conduct a small-scale human evaluation on two generation tasks, comparing LLMScore with three human raters using correlation metrics (Kendall, Spearman, and Pearson). While this shows moderate alignment, the study is narrow in scope, covering only a subset of tasks and omitting key indicators like inter-rater agreement or detailed error analyses. A more extensive human validation across diverse task types would strengthen the credibility and generalizability of the metric."}, "questions": {"value": "- Could you clarify the rationale behind your model selection and exclusions? While the paper evaluates 17 models, it is unclear why certain widely used legal-domain baselines were omitted. Clarifying your selection criteria like model size, instruction tuning availability and performance cutoffs would help contextualize your results.\n\n- Have you considered analyzing CoT prompting effectiveness by task type or complexity level? You find that CoT prompting slightly degrades performance overall, but it is applied uniformly across all tasks. Since IPBench is structured around cognitive complexity , have you tested whether CoT helps on higher-level tasks, e.g., legal reasoning, generation, but hurts on lower-level ones, e.g., concept recall?\n\n- Can you elaborate on the validation of LLMScore beyond the two generative tasks? The correlation analysis between LLMScore and human judgments is helpful, but it covers only two tasks and three models. Do you plan to extend this to more tasks? Also, could you report inter-rater agreement among the human evaluators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pVJeYrjen3", "forum": "gRmzWh2iLy", "replyto": "gRmzWh2iLy", "signatures": ["ICLR.cc/2026/Conference/Submission6607/Reviewer_EVqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6607/Reviewer_EVqu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898813438, "cdate": 1761898813438, "tmdate": 1762918929193, "mdate": 1762918929193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces 20 Intellectual Property (IP)-related tasks designed to benchmark LLMs. The authors evaluate 17 LLMs and provide insights on:\n* the underperformance of IP-oriented models compared to general-purpose ones,\n* differences in performance between English and Chinese evaluations,\n* the effects of zero-shot, few-shot, and chain-of-thought (CoT) prompts,,\n* the use of LLM-as-a-judge for generative task evaluation, and\n* the poor performance of models on patent classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The task construction and evaluation follow community standards. While this makes the work sound, it also means it is not particularly original (aside from focusing on the IP domain).\n* The range of tasks appears reasonably broad (within IP), at least from my limited familiarity with IP benchmarks.\n* The presentation is clear and well-structured."}, "weaknesses": {"value": "* Significance: It’s difficult for me to assess the overall significance of this benchmark. The IP domain is relatively niche, but it’s also one where LLMs could have substantial impact.\n* None of the findings are particularly surprising, something the authors themselves acknowledge. (For example: domain-specific models tend to underperform generalist models; the relative strength of chat vs. reasoning models depends on the task; model performance on English/Chinese correlates with the model’s primary pre-training language; more few-shot examples generally help; and CoT can underperform depending on the task.) On the positive side, the paper confirms that intuitions from other domains transfer to IP-related tasks.\n* Appendix E does not detail how each specific task was constructed.\n* The paper provides little context for interpreting model accuracy. What accuracy would be considered acceptable for practical deployment of LLMs in these tasks? What is the human inter-annotator agreement for each?\n* The results for LLM-as-a-judge show relatively low correlation with human judgments, even though they outperform alternative approaches."}, "questions": {"value": "* A few tasks exceed 90% accuracy and several exceed 80%. Does this imply that LLMs are suitable for practical use in those tasks?\n* Please provide more details on the cosine similarity–based quality filtering process.\n* It is unclear how human judgment is evaluated in Table 6, was it binary (correct/incorrect) or based on a rubric score?\n* It would be interesting to include the rank correlation between tasks.\n* Please include the LLM-as-a-judge prompts in the Appendix.\n* Please include the cost of evaluating each model on IPBench, at least for those accessed via API (e.g., GPT-4o).\n* Each example seems to have been checked by multiple annotators. Releasing these individual annotations (not just the final consensus) could make for a valuable dataset for research on LLM-based annotation.\n* The use of Webb’s Depth of Knowledge (DOK) framework doesn’t add much value to me, though it’s not necessarily a weakness either.\n* The acronyms IPC/CPC are not introduced."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QPZUwpmfIr", "forum": "gRmzWh2iLy", "replyto": "gRmzWh2iLy", "signatures": ["ICLR.cc/2026/Conference/Submission6607/Reviewer_4pw2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6607/Reviewer_4pw2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911243561, "cdate": 1761911243561, "tmdate": 1762918928832, "mdate": 1762918928832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IPBench, a large-scale, bilingual, and comprehensive benchmark tailored for evaluating LLMs on real-world intellectual property tasks. The benchmark is structured around a hierarchical cognitive taxonomy inspired by educational theory, spanning 20 distinct tasks across 8 IP mechanisms and totaling over 10,000 data points. The authors assess 17 prominent LLMs under various evaluation paradigms, providing granular performance and error analyses. They also propose LLMScore, an LLM-as-a-judge metric for evaluating generative output."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The benchmark covers a broader IP tasks, including trademarks, trade secrets, and legal reasoning, than prior datasets, providing a comprehensive benchmark in the vertical domain. \n2. The authors provide fine-grained error analysis, giving insights into model behavior and failure modes, such as dominance of reasoning or hallucination errors.\n3. Introduction of LLMScore, an LLM-as-a-judge metric validated against human judgments and correlated via multiple statistical metrics."}, "weaknesses": {"value": "1. Limited Discussion of Data Coverage Bias: While the dataset construction is described as broad and rigorous, and the distribution plots in Figure 2 and Tables 10–12 show diversity, there remains a heavy numerical dominance of patent tasks (67% of datapoints, per Figure 2b), which may reinforce the very bias the paper claims to correct.\n2. The benchmark's data selection in the IP domain (US/China only) contradicts its goal as a general evaluation tool. IP law is not universal; it is highly nation-dependent. Therefore, an IP benchmark lacking broad jurisdictional diversity has limited generalizability and utility.\n3. Only one open, IP-specialized LLM is evaluated (MoZi), due to ecosystem limitations. This restricts the ability to judge the gaps and needs between domain-adapted and general foundation models (and also proves the previous weakness))"}, "questions": {"value": "Please check the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VaMIbU0LAX", "forum": "gRmzWh2iLy", "replyto": "gRmzWh2iLy", "signatures": ["ICLR.cc/2026/Conference/Submission6607/Reviewer_TeDk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6607/Reviewer_TeDk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062083114, "cdate": 1762062083114, "tmdate": 1762918928352, "mdate": 1762918928352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IPBench, a large-scale, bilingual, and comprehensive benchmark tailored for evaluating LLMs on real-world intellectual property tasks. The benchmark is structured around a hierarchical cognitive taxonomy inspired by educational theory, spanning 20 distinct tasks across 8 IP mechanisms and totaling over 10,000 data points. The authors assess 17 prominent LLMs under various evaluation paradigms, providing granular performance and error analyses. They also propose LLMScore, an LLM-as-a-judge metric for evaluating generative output."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The benchmark covers a broader IP tasks, including trademarks, trade secrets, and legal reasoning, than prior datasets, providing a comprehensive benchmark in the vertical domain. \n2. The authors provide fine-grained error analysis, giving insights into model behavior and failure modes, such as dominance of reasoning or hallucination errors.\n3. Introduction of LLMScore, an LLM-as-a-judge metric validated against human judgments and correlated via multiple statistical metrics."}, "weaknesses": {"value": "1. Limited Discussion of Data Coverage Bias: While the dataset construction is described as broad and rigorous, and the distribution plots in Figure 2 and Tables 10–12 show diversity, there remains a heavy numerical dominance of patent tasks (67% of datapoints, per Figure 2b), which may reinforce the very bias the paper claims to correct.\n2. The benchmark's data selection in the IP domain (US/China only) contradicts its goal as a general evaluation tool. IP law is not universal; it is highly nation-dependent. Therefore, an IP benchmark lacking broad jurisdictional diversity has limited generalizability and utility.\n3. Only one open, IP-specialized LLM is evaluated (MoZi), due to ecosystem limitations. This restricts the ability to judge the gaps and needs between domain-adapted and general foundation models (and also proves the previous weakness))"}, "questions": {"value": "Please check the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VaMIbU0LAX", "forum": "gRmzWh2iLy", "replyto": "gRmzWh2iLy", "signatures": ["ICLR.cc/2026/Conference/Submission6607/Reviewer_TeDk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6607/Reviewer_TeDk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062083114, "cdate": 1762062083114, "tmdate": 1763600651883, "mdate": 1763600651883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}