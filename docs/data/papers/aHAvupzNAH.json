{"id": "aHAvupzNAH", "number": 13557, "cdate": 1758219198184, "mdate": 1759897428721, "content": {"title": "Distributional Inverse Reinforcement Learning", "abstract": "We propose a distributional framework for offline Inverse Reinforcement Learning (IRL) that jointly models uncertainty over reward functions and full distributions of returns. Unlike conventional IRL approaches that recover a deterministic reward estimate or match only expected returns, our method captures richer structure in expert behavior, particularly in learning the reward distribution, by minimizing first-order stochastic dominance (FSD) violations and thus integrating distortion risk measures (DRMs) into policy learning, enabling the recovery of both reward distributions and distribution-aware policies. This formulation is well-suited for behavior analysis and risk-aware imitation learning. Empirical results on synthetic benchmarks, real-world neurobehavioral data, and MuJoCo control tasks demonstrate that our method recovers expressive reward representations and achieves state-of-the-art imitation performance.", "tldr": "We develop a distributional offline IRL framework that infers reward distributions and risk-sensitive policies via stochastic dominance and distortion risk measures, enabling state-of-the-art performance on synthetic, neural, and MuJoCo benchmarks.", "keywords": ["Inverse Reinforcement Learning", "Neuroscience", "Reinforcement Learning", "Robotics"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e840ca0c42f51cebb5f5c515468a3ae548d4fe38.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for distributional inverse reinforcement learning that jointly models the full distributions of rewards and returns instead of only their expectations. The method formulates reward recovery as minimizing violations of first-order stochastic dominance and integrates distortion risk measures to learn risk-sensitive policies under offline settings. It establishes a theoretically grounded and empirically validated approach that connects distributional reward modeling with risk-aware decision-making, extending IRL to more realistic and interpretable domains."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and the method is sound\nThe experiments are comprehensive and logically organized. The choice of baselines is appropriate, and the positioning of the proposed method is consistent."}, "weaknesses": {"value": "The preliminaries and framework sections are somewhat lengthy and formula-heavy. Basic material such as the Bellman equation could be moved to the appendix to improve readability and keep the main text focused on the novel aspects.\nThe experiments do not include comparisons with the latest diffusion-based or energy-based IRL methods (from 2024–2025), which makes the empirical analysis slightly outdated."}, "questions": {"value": "1. The method assumes that the reward follows a parameterized distribution. How valid is this assumption when applied to real neural behavioral data? If the true reward distribution is heavy-tailed or multimodal, would this lead to model bias or reduced interpretability?\n2. The paper claims that DistIRL can capture higher-order reward structures. Could the authors provide concrete examples or quantitative evidence, such as skewness or kurtosis, to illustrate what is meant by “higher-order structure”?\n3. The paper also states that the learned reward distribution is consistent with dopamine signals in neural data. How is this “consistency” quantitatively evaluated? Beyond Pearson correlation, are there other metrics that could better reflect nonlinear or causal relationships?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DdAFXTU7RQ", "forum": "aHAvupzNAH", "replyto": "aHAvupzNAH", "signatures": ["ICLR.cc/2026/Conference/Submission13557/Reviewer_T6Qy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13557/Reviewer_T6Qy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394497556, "cdate": 1761394497556, "tmdate": 1762924155836, "mdate": 1762924155836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a distributional framework to mitigate the stochastic challenge of offline IRL. Specifically, authors leverage variational inference framework to connect FSD based distributional optimization problem to the conventional IRL problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated and well-written with clear structure. The variational inference framework used to connect original IRL optimization problem with a distributional variant is interesting and elegant. The experiment section is comprehensive from low-dimensional grid world to neural science tasks and to MuJoCo locomotion tasks, validating algorithm's effectiveness. The work itself is quite complete."}, "weaknesses": {"value": "1, The introduction and related work context should involve works on maximum causal entropy IRL based methods(MCE IRL)[1], which also target to solve the stochastic nature within the IRL problems[2,3,4,5].\\\n2, In Section 3.2, GAIL originally considers a MCE IRL problem instead of a MaxEnt IRL formulation.\\\n3, Reviewer is curious about impact of parameterizing different prior distribution. Though it has been discussed from line 314 to line 322, it would be great to have more ablation to investigate this part.\n4, In addition, as an offline approach, in ablation studies it would be great to add experiments to investigate impact of trajectory quantity to algorithm performances.    \n\n[1]:Gleave, Adam, and Sam Toyer. \"A primer on maximum causal entropy inverse reinforcement learning.\" arXiv preprint arXiv:2203.11409 (2022).\\\n[2]:Viano, Luca, et al. \"Robust inverse reinforcement learning under transition dynamics mismatch.\" Advances in Neural Information Processing Systems 34 (2021): 25917-25931.\\\n[3]:Bloem, Michael, and Nicholas Bambos. \"Infinite time horizon maximum causal entropy inverse reinforcement learning.\" 53rd IEEE conference on decision and control. IEEE, 2014.\\\n[4]:Wu, Runzhe, et al. \"Diffusing states and matching scores: A new framework for imitation learning.\" arXiv preprint arXiv:2410.13855 (2024).\\\n[5]:Zhan, Simon Sinong, et al. \"Model-based reward shaping for adversarial inverse reinforcement learning in stochastic environments.\" arXiv preprint arXiv:2410.03847 (2024).\\"}, "questions": {"value": "1, The idea of stochastic reward function is interesting. However, is it equivalent to stochastic dynamic? What could be the corresponding formulation of the MDP?\\\n2, From the preliminaries Section 3, it seems like reward function is not stochastic? Only transition dynamic is stochastic here?\\\n3, What is the motivation in line 213-214 using EBM to model likelihood function with $L_{FSD}$?\\"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zMDDTnbEJT", "forum": "aHAvupzNAH", "replyto": "aHAvupzNAH", "signatures": ["ICLR.cc/2026/Conference/Submission13557/Reviewer_8V98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13557/Reviewer_8V98"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539426947, "cdate": 1761539426947, "tmdate": 1762924154792, "mdate": 1762924154792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new offline IRL approach that explicitly models stochastic reward functions and aligns full return distributions between the expert and the learned policy. Unlike conventional IRL methods that recover only deterministic reward estimates or match expected returns, the proposed approach leverages first-order stochastic dominance (FSD) as a distributional matching criterion and integrates distortion risk measures (DRMs) into policy learning to enable risk-aware imitation. The method alternatively learns a conditional reward distribution via a variational inference formulation with an energy-based likelihood derived from the FSD violation loss, and optimizes a policy that maximizes a chosen DRM over the return distribution, estimated using quantile regression."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes a meaningful conceptual leap from deterministic IRL to distributional IRL. Modeling a full reward distribution is a novel and valuable direction for explaining variability in expert demonstrations. The paper opens a promising avenue for studying uncertainty-aware imitation and stochastic preference modeling in decision-making.\n\n1. The explicit use of First-Order Stochastic Dominance (FSD) as the optimization objective is innovative. Also, the formulation connects FSD, variational inference, and distortion risk measures, which is well-motivated and theoretically appealing. \n\n1. The experiments span both synthetic and real-world (rodent behavior) domains, providing an interesting interdisciplinary case that goes beyond typical RL benchmarks."}, "weaknesses": {"value": "1. There seems to exist theoretical flows in the derivations (correct me if I was wrong). The proof in Appendix B.1 contains an error in the change of variables involving the quantile function, incorrectly transforming the inequality $F_{Z^{\\pi}}(z) \\ge v$ to $z \\le F_{Z^{\\pi}}^{-1}(v)$ and $v \\ge F_{Z^{E}}(z)$ to $F_{Z^{E}}^{-1}(v) \\le z$ (should be $z \\ge F_{Z^{\\pi}}^{-1}(v)$ and $z \\le F_{Z^{E}}^{-1}(v)$ instead), which flips the direction of the integral. This error may result in the policy objective (Eq. 9) being the negative of the theoretically correct form derived from the FSD distance, which can create a fundamental mathematical disconnect between the paper's stated FSD goal and its actual algorithmic implementation.\n\n1. The framework's architecture combines multiple components that can be inherently prone to training instability. The minimax structure requires delicate balancing between the policy and the reward learning components to prevent divergence. This would be exacerbated by the highly complex gradient estimation required for the term $E_{q_{\\phi}}[\\mathcal{L}_{FSD}(\\pi, r)]$.\n\n1. The paper suffers from significant readability issues due to its immediate and simultaneous introduction of advanced concepts from disparate fields, including VI, FSD, DRM, and QR. This high conceptual load makes the core logic, especially the transformation of the complex FSD goal into the tractable maximum DRM objective for the policy, difficult to follow."}, "questions": {"value": "1. Under what conditions is the recovered reward distribution identifiable? Could different distributions explain the same expert data?\n\n1. In settings with very few expert trajectories (e.g., <5 or 1), how does the method perform compared to the baselines? Does the distributional reward model exhibit overfitting?\n\n1. What if applying the learned rewards in online RL? \n\n1. Could the authors show the convergence property of the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QjlSLDC3cp", "forum": "aHAvupzNAH", "replyto": "aHAvupzNAH", "signatures": ["ICLR.cc/2026/Conference/Submission13557/Reviewer_jq3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13557/Reviewer_jq3Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841690912, "cdate": 1761841690912, "tmdate": 1762924154241, "mdate": 1762924154241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Distributional Inverse Reinforcement Learning (DistIRL), a novel framework for offline IRL that addresses a key limitation of traditional methods: the assumption of a deterministic reward function. Conventional approaches typically recover a single point-estimate for the reward and match only the *expected* return of the expert. In contrast, DistIRL learns the full *distribution* of the reward function.\n\nThe core idea is to replace the standard mean-matching objective of MaxEntIRL with a new objective that minimizes First-order Stochastic Dominance (FSD) violations between the expert's and the learned policy's *return* distributions. This forces the model to capture higher-order moments of the return, which in turn provides a learning signal to recover the full underlying reward distribution.\n\nTo make this practical, the authors formulate the reward learning problem as a variational inference task, minimizing an FSD-based loss (Eq. 7). The corresponding policy optimization problem (Eq. 9) is intractable, but the authors propose an elegant surrogate objective (Eq. 13) based on Distortion Risk Measures (DRMs). This substitution connects the goal of distributional reward learning to the mechanism of risk-aware policy learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Novel and Principled Formulation:** The paper moves IRL from simple mean-matching to a more powerful distribution-matching paradigm. Using First-order Stochastic Dominance (FSD) as the objective is a principled, non-trivial, and novel approach to this problem.\n2.  **Elegant Solution:** The paper provides an elegant and practical solution. It identifies the intractability of the naive policy objective and cleverly substitutes it with a tractable surrogate based on Distortion Risk Measures (DRMs). This provides a clean conceptual link: learning a reward *distribution* is dual to learning a *risk-aware* policy.\n3.  **Compelling Real-World Validation:** The neuroscience experiment is a significant strength. Successfully recovering the skewed distribution of dopamine signals from raw behavioral data is a highly non-trivial result and showcases the method's potential for scientific discovery, going far beyond typical benchmark results."}, "weaknesses": {"value": "1.  **Lack of Convergence Analysis:** The authors rightly acknowledge that \"a rigorous convergence analysis is beyond the scope of this paper\". While they gesture towards potential avenues for such analysis (e.g., contraction properties), the absence of any formal guarantees for the three-timescale alternating optimization algorithm (critic, policy, reward network) is a theoretical gap.\n2.  **Sensitivity to DRM Choice:** The practical algorithm requires pre-selecting a specific DRM (e.g., CVaR) for the policy update. Proposition 4.6 implies that to *truly* match the FSD objective, the policy would need to be optimal under *all* DRMs, which is intractable. This raises a question: how sensitive is the recovered *reward distribution* to a \"mismatch\" in the chosen DRM? For example, if the expert data was generated by a risk-seeking policy, but the DistIRL algorithm uses a risk-averse CVaR objective, does it still recover the correct reward distribution? This sensitivity is not fully explored.\n3.  **Minor Naming Inconsistency:** There is a confusing inconsistency in the method's name in the main results. Table 2 labels the method \"IPMD (ours)\", while Table 3 uses \"DistIRL. (Ours)\". This appears to be a typo but should be corrected for clarity."}, "questions": {"value": "1.  Could you please clarify the naming inconsistency between Table 2 (\"IPMD (ours)\") and Table 3 (\"DistIRL. (Ours)\")? Is \"IPMD\" a typo, or does it refer to a specific variant?\n2.  The paper's main theoretical leap is to use a specific Distortion Risk Measure (DRM) $\\xi$ as a tractable surrogate for the policy objective. How sensitive is the final recovered *reward distribution* $q_{\\phi}$ to this choice? Proposition 4.6 suggests that a perfect FSD match requires optimality over *all* DRMs. What happens if the chosen $\\xi$ (e.g., risk-averse CVaR) does not match the true risk profile of the expert?\n3.  The experiment's success hinges on using a skew-normal (S-DistIRL) parameterization for the reward. This suggests the choice of $q_{\\phi}$ is important. How should a practitioner select the right parametric family for the reward distribution? Could this assumption be relaxed, for example, by using a non-parametric model for the reward distribution itself (e.g., a quantile-based representation, similar to the critic)?\n4.  The paper notes the lack of a convergence analysis. Could the authors briefly elaborate on the primary technical challenges? Is it the min-max nature of the objective, the non-convexity of the neural network updates, or the stability of the three-component (critic, policy, reward) stochastic approximation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DnC4OXExGV", "forum": "aHAvupzNAH", "replyto": "aHAvupzNAH", "signatures": ["ICLR.cc/2026/Conference/Submission13557/Reviewer_nY9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13557/Reviewer_nY9E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919716948, "cdate": 1761919716948, "tmdate": 1762924153776, "mdate": 1762924153776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}