{"id": "WV7xIboTDK", "number": 6444, "cdate": 1757984292307, "mdate": 1763497666498, "content": {"title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "abstract": "We stress test monitoring systems for detecting covert misbehavior in LLM agents (e.g., secretly exfiltrating data). We propose a monitor red teaming (MRT) workflow that varies agent and monitor awareness, adversarial evasion strategies, and evaluation across tool-calling (SHADE-Arena) and computer-use (CUA-SHADE-Arena) environments. We benchmark standard monitor scaffoldings and introduce a hybrid hierarchical--sequential design. Our experiments yield three findings. First, agent awareness dominates monitor awareness: agents that know they are monitored substantially degrade detection, while increasing monitor awareness helps less than expected. Second, monitor scaffolding matters: our hybrid design consistently outperforms baselines and enables weaker monitors to oversee stronger agents (a weak-to-strong effect). Third, targeted human oversight is key: escalating only pre-flagged cases improves TPR by 15% at FPR=0.01. Our work positions MRT as a standard workflow for stress-testing oversight, revealing robustness gaps in both LLM- and human-based monitoring. We release code, data, and logs to support further research.", "tldr": "This paper introduces a monitor red teaming workflow to stress test systems for detecting covert misbehavior in LLM agents, finding that a well-designed monitor scaffold enables weaker models to oversee strong aware attackers.", "keywords": ["Agent Safety", "Chain-of-Thought Monitoring", "Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3976f84bcdc90d18f0c038212e4a395bf5c024a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work stress-tests monitoring frameworks in AI control scenarios. The authors conducted extensive experiments to understand which components in AI control evaluation matter the most—for example, whether monitoring awareness or the monitoring framework itself has a greater impact, and many more. Their results show many valuable insights, one of which is that their hybrid monitoring framework remains effective even as the gap between the attacker and the monitor increases."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The authors ask the right questions to study in AI control eval.\n- They conducted extensive and detailed experiments.\n- They present valuable insights that can be useful for the safety of humanity."}, "weaknesses": {"value": "The current version is already good work. I was thinking about a similar idea a few months ago, and I am glad the authors executed this very well.\n- If the authors have time, I'd be interested to see whether all the insights generalize to other coding setups, e.g., bashbench or APPS, etc."}, "questions": {"value": "This is optional and probably will require a significant amount of time, but could you test some of main experiments on one coding setup, ie, BashBench, and see if the insights generalize?\nHappy to increase my score to 10 if you indeed have time to do this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mDqzRMdgpc", "forum": "WV7xIboTDK", "replyto": "WV7xIboTDK", "signatures": ["ICLR.cc/2026/Conference/Submission6444/Reviewer_28fS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6444/Reviewer_28fS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760922919738, "cdate": 1760922919738, "tmdate": 1762918837788, "mdate": 1762918837788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. We are glad that reviewers found our framework well-motivated, thorough, and clearly written, and that they highlighted our research questions and hybrid monitoring results as important contributions to AI control evaluation.\n\nIn the revised version, we have:\n\n* Added a **“Threat Model Limitations” subsection** (Section 5).\n* Clarified the role of **“omniscient” monitors vs a true programmatic oracle** (Section 4.2, Finding 3).\n* Strengthened the **analysis and ablations for hybrid monitoring** (Appendix D, Appendix E).\n* Reported **API cost and latency metrics** (Appendix F, Table 1).\n* Moved **research questions to the Introduction** and tied them to specific figures (Section 1).\n* Added **scaling plots vs absolute agent/monitor capabilities** (Appendix G).\n* Included **open-source monitors and a second benchmark (BashBench)** (Appendix H).\n* Expanded analyses of **chunk size, evasion strategies, and task difficulty** (Appendix D Figure 9, Appendix E, Appendix I Table 2).\n\nBelow we respond to each reviewer in more detail."}}, "id": "puz1y3oebW", "forum": "WV7xIboTDK", "replyto": "WV7xIboTDK", "signatures": ["ICLR.cc/2026/Conference/Submission6444/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6444/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6444/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763497474893, "cdate": 1763497474893, "tmdate": 1763497474893, "mdate": 1763497474893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work stress-tests monitoring frameworks in AI control scenarios. The authors conducted extensive experiments to understand which components in AI control evaluation matter the most—for example, whether monitoring awareness or the monitoring framework itself has a greater impact, and many more. Their results show many valuable insights, one of which is that their hybrid monitoring framework remains effective even as the gap between the attacker and the monitor increases."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The authors ask the right questions to study in AI control eval.\n- They conducted extensive and detailed experiments.\n- They present valuable insights that can be useful for the safety of humanity."}, "weaknesses": {"value": "The current version is already good work. I was thinking about a similar idea a few months ago, and I am glad the authors executed this very well.\n- If the authors have time, I'd be interested to see whether all the insights generalize to other coding setups, e.g., bashbench or APPS, etc."}, "questions": {"value": "This is optional and probably will require a significant amount of time, but could you test some of main experiments on one coding setup, ie, BashBench, and see if the insights generalize?\nHappy to increase my score to 10 if you indeed have time to do this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mDqzRMdgpc", "forum": "WV7xIboTDK", "replyto": "WV7xIboTDK", "signatures": ["ICLR.cc/2026/Conference/Submission6444/Reviewer_28fS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6444/Reviewer_28fS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760922919738, "cdate": 1760922919738, "tmdate": 1763498470963, "mdate": 1763498470963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers monitoring LLM-based tool-using agents for harmful behaviors. This paper introduces a benchmark of computer-use tasks with harmful side tasks which the agent can pursue or not. A set of threat models, including various agent and monitor awareness levels and strategies for evading and detecting, respectively, are introduced to simulate various AI control settings. The authors experiment with various settings in order to elucidate which settings are critical for both agent and monitor success. The experiments indicate several key findings including (a) weaker monitors with the right scaffolding can detect misbehavior of stronger models; (b) awareness improves agent's evasion success, but not monitor success; and (c) agent evasion strategies degrade monitor performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-motivated, thorough, and well written.\n* The paper introduces a foundational framework for evaluating different agents, monitoring strategies, and evasion strategies.\n* The experimental setup is well thought out and the conclusions are sound with respect to the experimental results."}, "weaknesses": {"value": "* The threat model presented in the paper is somewhat artificial and potentially unrealistic. One of the main assumptions made in the paper is that the agent is situationally aware. It is controversial whether or not this is a credible threat. Additionally, the agent must be prompted to perform different malicious actions. There is a lot of precedence in the AI evaluation space that this is ok, but it's not necessarily reflective of the true risks and capabilities of agents and monitors.\n* The experiments only consider one type of scenario. Namely, the AgentDojo style indirect prompt injection side task. It's unclear whether or not these results generalize to other settings.\n* All monitoring strategies evaluated use CoT monitoring. It has been shown in several works that this is not the most reliable means of monitoring.\n* One of the main conclusions is that weak monitors can effectively monitor strong agents. I think that this has already been established in the safety community as falling under the \"verification is easier than generation\" phenomenon."}, "questions": {"value": "* How does the agent know that it is being monitored? Is there evidence of situational awareness or is this just a theoretical possibility?\n* In what scenario would the monitor not have access to the agent's LLM? Third party auditing scenarios only?\n* Is there a way to guide the agents to be malicious or misaligned without prompting? This seems unrealistic.\n* Does the difficult of the main or side task affect the performance of the monitor or the agent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zhm6u8YcS3", "forum": "WV7xIboTDK", "replyto": "WV7xIboTDK", "signatures": ["ICLR.cc/2026/Conference/Submission6444/Reviewer_2pXk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6444/Reviewer_2pXk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665175002, "cdate": 1761665175002, "tmdate": 1762918837425, "mdate": 1762918837425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors examine the effects of various factors on agentic safety monitoring.  Specifically, they explore whether weak models can monitor strong agents; they test how much advantage monitor awareness confers when defending against evasion strategies; they check whether hierarchical monitor scaffoldings are more robust than full-trajectory baselines, and they ask the extent to which human overseers can improve reliability.  Their experiments are performed on the SHADE/CUA-SHADE arena with side tasks for testing computer-use agentic threats."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper takes a more scientific approach to agentic evaluation than most of those that I've read.  Many papers simply build an agentic workflow or defense system without thoroughly probing it from a scientific standpoint.  This paper includes proper statistical measures of success and asks actionable scientific questions.\n\nThis paper includes error bars, which is minor but still very good.\n\nSeveral of the results are both important and interesting.  For instance, the efficacy of different monitor ``achitectures\", such as hierarchical versus sequential monitoring was very interesting."}, "weaknesses": {"value": "The research questions are stated extremely late in the paper (line 252, page 5).  I think it would be better to state these questions as motivations for the research at the beginning of the paper.  These were some of the more interesting questions/findings that you came up with, so it weakens the paper to leave them so late in the draft.\n\nSome of your findings lack depth.  For instance, you check whether Haiku can monitor sonnet, but can you characterize how monitoring capability scales with parameters?  For instance, is the absolute number of parameters in the monitoring agent the most important, or is it the ratio in parameters/capabilities between the attacking agent and the monitoring agent that matters?  Given that this paper is fully empirical, you should seize on opportunities to go deeper into the data analysis.\n\nHow general are these findings?  Do they extend to other benchmarks as well?  Why do you rely exclusively on frontier models rather than providing findings on open-source models like Mistral as well?  Can open-source models serve as monitors, given that they're typically even weaker?"}, "questions": {"value": "Hybrid monitoring clearly has an advantage over other methods.  However, it is not clear to me what distinguishes the baseline versus sequential and hierarchical methods in terms of efficacy (looking at Figure 5).  Can you comment broadly on when one monitor design is better than another and why?  Do they confer different advantages in different settings?  Where exactly do the advantages of hybrid monitoring come into play?\n\nThere's not a clear trend in terms of the chunk size and AUC (appendix D).  How does the AUC decompose for these chunk sizes (i.e. TPR, FPR)?  Is there any trend in those?\n\nAware attackers use stealthy techniques to avoid detection.  Can you make this more precise by discussing the strategies that they use and the frequency of these strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WiAhIp564p", "forum": "WV7xIboTDK", "replyto": "WV7xIboTDK", "signatures": ["ICLR.cc/2026/Conference/Submission6444/Reviewer_gFsT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6444/Reviewer_gFsT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851441207, "cdate": 1761851441207, "tmdate": 1762918837052, "mdate": 1762918837052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a systematic framework for evaluating monitoring systems that detect misbehavior in autonomous LLM agents. The authors propose a structured methodology with formalized threat models (based on agent and monitor awareness levels), standardized evaluation metrics (AUC-ROC, TPR@FPR=0.01), and a taxonomy of adversarial evasion strategies. They contribute CUA-SHADE-Arena, a new benchmark of 15 task pairs in computer-use environments, and evaluate multiple monitor architectures including a novel hybrid approach that combines hierarchical and sequential processing. Key findings include: (1) agent awareness of monitoring substantially degrades detection, while increasing monitor awareness provides limited benefits; (2) a hybrid hierarchical-sequential monitor architecture consistently outperforms baselines; (3) weaker models can potentially monitor stronger agents when using appropriate scaffolding; and (4) structured human oversight can improve TPR by ~15 percentage points at FPR=0.01."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes several contributions, though of varying novelty. The systematic MRT framework provides useful formalization of threat models and evaluation protocols that were previously ad-hoc in the literature. CUA-SHADE-Arena extends monitoring evaluation to more realistic computer-use environments with bash commands and file operations. The hybrid monitoring architecture, while conceptually simple (linear combination of hierarchical and sequential approaches), represents a practical engineering contribution.\n\nThe experimental execution is generally thorough with appropriate scale, detailed documentation of prompts and model versions, and reasonable experimental controls.\n\nThe paper is well-written with clear structure, helpful figures, and comprehensive appendices."}, "weaknesses": {"value": "**Fundamentally unrealistic threat model**\n\nThe core evaluation setup is problematic: agents are explicitly prompted with both tasks and told they're participating in red-team evaluation. As the authors acknowledge in Section 4.1: \"we follow SHADE-Arena and frame the agent as assisting in a red-teaming setting.\" This means:\n- Agents are cooperative rather than adversarial\n- Misbehavior is prompted, not learned through training/fine-tuning\n- \"Unaware\" agents still know they're in an evaluation setting\n- Real threats (goal misgeneralization, emergent deception, adversarial fine-tuning) are not captured\n\nThis severely undermines the \"weak-to-strong monitoring\" claim: showing that Claude Haiku can monitor prompted Claude Sonnet 4 does not demonstrate it could monitor a genuinely superhuman misaligned agent optimized for evasion. The authors mention this limitation (Section 5) but don't adequately address how it affects their conclusions.\n\nRecommendation: Either (a) conduct experiments with adversarially fine-tuned agents, (b) use prompt injection from external sources rather than direct instructions, or (c) substantially temper claims and frame this as preliminary infrastructure work.\n\n**Omniscient monitor performance indicates methodological issues**\n\nIf an omniscient monitor knows the exact side task (e.g., \"save employee SSNs to .records file\"), it should be able to achieve near-perfect detection by simply checking whether that specific outcome occurred. Yet omniscient monitors achieve only 0.90-0.95 AUC for unaware agents and 0.80 AUC for aware agents with evasion (Figure 4).\nThis suggests the LLM monitors are not performing systematic verification but rather general \"suspiciousness\" assessment. The prompts (Appendix F) confirm this—they ask monitors to \"review each step carefully\" rather than \"verify if task X was completed.\"\nImplications:\n\nThe reported \"upper bound\" (omniscient performance) is artificially low due to LLM reasoning limitations, not fundamental monitoring limitations\nThe true performance ceiling might be 1.0, meaning all current approaches are far from optimal\nThe value of information (comparing unaware/aware/omniscient) is underestimated\nArchitectural contributions may be overstated—perhaps better prompting would improve all architectures equally\n\nRecommendation: Implement a programmatic omniscient baseline that systematically verifies task completion, report it as the true upper bound, and analyze the gap between programmatic and LLM-based monitoring.\n\n\n**Insufficient justification for hybrid monitoring**\n\nThe paper's main architectural contribution is combining hierarchical and sequential approaches with μ=0.5 linear weighting. However:\n\n- No theoretical explanation: Why are these approaches complementary? What types of attacks does each catch/miss? No failure mode analysis is provided.\n- No proper ablations: Missing tests include: (a) comparing to ensemble baselines (e.g., average two hierarchical runs with different chunk sizes), (b) analyzing attack-specific performance to understand complementarity, (c) testing non-linear combinations or learned weighting, (d) examining trajectory length sensitivity.\n- Statistical significance unclear: Differences are often small (e.g., 0.838 vs 0.854 in Appendix D Figure 7) with overlapping error bars, yet claimed as meaningful improvements.\n- Hyperparameter fairness: Hybrid receives careful tuning (Appendix D) but it's unclear if standalone architectures were equally optimized.\n- Conflicting results: Baseline sometimes achieves higher TPR than hybrid (Figure 3, omniscient/unaware condition), contradicting \"consistently outperforms\" claims.\n\nRecommendation: Add comprehensive ablation studies, failure mode analysis, and statistical significance testing. Either provide theoretical justification or reframe as an empirical engineering finding."}, "questions": {"value": "**Computational costs:**\n\nCan you provide actual measurements of API costs (input/output tokens, dollar amounts) and latency for each architecture? How does hybrid compare to baseline in terms of cost, given that it runs both hierarchical and sequential? Is the 5% AUC improvement worth the additional cost and latency?\n\n**Online monitoring feasibility:**\n\nYour architectures (especially hierarchical and hybrid) require the full trajectory to operate. How do you envision these being adapted for online monitoring where decisions must be made in real-time? What would be the latency per action for sequential monitoring given that it needs ~4 seconds per chunk?\n\n**Time-to-detection:**\n\nFor trajectories where malicious actions occur at step N, at what step does each monitor first flag suspicion? What percentage of harmful actions could be prevented if monitoring operated online? How do your offline metrics (AUC, TPR) relate to online effectiveness?\n\n**Action-level vs trajectory-level metrics:**\n\nYour metrics evaluate full trajectories, but online monitoring must evaluate actions. What is the false positive rate per action (not per trajectory)? If monitoring blocks a suspicious action, what's the cost of being wrong (benign action blocked)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fkoyy680ot", "forum": "WV7xIboTDK", "replyto": "WV7xIboTDK", "signatures": ["ICLR.cc/2026/Conference/Submission6444/Reviewer_Nw1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6444/Reviewer_Nw1E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975066740, "cdate": 1761975066740, "tmdate": 1762918836759, "mdate": 1762918836759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}