{"id": "FaJ7WwIstO", "number": 7195, "cdate": 1758011215993, "mdate": 1763669678673, "content": {"title": "Draw-In-Mind: Rebalancing Designer-Painter Roles in Unified Multimodal Models Benefits Image Editing", "abstract": "In recent years, integrating multimodal understanding and generation into a single unified model has emerged as a promising paradigm. While this approach achieves strong results in text-to-image (T2I) generation, it still struggles with precise image editing. We attribute this limitation to an imbalanced division of responsibilities. The understanding module primarily functions as a translator that encodes user instructions into semantic conditions, while the generation module must simultaneously act as designer and painter, inferring the original layout, identifying the target editing region, and rendering the new content. This imbalance is counterintuitive because the understanding module is typically trained with several times more data on complex reasoning tasks than the generation module. To address this issue, we introduce *Draw-In-Mind* (DIM), a dataset comprising two complementary subsets: (**i**) DIM-T2I, containing 14M long-context image–text pairs to enhance complex instruction comprehension; and (**ii**) DIM-Edit, consisting of 233K chain-of-thought imaginations generated by GPT-4o, serving as explicit design blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale, DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1 and Step1X-Edit. These findings demonstrate that explicitly assigning the design responsibility to the understanding module provides significant benefits for image editing. Our dataset and models will be publicly available.", "tldr": "We facilitate image editing via rebalancing designer-painter roles in UMMs.", "keywords": ["Image Editing", "Image Generation", "Unified Multimodal Model", "Multimodal"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1fb3d021b2fbde6356ece92b5e4cf446fe519ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a key limitation in current image editing models, which it frames as a \"responsibility imbalance\" between understanding and generation modules. The authors propose a new paradigm where the understanding module acts as a \"designer,\" generating a detailed Chain-of-Thought (CoT) blueprint, allowing the generation module to focus solely on \"painting.\" To support this, they introduce the Draw-In-Mind (DIM) dataset, which contains high-quality CoT \"design blueprints.\" Their model, DIM-4.6B, connects a frozen MLLM to a trainable DiT and achieves competitive or state-of-the-art results on benchmarks like ImgEdit and GEdit-Bench, validating the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Significant Dataset Contribution**: The DIM dataset is a valuable contribution. The authors have developed a systematic pipeline to generate high-quality \"design blueprints,\" creating a useful resource for the community to explore more fine-grained control in image generation.\n\n**Effective Architecture and Strong Results**: The model's simple design effectively isolates and demonstrates the power of the core paradigm and data. The fact that a 4.6B parameter model performs competitively against models over twice its size provides strong validation for the approach.\n\n**Thorough Experimental Validation**: The experiments are well-structured. The main results are clearly presented, the generalizability study (Table 5) effectively demonstrates adaptability to different \"designer\" models, and the ablation studies (Tables 6 & 7) offer valuable insights into the sources of performance gain."}, "weaknesses": {"value": "A key detail in the data generation process is that the \"designer\" model (GPT-4o) is provided with the target image for reference when creating the CoT blueprints for the training set. This creates a potential distribution mismatch between training and inference:\n\n**During Training Data Generation**: The CoT blueprints are \"oracle-guided,\" created with perfect knowledge of the desired outcome. They are essentially 100% accurate descriptions of the transformation.\n\n**During Inference**: The \"designer\" model must generate the CoT blueprint without seeing the target image, relying solely on prediction. These blueprints are therefore susceptible to errors, omissions, or misinterpretations.\n\nThis discrepancy raises a question about how well the model, trained on \"perfect\" instructions, can handle the inevitably noisier and sometimes incorrect instructions it will receive in a real-world setting. While the strong results in Table 5 (showing generalizability across different designers) are encouraging and suggest a degree of robustness, a direct discussion of this gap would strengthen the paper."}, "questions": {"value": "The current framework relies on an explicit, two-stage pipeline at inference time: first, calling a \"Designer\" (which can be an external model like GPT-4o or a separate open-source model) to generate the CoT blueprint, and second, feeding this blueprint into the \"Painter\" module. While effective, this explicitly modular design increases pipeline complexity.\n\nIt is worth discussing whether this \"designing\" capability could be internalized into the main understanding module itself, thereby streamlining the inference process. For instance, could the MLLM component of DIM-4.6B be trained to autoregressively generate the CoT blueprint before generating the control signals for the DiT, effectively merging the two stages into a unified, single-model call (perhaps via a multi-turn dialogue format or purely end-to-end training with CoT data)?\n\nDid you explore standard unified training approaches where the model is taught to implicitly \"reason\" (generate the blueprint internally) before \"acting\" (generating the image)?\n\nWhat are the key trade-offs that led to favoring the current explicit two-stage design over a more integrated, end-to-end approach? Is it primarily for the flexibility of swapping \"designers,\" or did you find that integrated models inherently struggle to balance these two responsibilities even with high-quality CoT training data?\n\nIn summary, this is a good paper that presents an inspiring and effective solution to a key challenge in image editing. The methodology is sound and the experiments are thorough. The questions raised are not intended as criticisms but rather to encourage a deeper discussion on the important trade-offs inherent in this design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KxEK7KOzNh", "forum": "FaJ7WwIstO", "replyto": "FaJ7WwIstO", "signatures": ["ICLR.cc/2026/Conference/Submission7195/Reviewer_B3js"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7195/Reviewer_B3js"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825681205, "cdate": 1761825681205, "tmdate": 1762919351093, "mdate": 1762919351093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank the reviewers for their dedicated time and constructive feedback. We are encouraged that all reviewers acknowledge the significance of the DIM data contribution. And we are also glad to see that the just-released Nano Banana Pro adopts the plan-then-edit idea similar to our Draw-In-Mind paradigm to significantly enhance editing performance, which verifies the effectiveness of our paradigm from an industrial perspective to some extent.\n\nThe insightful suggestions from the four reviews have significantly helped to improve the quality of our paper. We have addressed all concerns point-by-point below and incorporated the corresponding revisions in blue fonts into the manuscript. **We have explicitly marked the location of the relevant revisions for each question and respectfully invite the reviewers to refer to the revised manuscript for the best reading experience.** Please refer to the reviewer-specific responses below for details :)"}}, "id": "oNuzZBB8Y3", "forum": "FaJ7WwIstO", "replyto": "FaJ7WwIstO", "signatures": ["ICLR.cc/2026/Conference/Submission7195/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7195/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7195/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763669958775, "cdate": 1763669958775, "tmdate": 1763669958775, "mdate": 1763669958775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key issue in current image editing models: an imbalanced division of labor where the generator bears both design and painting responsibilities. To address this, the authors propose the Draw-In-Mind (DIM) dataset, which includes detailed, CoT blueprints for image edits. They also introduce a simple model, DIM-4.6B-Edit, which leverages an external designer to generate these blueprints. This approach shifts the design burden from the generator to the understanding module. Extensive experiments show that their method achieves state-of-the-art performance on multiple image editing benchmarks, outperforming much larger models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "*   **Well-Motivated Idea:** The paper is built on a clear and insightful observation about the fundamental limitations of current image editing models. The concept of rebalancing the \"design\" and \"painting\" responsibilities between modules is novel, well-justified, and intuitively appealing.\n*   **High-Quality Dataset:** The introduction of the DIM dataset, particularly DIM-Edit with its detailed CoT annotations, is a significant contribution. The ablation studies effectively demonstrate the value of this data, and the dataset is likely to be a valuable asset to the research community.\n*   **Comprehensive Evaluation:** The paper provides thorough experimental comparisons, convincingly demonstrating the advantages of the proposed DIM-4.6B-Edit model over other strong open-source baselines across multiple benchmarks and editing tasks."}, "weaknesses": {"value": "*   **Potentially Unfair Parameter Comparison:** The parameter count comparisons in Figure 1 and the main tables do not include the parameters of the External Designer (e.g., GPT-4o), which is a critical component of the proposed pipeline during inference. A significant portion of the performance gain likely stems from the powerful text refinement and CoT generation capability of this external model. Therefore, comparing the 4.6B parameter core model against the end-to-end parameters of other models may present an incomplete picture of the total \"cost\" of the method."}, "questions": {"value": "- The paper shows many successful editing examples. Could the authors provide some analysis or examples of cases where their method fails? Understanding the limitations and failure modes (e.g., types of instructions or image complexities that are challenging) would help the readers to better understand the boundaries of this approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wu8tzZFnDz", "forum": "FaJ7WwIstO", "replyto": "FaJ7WwIstO", "signatures": ["ICLR.cc/2026/Conference/Submission7195/Reviewer_PX1Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7195/Reviewer_PX1Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994696081, "cdate": 1761994696081, "tmdate": 1762919350245, "mdate": 1762919350245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper avoids complex architecture, using only a “two-layer MLP + small DiT”, thereby making it clear that the performance gains stem from data and the division of responsibilities, which keeps the causal narrative coherent. However, systematic evaluation on understanding tasks and on broader generative tasks remains insufficient, and the reliance on LLM-as-Judge and on GPT-4o as the data/designer limits the external validity of the conclusions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Emphasizes the importance and contribution of data.\n2. Demonstrates the effectiveness of \"think before drawing\" (plan-then-generate)."}, "weaknesses": {"value": "1. Evaluation relies on LLM-as-Judge; adding quantifiable, automated metrics is recommended.\n2. Claims to be a unified understanding–generation model, but lacks evaluation on understanding tasks.\n3. Insufficient exploration of how the new workflow impacts image generation."}, "questions": {"value": "1. Is there no way for a unified model to unify both thinking/reasoning and generation? If not, what is the significance of unifying understanding and generation? (Just for discussion)\n2. If this paper splits generation and understanding into two components, how does it differ from prior encoder-side work in models that use an MLLM to unify generation tasks? Is the main difference simply the addition of CoT?\n3. Why claim a unified understanding–generation model without evaluating understanding tasks or other generation tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e5VK7yukdT", "forum": "FaJ7WwIstO", "replyto": "FaJ7WwIstO", "signatures": ["ICLR.cc/2026/Conference/Submission7195/Reviewer_RiTv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7195/Reviewer_RiTv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995149348, "cdate": 1761995149348, "tmdate": 1762919349760, "mdate": 1762919349760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Draw-In-Mind (DIM) dataset and corresponding framework, which rebalances the roles of designer and painter in text-guided image editing. The authors identify an important issue in current editing models, that is, an imbalanced division of responsibilities, where the generator is overloaded with reasoning and design tasks. To address this, they propose the DIM dataset, consisting of DIM-T2I (14M long-context image–text pairs with CoT blueprints) and DIM-Edit (233K image editing pairs with CoT blueprints). Based on DIM-Edit, the proposed DIM-4.6B-Edit model achieves SOTA performance on image editing benchmarks with a simple architecture.\n\nThe paper’s main strengths lie in its explicit introduction of CoT reasoning into image editing, its high-value datasets and clear annotation taxonomy. The main finding of this paper is that comprehension, rather than model size, is key to editing performance. The experiments are thorough and convincing.\n\nHowever, the narrative focus could be more consistent, as T2I experiments slightly distract from the editing theme. The relationship between DIM-T2I and DIM-4.6B-Edit is unclear, and there are fairness concerns about using GPT-4o as an external reasoning module."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper explicitly introduces CoT reasoning in image editing and is the first to leverage an external designer to produce textual\nblueprints in a chain-of-thought style, which enables precise and stable image editing.\n2. This paper constructs two high value datasets for T2I and image editing, where DIM-Edit comprises 233K carefully designed CoT imaginations derived from existing editing datasets. If released publicly, this dataset would be extremely valuable for community-based MLLM image editing research.  Moreover, the paper also provides explicit annotation dimensions for both T2I and image editing,\nwhich would be highly helpful references for future multimodal dataset construction.\n3. The paper demonstrates that, for image editing, model size is not the dominant performance factor. Instead, improving the model’s instruction comprehension and text–image alignment capability is far more important. This offers an insightful and meaningful perspective for the field.\n4. The proposed DIM-4.6B-Edit model adopts an extremely simple architecture, which is just a two-layer MLP connecting an MLLM and a diffusion decoder, but it achieves SOTA performance on image editing tasks. This indicates that the paper’s data annotation paradigm has great potential for more sophisticated architectures.\n5. To ensure strict independence between training and evaluation data, the paper takes great care to avoid any data leakage. The experiments and analyses are sufficient, convincing, and logically coherent."}, "weaknesses": {"value": "Seen in Questions"}, "questions": {"value": "1. The title \"DRAW-IN-MIND: REBALANCING DESIGNER–PAINTER ROLES IN UNIFIED MULTIMODAL MODELS BENEFITS IMAGE EDITING\" emphasizes image editing as the main goal. However, the main experiments also include extensive T2I results (Table 2), which slightly dilute the paper’s thematic focus. The paper appears to have initially targeted both T2I and image editing but was later revised toward editing, leaving traces of both directions throughout the text. While this does not affect the experimental validity, it affects narrative coherence.\nFuture revisions could streamline the storyline to emphasize editing as the main task, framing T2I results as auxiliary evidence that supports DIM-Edit rather than a co-primary objective.\n\n2. Line 479-480 states\n\n>  \"DIM-T2I, 14M web-crawled image–text pairs with carefully crafted long-context prompts that\nprovide a foundation for complex CoT comprehension in editing.\"\n\nHowever, it remains unclear whether DIM-4.6B-Edit is trained based on DIM-4.6B-T2I (i.e., fine-tuned from its checkpoint) or whether it is simply trained with the same architecture on different datasets. If the latter is true, mentioning T2I results in the main narrative feels somewhat off-topic. Clarifying this relationship would make the methodology more coherent, that is, whether DIM-4.6B-Edit is initialized from DIM-4.6B-T2I. \n\n3. Line 297-299 states\n\n> \"During inference, we employ an external designer, e.g., GPT-4o, to prepare a blueprint in the same format as DIM-Edit, except without access to the target image, ensuring alignment with real usage scenarios.\"\n\nAlthough Table 2 compares DIM-4.6B-T2I with unified base models such as Show-o and BAGEL, DIM adopts a modular design that includes an external reasoning interface (GPT-4o). Strictly speaking, this introduces an auxiliary capability that is unavailable to other baselines.\nA more rigorous comparison would involve providing the same GPT-4o CoT blueprints to all models, to confirm whether the observed improvements stem from DIM’s superior CoT comprehension or from GPT-4o’s reasoning power itself.\n\n4. In Figure 2 (third example on the right), the optimized prompt reads \n\n> \"Remove the lower sign and the sticker on the ‘No Parking’ sign.\"\n\nHowever, the sticker mentioned in the text is not clearly visible in the original image. This inconsistency should be corrected or clarified.\n\n5. The paper uses an LLM to automatically evaluate CoT quality with a fine-grained scoring scheme (Line 254). However, the paper would be stronger if a small manually verified subset were included to cross-validate the LLM’s automatic scoring results. Such a human validation experiment could be briefly mentioned in the main text, with full results moved to the appendix.\n\n6. In Figure 1 (lower part), the meanings of the blue and red colors on the left and right sides are not explained. A short explanation clarifying what each color represents would improve readability.\n\n7. Line119-122 states \n\n> \"To ensure broad concept coverage, we opt for harvesting real-world images and annotate them with our in-house models from 21 dimensions, yielding 14M long-context image–text pairs, namely DIM-T2I, that form a robust foundation for complex CoT-guided editing.\"\n\nIt is unclear whether these 21 dimensions were derived from empirical analysis of existing datasets, theoretical considerations, or prior taxonomies. Providing the rationale or references for these dimensions would enhance transparency and reproducibility.\n\n8. The editing dataset, UltraEdit-160K-CoT, is filtered using SSIM, DINOv2, and CLIP similarity metrics before CoT annotation (Line 209).\nHowever, later in the pipeline, the data are again classified into misaligned, partially aligned, and aligned categories. Is the filtering in the first stage necessary? Why? how does it differ from the later alignment classification?\n\n9. Line 862-863\n\n> \"We include only 8K images from the training set of MagicBrush to avoid potential information leakage during evaluation. \"\n\nLine 269\n\n> \"we exclude distillation datasets such as BLIP3-o-60K (Chen et al., 2025a) to avoid any risk of data leakage\"\n\nIt is not explicitly explained why such leakage might occur. A short clarification of the specific risk would make this precaution clearer and more accessible to less familiar readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YzEIOJtVRD", "forum": "FaJ7WwIstO", "replyto": "FaJ7WwIstO", "signatures": ["ICLR.cc/2026/Conference/Submission7195/Reviewer_ui71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7195/Reviewer_ui71"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201659779, "cdate": 1762201659779, "tmdate": 1762919349205, "mdate": 1762919349205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}