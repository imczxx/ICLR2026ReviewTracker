{"id": "PCXobVZf6V", "number": 5676, "cdate": 1757926771344, "mdate": 1763116472590, "content": {"title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition", "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, a novel VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.", "tldr": "We propose AdaptVision, a novel learnable framework that empowers a VLM to adaptively select minimum vision tokens required for each task.", "keywords": ["efficient vision language model", "RL for reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9c5c7f1a21c56944a861e3b65e05c325f6e87f90.pdf", "supplementary_material": "/attachment/db5ac4ccb1648a2c83752439b7c0dcd7c86575ba.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new RL method, termed DTPO, to teach MLLM to use the bounding box tool, reducing the number of image tokens required during Inference. To achieve the above goals, the authors added a reward item for correct use of the tool, including an evaluation of correctness and requiring the area given by the model to be as close as possible to the size of the target area. Experiments on a series of benchmarks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method proposed by the author effectively reduces the usage of tokens while maintaining the original performance of the model."}, "weaknesses": {"value": "1. Lack of necessary explanations for some design.\n\n2. The authors' design lacks the necessary motivation. \n\nDetailed in Questions."}, "questions": {"value": "1. The authors should explain under how models can obtain a higher reward through Eq.8.\n\n2. Lack of direct comparison with GRPO. In the introduction, the authors point out the shortcomings of using GRPO directly, but the experiments lack direct analysis and demonstration of the problems these shortcomings (in line 098-104) may cause. There is also no comparison with the results obtained with GRPO. \n\n3. The authors' reward design lacks clear motivation and comparison. Why is the reward given in this way? What are its advantages and disadvantages compared to the rewad proposed in the VisionThink [a] ?\n\n[a] Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia. VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xuAkO1jA8X", "forum": "PCXobVZf6V", "replyto": "PCXobVZf6V", "signatures": ["ICLR.cc/2026/Conference/Submission5676/Reviewer_7aQJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5676/Reviewer_7aQJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760609371626, "cdate": 1760609371626, "tmdate": 1762918188774, "mdate": 1762918188774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Tnjsr2FAYu", "forum": "PCXobVZf6V", "replyto": "PCXobVZf6V", "signatures": ["ICLR.cc/2026/Conference/Submission5676/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5676/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763116471133, "cdate": 1763116471133, "tmdate": 1763116471133, "mdate": 1763116471133, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AdaptVision is a novel, efficient Vision-Language Model (VLM) paradigm that uses an adaptive, coarse-to-fine visual acquisition approach to solve the computational burden of excessive visual tokens. Inspired by how humans actively use their vision , the model first analyzes a low-resolution image and then autonomously decides whether to answer immediately or to call a bounding box tool to crop and acquire minimal, high-resolution visual detail only when necessary for accuracy. To train this dual-goal policy (accuracy and efficiency), the authors introduce Decoupled Turn Policy Optimization (DTPO), a reinforcement learning algorithm that separates advantage computation for tokens that belong to the tool step vs. tokens that belong to the answer step, so long 2-turn sequences don’t get under-trained compared to 1-turn ones. Extensive experiments demonstrate that AdaptVision achieves superior performance across VQA benchmarks while consuming significantly fewer visual tokens, using only a 33% token ratio relative to the vanilla high-resolution model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Coarse-to-fine visual cropping approach is simple and intuitive.\n- Tool call based approach integrates easily with existing LLM ecosystem without breaking standard architectures \n- Decoupled RL (DTPO) is simple but effective idea, splitting the advantage into “tool part” vs. “answer part” is a tidy fix to balance multi-turn vs single-turn answers."}, "weaknesses": {"value": "- Extra latency due to potential use of multiple inference turns\n- Learned cropping using RL has previously been explored in aesthetic/summarization context (e.g. [a]), and these prior approaches should have been benchmarked as off-the-shelf baselines\n\n[a] Cropper: Vision-Language Model for Image Cropping through In-Context Learning. Seung Hyun Lee et. al. CVPR 2025.\n\n- Some recent works such as ZoomEye[b] have explored learnable zooming/cropping capabilities for MLLMs, these should be reported a baselines.\n\nZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration. Haozhan Shen et. al. EMNLP 2025."}, "questions": {"value": "- Ablation of reward design would be useful. How sensitive is performance to (i) the weight on token cost, (ii) the bonus for correct answers, (iii) the penalty for unnecessary tool calls?\n\n- Compute the real wall-clock cost. You claim fewer visual tokens overall, but you add a second forward pass on some examples. Can you report: (a) % of examples that triggered a crop, (b) average end-to-end latency vs. a fixed-token baseline, (c) variance over the dataset?\n\n- If trained on “object-centric” images, does it still learn good crops on cluttered UIs, documents, or charts? A cross-domain eval would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qSxDFuIFDH", "forum": "PCXobVZf6V", "replyto": "PCXobVZf6V", "signatures": ["ICLR.cc/2026/Conference/Submission5676/Reviewer_fFan"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5676/Reviewer_fFan"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962340774, "cdate": 1761962340774, "tmdate": 1762918188405, "mdate": 1762918188405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of computational overhead in VLMs, which stems from the large number of visual tokens required to process high-resolution images. The authors propose AdaptVision, a new VLM paradigm inspired by human active vision.\n\nInstead of passively processing a fixed number of tokens, AdaptVision operates on a \"coarse-to-fine\" principle:\n\nIt first receives a low-resolution (1/4 size) version of an image, consuming only 25% of the standard visual tokens.\n\nThe model then performs an initial reasoning step. Based on this, it makes an autonomous decision:\na.  Answer Directly: If the low-resolution information is sufficient, it generates the answer.\nb.  Acquire More Detail: If the information is insufficient (e.g., for a detailed OCR or VQA task), it invokes a \"bounding box tool\" to request a specific, high-resolution crop of a key region from the original image.\n\nThis cropped region provides the necessary detail, and the model then generates the final answer.\n\nThe key technical innovation is the training methodology. The authors frame this adaptive decision-making process as an RL problem. They identify critical flaws in standard policy optimization (like GRPO), namely ambiguous credit assignment (is the reward for a good tool call or a good answer?) and imbalanced optimization (direct-answer sequences overpowering tool-call sequences)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The true contribution of this paper is DTPO. While \"active perception\" or \"zoom-in\" mechanisms have been explored, the training of such a policy is non-trivial. The authors' analysis of GRPO's failings (ambiguous credit and imbalanced optimization) is insightful, and their solution (decoupling the objective and advantage) is elegant and well-justified.\n\n- The data in Table 1 is impressive. Achieving 97.9% of the vanilla model's performance while using only 33% of the visual tokens is a SOTA-level efficiency trade-off. It clearly outperforms static down-sampling (92.1% perf at 25% tokens) and dynamic methods like VisionThink (95.8% perf at 52% tokens), proving the value of the adaptive, fine-grained cropping approach."}, "weaknesses": {"value": "-  The model is trained on VQA datasets where the task is often \"find a specific detail.\" This \"coarse-to-fine-crop\" policy is perfectly suited for this. How would this policy fare on tasks requiring holistic scene understanding (e.g., \"Describe the overall mood of the image\") or complex, multi-object reasoning (\"Are the person on the left and the person on the right related?\") where a single crop is insufficient? The policy might be overfit to VQA-style problems.\n\n- The two key reward components, $\\mathcal{R}_{acc}$ (answer correctness) and $\\mathcal{R}_{crop}$ (crop correctness), are determined by an external, proprietary model (GPT-4O). This is a fatal flaw for reproducibility. The agent is not being trained to solve the VQA task; it is being trained to satisfy the preferences of a black-box judge model. Any biases, quirks, or inconsistencies in GPT-4O are directly inherited by the policy. The resulting AdaptVision model is not a generalizable artifact, but a model specifically overfit to the outputs of another model.\n\n- The paper's own related work section (Section 5) cites multiple recent works (e.g., VisionThink, DeepEyes, Mini-O3) that \"support operations like zoom and crop\" or \"decide whether to use a low-resolution or the original image.\" The paper's claim to novelty seems to rest on the model autonomously determining the minimum tokens, but this is precisely the goal of all dynamic/adaptive methods. The contribution of a tool-based \"zoom\" is an incremental implementation detail, not a new paradigm as the abstract and introduction repeatedly claim."}, "questions": {"value": "- The paper presents AdaptVision as a \"novel VLM paradigm.\" However, prior work like VisionThink, DeepEyes, and Mini-O3 already implements dynamic resolution switching or \"zoom/crop\" functionalities. Could you precisely articulate the conceptual novelty of your approach beyond what appears to be an incremental implementation of an existing \"active perception\" idea?\n\n- How does this approach fundamentally differ from classic two-stage computer vision pipelines (e.g., region proposal + recognition), which also operate on a \"coarse-to-fine\" principle? Is this not a learned version of a region-of-interest (ROI)-based mechanism?\n\n- The core reward signal for both answer correctness ($\\mathcal{R}_{acc}$) and crop quality ($\\mathcal{R}_{crop}$) relies on GPT-4O. This introduces a major dependency on an external, proprietary, and non-deterministic model. How can the community reproduce or build upon this work if the reward oracle is a black box?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g0oQXkbaF0", "forum": "PCXobVZf6V", "replyto": "PCXobVZf6V", "signatures": ["ICLR.cc/2026/Conference/Submission5676/Reviewer_NtA4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5676/Reviewer_NtA4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985002649, "cdate": 1761985002649, "tmdate": 1762918188156, "mdate": 1762918188156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the VLM by proposing a new agent upon a Decoupled Turn Policy Optimization method. The proposed method learns to adaptively select the region of interest so that to get rid of the uncorrelated visual tokens. A reward shaping is carefully devised to encourage the performance. The proposed method allows improved performance with less visual token consumed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed method combines the agentic visual tool use with the token compression, which is quite interesting and inspiring. \n\n* The proposed method allows improved performance with less visual token consumed, which could be effective and efficient under some circumstances. \n\n* The proposed method is clear and easy-to-follow."}, "weaknesses": {"value": "* It seems that there lacks failing cases analyze. At what situation the proposed method can fail?\n\n* It seems that there lacks sufficient model discussion, such as oh the configurations of $\\alpha$ and $\\theta$. \n\n* Despite that this work claims an adaptive visual acquisition method, yet there lacks the statistical analysis on the adaptivity but mainly reports the averaged values. \n\n* How to adaptively adapt the number of selected visual tokens and the performance for different samples? \n\n* Does the proposed method have the risk of the reward hacking? More empirical and quantitative analysis is expected."}, "questions": {"value": "* It seems that the proposed method is limited to perform 1-turn or 2-turn tool calling. How to make sure the region-of-interest can be always captured? \n\n* The proposed method heavily rely on the MLLM to provide the proper region of interest to facilitate the cropping. What might be the performance of the MLLM on choosing the proper regions of interest? Quantitative analysis is expected. \n\n* Accordingly to Fig.4, outcome reward and the tool reward are at the same data range, more interpretations on this is expected. It seems that there is no machoism that normalize both the rewards to the same level."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FHfkKRDO7w", "forum": "PCXobVZf6V", "replyto": "PCXobVZf6V", "signatures": ["ICLR.cc/2026/Conference/Submission5676/Reviewer_LYZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5676/Reviewer_LYZR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988268887, "cdate": 1761988268887, "tmdate": 1762918187835, "mdate": 1762918187835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}