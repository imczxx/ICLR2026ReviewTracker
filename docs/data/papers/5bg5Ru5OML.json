{"id": "5bg5Ru5OML", "number": 8400, "cdate": 1758081909284, "mdate": 1759897787554, "content": {"title": "The Expressive Limits of Diagonal SSMs for State-Tracking", "abstract": "State-Space Models (SSMs) have recently been shown to achieve strong empirical performance on a variety of long-range sequence modeling tasks while remaining efficient and highly-parallelizable. However, the theoretical understanding of their expressive power remains limited.\nIn this work, we study the expressivity of input-Dependent Complex-valued Diagonal (DCD) State-Space Models (SSMs) on sequential state-tracking tasks for abstract groups. It is easy to show that a single DCD SSM layer with a universal decoder can track any Abelian group at finite precision by decomposing it into a product of cyclic groups. We show that this is tight by proving that such a model cannot track any non-Abelian group at finite precision.\nWe further establish the expressivity of multi-layer DCD SSMs. We show that a $k$-layer DCD SSM tracks a group if and only if that group has a subnormal series of length at most $k$, with Abelian factor groups. Empirically, while multi-layer models are theoretically expressive enough for solvable non-Abelian groups, we find they often fail to learn such solutions in practice, highlighting a gap between expressivity and learnability.", "tldr": "This paper studies the expressivity of diagonal complex-valued state-space models. A single layer can track Abelian groups but not non-Abelian ones. Stacking layers allows tracking solvable automata but not non-solvable ones.", "keywords": ["state-space model", "SSM", "LRNN", "linear RNN", "expressivity", "complex", "dynamical system", "state-tracking", "semigroup", "group", "automata", "Krohn-Rhodes"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc58043465a2fa99562b2c32bc3d1f50c542363e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a theoretical characterization of what diagonal State-Space Models (SSMs) can and cannot learn when tracking sequential state information. The main results can be summarized as\n    1. A single diagonal SSM layer with complex values can track any Abelian group but cannot track any non-Abelian group.\n    2. A k-layer diagonal SSM can track a group if and only if that group has a \"subnormal series\" of Abelian factor groups with length ≤ k. This means stacking layers expands expressivity to solvable groups in a precise, depth-dependent way.\n    3. While multi-layer models are theoretically expressive enough for certain non-Abelian groups, they consistently fail to learn these solutions in practice with standard gradient descent."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The notations and logical flow is very clear make it easy to follow the results.\nThis work provides a complete characterization of diagonal SSM expressivity and exposes a critical gap between representational capacity and optimization. \nDiagonal SSMs form a strict subset of the computational hierarchy: sufficient for Abelian group structures but provably insufficient for non-Abelian groups at single-layer depth, with depth providing only limited help in practice."}, "weaknesses": {"value": "The results are pure theoretical, it would be good if there is any connections to real applications."}, "questions": {"value": "The theoretical contribution is significant. I'm interested in whether there are any real applications that can make use of this results. For example in material science, there may exists certain task that the symmetry property may exists?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a7ZOsZAXI6", "forum": "5bg5Ru5OML", "replyto": "5bg5Ru5OML", "signatures": ["ICLR.cc/2026/Conference/Submission8400/Reviewer_MdGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8400/Reviewer_MdGT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913384457, "cdate": 1761913384457, "tmdate": 1762920301705, "mdate": 1762920301705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the theoretical expressivity of diagonal State-Space Models (SSMs) on group state-tracking tasks. They show that a single-layer input-dependent complex-valued diagonal (DCD) SSM can track a group G at finite precision if and only if G is Abelian. A $k$-layer DCD SSM can track a group G if and only if G has a subnormal series of length at most $k$ with Abelian factor groups. The authors show a gap between this theory and their experiments. While the theory shows multi-layer diagonal SSMs can express solvable non-Abelian groups, experiments demonstrate they struggle to learn these solutions in practice. Even 2-layer models that can theoretically represent S3 fail to learn generalizable solutions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides necessary and sufficient conditions for when diagonal SSMs can track groups, not just sufficient conditions or impossibility results\n2. The connection to group theory seems an elegant way to relate architectural constraints to algebraic properties.\n3. The paper doesn't just prove existence - it shows explicit constructions demonstrating how multi-layer diagonal SSMs can track non-Abelian groups.\n4. The experimental section reveals an important gap between expressivity and learnability\n5. The paper carefully handles finite precision constraints, which are practically relevant and often glossed over in theoretical work.\n6. The results apply to popular SSM variants like Mamba"}, "weaknesses": {"value": "1. The experiments only test on 5 groups and don't explore what makes some solvable groups learnable vs others. More extensive experiments would strengthen claims about the learnability gap.\n2. While the paper identifies that multi-layer models fail to learn non-Abelian groups, it doesn't deeply investigate why or propose solutions beyond noting \"optimization difficulties\"\n3. State-tracking is a specific synthetic task family. The paper doesn't clearly connect these limitations to practical sequence modeling tasks\n4. The paper doesn't compare with non-diagonal SSMs empirically, which would help quantify the cost of the diagonal constraint in practice.\n5. While mentioning block-diagonal structures could help, the paper doesn't explore intermediate architectures between fully diagonal and fully dense.\n6. The paper could better position results relative to circuit complexity findings ($TC^0$) and explain what new understanding this group-theoretic view provides--that was not very clear to me."}, "questions": {"value": "1. Which real-world sequence modeling tasks actually require tracking non-Abelian groups? I'm not familiar enough with the matter to know how common these requirements are in NLP and other related domains\n2. Can you identify any specific optimization challenges when learning non-abelian groups? (e.g. loss landscape, initiatlization?) Given the explicit construction for S3, could we initialize models closer to theoretical solutions to improve learnability?\n3. What's the minimal architectural change needed to make non-Abelian groups learnable? Would 2×2 block-diagonal suffice for S3?\n4. How do these limitations apply to transformers, if at all?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rngLtKBJRS", "forum": "5bg5Ru5OML", "replyto": "5bg5Ru5OML", "signatures": ["ICLR.cc/2026/Conference/Submission8400/Reviewer_644N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8400/Reviewer_644N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939449389, "cdate": 1761939449389, "tmdate": 1762920301208, "mdate": 1762920301208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the expressivity of input dependent, complex valued diagonal state space models. Is shows that under some mild decoder conditions, the single layer can track any abelian group but not any non-abelian group. The paper performs empirical experiments that show that while multi-layer DCD SSMs are theoretically expressive enough for non-abelian groups, they fail to learn it in practice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper improves the understanding of the expressivity of State Space Models (SSM), by focusing on a specific type of SSM and a particular data type (abelian vs non-abelian groups). This seems to be a fresh perspective on analyzing expressivity and the theory appears to be rigorous."}, "weaknesses": {"value": "One concern is on the significance of the result. Does this make the SSM architecture more expressive (or less) than a transformer? Does this have any practical implications on how we should train SSMs? \n\nAnother point that makes it harder to understand the significance of the theory is that the experimental results do not directly support the theory but instead suggest that the finding that even if the models theoretically can learn certain tasks, the optimization fails to do it. This is not in itself bad, but the paper would have been much stronger if there were empirical results that supported the theory. \n\nFor the task C_60, for instance, it's not clear how the training task is generated. Is the input 1,2,3,4,..., or is it a random draw of numbers between 1 and 59, or something else. \n\nMinor: \nSomewhat unusual formatting with the theorem boxes.\nI suspect that many people who are experts on SSMs may not be deeply familiar with group theory. Hence giving concrete example to show what for instance, C_60, is may allow a broader audience enjoy the paper."}, "questions": {"value": "Is there some relevant real world task that correspond to non-abelian group?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "twFcin9i59", "forum": "5bg5Ru5OML", "replyto": "5bg5Ru5OML", "signatures": ["ICLR.cc/2026/Conference/Submission8400/Reviewer_1jQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8400/Reviewer_1jQf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950860130, "cdate": 1761950860130, "tmdate": 1762920300876, "mdate": 1762920300876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper demonstrates the limitations of input-dependent complex diagonal SSMs in terms of expressiveness for groups state tracking. The authors show that a single layer can track Abelian groups, and k layers can solve up to depth k; thus a two-layer model can track S_3. The paper also shows that diagonal models are expressive enough in theory, but in fact hard to train on non-Abelian tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide clear iff theorems and link depth with group structure: i.e., 1-layer - Abelian, and k-layer - length <= k. The paper also provides a good demonstration of two-layer S_3 that illustrates the theory, and presents an interesting observation that expressivity does not directly lead to learnability in reality."}, "weaknesses": {"value": "- It is uncertain if the observations in this paper will directly lead to same results in real-world benchmarks such as language modeling.\n- the experiments in the paper are limited, not providing results with different state dimensions, precisions, and decoders.\n- it is uncertain how the training details in the paper, and unclear if it is actually true that expressivity != learnability."}, "questions": {"value": "- How many, and what different training settings have the authors tried?\n- Do the results look the same no matter how the settings (e.g., hyperparam, learning rate, weight decay, scheduling) change?\n- how crucial is the universal decoder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ejf47MYGGz", "forum": "5bg5Ru5OML", "replyto": "5bg5Ru5OML", "signatures": ["ICLR.cc/2026/Conference/Submission8400/Reviewer_6RjB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8400/Reviewer_6RjB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952407017, "cdate": 1761952407017, "tmdate": 1762920300355, "mdate": 1762920300355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "General Response:\n\nDear reviewers, thank you for your extensive reviews and constructive feedback. Please find below the updated tables of results. We are now reporting best length-extrapolation over 3 seeds, state dimension in {32, 64, 128}, learning rate in {1e-4, 5e-4, 1e-3}, and learning-rate scheduler in {fixed, reduce learning-rate on plateau, cosine}. We have also added the Abelian groups $C_{24}$, $C_2 \\times C_4$ and $C_3 \\times C_6$, and the non-solvable group $A_5$. The main takeaway remains the same.\n\n\n*Results for single-layer models:*\n| Task               | Mamba  | Negative Mamba | Simple AUSSM  | AUSSM  | RNN  |\n|--------------------|--------|----------------|---------------|--------|------|\n| $C_2$              | ✗      | 1000           | 160           | 1000   | 1000 |\n| $C_6$              | ✗      | ✗              | 240           | 940    | 1000 |\n| $C_{24}$           | ✗      | ✗              | 240           | 260    | 1000 |\n| $C_{60}$           | ✗      | ✗              | 300           | 240    | ✗    |\n| $C_2 \\times C_4$   | ✗      | ✗              | 140           | 200    | 1000 |\n| $C_3 \\times C_6$   | ✗      | ✗              | 500           | 200    | ✗    |\n| $S_3$              | ✗      | ✗              | ✗             | ✗      | 1000 |\n| $A_4$              | ✗      | ✗              | ✗             | ✗      | 1000 |\n| $A_5$              | ✗      | ✗              | ✗             | ✗      | ✗    |\n\n\n*Results for 2-layer models:*\n\n| Task               | Mamba  | Negative Mamba | Simple AUSSM  | AUSSM  | RNN  |\n|--------------------|--------|----------------|---------------|--------|------|\n| $C_2$              | ✗      | 1000           | 1000          | 200    | 1000 |\n| $C_6$              | ✗      | ✗              | 240           | 100    | 1000 |\n| $C_{24}$           | ✗      | ✗              | 300           | 160    | 1000 |\n| $C_{60}$           | ✗      | ✗              | 260           | ✗      | ✗    |\n| $C_2 \\times C_4$   | ✗      | 360            | 160           | ✗      | 1000 |\n| $C_3 \\times C_6$   | ✗      | ✗              | 260           | 200    | ✗    |\n| $S_3$              | ✗      | ✗              | ✗             | ✗      | 1000 |\n| $A_4$              | ✗      | ✗              | ✗             | ✗      | 1000 |\n| $A_5$              | ✗      | ✗              | ✗             | ✗      | ✗    |\n\n\n*Remark:* The group $C_4$ can be written as the subnormal chain $C_4 \\triangleright C_2 \\triangleright \\set{e}$, where the factors are all $C_2$. This means that, according to our theory, stacking two layers, each of which is capable of solving $C_2$, e.g., Negative Mamba, can do state-tracking for $C_4$. And we see in practice that 2-layer Negative Mamba is able to do state-tracking for $C_2 \\times C_4$. Interestingly, this is a case where the increased expressivity from stacking layers *is* usable through gradient-based learning. We will add these results and observations to the paper."}}, "id": "GLbZvk0w1O", "forum": "5bg5Ru5OML", "replyto": "5bg5Ru5OML", "signatures": ["ICLR.cc/2026/Conference/Submission8400/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8400/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8400/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697186274, "cdate": 1763697186274, "tmdate": 1763697186274, "mdate": 1763697186274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}