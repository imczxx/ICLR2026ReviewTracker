{"id": "TNqbfqSPoD", "number": 9627, "cdate": 1758131117227, "mdate": 1759897708069, "content": {"title": "QUART: Agentic Reasoning To Discover Missing Knowledge in Multi-Domain Temporal Data.", "abstract": "Existing AI systems for expert decision support, commonly treat incomplete information as missing data to be filled or ignored, but this essentially misapprehends the fundamental challenge experts face: recognizing what crucial information is still unknown. Methods like knowledge graphs and LSTMs rely on temporal patterns and embeddings, which reduce interpretability and fail to address knowledge\ngaps through logical relations. We present QUART (Query-based Understanding Agent for Reasoning Temporal data), a multi-domain framework that preserves semantic meaning and actively learns to identify and recall unexamined information by reasoning over causal connections via reinforcement learning. QUART integrates interpretable semantic causal graphs, multi-agent policies optimizing\ndecision utility, and explicit modeling of unknown information to drive strategic questioning based on logical causal dependencies instead of temporal sequences. It encodes clinical outputs dynamically, approximating real expert workflows with confidence measurements. Evaluated on over 40,000 patient histories from the MIMIC-III healthcare database, QUART achieves over 10% higher diagnostic accuracy than LSTM baselines, not by imputing missing data but by revealing and addressing information blind spots. Furthermore, its dynamic agents protect privacy by querying sensitive data only when necessary. Although illustrated on medical data, pilot studies are underway exploring the framework’s potential in education, product management, and office decision-making. This work lays a\nfoundation for trustworthy, interpretable AI assistants that improve expert decision-making in complex and sensitive environments.", "tldr": "", "keywords": ["Agentic AI", "Missing Information", "Temporal Reasoning", "Multi-Domain Learning", "Reinforcement Learning", "Neural-Symbolic", "Expert Systems", "Causal Inference"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/78be0f6f7f74fec4359a94498594f93e1b318b31.pdf", "supplementary_material": "/attachment/eed2fd3d81b6097859a21056b744bcb5c3607c77.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents QUART, a multi-domain framework that preserves semantic meaning and actively learns to identify and recall unexamined information by reasoning over causal connections via reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The treatment of missing information as an explicit knowledge gap (versus a data incompleteness issue) is a profound insight that mirrors real-world expert reasoning in domains like healthcare.\n2. The proposed paradigm of \"proactively identifying unknowns\" offers a valuable complement to current prediction-dominated AI systems."}, "weaknesses": {"value": "1. The paper suffers from organizational and clarity issues that fall below ICLR's standards. Specifically: (1) the abstract is overloaded and lacks a clear focus; (2) the introduction does not crisply define the problem and contributions; (3) the methodology is described in a fragmented manner across subsections, hindering comprehension; (4) the experimental section lacks crucial details, particularly on ablations, undermining credibility; and (5) the conclusion merely repeats findings instead of providing a higher-level synthesis.\n2. While the methodology combines neural, symbolic, and agentic components, their integration appears shallow. For instance, is the interaction between neural and symbolic modules merely a weighted combination? It is unclear if the design truly leverages their complementary strengths or is merely a modular assembly.\n3. The experimental section is disjointed from in-depth analysis. The ablation studies in the appendix merely report percentage gains without a mechanistic analysis of why key components contribute most, which significantly weakens the persuasiveness of the claims."}, "questions": {"value": "1. There is an inconsistency in terminology: the abbreviation \"QUART\" is used in the title and abstract, but it appears as \"QuART\" in the introduction. This error should be corrected throughout the manuscript for professionalism.\n2. How do the hyper-parameters be determined? Especially in Eq. (2), (10)?\n3. The paper aims to simulate the human expert's ability to discover novel knowledge gaps. However, its methodology remains fundamentally based on statistical inference from historical data, and thus does not genuinely address the core challenge of meta-cognition.\n4. The proposed three-way integration appears superficial. The symbolic and neural components are combined via a simple weighted formula, while the agent acts more as a plug-in module attached to the main network, lacking autonomous goals.\n5. The paper's claim of performing \"causal reasoning\" is significantly undermined by its methodology. The causal graph is constructed via Q-learning from co-occurrence data, which captures correlation, not causation. Furthermore, the reliance on manually defined temporal constraints (e.g., symptom precedes diagnosis) establishes temporal precedence, not a causal relationship.\n6. In the partially observable environment, Q-learning is atypically used to learn static association strengths (the reward matrix R) between concepts. This is an unnatural and inefficient choice, as simpler and more stable methods (e.g., mutual information) exist for this purpose. The motivation for using Q-learning here is unclear and unjustified.\n7. The paper uses improved diagnostic accuracy to demonstrate QUART's effectiveness. However, its core contribution should lie in identifying missing information, not in the final diagnosis itself. This creates a misalignment between the claimed innovation and the evaluation metric. Could the authors propose more appropriate evaluation metrics?\n8. The paper lacks comparison with SOTA models that also focus on identifying \"unknowns\" or quantifying uncertainty, such as uncertainty-based active learning systems or Bayesian deep learning models which quantify epistemic uncertainty. Could the author compare the proposed model with more SOTA baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "f9bzLqDn1m", "forum": "TNqbfqSPoD", "replyto": "TNqbfqSPoD", "signatures": ["ICLR.cc/2026/Conference/Submission9627/Reviewer_qZi7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9627/Reviewer_qZi7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760771831484, "cdate": 1760771831484, "tmdate": 1762921163063, "mdate": 1762921163063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes QUART (Query-based Understanding Agent for Reasoning Temporal data), a neural-symbolic-agentic framework that helps AI systems identify and reason about missing information in temporal datasets, rather than imputing or ignoring it. Instead of treating missing data as statistical noise, QUART models it as a discoverable knowledge gap. It uses (1) Semantic causal graphs to represent knowledge, (2) Dynamic agentic layers that generate targeted questions about unknowns, and (3) Q-learning to identify and prioritize which missing information to explore. The model is trained and evaluated on 40,000 MIMIC-III clinical cases, integrating symbolic reasoning with neural embeddings and agentic question generation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "How to deal with the missing information in temporal datasets is an old, well-defined, yet not fully addressed problem."}, "weaknesses": {"value": "1. QUART is largely a composite of known ideas: causal graphs, Q-learning–based prioritization, and attention-driven question generation. The “agentic layer” is described as a unique innovation, but in practice functions as a weighted attention or heuristic querying module rather than a genuine autonomous agent. The work repackages familiar components without offering new theoretical insights or formal advances in reasoning or causal learning.\n\n2. All experiments are conducted on the MIMIC-III dataset, which is a well-curated and extensively studied static corpus. The evaluation relies on synthetic ground truth, expert-labeled “missing concepts” for only 200 cases, and subjective 5-point Likert scores for question quality. These measures do not convincingly demonstrate genuine knowledge discovery or clinical utility. The system is therefore validated on contrived conditions rather than realistic, real-time clinical tasks.\n\n3. The paper repeatedly emphasizes causal reasoning, yet in the context of MIMIC-III, causality is not empirically identifiable or even relevant. The dataset consists of retrospective observational hospital records, where causal relations cannot be inferred from co-occurrence or temporal adjacency alone. The proposed causal graph and “temporal path discovery” equations rely on sequence order and frequency statistics, not interventions or counterfactuals. Consequently, the claim that QUART performs causal or epistemic reasoning is methodologically unsound: the model merely encodes correlations in event order.\n\n4. The baselines (LSTM, knowledge graph, rule-based expert systems, imputation) are ten years old and do not reflect the current state of reasoning systems. There is no comparison with modern approaches. \n\n5. The paper conflates missing data, unknown concepts, and expert blind spots under the same mathematical formulation. The so-called “missing knowledge entities” are defined heuristically via absent nodes in a causal graph, with no formal epistemic basis or verification that they correspond to genuine expert ignorance. This ambiguity undermines the central premise of the work: identifying “what the expert does not yet know.”\n\n6. The reinforcement learning component lacks a well-defined environment, reward signal, or policy interpretation. Equations (6)–(9) effectively describe static co-occurrence scoring, not interactive learning. The notion of “agents” dynamically generating questions is not supported by algorithmic evidence or behavioral evaluation. As a result, the agentic reasoning claim is overstated and under-substantiated.\n\n7. Despite frequent use of “causal” and “trustworthy,” the paper provides no quantitative validation of causal correctness or fairness. Ethical aspects such as data privacy, bias, or patient safety are acknowledged only superficially. Given that the system “queries sensitive data,” this omission is problematic and leaves serious practical concerns unaddressed.\n\n8. The manuscript is dense, repetitive, and overloaded with terminology (“semantic-preserving agentic causal reasoning”). Figures and tables are largely schematic, and algorithmic procedures are described in prose rather than pseudocode. As a result, the method is difficult to reproduce, and its conceptual flow unclear."}, "questions": {"value": "I do not have specific questions for the authors. In my view, the current approach is fundamentally misguided: it builds on an incorrect interpretation of causality and an artificial evaluation setup that does not address a real scientific problem. I recommend the authors rethink the research direction from first principles, as the present framework is unlikely to lead to meaningful or valid results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WQBxIVnh3c", "forum": "TNqbfqSPoD", "replyto": "TNqbfqSPoD", "signatures": ["ICLR.cc/2026/Conference/Submission9627/Reviewer_Kt5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9627/Reviewer_Kt5F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760905044206, "cdate": 1760905044206, "tmdate": 1762921162833, "mdate": 1762921162833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QUART, a novel neural-symbolic-agentic framework designed to address a critical challenge in expert decision support: the identification and active discovery of missing information or knowledge gaps, rather than merely imputing or ignoring missing data. The core innovation is an architecture that integrates a trainable agentic layer which dynamically generates specialized \"questioning agents\" to reason over causal connections and prioritize information gaps."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper shifts the paradigm from \"filling missing data\" to \"discovering missing knowledge,\" which is a more accurate and valuable model of expert reasoning.\nThe integration of a dynamic, learnable agentic layer (L4) within a neural-symbolic framework is a compelling technical contribution.\nThe paper provides a thorough empirical evaluation on a large, real-world dataset (MIMIC-III)."}, "weaknesses": {"value": "While an ablation study is mentioned in the appendix, the main paper lacks a clear, component-wise ablation in the core results section (e.g., Table 1). A more detailed breakdown in the main text is needed to definitively quantify the contribution of the agentic layer versus the neural-symbolic base.\nThe paper successfully demonstrates that the agentic layer improves performance, but it provides limited analysis or visualization of what the agents are actually learning to do. Did you observe any interpretable patterns or specializations in the dynamically created agents?\nThe description in Section 6 and the appendix remains high-level and lacks empirical results or detailed experimental setups for the education and project management domains. \nThe baseline models (LSTM, Knowledge Graphs) are well-established but somewhat classical. A comparison with a more recent, powerful baseline would better situate QUART's performance in the current landscape."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "F9oSHxgdf5", "forum": "TNqbfqSPoD", "replyto": "TNqbfqSPoD", "signatures": ["ICLR.cc/2026/Conference/Submission9627/Reviewer_1DMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9627/Reviewer_1DMk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814230514, "cdate": 1761814230514, "tmdate": 1762921162554, "mdate": 1762921162554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes QUART, a multi-step framework that focuses on discovering hidden knowledge in Medical data through a complex combination of Neural Symbolic Reasoning for \"questioning\" strategies (which the reviewer is unclear of its definition) and Q-learning to discover relationships between medical concepts.  The authors reports superior results on ill-defined \"Missing Information Detection\" and \"Clinical Question Relevance\" on MIMIC-III EHR dataset, compared with an LSTM model, a knowledge-based and rule-based system that are not elaborated on."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of solving missing medical concepts as a discovery problem is a unique and logical approach to this problem, which heavily contrasts with most of how existing research handles it. If successful, this helps uncover many hidden valuable knowledge for many medically relevant tasks.\n- Applying Q-Learning to discover medical relationships is a sound technical choice, and the reward function designed by the authors seems to be quite logical and makes sense based on real-world intuition for EHR data."}, "weaknesses": {"value": "1) Extreme Lack of clarity on various critical details and concepts\n\nThe medical concept sets: the source of the concepts (are they ICD-9 or ICD-10 codes, or some other sources), or if it's in-house data, then how they are constructed, and some quantitative samples of the medical concepts to illustrate what they entail. \nThe term \"Question\" and the process of \"Question generation\" are a very crucial part of the work, but I have no idea what this means or what the \"questions\" look like.\n\nMany other concepts mentioned are unexplained on what they are, or what they look like as well, such as the sources (expert attribution) and details (metadata) within a semantic node (line 222, page 5). What does \"medically relevant concept pairs\", which is used to define MedicalBonus in Equation 7, mean (line 266).\n\nAs quoted by the paper, \"We additionally leverage representations from pre-trained large language models (LLMs) to enhance neural attention over clinical concepts and guide question generation.\" (line 322-324), what are the pretrained large language models in use here? Do you mean a pretrained text embedding model, like ClinicalBert mentioned?\n\nWhat is the input data for each sample of a patient's trajectories: is it a set of clinical codes (ICD codes), or does it contain other information like clinical notes, test results, as you mentioned in lines 134-135.\n\nBenchmark details: Which datasets are you using as benchmarks here? And also, both the tasks you mentioned as benchmark (which I assume is defined in Section 3.2 (Line 141-148) are not well-defined, and don't seem to be an established benchmark in previous works.\n\nFor the \"Missing Information Detection\" task, how do you define and set this benchmark up? If I were to guess, then you pick 200 patients diagnosed with ICD codes, and the model attempts to predict other codes not mentioned as \"Missing concepts\", but the actual detail is very vague, especially for the general readers not familiar with EHR data conventions. \nFor the \"Question Quality\" part, like I've stated above, I do not know what the \"questions\" here mea,n so this metric is confusing to me as well.\n\n2) Hard to follow methodology\nThe main system figure (Figure 1) is really messy and not very illustrative of the working mechanism.\nWhat do \"Question Generation\" and \"Clinical Support\" (Figure 1) mean in this particular context?\nWhat is the role of \" DYNAMIC AGENT CREATION\"  in the system (Section 4.5).\nWhy do you need to detect anomalies in this task (341-347)?\nWhat is the RL feedback (Figure 1) in this system, where was it elaborated on in the paper?\nThe flow of the paper, in general, is just hard to follow, especially after Section 4.3\n\n3) Poorly-designed and unreliable experimental evaluation: \n\nLack of state-of-the-art baselines: The paper compares their experimental results with an LSTM model, a knowledge-based and a rule-based system. Meanwhile, the authors themselves mentioned the presence of many modern Deep Learning/foundation models based methods (Bio_ClinicalBert, general LLMs…) (Line 155-156). I believe comparison with these methods is necessary to justify the performance of QUART.\nVery small sample size: The main experimental results are conducted on only 200 data samples from a single dataset,MIMIC-III, which I believe is not sufficient for conclusive findings, even with statistical testing, while the MIMIC-III database has thousands of patients available.\nPoorly-defined evaluation protocols and metrics: Relating to the point about clarity above, the evaluation tasks and metrics are not defined or justified by the authors. These metrics do not seem to originate from other established works as well, which further brings the reliability and correctness of the results in this work into question.\nPoor ablation studies/analyses: The method consists of multiple working components with a lot of fixed hyperparameters. The paper only reports a very short ablation study (line 674-678), which does not illustrate well the importance and effects of the components to the system. No ablation studies on any of the various hyperparameters are provided as well, which is crucial for implementing such systems in real-world use cases.\n\n4) Temporal Causal Reasoning\nThe paper makes the claim to use BFS over architectures like LSTM to capture\nlogical causal relationships via temporal path discovery. I believe experimental comparisons should be made to justify that temporal path discovery is better than sequential learning (which is the norm for all temporal EHR works)"}, "questions": {"value": "An explanation/or illustrative example of various concepts mentioned in the paper (see Weaknesses)'.\nClearly define the input and output of the model, the benchmarks, and the evaluation protocol."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SLU0hUbcWE", "forum": "TNqbfqSPoD", "replyto": "TNqbfqSPoD", "signatures": ["ICLR.cc/2026/Conference/Submission9627/Reviewer_EnQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9627/Reviewer_EnQX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993404013, "cdate": 1761993404013, "tmdate": 1762921162232, "mdate": 1762921162232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}