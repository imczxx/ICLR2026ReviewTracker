{"id": "vGpEO5foQ6", "number": 16295, "cdate": 1758262778496, "mdate": 1759897249498, "content": {"title": "Revealing Stochastic Noise of Policy Entropy in Reinforcement Learning", "abstract": "On-policy MARL remains attractive under non-stationarity but typically relies on a fixed entropy bonus that conflates useful exploration with stochastic fluctuations from concurrently learning agents. We present \\emph{Policy Entropy Manipulation (PEM)}, a simple, drop-in alternative that treats entropy as a noisy measurement to be \\emph{denoised} rather than uniformly maximized. PEM applies positive--negative momentum to \\emph{entropy gradient} to form a high-pass, variance-controlled signal that preserves persistent exploratory trends while suppressing transient spikes. The method integrates seamlessly with heterogeneous, permutation-based on-policy algorithms, requires no critic or advantage changes, and uses a one-step warm start that recovers the standard entropy objective before momentum takes effect. Across benchmark tasks, PEM yields smoother and more stable learning curves, and improves coordination under heterogeneous observation/action spaces, resulting in stronger generalization than conventional entropy-regularized baselines. Our results indicate that noise-aware control of the entropy channel is an effective and principled way to stabilize exploration in cooperative MARL.", "tldr": "", "keywords": ["multi-agent reinforcement learning", "policy optimization", "decentralized partially observable Markov decision process"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4a4ed73d67b765170048440c860f18f896684e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper attacks On-policy Multi-Agent Reinforcement Learning (MARL). This seeting typically relies on a fixed entropy bonus that conflates useful exploration with stochastic fluctuations from concurrently learning agents. The paper presents Policy Entropy Manipulation\n(PEM), a simple, drop-in alternative that treats entropy as a noisy measurement to be denoised rather than uniformly maximized. PEM applies positive–negative momentum to entropy gradient to form a high-pass, variance-controlled signal that preserves persistent exploratory trends while suppressing transient spikes. Experiments demonstrate that PEM yields smoother and more stable learning curves,\nand improves coordination under heterogeneous observation/action spaces, resulting in stronger generalization than conventional entropy-regularized baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Identified a neat problem in the Multi-Agent Reinforcement Learning (MARL) setting.\n\n- The solution is simple and effective. \n\n- The evaluation on multiple environments demonstrates the effectiveness of the proposed method."}, "weaknesses": {"value": "- It is difficult to grasp the intuitive reason behind the introduction of the positive-negative momentum of policy entropy. The paper looks like a dedicated engineering effort to outperform existing baselines. However, I have learned relatively little by reading the paper.\n\n- The paper does not offer theoretical guarantees (or justifications) on the effectiveness of their approach.\n\n- The font in Figure 1 is way too small. I also do not understand what the figure is trying to communicate. Similar arguments apply to all other figures."}, "questions": {"value": "- Can the reviewers provide more intuitive rationale behind the introduction of the positive-negative momentum of policy entropy?\n\n- Can we quantify the generalization capability (and learning capability, etc.) theoretically?\n\n- Many claims are confusing. For example, in the abstract, it is argued that smoother and more stable learning curves result in stronger generalization. Why? It seems the two are not logically connected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "meXPO6PH2j", "forum": "vGpEO5foQ6", "replyto": "vGpEO5foQ6", "signatures": ["ICLR.cc/2026/Conference/Submission16295/Reviewer_VUAe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16295/Reviewer_VUAe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761092530126, "cdate": 1761092530126, "tmdate": 1762926438410, "mdate": 1762926438410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Policy Entropy Manipulation (PEM), an on-policy optimization method for controlling entropy in multi-agent reinforcement learning (MARL). The approach is motivated by the need to distinguish beneficial exploratory signals from detrimental stochastic fluctuations in heterogeneous multi-agent environments. PEM modifies the entropy term by introducing a positive-negative momentum mechanism that aims to suppress high-frequency noise while preserving coordinated, low-frequency exploration. Empirical results are presented across continuous and discrete multi-agent tasks, suggesting that the method stabilizes training dynamics and may improve coordination and generalization among agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a meaningful problem in multi-agent RL regarding entropy-induced instability and coordination.\n- Proposes an intuitive mechanism to separate beneficial exploration from random stochastic fluctuations.\n- Includes evaluation in both discrete and continuous control environments.\n- Presents clear motivation and algorithmic formulation."}, "weaknesses": {"value": "- No theoretical justification for the proposed entropy manipulation mechanism or its claimed benefits.\n- Empirical improvements are weak, with performance often comparable to or worse than baseline algorithms.\n- Experimental details are incomplete, and unclear metric definitions (e.g., “Final” vs. “Mean”).\n- Figures are inconsistently scaled and lack normalization, making direct performance comparison difficult.\n- Claims of improved training stability and convergence are not quantitatively supported or measured."}, "questions": {"value": "**Detailed Review:**\n\nThe paper proposes Policy Entropy Manipulation (PEM) as a method for controlling policy entropy in multi-agent reinforcement learning. The central idea is that policy entropy contains both useful exploratory signals and harmful stochastic noise. By applying a positive-negative momentum update to the entropy term, PEM aims to suppress spurious fluctuations while preserving coordinated exploration that aids learning stability.\n\nThe conceptual motivation is sound, as entropy regulation plays an important role in MARL. However, the method is introduced mainly at an intuitive level, without formal justification or empirical ablation to validate its core assumptions. There is no clear analysis explaining why high-frequency entropy variations are harmful or why the proposed momentum formulation should improve learning dynamics. This absence of theoretical grounding weakens the central argument.\n\nEmpirically, the method is evaluated in both discrete and continuous environments, yet the results do not show a consistent advantage over baselines. In many cases, PEM performs similarly to or below the baseline algorithms (HAPPO, MAPPO). For instance, Figure 1 indicates that the PEM-enhanced algorithm tracks the baseline HAPPO curve closely, without clear performance gain or stability improvement. Figures 2 and 3 show similar patterns, where training performance remains nearly identical across methods.\n\nThe claim that PEM improves convergence stability is not supported by quantitative metrics, as no explicit stability measurement or variance analysis is provided. It is also unclear how many independent runs were conducted or whether the results were averaged across seeds. This omission limits confidence in the conclusions. The missing environment references (lines 310–323) further complicate reproducibility, and the use of non-normalized scales in figures obscures direct comparison.\n\nWhile the paper deserves credit for addressing an important issue in MARL and offering a clear algorithmic proposal, the current version lacks sufficient theoretical and empirical depth. The idea could become impactful with further justification, ablation studies, and statistically sound experiments demonstrating consistent gains.\n\n**Questions:**\n\n1. How many independent runs or random seeds were used, and can variance or confidence intervals be reported?\n2. What is the difference between “Final” and “Mean” performance values in the tables?\n3. How is training stability quantified, and what metric supports the claim of improved convergence?\n4. Why does PEM perform similarly to or below HAPPO in several figures, and how should this be interpreted?\n5. Could the environment references (lines 310–323) be added for completeness and reproducibility?\n6. How does the proposed method handle continuous vs. discrete environments, and why do results appear similar across both settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x7sOoOq8he", "forum": "vGpEO5foQ6", "replyto": "vGpEO5foQ6", "signatures": ["ICLR.cc/2026/Conference/Submission16295/Reviewer_jRon"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16295/Reviewer_jRon"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970455821, "cdate": 1761970455821, "tmdate": 1762926438053, "mdate": 1762926438053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Policy Entropy Manipulation (PEM), a technique to smooth the entropy regularization constraint in multi-agent reinforcement learning (MARL). It replaces the entropy regularization in a regularized objective with positive-negative momentum in the hope of reducing the stochastic noise in the entropy component when optimizing the entropy-regularized objective. Empirically, it investigates the performance of HAPPO and HAAC combined with PEM in a few settings."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper has the following strength: It comes from an interesting perspective: In deep (multi-agent) reinforcement learning, it is widely believed that training value functions or policies are unstable. Understanding and improving the training dynamic are very valuable. This paper targets the stochastic noise of the entropy term when optimizing an entropy regularized objective, which is an interesting direction to stabilize training and performance."}, "weaknesses": {"value": "The paper has some significant weaknesses and need improvements for further review and publication:\n1. It makes weak empirical evaluation and unsubstantiated claims. First, the overall empirical results are weak and not convincing. The paper does not disclose the number of seeds nor include error measures in the plots. In addition, the hypoparameters and the process of tuning them are not included. These missing details invalidate the empirical results, resulting in unsupported claims.\n2. Its motivation is not clearly explained. The paper distinguishes useful exploration noise and detrimental stochastic noise throughout, but it didn’t properly define or describe the terms, leaving a hole in its exposition. These terms should be properly discussed in detail either in Sections 3 or 4.\n3. Its writing is not polished and has a lot of issues. 1) It is full of typos and errors. See the Questions section for an incomplete list. 2) Its title does not match the content: The paper didn’t really reveal stochastic noise of policy entropy. 3) The related work section is too long, and the discussion of maximum entropy RL and heterogeneous-agent RL are not necessary and distracting. 4) Section 5.1 should also discuss the baselines used. 5) Section 3.2 should be expanded to explain how the objective optimize policies of different agents."}, "questions": {"value": "Questions that may impact the evaluation:\n1. Does the proposed method require a buffer/memory that grows linearly with the timestep $t$?\n2. Could the authors provide the missing details about the experiments mentioned in the Weaknesses section?\n\nMinor suggestions:\n1. Line 112: The mixed use of $s_i$ and $o_i$ throughout the paper creates confusion.\n2. Line 126: Inconsistent font in the second equation.\n3. Line 169: The grammar of the sentence needs to be fixed.\n4. Line 172: The first addition in Eq. 5 should be a subtraction.\n5. Line 270: The title should be HAPPO with PEM\n6. Lines 450-458: Incorrect citation format.\n7. Line 453: DAD-DQN should be a typo.\n8. Line 459: “EFARL” should be a typo."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bd3oCQZryq", "forum": "vGpEO5foQ6", "replyto": "vGpEO5foQ6", "signatures": ["ICLR.cc/2026/Conference/Submission16295/Reviewer_qRsN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16295/Reviewer_qRsN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992354089, "cdate": 1761992354089, "tmdate": 1762926437523, "mdate": 1762926437523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers entropy not as a universal bonus to be maximized in policy gradient methods, but as a noisy estimate of what should be optimized. It adds a momentum based denoising method to the entropy (summing weighted sums of past entropy changes as an entropy bonus), and adds this as a bonus instead. The paper argues that this perspective is particularly important in multiagent reinforcement learning settings, and performs experiments on the multiagent particle environment, starcraft (Smac v2), and AirSim search and rescue. Their method is called PEM, and they compare it against known methods like MAPPO, HAPPO in combination with them. The results showed that MAPPO remains the best in some tasks (the “Reference” task while PEM+HAPPO slightly improved over HAPPO on the search task), while PEM-based methods slightly improved on some tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The writing was OK, and background and related works were discussed well."}, "weaknesses": {"value": "- The performance was not very strong with inconsistent results.\n\n- I couldn’t find how many runs the experiments were done for? Were these done with a single experimental run? Typically many experimental runs are necessary to obtain statistically significant results.\n\n- There were no error bars.\n\n- I didn’t fully grasp the motivation, why should we consider the entropy as noise that should be denoised? And why is this related to the non-stationarity? Policy entropy can be exactly computed for a given state and policy, so the noise would come mostly from the trajectory sampling. But, I don’t see how you denoise this, other than increasing the sample size.\n\n- The method seemed to be a generic policy gradient algorithm. Why was it used specifically for multi-agent settings? Could the main hypotheses be tested on simpler canonical settings that isolate the claims in the paper?\n\n- Other than reward curves, no other statistics about the performance were provided. This makes it difficult to establish what causes any changes in performance."}, "questions": {"value": "See the weaknesses. I’m afraid it would be difficult for me to change my assessment of this paper, as I believe the quality is not sufficient for me at this stage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KAcbr2By4k", "forum": "vGpEO5foQ6", "replyto": "vGpEO5foQ6", "signatures": ["ICLR.cc/2026/Conference/Submission16295/Reviewer_shPX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16295/Reviewer_shPX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998010984, "cdate": 1761998010984, "tmdate": 1762926437099, "mdate": 1762926437099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}