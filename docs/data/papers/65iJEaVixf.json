{"id": "65iJEaVixf", "number": 7624, "cdate": 1758029617975, "mdate": 1759897842905, "content": {"title": "FINSIGHT: TOWARDS REAL-WORLD FINANCIAL DEEP RESEARCH", "abstract": "Financial research reports are vital for investment decisions, yet their creation is a complex process that current AI methods struggle to fully automate due to challenges in domain knowledge, multimodal generation, and analysis depth. To address these limitations, we introduce FinSight, a novel multi-agent system designed to generate high-quality, multimodal financial reports.\nAt its core, FinSight features the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, driven by code to empower the system to process large-scale, multi-source data in an agentic manner. Recognizing the limitations of traditional methods in chart generation, we propose an Iterative Vision-Enhanced Mechanism to progressively refine visualizations into professional-grade charts. Furthermore, we have developed a Two-Stage Writing Framework that expands concise Chains-of-Analysis into comprehensive and coherent multimodal reports.\nExperiments conducted on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.", "tldr": "", "keywords": ["Financial", "deep research", "report generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b27b93d1d18139015cb4c376668bfc27ab08a10e.pdf", "supplementary_material": "/attachment/720604f79014727876077666e14235a3f0a33d3f.zip"}, "replies": [{"content": {"summary": {"value": "FinSight introduces a multi-agent system for automated financial report generation. Its CAVM architecture and iterative vision mechanism enable accurate data synthesis and professional-grade charts, and the two-stage writing framework enhances coherence and depth. FinSight uses LLM as judge to evaluate the qualities of generated reports, comparing with deep research models. Experimental results demonstrate that FinSight achieves the highest scores across all evaluation dimensions, outperforming both deep search agents and LLMs equipped with search tools."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework diagram of Finsight is very clear, and the overall workflow is well-structured and easy to follow.\n2. The proposed two-stage writing framework substantially enhances the overall quality of report generation. Also, the iterative vision-enhanced mechanism effectively integrates textual reasoning with visual elements to meet the needs of multimodal financial analysis."}, "weaknesses": {"value": "1. The approach largely extends the deep research agent framework to the financial domain, with improvements mainly in the writing and generation stages. While effective, the overall innovation appears somewhat limited. Also, CAVM is overclaimed as a novel agent architecture. While FinSight includes limited code-execution components for data visualization and analysis, its overall workflow is primarily coordinated through multi-agent collaboration enhanced by code execution rather than code-driven reasoning.\n\n2. In the report generation evaluation process, Finsight uses LLM as judge to give scores on 3 different aspects. However, for the factual accuracy dimension, aspects such as citation accuracy and faithfulness are typically measurable through formula- or rule-based metrics in RAG. Relying on an LLM to subjectively score these aspects may introduce bias or inconsistency.\n\n3. In the experiments, FinSight was compared with LLMs with search tools and deep research agents. As shown in Figure 4, the reports generated by FinSight are significantly longer and get better scores. However, this correlation between report length and quality score may not be entirely desirable. It could suggest a potential bias in the LLM-based evaluation, which might favor longer outputs while paying less attention to their actual quality. A relatively fair comparison could be achieved by controlling the report length across methods.\n4. The paper does not include the implementation code, which may limit the reproducibility of the proposed method."}, "questions": {"value": "1. How is code execution integrated into the FinSight workflow? From the overall framework, it seems that the writing stage mainly involves text generation rather than executable code.\n2. Could the authors clarify whether the proposed *variable memory* primarily serves as a conceptual abstraction for flexible variable access, or if it introduces a genuinely new representation or learning mechanism in memory design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YtIGAmDkm0", "forum": "65iJEaVixf", "replyto": "65iJEaVixf", "signatures": ["ICLR.cc/2026/Conference/Submission7624/Reviewer_vZ9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7624/Reviewer_vZ9e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761030149632, "cdate": 1761030149632, "tmdate": 1762919704396, "mdate": 1762919704396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FinSight multi-agent system for generating long-form, multimodal financial research reports. Two main contributions are a Code Agent with Variable memory framework that unifies data, tools, and agents together with code execution, and a two stage writing framework with Chains of Analysis to produce structured reports. A vision-enhanced loop is also included to critique and refine code-generated charts (stock price charts, etc). The paper provides several examples in the appendix. \n\nThe paper outlines the main problem as producing a unified text, visual, and citation-grounded report with access to real-time data (via tools) and grounded content generation (VLM critic, graphs, visual generation). The system is evaluated on a small 20 target benchmark and evaluated on 9 metrics by an LLM-as-Judge (Gemini-2.5-pro). The framework is compared against several commercial dee research platforms (OpenIAI, Grok, Deepseek)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. System design is coherent and includes formulations for the CAVM updates. There is a clear decomposition and separation of modalities with a programmable memory abstraction. \n2. Integration of modalities is well defined and motivated, and examples in appendix look good. \n3. Iteration on stock charts highlights the core contrition of the VLLM critic. This is well-motivated, and the iterative model, factor analysis is well specified. \n4. Generally readable minus 1 typo. \n5. Ablations cover each component and experiments cover wide variety of commercial research platforms. \n6. Transparency on toolchain, judge prompts is very helpful for reproducibility."}, "weaknesses": {"value": "1. LLM-as-judge without calibration or human study. Scores are produced by Gemini-2.5-Pro using list-wise prompts, but the paper does not report inter-rater reliability, judge sensitivity to prompt phrasing, or correlation with human experts. No robustness analysis (e.g., seed variation, alternative judges, bootstrap CIs) is provided. The bias in gemini is evident from the experiments (gemini is often 2nd best). \n2. Small, in-distribution benchmark. There are only 20 examples, and the formats/layouts/color schemes are similar. There is no OOD test on alternative firm reports or other market/regulators (EU reports, EDGAR) - generalizability is unclear. \n3. The decision to restrict google search api to a single region -> is this applied to all models? How are the authors mitigating regional bias from one provider vs another. This should be made clearer.\n4. Metric definitions are all subjective, given the small evaluation set the anticipated variability on repeated judgements is not clear. what is the CI on these numbers? Furthermore, evaluation is qualitative - this paper would benefit from some quantitative metrics (difference in pixels from real data vs generated chart, etc.)"}, "questions": {"value": "1. What is the agreement between gemini and human labels? What is the sensitivity to prompt variants and stochastic generations? 20 samples is a small set, what are the confidence intervals and variability in measurements under repeated analysis?\n2. Beyond LLM scoring, did you evaluation precision/recall on citations or fact checking? How did you evaluate that claims are grounded in sources? How are you auditing the factual and quantitative claims in the reports? Consider adding a human review study of the reports for factuality and hallucination detection. \n3. Baselines: How did you ensure each baseline had the same regional access to data?\n4. Can you comment on plans for out of domain generalization experiments? Have you considered public filings from other jurisdictions (US annual reports, 10Ks, EU annual reports). These are mandated to be public, which helps reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BioY1Oev4x", "forum": "65iJEaVixf", "replyto": "65iJEaVixf", "signatures": ["ICLR.cc/2026/Conference/Submission7624/Reviewer_9Dq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7624/Reviewer_9Dq6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761270849390, "cdate": 1761270849390, "tmdate": 1762919703788, "mdate": 1762919703788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FinSight, a multi-agent system for automatically generating multimodal financial research reports that integrate text, visualizations, and citations. The system addresses three challenges—domain knowledge, multimodal generation, and analytical depth—by introducing a Code Agent with Variable Memory (CAVM) for tool-based data processing, and a Two-Stage Writing Framework that transforms chains of analysis into structured reports. An Iterative Vision-Enhanced Mechanism refines visual outputs into professional-looking charts. Experiments on company- and industry-level tasks show FinSight outperforming commercial deep research systems in factual accuracy, analytical depth, and presentation quality based on automatic and LLM-based evaluations. However, no human evaluation or code release is provided. While the work tackles a novel and practically interesting problem, the paper’s structure and methodological clarity are limited, and the evaluation lacks rigor."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The topic is timely and practically relevant, addressing the automation of multimodal financial analysis—a domain rarely explored in LLM research. The proposed FinSight system integrates multiple functional agents (retrieval, visualization, and report writing) into a coherent pipeline. The idea of separating analytical reasoning from structured writing via the Two-Stage Writing Framework is conceptually sound and could be useful for other applied domains. The paper provides qualitative examples demonstrating real-world potential and shows moderate engineering effort to bridge LLM reasoning with data-driven financial reporting."}, "weaknesses": {"value": "1. The problem formulation lacks rigor. The definition of the financial report R as a set ignores its hierarchical and sequential structure, and the mapping from query q to report R is not well formalized.\n2. The paper’s overall organization is unclear, with descriptions of system components, data collection, and evaluation interleaved, making it difficult to follow the main contributions.\n3. No code, data, or benchmark release, which limits reproducibility. Given the complexity of the multi-agent design, partial release or detailed pseudo-code would be essential for validation.\n4. Evaluation relies entirely on automatic or LLM-based metrics, with no human expert assessment. The claim that FinSight “approaches human-expert quality” is not substantiated."}, "questions": {"value": "1. Please clarify how “human-expert quality” was defined and whether any human evaluation was performed.\n2. Do you plan to release code or data to support reproducibility?\n3. How is the “financial research report” defined—does it target professional investment research or general analytical summaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k2NnwAi6jN", "forum": "65iJEaVixf", "replyto": "65iJEaVixf", "signatures": ["ICLR.cc/2026/Conference/Submission7624/Reviewer_2oEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7624/Reviewer_2oEk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869284581, "cdate": 1761869284581, "tmdate": 1762919703428, "mdate": 1762919703428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}