{"id": "t5GUEuIsxR", "number": 21996, "cdate": 1758324558973, "mdate": 1759896891872, "content": {"title": "FutureFill: Fast Generation from Convolutional Sequence Models", "abstract": "We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill—a general-purpose fast generation method for any sequence prediction algorithm based on convolutional operators. FutureFill reduces generation time from quadratic to quasilinear in the context length. Moreover,  when generating from a prompt, it requires a prefill cache whose size grows only with the number of tokens to be generated—often much smaller than the caches required by standard convolutional or attention‐based models. We validate our theoretical claims with language modeling experiments and demonstrate substantial efficiency gains when generating from a deep convolutional sequence prediction model.", "tldr": "FutureFill introduces a fast autoregressive generation method for convolutional sequence-prediction algorithms — reducing generation time from quadratic to quasilinear in the context length and is supported by theoretical results and experiments.", "keywords": ["convolutional models", "fast inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40ea8307551bf78baa3e602f07861b60ff6ccdbe.pdf", "supplementary_material": "/attachment/e81411979bdf2f40704fb7c63f766aebb87cc373.zip"}, "replies": [{"content": {"summary": {"value": "This paper continues the line of work aiming to speed up inference with respect to sequence length and to avoid the quadratic complexity of standard Transformers. The authors propose a method for fast generation using convolutional sequence models. They focus on exact auto-regressive generation from convolutional models, reducing both generation time and cache size, achieving O(N log N) complexity instead of O(L²).\nEmpirical results at small scale show that the proposed algorithms achieve sub-quadratic scaling compared to naive convolution implementations, and they report up to 1.7× speedup over the baseline.\nOverall, this is a solid  paper that provides a practical speedup method, but the experimental validation feels limited."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work is technically sound and contributes to the ongoing effort of making sequence models more efficient at inference time."}, "weaknesses": {"value": "The experiments are limited to relatively small language models (below 1B parameters), which makes it hard to assess the impact at realistic scales. In addition, only inference speed results are presented — there is no evaluation of model quality (e.g., perplexity or downstream performance), which is important to verify that the gains do not come at the cost of degraded output quality."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MBPeZnj7WA", "forum": "t5GUEuIsxR", "replyto": "t5GUEuIsxR", "signatures": ["ICLR.cc/2026/Conference/Submission21996/Reviewer_y912"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21996/Reviewer_y912"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761217996160, "cdate": 1761217996160, "tmdate": 1762942012110, "mdate": 1762942012110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"FutureFill,\" a novel and efficient method for auto-regressive generation from sequence models that use convolutional operators. The primary contribution is an algorithmic technique that reduces the computational complexity of generating L tokens from scratch from a quadratic $O(L^2)$, which is typical for naive online convolution, to a quasilinear. The paper presents two concrete algorithms based on this idea:\n\n1. Continuous-FutureFill: An algorithm that achieves the $O(L log^2 L)$ runtime with $O(L)$ memory.\n2. Epoched-FutureFill: A more practical variant that offers a trade-off between runtime and memory.\n\nFurthermore, the paper shows that when generating K tokens from a prompt of length L, FutureFill significantly reduces the required cache size from $O(L+K)$ to $O(K)$, a crucial improvement for long-context applications. The authors validate their theoretical claims with experiments on both synthetic data and large-scale (up to 826M parameters) convolutional language models, demonstrating empirical speedups of up to 2x over baseline methods on modern hardware."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1: The paper tackles a critical and well-known bottleneck in sequence modeling, i.e. the slow quadratic-time generation process for models based on convolutions. Making this efficient, especially in long-sequnece scenerios is a major practical contribution.\n\n2. The paper is clearly written and the theoretical claims are well-supported by theorems and complexity. The experiments not only demonstrate asymptotic behavior but also wall-clock time improvements.\n\n3. The idea of futurefill is intuitive and based on observations and intuition regarding the properties of convolution and FFT."}, "weaknesses": {"value": "1. The paper points out that there has been independent and concurrent work that achieves the same runtime complexity, which slightly tempers the novelty.\n\n2. The paper seems to focus only on FlashSTU-T model and would be nice to show how this is generalizable to other convolutional models (e.g. Hyena).\n\n3. While the paper does demonstrate real speed improvement of 2x, it was not as dramatic as the difference from $O(L^2)$ to $O(L)$. More detailed analysis here on why the speedup was not significant would be interesting and informative."}, "questions": {"value": "1. Compared to other works that achieve the same complexity, could the authors elaborate more on the qualitative or practical differences between FutureFill and other concurrent works?\n\n2. How would the algorithm perform if the generation length is not known in advance, i.e. the epoch length \n was set to the theoretical optimum but what should peopole use in real-world scenerios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OOtwprgPmA", "forum": "t5GUEuIsxR", "replyto": "t5GUEuIsxR", "signatures": ["ICLR.cc/2026/Conference/Submission21996/Reviewer_d6Cp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21996/Reviewer_d6Cp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855261336, "cdate": 1761855261336, "tmdate": 1762942011879, "mdate": 1762942011879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper positions itself with respect to recent work that uses convolutional operators as a way of mitigating complexity issues in autoregressive attention-based models.  Specifically, they propose a method FutureFill that reduces complexity in text generation below quadratic in context length; a core part of the idea is that there is a memory trade-off that permits the reduction in inference time complexity, which also allows a spectrum of algorithm variants in terms of that trade-off.  The paper contains some theoretical results on this complexity, and also some experimental results looking empirically at inference times as a function of context length, as well as checking the performance on several downstream tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* Overall, I think this is quite a strong paper.  The use of convolutional approaches as in e.g. Hyena is an important direction to address the quadratic complexity issue, and this paper’s contribution looks like an important step in that, and could well be adopted quite widely as a source of performance improvements."}, "weaknesses": {"value": "These are mostly relatively minor.\n\n* The Abstract is fairly short and bare-bones; it’s not really until reading the paper that the actual importance of the work comes through.\n\n* It’s fairly reasonable given space constraints to save most of the literature review of Sec 1.1 for the appendix.  However, something that I thought was missing in both Sec 1.1 and the appendix’s extended version was the discussion of differences wrt Oncescu et al. (2024).  This is presented as an independent work that achieves the same complexity result as the present paper, but there’s no argument made as to why FutureFill then is necessary.  Is there something more advantageous about the memory trade-off inherent in FutureFill?  Is there some limitation to Oncescu?  It’s not made clear why a new method with this complexity is a useful thing to have, and this is quite crucial for understanding the importance of the present paper's contribution.\n\n* Fig 5 in App D.3 is presented a depiction of the FutureFill operation between an input sequence and a convolutional filter; it’s where the reader is supposed to get a concrete idea of how the method works, to supplement the mathematical definitions.  However, the diagram is very schematic and abstract, and not actually much of a help.  Some fleshing out of the diagram and explanation in D.3 would be helpful.\n\n* For the proof of Propn 1, it would be helpful to link explicitly to App F."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "n8ABm1dwJJ", "forum": "t5GUEuIsxR", "replyto": "t5GUEuIsxR", "signatures": ["ICLR.cc/2026/Conference/Submission21996/Reviewer_9QkU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21996/Reviewer_9QkU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901420118, "cdate": 1761901420118, "tmdate": 1762942011538, "mdate": 1762942011538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FutureFill, a fast inference algorithm for convolutional sequence models that reduces generation complexity by precomputing future contributions via FFT-based convolutions. The method maintains exact output equivalence with standard convolution while achieving notable latency improvements on FlashSTU-T models, all without retraining or architectural changes. It also presents two practical variants—Epoched and Continuous FutureFill—that balance memory and speed for different deployment settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a less-studied yet important bottleneck in convolutional language models.\n- Strong theoretical foundation with clear runtime and correctness guarantees.\n- Training-free and exact—no compromise on model quality.\n- Shows consistent practical gains and integrates seamlessly with existing architectures.\n- Provides clear implementation details enabling reproducibility."}, "weaknesses": {"value": "- Scalability to multi-billion parameter models not yet validated.\n- Baselines limited; comparison with Hyena, RWKV, and S4 models would be useful.\n- Reports only latency metrics; including FLOPs or energy-based analysis would make results more hardware-agnostic.\n- Hardware dependency unclear—speedups may vary across GPUs/TPUs.\n- Memory–latency trade-offs and cache behavior could be analyzed more deeply."}, "questions": {"value": "- How does FutureFill scale with model size and longer sequence lengths (e.g., >100K tokens)?\n- Are the latency gains consistent across different hardware backends?\n- Could the authors report FLOPs per token to complement latency results?\n- How does the method compare with other efficient convolutional architectures in total throughput and memory cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iUpNLUwHcJ", "forum": "t5GUEuIsxR", "replyto": "t5GUEuIsxR", "signatures": ["ICLR.cc/2026/Conference/Submission21996/Reviewer_SKar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21996/Reviewer_SKar"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104468009, "cdate": 1762104468009, "tmdate": 1762942011218, "mdate": 1762942011218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}