{"id": "cXo4vQudaa", "number": 5924, "cdate": 1757946635234, "mdate": 1759897944517, "content": {"title": "Enhancing Trustworthiness of Fine-Tuned LLMs via Regularized Subset Selection", "abstract": "Supervised fine-tuning (SFT) improves large language model (LLM) perplexity but can also degrade trustworthiness—leading to the generation of untruthful, biased, or unsafe content during user interactions. These issues are often traced back to specific phrases or patterns in the training data. However, correcting them usually requires expensive retraining or new data collection. In this work, we propose a two-stage, compute-efficient repair of the post-SFT models that enhances trustworthiness while preserving the downstream performance. In the first stage, we identify the training samples responsible for failures on trustworthiness metrics like truthfulness, stereotypical bias, and machine ethics—and select a small, diverse subset of these examples using a determinantal point process (DPP)-based regularization. In the second stage, we repair the model under the framework of proximal Bregman response function (PBRF) using a gradient ascent update, which enhances trustworthiness while preserving downstream task performance (perplexity). We evaluate our method on multiple LLMs of varying sizes and demonstrate up to 21\\% improvement in trustworthiness metrics with minimal impact ($\\leq1$ %) on perplexity. Our method provides a computationally efficient approach to enhance post-SFT models and offers a practical alternative to hours of retraining required for model repair", "tldr": "", "keywords": ["LLM", "Trustworthiness", "Subset Selection", "Submodularity", "Data Attribution"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2209efbbdeddac949b6989a4a7ed22f703ce8744.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a two-stage, compute-efficient method for repairing post-SFT (supervised fine-tuned) large language models (LLMs). The approach first identifies a representative subset of detrimental training samples and then enhances model trustworthiness through a gradient ascent update. Experiments demonstrate that the proposed method improves model trustworthiness with minimal negative impact on downstream performance, as measured by perplexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well-organized, making the proposed method easy to follow.\n2. Extensive experiments show that the approach effectively improves trustworthiness without substantially degrading the model’s general capabilities.\n3. The use of gradient ascent for targeted unlearning provides a cost-efficient alternative to retraining-based methods."}, "weaknesses": {"value": "1. According to the authors, the optimization procedure is adapted from [1], and the diverse subset selection is based on [2]. However, it remains unclear how these methods are specifically modified or integrated to address the LLM repair problem. Without clearer differentiation from prior work, the contribution risks appearing incremental.\n2. The evaluation focuses primarily on metrics that are directly related to loss functions (e.g., log-odds and perplexity). To more convincingly demonstrate that general model capabilities are preserved, the authors could include standard benchmark metrics such as MMLU or GSM8K accuracy.\n3. The proposed method assumes full access to the SFT dataset, which limits its applicability to real-world settings where proprietary fine-tuning data are unavailable or the LLM undergoes a further RL process. A discussion on how this method could be adapted or approximated in such cases would strengthen the paper's practical relevance.\n4. The proposed method, *when used reversely, may intensify the ethics problems of LLMs*.\n\nI'm willing to raise my score if the authors can further explain their contributions compared to previous works, e.g., how they modify existing methods to adapt to LLMs.\n\n> **References**\n>\n> [1] If influence functions are the answer, then what is the question?.\n>\n> [2] Studying large language model generalization with influence functions."}, "questions": {"value": "1. What would happen if the proposed repair process were applied iteratively or multiple times? Would the increases in perplexity accumulate to the point where retraining becomes necessary?\n2. Could the authors provide more details on the computational cost of each stage, particularly the time spent on identifying detrimental points and selecting diverse subsets? As SFT datasets continue to grow, these steps might dominate the total computation time.\n3. How feasible is the proposed method for models whose SFT data are not available? For open-source LLMs that have undergone extensive safety alignment, could the method still yield measurable improvements in trustworthiness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EoZG4K95bF", "forum": "cXo4vQudaa", "replyto": "cXo4vQudaa", "signatures": ["ICLR.cc/2026/Conference/Submission5924/Reviewer_Fnne"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5924/Reviewer_Fnne"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760794579327, "cdate": 1760794579327, "tmdate": 1762918354638, "mdate": 1762918354638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a post-SFT repair method to improve LLM trustworthiness (truthfulness, ethics, bias) without retraining from scratch, while maintaining downstream performance. The approach identifies training examples that most degrade trust metrics and selects a small, diverse subset using influence-based scores and a DPP regularizer. Then, a proximal Bregman response function–based gradient-ascent update increases the loss on these samples while approximately preserving performance.\n\nThe method uses a differentiable log-odds trustworthiness surrogate and EK-FAC for scalable inverse-Hessian approximations. Experiments on Pythia and Qwen models show consistent improvements in trust metrics (up to ~20 percent) with minimal perplexity increase (<2 percent), outperforming simple gradient-ascent baselines and competing favorably with DPO."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a principled way to post-hoc improve a SFT model’s performance on trustworthiness metrics while maintaining performance on the downstream task. \n- The experiments are thorough with six models (Pythia & Qwen families), three trust aspects (truthfulness, stereotypical bias, machine ethics). Comparisons to other ‘repair schemes’ SGA/GA/GA+KL show strong performance in terms of improving trustworthiness with preserving perplexity.\n- Well-designed ablations (e.g Section 4.6) to present the direct effects of different choices (e.g. DPP)."}, "weaknesses": {"value": "- The paper claims that both terms in Eq. (6) are submodular, enabling a greedy approximation, but it does not prove or cite the \nsubmodularity of the attribution term, log sum_j gamma^j .\n\n- DPO is compared on perplexity but not on the same trust metrics; a fuller comparison (same compute budget) would clarify trade-offs.\n\n- The evaluations aren’t based on model generations but on log probabilities on fixed data sets.  This weakness is acknowledged by the authors. Instead of only considering perplexity (which aligns with the loss used in the SFT phase), further downstream tasks could be considered for a more whole analysis."}, "questions": {"value": "Could you please follow up on the submodularity result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "msrUy43nbZ", "forum": "cXo4vQudaa", "replyto": "cXo4vQudaa", "signatures": ["ICLR.cc/2026/Conference/Submission5924/Reviewer_YCTW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5924/Reviewer_YCTW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866374791, "cdate": 1761866374791, "tmdate": 1762918354327, "mdate": 1762918354327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new method for increasing perplexity while not degrading perplexity too much. The authors present it as a two-stage process: identify subset of training points that harm trustworthiness, then use proximal Bregman response function."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is well written and clear, tackling the problem of improving trustworthiness in large language models\n- Paper contains empirical results from 6 models across 3 tasks."}, "weaknesses": {"value": "- Authors don't consider other metrics like privacy and robustness (adversarial, OOD)\n- No other trustworthiness methods as baselines in the main paper"}, "questions": {"value": "- Why were the specific trustworthiness metrics chosen?\n- In Table 2, why does truthfulness decrease with a common subset?\n- Could the authors provide runtimes for their method and also measure for existing methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VkGZY8Yvyp", "forum": "cXo4vQudaa", "replyto": "cXo4vQudaa", "signatures": ["ICLR.cc/2026/Conference/Submission5924/Reviewer_fr2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5924/Reviewer_fr2q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898200380, "cdate": 1761898200380, "tmdate": 1762918353979, "mdate": 1762918353979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an efficient and lightweight two-stage repair method to enhance the trustworthiness of fine-tuned models without sacrificing downstream task performance. The method first identifies detrimental samples in the training data that cause a decline in trustworthiness using data attribution techniques. Subsequently, it performs gradient ascent on this subset within the Proximal Bregman Response Function (PBRF) framework to precisely \"unlearn\" the negative influence of these samples. The PBRF framework ensures that the model parameters do not deviate excessively from the original model, thereby preserving its core capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Important and Practical Problem: The issue of Supervised Fine-Tuning (SFT) undermining model trustworthiness is a common pain point in applying LLMs to real-world scenarios, especially in user-facing applications. The proposed lightweight repair method is more cost-effective than traditional retraining or RLHF, making it highly practical.\nSolid and Novel Methodology: The work skillfully integrates advanced techniques from multiple fields. It combines data attribution (approximated by EK-FAC), diversity-based subset selection (DPP), and constrained model updating (gradient ascent under the PBRF framework) to form a logically coherent and technically sound solution.\nComprehensive and Convincing Experimental Design: The experiments cover models of different architectures (Pythia, Qwen) and sizes, and utilize standard trustworthiness benchmarks (e.g., TruthfulQA, DecodingTrust)."}, "weaknesses": {"value": "The experiments (Table 2) indicate that using a \"common subset\" to simultaneously improve all trustworthiness dimensions is less effective than targeted repair for each dimension individually. In practice, this implies that users may need to run the repair process separately for each dimension of concern (e.g., bias, safety), which increases operational complexity.\nStrong Dependence on Paired Evaluation Data: The method's core relies on high-quality, paired trustworthiness evaluation data (proponent/opponent) to compute attribution signals and optimization objectives. In some vertical domains (e.g., healthcare, law, financial compliance), constructing such datasets is an expensive and time-consuming task. Furthermore, the definition and quantification of \"untrustworthiness\" are often contentious and context-dependent in open-ended tasks, which can lead to unstable metrics and insufficient coverage."}, "questions": {"value": "The paper is presented with exceptional clarity and is very thorough, leaving me with no major questions. The authors have done a commendable job of anticipating potential queries and addressing them proactively through their detailed methodology and comprehensive experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5J6Pk38Sz7", "forum": "cXo4vQudaa", "replyto": "cXo4vQudaa", "signatures": ["ICLR.cc/2026/Conference/Submission5924/Reviewer_PmHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5924/Reviewer_PmHy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007360585, "cdate": 1762007360585, "tmdate": 1762918353698, "mdate": 1762918353698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}