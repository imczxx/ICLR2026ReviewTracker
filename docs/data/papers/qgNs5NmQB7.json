{"id": "qgNs5NmQB7", "number": 16889, "cdate": 1758269981188, "mdate": 1763749985230, "content": {"title": "TangoFlux: Text to Audio Generation with CLAP-Ranked Preference Optimization", "abstract": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in 3.7 seconds on a A40 GPU. A key challenge in aligning TTA models lies in creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference data to enhance TTA alignment. We show that the audio preference dataset generated using CRPO outperforms the static alternatives. With this framework, TangoFlux achieves state-of-the-art performance across both objective and subjective benchmarks. https://tangoflux56.github.io/TangoFlux/ holds the model-generated audio samples for comparison.", "tldr": "Text to audio generation with semi online preference optimization performed on self-generated audio samples", "keywords": ["text to audio", "flow matching", "preference optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0535af5833cc7f3b56c4da914bf0f9aea08f9fd.pdf", "supplementary_material": "/attachment/aec0fdd7092c1b7662751881c104952df41477f8.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces TangoFlux, a TTA model trained on open data and aligned via CLAP-Ranked Preference Optimization (CRPO). CRPO performs an online, self-improving preference-optimization loop that ranks multiple candidates per prompt with CLAP and optimizes rectified-flow models accordingly."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Applying online data generation outperforms baselines that use static preference datasets.\n- The system can generate up to 30 seconds of audio at 44.1 kHz, an uncommon setting, as most text-to-audio models produce ~10 seconds at 16 kHz."}, "weaknesses": {"value": "Lacking metrics of FD using PANNs. This metric is the most common metric used by TTA models for evaluating fidelity. (The paper only reports FD using OpenL3)\n\nThe evaluation section is missing comparisons to several publicly available state-of-the-art Text-to-Audio models. Specifically, an updated comparison should ideally include results for:\n- Make-An-Audio 2 [A]\n- EzAudio [B]\n- ConsistencyTTA [C]\n- AudioLCM [D]\n\nParticularly, ConsistencyTTA and AudioLCM also target on fast inference with consistency models.\n\n[A] Huang, Jiawei, et al. \"Make-an-audio 2: Temporal-enhanced text-to-audio generation.\" arXiv preprint arXiv:2305.18474 (2023).\n\n[B] Hai, Jiarui, et al. \"Ezaudio: Enhancing text-to-audio generation with efficient diffusion transformer.\" arXiv preprint arXiv:2409.10819 (2024).\n\n[C] Bai, Y., Dang, T., Tran, D., Koishida, K., Sojoudi, S. (2024) ConsistencyTTA: Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation. Interspeech 2024\n\n[D] Liu, Huadai, et al. \"AudioLCM: Text-to-Audio Generation with Latent Consistency Models.\" CoRR (2024).\n\nFor variable duration, I think it is worth mentioning that the model always operates on a fixed-length latent space, but the duration conditioning explicitly controls how much of that latent space has actual content other than silence. Make sure readers know that this work is not an autoregressive model that can generate an arbitrary length of latent sequence.\n\nLacking references:\n\nThe Related Works section could be strengthened by citing influential but non-public models, such as AudioTurbo [E] and IMPACT [F], to provide a comprehensive overview of the field, similar to how ETTA and Audiobox are referenced. While direct performance comparisons to proprietary models are understandably impossible, acknowledging their existence in the literature is a key academic practice (This will not affect the overall ratings for this work, but it is nice to have).\n\n[E] Zhao, Junqi, et al. \"AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion.\" arXiv preprint arXiv:2505.22106 (2025).\n[F] Huang, Kuan-Po, et al. \"IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio Generation with Diffusion Modeling.\" ICML 2025"}, "questions": {"value": "The batch size used for measuring inference time is not explicitly stated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "igSoVzzy7X", "forum": "qgNs5NmQB7", "replyto": "qgNs5NmQB7", "signatures": ["ICLR.cc/2026/Conference/Submission16889/Reviewer_t9Xr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16889/Reviewer_t9Xr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600841066, "cdate": 1760600841066, "tmdate": 1762926923240, "mdate": 1762926923240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a state-of-the-art in-the-wild audio generation model called TangoFlux. The innovations are two-fold:\n- A flow-matching model based on FluxTransformer and MMDiT that can generate high-quality audio with variable duration.\n- A fine-tuning method called CRPO that alternates between generating preference datasets and using DPO to learn from that dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I found this paper nice and interesting to read, and I learned a lot from it.\n\nI really like the ablation study between CRPO loss and DPO loss (Figure 3) as well as other settings (Table 3). Super intuitive and informative."}, "weaknesses": {"value": "The paper is already in good overall shape. That said, there are some remaining weaknesses:\n- Human evaluation is performed with 50 human-created and GPT-4o-generated prompts (Section 3.4). This prompt bank seems somewhat small, especially because the 0.2486 OVL z-score of TangoFlux is moderate compared to existing work. Would it be possible to increase the scale of subjective evaluation? If not, some confidence interval or hypothesis testing analyses would greatly clarify the quality advantage of TangoFlux over existing work.\n\nI would also appreciate it if the authors could respond to my questions below."}, "questions": {"value": "- Which encoder did the KAD metric use? If it is the same encoder as the FD metric, did you find KAD better/more correlated with subjective quality than FD, or worse/less?\n- CRPO significantly improves subjective REL. Is there an analysis on what aspects of prompt following have been improved (i.e., temporal relationship of audio events, duration control, etc.)? Do you think conditioning effectiveness can also be improved via CFG?\n- TangoFlux allows for duration control. How accurate is this control? Does duration control affect generation quality? Would it be possible to present a scatter plot between the requested duration and the actual generated length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SNHeRDlmoI", "forum": "qgNs5NmQB7", "replyto": "qgNs5NmQB7", "signatures": ["ICLR.cc/2026/Conference/Submission16889/Reviewer_ddb1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16889/Reviewer_ddb1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530691758, "cdate": 1761530691758, "tmdate": 1762926922733, "mdate": 1762926922733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study introduces an iterative alignment framework named CRPO to address the challenges Text-to-Audio (TTA) models face in adhering to complex prompts. The method employs a self-improvement loop, leveraging the CLAP model as a proxy reward model to automatically generate and curate preference data. Furthermore, the authors enhance the traditional Direct Preference Optimization (DPO) loss function by incorporating a regularization term to stabilize the training process. Ultimately, they train and open-source a highly efficient and high-performance TTA model called TANGOFLUX based on this method, which demonstrates state-of-the-art performance across multiple evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The most significant strength of this work is its delivery of a practical and scalable alignment solution for the TTA domain, which lacks off-the-shelf tools. It cleverly repurposes the CLAP model as an effective reward model, addressing the core bottleneck of preference data creation. Technically, the introduction of the LCRPO loss function demonstrates a deep understanding of the potential pitfalls in preference optimization and presents a robust solution. Finally, releasing an open-source, state-of-the-art model trained on public data not only advances the field but also provides an invaluable resource and benchmark for future research."}, "weaknesses": {"value": "Over-reliance on the Proxy Reward Model: The alignment process is inherently constrained by the capabilities of the CLAP model, as its potential biases and limitations as a proxy reward model could be amplified during iterative training.\n\nRisks of Self-Correction and Limited Generalization Testing: The self-improvement cycle risks reinforcing its own flaws, and the model's generalization is evaluated on a relatively small out-of-distribution dataset, which may not be fully conclusive.\n\nLimited Novelty: The proposed method is not fundamentally novel, as it primarily combines and applies existing, mature technologies to a new domain rather than introducing a new core algorithm.\n\nLack of Polish in Presentation: The manuscript contains minor typographical errors, such as a quotation mark issue on line 263, which may suggest a need for more thorough proofreading.\n\nThe referee acts as the athlete; CLAP RL; CLAP testing."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "E91yicsghq", "forum": "qgNs5NmQB7", "replyto": "qgNs5NmQB7", "signatures": ["ICLR.cc/2026/Conference/Submission16889/Reviewer_nhq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16889/Reviewer_nhq3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665719737, "cdate": 1761665719737, "tmdate": 1762926922282, "mdate": 1762926922282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes text-to-audio (TTA) alignment via DPO. The core idea is to curate a synthetic preference dataset using CLAP: after training an audio generator, the system samples N audios per prompt, selects a winner/loser by CLAP similarity, and forms triplets for DPO. The authors iterate this process and additionally propose keeping a flow-matching loss on winners alongside DPO (denoted ùêø_CRPO). A flux-like architecture is adopted as the backbone."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and readable paper.\n- The method is intuitive. the evaluation and metrics follows standard evaluation.\n- the authors promised to release code and model weights is valuable for the community."}, "weaknesses": {"value": "**Novelty:** the authors main contribution which is CLAP-driven automatic preference curation for DPO is closely related to Tango-2 and CLIP-DPO as mention in L155-L160, with the main differences being that Tango-2 and CLIP-DPO operates on a static dataset. Running the same loop for multiple iterations seems to me an incremental extension rather than a novel contribution. \n\n**Evaluation:** the base model is fine-tuned on AudioCaps (L833), which I believe would bias the results on AudioCaps and undermines comparisons to open-source baselines (Stable Audio Open, AudioLDM2). An out-of-distribution evaluation or reporting pre-finetuning numbers for all models would help understanding the effectiveness of the proposed method."}, "questions": {"value": "- in Figure 2, since CLAP is used to construct preferences, reporting FAD or IS would help understand more the gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RKWtGLwESY", "forum": "qgNs5NmQB7", "replyto": "qgNs5NmQB7", "signatures": ["ICLR.cc/2026/Conference/Submission16889/Reviewer_w4XM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16889/Reviewer_w4XM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989303733, "cdate": 1761989303733, "tmdate": 1762926921830, "mdate": 1762926921830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}