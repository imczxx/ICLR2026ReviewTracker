{"id": "1MX3QC0bSH", "number": 21980, "cdate": 1758324334300, "mdate": 1759896892828, "content": {"title": "ESSA: Evolutionary Strategies for Scalable Alignment", "abstract": "Alignment of large language models (LLMs) typically relies on reinforcement learning from human feedback (RLHF) with gradient-based optimizers such as PPO or GRPO. While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on low-rank LoRA adapters and further compresses their parameter space by optimizing only the singular values from an SVD decomposition of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead.", "tldr": "", "keywords": ["evolutionary strategies", "large language models", "alignment", "LoRA", "SVD", "reinforcement learning from human feedback"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b10500f2c18f2446a15c07a8ff9b3d33ee4c77e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed a gradient-free method ESSA for training LLMs. The main idea is to update the singular values of LoRA adapters via evolutionary optimization, which only requires forward pass and avoids gradient computation. The method is shown to be time-efficient and achieves better downstream performance compared to GRPO in the task of LLM finetuning on math and instruction following datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The method is time-efficient and computational-efficient, making it a potential approach for optimizing LLMs under the limited computational resource scenarios, e.g. edge devices.\n* The algorithm is relatively easy to implement and does not require gradient computation."}, "weaknesses": {"value": "* **The applicable scope of the method is limited.** \n  * ESSA is strongly coupled with the setting of LoRA-based RLHF on a well-initialized SFT model. \n  * If I understand the algorithm correctly, when the model cannot be fit into a single GPU, ESSA's speed will be severely affected since each GPU cannot run the evaluation independently.\n  * The acceleration strongly depends on the number of available GPUs. When the computational resource is limited, e.g. only a single GPU is available, the acceleration seems to be diminishing.\n\n* **The comparison between ESSA and GRPO seems unfair.** As mentioned in line 200-203, the ESSA is trained using INT4 while GRPO is trained using BFloat16; the later naturally requires more computational FLOPs and induces additional time. Have the authors compared two methods under the same computational data type?\n\n* **The paper is poorly written; key definitions are missing.** There is no mathematical formulation of the algorithm, making it hard to interpret the Figure 1. For instance, it is unclear what is \"layer n\", \"rank N\" and \"solution length\". The explicit statement of the algorithmic update (i.e. CMA-ES algorithm) is missing as well. I suggest the authors to present a formal algorithmic description for a clearer presentation."}, "questions": {"value": "* It seems that the evaluation data type is not aligned for ESSA and GRPO. According to Figure 7, the initial accuracy of both methods exhibit evident difference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OeNs0ihR3H", "forum": "1MX3QC0bSH", "replyto": "1MX3QC0bSH", "signatures": ["ICLR.cc/2026/Conference/Submission21980/Reviewer_ojT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21980/Reviewer_ojT5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760771753342, "cdate": 1760771753342, "tmdate": 1762942006180, "mdate": 1762942006180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ESSA, a gpu-light post-training method that evolves LoRA adapters instead of doing gradient-based RLHF. LoRA adapts large models by adding small low-rank matrices to selected layers and training only those adapters. Evolutionary algorithms (e.g., CMA-ES) are gradient-free searches that sample candidate parameters, score them with a reward, and update a population toward better candidates. ESSA SVD-decomposes the LoRA adapters, freezes the singular vectors, and evolves only the singular values using inference-only evaluations, enabling easy parallelism and INT8/INT4 training. ESSA shows strong scaling with more GPUs and often reaches target accuracy faster than GRPO; quantized training incurs small accuracy loss; tasks include GSM8K/advanced math, IFeval, and a HelpSteer setup."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**: T*he method has no original component, but manage to blend together known ideas in an interesting way.*  The core move is casting alignment as inference-only evolutionary search in a *compressed* SVD-LoRA subspace (optimizing only singular values). This  is a creative recombination of known ideas (LoRA, CMA-ES/ES) that manage to make post-training faster, as it removes the need to computate gradient, allowing the search to be done in INT8/INT4. This is a practical, underexplored angle, that allows consumer grade gpu to align bigger models.\n\n**Empirical Quality:** *TL;DR: the presented result are convincing, and this is good. But they are far to be enough*. \nEmpirically, the work delivers credible *systems* evidence: strong wall-clock scaling with GPU count, and clear time-to-quality wins over GRPO at high scale. Nevertheless, it would require more experiments to assert this is a definitive result *(see weakness section)*\n\n**Clarity:** part of the work are clear, such as the main ideas of the algorithm.\n\n**Significance:** If the goal is to make post-training alignment scalable and hardware-friendly, this approach is consequential: (i) it materially reduces inter-GPU communication, (ii) it tolerates training-time quantization with small accuracy loss, and (iii) it works across several tasks (math reasoning, instruction following, reward-model tuning) with broadly competitive accuracy."}, "weaknesses": {"value": "1. **Presentation quality.** The manuscript quality is low. It has numerous concrete issues that impede review and reproducibility. In particular, the paragraph “Task and Models” contains major grammar mistakes and punctuation; periods used instead of commas, fragments like “. GSM8K with accuracy as the primary metric.” where sentences do not have verbs. Other problems (but there are many other): \n    1. Figure problems: Figure 2 right-panel lower-left square has no numbers; Figure 2 left-panel has inconsistent digit colors; Figure 3 description contains a typo “!x!” artifact.\n    2. Table 1 lacks headers/column names/model identifiers, making it impossible to parse.\n    3. *Communication of clarity and novelty.* The paper does not crisply articulate *what is new* beyond “apply ES to SVD-LoRA after SFT.” As written, it reads like applying well-known algorithms (LoRA + CMA-ES/ES) to select singular directions post-SFT. The contribution needs to be isolated: specify the novel elements (e.g., singular-values-only parameterization, quantized-training recipe, communication scheme) and contrast against close alternatives.\n    4. *Method section organization and writing.* The “Method” subsection reads like an extension of Related Work: it re-introduces LoRA and CMA-ES, breaks narration into many short paragraphs, and uses inconsistent/broken formatting (examples include “1.Samples vs 2.  For each”). This hurts readability and makes the contribution hard to identify.\n2. **Theory section is unclear and contextless***.* The manuscript jumps into “latency of gradient-based online alignment” and a latency model without prior grounding. Key terms are not defined where needed (e.g., what an “all-reduce of model-size gradients” specifically entails in this setup? what is a strong base? what is a latency model?). The theoretical *contribution* is unclear and hard to grasp.\n3. **Baseline breadth and fairness.** Comparisons lean heavily on GRPO with thin hyperparameter tuning, while ESSA receives broader sweeps and is sometimes run with INT8/INT4 where GRPO remains BF16, an apples-to-oranges wall-clock comparison. Can GRPO be run in INT8? Stronger non-RL baselines (DPO https://arxiv.org/abs/2305.18290 /ORPO https://arxiv.org/abs/2403.07691/ SIMPO https://arxiv.org/abs/2405.14734) are missing; these are common industry references and could materially close gaps.\n4. **Evaluation design**. The study emphasizes time-to-accuracy but *does not account for tokens or total FLOPs;* without accurate budget accounting (with different budgets), “scalability” is only half-substantiated. \n5. **Missing Ablations:** Critical ablations are absent that isolate the dependence on SFT and show use how the method perform: (i) base model with no finetuning evaluation, (ii) ESSA without the extra SFT (e.g., start from a pretrained model), and (iii) SFT-only without ESSA on identical splits. Furthermore, it is not clear whether the cost of SFT is taken into account in the cost of ESSA as it is part of it\n6. **Datasets, scope, and external validity.** Most experiments are on mathematical-reasoning corpora (GSM8K, MATH variants), which are *not* alignment datasets; labeling outcomes as “alignment” overstates generality. One of the main 7B backbone is Qwen2.5-math, already math-specialized, so measured gains are entangled with the backbone’s pre-training; it remains unclear how well the method helps on non-specialized, less-aligned bases. The paper needs breadth: add practitioner-standard families (Gemma, Mistral) and non-math datasets to support claims of generality.\n\n------\nIn conclusion, I suggest rejecting ESSA because, despite the interesting idea, the paper’s presentation and novelty are unclear (broken writing, malformed figures/tables, theory without concrete claims) and the evaluation is insufficient and biased (narrow baselines, missing ablations/cost accounting, and limited, non-alignment datasets)."}, "questions": {"value": "- What, precisely, is the paper’s **novel contribution** beyond applying a standard ES (e.g., CMA-ES) to search singular values after as SFT?\n- can you please provide a **single end-to-end algorithm box** (inputs, outputs, steps, hyperparameters) that makes the narration linear and self-contained?\n- What is the formal **contribution** of the theory section? Is there a theorem/lemma that yields testable predictions (e.g., iteration time, scaling law, or sample complexity)?\n- what is meant by  **“all-reduce of model-size gradients”** explicitly?\n- can you provide a **closed-form expression** for ESSA’s per-iteration **communication cost**: bytes/GPU = f(population size, seeds, rewards, ranks, layers). \n- What is model latency?\n- What does “**strong base**” mean in “Unless stated otherwise, we start from strong base and SFT checkpoints”?  How does it differe from SFT? Do you fine tune additionally models like Qwen-Math for Essa?\n- You use mainly **math-reasoning datasets** (GSM8K, MATH500). In what sense are these **alignment** tasks? If the claim is “alignment,” where are safety/harms/behavioral alignment metrics?\n- Do you take into account SFT  in ESSA resource count?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TiBwtEzpZX", "forum": "1MX3QC0bSH", "replyto": "1MX3QC0bSH", "signatures": ["ICLR.cc/2026/Conference/Submission21980/Reviewer_DR9j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21980/Reviewer_DR9j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584623348, "cdate": 1761584623348, "tmdate": 1762942005907, "mdate": 1762942005907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ESSA is a gradient-free alignment framework that optimizes only the singular values of LoRA adapters after an SVD parameterization; the SVD bases are fixed from an SFT warm-start, and CMA-ES evolves the singular-value vector using *inference-only* evaluations (INT8/INT4 supported). The paper reports comparable or better quality than GRPO across math (GSM8K, PRM800K, advanced-math suites) and instruction following (IFEval), with markedly better scaling in GPU count and wall-clock time (e.g., Qwen2.5-32B on PRM800K) while communicating only seeds and scalar rewards."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Gradient-free *ES loop* on a compact search space (singular values only); inference-only and quantization-friendly (INT8/INT4) for large models.\n- Scaling results and latency model argue for superior parallel efficiency vs. GRPO; communication volume is seeds+rewards only.\n- Empirics: consistent time-to-quality gains over GRPO and competitive final accuracy across several backbones and tasks; ablations on population size, LoRA rank, and fraction of trainable singular values (α)."}, "weaknesses": {"value": "1. Initialization clarity: The method relies on an **SFT stage** to initialize LoRA factors $A, B$, but it is not fully explicit whether a **single** SFT LoRA per (task, backbone) is used or **multiple** LoRA initializations are required across experiments. Greater specificity would aid reproducibility.\n2. **Scope of “gradient-free”**: The optimization loop is gradient-free, but the pipeline **depends on SFT**; the paper acknowledges warm-start sensitivity (accuracy improves with more SFT data), so the claim should be framed as “gradient-free **post-SFT** optimization.”\n3. **Timing accounting**: In wall-clock comparisons (e.g., Figures 3-8), it remains ambiguous whether the reported times **exclude** the SFT warm-start and with what cost.\n4. **Baselines**: The primary online baseline is **GRPO**; a rationale is given, but readers may expect positioning relative to **DPO/ORPO/SIMPO** beyond the Related Work discussion."}, "questions": {"value": "1. **SFT initialization**: Please detail the **number and scope** of SFT initializations used. Is there **one LoRA** (per backbone, per task) that is SVD-parametrized, or are **multiple** SFT LoRA adapters prepared? Include dataset size, epochs, and optimizer settings for SFT to enable reproduction.\n    \n2. **Timing inclusions**: Do the wall-clock curves in **Figures 3-8** **include or exclude** the SFT warm-start? If excluded, please report the SFT cost (minutes/GPUs) and **time-to-quality including SFT** to fully reflect end-to-end alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z9bkBNGF3C", "forum": "1MX3QC0bSH", "replyto": "1MX3QC0bSH", "signatures": ["ICLR.cc/2026/Conference/Submission21980/Reviewer_7J5i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21980/Reviewer_7J5i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676693625, "cdate": 1761676693625, "tmdate": 1762942005722, "mdate": 1762942005722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ESSA is a gradient-free alignment framework that optimizes only the singular values of LoRA adapters after an SVD parameterization; the SVD bases are fixed from an SFT warm-start, and CMA-ES evolves the singular-value vector using *inference-only* evaluations (INT8/INT4 supported). The paper reports comparable or better quality than GRPO across math (GSM8K, PRM800K, advanced-math suites) and instruction following (IFEval), with markedly better scaling in GPU count and wall-clock time (e.g., Qwen2.5-32B on PRM800K) while communicating only seeds and scalar rewards."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Gradient-free *ES loop* on a compact search space (singular values only); inference-only and quantization-friendly (INT8/INT4) for large models.\n- Scaling results and latency model argue for superior parallel efficiency vs. GRPO; communication volume is seeds+rewards only.\n- Empirics: consistent time-to-quality gains over GRPO and competitive final accuracy across several backbones and tasks; ablations on population size, LoRA rank, and fraction of trainable singular values (α)."}, "weaknesses": {"value": "1. Initialization clarity: The method relies on an **SFT stage** to initialize LoRA factors $A, B$, but it is not fully explicit whether a **single** SFT LoRA per (task, backbone) is used or **multiple** LoRA initializations are required across experiments. Greater specificity would aid reproducibility.\n2. **Scope of “gradient-free”**: The optimization loop is gradient-free, but the pipeline **depends on SFT**; the paper acknowledges warm-start sensitivity (accuracy improves with more SFT data), so the claim should be framed as “gradient-free **post-SFT** optimization.”\n3. **Timing accounting**: In wall-clock comparisons (e.g., Figures 3-8), it remains ambiguous whether the reported times **exclude** the SFT warm-start and with what cost.\n4. **Baselines**: The primary online baseline is **GRPO**; a rationale is given, but readers may expect positioning relative to **DPO/ORPO/SIMPO** beyond the Related Work discussion."}, "questions": {"value": "1. **SFT initialization**: Please detail the **number and scope** of SFT initializations used. Is there **one LoRA** (per backbone, per task) that is SVD-parametrized, or are **multiple** SFT LoRA adapters prepared? Include dataset size, epochs, and optimizer settings for SFT to enable reproduction.\n    \n2. **Timing inclusions**: Do the wall-clock curves in **Figures 3-8** **include or exclude** the SFT warm-start? If excluded, please report the SFT cost (minutes/GPUs) and **time-to-quality including SFT** to fully reflect end-to-end alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z9bkBNGF3C", "forum": "1MX3QC0bSH", "replyto": "1MX3QC0bSH", "signatures": ["ICLR.cc/2026/Conference/Submission21980/Reviewer_7J5i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21980/Reviewer_7J5i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676693625, "cdate": 1761676693625, "tmdate": 1763572772516, "mdate": 1763572772516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ESSA (Evolutionary Strategies for Scalable Alignment), a gradient-free framework for aligning large language models that uses evolutionary strategies to optimize low-rank LoRA adapters. The key innovation is decomposing LoRA matrices via SVD and optimizing only the singular values using CMA-ES, enabling inference-only training in quantized modes. The authors evaluate ESSA on mathematical reasoning, instruction following, and general assistant tasks, comparing against GRPO baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novel Technical Approach. The LoRA + SVD + inference-only training is quite novel and interesting. Besides, the setting is quite realistic given the fact that the model is becoming larger and larger.\n* Strong empirical results across different tasks such as math, instruction following and general-purpose assistants.\n* Efficiency. Authors demonstrate faster inference and quantization compatibility."}, "weaknesses": {"value": "I don't think the paper has major flaws. I am curious why SVD-GRPO performs much worse. Does that mean SVD and ES are coupled?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DVFh278dI6", "forum": "1MX3QC0bSH", "replyto": "1MX3QC0bSH", "signatures": ["ICLR.cc/2026/Conference/Submission21980/Reviewer_5kYU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21980/Reviewer_5kYU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157375617, "cdate": 1762157375617, "tmdate": 1762942005526, "mdate": 1762942005526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}