{"id": "J5pqVzrwNo", "number": 17533, "cdate": 1758277257902, "mdate": 1763638186959, "content": {"title": "ReSaM: Representation-Level Safety Margin Alignment for Vision–Language Models", "abstract": "We study the problem of Pseudo-Benign Failures in vision--language models (VLMs): multimodal inputs that appear harmless but elicit dangerous or policy-violating responses. Our analysis shows that these failures arise from a representational misalignment: the model's internal embedding space exhibits a distributional gap between pseudo-benign inputs and unsafe inputs located in the refusal region, causing failures outside the safety margins of models. We introduce Representation-Level Safety Margin Alignment method (ReSAM), a lightweight representation-space alignment method that: (i) computes direction vectors separating refusal and non-refusal representations, (ii) quantifies refusal behavior by projecting embeddings of inputs onto this direction, and (iii) optimizes a safety-margin loss that pushes unsafe and pseudo-benign queries above a learned margin while pulling safe examples below it. ReSAM introduces a new paradigm for multimodal safety alignment: it requires no manual annotations, instead deriving supervisory signals directly from its own representation space. Despite this minimal supervision, ReSAM achieves a 68% improvement in safety over strong baselines, and remarkably, we further observe that incorporating only a handful of pseudo-benign queries (as few as five) during training suffices to raise safety to 94.6%. Beyond these empirical gains, our analysis reveals that safety gradients concentrate in a low-rank subspace, suggesting that multimodal safety is governed by an intrinsic structure that can be systematically identified and controlled.", "tldr": "ReSAM fine-tunes VLM embeddings to separate safe and unsafe/pseudo-benign inputs, achieving substantial safety gains without external labels.", "keywords": ["large multimodal model", "VLM", "Safety alignment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3691e27253ae18f8d5b7778b0dfb51b77db050fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses pseudo-benign failures in vision-language models, where harmless-looking inputs trigger unsafe responses. The authors propose ReSAM, a simple and effective representation-level alignment method that adjusts the safety margin in embedding space without manual annotation. Experiments show significant safety improvements over baselines, revealing that multimodal safety aligns with a low-rank structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well organized and clear\n2. The motivation and methodology are well aligned, both aiming to address the pseudo-benign failure problem.\n3. ReSAM performs well on challenging benchmarks such as MSSBench and SIUO, and mitigates pseudo-benign failures to a certain extent.\n4. ReSAM is data efficient, using only a small amount to fine-tune VLMs"}, "weaknesses": {"value": "1. Results in Table 3 and Figure 8 indicate that ReSAM may misclassify safe samples into unsafe categories, leading to the over-prudence phenomenon. Can the authors provide some results to clarify this point? e.g. evaluate models on the MSSBench safe subset, and report more general benchmark results\n\n2. Could the authors provide the safety performance of ReSAM on Qwen2.5-7B compared to other models? It would be valuable to see how much improvement ReSAM brings to more recent VLMs.\n\n3. Whether the ability obtained by ReSAM generalized to more complex input settings? e.g. MIS-hard [1]\n\n4. ReSAM is similar to some activation calibration methods, which align VLMs at inference time [2][3]. The authors should add some comparison and analysis in Related Work.\n\n[1] Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models\n\n[2] Understanding and Rectifying Safety Perception Distortion in VLMs\n\n[3] Unraveling and mitigating safety alignment degradation of vision-language models"}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "goQaVsfmY8", "forum": "J5pqVzrwNo", "replyto": "J5pqVzrwNo", "signatures": ["ICLR.cc/2026/Conference/Submission17533/Reviewer_NfWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17533/Reviewer_NfWa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807512660, "cdate": 1761807512660, "tmdate": 1762927408427, "mdate": 1762927408427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of having multimodal inputs that may appear harmless but they may elicit dangerous answers from LLMs. See Figure 1 for an example which does a good job summarizing the main idea behind the paper.\nThe paper brings an alignment method (called ReSAM) that essentially aligns the model to show a refusal behavior for this type of inputs"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The model shows a 68% boost in safety while preserving the model's general capabilities (i.e., still not refuse in safe cases, false positives). The results on all the public benchmarks in the paper are impressive.\n\nOnly requires a handful of examples to gear the model weights to handle pseudo-benign queries."}, "weaknesses": {"value": "Some papers like https://arxiv.org/pdf/2410.09047?, https://arxiv.org/abs/2501.18100, https://arxiv.org/abs/2504.15585, https://arxiv.org/abs/2502.14881 may merit a mention\n\nThe definition of pseudo-benign is very nuanced and difficult to distinguish. For example, the example of Figure 1could be understood as clearly not pseudo-benign depending on the context. The evaluation, mostly in Tables 1, 2 and 3 is not really assessing the corner cases between what is considered pseudo-benign and poorly toxic.\n\nFor the regression benchmarks in table 3, It would have been even better if you present results with datasets that are more recent or nuanced to the particular issue at hand of pseudo-benign queries. The datasets used are from 2024, and it is likely that the core models being evaluated have seen those during training. In other words, does this regression robustness generalize to unseen queries or even more nuanced examples ?"}, "questions": {"value": "Have you thought what happens in a multi-turn scenario where multiple prompt-answer pairs happen between the user and the model? Do you think this approach will still work when those turns are part of the history?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper is about safety alignment. As such, I think it is recommended to check for ethical issues in case there are concerns on the claims, examples used, etc. If the area chair thinks otherwise, please disregard this."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KN5BDgf2rQ", "forum": "J5pqVzrwNo", "replyto": "J5pqVzrwNo", "signatures": ["ICLR.cc/2026/Conference/Submission17533/Reviewer_q18m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17533/Reviewer_q18m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931927103, "cdate": 1761931927103, "tmdate": 1762927408012, "mdate": 1762927408012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReSAM, a representation-level safety alignment method for VLMs targeting “pseudo-benign failures.” It (1) extracts a refusal-vs-non-refusal direction at a selected internal layer, (2) projects query embeddings onto this direction to quantify refusal tendency, and (3) applies a projection-guided safety-margin loss that pushes unsafe/pseudo-benign queries toward refusal while pulling safe queries away. The approach is data-light (≤800 samples; even 5 pseudo-benign examples can yield large gains) and claims strong safety improvements with small utility drops; it also reports that safety gradients concentrate in a low-rank subspace."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, lightweight formulation (mean-difference direction + projection-based target + cosine loss).\n\n- Strong reported safety gains across multiple pseudo-benign and harmful-query benchmarks, with minimal drop on MMMU/LiveBench."}, "weaknesses": {"value": "- The “refusal”/“non-refusal” sets use a fixed refusal-phrase list to define regions and to score DSR. Risks: false positives/negatives, distribution-specific phrasing, and optimization that learns the list rather than “safety.” Please quantify sensitivity to this list and evaluate with human or rubric-based labels, not dependent on the same phrases.\n\n- General capability (MMMU/LiveBench) is not a proxy for helpfulness under safety pressure. Provide a targeted, benign-but-sensitive helpfulness suite (e.g., first-aid advice, non-self-harm mental-health support, legal/medical info requests) to show that alignment doesn’t collapse into blanket refusals.\n\n- The direction r is computed from the model’s own refusals; if the seed policy is biased or brittle, ReSAM may ratify it. It would help to show results when the seed refusal region is perturbed (e.g., alternative refusal detectors, noisy/manual seeds) and report stability. \n\n- Missing related works [1,2] that may affect novelty.\n\n[1] Zou, Xiaohan, et al. \"Understanding and Rectifying Safety Perception Distortion in VLMs.\" arXiv preprint arXiv:2502.13095 (2025).\n\n[2] Liu, Qin, et al. \"Unraveling and mitigating safety alignment degradation of vision-language models.\" arXiv preprint arXiv:2410.09047"}, "questions": {"value": "- How robust are results if you replace the refusal-phrase list with a learned refusal classifier or human labels?\n\n- Does r transfer across models without recomputation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FyQxXJaIsW", "forum": "J5pqVzrwNo", "replyto": "J5pqVzrwNo", "signatures": ["ICLR.cc/2026/Conference/Submission17533/Reviewer_hvB9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17533/Reviewer_hvB9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978788209, "cdate": 1761978788209, "tmdate": 1762927407492, "mdate": 1762927407492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Representation-Level Safety Margin Alignment method (ReSAM), a lightweight, robust, and data-efficient framework designed to improve the safety alignment of Vision-Language Models (VLMs). The core issue addressed in this work is the phenomenon of Pseudo-Benign Failures, in which multimodal inputs that appear harmless or innocuous can nonetheless trigger unsafe, harmful, or policy-violating outputs from VLMs. The authors attribute this problem to a fundamental representational misalignment in the model’s embedding space—specifically, that the internal representation of pseudo-benign inputs is not properly situated within the “refusal region,” where overtly unsafe inputs are typically placed. As a result, these pseudo-benign queries fail to be recognized as risky, leading to inappropriate model behaviors. To address this, ReSAM aims to reshape the model’s internal representation space by enforcing a more explicit and robust safety margin. By identifying a direction vector that separates safe from unsafe regions in the embedding space, the method enhances the model’s capacity to refuse harmful prompts while maintaining performance on benign ones. Overall, the paper presents a coherent and well-motivated approach to improving safety in multimodal alignment. However, one concern is that the idea of using direction vectors in the representation space for safety alignment has been explored in previous literature, and the novelty of this contribution may require further clarification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper introduces a lightweight and data-efficient framework that aims to improve the safety of Vision-Language Models without the need for extensive retraining or external datasets. The central innovation lies in identifying a safety-margin direction that effectively separates the representation space into refusal and non-refusal regions. Because the approach relies solely on query-based representations, it can enhance model safety in a self-contained and resource-efficient manner, avoiding dependence on additional data or heavy computational overhead.\n\n2.A safety margin loss is used to push unsafe queries above a threshold and pulling the safety queries below it.\n\n3.The authors demonstrate that ReSAM achieves consistent and effective results across multiple benchmark datasets, showing notable improvements.\n\n4.A new finding is the multimodal safety is concentrated within a low-rank intrinsic subspace."}, "weaknesses": {"value": "1.The definition of pseudo-benign failures—inputs that appear harmless but elicit unsafe or policy-violating responses—is clearly articulated. The dataset from Zhou et al. (2024a) indeed exemplifies this phenomenon, as shown in Figure 1. However, the other datasets used for evaluation, such as VLGuard and MSSBench, primarily consist of clearly harmful inputs, including explicit unsafe or malicious instructions. These datasets do not fully correspond to the “pseudo-benign” category that the paper aims to analyze and mitigate. Consequently, it becomes less clear whether the proposed ReSAM method specifically addresses pseudo-benign failures or simply improves general refusal accuracy on harmful prompts. The authors could enhance the paper by clarifying this distinction and possibly conducting additional experiments on datasets more representative of pseudo-benign cases.\n\n2.In terms of novelty, the proposed approach bears strong conceptual resemblance to prior works that also explore direction-based safety alignment or embedding-space calibration. I found some papers: \n\n- Coca: Regaining Safety-Awareness of Multimodal Large Language Models with Constitutional Calibration, \n\n- InferAligner: Inference-Time Alignment for Harmlessness through Cross-Modal Guidance, \n\n- Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models. \n\nThese studies similarly employ representational manipulation or alignment strategies to separate safe and unsafe regions in the latent space. As such, while ReSAM is methodologically elegant, its contribution may appear incremental unless the authors provide a more detailed comparison and emphasize what distinguishes their method from these existing approaches, either in terms of efficiency, interpretability, or robustness.\n\n3.A conceptual concern arises regarding the selection of the direction vector. In Section 3.1, the paper defines this direction as extending from safe queries to unsafe queries. However, the abstract and introduction emphasize that pseudo-benign failures stem from the gap between pseudo-benign and unsafe inputs. This raises an important question about the rationale behind the chosen direction: why is the direction derived between safe and unsafe queries, rather than between pseudo-benign and unsafe ones, or between pseudo-benign and safe inputs. A deeper justification or empirical comparison of different direction formulations would make the contribution more convincing."}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HrDvPPMMxN", "forum": "J5pqVzrwNo", "replyto": "J5pqVzrwNo", "signatures": ["ICLR.cc/2026/Conference/Submission17533/Reviewer_udaZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17533/Reviewer_udaZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985049652, "cdate": 1761985049652, "tmdate": 1762927407127, "mdate": 1762927407127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all Reviewers"}, "comment": {"value": "We thank all the reviewers for their insightful comments and helpful suggestions. We hope our responses and paper updates alleviate the concerns raised.\n\nFollowing the reviewer's feedback, we update ReSAM mainly with the following context:\n\n- **Safety Pressure Testing on Benign-but-Sensitive Data (Section 4.2)**: In response to **Reviewer UdaZ, HvB9, and NfWa**, we test ReSAM on benign-but-sensitive datasets to assess over-rejection of assistance requests. The results show that ReSAM maintains a fine-grained safety margin with good performance.\n\n- **Data Source and Robustness Testing for the r Module (Section 4.6)**: Following feedback from **Reviewer UdaZ and HvB9**, we test ReSAM’s robustness with varying data sources and Gaussian noise perturbations. The results confirm that $\\mathbf{r}$ remains a strong supervision signal, even under perturbations.\n\n- **Method Comparisons and Latest Results (Section 2.2 and Appendix)**: In response to **Reviewer q18m and NfWa**, we provide a detailed method comparison in Section 2.2 and update the quantitative results in Appendix 4.2 and 7, showcasing ReSAM’s strong generalization across new methods, datasets, and metrics.\n\nWe highlight the corresponding modifications in the manuscript with color blue for your convenience. Again, thank you for your hard work. We believe your feedback has significantly improved the paper, and we look forward to further engaging with you during the discussion. Please find our responses to each of you below."}}, "id": "0KJ5ITiwJd", "forum": "J5pqVzrwNo", "replyto": "J5pqVzrwNo", "signatures": ["ICLR.cc/2026/Conference/Submission17533/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17533/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission17533/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763647752102, "cdate": 1763647752102, "tmdate": 1763647752102, "mdate": 1763647752102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}