{"id": "sKMgGQQy7g", "number": 6601, "cdate": 1757990220143, "mdate": 1759897905750, "content": {"title": "SesaHand: Enhancing 3D Hand Reconstruction via Controllable Generation with Semantic and Structural Alignment", "abstract": "Recent studies on 3D hand reconstruction have demonstrated the effectiveness of synthetic training data to improve estimation performance. However, most methods rely on game engines to synthesize hand images, which often lack diversity in textures and environments, and fail to include crucial components like arms or interacting objects. Generative models are promising alternatives to generate diverse hand images, but still suffer from misalignment issues. In this paper, we present SesaHand, which enhances controllable hand image generation from both semantic and structural alignment perspectives for 3D hand reconstruction. Specifically, for semantic alignment, we propose a pipeline with Chain-of-Thought inference to extract human behavior semantics from image captions generated by the Vision-Language Model. This semantics suppresses human-irrelevant environmental details and ensures sufficient human-centric contexts for hand image generation. For structural alignment, we introduce hierarchical structural fusion to integrate structural information with different granularity for feature refinement to better align the hand and the overall human body in generated images. We further propose a hand structure attention enhancement method to efficiently enhance the model's attention on hand regions. Experiments demonstrate that our method not only outperforms prior work in generation performance but also improves 3D hand reconstruction with the generated hand images.", "tldr": "", "keywords": ["Controllable Hand Image Generation", "3D Hand Reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2194e419d0f61eb875a835928b50600c45a494ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a mesh-controlled hand-image generation model, trained by a well-prepared dataset extracted through Chain-of-Thought (COT), and enhanced by multi-scale structural information and text-guided hand structure attention. Experiments demonstrate its superior performance in generation quality and improvement to the 3D hand reconstruction task, compared to SOTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is in a good writing style.\n2. The algorithm design, including image semantics extraction and two structural alignment modules, is reasonable. They are suited to the hand-image generation task. \n3. The results show promising performance in both generation quality and benefiting hand reconstruction."}, "weaknesses": {"value": "### Major Weakness\n- The metrics reported in Table 1 are not complete compared to AttentionHand [1], including MSE-2D, MSE-3D, and user preference.\n- In Table 2, HandBooster [2] can be compared, as it also uses mesh-controlled generated images to enhance 3D hand reconstruction.\n- In evaluation, what's the text prompt input to the ControlNet/Stable Diffusion in SesaHand? Do they align with AttentionHand? This is important to ensure a fair comparison.\n- In Table 2, for the re-implemented results of InterWild, why does only **RRVE on HIC** differ so much from the official one (24.20 vs. 21.35)? Yet the other metrics are all the same. Not sure if this is correct or not.\n\n### Minor Weakness\n- In the section of related work, ``Hand Image Synthesis`` and ``Diffusion-Based Hand Image Generation`` can be merged into one paragraph, as they are closely related.\n\n\n[1] AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild, ECCV 2024.\n[2] HandBooster: Boosting 3d hand-mesh reconstruction by conditional synthesis and sampling of hand-object interactions, CVPR 2024."}, "questions": {"value": "Although this task uses hand mesh to control image generation, I'm still wondering, can the model generate hand images without any hand mesh, just guided by the text? Will the SesaHand still perform better than previous methods in this case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ahM02YT7S0", "forum": "sKMgGQQy7g", "replyto": "sKMgGQQy7g", "signatures": ["ICLR.cc/2026/Conference/Submission6601/Reviewer_CFW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6601/Reviewer_CFW1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930176273, "cdate": 1761930176273, "tmdate": 1762918927213, "mdate": 1762918927213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SesaHand, a framework for generating hand images conditioned on 3D hand parameters, achieving enhanced semantic and structural alignment in controllable hand image generation compared to existing methods. The synthesized images can be utilized for training 3D hand reconstruction models. The technical contributions are twofold. First, for semantic alignment, the framework employs chain-of-thought inference to generate effective human-centric image captions (used as inputs to the image diffusion model), thereby improving the semantic conditioning of the generated hand images. Second, for structural alignment, it introduces a hierarchical structural fusion mechanism to integrate hand structure information, along with a hand structure attention enhancement module. Experimental results demonstrate that the proposed method produces higher-quality hand images and yields better performance when used to train hand estimation models than the existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**(1) Good presentation quality**\n\nThe paper is well written overall, and the text is easy to follow.\n\n**(2) Good experimental results**\n\nThe proposed method achieves strong results in both (1) controlled hand image generation and (2) hand pose estimation (when training an estimation model on the generated hand images). However, I have a few questions regarding the comparison settings (see the weaknesses section below).\n\n**(3) Good analysis to justify model design choices**\n\nAlthough the proposed method is relatively simple and straightforward, it provides good analysis to justify the design choices. For example, I appreciate the prompt influence analysis used to validate the proposed text prompt extraction strategy."}, "weaknesses": {"value": "**(1) Questionable omission of FoundHand in quantitative comparisons**\n\nI believe FoundHand is one of the most relevant baseline works to this paper. Although it is discussed in the related work section and qualitative comparisons are provided in the supplementary material, I wonder why this baseline is omitted from the main quantitative experiments for both hand image generation and hand pose estimation. Since the code is publicly available, was there any specific reason why this comparison could not be performed?\n\n**(2) Questionable experimental setting**\n\nIn the experiments, when explaining the suboptimal KID performance of this method compared to AttentionHand, it is mentioned that “Although our KID score slightly underperforms AttentionHand, this may be due to the random sampling strategy in the KID calculation.” However, this statement raises concerns about the whole experimental rigor. In related fields (e.g., diffusion-based motion generation [1]), it is standard practice to run N = 10 experiments and report the mean metric values to mitigate the influence of such randomness. Have you also considered this?\n\n[1] Tevet et al., Human Motion Diffusion Model, ICLR 2023.\n\n**(3) Novelty**\n\nThe proposed modules are not highly novel, as they largely combine existing techniques for enhancing the controllability of diffusion-based generative models. However, I consider this limitation relatively minor."}, "questions": {"value": "Please refer to the weaknesses (1) and (2) above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sxfQc0lblF", "forum": "sKMgGQQy7g", "replyto": "sKMgGQQy7g", "signatures": ["ICLR.cc/2026/Conference/Submission6601/Reviewer_j2h5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6601/Reviewer_j2h5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954144013, "cdate": 1761954144013, "tmdate": 1762918926737, "mdate": 1762918926737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SesaHand, a controllable hand-image generation framework that aims to improve downstream 3D hand reconstruction by enforcing both semantic and structural alignment. On the semantic side, the method uses a CoT pipeline to distill human-behavior semantics from VLM captions. On the structural side, it aggregates multi-resolution self-attention maps via hierarchical fusion and adds a lightweight bias in cross-attention to emphasize hand regions. Built on ControlNet-conditioned diffusion with hand-mesh images, the model generates diverse, human-centric images and mitigates common synthetic-data issues such as “floating hands” and hand–scene misalignment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The CoT-driven semantic extraction and the hierarchical structural fusion are well motivated and technically coherent; together they target complementary failure modes (semantic drift vs. structural misalignment).\n2. The supplemental comparisons against commercial models indicate task-specific controllability and suggest that the proposed design adapts well to hand-centric scenarios where generic generators struggle."}, "weaknesses": {"value": "1. The perceptual quality of generated images appears limited: several qualitative examples show unnatural finger articulation and color artifacts. Relative to SOTA hand rendering methods [i-iii] (acknowledging different goals and toolchains), the realism gap remains noticeable.\n2. Section 4.2 does not provide sufficient detail for the reconstruction setup. The paper does not specify the number of synthetic images used, training schedule, the proportion of generated vs. original data, or whether models are trained from scratch or fine-tuned. These omissions hinder reproducibility and attribution of gains.\n3. Efficiency is not quantified. The paper does not report module-wise runtime (CoT captioning, attention fusion, diffusion sampling), end-to-end throughput, training/inference cost, or memory usage. Given that efficiency is a practical criterion for synthetic data generation, the absence of such measurements weakens the empirical case.\n4. The discussion of hand–object interaction synthesis is limited. The paper does not address how to generate more natural interaction images with plausible contact, occlusion handling, and object pose consistency—factors that matter for reconstruction in interactive scenes.\n\n[i] Chen, Zhaoxi, et al. \"URhand: Universal relightable hands.\" CVPR. 2024.\n\n[ii] Potamias, Rolandos Alexandros, et al. \"Handy: Towards a high fidelity 3d hand shape and appearance model.\" CVPR. 2023.\n\n[iii] Iwase, Shun, et al. \"Relightablehands: Efficient neural relighting of articulated hand models.\" CVPR. 2023."}, "questions": {"value": "The following questions are based on the weaknesses discussed above; please refer to that section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iAu9TXpWHq", "forum": "sKMgGQQy7g", "replyto": "sKMgGQQy7g", "signatures": ["ICLR.cc/2026/Conference/Submission6601/Reviewer_46de"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6601/Reviewer_46de"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994563468, "cdate": 1761994563468, "tmdate": 1762918924889, "mdate": 1762918924889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a pipeline to generate synthetic hand images to improve 3D hand reconstruction. It addresses:\n(1) Semantic alignment using COT to extract \"human behavior semantics\" for text prompts.\n(2) Structural alignment using hierarchical fusion for hand-body coherence and an attention bias to focus on hand regions. \nExperiments show that this generated data improves 3D hand reconstruction on in-the-wild datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem statement of improving hand image generation quality to improve hand estimation task is sound. Data scarcity and fidelity is a key problems and current SOTA image generation models still fail to generate good hands occasionally.\n2. This work shows using COT to improve the caption alignment of VLM and attention bias improves hand image generation compared with some prior works. Thereby, improving 3D hand reconstruction from synthetic images.\n3. Writing and presentation clarity are good, and the paper is well-organized."}, "weaknesses": {"value": "1. Though the paper focuses on improving semantic and hand structural alignment, objects in HOI alignment seem to be neglected; most examples only show simple cases of hands holding stuff. This work would be stronger if a more challenging and comprehensive study of HOI generation were demonstrated.\n2. The examples lack diversity in style and viewpoint perspectives. It's difficult to judge how robust the proposed method is.\n3. Comparison with the latest works, like FoundHand, is under investigation. See questions below."}, "questions": {"value": "1. In Fig. 7 w/ SE+SF+AE, the coffee-holding hand in the first example has a \"broken\" finger, and the small left hand in the second example has only 4 fingers. Despite all the improvement modules, the model still seems to struggle with generating plausible hand structures. What is the cause of this, and how do authors plan to address this next?\n2. Why not consider an explicit object condition, like object mask, for HOI alignment? How does the proposed method perform for more challenging HOIs, like fingering a guitar?\n3. FoundHand shows in their Domain Transfer experiment that finetuning HaMeR with 10k generated images from ReInterHand to EpicKitchen improves 3D reconstruction accuracy. How does SesaHand compare with this baseline?\n4. Most examples are shown in 3rd third-person view. The authors claim that their method can also generate an egocentric view, but Fig.10 only shows one sample of an egocentric view. Why not include a comprehensive experimental comparison on some egocentric hand data, e.g. HOI4D, ARCTIC, EpicKitchen, etc? Also, FoundHand shows many examples of egocentric generation in their results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ytdWQ337P8", "forum": "sKMgGQQy7g", "replyto": "sKMgGQQy7g", "signatures": ["ICLR.cc/2026/Conference/Submission6601/Reviewer_bUzN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6601/Reviewer_bUzN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140943597, "cdate": 1762140943597, "tmdate": 1762918924528, "mdate": 1762918924528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}