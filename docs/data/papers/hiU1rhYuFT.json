{"id": "hiU1rhYuFT", "number": 11116, "cdate": 1758189636936, "mdate": 1759897607587, "content": {"title": "OFS: Overlapped Feature Synthesis for Communication-Efficient Federated Learning", "abstract": "Federated learning (FL) is deemed as a promising privacy-preserving distributed learning paradigm for decentralized non-IID data. However, it inevitably introduces significant communication overhead caused by frequent gradient exchanges, limiting its scalability. To mitigate this issue, recent work proposes to use data distillation for communication costs reduction.\nYet, the existing approaches fail to fully exploit the distilled features, resulting in a suboptimal compression ratio. In this paper, we propose a novel method—Overlapped Feature Synthesis(OFS)—that enables global feature sharing during compression, enhancing both communication efficiency and model performance. Specifically, we introduce a global feature Sampler, which extracts several small feature maps from a large global feature map to enable parameter sharing. To balance global and personalized parameters, an offset coefficient and multiple sampling strategies are introduced to allow for a flexible trade-off between compression efficiency and model performance.Extensive experiments demonstrate that OFS achieves better convergence with a lower compression rate compared to competing methods. Compared to state-of-the-art data distillation methods, our approach achieves an approximately 1\\% improvement in accuracy while maintaining a 10\\% higher compression rate. Moreover, we conduct ablation studies and visualizations to investigate the effects of the offset coefficient, the number of clients, and the number of local training epochs on the effectiveness of our method. Furthermore, we analyze the relationship between global and personalized model parameters.", "tldr": "", "keywords": ["communication-efficient federated learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fc2b964d4c9e918bbcb71e141e1acbc2b9b7b2f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel compression method for federated learning that addresses redundancy in existing data distillation approaches, specifically 3SFC (Single-Step Synthetic Feature Compression). The core innovation is an \"overlapping mechanism\" where synthetic datasets are decomposed into globally shared and locally personalized components. By introducing parameters (α,β,n₁,n₂) that control how feature maps overlap spatially, OFS enables parameter sharing across multiple synthetic images while maintaining personalized features. Experiments on MNIST, EMNIST, FMNIST, CIFAR-10, and CIFAR-100 show approximately 1% accuracy improvement over 3SFC at similar or higher compression rates."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "# 1. Originality: \n\nThe idea of spatially overlapping synthetic features to create shared global parameters appears to be new in the federated learning compression works. Also, Figure 1 provides a reasonable motivation by visualizing feature redundancy in 3SFC using Grad-CAM++.\n\n# 2. Quality:\n\nThe author validate their proposed method across 5 datasets and 5 model architectures with varying client numbers (10, 20, 40), and they also provide ablation studies: Table 3 (offset coefficient), Table 4 (communication budget and local iterations) with useful analysis. \n\n# 3. Clarity:\n\nThis paper provides a clear system diagram in Figure 2, a well-structed algorithm in Algorithm 1 for the proposed method.\n\n# 4. Significance:\n\nCommunication efficiency is a bottleneck in real-world federated learning, and if the overlapping idea works, it could potentially apply to other distillation methods."}, "weaknesses": {"value": "# 1. Originality\n\n1. The paper acknowledges building \"upon the traditional 3SFC method\" and the overlapping design is essentially a post-processing modification. The encoder/decoder architecture states \"we do not reiterate those details here\" - suggesting the core contribution is a wrapper around existing 3SFC rather than a fundamentally new compression paradigm.\n\n2. The overlapping mechanism in this work is  mathematically straightforward - it's essentially image cropping with stride. The relationship $B = C × (H × α × (n₁-1) + H) × (W × β × (n₂-1) + W) × n_{sample}$ is a basic (and obvious) geometric calculation. We invite the authors to clarify the algorithmic innovation beyond parameter tuning.\n\n3. The authors claim OFS is \"conceptually distinct\", but the paper doesn't clearly explain why standard overlapping/stride-based approaches in CNNs, or other compression methods wouldn't achieve similar effects. The claim that \"the overlapping structure is non-trivial to integrate into standard compressors like sparsification or quantization\" lacks justification.\n\n4. The related work section does not cover existing works in a few important aspects: (i) gradient compression methods that use structured sparsity or low-rank decomposition, (ii) recent federated learning papers that address feature redundancy through other means, and (iii) data distillation work beyond 3SFC that might have addressed similar issues.\n\n# 2. Quality: \n\n1. There are a few concerns regarding the experimental setups. The first one is the batch sizes vary significantly (512/256/128 for 10/20/40 clients). Justification is needed here for this specific scaling. This makes it harder to isolate the effect of OFS and. optimization dynamics. The second is for 100 epochs of training. It is questionable for convergence, as Figure 3 shows some curves still improving. The third one is the missed SEM/confidence interval values in the reported results. The fourth one is about the statement of \"The compression rates of DGC and 3SFC were matched to that of OFS\", which seems circular. We invite the authors to clarify how to ensure fair comparison when tuning baselines to match the proposed method.\n\n2. There are also concerns regarding the performance improvement. For MNIST/MLP: OFS (0.8827) barely improves over 3SFC (0.8798) at 125× compression. For CIFAR-100/ResNet (20 clients): 3SFC (0.0585) actually outperforms OFS (0.0649) – indeed the OFS is better, but the gains are inconsistent. For CIFAR-100/ResNet (10 clients): The improvement is only 0.0498 vs 0.0438, but both are terrible absolute accuracies (4-5%). The \"approximately 1% improvement\" claim in the abstract is misleading - from Table 1, many improvements are much smaller (e.g., 0.3-0.5%).\n\n3. Some important analyses are missing. There is no discussion of the computational overhead of the overlapping/decomposing operations. We invite the author to elaborate more on the merge operation and whether it would significantly increase latency. Also, it seems that the proposed method also introduces extra memory overhead, as creating overlapped images requires storing larger tensors before decomposition. In addition, we would like to invite the authors to illustrate on a theoretical analysis of why overlapping helps convergence or what the optimization landscape looks like.\n\n# 3. Clarity:\n\n1. There is quite a lot of confusion from the notation in the paper. For example, in Equations 12-13, the notation switches between $D^t_{syn,i}$ and ${D^t_{syn,i}}'$ but the relationship isn't immediately clear. In the proposed method, there are $n_{sample}$ images of size $C×(H×α×(n₁-1)+H)×(W×β×(n₂-1)+W)$ that decompose into $n_{sample}× n₁ × n₂$ images of size $C×H×W,$ but it is unclear where do these patches come from spatially. Also, in Equation 14, it is unclear what exactly are $D^t_{globe,i,x}$ and $D^t_{personalize,i,x}$ as there are no definitions for them. \n\n2. There are some key concepts that are never clearly stated. For \"Overlapping mechanism\", the paper never precisely defines what \"overlapping\" means. From context, it seems like sliding window extraction. Also, \"For instance, along the height dimension H, adjacent patches are overlapped by a ratio of α\". However, this is backwards. If α is the offset, then overlap ratio is (1-α), instead of α. It also seems that there is \"Global vs. personalized parameters\", which is introduced in Equation (14). But there's no clear explanation of how the decomposition enforces this semantic separation. We would like to invite the author to clarify this issue.\n\n3. There are some inconsistent terminologies in the manuscript. For example, \"synthetic dataset\" vs \"synthetic features\" vs \"compressed images\" are used interchangeably. As for \"communication cost\" vs \"communication budget\" vs \"compression ratio\", they are related but need clearer definitions.\n\n# 4. Significance:\n\n1. There are several issues that might significantly limit the real-world impact of this work. First of all, the improvements are modest (typically 0.5-1.5% over 3SFC) and come at the cost of additional hyperparameters (α,β,n₁,n₂). Also, Table 2 shows OFS at 1.5×B often underperforms 3SFC at 2×B, which indicates that the user even needs careful tuning to beat the baseline. \n\n2. There are some concerns about the scalability of this work. First of all, all experiments use relatively small models (ConvNet, ResNet, RegNet without batch norm/dropout), and there are no experiments on modern large-scale models (Vision Transformers, large ResNets). Also, there are no experiments on realistic federated learning scenarios (e.g., cross-device settings with hundreds/thousands of clients). In addition, the high compression ratios (125×, 666×, 1785×) seem unrealistic, which seems to be even usable in practice"}, "questions": {"value": "Answering all points stated in the Weaknesses would be greatly appreciated. In particular:\n\n1. We would like to invite the authors to clarify a few critical technical steps in the proposed method. First of all, how exactly does the \"segmentation\" work in Step 3 of Figure 2? In Algorithm 1 line 12, it needs to \"Merge D_(syn,i)^t' according to overlapping design\" but this is not clear. Also, it is unclear how to handle boundaries when n₁ × n₂ patches don't perfectly tile. In addition, it is unclear what happens to the \"shared\" parameters during gradient updates. If multiple local patches share global parameters, how is gradient aggregation handled? We would like to invite the authors to provide clarification on these issues. \n\n2. Regarding the theoretical understanding of the proposed method, we would like to invite the authors to provide clarification on the choice of spatial overlapping. The results in the current manuscript are purely empirical. Also, it would be better to include a comparison with techniques with potentially similar effects, such as simply averaging synthetic images, using a shared \"base image\" + personalized residuals, or applying PCA/SVD to synthetic features.\n\n3. As stated in weakness No.3 for Quality, we invite the authors to provide an analysis of the computational overhead of the overlapping/decomposing operations. To be specific, if the merge operation would significantly increase latency. For the memory overhead comparison, we invite the authors to also include an analysis for extra memory cost by “overlapped images”. \n\n4. As stated in weakness No. 2 for Significance, we invite the authors to provide an analysis on the scalability issue of this work. For example, we invite the authors to include some empirical analysis for large-scale models (Vision Transformers, large ResNets). Also, we invite the authors to include a justification for the choice of high compression ratios, and if it is reasonable for rela-world usage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bo5EBOroPH", "forum": "hiU1rhYuFT", "replyto": "hiU1rhYuFT", "signatures": ["ICLR.cc/2026/Conference/Submission11116/Reviewer_BLrZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11116/Reviewer_BLrZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760822431592, "cdate": 1760822431592, "tmdate": 1762922290698, "mdate": 1762922290698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the high communication cost of Federated Learning , focusing on data distillation methods. The authors identify a key limitation in existing methods like Single-Step Synthetic Feature Compression: the synthetic data generated for communication contains significant feature redundancy, leading to suboptimal compression. To solve this, the paper proposes Overlapped Feature Synthesis, a novel framework built upon 3SFC. The core contribution is a shared-parameter overlapping design. In this design, clients initialize a large synthetic feature map which is then decomposed into multiple smaller, overlapping patches. Gradients are updated on these patches, and due to the overlap, parameters in the shared regions are updated synchronously. This mechanism allows different synthetic samples to share parameters, effectively reducing redundancy and improving information utilization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The core idea of Overlapped Feature Synthesis is original, elegant, and well-motivated. It directly targets a clear weakness (feature redundancy) in the 3SFC baseline, and the proposed overlapping mechanism is a clever way to address it.\n\n2.The paper is backed by extensive and rigorous experiments. The consistent outperformance of OFS against all baseline categories (quantization, sparsification, and especially the SOTA 3SFC data distillation) is very convincing. The results in Table 2, which show OFS can maintain performance with a significantly smaller communication budget than 3SFC.\n\n3.The authors provide excellent ablation studies that analyze the key components of their method. The investigation of the offset coefficient $\\alpha$ (Table 3) provides valuable insight into the trade-off between shared and personalized parameters, showing that complex tasks benefit more from shared global features.\n\n4.The paper is very clear, with excellent figures (especially Figure 1 and 2) that build strong intuition for both the problem and the solution."}, "weaknesses": {"value": "1.The paper focuses exclusively on communication efficiency but completely omits any discussion of computational overhead. The proposed method, with its client-side initialization, segmentation , and merging , plus server-side decomposition, appears to add non-trivial computation compared to the simpler 3SFC. A wall-clock time comparison or analysis of the computational complexity is a critical missing piece for a paper on efficiency.\n\n2.Lacks theoretical convergence or error-compensation analysis quantifying how overlap affects bias/variance of reconstructed gradients and stability across rounds.\n\n3.Comparative scope on distillation baselines is narrow; results against FedSynth and other recent synthetic-data compressors beyond 3SFC would strengthen claims of general superiority.\n\n4.The method introduces several new hyperparameters: $\\alpha$ (offset H), $\\beta$ (offset W), $n_1$ (overlaps H), and $n_2$ (overlaps W). The paper provides an excellent ablation for $\\alpha$ but no analysis for $n_1, n_2$, or $\\beta$. How are these values chosen? The sensitivity to the number of overlapping patches ($n_1, n_2$) is unaddressed."}, "questions": {"value": "1.Could you please comment on the computational overhead of the proposed OFS method? How does the wall-clock time per round (for both client and server) compare to the 3SFC baseline?\n\n2.Hyperparameter Choice ($n_1, n_2, \\beta$): The ablation for the offset $\\alpha$ is very clear. Could you provide insight into how the number of overlapping images ($n_1, n_2$) and the width offset ($\\beta$) are chosen? How sensitive is the performance to these hyperparameters?\n\n3.Please clarify the \"Global synthetic feature\" shown in Figure 2. Is this feature map transmitted from the server to the client? If so, how is it used by the client in the Shared-Parameter overlapping design, and how does it relate to the client's own initialization of $D_{syn,i}^{t}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x65Wk8CFli", "forum": "hiU1rhYuFT", "replyto": "hiU1rhYuFT", "signatures": ["ICLR.cc/2026/Conference/Submission11116/Reviewer_4cDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11116/Reviewer_4cDx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663453561, "cdate": 1761663453561, "tmdate": 1762922289596, "mdate": 1762922289596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of the existing method, 3SFC, which is redundancy in the synthesized feature as a compression method to reduce the communication overhead of federated learning. The proposed method introduces OFS, which is an overlapping mechanism that structurally decomposes each synthetic sample into globally shared and locally personalized components to improve information utilization and reduce redundant communication. Extensive experiments under various models and datasets demonstrate the superiority of the proposed method in accuracy, even under constrained communication overhead. However, the score of this paper tends toward rejection due to: (1) the explanation of the core OFS algorithm is not well specified, missing important details that justify the contribution of this paper, and (2) the experimental claims do not fully support the stated contributions of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses the limitations of the existing 3FSC method through a structural modification. Extensive experiments across various models and datasets demonstrate that the proposed approach achieves superior accuracy while maintaining low communication overhead."}, "weaknesses": {"value": "(1) The explanation of the core OFS algorithm lacks critical details that justify the paper's contribution. To address this concern, please clarify and provide detailed explanations for the following:\n\n(1-a) The paper uses these terms(feature, dataset, and image) interchangeably without a clear definition: Fig. 1 shows \"Feature distribution\" suggesting feature maps, section 4 consistently refers to \"synthetic dataset $D^t_{syn,i}$\", the method name is \"Overlapped Feature Synthesis\", and Eq. 12 defines $D^t_{syn,i}$ with tensor dimensions. What exactly is being transmitted: feature maps extracted from intermediate layers or synthetic images in the input space? If these are features, from which layer are they extracted? If these are synthetic images in the input space, how are synthetic images generated and optimized?\n\n(1-b) Figure 2-2 (a) \"Original Image\" appears to be an H×W image used as the initialized synthetic dataset in the proposed method, whereas Eq. (12) defines $D^t_{syn,i}$ with dimension $C×(H×α×(n_1−1)+H)×(W×β×(n_2−1)+W)$ as the initialized synthetic dataset. Which one actually represents the \"initialized synthetic dataset\"? Please clarify the relationship between the visualization in Figure 2-2 a), b) and the mathematical formulation.\n\n(1-c) For a better understanding of the author's problem definition, motivation, and intuition of the method, the data distillation method needs to be explained, including more details: basic procedure to synthesize representation (e.g., how many synthesis data are generated, how to use the distillation, and why this method can compress the communication cost. Without a basic understanding of the data distillation method, it is hard to understand why the synthetic datasets generated often contain repetitive or overlapping information, which is the core problem of this paper. Also, regarding the explanation of Fig.1, it is hard to recognize which part of the image is classified as \"redundant\", since the images in Fig. 1(a), the upper image and the lower image, look different and do not have a redundant part.\n\n(2) The experimental claims do not fully support the contribution of the proposed method. To address this concern, please clarify and provide detailed explanations for the following:\n\n(2-a) The statement in the abstract section, \"~1% improvement in accuracy while maintaining a 10% higher compression rate\", is confusing because \"improvement...while maintaining higher compression\" suggests both benefits simultaneously, but Table 1 shows better accuracy at the same compression, and Table 2 shows maintained accuracy at higher compression.\n\n(2-b) The paper compares OFS against 3SFC, but it needs to provide the most obvious and necessary baseline. Since OFS decomposes n_sample images into $n_1×n_2×n_{sample}$ patches, a fair comparison would be to evaluate whether overlapping these patches works better than simply using more independent synthetic samples in 3SFC. Table 2 presents a comparison between OFS(2×B) and 3SFC(2×B), but it does not clarify whether 3SFC uses the same number of total samples as OFS or maintains its original number of samples. This ambiguity is critical because, without this comparison, it cannot determine whether the observed improvements stem from the overlapping structure or simply from using more synthetic samples. This baseline is essential to validate the core contribution of the overlapping mechanism and should be included to properly support the paper's claims.\n   \n(2-c) Table 3 only tests α ∈ {0.1, 0.25, 0.5}. What about α = 0 (no overlap)? This would directly test whether overlapping helps at all, which is fundamental to validating the paper's main contribution. \n\nAlso, there are several minor things to improve the paper:\n\n(3) The reference format does not consistently use proper citation style (e.g., use \\citep or \\citet appropriately).\n\n(4) The first two sentences in the Introduction section are redundant and could be integrated into one sentence for better clarity.\n\n(5) Since the paper's content focuses on distillation techniques for compression, the related work section would be more helpful if it contained more specific details about data distillation techniques rather than extensive coverage of sparsification and quantization methods, which are less relevant to understanding the proposed contribution. \n\n(6) The encoder \"compresses data into $D^t_{syn,i}$'\" (line 201) - but shouldn't it be compressing gradients into the synthetic dataset?"}, "questions": {"value": "Please refer to the Weakness section for detailed comments. In particular, I would appreciate clarification on the questions raised for each weakness. I will reconsider my evaluation after reviewing the authors’ rebuttal to these points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KievHRoJus", "forum": "hiU1rhYuFT", "replyto": "hiU1rhYuFT", "signatures": ["ICLR.cc/2026/Conference/Submission11116/Reviewer_C91N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11116/Reviewer_C91N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816147933, "cdate": 1761816147933, "tmdate": 1762922289110, "mdate": 1762922289110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Federated Learning (FL) is a promising privacy-preserving approach for decentralized non-IID data, but frequent gradient exchanges incur high communication costs, limiting scalability. Recent works use data distillation to reduce this overhead, yet fail to fully exploit distilled features, yielding suboptimal compression. They propose Overlapped Feature Synthesis (OFS), which enables global feature sharing during compression to boost both communication efficiency and model accuracy. A global feature sampler extracts small feature maps from a large shared map for parameter reuse. An offset coefficient and multiple sampling strategies balance global and personalized parameters, allowing flexible trade-offs. Experiments show OFS achieves better convergence at lower compression rates. It improves accuracy by ~1% over state-of-the-art distillation methods while maintaining a 10% higher compression rate. Ablation studies and visualizations further analyze the impact of the offset coefficient, client count, and local epochs, and reveal the interplay between global and personalized parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well presented.\n2.\tThe topic is important."}, "weaknesses": {"value": "1.\tThe performance of the method depends on the efficiency of the encoders. However, obtaining such an accurate encoder-decoder adapting to the local data itself may be difficult. \n2.\tThe improvement seems tiny in most settings, as shown in Table 1, which appears as an error bar to some extent.\n3.\tThe experiments are insufficient. Most FL works adopt 100 clients with 10% selected in each round. \n4.\tTypo: Competetors."}, "questions": {"value": "1.\tHow many layers in ResNet and RegNet in the evaluation?\nOther, please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bSI7TEzT96", "forum": "hiU1rhYuFT", "replyto": "hiU1rhYuFT", "signatures": ["ICLR.cc/2026/Conference/Submission11116/Reviewer_7ujY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11116/Reviewer_7ujY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954653321, "cdate": 1761954653321, "tmdate": 1762922288708, "mdate": 1762922288708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}