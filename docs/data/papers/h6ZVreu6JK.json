{"id": "h6ZVreu6JK", "number": 2349, "cdate": 1757061875154, "mdate": 1759898154471, "content": {"title": "Crossing the Separation Point: Stabilizing Decision-Focused Learning with Variational Free-Energy", "abstract": "Decision-focused learning (DFL) integrates predictive modeling with downstream optimization by training the prediction model to end-to-end minimize decision regret. However, the mapping from predicted parameters to decisions can be discontinuous, making gradients unstable and optimization unstable, particularly in the early stages of training. Existing approaches typically rely on heuristic warm-up phases through pretraining for stabilization, but the lack of theoretical understanding limits their effectiveness and generalizability. In this work, we propose a principled framework based on variational free energy (VFE) to analyze and address these challenges. We characterize the existence of critical separation points in the training dynamics, which mark a sharp transition in the alignment between prediction accuracy and decision quality. Building on this analysis, we develop Adaptive Recursive Annealing (ARA), a training strategy that adaptively regulates the learning dynamics without requiring warm-up. ARA improves training stability and decision performance by aligning upstream and downstream objectives throughout the learning process. Experiments on multiple benchmark problems demonstrate that our method consistently improves convergence behavior and downstream decision quality, offering a robust and theoretically grounded alternative to existing heuristics.", "tldr": "Learning dynamic matters the stability in decision-focused learning.", "keywords": ["Learning Dynamics", "Decision-focused Learning", "Variational Free-Energy"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1befa6a04fffbf4072ec53d13e6bbf4bf4e1df9f.pdf", "supplementary_material": "/attachment/5e5983c800c44881ea4e0875b6fc43b1685f88e4.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenge of non-smoothness in decision-based learning objectives (in linear program case), where a predictive model’s output directly determines downstream decisions. The authors propose a framework that measures “decision jumps” — the sensitivity of the optimal decision to small perturbations in model predictions — as a way to quantify and address the non-smoothness issue. They theoretically analyze the connection between this separation gap and a combined loss function involving mean squared error (MSE) and decision loss, showing that the weighted sum can encourage smoother alignment. To further improve differentiability, they introduce a variational relaxation that makes the decision stochastic, yielding a smoother surrogate objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper focuses on the fundamental issue of non-smoothness in decision-focused learning, which is a well-known bottleneck in bridging prediction and optimization. I do think this issue is an important issue that is worth exploring. This is good but on the other hand I also don't find the proposed solution addresses the fundamental issue completely though.\n\n* The gradient and separation analysis offers a clear theoretical perspective on how decision non-smoothness can be characterized and partially mitigated.\n\n* The perturbation-based “decision jump” measure is intuitive and connects directly to model sensitivity, making the analysis accessible.\n\n* The authors make a commendable effort to clearly define and quantify the degree of non-smoothness, which is useful for the broader research community."}, "weaknesses": {"value": "* While the problem is important, the proposed method is relatively straightforward and I am not completely satisfied with the solution — the non-smoothness issue remains fundamentally unresolved. One option is to analyze from the non-smooth non-convex optimization algorithm to design gradient descent with convergence guarantee, but I can see that this can be an overkill. A more elegant way using the property of linear program to design gradient descent algorithms would be good. In my opinion, what is lacking in this paper is the theoretical convergence analysis that leverages the properties in linear program / decision-focused learning.\n\n* The use of weighted MSE and decision loss, and the relaxation idea are both not new. From the methodological perspective, it doesn't offer new algorithms except the dynamic way to adjust the weight. Empirically it works but theoretically it is unclear."}, "questions": {"value": "Could you explain more about the definition of the running normalization factors $\\sigma_\\text{pred}$ and $\\sigma_\\text{dec}$ in page 4? I don't see a formal definition and the use in the loss function, but not sure if I am missing anything."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "em1M3KzpV6", "forum": "h6ZVreu6JK", "replyto": "h6ZVreu6JK", "signatures": ["ICLR.cc/2026/Conference/Submission2349/Reviewer_kQuS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2349/Reviewer_kQuS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797899441, "cdate": 1761797899441, "tmdate": 1762916202848, "mdate": 1762916202848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers a (now) classical Decision Focused Learning (DFL) setting, where a ML model is used to estimate the coefficients of the linear cost function for a decision problem with a fixed feasible space. The paper studies the loss landscape in such a setting, involving both the prediction loss and the decision loss.\n\nThe authors argue that poor stability of the loss function has significant adverse effect on the effectiveness of common  DFL methods, and that stability is poorest close to points where the prediction loss and the decision loss have opposite directional derivatives. The occur at points in the parameter space that are associated with a switch of the optimal solution.\n\nThe authors propose to analyze stability by combining the prediction and decision loss in a Lyapunov function (weighted combination), which is then smoothed by introducing a Gibbs distribution over the feasible decisions based on their cost. The normalization constant for the distribution is estimated via a biased Monte-Carlo approach.\n\nThen, they suggest using the free energy of the Lyapunov function as a new loss. They also introduce repulsive term (which penalize high solution variability), aiming to steer the optimization process away from regions associated to decision switches.\n\nIn the provided empirical evaluation, the approach has improved performance compared to other DFL methods for the same setting, with a few exceptions that are sufficiently well discussed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "In my opinion, the work makes a very interesting attempts at studying the loss landscape of a classical DLF setting, which has received somewhat limited attention, despite the growing body of research targeting the topic. The insight that \"conflicts\" between the prediction and the decision loss are associated to switching points in the space of optimal solution is insightful (and obviously true in hindsight, as it often is for this kind of discoveries).\n\nAssuming I have correctly understood the training approach (see later), the idea of actually using the prediction loss at training time is also interesting, as it can provide a useful gradient whenever it does not conflict with the (much harder to handle) decision loss.\n\nThe paper reads mostly well, assuming familiarity with the topic, though I would not consider clarity a major strength of the work (see later)."}, "weaknesses": {"value": "The work also suffers from a number of weaknesses.\n\nAs a first, minor, point, the authors do not provide a clearly defined, self-contained description of what their final training algorithm is. Even the fact that the free energy of the Lyapunov function is used as a loss is never directly stated. Clarifying this points would make the work much more readable.\n\nA second, this time considerable, issue is the fact that the proposed solution requires the computation of a normalization constant, which in turn is done by repeatedly sampling the solution space, and by subsequently solving the optimization problem. This process is bound to aggravate the already limited scalability of DFL at training time, given that constrained decision problems can be NP-hard to solve. The empirical evaluation also does not mention the value of K, i.e. how many samples are employed.\n\nThe high computation cost of the approach could in principle be worth it, if the benefits in terms of solution quality were significant. However, based on the reported results, this does not seem to be the case: while the method does outperform several relevant baselines, it does so by a small margin in all but one benchmark. Overall, this results in a limited practical appeal for the method, also considered that the approach comes with additional hyperparameters (e.g. number of samples) and potential issues (e.g. making sure that the biased sampling distribution tracks the desired one).\n\nOn the technical side, the rationale for the introduction of the repulsive terms does not appear fully convincing. For example, in the simple problem from Figure 1(b), the repulsive term would actually decrease the chance that gradient descent can escape the local optima between 0.5 and 0.75. Including an ablation study where the repulsive terms is disable would at least provide empirical evidence for its effectiveness.\n\nAgain from a technical perspective, the analysis from lines 155-163 is a bit too informal and contains some mistakes. First, it seems the discussed instability is linked to the predicted cost, rather than to the true cost; second, it seems to happen when the cost vector is perpendicular to a constraint, not when it is \"located on the boundary of the feasible set\"; finally, in such a situation there actually exists a vector s.t. an infinite number of equivalent solutions exist. It is my impression that the behavior the authors are concerned with is actually a switch of the optimal solution when the predicted cost vector receives a small adjustment, which corresponds to discontinuities in the example from Figure 1(b). I do not consider this a major issue, since it is easy to fix and the actual algorithmic components are designed precisely for such solution-switch events.\n\n\nFinally, on the clarity side, the text frequently speaks about terms or ideas that are introduced or used much later. For example, in classical DFL problems the prediction loss is completely ignored at training time and therefore does not \"pull\" at all; saying so only makes sense in the context of the new loss proposed here. As another example, the \\sigma scaling constants are used many lines after they are first mentioned. There are other similar cases."}, "questions": {"value": "* Can you confirm that the free energy (plus the repulsive term) is the actual loss function for the method?\n* What is the value of K used in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dKlvMikT9a", "forum": "h6ZVreu6JK", "replyto": "h6ZVreu6JK", "signatures": ["ICLR.cc/2026/Conference/Submission2349/Reviewer_n7Ry"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2349/Reviewer_n7Ry"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931991067, "cdate": 1761931991067, "tmdate": 1762916202708, "mdate": 1762916202708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adaptive Recursive Annealing, a technique to stabilize the training of decision-focused learning frameworks. The study analyzes the misalignment between upstream losses (parameter prediction quality) and downstream objectives (decision quality w.r.t. regret), identifying points during the training where the gradients point in opposite directions. The proposed framework regulates these conflicting losses at training, helping to mitigate the need to \"warm start\" the training (e.g., pretraining on the predictions and finetuning end-to-end). The empirical study shows that the method performs competitively for several of the explored optimization tasks, providing an alternative approach to traditional DFL pipelines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Separation Point Perspective:** The analysis of separation points and associated discussion would likely be of interest to the DFL community. This perspective is useful for diagnosing some of the challenges with current approaches, and it is articulated well by the paper.\n\n- **Methodological Contribution:** Several interesting ideas, with promising ramifications; for example, adaptive weighting seems to be a more principled approach than empirically optimized warm up schedules."}, "weaknesses": {"value": "- **Empirical Performance:** The performance described in Table 1 is a bit mixed. While effective in some settings, it is much worse than the baselines in others. While negative results are useful for analysis, they do not support more general claims of consistent improvement.\n\n- **Error Bars:** The evaluation lacks error bar reporting, using only a single fixed seed. Adding more runs / seeds would make these results more robust.\n\n- **Missing Ablations:** It would be valuable to better ablate the role of experimental hyperparameters and their comparison to the baselines (e.g., training runtime, number of epochs to convergence, variations of the fixed warm up period).\n\n- **Missing References:** There are several missing references throughout the paper. The proofs in particular seem to reference assumptions / lemmas that are not included in the paper. This makes it difficult to check their accuracy. While I expect this could be easily resolved, the absence of these makes it difficult to assess the \"soundness\" of the theory. An related, but more minor concern is several other typos that should be checked."}, "questions": {"value": "- Is there any intuitions associated with the negative result on the cubic task? Is there a connection to Figure 2's landscape that can be made?\n\n- What is regret %? This is a percentage of what?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bY4ao3IBcv", "forum": "h6ZVreu6JK", "replyto": "h6ZVreu6JK", "signatures": ["ICLR.cc/2026/Conference/Submission2349/Reviewer_GBSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2349/Reviewer_GBSN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938353460, "cdate": 1761938353460, "tmdate": 1762916202542, "mdate": 1762916202542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes to main classes of contributions. First, it proposes a method for decision focused learning that smooths a nondifferentiable optimization problem via random perturbations of the objective and combines the decision-focused and prediction losses during training. Second, it makes a conceptual argument that training instabilities in decision-focused learning are caused by points where the true cost vector induces multiple optimal solutions, and that this drives the prediction and decision losses to have conflicting gradients."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of characterizing the training dynamics of models with optimization in the loop is interesting and could result in improved methods for training. It would be valuable to have a principled way to control the weight between prediction and decision losses over the course of training."}, "weaknesses": {"value": "Treating the paper first as simply proposing a new method for decision focused learning, I have two main concerns. First is that weighted combinations of decision and prediction loss are already common, as are random perturbation strategies to smooth the optimization problem (see eg  Berthet et al 2020, \"Learning with Differentiable Perturbed Optimizers\"). Second is that the empirical results don't seem to show a very consistent improvement over baselines like SPO+ or learn-to-rank approaches. \n\nRegarding the paper's conceptual argument about training dynamics, I found the presentation hard to follow and have several questions listed below."}, "questions": {"value": "(1) What is the value of v in the definition 2.1?\n(2) Is sigma supposed to appear in the definition of V_\\lambda(hat(c))? And does it have to be set in some particular way in order for theorem 3.1 to hold, or is theorem 3.1 supposed to hold for any possible choice of sigma (which seems unlikely)?\n(3) Is it without loss of generality to only discuss two distinct minimizers for a knife-edge value of c? Eg for a linear program, there could be multiple vertices representing optimal solutions all of which lie on a given face of the polytope. \n(4) Are the gradients problematic when the true cost vector c is on the boundary, or when the prediction c-hat is on the boundary? It seems like it should be the later, but the paper switches between talking about separation points in terms of c-hat (definition 2.1) and c (theorem 3.1). If the issue is for the true c to be on the boundary, isn't this a quite low-probability event if we think of the c's as being random in some way?\n(5) At a high level, if pretraining approaches first use one loss and then the other (without using both simultaneously), why is it a problem that their gradients might disagree at particular points?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dz9PLF8DXk", "forum": "h6ZVreu6JK", "replyto": "h6ZVreu6JK", "signatures": ["ICLR.cc/2026/Conference/Submission2349/Reviewer_6Qez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2349/Reviewer_6Qez"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943438065, "cdate": 1761943438065, "tmdate": 1762916202312, "mdate": 1762916202312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}