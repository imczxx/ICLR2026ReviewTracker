{"id": "v1p4bDnDAP", "number": 4746, "cdate": 1757756607679, "mdate": 1759898016319, "content": {"title": "Two Birds with One Stone: Neural Tangent Kernel for Efficient and Robust Gradual Domain Adaptation", "abstract": "Gradual Domain Adaptation (GDA) bridges large distribution shifts through intermediate domains, yet faces challenges in computational overhead and error accumulation. In view of these problems, we propose GradNTK, a novel framework to employ the Neural Tangent Kernel (NTK) as one stone to \"hit\" two birds of the efficiency and robust issues in GDA. \nOn one hand, by exploiting the short-time dynamics of wide neural networks, GradNTK instantiates an NTK-induced Maximum Mean Discrepancy (MMD) as a differentiable domain-alignment metric that enforces smooth transitions between adjacent domains while maintaining near-linear computational cost. \nOn the other hand, the same NTK dynamics generate a prospective utility function to weight source/target samples by their shift sensitivity, enabling curriculum-guided gradual adaptation while avoiding error accumulation.\nExperiments on Portraits, Rotated MNIST and CIFAR-100-C demonstrate superior performance (e.g., 95.1\\% on Rotated MNIST, 99.5\\% on Color-Shift MNIST), while reducing training time by 1.8× compared to prior GDA methods.", "tldr": "One kernel, two roles: short-time NTK yields a differentiable NTK-MMD for smooth alignment and a utility score for per-sample weighting, enabling near-linear, single-pass GDA.", "keywords": ["Gradual Domain Adaptation", "distribution shift", "Neural Tangent Kernel", "Out-of-distribution Generalization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c154b77810e08610e6092c937806d0b9a78c4f7d.pdf", "supplementary_material": "/attachment/29049a6c431fd7c9ceb5b082f61216ee1bfcf828.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a general and important problem in machine learning, namely gradual domain adaptation (GDA). The authors propose GradNTK, a new framework that employs the Neural Tangent Kernel (NTK) to mitigate computational overhead and error accumulation in GDA. Specifically, the framework introduces an NTK-MMD loss and a sample reweighting function to facilitate domain transition. While the paper is clearly written and easy to follow, the overall novelty remains limited because both NTK-based matching and sample reweighting strategies are well-studied in the broader domain adaptation literature."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "-\tThe paper is well-organized and the presentation is clear..\n-\tThe proposed GradNTK framework integrates NTK-MMD loss and sample reweighting, and the experiments show some degree of effectiveness in GDA scenarios."}, "weaknesses": {"value": "-\tBoth neural kernel methods [1–4] and sample reweighting techniques [5–8] have been extensively explored in domain adaptation. Applying them to the gradual domain adaptation setting is a straightforward extension and does not provide sufficient novelty for ICLR.\n-\tThe overall experiment design is not sufficient to verify the effectiveness. More diverse datasets and larger backbones are preferred.\n-\tThe comparison in Table 4 appears to be unfair. Test-time adaptation (TTA) methods only require the source model, whereas the proposed framework requires access to source data at each adaptation stage. Moreover, many recent TTA methods [9-12] achieve better performance with less information and lower computational cost. As a result, the empirical advantage of the proposed framework remains unclear.\n\n**References:**\n\n[1] Jacot, Arthur, Franck Gabriel, and Clément Hongler. \"Neural tangent kernel: Convergence and generalization in neural networks.\" Advances in neural information processing systems 31 (2018).\n\n[2] Jia, Sheng, et al. \"Efficient statistical tests: A neural tangent kernel approach.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Cheng, Xiuyuan, and Yao Xie. \"Neural tangent kernel maximum mean discrepancy.\" Advances in Neural Information Processing Systems 34 (2021): 6658-6670.\n\n[4] Shimizu, Eiki, Kenji Fukumizu, and Dino Sejdinovic. \"Neural-kernel conditional mean embeddings.\" arXiv preprint arXiv:2403.10859 (2024).\n\n[5] Tachet des Combes, Remi, et al. \"Domain adaptation with conditional distribution matching and generalized label shift.\" Advances in Neural Information Processing Systems 33 (2020): 19276-19289.\n\n[6] Guo, Zong, et al. \"Gradual domain adaptation with sample transferability exploitation for person re-identification.\" 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2022.\n\n[7] Ru, Jinghan, et al. \"Imbalanced open set domain adaptation via moving-threshold estimation and gradual alignment.\" IEEE Transactions on Multimedia 26 (2023): 2504-2514.\n\n[8] Chen, Hong-You, and Wei-Lun Chao. \"Gradual domain adaptation without indexed intermediate domains.\" Advances in neural information processing systems 34 (2021): 8201-8214.\n\n[9] Wang, Qin, et al. \"Continual test-time domain adaptation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[10] Döbler, Mario, Robert A. Marsden, and Bin Yang. \"Robust mean teacher for continual and gradual test-time adaptation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[11] Press, Ori, et al. \"Rdumb: A simple approach that questions our progress in continual test-time adaptation.\" Advances in Neural Information Processing Systems 36 (2023): 39915-39935.\n\n[12] Song, Junha, et al. \"Ecotta: Memory-efficient continual test-time adaptation via self-distilled regularization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZSHX6LpQ47", "forum": "v1p4bDnDAP", "replyto": "v1p4bDnDAP", "signatures": ["ICLR.cc/2026/Conference/Submission4746/Reviewer_HY6a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4746/Reviewer_HY6a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760929220212, "cdate": 1760929220212, "tmdate": 1762917552482, "mdate": 1762917552482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GradNTK, which integrates the Neural Tangent Kernel (NTK) into Gradual Domain Adaptation (GDA). It uses NTK in two roles:\n1. An NTK-induced Maximum Mean Discrepancy (MMD) is employed as a differentiable alignment loss between adjacent domains, aiming to improve efficiency and smoothness.\n2. An NTK-based reweighting function is used to assign sample weights based on “shift sensitivity,” intending to mitigate error accumulation during gradual adaptation. \n \nExperiments on Rotated MNIST, Color-Shift MNIST, Portraits, CIFAR-10-C, and CIFAR-100-C show the method’s effectiveness for GDA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The derivation connecting NTK linearization, witness functions, and MMD appears to be pedagogically clear.\n2. Using NTK for both alignment and reweighting appears conceptually elegant.\n3. Replacing the traditional MMD with NTK-based short-time dynamics reduces memory overhead."}, "weaknesses": {"value": "1. Most technical ingredients—NTK linearization, MMD witness formulation, NTK-MMD, and pseudo-labeling—are well-established concepts. The contribution seems to be primarily a straightforward combination.\n2. Large portions of Section 3 appear to reiterate textbook material, which may obscure the core novelty. It would be advisable to condense these derivations and clearly separate known results from new contributions to make the key insight stand out.\n3. As shown in Table 5, removing the NTK-reweighting module causes almost no drop in accuracy, suggesting that this component contributes little to the claimed “robustness.”\n4. Beyond TENT, several test-time adaptation (TTA) methods such as CoTTA [a] and RMT [b] also address gradual and continual adaptation scenarios, but without access to source data. This makes TTA a more challenging setting compared to GDA. It would be valuable to clarify the conceptual and practical position of GDA relative to these recent TTA frameworks—specifically, what assumptions GDA relaxes or strengthens, and in what scenarios it is preferable. Including such discussion and comparisons would provide a more informative contextualization of the proposed method.\n5. All datasets used involve relatively smooth or synthetic domain shifts. It is unclear whether GradNTK would maintain its effectiveness under more realistic domain gaps.\n\n[a] Continual Test-Time Domain Adaptation. CVPR2022. \\\n[b] Robust Mean Teacher for Continual and Gradual Test-Time Adaptation. CVPR2023.\n\n**Minor comments**\n- In Figure 1, “(i) GDA” should be “(i) UDA.”\n- Line 51: citation formatting error."}, "questions": {"value": "Additional questions beyond those in Weaknesses:\n\n1. Can the authors clarify whether GradNTK requires explicit intermediate domain labels, or could it operate in a fully online streaming manner (closer to TTA)?\n2. Does the NTK reweighting function actually change sample selection over time, or does it remain nearly uniform (or static) in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m4HAUJocvp", "forum": "v1p4bDnDAP", "replyto": "v1p4bDnDAP", "signatures": ["ICLR.cc/2026/Conference/Submission4746/Reviewer_c31j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4746/Reviewer_c31j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673923189, "cdate": 1761673923189, "tmdate": 1762917552249, "mdate": 1762917552249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the gradual domain adaptation problem, where the gap between the source dataset and target dataset could be tracked by the continuous intermediate domains between them. To address the computation cost and error accumulation in existing methods, this work proposes the Neural NTK method as an efficient distance estimation and develops an NTK-based weight to reweigh the loss in risk estimation. Experiment results show that the proposed method achieves promising performance compared with advanced methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The motivation of improving distribution discrepancy estimation and gradual error propagation is reasonable and meaningful.\n\n+ The developed method with the Neural NTK estimator and weight is technically sound.\n\n+ The empirical performance is significant compared to the advanced methods, and the empirical analysis is consistent with the theoretical results."}, "weaknesses": {"value": "+ The validity of the proposed method in empirical scenarios needs further clarification, i.e., convergence of approximation, metric property and guarantees of reweighing objective.\n\n+ The math rigor could be improved, e.g., some notations are unclear."}, "questions": {"value": "Q1. Despite the requirement of infinite width of NTK for good approximation, it is also uncertain that the constructed parameterized MMD via $\\Theta$ still ensures metric property. Specifically, since the MMD in Eq. (12) is restricted to the function space parameterized by $\\Theta$, could it still measure the distance between distributions? (recall that the metric property of kernel MMD is only satisfied by specific kernels like Gaussian). \n\nQ2. The weights constructed in Eq. (15) need further justification. Since the weight is used in the risk estimation in Eq. (22), which seems to be similar to the common importance reweighting strategy, it would be important to show some theoretical results that such a weight could benefit the model learning, e.g., reduce the bias of risk estimation.\n\nQ3. How to understand the Neural NTK for gradual discrepancy estimation in practical scenarios with finite width. Are there quantitative results that could control the error? \n\nQ4. Recall that there are actually typical covariate shift methods that also consider the importance reweighting technique to reduce the gap between source risk and target risk. It would be interesting to demonstrate the significance of the proposed method compared with these methods, i.e., the difference of weight construction. \n\nQ5. What is the definition of $\\pi_\\mathbb{X}$ in Line 173? In my understanding, $Z$ is the push-forward distribution under $r_\\psi$, then $\\pi_\\mathbb{X}$ seems to be redundant.\n\nQ6. What is $\\Delta_\\mu$ in Eq. (13)? Does it imply the difference of mean values of source and target distributions? i.e., similarly defined as $g$ that is the difference of the Neural NTKs $f_T$ and $f_0$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GE1KUn4tGp", "forum": "v1p4bDnDAP", "replyto": "v1p4bDnDAP", "signatures": ["ICLR.cc/2026/Conference/Submission4746/Reviewer_ZvRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4746/Reviewer_ZvRF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845793545, "cdate": 1761845793545, "tmdate": 1762917552030, "mdate": 1762917552030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}