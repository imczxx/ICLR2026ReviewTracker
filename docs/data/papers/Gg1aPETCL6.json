{"id": "Gg1aPETCL6", "number": 2324, "cdate": 1757058498931, "mdate": 1759898155765, "content": {"title": "SinkTrack: Attention Sink based Context Anchoring for Large Language Models", "abstract": "Large language models (LLMs) suffer from hallucination and context forgetting. These problems are caused by attention drift, where LLMs’ focus shifts towards newly generated tokens and away from the initial input context. To address this, we make use of a related, intrinsic characteristic of LLMs: attention sink – the tendency to consistently allocate high attention to the very first token (i.e., ⟨BOS⟩) of a sequence. Concretely, we propose an advanced context anchoring method, SINKTRACK, which treats ⟨BOS⟩ as an information anchor and injects key contextual features (such as those derived from the input image or instruction) into its representation. As such, LLM remains anchored to the initial input context throughout the entire generation process. SINKTRACK is training-free, plug-and-play, and introduces negligible inference overhead. Experiments demonstrate that SINKTRACK mitigates hallucination and context forgetting across both textual (e.g., +18.9% on QuAC with Llama3.1-8B-Instruct) and multi-modal (e.g., +23.0% on M3CoT with Qwen2.5-VL-7B-Instruct) tasks. Its consistent gains across different architectures and scales underscore the robustness and generalizability. We also analyze its underlying working mechanism from the perspective of information delivery. Our source code is available at anonymous GitHub.", "tldr": "", "keywords": ["Large Language Models", "Multimodal Large Language Models", "Hallucination", "Context Forgetting"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0fa240126d2ed2029759692e8d969d66893fee64.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SINKTRACK, a training-free, dual-track cross-attention mechanism, solves the LLM problem of \"forgetting\" the original input by turning the naturally stable first token into an active information anchor that constantly and adaptively retrieves the most important details from the context."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents novel solutions to mitigate context forgetting and hallucination in LLMs by actively leveraging the model's intrinsic \"attention sink\" tendency towards the initial <bos> token.\n2. The quality of the work is evident through the systematic exploration of context anchoring methods, such as soft injection and SINKTRACK, which effectively enhances context coherence. The empirical results on relevant datasets confirm the method’s efficacy and significance as a solid step forward in improving LLM reliability over long contexts.\n3. This paper excels in clarity. The methodology is simple and well-motivated, and the overall organization of the paper is clear and easy to follow."}, "weaknesses": {"value": "1. Sticking all the important context onto just the first token might create an information bottleneck for really long documents. The author needs to consider the impact of this.\n\n2. The 'CoT' baseline is too simple, making the new method's performance improvements look bigger than they might actually be. The author needs to compare with more advanced methods.\n\n3. It is critical to include experiments demonstrating that SINKTRACK does not hurt the model's general abilities, such as common sense reasoning and instruction following, in standard short-context benchmarks."}, "questions": {"value": "1. Why is the $L_2$ norm used for Information Gain in Equation 5 instead of cosine similarity or another metric?\n\n2. In \"soft injection,\" does the \"information vector\" refer to the vector corresponding to the image information?\n\n3. Does soft injection perform fusion on the $\\langle\\text{bos}\\rangle$ hidden state before the key-value projection, and how is the $\\alpha$ coefficient set?\n\n4. Why does the CoT (Chain-of-Thought) method show a decrease in performance compared to the baseline in Table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wRU2ShmT4A", "forum": "Gg1aPETCL6", "replyto": "Gg1aPETCL6", "signatures": ["ICLR.cc/2026/Conference/Submission2324/Reviewer_YQZu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2324/Reviewer_YQZu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760780720957, "cdate": 1760780720957, "tmdate": 1762916194673, "mdate": 1762916194673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reproduction Issues and Concerns about Method Validity"}, "comment": {"value": "Hi authors,\n\nI was very interested in this paper. I've spent the better part of the last two weeks trying to reproduce your work and test your code, but I've run into several issues and have some concerns about the paper's claims.\n\n**1. Method Appears to Harm Performance and Lacks Reproducibility**\n\nI tested your provided Qwen2.5VL and Gemma3 models, and I also implemented your method on LLaVA-next.\n\nMy findings are deeply concerning: On a wide range of direct-answer benchmarks (MMMU-Pro, MMStar, ScienceQA, VQA, and POPE), **the method consistently harms model performance, scoring significantly lower than the baseline.** Even when I prompted the model to explain first and then respond in a specific format (eg, output {answer} format), this method also reduced the performance (For k=1,3,5,7). \n\nI'm seeing this exact negative trend on text-only LLMs as well. I have been unable to improve LLama 3.1 on tasks like SST2, AQUA, MMLU, and BoolQ. In my tests, when the LLM is already producing a direct, valid answer, the BOS token's representation modification actually causes it to produce an incorrect answer.\n\nI think this method doesn't actually improve the VLM's understanding of the image or fix visual hallucinations. **It seems highly likely that the reported metric gains are not from real improvements, but are instead coming from \"format correction.\"**\n\nIn other words, the method just corrects `invalid` outputs that your parser couldn't extract, even when the model's underlying answer was correct.\n\nIsn't it more likely that the \"context forgetting\" you're observing is just the model forgetting the **output format**, not the actual question or image? If so, simply re-emphasizing the format would give you a massive (and artificial) boost.\n\nThe rigid prompt format you've engineered (`**Answer: xx**`) seems to be the real culprit. It's likely that your CoT baseline isn't \"forgetting\" the question; it's just failing to adhere to this *specific* format after a long chain of thought. This is a well-known instruction-following problem, not a novel one. \n\nThere's a ton of existing work on forcing LLMs to output correct formats (e.g., simple prompt modifications, or attention steering techniques like PATSA). I also remember there is a paper that solved that by just improving BOS's attention. As reviewers said, you didn't compare SINKTRACK to \\*any\\* of these simpler methods, which should have been obvious baselines.\n\n- Could you please provide the raw counts of `invalid` outputs for your vanilla baseline vs. your method? And can you differentiate between gains from **true format correction** and gains from **actual visual perception**?\n- And critically, can you explain why your method **hurts model understanding** when the model is already producing a valid, direct answer?\n- Is there a known bias in Qwen2.5VL and Gemma3? Their image token strategies (e.g., `image start/end` tokens) are very different from the LLaVA series, which might be a confounder.\n- Given all this, is the added complexity of your method really worth it? It requires complex modifications to model files and adds extra attention overhead.  It's not at all clear that this provides any benefit over many direct and simpler techniques.\n\n**2. Implementation Issues**\n\nAs a practical note, modifying the LLaVA family is extremely cumbersome because its modeling file depends on a separate LM modeling file. It’s difficult to patch cleanly. Did you use a cleaner approach?\n\nAlso, based on my extended testing, improvements that appear on Gemma3 and Qwen2.5VL often fail completely on the LLaVA series. Bidirectional attention among visual tokens seems heavily influenced by pretraining distributional biases. It may be worth testing a broader set of models.\n\n**3. Unfair Baseline Comparison**\n\nI noticed your method calls `scaled_dot_product_attention` directly, while the baseline uses the VLM/LLM's internal attention implementation. These are not equivalent and can have different numerical/performance outcomes. For a fair comparison, your baseline should also be modified to explicitly call `scaled_dot_product_attention`.\n\n**4. Theoretical and Empirical Concerns**\n\n- Your method's reliance on \"skip layers\" (a hyperparameter k) seems very sensitive to the specific dataset as I tested. This cross-layer query (e.g., layer m's query to layer m-n's key) doesn't seem theoretically sound. Transformers learn intra-layer interactions. Can you provide any theoretical justification for why this cross-layer attention should work at all?\n- Also, I've observed that for VLMs, the `<bos>` token (which is, by nature, a *text* token) pays far less attention to the image tokens than it does to the other text tokens. This seems to work against your core premise of using it as an *image* anchor.\n\nI've spent a lot of time on this, and I'm currently unable to get this to work as advertised. \n\nAlthough I am not a reviewer, I would appreciate any clarification or comment you can provide.\n\nThanks."}}, "id": "ZMX9VwL1up", "forum": "Gg1aPETCL6", "replyto": "Gg1aPETCL6", "signatures": ["~Chris_Chao1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Chris_Chao1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2324/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763089989038, "cdate": 1763089989038, "tmdate": 1763089989038, "mdate": 1763089989038, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a test-time method to mitigate reduce hallucination in MLLMs, and context forgetting on both LLMs and MLLMs. It uses soft injection to fuse information into LLM's computational flow."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well-written and easy to follow.\n2. The authors conducted comprehensive experiments on multiple tasks and multiple LLMs to validate their idea."}, "weaknesses": {"value": "1.\tMethodological clarity:\nSome details in the method section remain unclear. For example, it is not explicitly stated how f_{\\text{info}} is obtained. From Figure 3, it appears that f_{\\text{info}} results from mean pooling over the encoder outputs; however, the architecture details of the Vision/LLM encoder are missing. The authors also mention that this work focuses only on decoder-only LLMs, which further raises questions about how the encoder component is integrated. In addition, an analysis of the extra computational overhead introduced by this module would be helpful.\n2.\tExperimental setup:\nIn Table 2, it is unclear why Chain-of-Thought (CoT) prompting performs worse than Direct Prompting, particularly on text-only tasks. Moreover, the current baselines seem rather limited. Including stronger or more diverse baselines would better demonstrate the effectiveness and generalizability of the proposed method.\n3. Definition ambiguity:\nThe distinction between vertical and horizontal information flow is not clearly justified. It is unclear whether this formulation is newly introduced by the authors or adapted from prior work. Providing clearer definitions and theoretical grounding would strengthen the analysis.\n4. Potential information loss and generalizability:\nThe proposed approach compresses all information into a single token, which may still lead to information loss. Although the reported results on the two tasks are promising, the generalizability of this information compression strategy remains uncertain. Further empirical validation on broader or more diverse tasks would make the claims more convincing."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kFkbqKpuIL", "forum": "Gg1aPETCL6", "replyto": "Gg1aPETCL6", "signatures": ["ICLR.cc/2026/Conference/Submission2324/Reviewer_85VA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2324/Reviewer_85VA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760803527209, "cdate": 1760803527209, "tmdate": 1762916194525, "mdate": 1762916194525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles hallucination and context forgetting caused by attention drift. The authors observe that, during generation, LLMs gradually shift their focus toward newly generated tokens and away from the initial context. Inspired by attention sink, they propose SinkTrack, a training-free, inference-time enhancement that treats the BOS token as an information anchor and continually injects key contextual features (e.g., image or instruction embeddings) into its representation. Experiments on both text-only and multi-modal QA show that SinkTrack suppresses drift-induced hallucinations and forgetting, outperforming both direct inference and chain-of-thought reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper highlights how attention drift triggers hallucination and context forgetting, which is a key insight for both long-context LLMs and long-context multi-modal LLMs.  \n2. The authors introduce a novel, training-free inference enhancement that alleviates the above issues with negligible overhead.  \n3. The effectiveness is demonstrated across the Qwen, Gemma, MiniCPM, and Llama families, accompanied by visualization analyses and discussions."}, "weaknesses": {"value": "1. The evaluation benchmarks are somewhat outdated. While my limited knowledge of vision-related tasks and hallucination-related discussion prevents me from assessing those aspects, relying solely on QuAC and SQuAD to evaluate SinkTrack’s long-context improvements is clearly insufficient. The authors should first justify why QuAC is representative and then add results on standard long-context benchmarks such as LongBench [1], L-Eval [2], NIAH [3], and RULER [4], or more recent long-term conversation benchmarks like LongMemEval [5].\n2. The authors compare SinkTrack only against direct query and Chain-of-Thought, without comparison with the methods cited in Related Work. While three main streams of prior work are reviewed, their empirical relationship to SinkTrack has not been examined, such as an experimental comparison with a retrieval-augmented method. Including such comparisons would greatly strengthen the cohesion of the paper and the soundness of the method.\n3. The evaluation does not state the context lengths used, nor does it examine how attention drift varies as prompt length grows. Because SinkTrack itself relies on attention-based calculation, I am worried about whether the attention for SinkTrack could also drift when the prompt becomes very long. \n\n[1] LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding https://arxiv.org/abs/2308.14508\n\n[2] L-Eval: Instituting Standardized Evaluation for Long Context Language Models https://arxiv.org/abs/2307.11088\n\n[3] Needle In A Haystack - Pressure Testing LLMs https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n\n[4] RULER: What's the Real Context Size of Your Long-Context Language Models? https://arxiv.org/abs/2404.06654\n\n[5] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory https://arxiv.org/abs/2410.10813"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UmrNKsw7gx", "forum": "Gg1aPETCL6", "replyto": "Gg1aPETCL6", "signatures": ["ICLR.cc/2026/Conference/Submission2324/Reviewer_Utsb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2324/Reviewer_Utsb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760869237218, "cdate": 1760869237218, "tmdate": 1762916194295, "mdate": 1762916194295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method to enable context anchoring (reducing hallucination) for language models, the key idea is to leverage the attention sink phenomena and fuse the hidden representation of context into the sink token to encourage more attention to the context.\n\nExperiments are conducted on three LLMs and four VLMs, showing improved performance over baseline (direct inference and CoT inference)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is written relatively clearly.\n* The idea of leveraging attention sink to encourage context anchoring is interesting."}, "weaknesses": {"value": "* The baseline is limited: Using attention sink to enhance context anchoring is nice, yet this paper does not compare to prior training-free methods to mitigate halluciation. To name a few: [ACT(ICML 2024)](https://arxiv.org/pdf/2406.15765), [VCD(CVPR 2024)](https://arxiv.org/pdf/2311.16922), [OPERA(CVPR 2024)](https://arxiv.org/pdf/2311.17911), [SID(ICLR 2025)](https://arxiv.org/pdf/2408.02032), [DAC(ICML 2025)](https://arxiv.org/abs/2502.01969).\n\n* It is nice that the paper evaluates on both image and textual benchmarks, yet i found the choice of textual benchmark a bit unconventional. If the idea is to mitigate hallucination for long context, evaluation on well-studied benchmarks such as [LongBench](https://arxiv.org/abs/2308.14508) would be appropriate."}, "questions": {"value": "* IIUC, the SINKTRACK method lets the <BOS> token attend to the rest of the tokens (instead of replacing the K,V of its own with the mean pooling of the rest of the token). If so, that should be illustrated clearly in Figure 3 (which shows the mean pooling operation).\n* I would suggest the author to present their full methods more clearly. Currently the paper is written to illustrate the \"failed\" attempt first (hard injection, soft injection, mean pooling), which makes it a bit hard to understand the actual proposed method. These comparison can be included in the ablation study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wjcljtTjY4", "forum": "Gg1aPETCL6", "replyto": "Gg1aPETCL6", "signatures": ["ICLR.cc/2026/Conference/Submission2324/Reviewer_DYoN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2324/Reviewer_DYoN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919608296, "cdate": 1761919608296, "tmdate": 1762916194023, "mdate": 1762916194023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SinkTrack, a training-free, plug-and-play mechanism that enhances large language models (LLMs) by addressing two key issues: hallucination and context forgetting. The authors identify attention drift—the model’s tendency to focus increasingly on recent tokens during generation. They exploit an opposing, intrinsic behavior called the attention sink, where the first token (⟨BOS⟩) consistently receives high attention throughout decoding. SinkTrack injects key contextual information into the ⟨BOS⟩ token’s representation, and introduces a dual-track attention mechanism: one track performs adaptive cross-attention for ⟨BOS⟩, while the other maintains standard causal self-attention for other tokens. This allows ⟨BOS⟩ to query relevant information dynamically, counteracting information decay during long-context generation. The experimental results show its promise. SinkTrack requires no training, introduces negligible inference overhead, and is compatible across architectures and modalities. The authors also provide interpretability analyses of information flow, showing how the ⟨BOS⟩ token propagates injected information vertically (across layers) and horizontally (across tokens)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "SinkTrack is training-free, plug-and-play, and requires only a one-time injection at inference.\n\nThe dual-track design elegantly balances adaptation (via BOS cross-attention) and model integrity (via standard causal flow), preserving pretrained representations while enhancing context retention.\n\nEvaluations span six datasets, covering both text and vision-language reasoning."}, "weaknesses": {"value": "The approach is empirically strong but lacks formal justification for why attention anchoring should improve global consistency.\n\nThere’s no discussion of convergence, gradient flow, or formal guarantees that information propagation remains stable as sequence length grows.\n\nThe early versions of SinkTrack use mean-pooling for contextual information compression, which can cause information loss in very long contexts.\n\nWhile the benchmarks are diverse, all are QA or reasoning tasks. The generality claim would be stronger with results on open-ended generation, summarization, or code generation.\n\nThe writing is dense in places and could be more accessible. For instance, the transitions between hard, soft, and dual-track injection could be improved."}, "questions": {"value": "Would you please provide more justification for attention anchoring, and its convergence properties?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X5ijIrfrcF", "forum": "Gg1aPETCL6", "replyto": "Gg1aPETCL6", "signatures": ["ICLR.cc/2026/Conference/Submission2324/Reviewer_gHxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2324/Reviewer_gHxw"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070873431, "cdate": 1762070873431, "tmdate": 1762916193822, "mdate": 1762916193822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}