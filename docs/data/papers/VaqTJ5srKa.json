{"id": "VaqTJ5srKa", "number": 7484, "cdate": 1758024148770, "mdate": 1759897850025, "content": {"title": "Fairness-Aware Multi-view Evidential Learning with Adaptive Prior", "abstract": "Multi-view evidential learning aims to integrate information from multiple views to improve prediction performance and provide trustworthy uncertainty estimation. Most previous methods assume that view-specific evidence learning is naturally reliable. However, in practice, the evidence learning process tends to be biased. Through empirical analysis on real-world data, we reveal that samples tend to be assigned more evidence to support data-rich classes, thereby leading to unreliable uncertainty estimation in predictions. This motivates us to delve into a new Biased Evidential Multi-view Learning (BEML) problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning (FAML). FAML first introduces an adaptive prior based on training trajectories, which acts as a regularization strategy to flexibly calibrate the biased evidence learning process. Furthermore, we explicitly incorporate a fairness constraint based on class-wise evidence variance to promote balanced evidence allocation. In the multi-view fusion stage, we propose an opinion alignment mechanism to mitigate view-specific bias across views, thereby encouraging the integration of consistent and mutually supportive evidence. Theoretical analysis shows that FAML enhances fairness in the evidence learning process. Extensive experiments on six real-world multi-view datasets demonstrate that FAML achieves more balanced evidence allocation and improves both prediction performance and the reliability of uncertainty estimation compared to state-of-the-art methods.", "tldr": "We propose a FAML framework that mitigates biased evidence allocation across classes by introducing a adaptive prior and fairness constraints, achieving more reliable and fair uncertainty estimation.", "keywords": ["multi-view evidential learning", "uncertainty estimation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af8cb3a4fb470eb90146bc05f4a8e2c3522b415c.pdf", "supplementary_material": "/attachment/8e77c1613f60c4a1c2fec8181e438492cf776445.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical bias in multi-view evidential learning: existing methods assume reliable view-specific evidence learning, but real-world data shows samples allocate more evidence to data-rich classes, harming uncertainty estimation reliability. To solve this newly defined Biased Evidential Multi-view Learning (BEML) problem, it proposes the Fairness-Aware Multi-view Evidential Learning (FAML) framework, which integrates three key components: a training trajectory-based adaptive prior (to calibrate bias), a class-wise evidence variance fairness constraint (to balance allocation), and an opinion alignment mechanism (to reduce cross-view bias). Theoretical analysis confirms FAML enhances evidence learning fairness, while experiments on 6 real-world datasets demonstrate FAML outperforms state-of-the-art methods in balanced evidence distribution, prediction performance, and uncertainty estimation reliability. The work also contributes by highlighting this implicit unfairness, proving the adaptive prior expands minority class evidence margins (with improved generalization error bounds), and validating FAML’s superiority."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "-  The paper acutely captures a neglected yet prevalent issue in multi-view evidential learning: implicit unfairness in evidence allocation caused by differences in class data volume, which undermines the reliability of uncertainty estimation. This finding directly challenges the flawed assumption of existing methods that \"view-specific evidence learning is inherently reliable,\" clarifies key optimization directions for future research, and reflects a deep understanding of practical pain points in the field.\n- The proposed FAML method forms a closed-loop optimization from three dimensions, with strong targeting and novel design:\nAdaptive Prior: Dynamically adjusted based on training trajectories, effectively calibrating evidence learning bias caused by class imbalance.\nFairness Constraint: Directly promotes balanced evidence allocation across different classes through class-wise evidence variance control.\nOpinion Alignment Mechanism: Reduces view-specific bias during multi-view fusion, ensuring the integrated evidence is consistent and mutually supportive.\n- The paper features both solid theoretical foundations and sufficient experimental validation, enhancing the persuasiveness of its research conclusions. First, it proves through derivation that the adaptive prior can expand the evidence margin for minority classes and provides an improved factor for the generalization error bound, offering mathematical support for the method’s effectiveness. \n-  Extensive experiments on 6 real-world multi-view datasets not only verify FAML’s advantages in prediction performance but also demonstrate its ability to achieve fairer and more reliable uncertainty estimation, ensuring the universality of the results."}, "weaknesses": {"value": "- The paper does not verify FAML’s performance under input noise (e.g., Gaussian noise on view features like GIST/HOG in Scene15), leaving unclear if its evidence fairness and uncertainty estimation hold for imperfect real-world data\n- Though tested on datasets with 2-6 views, it lacks analysis on how view count changes (e.g., reducing Caltech-101’s 6 views to 3) affect FAML’s fairness (FD) and opinion alignment effectiveness"}, "questions": {"value": "- The paper updates the adaptive prior every 5 epochs starting from the 20th training epoch. Was there any preliminary experiment to confirm that a 5-epoch update interval is more suitable for maintaining training stability compared to other intervals (e.g., 3 or 10 epochs)?\n- When defining the fairness degree (FD) based on class-wise evidence variance, did you observe how FD changes dynamically across training epochs (e.g., whether it drops faster in early or late stages)? And does this change trend correlate with the gradual increase of λ (the balancing coefficient) in the loss function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zI5cmK8HWn", "forum": "VaqTJ5srKa", "replyto": "VaqTJ5srKa", "signatures": ["ICLR.cc/2026/Conference/Submission7484/Reviewer_Tpet"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7484/Reviewer_Tpet"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468606364, "cdate": 1761468606364, "tmdate": 1762919602059, "mdate": 1762919602059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel and important problem: Biased Evidential Multi-view Learning (BEML). The authors demonstrate through empirical analysis that existing multi-view evidential learning methods tend to allocate more evidence to data-rich (majority) classes, leading to biased and unreliable uncertainty estimates for data-poor (minority) classes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "To address this, the paper proposes a Fairness-Aware Multi-view Evidential Learning (FAML) framework. The method has three core components:\n\nAn Adaptive Prior based on training trajectories, which dynamically adjusts the Dirichlet prior to provide more support to classes that are under-represented or poorly performing.\n\nAn explicit Fairness Constraint, which penalizes high variance in evidence allocation across different classes, encouraging a more balanced evidence distribution.\n\nAn Opinion Alignment mechanism, which minimizes the dissonance between opinions from different views during fusion to mitigate view-specific biases.\n\nThe authors provide strong theoretical grounding for their adaptive prior, proving that it increases the evidence margin for minority classes and tightens the generalization error bound. Comprehensive experiments on six real-world datasets show that FAML significantly outperforms state-of-the-art methods in terms of accuracy (especially for tail classes), calibration error (ECE), and uncertainty reliability (AUROC, FPR-95)."}, "weaknesses": {"value": "While this is an excellent paper, the following minor revisions could further strengthen its clarity and impact:\n\nStrengthen the \"Related Work\" on Fairness and Subpopulation Shift: The core problem FAML solves—evidence bias due to class imbalance—is deeply connected to the broader fields of AI fairness and subpopulation shift. To better position the paper's contribution, the \"Related Work\" section should be expanded to include and discuss key works from this area. For instance, the authors should cite foundational work like GroupDRO (Sagawa et al., 2019), which formalized the goal of improving worst-group generalization in subpopulation shifts. More importantly, citing a paper like UMIX (Han et al., 2022), which explicitly links Uncertainty-Aware methods to solving Subpopulation Shift, would be highly relevant. Discussing these papers would allow the authors to clearly articulate FAML's unique contribution: while GroupDRO tackles the problem via the loss function and UMIX uses uncertainty to guide data augmentation, FAML introduces a novel approach by manipulating the evidential learning framework itself (via adaptive priors and fusion) to achieve fairness in a multi-view setting.\n\nIntuition for the Adaptive Prior (Eq. 5): The paper should more explicitly state the key intuition behind Equation 5. The current description is accurate but subtle. The authors should clearly emphasize that this formula creates an inverse relationship: the worse a class performs (i.e., the fewer samples are correctly classified, the smaller the denominator), the larger the adaptive prior (Beta_k) becomes. This \"compensatory\" mechanism is the core of the idea and should be stated plainly.\n\nExplicit Formulation of L_fc and mu schedule: In Section 3.2.3, the paper introduces the fairness loss L_fc, stating it is based on Definition 1. For absolute clarity, the paper should explicitly write out the final loss term (e.g., L_fc = Var(...)). Furthermore, the balancing coefficient mu is described as \"gradually changing from 0 to 1.\" Please specify the exact schedule used (e.g., linear, exponential) to enhance reproducibility.\n\nMotivation for Dissonance Degree (Eq. 10): The \"Dissonance Degree\" used for opinion alignment is a novel metric. The authors should add a brief sentence justifying this specific choice (sum of absolute differences in variance) over other, more traditional divergence measures (e.g., KL or JS divergence on the Dirichlet means/probabilities)."}, "questions": {"value": "As shown in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iHH0ycQT6i", "forum": "VaqTJ5srKa", "replyto": "VaqTJ5srKa", "signatures": ["ICLR.cc/2026/Conference/Submission7484/Reviewer_bZ2G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7484/Reviewer_bZ2G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620438613, "cdate": 1761620438613, "tmdate": 1762919601719, "mdate": 1762919601719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper highlights an overlooked bias in multi-view evidential learning, where evidence allocation tends to favor data-rich classes, leading to unreliable uncertainty estimates. Instead of relying on fixed uniform priors like traditional EDL methods, it adaptively adjusts class priors based on training trajectories in a principled manner. Additionally, a fairness constraint on class-wise evidence allocation and an opinion-alignment regularization across views ensure the consistent allocation of evidence across views. Experiments on six multi-view datasets demonstrate superior region accuracy and ECE compared to baselines, with ablation studies confirming the contributions of each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well organized, and the proposed methodology is enlightening.\n\n2. The motivation behind the paper is clear, and the theoretical analysis is complete.\n\n3. The proposed method offers novel insights, particularly in using training trajectories to adjust class priors, thereby mitigating view-specific bias throughout the multi-view fusion process\n\n4. The proposed method shows a clear performance improvement in a series of experiments."}, "weaknesses": {"value": "1. In this paper, the notion of fairness seems to focus on balancing the evidence allocation across different classes, rather than addressing fairness in terms of sensitive attributes like race or gender in a broader sense.\n\n2. Is this approach intended as a general framework? Can other trusted multi-view fusion methods also adopt similar strategies to improve model performance even on balanced datasets?\n\n3. Some implementation details seem to be missing. For instance: How does the hyper-parameter $\\mu$ change during training? and How is the metric ECE calculated in your experiments.\n\nIf the authors can address my questions, I am willing to increase my score."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KHnOITz7UL", "forum": "VaqTJ5srKa", "replyto": "VaqTJ5srKa", "signatures": ["ICLR.cc/2026/Conference/Submission7484/Reviewer_RwdE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7484/Reviewer_RwdE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804272779, "cdate": 1761804272779, "tmdate": 1762919601305, "mdate": 1762919601305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of unreliable uncertainty estimation in multi-view evidence learning, which stems from biased evidence collection. The authors propose a framework called Fairness-Aware Multi-view Evidential Learning. This method uses a training-trajectory-based adaptive prior to calibrate the Dirichlet parameters, aiming to mitigate the evidence bias. The approach includes theoretical guarantees and is validated through experiments on six real-world datasets to demonstrate its performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has a clear motivation and effectively solves the problems of biased evidence multi-view learning.\n\n2. The paper offers a clear and well-grounded theoretical analysis that connects the adaptive prior design to margin theory, helping explain why the proposed approach could improve model's generalization.\n\n3. The comparison experiments are comprehensive, including six representative multi-view datasets."}, "weaknesses": {"value": "1. This work proposes an EDL-based multi-view classification method. However, the literature review for existing EDL-based multi-view methods is insufficient. The authors should provide a more comprehensive discussion of related work in this specific domain, such as, but not limited to, [1, 2].\n\n2. The text in Figure 1 is too small, and there is no explanation of what the points, lines, and colors in the figure represent or why it is imbalanced. The blue in the legend of Figure 1 is different from that in the figure.\n\n3. The phrase \"most existing studies generally assume that view-specific evidence learning is inherently reliable\" is ambiguous, especially the use of the word \"reliable.\" What you want to convey is that the evidence learned from this view is unreliable, but it may lead readers to misunderstand that the view itself is unreliable.\n\n4. Punctuation is also part of the formulas and needs to be added.\n\n[1] Enhancing Testing-Time Robustness for Trusted Multi-View Classification in the Wild. CVPR 2025.\n\n[2] Trusted multi-view classification with expert knowledge constraints. ICML 2025."}, "questions": {"value": "1. What role did the opinion alignment play in promoting fairness? It seems irrelevant to fairness?\n\n2. The paper positions fairness as a key design goal, yet there are no reported quantitative fairness evaluation metrics to determine this. Could the authors provide explicit metrics or quantitative analysis to support the fairness claims?\n\n3. The degraded baseline model is introduced for the visualization. This baseline is described as FAML without the fairness-aware components. It is unclear if this is a re-run of an existing baseline (e.g. TMC) ? What are the hyper-parameters of the compared baseline?\n\n4. In subjective logic, formulas rely on fixed priors to compute belief mass and uncertainty. When the prior becomes adaptive, do these formulations still hold as originally defined? Are the theoretical assumptions of subjective logic still satisfied after introducing the adaptive prior?\n\n5. Could you discuss the robustness of FAML in the presence of potential noisy views. How does the adaptive prior perform in such scenarios?\n\n6. Check for all possible typos in the manuscript. e.g., \"bias exhibit view-specific pattern\" should be \"bias exhibits view-specific pattern\" in Line 20."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yGjZRah1Ye", "forum": "VaqTJ5srKa", "replyto": "VaqTJ5srKa", "signatures": ["ICLR.cc/2026/Conference/Submission7484/Reviewer_yDqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7484/Reviewer_yDqP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904924087, "cdate": 1761904924087, "tmdate": 1762919599861, "mdate": 1762919599861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}