{"id": "B2FQwbyxIZ", "number": 13657, "cdate": 1758220515193, "mdate": 1763756372179, "content": {"title": "From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning", "abstract": "Algorithms that exploit factored Markov decision processes are far more sample‑efficient than factor‑agnostic methods, yet they assume a factored representation is known a priori---a requirement that breaks down when the agent sees only high‑dimensional observations. Conversely, deep reinforcement learning handles such inputs but cannot benefit from factored structure. We address this representation problem with Action‑Controllable Factorization (ACF), a contrastive learning approach that uncovers independently controllable latent variables---state components each action can influence separately. ACF leverages sparsity: actions typically affect only a subset of variables, while the rest evolve under the environment's dynamics, yielding informative data for contrastive training. ACF recovers the ground‑truth controllable factors directly from pixel observations on three benchmarks with known factored structure---Taxi, FourRooms, and MiniGrid‑DoorKey---consistently outperforming baseline disentanglement algorithms.", "tldr": "ACF, contrastive algorithm for learning representations that disentangle independently controllable elements", "keywords": ["disentanglement", "factored mdps", "factorization", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32182936e52cdd2f59514f6bea44e1876739b343.pdf", "supplementary_material": "/attachment/d62d5d05dee6801272ee126af07806658efb5bb6.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of learning controllable factorized representations from raw observations. The main idea is to contrast the transitions given the correct action against no-op actions. Additionally, they also employ an inverse dynamics loss and mutual information objective between the current and next states. The show their approach successfully recovers the true factorized representation across various environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed objectives are well motivated and simple and clearly explained\n\nThe empirical results support the claims\n\nThe paper is well written and the intuition is clearly explained."}, "weaknesses": {"value": "Missing comparison to AC-State (https://arxiv.org/abs/2207.08229). A closely related line—AC-State—also tackles the same problem. I think it would be a valid baseline to compare against. I would also encourage to cite and discuss it in their paper.\n\nThe authors motivate learning a controllable state for RL, however there are not RL experiments with the learned representations. It would be great to verify if the learned representations help in RL compared to representations learned from the baselines and also non factored representations.\n\nI am bit skeptical whether this approach would apply to real-world complex scenes as the paper only explored synthetic domains. The method assumes that an action cleanly affects only a subset of factors but the real-world is more messy with effects of actions being lagged, stoachastic, or entangled. I am curious what the authors think about this?"}, "questions": {"value": "what policy is used to collect the trajectories used for training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aKxTONcTTn", "forum": "B2FQwbyxIZ", "replyto": "B2FQwbyxIZ", "signatures": ["ICLR.cc/2026/Conference/Submission13657/Reviewer_w7hc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13657/Reviewer_w7hc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761270286371, "cdate": 1761270286371, "tmdate": 1762924228742, "mdate": 1762924228742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of learning a factored state representation directly from high-dimensional observations(i.e. pixels). The authors propose Action-Controllable Factorization (ACF), a novel contrastive learning method to uncover these factors automatically. ACF's key idea is to first factorize the MDP transition dynamics via the introduction of several energy functions, and learning the controllable factors by leveraging such factorization.The authors provide an identifiability theorem under certain assumptions and demonstrate empirically on several visual benchmarks (Taxi, FourRooms, DoorKey) that ACF significantly outperforms existing disentanglement and representation learning baselines in recovering the ground-truth factors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-motivated: distangled respresentation is very important in RL, which can help agents discover useful information from the environment. The propsed method factorizes the transition dynamics, which put an insightful prior bias upon the MDP. \n2. The experiments directly prove the effectness of ACT to extract distangle factors from pixels"}, "weaknesses": {"value": "1. The benifits of learned factors are not shown end-to-end: Although authors has shown that ACL can learn the distangled factors from several toy environment, the benifits of such distangled factors are not shown end-to-end (by running a RL algorithm based on the leared factors).\n2. The Assumption is too strong: From the paragraph in `Factorizing the Controllable Variables`,  we can see that there are actually several assumption: (1) There exists a no-op action which does not affect the environment; (2) The action is discrete; (3) There are an one-to-one mapping between factors $s$ and action $a$. These assumptions hinders ACL's application.\n3. ACL can not capture  long-term controllable factors: According to the paper, ACL learns the factors only via 1-step transition, which means it may fail to capture controllable factors that requires multi-step environment interactions.\n4. non-efficienct related works:  there are some works that is closely related to this works such as [1][2], which also learns the useful representations. It is better to include more discusssion and comparision against these works.\n \n[1] Learning controllable elements oriented representations for reinforcement learning\n[2] Predictive information accelerates learning in rl"}, "questions": {"value": "1. The loss (4) and (5) put constraints on energy function $E$ so that it can be learned. However, does (4)/(5) can sufficient to make the learned energy function $E ∝ \\log T (z'_i | z, a)$, as required in Theorem 3.1? \n2. Have you ever try multi-step transition boostraping to make ACL able to capture long-term controllable factors, as [1] does?\n\n\n[1] Learning controllable elements oriented representations for reinforcement learning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WW8zJTEqek", "forum": "B2FQwbyxIZ", "replyto": "B2FQwbyxIZ", "signatures": ["ICLR.cc/2026/Conference/Submission13657/Reviewer_2zgD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13657/Reviewer_2zgD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742005604, "cdate": 1761742005604, "tmdate": 1762924228416, "mdate": 1762924228416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to recover controllable variables from raw pixels by leveraging contrastive learning and sparsity measures on the action-conditioned dynamics functions. In general, the approach builds on principles from causal representation learning (essentially nonlinear ICA), where the state-to-observation mapping is assumed to be bijective, and the action serves as a causal signal to uncover the state dimensions directly influenced by it. Specifically, the method enforces two key properties: (1) the action effects are sparse, and (2) actions cause significant changes in the corresponding state factors. To achieve this, the authors use contrastive objectives and parameterize the dynamics functions as an energy function. In evaluation, the method can identify independent components controllable by actions across several RL domains, measured using $R^2$ scores and latent traversals.\n\nOverall, the motivation is reasonable, learning independent controllable components is indeed valuable for precise control in RL. However, in its current form, several aspects related to the assumptions, objective functions, and empirical comparisons could be improved. For this initial review, I would give a rating of 4, but I’m open to revising it after the authors address these points in the discussion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, the motivation is reasonable and useful for RL, especially in environments with many distractors, where identifying element-wise controllable factors can really help.\n\n2. The theory, although largely built on existing CRL literature, looks generally sound.\n\n3. The experiments, while not very extensive, is still sufficient to verify the main claims about action sparsity and the proposed algorithm"}, "weaknesses": {"value": "I listed both the weaknesses and questions here as some of them have overlappings. \n\nW1: On the bijection assumption, I agree it can hold in these toy settings, but in more realistic RL domains (with occlusions, partial views, manipulation scenes, locomotion with self-occlusion, etc.), the observation is not bijective to the underlying state. In those cases, you would need extra assumptions or side information (multi-view, proprioception, actions-as-interventions, or temporal smoothing) to recover the controllable factors. It would be good to discuss this limitation more clearly, otherwise it’s hard to see how the method scales to typical RL benchmarks the paper seems to target.\n\nW2: As I understand it, the identifiability is only up to permutation, which is fine theoretically. But algorithmically, does that mean you need to match the learned components to semantic factors every time? If the permutation can change across runs or even across time, that could be inflexible for sequential control, where you want a stable notion of “this dimension is the joint angle” or “this is the gripper.” I get this is standard in CRL, but it’d be good to explain how you keep this stable in an RL setting.\n\nW3: Why do we need to stick to discrete actions? I see that it makes the energy-based formulation easier, but in principle the framework should also work with continuous actions, the sparsity constraint (only some factors change per action) is not inherently discrete. Since many RL domains use continuous control, it would be nice to either ex-tend to that or explain what blocks it.\n\nW4: Related to that, the evaluation would be much more convincing if you tried a more realistic setup like Distracting/Distracted DMControl [1], where the observation is not clean and the bijection basically breaks. That would directly test whether the method can still recover the controllable factors when there are distractors.\n\nW5: For controllable-state learning, there are several very relevant lines of work — bisimulation-based representation learning [2], invariance-based methods [3], denoised/abstraction MDPs [4], and especially the recent work that studies identifiability of denoised MDPs from a CRL viewpoint [5]. Since they also talk about when the controllable part is identifiable, it would be valuable to compare or at least position your assumptions and guarantees against these.\n\nW6: Finally, it would be good to show more clearly how the recovered element-wise latent space actually helps RL — especially for generalization under distractors. Prior works [3–5] usually connect “better identified controllable states” to “better downstream policy.” Here the connection is mostly implicit. A small experiment showing that better identification leads to etter policy learning would make the story much stronger.\n\n\n[1] Ortiz, Joseph, et al. \"DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors.\" Advances in Neural Information Processing Systems 37 (2024): 6574-6602.\n\n[2] Zhang, Amy, et al. \"Learning invariant representations for reinforcement learning without reconstruction.\" arXiv preprint arXiv:2006.10742 (2020).\n\n[3] Rudolph, Max, et al. \"Learning Action-based Representations Using Invariance.\" arXiv preprint arXiv:2403.16369 (2024).\n\n[4] Wang, Tongzhou, et al. \"Denoised mdps: Learning world models better than the world itself.\" arXiv preprint arXiv:2206.15477 (2022).\n\n[5] Liu, Yuren, et al. \"Learning world models with identifiable factorization.\" Advances in Neural Information Processing Systems 36 (2023): 31831-31864."}, "questions": {"value": "Other than the above points, I still have questions as below\n\n1. Now that you extensively use actions as surrogates to identify hidden variables, then will the action distirbution plays an important role here? Here I mean whether the action is diverse or expert enough, one simple case to verify could be use different versions of demonstrations in D4RL dataset and compare the identifiability quality.\n\n2. Not really a question, but this new dataset might be intersted of you, they have the latent variables for many RL domains (like robotics) and then you can also evaluate on them (this is not necessary at all for rebuttal but just give an illsurtations). \n\nChen, Guangyi, et al. \"CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations.\" arXiv preprint arXiv:2510.14049 (2025)\n\n3. I am wondering what if we also consider the reconstruction objectives in the framework? Will this empirically benefit the downstream tasks? Or similar ideas of predicting rewards/value functions in TD-MPC. Then you can essentially have the add-ons on Dreamer and TD-MPC to show this would be a fantastic add-on for world models to make them really \"identifiable\" world models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o1f6beAGJK", "forum": "B2FQwbyxIZ", "replyto": "B2FQwbyxIZ", "signatures": ["ICLR.cc/2026/Conference/Submission13657/Reviewer_h8z1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13657/Reviewer_h8z1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927624760, "cdate": 1761927624760, "tmdate": 1762924228083, "mdate": 1762924228083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Action-Controllable Factorization (ACF), a representation learning framework that tries to discover independently controllable latent variables directly from pixel inputs. The core idea is to look at how an environment changes when an action is taken compared to when it isn’t, and then use that difference to learn which parts of the latent space are affected by each action. The authors formalize this through a contrastive objective and show some theoretical results about identifiability under sparsity and independent controllability assumptions. Experiments are done on pixel-based grid worlds like Taxi, FourRooms, and MiniGrid-Doorkey. ACF seems to recover meaningful latent factors and beats several existing methods, such as GCL, DMS, and MSA, in terms of factor-identification metrics. The paper is clear, nicely written, and the visualizations are quite convincing. However, the work feels a bit incremental and mostly demonstrated on simple toy domains, which limits how strong the contribution actually is."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a well-motivated problem: how to learn controllable factors directly from pixels and it’s explained clearly.\n\n- The contrastive, action-conditioned idea is elegant and easy to implement. It fits nicely with existing energy-based or contrastive learning approaches.\n\n- The theory section is solid. The identifiability discussion helps give the method more depth, even if the assumptions are a bit idealized.\n\n- The experiments are clean and reproducible. The visualizations make it easy to see what each latent dimension corresponds to.\n\n- Overall, it’s a well-written and technically polished paper that’s pleasant to read."}, "weaknesses": {"value": "- The main idea isn’t that new. It builds pretty directly on Independently Controllable Factors (Thomas et al., 2018) and other causal representation works. The contrastive setup is nice. But I am not sure if it is a huge conceptual jump.\n\n- The experiments are very limited. Only simple grid worlds where each action clearly affects one variable. These settings make the problem almost too easy.\n\n- The paper claims that ACF helps with sample efficiency in RL, but there are no actual RL results or downstream policy experiments. That weakens the motivation.\n\n- The theoretical assumptions (independent controllability and sparsity) are quite strong and unlikely to hold in realistic domains, but the paper doesn’t really explore what happens when they fail.\n\n- Baseline comparisons are also narrow. It would help to include stronger or more modern structured RL methods to see where ACF really stands."}, "questions": {"value": "- Can you show that using ACF representations actually speeds up or improves RL training in practice?\n\n- How does ACF behave when multiple actions influence the same latent variable? Does it still find meaningful factors?\n\n- Any chance this approach could scale to more complex visual domains (like DM Control or Atari)?\n\n- How expensive is the contrastive training compared to simpler VAE-style objectives?\n\n- Would weaker or noisier interventions still work, or does ACF rely heavily on clean, discrete actions?\n\nI am willing to raise the score if the questions and weaknesses above can be well explained by the authors"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ys8dO1jhsS", "forum": "B2FQwbyxIZ", "replyto": "B2FQwbyxIZ", "signatures": ["ICLR.cc/2026/Conference/Submission13657/Reviewer_ANPg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13657/Reviewer_ANPg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939473290, "cdate": 1761939473290, "tmdate": 1762969359689, "mdate": 1762969359689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}