{"id": "95sD6KKq51", "number": 10080, "cdate": 1758160051144, "mdate": 1759897675468, "content": {"title": "ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding", "abstract": "Understanding long videos requires Multimodal Large Language Models (MLLMs) to grasp multi-timescale information, often organized in hierarchies. However, current long-video understanding benchmarks either overlook multi-timescale design or distribute questions targeting different timescales across different videos. This approach entangles timescales with video content, thereby hindering a clear assessment of MLLM multi-timescale performance. To address this, we introduce ScaleLong, the first benchmark to disentangle these factors by embedding questions targeting four hierarchical timescales\\textemdash clip (seconds), shot (tens of seconds), event (minutes), and story (hours)\\textemdash all within the same video content. This ``within-content'' multi-timescale questioning design enables direct comparison of model performance across timescales on identical videos. ScaleLong features 269 videos (avg. 86 min) from 5 main categories and 36 sub-categories, with 4–8 carefully designed questions, with at least one question targeting each timescale. Evaluating 22 MLLMs reveals a distinct U-shaped performance trend: higher accuracy at the shortest (clip) and longest (story) timescales, with a dip at intermediate levels. Furthermore, ablation studies demonstrate that increased visual token capacity consistently enhances reasoning across all timescales. ScaleLong offers a crucial fine-grained, multi-timescale benchmark for advancing MLLM capabilities in long-video understanding. The code and dataset are available at \\url{https://anonymous.4open.science/r/ScaleLong-7717}.", "tldr": "ScaleLong embeds four timescale questions in the same videos to benchmark MLLMs’ understanding, revealing a U-shaped performance curve.", "keywords": ["Multi-Timescale Benchmark", "Long Video Understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08c0a8b64d64fc3c9d84252670ef720e53f266b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ScaleLong, a new benchmark designed to evaluate multi-modal large language models on long video understanding across multiple temporal scales. Unlike prior benchmarks that either use short clips or do not explicitly disentangle temporal granularity, ScaleLong embeds four levels of questions within the same video, including clip, shot, event, and story. It enables direct comparison across timescales. The benchmark includes 269 long videos (avg. length 86 minutes) from 5 main- and 36 sub-categories, each with 4-8 carefully designed questions. Extensive experiments on 23 commercial and open-source MLLMs reveal a consistent U-shaped performance trend, where models perform well on the shortest and longest timescales but struggle at intermediate ones. The authors also provide ablation studies indicating that increasing visual token capacity improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, the problem addressed is well-motivated: existing benchmarks cannot reliably disentangle model performance across different temporal scales in long videos, and the within-content question design is a meaningful contribution.\n2. The dataset construction methodology is described clearly, including multi-stage annotation and quality control to ensure question grounding and discourage guessable answers.\n3. The experimental study is thorough, covering a diverse set of 23 models spanning multiple sizes and both closed & open-source settings, which makes the analysis convincing.\n4. The discovery of a U-shaped performance curve across timescales is interesting and insightful, and is further supported by human baselines for difficulty validation."}, "weaknesses": {"value": "1. My major concern is the size of the benchmark. It contains 269 videos, which is slightly smaller than other recent long-video benchmarks, and the paper mostly argues for \"quality over scale\". It might be useful to include more justifications, e.g., a statistical analysis of annotation diversity, to demonstrate the effectiveness and significance of such a small benchmark.\n2. The U-shaped trend is an important and interesting observation, but the explanation provided is somewhat speculative (e.g., \"models excel at short and global context but struggle mid-term\"). More controlled experiments to identify why mid-level reasoning fails would strengthen the conclusion.\n3. Given the observed U-shaped trend, I believe it might be also interesting to investigate more types of video understanding models, e.g., grounded video understanding methods [1-3] and agentic solutions [4-5], rather than running experiments solely with end2end methods. More discussions on the performance trend of different types of models shall provide more insights to the readers.\n\n[1] SeViLA: Self-Chained Image-Language Model for Video Localization and Question Answering\n[2] VideoChat-TPO: Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment\n[3] VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning\n[4] VideoAgent: Long-form Video Understanding with Large Language Model as Agent\n[5] VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding"}, "questions": {"value": "Please refer to the weakness section for my questions. Overall, I leaning to accepted this paper given the considerable quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6WVuKMmGgg", "forum": "95sD6KKq51", "replyto": "95sD6KKq51", "signatures": ["ICLR.cc/2026/Conference/Submission10080/Reviewer_dYA8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10080/Reviewer_dYA8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407826405, "cdate": 1761407826405, "tmdate": 1762921468974, "mdate": 1762921468974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ScaleLong, a benchmark for measuring the model's ability to understand content at different temporal scales (Clip, Shot, Event, and Story) in long videos. It uses five real-world video categories collected from YouTube, with an average video length of over one hour. This paper comprehensively tests existing MLLMs (both open-source and closed-source) on ScaleLong, revealing some of their shortcomings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation for this study is sound, and the comprehensive measurement of the model's ability to perceive and understand temporal information at different scales in long videos is meaningful.\n2. The paper is clearly organized and easy to understand."}, "weaknesses": {"value": "1. Long videos collected from YouTube, particularly TV and sports videos, often feature famous events. The training corpus for MLLMs may contain information about these events. How can the authors ensure that the questions they ask are not answered by the inherent knowledge contained in the model? \n2. Where do the four time scales of Clip (less than 3 seconds), Shot (4-15 seconds), Event (16-10 minutes), and Story (more than 10 minutes) and their respective time range division standards come from?\n3. (1) If the upper limit of the model input is 256 frames, for an 89-minute video, the average time range contained in each frame will reach 89*60 / 256 = 20.85 seconds, which means that many CLIPs and Shots will be ignored when sampling video frames, while MLLMs performs relatively well at the Clip-level time scale. Does this mean that many Clip-level questions are related to a large number of clips in the video? Therefore, can they be truly called Clip-level questions? (2) On the contrary, some Shot and Event-level questions may be related to dense spatio-temporal motions, so sparse frame sampling cannot cover the complete spatiotemporal clues, resulting in low model performance.\n4. In addition, can part of the reason why counting questions cannot be answered well be attributed to insufficient input frames? After all, sparse video content sampling may cause the time when objects appear to be ignored? The authors should further explore the impact of the number of input frames on different question types and timescales."}, "questions": {"value": "1. How is the error defined and measured in Section 4.4, such as missing information and spatial replacement in L425?\n2. New models, such as InternVL3 and InternVL3.5, could also be tested before the paper is submitted.\n\nIn summary, I believe the benchmark's design is sound, but some details concern me. I would be happy to discuss the points raised in the weaknesses and questions with the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ryXaUykxHI", "forum": "95sD6KKq51", "replyto": "95sD6KKq51", "signatures": ["ICLR.cc/2026/Conference/Submission10080/Reviewer_41fk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10080/Reviewer_41fk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472748218, "cdate": 1761472748218, "tmdate": 1762921468581, "mdate": 1762921468581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ScaleLong, a benchmark for evaluating multimodal large language models on long-video understanding across multiple temporal scales. Each of its 269 Creative Commons YouTube videos (average length ≈ 86 minutes) includes questions at four hierarchically defined levels: Clip, Shot, Event, and Story, allowing direct within-video comparisons of reasoning ability over time.\n\nThrough evaluation of state-of-the-art MLLMs, the study finds a consistent U-shaped performance trend: models handle short (Clip) and long (Story) reasoning relatively well but degrade sharply at intermediate timescales (Shot, Event). This reveals a systematic weakness in sustaining medium-range temporal coherence.\n\nThe ablation experiments show that temporal coverage (more frames) contributes more to performance than higher resolution, emphasising the primacy of temporal context over visual detail. The analysis of distractor errors further exposes persistent difficulties with evidential completeness and spatial grounding, even in top models.\n\nCompared with existing benchmarks (e.g., MVBench, NExT-QA, LVBench, CinePile), ScaleLong is different,  in providing multi-timescale evaluation within the same video, enabling controlled analysis of temporal reasoning rather than content variation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Comprehensive benchmark to embed questions at four hierarchical temporal scales within identical video content, enabling within-content comparison that isolates timescale effects from content variability\n2) Section 4.3.2 and Figure 3(c) analyze different frame-resolution combinations described as under \"fixed visual-token budget,\" revealing timescale-dependent optimal allocations (Clip benefits from many low-res frames; Story from balanced configurations)\n3) Ten predefined distractor types enable systematic failure mode identification, revealing specific weaknesses (missing-information 53% error, spatial-replacement 46.6% error vs. frequency-replacement 13.3% error)\n4) The paper presents empirical validation of timescale boundaries. Up-scaling experiments demonstrate sufficiency (minimal performance gain from expanded context); down-scaling experiments demonstrate necessity (VGRR 25-33% shows substantial information loss from restricted context)\n5) The benchmark consists of 269 videos averaging 86 minutes across 36 subcategories address genuine gap in existing benchmarks which mainly focus on short clips or lack multi-timescale evaluation."}, "weaknesses": {"value": "1) Humans evaluated on \"Whole Video\" (continuous playback, ~150,000 frames) while models receive sparse samples (32-256 frames). The 23.1-point human-model gap may conflate access differences with capability differences , thus making it hard to interpret.\n\n2) Given 5 task families × 4 scales × many categories, some slices will be small. The authors make strong qualitative claims (e.g., U-shape) but the paper has no per-slice CIs/bootstraps and no inter-annotator agreement figures for annotation reliability. This practically limits how confidently we can generalize beyond aggregate means.\n\n3) Each video contains 4-8 questions spanning timescales, but the paper never states whether the questions are evaluated independently (fresh context) or sequentially (shared context).\n\n4) The paper reports no quantitative reliability metrics (percent agreement) despite requiring \"independent approval from two additional annotators.\" Timescale categorization requires subjective judgment about temporal boundaries, without reliability validation, cannot assess measurement quality of fundamental category labels.\n\n5) The paper does not clearly state how frames are selected from 86-minute videos. In my understanding, ifferent strategies systematically advantage different timescales: 32 frames from 5,160 seconds requires explicit sampling algorithm specification for validity."}, "questions": {"value": "1) Do the Appendix A.3 timescale validation results replicate across architecturally diverse models beyond InternVL-2.5-8B?\n\n2) Can you provide human performance when restricted to the same sparse frame samples models receive, to decompose the 23.1-point gap into access vs. capability components?\n\n3) What are the explicit duration ranges or decision rules you provided to annotators for each of the four levels (especially Event vs Story)? A short table with time bounds/examples would help others replicate the labeling policy.\n\n4) The paper ensures ≥1 question per scale per video and cover five task families. But, it is not clear how does the authors balance task types across scales (e.g., Causal vs. Counting at Event vs. Story) to avoid systematic pairing effects that could bias the U-shape? A per-scale task histogram would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2nUr8tlgMR", "forum": "95sD6KKq51", "replyto": "95sD6KKq51", "signatures": ["ICLR.cc/2026/Conference/Submission10080/Reviewer_6e8r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10080/Reviewer_6e8r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604765560, "cdate": 1761604765560, "tmdate": 1762921468244, "mdate": 1762921468244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce ScaleLong, a long-video benchmark that embeds questions targeting four hierarchical timescales, allowing direct comparison of model ability across temporal granularities. It contains 269 ~86-minute videos spanning 5 categories/36 subcategories, each with 4–8 carefully curated questions, rigorous quality control, and tasks covering causal reasoning, object/action understanding, summarization, and counting. Evaluating 23 MLLMs reveals a consistent U-shaped trend - stronger on the shortest and longest scales, weaker at intermediate shot/event levels - with ablations showing that allocating more visual tokens improves performance across timescales."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel Temporal Framework\nThe paper’s hierarchical division of video understanding into clip-, shot-, event-, and story-level timescales is a novel and insightful framework that enables fine-grained analysis of temporal reasoning abilities within a single benchmark.\n\nInteresting ablation studies\nThe ablation experiments are thoughtfully designed to show interesting insights about the effects of visual token allocation and temporal coverage\n\nRigorous Dataset Construction\nThe dataset creation process is meticulous, featuring multi-stage quality control, diverse content selection, and balanced question design to ensure fairness and reliability across temporal levels."}, "weaknesses": {"value": "Limited Illustrative Examples\nThe paper provides only a small number of examples from the benchmark, which makes it difficult to fully appreciate the nuances of question design across different time scales.\n\nInconsistent Story-Level Definition\nAlthough the paper claims that story-level questions require holistic narrative understanding, the provided example of a story level question and answers focuses on a sequential event listing rather than true integration of information from the entire video. \n\nInsufficient Distractor Analysis – The distractor creation process lacks detailed evaluation of more superficial distractor properties (e.g., length of the answers). Given that large language models can exploit superficial answer features - such as selecting the longest or most specific option - further investigation into distractor balance would strengthen the benchmark’s validity. The annotator instructions state  to design distractors with length in mind but there is no statistics on it to verify."}, "questions": {"value": "My questions are based mainly on weaknesses - more examples, more statistics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZU4nHYaX59", "forum": "95sD6KKq51", "replyto": "95sD6KKq51", "signatures": ["ICLR.cc/2026/Conference/Submission10080/Reviewer_dL49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10080/Reviewer_dL49"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163001537, "cdate": 1762163001537, "tmdate": 1762921468023, "mdate": 1762921468023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}