{"id": "YKOMMh2pSg", "number": 9532, "cdate": 1758126343532, "mdate": 1759897713724, "content": {"title": "QRIM: Quantum Robust Inner Minimization for Reinforcement Learning", "abstract": "Reinforcement learning (RL) often fails when faced with unexpected environmental changes that were unseen during training. Robust reinforcement learning (RRL) tackles this challenge by optimizing policies against the worst-case scenario defined within an uncertainty set. However, RRL remains impractical due to the cost of the Max-Min optimization, where it suffers from the query complexity for exhaustively finding the worst-case (dubbed ‚ÄòMin‚Äô) within the uncertainty set ùí∞, i.e., O(|ùí∞|). By viewing this via a lens of quantum perspective, we raise a pivotal question: If we can query from the environment with quantum superpositions, is it possible to accelerate the Max-Min optimization of RRL? Our answer is ‚ÄòYes‚Äô. Our method, called quantum robust inner minimization (QRIM), encodes the uncertainty set with quantum superposition and amplifies low-return cases, thus enabling RL for solving the robust (i.e., worst-case) Bellman equation. Importantly, QRIM achieves a quadratic speed-up in query complexity without altering the outer RL pipeline, i.e., O(‚àö|ùí∞|). In both classical emulation of the quantum-access oracle and full quantum routines, QRIM learns more robust policies for unseen task variations than classical RL methods, while achieving a quadratic reduction in query complexity compared to classical RRL methods.", "tldr": "", "keywords": ["Reinforcement learning", "Quantum computing"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6243ee624e59d4a90f95ab4e22d8d9bc633a17c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work deals with the potential benefits of quantum computing for RL. It demonstrates that in the case of robust RL, where worst-case performance is optimized across a set of potential environments, quantum techniques can achieve a quadratic advantage in sample efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find it particularly strong that the work highlights another area in which QC can offer an advantage for RL, especially since it is a practice-relevant area (robust RL)."}, "weaknesses": {"value": "I see no real weakness, but the integration with current similar work could be a bit stronger. In particular, an outlook or assessment of further development and the use of real quantum hardware would be beneficial.\nFor example, [1], in which the algorithm from [2], which, like the present work, demonstrates a quadratic advantage in sample efficiency for RL, was investigated in minimal size on real quantum hardware.\n\n\nFurther:\n\"kernal\" -> \"kernel\"\n\"markov\" -> Markov\" (use {Markov})\n\"wasserstein\" -> \"Wasserstein\"\n\"bellman\" -> \"Bellman\"\n\n\n[1] D. Hein et al., From Classical Data to Quantum Advantage--Quantum Policy Evaluation on Quantum Hardware, 2025\n\n[2] S. Wiedemann et al., Quantum Policy Iteration via Amplitude Estimation and Grover Search ‚Äì Towards Quantum Advantage for Reinforcement Learning, 2023"}, "questions": {"value": "* Would you rather place your work in online RL or offline RL?\n* Would mentioning the QRL survey [3] be useful, or does that not fit with the chosen grouping of QRL areas?\n\n[3] N. Meyer et al., A survey on quantum reinforcement learning, 2022"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BHNAzIuJFk", "forum": "YKOMMh2pSg", "replyto": "YKOMMh2pSg", "signatures": ["ICLR.cc/2026/Conference/Submission9532/Reviewer_YZjp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9532/Reviewer_YZjp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761070013582, "cdate": 1761070013582, "tmdate": 1762921096418, "mdate": 1762921096418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of the high computational cost associated with Max-Min optimization in robust reinforcement learning (RRL). The authors investigate whether it is possible to accelerate the Max-Min optimization process by leveraging a quantum oracle for the environment. The paper demonstrates that the proposed approach achieves a quadratic speed-up in query complexity compared to classical RRL methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The manuscript is very well-written, structured, and easy to follow. The authors present QRL and RRL concepts in a clear and engaging manner. Algorithms 1 and 2 are particularly effective in bridging various components of the proposed approach. The experimental results obtained on the two benchmark environments are impressive and substantiate the claims made."}, "weaknesses": {"value": "However, the paper lacks a detailed explanation of how the quantum environment oracles are constructed for the benchmark environments, FrozenLake and CartPole. Given that the quantum-accessible environments form the core of the proposed approach, more comprehensive information on this aspect is essential. In this context, the authors could consider referencing the following research, which demonstrates a practical implementation of a quantum oracle in reinforcement learning:\n\nSimon Wiedemann, Daniel Hein, Steffen Udluft, and Christian B. Mendl. Quantum policy iteration via amplitude estimation and Grover search ‚Äì Towards quantum advantage for reinforcement learning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.\nMinor Issues:\nAlgorithm 2: The usage of epsilon ($\\epsilon$) lacks a subscript, which should be added for clarity.\nPage 6: The final sentence on this page includes ‚Äú(dm control CartPole Swingup),‚Äù which could be rephrased for better clarity and to avoid potential confusion.\nPage 9: There is a typographical error in Fig. 5, where ‚ÄúCartplole‚Äù should be corrected to ‚ÄúCartPole.‚Äù"}, "questions": {"value": "How are the quantum oracles of the benchmark environment constructed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RxNEMFIMV0", "forum": "YKOMMh2pSg", "replyto": "YKOMMh2pSg", "signatures": ["ICLR.cc/2026/Conference/Submission9532/Reviewer_YhQS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9532/Reviewer_YhQS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838747706, "cdate": 1761838747706, "tmdate": 1762921095111, "mdate": 1762921095111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a quantum based approach to solving the inner minimization problem of robust reinforcement learning algorithms, that promises . The method assumes a quantum-accessible environment that enables the use of quantum minimum finding to speed up searching the uncertainty set for parameters that minimize the resulting reward. Two simple environments are used for evaluation: one with discrete, and one with continuous actions. The results show quadratic speed up in number of oracle calls."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The method is well motivated and preliminaries are introduced concisely. Assumptions are also clearly delineated. The computational complexity in terms of the number of queries, error and failure probability is given with proof. Experimental setup is chosen adequately to show the promised improvement empirically.\n\nClever use of quantum entanglement results in a tangible quantum advantage. The most convincing strength of this paper is the possibility of drop-in replacement into any existing robust RL algorithm (given the assumptions hold)."}, "weaknesses": {"value": "The method assumes a quantum-accessible environment model which is rather restrictive. \nThe number of environments used for evaluation is limited. Using more environments with larger action spaces would further improve the quality of the paper. \n\nMinor errors:\n- Section 3.2: \"kernal\"\n- Section 3.3: \"We stand on\" worded unnaturally?\n- Section 5.1: \"Robust inner minimization is applied at the appropriate temporal... \" missing a word?\n- Section 6: \"minimin\""}, "questions": {"value": "- What is the symbole $\\Delta_{S}$ in Definition 1?\n- Can you explain in more detail what the \"aux\" qbits are used for?\n- In the CartPole environment, the worst-case performance of the proposed method is significantly worse than for the baseline models. Can you identify a reason for this gap?\n- You are reporting **median** coherent oracle calls to assess the query complexity. Is the variance of the query count large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h6P8eqTbB2", "forum": "YKOMMh2pSg", "replyto": "YKOMMh2pSg", "signatures": ["ICLR.cc/2026/Conference/Submission9532/Reviewer_29uc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9532/Reviewer_29uc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930855686, "cdate": 1761930855686, "tmdate": 1762921094784, "mdate": 1762921094784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the advantages of modeling environment interactions in reinforcement learning using quantum computing concepts. The authors formalize uncertainty in robust RL objectives through a quantum-inspired representation based on superposition, which is used to accelerate the inner minimization problem and potentially improve sample efficiency in quantum-compatible environments.\n\nThe proposed approach aims to extend quantum reinforcement learning (QRL) beyond the design of quantum-enhanced policies, suggesting that environmental uncertainty and stochastic dynamics can also be modeled using quantum phenomena. This is an intriguing idea that could bridge the conceptual gap between quantum mechanics and environment modeling in RL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper engages with a timely and conceptually rich topic, namely the use of quantum principles for modeling environmental uncertainty in RL.\n- The proposed formulation could help frame future discussions on quantum-compatible environment representations and robustness in hybrid quantum-classical learning setups.\n- The theoretical framing is reasonably well-motivated, even if the presentation is difficult to follow."}, "weaknesses": {"value": "- Incremental novelty: The core idea builds upon earlier proposals to apply quantum concepts to environment modeling. The contribution here is a modest extension, not a genuinely new paradigm.\n- Limited empirical support: Experimental results are largely comparable between classical RL and classical robust RL. The claimed benefits (improved robustness, faster convergence) are not demonstrated convincingly.\n- Poor organization: The structure obscures the motivation and significance of the method. The overall narrative could benefit from clearer framing and reordering.\n- Lack of illustrative examples: A small, well-chosen toy example could clarify the proposed mechanism and highlight the degradation the authors aim to mitigate.\n- Missing discussion of limitations: The paper does not adequately address the scalability challenges or hardware constraints of the proposed approach."}, "questions": {"value": "1. Hardware applicability: Is the approach intended for near-term NISQ devices or future fault-tolerant architectures?\n2. Noise modeling: Were the reported experiments noise-free simulations or did they incorporate realistic hardware noise models?\n3. Demonstrated advantage: Can the authors show a specific setting where the proposed uncertainty representation yields a measurable policy improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kz58Vkoyl5", "forum": "YKOMMh2pSg", "replyto": "YKOMMh2pSg", "signatures": ["ICLR.cc/2026/Conference/Submission9532/Reviewer_zi4K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9532/Reviewer_zi4K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000796200, "cdate": 1762000796200, "tmdate": 1762921094407, "mdate": 1762921094407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}