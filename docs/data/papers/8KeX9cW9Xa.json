{"id": "8KeX9cW9Xa", "number": 7335, "cdate": 1758016509684, "mdate": 1759897859188, "content": {"title": "Discerning Minds or Generic Tutors? Evaluating Instructional Guidance Capabilities in Socratic LLMs", "abstract": "The conversational capabilities of large language models hold significant promise for enabling scalable and interactive tutoring. While prior research has primarily examined their ability to generate Socratic questions, it often overlooks a critical aspect: adaptively guiding learners in accordance with their cognitive states. This study moves beyond question generation to emphasize instructional guidance capability. We ask: Can LLMs emulate expert tutors who dynamically adjust strategies in response to learners' states? To investigate this, we propose GuideEval, a benchmark grounded in authentic educational dialogues that evaluates pedagogical guidance through a three-phase behavioral framework: (1) Perception, inferring learner states; (2) Orchestration, adapting instructional strategies; and (3) Elicitation, stimulating proper reflections. Empirical results indicate that existing LLMs often fail to provide effective adaptive scaffolding when learners experience confusion or require redirection. To complement the quantitative evaluation, we conduct a detailed failure case analysis, providing an intuitive understanding of these shortcomings. Furthermore, we introduce a behavior-guided finetuning strategy that leverages behavior-prompted instructional dialogues, substantially enhancing guidance performance. By shifting the focus from isolated content evaluation to learner-centered state-aware interaction, our work advocates a more dialogic paradigm for evaluating Socratic LLMs.", "tldr": "This study introduces GuideEval, a benchmark for evaluating Socratic LLMs’ ability to provide adaptive instructional guidance beyond question generation.", "keywords": ["Socratic LLM Evaluation; Educational Intelligent Tutoring; Cognitive State-aware Evaluation; Behavior-guided Finetuning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45b8699259d4cda5ec1e78fac41804744b9fd836.pdf", "supplementary_material": "/attachment/83ed59f60543c074df2beb1c7ec8d1ed41695b29.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces GuideEval, a benchmark to evaluate the tutoring capabilities of LLMs across three stages based on interactions with real students.\nThe authors use the benchmark to evaluate current commercial and open-source LLMs and find, for example, that they struggle to adapt to learner states, especially when critiquing wrong solutions.\nThe authors also use their data to finetune an LLM and show that finetuning improves these shortcomings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The research area is severely lacking resources that use real students (mainly due to ethical and regulatory problems / hurdles), so releasing a dataset of this scale with real students is quite impactful.\n\n- The results show interesting observations for the future, for example, they point out the sycophancy of current LLMs which can be bad for learning."}, "weaknesses": {"value": "- In general, the discussion of related work is a bit short and many findings of the paper have been discovered in prior works (though in different form). For example, Wang et al. 2023 already shows that LLMs tend to not actively engage with student error patterns and Daheim et al. 2024 show that identifying student mistakes is a challenging task even for sota LLMs. Another example is Dinucu-Jianu et al. 2025 who show that there exists a trade-off between pedagogy and giving hints (here the authors also point out that only focusing on not telling solutions leads to poor teaching). Scarlatos et al. 2025 also use DPO but this is not discussed, too. \nI don't think that this takes away from the papers findings in general but proper discussion would improve it.\nThe authors also do not discuss knowledge tracing but the goal of this field is precisely what the paper sets out: to adapt to current learner states. I think a discussion of such concepts could also be helpful.\n\n- I am wondering about more details about the dataset, many seem missing, as the discussion seems limited to Sec. 2.3.\nFor example, no annotator agreement is reported and beyond the number of turns and dialogues there are no further statistics. I think even reporting simple statistics would be helpful. The domain is also not specified beyond saying that it consists of middle school science questions. It is also mentioned that humans verified and even edited utterances (to create negatives) but the exact process is not detailed.\n\n- The agreement between LLM and humans is fairly high but only checks binary preference over a combination of metrics and not agreement for a specific metric which limits expressiveness for how reliable the individual metrics are."}, "questions": {"value": "- How many examples were used for creating Tab. 3?\n\n- Will you release the dataset publicly? Based on the conclusion I would assume so but I did not find it written anywhere.\n\n## References\n\nWang et al., Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes, NAACL 2024\n\nDaheim et al., Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors, EMNLP 2024\n\nDinucu-Jianu et al., From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning., arXiv 2025\n\nScarlatos et al., Training llmbased tutors to improve student learning outcomes in dialogues, AIED 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eIxOvbVZME", "forum": "8KeX9cW9Xa", "replyto": "8KeX9cW9Xa", "signatures": ["ICLR.cc/2026/Conference/Submission7335/Reviewer_m5P2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7335/Reviewer_m5P2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760518733069, "cdate": 1760518733069, "tmdate": 1762919455210, "mdate": 1762919455210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducted an extensive evaluation of the capabilities of LLMs in guiding learners and dynamically adapting its responses to the learners' states. To achieve this, they also collected a benchmark dataset of real multi-turn dialogues of learners from a Socratic tutoring platform."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper conducted an extensive analysis of various LLMs and provides several insights on their capability to recognize learner states, to guide / scaffold, and to elicit further follow-ups.\n- The collected dataset GuideEval can help advance the field further.\n- LLM-based scoring were validated with human annotations\n- The failure analysis provides useful insights"}, "weaknesses": {"value": "- The authors evaluated the consistency between LLM based scoring and the Human annotators using the proportion of the same labels. I am not sure if this is the right way to go about it since simply showing the proportion of agreement can be misleading, especially if there is an imbalance in the label distribution. I believe there are more appropriate inter-rater agreement metrics that account for these.\n- The failure analysis categorizes the types of failures but the authors did not seem to provide the frequencies of occurrence for each type of failure category. \n- The authors only measured how the LLMs responded. For example, P-affirm and P-redirect scores are only based on how the LLM responded. But this does not tell us whether or not the LLM can recognize the learner states. It might be the case that the LLM does indeed recognize but just does not know how to respond properly."}, "questions": {"value": "- How well can LLMs recognize the learner states (independent of how affirmative their responses are)?\n- What is the distribution of failure types for each of the LLM models? Do they fail in similar ways?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The authors collected a dataset of learner dialogues. The responses were anonymized.\nNot sure if this warrants an ethics review, but I'm filling this in since it seems to matches one of the categories (human subjects) for ethics review."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f7wsxxf2K2", "forum": "8KeX9cW9Xa", "replyto": "8KeX9cW9Xa", "signatures": ["ICLR.cc/2026/Conference/Submission7335/Reviewer_2FLd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7335/Reviewer_2FLd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921194473, "cdate": 1761921194473, "tmdate": 1762919454516, "mdate": 1762919454516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GuideEval, a benchmark for evaluating instructional guidance capabilities of large language models (LLMs) when serving as Socratic tutors. The authors argue that existing evaluations focus primarily on question generation while overlooking adaptive guidance—the ability to dynamically adjust teaching strategies based on learners' cognitive states. The paper proposes a three-phase behavioral framework: (1) Perception - inferring learner states (accurate/erroneous/comprehension/confusion); (2) Orchestration - adapting instructional strategies through scaffolding; and (3) Elicitation - stimulating deeper thinking through strategic questioning.\n\nThe authors construct a dataset of 5,177 test samples from authentic tutoring dialogues with contrastive student state pairs, enabling controlled evaluation of model adaptivity. They evaluate 14 LLMs across 6 metrics derived from their framework, finding that models struggle with error detection, adapting to implicit cognitive cues, and maintaining consistent guidance strategies. The paper includes detailed failure pattern analysis and demonstrates that behavior-guided finetuning with Chain-of-Thought distillation substantially improves guidance performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on critical gap in LLM tutoring evaluation by focusing on adaptive guidance rather than static content quality.\n\n2. The three-phase model is well-motivated by educational psychology literature and operationalized into measurable metrics.\n\n3. The exp covers 14 diverse models, revealing consistent failure patterns across architectures.\n\n4.The paper contains a detailed failure case analysis, providing an intuitive understanding beyond quantitative metrics.\n\n5. I really like the comparative analysis of different training strategies part. The finding that outcome-only SFT degrades performance while process supervision (CoT Distillation) and pairwise preference optimization (DPO) provide substantial gains is a critical, actionable insight for the community."}, "weaknesses": {"value": "1. it comes with limited scope: dataset topic - middle school science problems in Chinese. It would be more curated if you expand it to other difficulty levels and languages.\n\n2. The cognitive modeling with 4 states (Accurate, Erroneous, Comprehension, Confusion) may be too simplified to capture nuanced learning states. As authors acknowledge, it doesn't capture individual learner profiles, misconception history, or engagement patterns"}, "questions": {"value": "1. Have you tested or do you plan to test this framework on other domains (e.g., humanities, programming) or age groups? What challenges do you anticipate?\n\n2. Can you provide comparison with human tutor performance on the same benchmark? This would contextualize LLM performance.\n\n3. The \"Designed Prompt\" used to generate the training data for the finetuning experiments is very explicit and rule-based. How can we be sure that the model learned a generalizable instructional guidance capability rather than just learning to mimic the explicit rules baked into the generation prompt? Have you tested or are you planning to test the finetuned model on out-of-domain tasks to see if the \"skill\" transfers?\n\n4. The filtering mechanism for training data (equations on p.18) is complex. How sensitive are results to these hyperparameters (β, threshold)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3eKM2lXtUF", "forum": "8KeX9cW9Xa", "replyto": "8KeX9cW9Xa", "signatures": ["ICLR.cc/2026/Conference/Submission7335/Reviewer_y1Aq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7335/Reviewer_y1Aq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950026584, "cdate": 1761950026584, "tmdate": 1762919453660, "mdate": 1762919453660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether LLM tutors can deliver adaptive instructional guidance in Socratic dialogues rather than relying on generic questioning. The authors conceptualize guidance as a three-phase behavior: Perception (inferring the learner’s state), Orchestration (selecting an appropriate next-step strategy), and Elicitation (formulating prompts suited to the learner’s state). Building on this framework, they introduce GuideEval, a benchmark composed of real multi-turn tutoring dialogues with contrastive student states, and define phase-aligned evaluation metrics to assess model performance across these dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear behavioral decomposition with actionable metrics. The three-phase split translates “be a better tutor” into concrete, checkable behaviors, offering conceptual clarity and operational guidance that enable reproducible, phase-wise diagnosis across different models.\n\n2. Useful failure taxonomy grounded in qualitative evidence. The paper goes beyond reporting average behaviors and highlights failure modes supported by dialogue snippets, providing interpretability and practical insight into model behavior."}, "weaknesses": {"value": "1. Human–LLM agreement is reported without sample size or reliability statistics. In Table 3, the claim that “LLMs can serve as reliable and scalable evaluators of instructional behaviors” rests on high agreement ratios and minimal score deviations, but the paper omits sample size per metric or level, sampling protocol, number of human raters, and inter-rater reliability. Without these, chance agreement and selection bias cannot be ruled out, especially with coarse labels (binary or 3-point) that inflate raw agreement.\n\n2. Prompt–rubric inconsistency. Generation prompts forbid giving final answers (“Do not directly provide the final answer or full solution process” in Original/Rule templates), yet the O-Advance evaluation rubric awards full credit when “the model provides the final answer.” This contradiction allows models to achieve high orchestration scores while violating generation rules. Please align the prompt and rubric definitions.\n\n3. All models are decoded at temperature = 0.1. Please justify this setting and provide an ablation across temperatures to ensure conclusions are not artifacts of low-variance decoding.\n\n4. Human annotators reportedly revised model outputs to create “state-edited counterparts” (e.g., answer-correctness flips, comprehension/confusion flips). Please specify the exact editing operations and provide per-operation statistics, including counts, edit distance, token-level change distribution, and human vs. synthetic proportions."}, "questions": {"value": "1. What are the details of sample size per metric or level, sampling protocol, number of human raters, and inter-rater reliability statistics?\n\n2. Generation prompts forbid giving final answers, while the O-Advance rubric rewards them. How are these conflicting criteria reconciled to ensure consistent evaluation?\n\n3. How many items per metric were used for the LLM–human consistency analysis, and how were they sampled from the full dataset?\n\n4. Could you replicate headline results using at least one alternative judge and report inter-judge agreement and sensitivity to confirm robustness?\n\n5. Since training and evaluation data are drawn from the same source pool, do problem contexts (e.g., identical or near-duplicate problem IDs, stems, passages, or scaffolds) repeat across splits? If so, what is the overlap rate, and how does it affect evaluation outcomes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gvdPG42QQM", "forum": "8KeX9cW9Xa", "replyto": "8KeX9cW9Xa", "signatures": ["ICLR.cc/2026/Conference/Submission7335/Reviewer_QMx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7335/Reviewer_QMx2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976282933, "cdate": 1761976282933, "tmdate": 1762919452942, "mdate": 1762919452942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}