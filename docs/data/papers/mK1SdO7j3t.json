{"id": "mK1SdO7j3t", "number": 19263, "cdate": 1758294885818, "mdate": 1759897048761, "content": {"title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model", "abstract": "Recently, augmenting Vision-Language-Action models (VLA) with world modeling has shown promise in improving robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLA models across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while still enabling cross-modal knowledge sharing.\nIn addition, we introduce independent noise perturbations for each modality and a decoupled flow-matching loss. This design enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Based on the decoupling of modalities during training, we also introduce a joint sampling method that supports test-time scaling, where action and vision tokens evolve asynchronously at different rates. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6\\% gains over baseline methods, while our test-time scaling approach provides an additional 2–5\\% boost. On real-world tasks with the Franka Research 3, DUST improves success rates by 13\\%, confirming its effectiveness beyond simulation. Furthermore, pre-training on action-free videos from BridgeV2 yields significant transfer gains on RoboCasa, underscoring DUST’s potential for large-scale VLA pretraining.", "tldr": "We introduce dual-stream diffusion with independent noise schedules to jointly model actions and future states, improving VLA model performance.", "keywords": ["vision-language-action models", "world modeling", "diffusion models", "robot learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bb4a45f65de25df4e86cfb9f43b4729c6a6e386.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DUST, a world-model–augmented VLA framework that resolves modality conflicts by using a dual-stream diffusion transformer with separate yet cross-sharing modality streams, independent noise per modality, and a decoupled flow-matching loss—learning the joint distribution without a unified latent space. A joint sampling method enables test-time scaling with asynchronous evolution of action and vision tokens, yielding up to 6% gains on RoboCasa and GR-1, an additional 2–5% boost from scaling, and a 13% success-rate improvement on real Franka tasks; action-free video pretraining (BridgeV2) further enhances transfer on RoboCasa."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Method: The paper proposes a Dual-Stream Diffusion framework for unified vision-language-action modeling. Two diffusion streams—visual and action—are trained in parallel. This dual-branch design provides a fresh direction for integrating perception and control under diffusion-based generative modeling.\n\nWriting: The paper is well written, the method is clearly presented, and the figures/tables are complete and easy to read.\n\nExperiments: The ablation studies are well conducted and validated the effectiveness of model architechture."}, "weaknesses": {"value": "The primary concern is how the proposed dual-stream diffusion compares to VLM-based unified understanding–prediction VLAs. In addition to DiT-based image-prediction policies (e.g., PAD, VPP), there now exist VLM-driven unified VLA models (e.g., UP-VLA[1], DreamVLA[2]) that likewise address modality conflicts and improve performance across diverse tasks, with the added benefit of strong semantic generalization. The current evaluation focuses on the former class while omitting the latter, which is increasingly becoming the mainstream baseline for comparison.\n\nFurthermore, the work lacks results on mainstream simulation environments (e.g. Calvin, SimplerEnv, and Libero), preventing a fair comparison with advanced models, including the above methods and the pi0 series.\n\n[1] Zhang J, Guo Y, Hu Y, et al. Up-vla: A unified understanding and prediction model for embodied agent[J]. arXiv preprint arXiv:2501.18867, 2025.\n\n[2] Zhang W, Liu H, Qi Z, et al. Dreamvla: a vision-language-action model dreamed with comprehensive world knowledge[J]. arXiv preprint arXiv:2507.04447, 2025."}, "questions": {"value": "1. Primary concerns about methods and experiments can be seen in weaknesses.\n2. Missing demos on real-world experiments. From the reported details, the real-world evaluation is confined to pick-and-place and primarily in-domain, which limits evidence for semantic and skill generalization. In addition, the real-robot results do not include comparisons against advanced baselines, making it difficult to contextualize performance relative to the current state of the art.\n3. The authors mention DUST’s high-frequency inference. How does DUST improve inference efficiency compared to other methods, and is there a concrete speed comparison across approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5BrLQQ3fzm", "forum": "mK1SdO7j3t", "replyto": "mK1SdO7j3t", "signatures": ["ICLR.cc/2026/Conference/Submission19263/Reviewer_uhej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19263/Reviewer_uhej"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489220208, "cdate": 1761489220208, "tmdate": 1762931231001, "mdate": 1762931231001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DUST, a vision-language-action (VLA) framework designed to address modality conflicts through joint world modeling and action prediction. DUST employs a VLM-based modality encoder to extract semantic representations from visual and language inputs, while an MMDiT model conditioned on these features predicts future images and action sequences. This design decouples modality generation while preserving cross-modal knowledge transfer. Experiments conducted on RoboCasa, GR-1 simulated benchmarks, and real-world settings demonstrate that DUST outperforms existing baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Propose the dual-stream multimodal diffusion transformer for action and image prediction, and the ablation process also demonstrated the effectiveness of separately processing different modes of propagation.\n- DUST has achieved superior performance over the backbone method in two simulated environments.\n- DUST can benefit from pretraining on internet-scale data, as ablation studies show."}, "weaknesses": {"value": "- **Inadequate ablation analysis**  \n  The ablation experiments lack of the using of VLM module. Please supplement the relevant experiments by replacing LLM with a common language model (such as the settings of PAD or MDT).\n\n- **Lack of real world demonstration**  \n The real-world experiments involve relatively simple tasks and scenarios. Moreover, the absence of demonstration videos raises concerns about the model’s real-world performance.\n\n- **Insufficient baselines**  \n Due to insufficient baselines, the results may lack persuasiveness."}, "questions": {"value": "1. How do other new VLA approach perform on GR-1 benchmark? And supplement some baseline methods (π₀, MDT, Seer, PAD) for the experiment.\n2. In real-world experiments, for tasks with low success rates (such as task 4), how do the DUST fail? Please provide the corresponding failure case video"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Although DUST exhibits significant improvement over the baseline in a simulated environment, it merely incorporates some common techniques and modules based on existing work. The simplicity of real-world experiments and the lack of video demonstrations also raise concerns about the model's performance."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XT8lzc6KoE", "forum": "mK1SdO7j3t", "replyto": "mK1SdO7j3t", "signatures": ["ICLR.cc/2026/Conference/Submission19263/Reviewer_86nD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19263/Reviewer_86nD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743117088, "cdate": 1761743117088, "tmdate": 1762931230460, "mdate": 1762931230460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DUST, a Dual-Stream Diffusion framework that augments Vision-Language-Action (VLA) models with explicit world modeling.\nUnlike prior works that either unify modalities in a shared latent space (PAD, EnerVerse) or separate them with one-way conditioning (Video Policy, FLARE), DUST introduces a dual-stream multimodal diffusion transformer that preserves separate vision and action streams while enabling bidirectional information exchange through shared attention layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The dual-stream diffusion structure elegantly balances modality decoupling with cross-modal communication\n2. The asynchronous denoising method during test-time is useful.\n3. The presentation is clear with nice figures, the writing is easy to follow."}, "weaknesses": {"value": "1. Real-world validation is only conducted on four pick-and-place tasks. Including a broader range of tasks could further verify the effectiveness of the proposed framework. (Given the limited rebuttal phase, the authors do not need to add additional real-world experiments.)\n2. The paper lacks direct comparisons with the most relevant baselines, such as PAD, Video Policy, Video Prediction Policy, and UAV."}, "questions": {"value": "1. Could the authors include the most relevant baselines that also use prediction-based methods to enhance VLA models?\n\n2. The authors may also consider evaluating on more widely used benchmarks such as CALVIN and LIBERO. Since these benchmarks are approaching limit, it is acceptable not to achieve state-of-the-art performance, but the results should at least be comparable to prior advanced methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0aAKcODt0h", "forum": "mK1SdO7j3t", "replyto": "mK1SdO7j3t", "signatures": ["ICLR.cc/2026/Conference/Submission19263/Reviewer_qvHU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19263/Reviewer_qvHU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808622104, "cdate": 1761808622104, "tmdate": 1762931229963, "mdate": 1762931229963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DUST presents a well-designed approach to integrating world modeling into Vision-Language-Action frameworks through a dual-stream diffusion architecture. The decoupled yet interactive modality design is conceptually sound and empirically effective, showing consistent performance gains across simulation and real-world tasks. However, while the results are promising, the methodological novelty appears moderate given the growing body of diffusion-based multimodal policy learning research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I think this paper made a good summary for the world model-based VLAs.\n2. Frankly speaking, the number of experiments is quite a lot."}, "weaknesses": {"value": "1. Overall, the experimental evaluation is required to improved. There are only pick and place tasks.\n2. How about the control frequency? \n3. I do not think there are significant difference between (b) and (C) in Figure 1."}, "questions": {"value": "1. Without more experiments about non-pick-and-place tasks. This paper is not acceptable.\n2. Do you have any pretraining stage?\n3. You should use pi0 as your baseline model, especially you use a similar architecture to pi0. or another diffusion-based VLA. \n\n\nMy major concern is the experimental evaluation and unsuitable baselines. Therefore, I did not think this approach is well evaluated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vs8MLoucfS", "forum": "mK1SdO7j3t", "replyto": "mK1SdO7j3t", "signatures": ["ICLR.cc/2026/Conference/Submission19263/Reviewer_ZXAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19263/Reviewer_ZXAK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921332518, "cdate": 1761921332518, "tmdate": 1762931229251, "mdate": 1762931229251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}