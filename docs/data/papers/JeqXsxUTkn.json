{"id": "JeqXsxUTkn", "number": 4145, "cdate": 1757611551088, "mdate": 1759898050889, "content": {"title": "SPOT: Structured Prompting with Object-centric Tokens for open-world scene graphs", "abstract": "Scene graphs provide a compact and structured representation of visual scenes by capturing objects and their relationships, making them valuable for downstream tasks in vision-language reasoning and robotics. While early work focused on closed-vocabulary settings, newer efforts have shifted toward open-world scene graph generation (SGG) to better handle diverse real-world scenarios. Recent works explore leveraging VLMs and LLMs in open-world settings for their broad, openvocabulary knowledge. However, existing approaches often rely on proprietary models like GPT-4o and are limited by the unstructured output behavior and weak spatial and object-level reasoning capabilities of pretrained models. We introduce SPOT, a structured prompting framework that augments open-source VLMs with spatial reasoning abilities for scene graph generation with minimal training. By combining object-centric visual features with the model’s knowledge priors, SPOT achieves competitive or superior relation prediction compared to large proprietary models. Additionally, SPOT demonstrates strong cross-domain generalization, including extension to 3D scenes. Our approach is built upon open-source models, offering a scalable and accessible framework for harnessing VLMs for SGG.", "tldr": "This paper introduces SPOT, a novel framework that uses structured prompts and object-centric visual features to achieve superior spatial reasoning for scene graph generation.", "keywords": ["Scene graph generation", "Open-vocabulary learning", "Spatial reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d68407c1e7911ef1eabb05842a80e8c4d880336.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SPOT (Structured Prompting with Object-centric Tokens), a novel, fully open-source framework for Open-World Scene Graph Generation (SGG) that leverages Vision-Language Models (VLMs). The core problem addressed is that existing VLM/LLM-based SGG methods often rely on proprietary models (like GPT-4o), use unstructured prompts, and struggle with weak spatial reasoning, leading to vague or physically implausible relation predictions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strong Spatial Grounding: The core innovation of using Object-Centric Visual Features (embedding coordinate-aware visual tokens for each object) successfully counters the VLM's strong textual prior, leading to more visually grounded and accurate predictions, especially for non-canonical spatial relations.\n\nEfficiency and Practicality: The combination of Spatially Aware Pruning and the Structured Prompt drastically improves the efficiency and output quality. Pruning reduces unnecessary VLM calls, while the prompt constrains the output, leading to higher recall and better overall scene graph quality. \n\nNovel Evaluation Metric: The Accuracy/Recall with Similarity metric provides a necessary, robust solution for fairly evaluating open-vocabulary predictions against closed-set ground truth, a valuable contribution to the open-world SGG community."}, "weaknesses": {"value": "Dependency on External Detection: The pipeline is fundamentally dependent on the performance of the external open-vocabulary object detector (e.g., Grounding DINO). Errors in object bounding boxes or labels produced by the detector directly limit the potential accuracy of the subsequent relation prediction module.\n\nHeuristic Pruning: The use of a simple, rule-based spatial filter with a fixed distance threshold (0.8) is efficient but heuristic. This may result in prematurely discarding long-range, but semantically relevant, relations (e.g., \"person looking at TV\") that the VLM might otherwise correctly infer based on semantic context.\n\nEvaluation: When considering relationships between nearby objects, how common relational vocabulary such as \"near\" or \"next to\" are handled is also worth considering. It remains to be assessed whether the framework proposed in the paper encourages the model to output such vague relational terms."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ev8NxU3pHG", "forum": "JeqXsxUTkn", "replyto": "JeqXsxUTkn", "signatures": ["ICLR.cc/2026/Conference/Submission4145/Reviewer_8o4S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4145/Reviewer_8o4S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623012007, "cdate": 1761623012007, "tmdate": 1762917198336, "mdate": 1762917198336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPOT (Structured Prompting with Object-centric Tokens), an open-source framework that adapts open-vision language models (VLMs) like LLaVA-OV for open-world scene graph generation (SGG). Here are three main contributions from SPOT:\n- A structured fill-in-the-blank prompting template that explicitly constrains the model to predict relation predicates for candidate object \n- Object-centric visual tokens, computed by pooling SigLIP patch embeddings inside bounding boxes and concatenating them with object text tokens.\n- A distance-based relation pruning mechanism that filters implausible object pairs based on geometric proximity.\n\nSPOT is evaluated on both 2D benchmarks (Visual Genome, Panoptic Scene Graph) and 3D datasets (3DSSG, ScanNet). Experiments show improved recall and semantic alignment compared to prior open-source models and even proprietary systems like GPT-4o, particularly in cross-domain generalization.\n\nWhile the results are solid, the contribution is mainly an engineering consolidation of existing ideas (structured prompting, visual token pooling, heuristic pruning) into an open, reproducible pipeline. The “3D extension” is largely a projection-based adaptation of 2D reasoning rather than true 3D spatial modeling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This paper introduces a clear and reproducible system design, leveraging open-source models (LLaVA-OV + SigLIP) instead of proprietary GPT-4o.\n- This paper conducts carefully experiments including: (1) comprehensive ablations: language vs. vision priors, coordinate vs. visual tokens, depth integration, pruning strategies. (2) Strong empirical validation across in-domain and cross-domain datasets, including adaptation to 3DSSG. (3) Structured prompting framework demonstrably improves recall and semantic precision across diverse VLMs (Table 1).\n- This paper's writing is very clear."}, "weaknesses": {"value": "- Limited novelty: The main contributions (templated prompting, patch pooling, distance pruning) are refinements of known techniques rather than new formulations.\n- 3D reasoning overstated: with current model design, it is confusion for me to see why the method would work for multiview images. Therefore, I check out the supplementary material, which I find a lot more modules are getting introduced and I am not sure how your model gets panoptic labels from multiview images. As you stated in the paper: *Specifically, a segmentation model is used to produce instance masks, and an open-vocabulary detector provides object labels*. Is this segmentation model 2D or 3D, how to ensure instance id is bounded to a specific object? In the other hands, concept fusion is feedforward. I do not think this distinction has been mentioned in the main text.\n3. Lack of comparisions of state of art models like Qwen2.5-VL/GPT-5 and more."}, "questions": {"value": "1. This paper will be benefits if the target is 3D scenes. If SPOT only distill VG data for training, it is unclear for me to see why the 3D spatial awareness is better. \n2. I see authors praises on modular models. Can the author also provide a more end to end model for the whole-scene setting?\n3. Have the authors tested robustness to noisy detections or missing objects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FwioiFG62N", "forum": "JeqXsxUTkn", "replyto": "JeqXsxUTkn", "signatures": ["ICLR.cc/2026/Conference/Submission4145/Reviewer_8srQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4145/Reviewer_8srQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860728120, "cdate": 1761860728120, "tmdate": 1762917198138, "mdate": 1762917198138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose SPOT, an open-source novel structured prompting framework leveraging vision language models for scene graph generation. It demonstrates effective spatial and semantic reasoning, outperforming proprietary baselines like GPT-4o on open-world benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Authors introduce Structured Prompting with Object-centric Tokens (SPOT) for open-world scene graph generation. It combines pretrained VLMs with new techniques to enhance their relational reasoning capabilities.\n\n- Authors design a template-based structured prompt (in contrast to the free-form prompts of prior works that more precisely guides the model to produce comprehensive scene graphs both out-of-the-box and after refinement through fine tuning.\n\n- Authors encourage the model to consider the visual scene layout through integration of an object-centric visual feature when predicting relations. This additional signal improves upon the VLM’s standard processing of the image, increasing spatial alignment and relation accuracy on the open Visual Genome and PSG benchmarks.\n\n- To enable open-world prediction with an external object detector and no pre-defined vocabulary, authors propose leveraging spatially aware pruning and integrating flexibility during evaluation to minimize penalties for semantically similar predictions. This approach goes beyond the protocol which exhaustively constructs a fully-connected graph over all object pairs, which proves computationally expensive and suffers from redundant and irrelevant relation predictions."}, "weaknesses": {"value": "1) The authors chose GroundingDINO as their open-vocabulary object detector, but the paper does not provide a quantitative justification for why they chose this method.\n\n2) The comparison with SOTA graph generation methods is incomplete; various efficient graph generation methods like FROSS [1] exist.\n\n[1] Hou, H. Y., Lee, C. Y., Sonogashira, M., & Kawanishi, Y. (2025). FROSS: Faster-than-Real-Time Online 3D Semantic Scene Graph Generation from RGB-D Images. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 28818–28827).\n\n3) Methods for encoding individual objects in images are known and have been successfully applied in Chat-Scene [1], GPT4Scene [2], and others.\n\n[1] Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, et al. Chat-scene: Bridging 3D scenes and large language models with object identifiers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024\n\n[2] Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. Gpt4scene: Understand 3D scenes from videos with vision-language models. arXiv preprint arXiv:2501.01428, 2025.\n\n4) The authors state that SPOT is a scalable and fully open-source framework; however, no links to the anonymous repository could be found in the appendices or in the text of the article to confirm this statement.\n\n5) The article's formatting should be improved; punctuation marks are missing at the end of the formulas."}, "questions": {"value": "1) The authors write that SPOT is a scalable and fully open-source framework, but no links to an anonymous repository could be found in the appendices or the text of the article to confirm this claim. How will the method perform on other popular benchmarks that evaluate the quality of graph generation, such as STAR [1] and PVSG [2]?\n\n[1] Wu, B., Yu, S., Chen, Z., Tenenbaum, J. B., & Gan, C. (2024). Star: A benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711.\n\n[2] Yang, J., Peng, W., Li, X., Guo, Z., Chen, L., Li, B., ... & Liu, Z. (2023). Panoptic video scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 18675–18685)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HNAXaHSafV", "forum": "JeqXsxUTkn", "replyto": "JeqXsxUTkn", "signatures": ["ICLR.cc/2026/Conference/Submission4145/Reviewer_cT6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4145/Reviewer_cT6c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994170700, "cdate": 1761994170700, "tmdate": 1762917197923, "mdate": 1762917197923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Structured Prompting with Object-centric Tokens (SPOT) for scene graph generation in open world scenarios. SPOT shows strong performance on 2D benchmarks and can be further extended to 3D scene graph generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work introduced SPOT to augment VLMs’ ability on open-world scene graph generation\n2. The experiment results and ablation study results are extensive and good."}, "weaknesses": {"value": "1. In the proposed model, is there any design difference for processing spatial relation and semantic relation? What is the evaluation for each category of relations in In-Domain Evaluation?\n2. In Cross-Domain Evaluation, what is the generalization performance of the proposed model in spatial relation and semantic relation?"}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "up1B5F5Fxy", "forum": "JeqXsxUTkn", "replyto": "JeqXsxUTkn", "signatures": ["ICLR.cc/2026/Conference/Submission4145/Reviewer_oinB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4145/Reviewer_oinB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762294971767, "cdate": 1762294971767, "tmdate": 1762917197705, "mdate": 1762917197705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}