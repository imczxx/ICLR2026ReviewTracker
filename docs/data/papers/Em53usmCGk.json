{"id": "Em53usmCGk", "number": 676, "cdate": 1756767216500, "mdate": 1759898247710, "content": {"title": "Uni-Map: Unified Camera-LiDAR Perception for Robust HD Map Construction", "abstract": "High-definition (HD) map construction methods play a vital role in providing precise and comprehensive static environmental information essential for autonomous driving systems.  The primary sensors used are cameras and LiDAR, with input configurations varying among camera-only, LiDAR-only, or camera-LiDAR fusion based on cost-performance considerations, while fusion-based methods typically perform the best.  However, current methods face two major issues: high costs due to separate training and deployment for each input configuration, and low robustness when sensors are missing or corrupted.  To address these challenges, we propose the Unified Robust HD Map Construction Network (Uni-Map), a single model designed to perform well across all input configurations.  Our approach designs a novel Mixture Stack Modality (MSM) training scheme, allowing the map decoder to learn effectively from camera, LiDAR, and fused features. We also introduce a projector module to align Bird's Eye View features from different modalities into a shared space, enhancing representation learning and overall model performance. During inference, our model utilizes a switching modality strategy to adapt seamlessly to any input configuration, ensuring compatibility across various modalities.  To evaluate the robustness of HD map construction methods, we designed 13 different sensor corruption scenarios and conducted extensive experiments comparing Uni-Map with state-of-the-art methods. Experimental results show that Uni-Map outperforms previous methods by a significant margin across both normal and corrupted modalities, demonstrating superior performance and robustness.  Notably, our unified model surpasses independently trained camera-only, LiDAR-only, and camera-LiDAR MapTR models with a gain of 4.6, 5.6, and 5.6 mAP on the nuScenes dataset, respectively. The code and models will be released.", "tldr": "In this paper, we propose the Unified Robust HD Map Construction Network (Uni-Map), a single model designed to perform well across all input configurations.", "keywords": ["HD map construction; Autonomous driving"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58f58d84530af25ed6949f2b089dd4c49e857051.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Uni-Map, a unified and robust framework for high-definition (HD) map construction in autonomous driving that works across camera-only, LiDAR-only, and fusion configurations. Unlike prior methods requiring separate models for each modality, Uni-Map leverages a Mixture Stack Modality (MSM) training scheme and a projector module to align BEV features from different sensors into a shared latent space. This enables effective multi-modal learning and flexible adaptation to any input configuration during inference. The authors further design 13 sensor corruption scenarios to evaluate robustness, showing its resistance to sensor failure or degradation. Experimental results on nuScenes demonstrate that Uni-Map not only achieves the state-of-the-art accuracy but also greatly improves cross-modality robustness and deployment efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written. Figures are clear to express the design and ideas.\n2. Robustness is a critical problem for autonomous driving in practical applications.\n3. Experiments are extensive under 13 sensor corruption combinations to verify the effectiveness."}, "weaknesses": {"value": "1. The proposed method lacks new insights. The modality projector module seems the simple MLP of BEV features from different modalities, previous approaches also support for such multi-modality design. \n2. The MSM seems the practical engineering implementation, with no clear explanation of the underlying mechanism ensuring feature alignment or robust modality adaptation. \n3. The switching modality strategy lacks a clear functional link to the claimed robustness improvements. Although the paper emphasizes performance under sensor corruption, the switch mechanism itself does not directly handle or detect corruption, making its practical purpose and contribution unclear.\n\nOverall, this paper identifies the practical issue of robustness under sensor corruption in HD map construction. However, the proposed solution is methodologically simple and largely engineering-oriented, lacking both theoretical depth and innovation. While the experiments are extensive, the contributions are incremental and the design choices (e.g., projector and modality switching) do not convincingly address the core robustness problem."}, "questions": {"value": "1. It's  necessary to  clarify the underlying mechanism ensuring feature alignment or robust modality adaptation. \n\n2. need to justify the effectiveness of switching modality strategy on improving the robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zD2WquzGpY", "forum": "Em53usmCGk", "replyto": "Em53usmCGk", "signatures": ["ICLR.cc/2026/Conference/Submission676/Reviewer_oshD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission676/Reviewer_oshD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724524466, "cdate": 1761724524466, "tmdate": 1762915579030, "mdate": 1762915579030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified robust HD map construction network Uni-Map to perform across different single or fusion input modalities. Uni-Map proposes Mixture Stack Modality (MSM) training scheme for multi-modality fusion.  This paper also introduced a projection module to align BEV features, enhancing representation learning and overall model performance. Experiments across 13 sensor corruption scenarios are conducted to verify the robustness. Uni-Map achieves the sota accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper is well-written and easy to follow.\n\n2.\tExtensive experiments are conducted across comparisons with previous approaches and the sensor corruption benchmarks."}, "weaknesses": {"value": "1.\tThe mixture stack modality scheme and the projector lack novelty, which has been investigated in previous modality fusion works. Besides, theoretical analysis is required to verify the effectiveness of your design. \n\n2.\tThe implementations of Uni-Map are conducted based on MapTR and HIMap. What if we change to a more robust baseline like MapTRV2? In the like 342 of Table 1, MapTRV2 seems to achieve better performance than UniMap (MapTR). It also requires more consideration.\n\n3.\tIn Table 2, UniMap requires longer training time compared with the previous MapTR. What causes such long-term training? Besides, is there any analysis of inference speed to satisfy the requirement of real-time inference in autonomous driving?"}, "questions": {"value": "Questions are proposed in the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KuDjAb2q1y", "forum": "Em53usmCGk", "replyto": "Em53usmCGk", "signatures": ["ICLR.cc/2026/Conference/Submission676/Reviewer_GdJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission676/Reviewer_GdJW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726669870, "cdate": 1761726669870, "tmdate": 1762915578910, "mdate": 1762915578910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method for constructing graph-based HD maps from multi-modal input (camera + lidar) for autonomous driving, which is robust to sensor corruptions (e.g. missing sensor data). Following similar works in the domain, the method first constructs BEV representations from modality specific encoders, and fuses these in a shared feature space, and finally applies a common prediction head. The proposed \"MSM\" training strategy constructs batches that contain 3 variants of the feature maps: camera-only, lidar-only, and camera+lidar fused; This way, the final loss is computed on batches containing mixed modalities during end-to-end training.\nThe method is tested on the nuScenes HD map benchmark using the original (but outdated) train/val/test splits, on which it is shown to outperform various prior works. Additional experiments show that training with various modality combinations improves robustness in various sensor corruption cases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to read, and the overall architecture is sensible.\n- Experiments compare against various reported baselines, and clearly indicate choices for BEV Encoder and Backbone; The method outperforms the MapTR baseline, on which it is based, also in case of corrupted data.\n- There are various ablation studies to validate the design choices.\n- MixtureStack, the proposed approach which constructs batches containing multiple modality combinations, appears to outperform randomly dropping modalities in the ablation study (a common approach in many works in this field). If this could be shown on various network architectures, it could be a valuable and broadly applicable contribution."}, "weaknesses": {"value": "[W1] I appreciate that the authors must have put a lot of work into the extensive experiments on nuScenes benchmark for mapping. Unfortunately, it was shown in [Lilja'24] that the common splits for popular benchmarks such as nuScenes are not suited to evaluate such HD mapping tasks. They show that methods that use these splits are mostly remembering locations with the maps from the training data, since the same area occurs in both training and test splits (splits were originally designed for object detection tasks), and are not suited for assessing a network's ability to reconstruct static map elements. [Lilja'24] further proposed proper splits, and showed performance is dramatically lower of all baselines. The current submissions seems to still use the old splits based on reported baseline performances. We therefore can't be sure if any improved performance isn't actually due to the key issue from [Lilja'24]: the network might just be better at remembering (overfitting) the scenes from the training data, rather then better constructing maps from sensor data.\n\n[W2] There is nearly no methodological novelty. The base network architecture is built from common parts (encoders, camera-to-BEV mapping, etc.). The idea of improving robustness of multi-modal BEV representations by exposing the network to missing modalities during training has already been studied in close related BEV tasks, such as object detection and segmentation, such as MetaBEV [Ge'23], UniBEV [Wang'24]  (where it is often referred to \"modality dropout\" or \"Corruption-Augmented Training\" [Xie'25]). The proposed method aligns multi-modal features by introducing an additional layer between the (fused) features and prediction head, but it is not clear why this isn't any different from just having a slightly deeper decoder header which may improve prediction quality.\n\n[W3] As I stated in the strengths, I believe the most interesting part is _how_ this paper proposes to train the network with different sensor modalities, by deterministically composing batches (MSM), instead of randomly dropping modalities during training (Random Select), as is commonly done. However, there are too few experimental details to know if the MSM to RS comparison is fair, e.g. if hyperparameters have been optimized for each case separately, if batch sizes were similar, etc. Plus, it is not really explained why MSM would perform better than RS, since in principle both approaches should expose the network to roughly the same sensor input configurations. Finally, if this would have been the main focus of the paper, the approach could have been tested on various existing network architectures.\n\n[W4] It is good that the paper presents experiments on diverse combinations of corruptions on nuScenes. However, it would have been better of the approach followed prior approaches for benchmarking sensor corruption on nuScenes instead of inventing its own [Xie'23,Xie'25], extending those benchmarks to the HD mapping task (and then of course using suitable splits, see [W1]).\n\nIn summary, I saw nothing truly new [W2][W4]. The strategy for exposing the network to different sensor configurations could be interesting, but isn't explored sufficiently [W3]. Overall, it is mostly a new application of (common) BEV robustness training strategies to a (common) HD mapping head instead of a detection or segmentation head [Ge'23][Wang'24]. Unfortunately, the paper submission does not account for known problems in this HD mapping benchmark, making results unconvincing unless they are redone using proper splits [Lilja'24].\n\n[Lilja'24]: \"Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It\"\nA Lilja, J Fu, E Stenborg, L Hammarstrand, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024\n[Xie'23]: Xie, Shaoyuan, et al. \"RoboBEV: Towards robust bird's eye view perception under corruptions.\" arXiv preprint arXiv:2304.06719 (2023).\n[Xie'25]: Xie, Shaoyuan, et al. \"Benchmarking and improving bird's eye view perception robustness in autonomous driving.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2025).\n[Ge'23]: Ge, Chongjian, et al. \"MetaBEV: Solving sensor failures for 3d detection and map segmentation.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[Wang'24]: Wang, Shiming, et al. \"UniBEV: Multi-modal 3d object detection with uniform bev encoders for robustness against missing sensor modalities.\" 2024 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2024."}, "questions": {"value": "In the proposed MSM scheme you construct batches with diverse sensor modalities by deterministically stacking camera-only, lidar-only and camera+lidar fused maps. How is this conceptually different from randomly selecting camera-only, lidar-only and camera+lidar (each with chance 1/3) ? I would expect that on large number of samples exposing the network to those cases either randomly or deterministically would not matter much.\nIf I understood correctly, your Random Select Modality (RSM) baselines implements this non-deterministic approach, and in your experiments performs worse than MSM.\nCould you provide more details on how you implemented RSM: did each sensor configuration in equal porportions to your MSM approach? Did you adjust the training hyperparameters for the different batch sizes, and did you run the appropriate number of epochs for RSM (in simple terms, MSM appears to use 3x bigger batches than RSM if it would only randomly select a single sensor configuration for the whole batch)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Z3fkv1ZiSE", "forum": "Em53usmCGk", "replyto": "Em53usmCGk", "signatures": ["ICLR.cc/2026/Conference/Submission676/Reviewer_1jUd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission676/Reviewer_1jUd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734524583, "cdate": 1761734524583, "tmdate": 1762915578791, "mdate": 1762915578791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two practical challenges in HD map construction for autonomous driving: 1) the high cost and maintenance overhead of training and deploying separate models for different sensor configurations, and 2) the lack of robustness of fusion models when one or more sensors fail or are corrupted. The paper proposes Uni-Map, a unified network architecture that operates effectively across all three input configurations. A Mixture Stack Modality (MSM) training scheme is proposed to force a single, shared map decoder to learn a robust and generalized representation suitable for any input modality. Experimental results show that Uni-Map outperforms previous methods by a significant margin."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses HD map construction under missing or corrupted sensors, which is a challenging and practical scenario.\n2. The paper is well written and easy to follow.\n3. The paper provides extensive experiments, showing the effectiveness and versatility of the proposed method."}, "weaknesses": {"value": "1. In Figure 5(a), the Camera Crash scenario causes the fusion model's mAP to drop to 44.1. This improves over MapTR (39.1) but is significantly worse than Uni-Map's own LiDAR-only performance (61.2). This implies that feeding known-corrupted data into the fusion branch is suboptimal, and the model would be better off ignoring the bad camera data and just running in LiDAR-only mode. The current switching strategy does not account for this. It only handles missing sensors, not corrupted ones. A more robust system would require a mechanism to detect corruption and switch to the most reliable branch.\n\n2. It is unclear if the model's robustness to synthetic, independent corruptions will transfer to real-world scenarios where noise is often correlated across sensors (e.g., heavy rain degrading both camera and LiDAR simultaneously).\n\n3. The Mixture Stack Modality (MSM) module just projects features to a shared space and stacks them together. The novelty and insight are limited."}, "questions": {"value": "1. Following Weakness 1, the current inference strategy seems suboptimal for sensor corruption (as opposed to missing sensors). Did the authors consider a more dynamic inference strategy?\n\n2. The shared projector is shown to be beneficial and to align features. Could the authors provide a more quantitative analysis of this alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MuoS7Yb8zD", "forum": "Em53usmCGk", "replyto": "Em53usmCGk", "signatures": ["ICLR.cc/2026/Conference/Submission676/Reviewer_Gp1M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission676/Reviewer_Gp1M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922893179, "cdate": 1761922893179, "tmdate": 1762915578689, "mdate": 1762915578689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}