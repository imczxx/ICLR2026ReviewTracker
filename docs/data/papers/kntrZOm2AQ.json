{"id": "kntrZOm2AQ", "number": 21402, "cdate": 1758317171632, "mdate": 1763731886126, "content": {"title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment", "abstract": "Parameter-efficient fine-tuning techniques such as Low-rank Adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated.  The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms four state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\\times$.", "tldr": "We propose a federated fine-tuning framework with a single low-rank Gram matrix and adopts Procrustes alignment on the decomposed matrix to improve the fine-tuning performance.", "keywords": ["Federated fine-tuning", "low-rank Gram matrix", "Procrustes alignment"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad661862f71fdf0b3e0a96bd97306e0749fe5754.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FLoRG, a novel federated fine-tuning framework for large language models (LLMs) that improves upon the conventional LoRA-based approach. Traditional federated LoRA uses two separate low-rank matrices, which can lead to aggregation errors and decomposition drift during global model updates. To address these issues, FLoRG employs a single low-rank matrix and aggregates its Gram matrix to eliminate aggregation errors and reduce communication costs. Additionally, the authors introduce a Procrustes alignment mechanism to align decomposed matrices between rounds, thereby mitigating decomposition drift. Theoretical analysis demonstrates that this design achieves a tighter convergence bound. Extensive experiments on multiple LLM fine-tuning benchmarks show that FLoRG achieves higher downstream task accuracy and reduces communication overhead by up to 82% compared to state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors provide a theoretical convergence analysis and rigorously show that the proposed Procrustes alignment leads to a tighter convergence bound, which enhances the credibility of the method.\n2. Experimental results on multiple LLM fine-tuning benchmarks demonstrate consistent improvements over several state-of-the-art baselines in both accuracy and communication efficiency.\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The experiments are limited to natural language understanding tasks with relatively small models. It would strengthen the paper to include evaluations on larger models or additional task types to demonstrate broader applicability.\n2. Updating only the low-rank matrix  $A$ may limit the model’s representation capability, potentially constraining its ability to adapt to more complex tasks.\n3. The paper lacks an analysis of efficiency in terms of computational cost, memory usage, or communication overhead."}, "questions": {"value": "1. How does the proposed method perform on more general tasks such as question answering and dialogue?\n2. The performance of the proposed method when applied to more popular and larger language models, such as LLaMA?\n3. The local training stage of the proposed method is interesting. It seems applicable to centralized learning as well. How does it perform under centralized learning? Compared with centralized learning, what advantages does it provide in the FL setting?\n4. It is unclear whether updating only module A is sufficient to learn good representations of the client data. Could the authors clarify or provide evidence?\n5. Could the authors provide details on how the dataset was used?\n\nIf the authors can adequately address my concerns, I am willing to increase my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "352RLntfMi", "forum": "kntrZOm2AQ", "replyto": "kntrZOm2AQ", "signatures": ["ICLR.cc/2026/Conference/Submission21402/Reviewer_kKoy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21402/Reviewer_kKoy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835642063, "cdate": 1761835642063, "tmdate": 1762941749534, "mdate": 1762941749534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FLoRG replaces the usual LoRA two-factor update BA with a single low-rank matrix A and updates the model via a Gram matrix aggregation. Clients locally SGD-update A; the server linearly aggregates n∑An⊤An, then eigendecomposes the aggregated Gram and applies a Procrustes alignment to pick a decomposition closest to the previous A, which stabilizes directions and enforces a target rank. The authors prove a non-convex convergence bound in which Procrustes alignment cancels a “drift” term, and empirically report higher GLUE accuracy vs. FedIT/FeDeRA/FFA-LoRA/FedSA-LoRA with up to 82% fewer transmitted parameters to reach target accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Bias-free aggregation with one matrix.\n2. Convergence bound tightens when alignment is used; ablations show sizeable accuracy gains from Procrustes; headline comms savings to target accuracy."}, "weaknesses": {"value": "1. The approach relies on semi-orthogonal L,R that never update; performance is sensitive to their initialization.\n2. Each round per layer requires eigendecomposition of Q and an SVD for Procrustes; scalability or latency with many layers or clients isn’t benchmarked."}, "questions": {"value": "1. How robust is FLoRG if L,R are learned (slowly) or adapted per layer/round? Can you provide theory/ablation for updating L,R versus keeping them fixed?\n2. What are per-round costs of eigendecomposition + Procrustes across all LoRA layers at N>100 clients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mCoXj5khd7", "forum": "kntrZOm2AQ", "replyto": "kntrZOm2AQ", "signatures": ["ICLR.cc/2026/Conference/Submission21402/Reviewer_fXbx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21402/Reviewer_fXbx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855379959, "cdate": 1761855379959, "tmdate": 1762941749229, "mdate": 1762941749229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FLoRG, a framework for federated fine-tuning of LLMs using Low-rank Adaptation. The authors point out two primary challenges with existing federated LoRA methods:  Aggregation Error (caused by naively aggregating the LoRA matrices B and A separately), Decomposition Drift (caused by the fact that there is not unique decomposition matrix) \nThe authors tackle these challenges with a two-part solution:\n\n* Gram Matrix Aggregation: Instead of LoRA's two matrices (B,A), FLoRG uses a single trainable low-rank matrix A and utilizes existing linear algebra techniques to convert the A matrix to the original dimension of the $\\Delta W$. \n\n* Procrustes Alignment: The server decomposes the aggregated weights, it performs a Procrustes alignment step. This solves an optimization problem to find an orthogonal matrix that best aligns the new A~t+1 with the matrix from the previous round At, thereby minimizing the \"decomposition drift\".\n\nThe paper provides a theoretical convergence analysis showing that the Procrustes alignment step results in a tighter convergence bound. The authors also empirically show that their method outperform four baselines on GLUE benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-written. The authors did a good job categorising and explaining the existing problems. \n* The algorithm performs better than the mentioned baselines. \n* The authors provide a convergence analysis for FLoRG."}, "weaknesses": {"value": "* Clarity on Communication Saving. I would appreciate it if the authors explained the communication saving part of their claim. Did they measure the communication compared to full matrix communication or other Federated LoRA methods?\n\n* Server-Side Computational Overhead: The paper does not discuss the server-side computational cost, which appears to be substantial, especially doing matrix decomposition and solving optimization. \n\n* The baselines are considerably basic. By just checking recent ACL and ICML conferences, I found recently accepted papers on Federated LoRA. The merits of the paper is not clear for me considering it is missing several works."}, "questions": {"value": "* Information about the setting is missing. For example, what is the parameter for different levels of heterogeneity? How did you do LDA for datasets without labels? \n\n* What is the federated learning setting, how many clients participate each round?\n\n* Did you do hyperparameter search? \n\n* Are the results averaged for different random seeds or they are done only for one seed?\n\nPlease also check the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "64TGpiJLn9", "forum": "kntrZOm2AQ", "replyto": "kntrZOm2AQ", "signatures": ["ICLR.cc/2026/Conference/Submission21402/Reviewer_F3oY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21402/Reviewer_F3oY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939997834, "cdate": 1761939997834, "tmdate": 1762941748725, "mdate": 1762941748725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue that the federated aggregation of LoRA updates may not accurately reflect the intended global aggregation result. To this end, the authors propose FLoRG, which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix. Extensive experiments demonstrate its superiority over the existing works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a well-structured theoretical analysis with formal proofs, offering strong theoretical soundness and clear convergence guarantees.\n2. It explores an interesting and under-studied problem—eliminating aggregation bias and decomposition drift in federated LoRA fine-tuning—introducing new insights into parameter-efficient federated learning.\n3. The paper is clearly written, correctly annotated, and provides a thorough description of the proposed FLoRG framework, making it easy to follow and reproducible."}, "weaknesses": {"value": "1. The paper does not address the partial client participation scenario, which is common in practical federated learning settings. Evaluating FLoRG under varying client availability would strengthen its applicability.\n2. The experiments are conducted only on OPT-125M and RoBERTa-large, which are relatively dated compared to current state-of-the-art LLMs such as LLaMA-3 and Qwen-2.5. Using more recent backbones would better demonstrate the scalability and relevance of FLoRG.\n3. The paper reports final accuracy and communication cost but does not include convergence curves showing performance versus communication rounds. Such a figure would provide clearer insights into the training dynamics and stability of FLoRG compared with baselines."}, "questions": {"value": "**See weaknesses.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ua8TroiqW", "forum": "kntrZOm2AQ", "replyto": "kntrZOm2AQ", "signatures": ["ICLR.cc/2026/Conference/Submission21402/Reviewer_srrB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21402/Reviewer_srrB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058598352, "cdate": 1762058598352, "tmdate": 1762941747892, "mdate": 1762941747892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}