{"id": "nKCrn6ZMIz", "number": 4641, "cdate": 1757732794198, "mdate": 1759898022060, "content": {"title": "Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety", "abstract": "Multimodal large language models (MLLMs) are increasingly positioned as AI collaborators for building complex web-related applications like GUI agents and front-end code generation. However, existing benchmarks largely emphasize visual perception or UI code generation, showing insufficient evaluation on the reasoning, robustness and safety capability required for end-to-end web applications.  To bridge the gap, we introduce a comprehensive web understanding benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and Safety across eight tasks, such as position relationship reasoning, color robustness, and safety critical detection, etc. The benchmark is constructed from 729 websites and contains 3799 question–answer pairs that probe multi-step inference over page structure, text, widgets, and safety-critical interactions. To ensure reliable measurement, we adopt standardized prompts, deterministic evaluation scripts, and multi-stage quality control combining automatic checks with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The results reveal significant gaps: models still struggle with compositional and cross-element reasoning over realistic layouts, show limited robustness when facing perturbations in user interfaces and content such as layout rearrangements or visual style shifts, and are rather conservative in recognizing and avoiding safety critical or irreversible actions. Our code is available at https://anonymous.4open.science/r/WebRSSBench/.", "tldr": "", "keywords": ["MLLMS", "BENCHMARK", "ROBUSTNESS"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e60a77224cd01c4932ee7687e5fa1198e25228d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Propose WebRSSBench, evaluating the web understanding ability of MLLM from three dimensions: Reasoning / Robustness / Safety, covering 8 subtasks. It covers 729 websites and 3,799 QA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The coverage and issue selection are reasonable, put the urgently needed capabilities of actual web page agents (spatial relationships, cross-element semantics, form/prompts, key button recognition, and risk avoidance) in a continuous evaluation, with clear engineering value."}, "weaknesses": {"value": "- The text distortion algorithm includes reversing strings, shuffling characters, etc. (Appendix C.2), which directly destroys readability and usability, going beyond the premise of \"semantic preservation\". In this case, measuring \"robustness\" is no longer fair?\n\n- In color robustness and text/form semantic tasks, the authors use cross-model consensus or \"semantic centroid\" as the gold standard, which embeds the preferences of closed-source models into the gold standard, and weakens reproducibility?"}, "questions": {"value": "see weekness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zm2z2H0iBP", "forum": "nKCrn6ZMIz", "replyto": "nKCrn6ZMIz", "signatures": ["ICLR.cc/2026/Conference/Submission4641/Reviewer_5LLh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4641/Reviewer_5LLh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882695890, "cdate": 1761882695890, "tmdate": 1762917484712, "mdate": 1762917484712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WebRSSBench, a benchmark designed to evaluate three key aspects of multimodal large language models (MLLMs): reasoning, robustness, and safety. The benchmark includes eight tasks across 729 websites and 3,799 QA samples. Data are collected from Mind2Web, WebMMU, WebSRC, and design-oriented webpages, with a subset selected based on specific criteria. The authors show that closed-source models generally outperform open-source models and demonstrate that LoRA fine-tuning helps narrow this performance gap."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper defines interesting and practical tasks, such as hint text prediction and form filling, which could be valuable for assessing whether MLLMs truly understand webpage content.\n\n- The inclusion of a safety evaluation dimension is notable and timely, given the increasing deployment of autonomous agents.\n\n- State-of-the-art closed-source MLLMs are included in the evaluation, providing a strong comparative baseline."}, "weaknesses": {"value": "- The description of how ground truth is derived is unclear. As understood, the ground truth is based on the consensus of the 12 evaluated models. If a new model (e.g., model #13) were to be tested, would the evaluation require recomputing the consensus with all 13 models, potentially altering previous results? More context and clarification are needed here.\n\n- Table 1 appears cluttered; splitting it into two tables or moving less critical details to the appendix would improve readability.\n\n- The main text omits essential details of the fine-tuning process. It is unclear why LoRA was chosen over full fine-tuning and what data were used for training. Appendix D mentions LoRA settings and an 8:1:1 train/validation/test split, but the data source remains unspecified. Are the authors fine-tuning on 80% of the benchmark’s own data and evaluating on the remaining 10%?\n\n- Missing citations: prior work on MLLM robustness to positional and visual perturbations—such as PairBench [1] for spatial reasoning and color perturbation robustness, and VisMin [2] for minimal-change spatial reasoning—should be discussed to better situate this benchmark within existing research.\n\n- Numerous minor typos, including inconsistent capitalization (e.g., “Model” in L335, “closed Source Model” in L405, “Model” in L465, and “We” in L243).\n\n[1] PairBench: Are Vision-Language Models Reliable at Comparing What They See?\n\n[2] VisMin: Visual Minimal-Change Understanding"}, "questions": {"value": "- How is the ground truth maintained or extended when new models are added to the benchmark?\n\n- What do the authors exactly mean by \"extensibility\" and how can WebRSSBench be applied to new test cases and dimensions? \n\n- What data are used for LoRA fine-tuning, and how are they separated from evaluation data to prevent data leakage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0TwWM9ow6J", "forum": "nKCrn6ZMIz", "replyto": "nKCrn6ZMIz", "signatures": ["ICLR.cc/2026/Conference/Submission4641/Reviewer_qFF1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4641/Reviewer_qFF1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948198281, "cdate": 1761948198281, "tmdate": 1762917484444, "mdate": 1762917484444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose WebRSSBench, a benchmark for website understanding that combines reasoning, robustness, and safety across 8 tasks built from multiple sources (729 sites). They evaluate 12 models and the main findings is that reasoning is hard, models are brittle to UI perturbations, and closed-source models lead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Like the clear task coverage spanning spatial reasoning (e.g. relative position), UI grouping, form filling, hint text plus three perturbation types (color, text, layout) and a safety task.\n-  Dataset sources draw from real webpages/design communities and prior datasets (Mind2Web, WebMMU, WebSRC, Moz Top 500 etc).\n- Clear qualitative takeaways on reasoning difficulty and brittleness to layout/color/text changes."}, "weaknesses": {"value": "- Several ground truths are derived from cross-model consensus (e.g., majority-voted buttons / semantic centroids), risking circularity and shared bias. Please consider human-verified GT or task-verifiable signals.\n- not an expert, but here safety is evaluated via \"does the page contain any safety-critical button? output its text, else ‘sorry’\" and recall is used because pages are curated to contain such elements. But the paper does not describe a gold process for identifying safety critical affordances (beyond manual screening) nor tackle ambiguity (e.g., “reset”, “archive”).\n- Major concern to me is perturbations may not mirror real breakages, feels too synthetic: recoloring 10–30% of buttons with strong colors is synthetic, but real failures often come from contrast ratios, disabled states, or theme toggles. Consider WCAG driven contrast shifts and \"disabled/ghost\" button styles. character swaps , reversal, etc. feel adversarial for OCR, but production issues are often localization,  icon-only buttons, or mixed scripts. Include these... same for DOMs modern systems are updated.\n- Filtering (e.g. page length thresholds) may bias toward mid-complex pages, nice to quantify how filtering shifts distributions (language, verticals, accessibility)\n- Color robustness score for example is  tied to performance gaps rather than standard stat measures; no motivation of the 20 bound, and no uncertainty intervals. \n- LoRA gains are promising but under specified: splits, leakage controls (site/template level), seeds, compute, early-stopping and transfer across perturbations are missing.\n\nConsider including one or more of the above or provide evidence if any of the points are not meaningful in the current study.\n\n- [minor] avoid hyphenation and line breaks in title.\n- Table 2 header typo: \"Positon\" instead of  “Position”.\n- Prompt typo: \"botton\" instead of \"button\"\n\nSuggestions:\n- Expand perturbations: ark-mode/contrast changes, disabled/ghost buttons, icons only etc..\n- add human correlation studies\n\nOVerall needs substantial revision!"}, "questions": {"value": "- How often do consensus derived groundtruth disagree with human annotations on a held out set? would be nice to see a correlation.\n- [optional] In safety detection, how do you differentiate critical vs. cautionary actions (e.g. \"Sign out\" vs. \"Delete account\")?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QYf2mzevZv", "forum": "nKCrn6ZMIz", "replyto": "nKCrn6ZMIz", "signatures": ["ICLR.cc/2026/Conference/Submission4641/Reviewer_VfyW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4641/Reviewer_VfyW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969735507, "cdate": 1761969735507, "tmdate": 1762917483987, "mdate": 1762917483987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WebRSSBench, a benchmark for multimodal LLMs that jointly evaluates Reasoning, Robustness, and Safety across eight web-understanding tasks. It is made of 729 real webpages and 3799 QA pairs. It provides deterministic scripts and evaluates 12 open- and closed-source MLLMs. Overall, results show that closed-source models have better performance, in particular on safety. Authors also surface three brittleness patterns under color, text, and layout perturbations. Finally, targeted LoRA finetuning was experimented to demonstrate boost in position reasoning, UI grouping, and color robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents diverse corpus with 729 webpages and 3799 QA pairs drawn from real sites and design communities. \n\nThis paper introduces programmatic color, text, and layout perturbations with before, after comparisons to make robustness measurement explicit and reproducible.\n\nThe use of deterministic scripts and multistage quality check in improving reliability of results ensures the high standard of the dataset.\n\nAuthor has also trained with LoRA finetuning and demonstrates improvement in the needed areas."}, "weaknesses": {"value": "The author claims safety evaluation as a strength of the benchmark. However, there are only 45 questions under the safety category. It would be much more statistically sound if the author could include more questions in this category. \n\nIt would be helpful to provide more reasoning around why these scenarios are selected: position relationship reasoning, form filling, hint text prediction, and UI grouping. And if they have covered MLLM’s usage scenarios to a good extent. So that testing on these tasks can accurately reflect models’ abilities. \n\nThere is also a large variation in the number of questions between each category, ranging from 720 to 45, it is best to have them in closer numbers. \n\nProvide more details in training, how are training data constructed etc. \n\nIt would be great to see how human perform as a baseline."}, "questions": {"value": "As listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sKtFkmip6D", "forum": "nKCrn6ZMIz", "replyto": "nKCrn6ZMIz", "signatures": ["ICLR.cc/2026/Conference/Submission4641/Reviewer_L7HZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4641/Reviewer_L7HZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986036367, "cdate": 1761986036367, "tmdate": 1762917483473, "mdate": 1762917483473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}