{"id": "MDHVDfUrDz", "number": 17923, "cdate": 1758282069689, "mdate": 1759897144893, "content": {"title": "CodecSep: Prompt-Driven Universal Sound Separation on Neural Audio Codec Latents", "abstract": "Text-guided sound separation supports flexible audio editing across media and assistive applications, but existing models like AudioSep are too compute-heavy for edge deployment. Neural audio codec (NAC) models such as CodecFormer and SDCodec are compute-efficient but limited to fixed-class separation. We introduce CodecSep, the first NAC-based model for on-device universal, text-driven separation. CodecSep combines DAC compression with a Transformer masker modulated by CLAP-derived FiLM parameters. Across six open-domain benchmarks under matched training/prompt protocols, \\textbf{CodecSep} surpasses \\textbf{AudioSep} in separation fidelity (SI-SDR) while remaining competitive in perceptual quality (ViSQOL) and matching or exceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream deployments, it needs just 1.35~GMACs end-to-end -- approximately 54× less compute (25× architecture-only) than spectrogram-domain separators like AudioSep -- while remaining fully bitstream-compatible.", "tldr": "We propose CodecSep, a compute-efficient, text-guided universal sound separation model that operates in neural audio codec space and outperforms prior methods like AudioSep with 25× lower compute.", "keywords": ["Audio Source Separation", "Universal Sound Separation", "Text-guided Sound Separation", "Neural Audio Codecs", "DAC"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80b2021229c65168a79f0a1f89c9ccf77ad4ba91.pdf", "supplementary_material": "/attachment/70a99938322980869f3b546b28ab11855cba84bd.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce CodecSep, a Neural Audio Codec (NAC)-based model for universal, text-driven on-device separation. The model integrates a former NAC model (the DAC compression model) with a transformer masker modulated by CLAP-derived FiLM parameters. Evaluated on six open-domain benchmarks with matched training and prompt settings, CodecSep outperforms at least one other model (AudioSep) in separation fidelity (SI-SDR), achieves comparable perceptual quality (ViSQOL), and obtains competitive results on fixed-stem baselines (compared to TDANet, CodecFormer or SDCodec). In addition, it is shonw that the proposed model is of much lower complexity than spectrogram-based models like AudioSep, while remaining fully bitstream-compatible for efficient on-device audio separation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of the paper are:\n\n-\tThe overall concept of the proposed model (exploiting text embeddings to guide audio source separation) is interesting and is worth further investigation.\n\n-\tThe proposed approach (performing masking in the latent domain) is sound and well-motivated.\n\n-\tMany experiments were conducted to support the merits of the proposed approach"}, "weaknesses": {"value": "The main weaknesses of the paper are:\n\n-\tThe paper is on the overall difficult to read and follow. Technical details about the method are given rather early in the paper (for instance, beginning of section 3.1 with DAC model), and provided as a list of bullet points (DAC.., Text-guided.., Why NAC latents… etc).  The model itself is only described in 3.2 (in a very condensed section). As a result, the paper misses a central section describing the model in conceptual form first and then with technical details. An additional conceptual scheme/diagram precising how and when discrete information is used would also facilitate the readability. \n\n-\tThe experimental plan is rich, diverse and solid, but maybe too diverse which dilutes the most important outcomes (the main message). For instance, the experiments on using ambiguous prompts are probably less conclusive and if omitted, more place would be given to better discuss the results.\n\n-\tThere are no sound examples provided on a demo page to illustrate the merits and limits of the method.\n\n-\tThe limitations are not discussed in the main paper (only in appendix)."}, "questions": {"value": "-\tAbout Perceptual test: How were chosen the 20 participants? are they expert in audio perceptual tests? in audio coding/separation test ? How were chosen the 20 dnr-v2 stem test mixtures? What means exactly “per-clip sfx prompts”? (maybe give an example).\n\n-\tResults on Table 2 could be further discussed or clarified: It is said :””..COdecSep is competitive with or stronger than fixed-stem baselines ..”. From the table it rather seems that SD-Codec, without text guiding, is significantly outperforming the proposed model (even on SfX) with generic prompt. \n\n-\tThe use of continuous or discrete information in the proposed model is not always clear (and it is mentioned at some places that the quantization is optional). Are all results obtained with no discrete information (e.G. by passing the RVQ) ? Which results have been obtained using the optional quantization step ?\n\n-\tThere are no mention about potential code release, although the technical details provided seem sufficient to reproduce the work. Do you plan to release the code to allow better reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oqnWbiheXt", "forum": "MDHVDfUrDz", "replyto": "MDHVDfUrDz", "signatures": ["ICLR.cc/2026/Conference/Submission17923/Reviewer_TLeL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17923/Reviewer_TLeL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569041605, "cdate": 1761569041605, "tmdate": 1762927738094, "mdate": 1762927738094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a model, designated CodecSep, which seeks to address a significant trade-off in the field of audio separation. Currently, a dichotomy exists between two classes of models; on one hand, flexible text-prompted systems such as AudioSep offer the capability to separate a wide array of sounds based on textual descriptions, however these models are characterized by their substantial computational requirements, rendering them unsuitable for on-device applications. Conversely, highly efficient models predicated on neural audio codecs are limited to a predefined set of separable categories, for instance speech, music, and sound effects. The main contribution of CodecSep is the reconciliation of these two paradigms, it facilitates text-driven, universal sound separation by operating directly within the compressed latent space of a pre-trained neural audio codec. Instead of employing a computationally intensive spectrogram-based architecture, CodecSep utilizes a lightweight Transformer masker guided by textual prompts through the application of Feature-wise Linear Modulation (FiLM) conditioning. This design simplifies the separation task considerably, and the final system demonstrates efficiency which makes it viable for real-world deployment on edge devices, but also achieves state-of-the-art separation quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed model outperforms larger models on the same conditional separation task.\n2. The evaluation is thorough enough for publication to ICLR and to make the main points that the authors want to pass to the reader.\n3. The authors try to somehow quantify the complexity of their models by using GMACs, however, they did not make a holistic analysis of their computational requirements for their separation model [A] (e.g. the main concerns of execution time and actual memory footprint are missing to show that the proposed model can be deployed on a real-world device)\n\n[A] Tzinis, E., Wang, Z., Jiang, X. and Smaragdis, P., 2022. Compute and memory efficient universal sound source separation. Journal of Signal Processing Systems, 94(2), pp.245-259."}, "weaknesses": {"value": "The paper has many weaknesses and I will try to write them down with decreasing order of significance.\n\n1. The authors completely omit to place their contribution to a reasonable context with the literature and show what is the real novelty being introduced with their method. For example, separation into the latent space of general sounds has been introduced for quite a while [A] and its variations like speech enhancement [B]. Moreover, the context of flexible condition based separation using FiLM layers with efficient models has also been extensively analyzed in the past literature [D, E, F]. The authors completely omit to compare against the proposed methods in the literature, which also display state-of-the-art results with much smaller models and they choose to only compare against a much more computationally heavy model. The authors need to perform a head-to-head comparison with the methods in the literature that have exactly the same setup (Figure 1 in this paper shows exactly the same conditioning mechanism is used as the one proposed in [F] with the only difference that a more complex encoder / decoder pair is used and instead of U-ConvBlocks, transformer based layers are used in the separator). \n2. The authors did not make a holistic analysis of their computational requirements for their separation model (e.g. the main concerns of execution time and actual memory footprint are missing to show that the proposed model can be deployed on a real-world device). GMACs is not the way to show that a model is capable of running efficiently on an edge device. The authors can try CPU / smaller GPUs to show the efficiency of their model and compare that to state-of-the-art models in the literature like [D, F] which are currently being used for on-device implementations.\n3. Please remove the second decimal from SNR metrics, a human being cannot understand differences of 0.1 dB. \n4. The model's performance is fundamentally capped by the quality of the frozen DAC models it's built upon (which is a natural problem with frozen auto-encoder methods). The same problem exists for all methods that they are using latent targets for separation [B]. It would be nice to show what is the maximum separation performance one could gain by using the ideal targets in this context, similar to [B] and a much more novel contribution could be to show how to make those targets easier to estimate (that could be another paper though).\n\n\n[B] Tzinis, E., Venkataramani, S., Wang, Z., Subakan, C. and Smaragdis, P., 2020, May. Two-step sound source separation: Training on learned latent targets. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 31-35). IEEE.\n\n[C] Casebeer, J., Vale, V., Isik, U., Valin, J.M., Giri, R. and Krishnaswamy, A., 2021, June. Enhancing into the codec: Noise robust speech coding with vector-quantized autoencoders. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 711-715). IEEE.\n\n[D] Tzinis, E., Wichern, G., Smaragdis, P. and Le Roux, J., 2023, June. Optimal condition training for target source separation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE.\n\n[E] Mahmud, T., Amizadeh, S., Koishida, K. and Marculescu, D., 2024. Weakly-supervised audio separation via bi-modal semantic similarity. arXiv preprint arXiv:2404.01740.\n\n[F] Tzinis, E., Wichern, G., Subramanian, A.S., Smaragdis, P. and Le Roux, J., 2022. Heterogeneous Target Speech Separation. In Proc. Interspeech 2022 (pp. 1796-1800)."}, "questions": {"value": "THe authors mention implicitly that although other methods in the literature \"assume a fixed maximum number of sources \", their method does not. Considering that this is true, why do the authors choose to only perform experimetns with up to 3 sources? This is not a simple omission but also misleading to the way the paper is written, if the authors want to claim that not defining the maximum number of sources would be beneficial to some extent, why don't they also show the appropriate experimetns and compare against the methods that they consider inferior to show that this is an important aspect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cJe5JUyIjN", "forum": "MDHVDfUrDz", "replyto": "MDHVDfUrDz", "signatures": ["ICLR.cc/2026/Conference/Submission17923/Reviewer_pDqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17923/Reviewer_pDqQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922552011, "cdate": 1761922552011, "tmdate": 1762927737700, "mdate": 1762927737700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CodecSep, a prompt-driven universal sound separation (USS) model that performs separation directly in neural audio codec (NAC) latent space. The model combines a frozen Descript Audio Codec (DAC) encoder–decoder backbone with a FiLM-conditioned Transformer masker modulated by CLAP text embeddings. CodecSep achieves higher SI-SDR and comparable ViSQOL to AudioSep across six benchmarks, while requiring ~54× less compute in code-stream deployment. Ablations confirm the benefits of masking over latent generation and the effectiveness of FiLM conditioning. Human listening tests (MOS–LQS) also indicate perceptual gains."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces a highly efficient text-conditioned universal sound separation by operating in the Neural Audio Codec (NAC) latent space. It achieves a 54x compute reduction (1.35 GMACs end-to-end) using codec latents over AudioSep without performance degradation, demonstrating strong potential for real-time and on-device audio processing applications."}, "weaknesses": {"value": "1. The primary comparison is against AudioSep, which is an established benchmark and the representative of the computation-heavy spectrogram-domain approach. However, the paper would be significantly strengthened by including a comparison with more recent models."}, "questions": {"value": "1. Could the authors provide a comparison with more recent models in this domain?\n\n2. Table 9 presents a comparison of performance using a 16 kHz DAC backbone versus a 48 kHz EnCodec backbone for bandwidth scaling. The authors attribute the observed performance reduction at 48 kHz primarily to the challenges of higher bandwidth (longer sequences and complex high-frequency content). However, this is not a fair comparison, as it simultaneously changes two critical variables: bandwidth (16 kHz -> 48 kHz) and codec (DAC -> EnCodec). Could the authors provide a comparison with 24 kHz and 44.1 kHz DAC backbones?\n\n3. Figure 1 shows that text prompts are fed into BERT not CLAP. Is this a typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nd1xYJDtM4", "forum": "MDHVDfUrDz", "replyto": "MDHVDfUrDz", "signatures": ["ICLR.cc/2026/Conference/Submission17923/Reviewer_wCZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17923/Reviewer_wCZU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994680430, "cdate": 1761994680430, "tmdate": 1762927737328, "mdate": 1762927737328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a prompt-driven universal sound separation framework that combines a DAC-based audio codec with a conditioned CLAP-based prompt embedding. The approach adopts an encoder–masker–decoder architecture, where the CLAP-based prompt embedding generates a mask to extract (i.e., separate) a specified sound source. While there are many possible design choices for implementing this framework, the paper provides clear reasoning for the selected architecture and supports these choices with experimental evidence. The proposed method outperforms the baseline AudioSep across three domains—speech, music, and environmental sounds—while achieving significantly higher computational efficiency due to its streamlined design. Furthermore, comprehensive ablation studies are conducted to validate the contributions of individual components."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Efficient realization of prompt-driven universal sound separation through the integration of a neural audio codec and an encoder–masker–decoder architecture.\n- Comprehensive justification of design choices, supported by experimental evidence that validates the effectiveness of each component.\n- Strong experimental performance, demonstrating superior results and significantly higher computational efficiency compared to the baseline method, AudioSep.\n- Inclusion of human evaluation provides valuable subjective validation of the separated sources and strengthens the overall experimental credibility."}, "weaknesses": {"value": "* **Poor presentation and clarity:** Many equations and variables are introduced without proper explanation or are explained later in the text, leading to redundancy and making the paper difficult to follow (see some of my detailed comments later).\n* **Limited novelty:** Although the design choices are reasonable and well-justified, the proposed method represents only an incremental improvement over AudioSep, primarily through better architectures/configurations and the integration of a neural audio codec (NAC).\n* **Insufficient evaluation metrics:** The experiments rely mainly on SI-SDR and ViSQOL, which do not fully capture perceptual or task-relevant performance. Additional evaluations—such as proxy MOS (e.g., UTMOS) for speech quality and speaker similarity—would strengthen the experimental results.\n* **Lack of downstream task validation:** The study does not assess the proposed method’s effectiveness in practical downstream applications. For instance, results such as those in Table 4 could be extended to tasks like audio tagging or audio captioning to better demonstrate real-world utility."}, "questions": {"value": "**Questions and Suggestions for the Authors**\n\n1. **On the use of SI-SDR:**\n   Many audio codec–based methods emphasize perceptual sound quality rather than perfect signal reconstruction, which can weaken time alignment and amplitude consistency. In such cases, SI-SDR may not serve as an appropriate primary metric. Does this consideration apply to your method? If so, would alternative perceptual metrics (e.g., FAD, ViSQOL, or PEAQ) be more suitable, or do you believe SI-SDR remains appropriate for sound separation tasks? Please clarify this point.\n\n2. **Clarification on mask application:**\n   It is unclear where the masking operation is applied—specifically, whether it occurs before or after quantization, and whether it operates on discrete or continuous representations. Although this becomes clearer in Section 3 and Figure 1, providing an earlier clarification (e.g., in the abstract or introduction) would help readers follow the method more easily.\n\n3. **Interpretability claim:**\n   The paper claims that CodecSep is “interpretable.” However, since masking is applied to a continuous representation rather than a discrete codebook, it is not immediately clear how interpretability is achieved. Could you elaborate on what aspect of the system provides interpretability and how it is assessed?\n\n4. **Dataset references:**\n   The introduction mentions several datasets without providing citations, making it difficult to understand which data are used early in the paper. Please include proper references when the datasets are first introduced.\n\n5. **Readability of Section 3.1:**\n   This section is challenging to follow due to presentation issues. In particular:\n\n   * Many important equations appear inline, making them hard to read.\n   * Several variables (e.g., $d$, $Z$) are used before being defined.\n   * The projection process in the text-guided sound separation explanation should explicitly include $Quant(\\cdot)$.\n   * The equations and explanations in lines 173–190 (page 4) largely overlap with those on page 3. It would be clearer if the authors first presented the DAC backbone and CodecSep using the style found on page 4, that is, with independent equations and detailed explanations of each variable. Subsequent sections could then reference these definitions and present the equations more compactly. Currently, the presentation takes the opposite approach, which makes the section repetitive and harder to follow.\n   * Variables such as $x(t)$, $Z \\in \\mathbb{R}^{d \\times T}$, $d$, $T$, and $e_\\tau$ are redefined multiple times (e.g., in Section 3.2).\n\n6. **On the SDR range differences:**\n   The SDR results vary considerably across domains. Could you discuss why the SDR range differs so much (e.g., due to data characteristics, separation difficulty, or codec behavior)?\n\n7. **On evaluation metrics:**\n   As noted earlier, SI-SDR may not fully capture perceptual or intelligibility aspects, especially for speech and music. Have you considered including perceptual metrics such as **PESQ**, **POLQA**, or **STOI** for speech, and **MCD** or **FAD** for music? These could provide a more comprehensive evaluation of perceptual quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yne6fCy3v7", "forum": "MDHVDfUrDz", "replyto": "MDHVDfUrDz", "signatures": ["ICLR.cc/2026/Conference/Submission17923/Reviewer_QbS3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17923/Reviewer_QbS3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179139127, "cdate": 1762179139127, "tmdate": 1762927736894, "mdate": 1762927736894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}