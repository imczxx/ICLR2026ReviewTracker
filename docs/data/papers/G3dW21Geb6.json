{"id": "G3dW21Geb6", "number": 22241, "cdate": 1758328221487, "mdate": 1759896878195, "content": {"title": "CosyCPT: Coreness-Aware Synthetic Continued Pretraining", "abstract": "Synthetic continued pretraining adapts LLMs to specific domains by fine-tuning them on synthetic data that augments real domain data. However, existing methods are often data-inefficient (requiring massive synthetic corpora to enumerate all relational facts) and fail to account for the relative importance of different entity relationships. In this paper, we propose coreness-aware synthetic continued pretraining (CosyCPT), a systematic pipeline that addresses both limitations. Our method (1) constructs a graph representation of entity relations in a document, (2) quantifies relation importance via coreness scores derived from the graph, and (3) leverages these scores to guide synthetic data sampling and augmentation for continued pretraining. We investigate four definitions of entity coreness and four formulations of relation coreness, verifying that multiple variants of coreness-aware sampling can outperform random sampling of augmented data for synthetic continued pretraining. We offer a mathematical analysis, proving that (1) given a learning budget, maximizing the expected accuracy on a query set about relational knowledge in a document collection is an NP-complete problem, (2) coreness-aware sampling is the optimal solution when each query examines one entity pair, and (3) coreness-aware sampling has a better upper bound for expected accuray than random sampling.", "tldr": "", "keywords": ["synthetic continued pretraining", "knowledge acquisition", "large language models", "graph mining", "sampling", "data augmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa9f13c967adc91ebf5c3b7ebe4bc67ab8e87d54.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces COSYCPT, a coreness-aware framework for synthetic continued pretraining (CPT) of large language models. It argues that prior CPT approaches such as EntiGraph sample synthetic relational data uniformly across entities, wasting tokens on unimportant relations. COSYCPT instead models entity importance using graph coreness measures (e.g., PageRank, degree, betweenness, closeness) and samples synthetic data proportionally to these scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, The presentation of the paper is good. Math notations and proofs are clear and easy to follow.\n\n2, The motivation that uniform sampling is not efficient makes sense and the proposed method that scores the importance by structual centrality is simple and effective.\n\n3, The experiment results in closed-book setting is convincing, consistently outperforming the baselines."}, "weaknesses": {"value": "1, Limited LLM models and benchmarks in experiments. The paper only uses one model (Llama-3.1-8B-Instruct) and one benchmark (QuALITY benchmark) in the main results, which is not enough to show the priority of the proposed method empirically.\n\n2, The cost of graph construction is not discussed in the paper. The number of LLM APIs / the number of tokens / time cost of graph construction should be added."}, "questions": {"value": "The only question I have is about the limited evaluation (please see weaknesses). I will raise the score if extra experiements on more LLMs or more benchmarks still show the priority over the baseline methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7GkgcDQHhQ", "forum": "G3dW21Geb6", "replyto": "G3dW21Geb6", "signatures": ["ICLR.cc/2026/Conference/Submission22241/Reviewer_abHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22241/Reviewer_abHG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763679767, "cdate": 1761763679767, "tmdate": 1762942131118, "mdate": 1762942131118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COSYCPT to improve the data-efficiency of Synthetic Continued Pretraining for LLMs, addressing the inefficiency of prior methods like EntiGraph that rely on uniform random sampling of entity relations . The COSYCPT method first builds an entity-relation graph from source documents . It then uses graph theory, specifically PageRank, to compute a \"coreness score\" for all entity pairs . Using these scores, it employs coreness-aware weighted sampling to preferentially generate synthetic data about the most important relations . This focused approach is shown to be theoretically optimal under certain assumptions and empirically outperforms random sampling on the QUALITY QA benchmark, particularly in closed-book settings ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a practical and significant problem in domain adaptation: the data inefficiency of synthetic continued pretraining . The authors correctly identify a key weakness in prior work, namely that uniform random sampling treats all entity relations as equally important, which can dilute training signals and waste resources . The paper's central premise—that a more efficient path is to selectively sample data based on its structural importance or \"coreness\" —is intuitive and presents a well-motivated approach to this problem."}, "weaknesses": {"value": "1. Incremental Contribution: The paper's contribution is incremental. The authors merely apply weighted sampling to the existing EntiGraph framework . This is achieved by testing a collection of mature, off-the-shelf graph algorithms (like PageRank ) rather than introducing a novel contribution to the framework itself.\n\n2. Marginal Gain for Sacrificed OOD Performance: The method's claimed improvements appear marginal and come at the cost of OOD generalization, as shown in Table 3. The paper's strategy of focusing on \"core\" data patterns seems to directly cause this drop in OOD performance—a significant trade-off that is not adequately discussed.\n\n3. Low Figure Quality: The figures are of low quality and detract from the paper's clarity. Figure 1 contains distracting artifacts (e.g., red underlines). Figure 2, which shows the core results, has low resolution and an odd aspect ratio, making scores difficult to read; it should be replaced with a table for clarity. Figure 3 is cluttered and hard to interpret."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eMfcrcYpQY", "forum": "G3dW21Geb6", "replyto": "G3dW21Geb6", "signatures": ["ICLR.cc/2026/Conference/Submission22241/Reviewer_kiEM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22241/Reviewer_kiEM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845396688, "cdate": 1761845396688, "tmdate": 1762942130480, "mdate": 1762942130480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the problem of compute-efficiency for continued pretraining on synthetic data augmentations. Their approach is to (1) form a knowledge graph over entities contained in documents, (2) apply entity and edge centrality metrics to obtain an importance score for each edge, and (3) to sample synthetic data proportional to these edge importance scores. They compare to random sampling (EntiGraph) on the QuALITY dataset of articles and books, and provide a simple mathematical model justifying their weighted sampling scheme given oracle importance weights."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generally well-written and clear.\n- The problem setting (improving synthetic data generation for continued pretraining) is timely and of interest to the ICLR community.\n- The method is intuitive: it is analogous to data curation, focusing pretraining compute on high importance relationships in the text. The task-- and domain-agnostic nature of the graph centrality metrics is also appealing (this could be better articulated by the authors)."}, "weaknesses": {"value": "- The authors confusingly refer to their approach as a data-efficiency method. The usual definition of data efficiency in the pretraining and synthetic data literature is in improving performance given a fixed number of unique tokens in a seed corpus [1]. The present work is really an improvement in compute-efficiency, that is, the slope of the \"val loss versus log(synthetic tokens)\" curve is better. The paper's claims should be rewritten to reflect this; e.g., data-efficiency gains are claimed in Line 3, 70, 109, etc.\n- Given the above, the motivation for pursuing compute-efficiency of domain-specific continued pretraining is not clearly presented. For example, the authors could argue that many less well-resourced companies and organizations will want to do synthetic CPT on their proprietary datasets.\n- The empirical gains do not appear very significant over random sampling: an inconsistent 1-4% accuracy improvement, on a single dataset (QuaLITY). I would appreciate if the authors could run out larger-scale synthetic CPT runs until one of the approaches begins to asymptote, and ideally add another domain for continued pretraining. This is a particularly useful experiment because if CosyCPT does end up reaching a higher asymptote than EntiGraph, then this paper can actually make a data-efficiency as opposed to compute-efficiency claim (which is more salient in the setting of small proprietary datasets).\n\n[1] Muennighoff et al., 2023. Scaling Data-Constrained Language Models. In NeurIPS."}, "questions": {"value": "Questions\n* Can the authors improve the articulation of their main claim, which is improving the compute-efficiency of domain-specific continued pretraining? They should justify why studying compute-efficiency in a data-constrained setting is useful. As a devil's advocate, is it really so many FLOPs to just train on 100x or 1000x the tokens of a small proprietary corpus?\n* Alternatively, can the authors run larger-scale experiments which demonstrate CosyCPT reaches a higher asymptotic loss than EntiGraph? This would support the claim that CosyCPT improves data efficiency. \n* Can the author run CosyCPT and baselines on another domain (e.g., code or another specialized domain) to support their empirical claims?\n\nI would be willing to increase my score if the improved experiments support the data-efficiency claim or the paper provides a more focused pitch for compute-efficiency.\n\nClarifications\n* EntiGraph constructs entity graphs for each document. It is unclear reading the exposition of your method whether the entity graphs are on a per-document or per-corpus level.\n* The 5.4 Instruction Following experiment is just a guardrail to demonstrate that the synthetic CPT does not harm instruction following capabilities? Do you do any replay on instruction tuning data to get this to work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ynCqgo47qt", "forum": "G3dW21Geb6", "replyto": "G3dW21Geb6", "signatures": ["ICLR.cc/2026/Conference/Submission22241/Reviewer_aeQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22241/Reviewer_aeQX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942847798, "cdate": 1761942847798, "tmdate": 1762942130123, "mdate": 1762942130123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a systematic pipeline, \"COSYCPT: Coreness-Aware Synthetic Continued Pretraining\". COSYCPT is a systematic graph-theoretic framework designed to prioritize the most structurally important knowledge within a document collection for synthetic data generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a clear modular pipeline, and it is relatively easy to reproduce and plug into existing CPT workflows.\n2. This paper proves the NP-hardness of the budgeted selection problem. And they show that coreness sampling is optimal when queries involve a single entity pair."}, "weaknesses": {"value": "1. For the assumption mentioned between lines 776 to 779, this paper's method is based on this assumption (\"centrality\"), and they claimed it is a reliable substitute for its actual semantic importance. However, this may not hold true in all domains; for example, in legal or medical fields [1], a single rarely connected fact could be critically important despite having low centrality.\n2. Optimality proof (Line 471 Theorem 2) assumes all queries are pairwise $q_2 = 1$, which is a strong assumption; the relevance of this assumption to real tasks with multi-entity reasoning is unclear. This may oversimplification of knowledge.\n3. In the experiment section, this paper only includes a single dataset QUALITY, and a single task QA (English). This is relatively hard to claim that this method has a strong performance that will generalize to other types of documents, such as other languages beyond English.\n\n[1] He X, Zhang J. Why do hubs tend to be essential in protein networks?. PLoS genetics. 2006 Jun;2(6):e88."}, "questions": {"value": "Please explain the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "s439Tdw4AH", "forum": "G3dW21Geb6", "replyto": "G3dW21Geb6", "signatures": ["ICLR.cc/2026/Conference/Submission22241/Reviewer_bXXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22241/Reviewer_bXXL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974344761, "cdate": 1761974344761, "tmdate": 1762942129776, "mdate": 1762942129776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}