{"id": "gUzdPLGq0y", "number": 11631, "cdate": 1758202676617, "mdate": 1763572461153, "content": {"title": "Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains", "abstract": "Imbalanced Domain Generalization (IDG) focuses on mitigating both *domain and label shifts*, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the *technical* complexity of handling their entanglement and the paucity of *theoretical* foundations. In this paper, we begin by *theoretically* establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we *technically* propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a reweighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL.", "tldr": "", "keywords": ["Imbalanced Domain Generalization", "Generalization Bound", "Domain Generalization", "Negatives-Dominant", "Contrastive Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc7433e889b28c93f26f9a5d3279da4cd0e8a0d0.pdf", "supplementary_material": "/attachment/445bc8aa24d35b365e1f0cd6ed4f0dbe02236a42.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the Imbalanced Domain Generalization (IDG) problem, where both domain and label shifts co-occur. It derives a novel generalization bound highlighting the roles of posterior discrepancy and decision margin, and introduces Negative-Dominant Contrastive Learning (NDCL) — a contrastive framework emphasizing negative samples to enlarge margins and balance gradients. Experiments on several datasets show consistent improvements across multiple imbalance settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides the a novel theoretical generalization bound tailored to IDG.\n2. The paper proposes an innovative negative-dominant contrastive mechanism that avoids explicit resampling. The theoretical insights are linked to the algorithmic formulation, making the overall framework coherent and conceptually grounded.\n3. The method demonstrates consistent gains across multiple benchmarks and imbalance scenarios, including severe long-tailed and heterogeneous shifts, showing both stability and scalability."}, "weaknesses": {"value": "1. The paper does not provide quantitative descriptions of dataset sizes, the number of domains per dataset, or the imbalance ratios (e.g., majority-to-minority class counts). Such details are critical for understanding the severity of imbalance under each scenario. Experiments on large-scale or high-resolution datasets could better demonstrate the effectiveness of NDCL.\n2. Some key hyperparameters—including α, β (loss trade-offs), and the Beta distribution coefficient ρ for hard negative mixing—are not explicitly listed. It is suggested the author provide more sensitivity analysis such as their ranges.\n3. Although an ablation table is provided, it only tests the presence/absence of NDCL components. It would be better to quantify how the margin term and posterior discrepancy term individually influence performance. A controlled ablation aligning with Theorem 1 would strengthen the link between theoretical and empirical results.\n4. NDCL involves multiple sub-objectives and prototype computations. It is suggested to provide more details about the training time, GPU memory usage, or complexity comparisons with baseline contrastive methods, which is essential to assess the practicality.\n5. The paper would be strengthened by including several more concrete real-world IDG scenarios, which would better illustrate the practical relevance of the problem under investigation."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tjAMU8PVOQ", "forum": "gUzdPLGq0y", "replyto": "gUzdPLGq0y", "signatures": ["ICLR.cc/2026/Conference/Submission11631/Reviewer_Zmdo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11631/Reviewer_Zmdo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761201663728, "cdate": 1761201663728, "tmdate": 1762922703597, "mdate": 1762922703597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles Imbalanced Domain Generalization (IDG), where domain and label shifts co-occur, leading to biased decision boundaries. The authors first derive a generalization bound highlighting the roles of posterior discrepancy P(Y∣X)P(Y|X)P(Y∣X) and decision margin, extending classical DG theory. Guided by this, they propose Negative-Dominant Contrastive Learning (NDCL), which reshapes decision boundaries via three objectives: (1) a negative-dominant contrastive loss to enlarge inter-class margins, (2) a re-weighted cross-entropy for intra-class compactness, and (3) a prediction-central alignment ensuring cross-domain posterior consistency. Experiments on VLCS, PACS, and OfficeHome show NDCL consistently outperforms 21 baselines, achieving stronger generalization under severe imbalance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s key strength lies in addressing an underexplored yet practically important problem — Imbalanced Domain Generalization (IDG) which combines challenges of domain and label shift. Its theoretical formulation is a notable step forward, introducing a generalization bound that jointly accounts for posterior discrepancy and decision margin, offering a fresh lens on generalization under imbalance. Methodologically, the proposed Negative-Dominant Contrastive Learning (NDCL) framework is a creative adaptation of existing contrastive paradigms, emphasizing negatives as a dominant learning signal an idea that is both intuitive and empirically supported. From a quality standpoint, the experimental design is thorough, spanning multiple benchmarks (VLCS, PACS, OfficeHome) and including ablations and new imbalance settings, demonstrating reproducibility and effort. In terms of clarity, the visualizations (e.g., margin discrepancy analysis) effectively highlight the intended behavior of NDCL, even though some sections are dense. Finally, in terms of significance, the paper opens a promising direction for robust representation learning under long-tailed, multi-domain settings, which can inspire follow-up work on fairness, calibration, and federated generalization. Overall, it offers moderate originality, solid experimental quality, and conceptual value for the robustness and DG community."}, "weaknesses": {"value": "The paper’s main weakness lies in the gap between its theoretical claims and empirical validation. While the proposed generalization bound elegantly integrates posterior discrepancy and decision margin, it remains largely unverified quantitatively — no experiments explicitly measure or correlate these terms with observed performance. A small-scale synthetic or analytical validation could strengthen this theoretical link.\nFrom a novelty standpoint, NDCL’s “negative-dominant” formulation is conceptually interesting but derivative of existing ideas such as SupCon (Khosla et al., 2020), hard-negative mining (Kalantidis et al., 2020), and re-weighted CE losses (Cao et al., 2019). The paper would benefit from clearer differentiation, perhaps through ablations isolating the effect of negative weighting versus standard InfoNCE.\nThe experimental analysis, though broad, remains performance-centric — lacking statistical tests, calibration or robustness metrics, and computational cost evaluations (important since NDCL adds prototype alignment and hard negative generation). Moreover, all evaluations are limited to standard DomainBed datasets; assessing performance in real-world long-tailed or medical domains could enhance generalizability.\nFinally, the writing density and notation complexity obscure key intuitions, and the connection between Theorem 1 and NDCL’s design is more rhetorical than formally derived. Making this linkage more explicit or providing geometric visualizations of how NDCL reshapes decision boundaries would significantly improve clarity and credibility."}, "questions": {"value": "Theoretical–Empirical Link: The generalization bound is central, but no quantitative validation is shown. Can the authors empirically measure how posterior discrepancy or margin width correlates with target-domain accuracy to substantiate Theorem 1? Negative-Dominant Contrastive Design: NDCL’s main novelty lies in prioritizing negatives. Could the authors clarify how this differs from SupCon or debiased contrastive learning and provide ablations isolating this effect?  Hard Negative Mixup Justification: The mixup strategy adds complexity. How much performance gain does it offer compared to standard hard-negative sampling? Posterior Alignment Mechanism: How does the prediction-central alignment (Eq. 3) improve over traditional feature or prototype alignment methods?  Training Efficiency: What is the computational overhead of NDCL relative to standard DG baselines, and is it practical for large-scale or real-time applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sSZgatjKzg", "forum": "gUzdPLGq0y", "replyto": "gUzdPLGq0y", "signatures": ["ICLR.cc/2026/Conference/Submission11631/Reviewer_xKfM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11631/Reviewer_xKfM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912138803, "cdate": 1761912138803, "tmdate": 1762922703285, "mdate": 1762922703285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of Imbalanced Domain Generalization (IDG), where both domain shift and label imbalance coexist across training domains. The authors first derive a theoretical generalization bound based on H-divergence, emphasizing the importance of posterior discrepancy and decision margin for robust generalization. Motivated by this analysis, they propose Negative-Dominant Contrastive Learning (NDCL), which reformulates contrastive learning to emphasize negatives as the main signal, combines it with adaptive re-weighted cross-entropy for intra-class compactness, and enforces cross-domain posterior alignment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Imbalanced Domain Generalization is an important and practical extension of the DG setting, and the paper provides a good formalization.\n\n2. The generalization bound highlights posterior discrepancy and margin effects, offering a potentially useful conceptual perspective.\n\n3. NDCL integrates several mechanisms (contrastive, reweighting, alignment) into a unified training objective.\n\n4. The authors compare with over twenty baselines across multiple imbalance configurations and datasets.\n\n5. The paper is well structured, with helpful visualizations (e.g., Figures 1–4)."}, "weaknesses": {"value": "1. The proposed NDCL mainly combines known components: (a) The “negative-dominant” contrastive loss is a simple reformulation of the InfoNCE / SupCon objective with reversed emphasis; similar ideas exist in hard-negative mining and OOD contrastive learning. (b) The re-weighted CE and prototype alignment are standard practices in long-tailed learning and multi-domain contrastive frameworks.\nThe overall design feels incremental rather than conceptually new.\n\n2. The presented generalization bound resembles existing DG theory with added posterior and margin terms. The derivation lacks rigor and empirical verification. The connection between the bound and NDCL’s specific loss terms is qualitative rather than principled.\n\n3. Improvements over strong baselines (e.g., Fish, PGrad, BoDA, SAMALTDG) are within 1–2% and not statistically analyzed.\nThe paper lacks confidence intervals or significance tests, and does not show clear advantages in difficult subgroups.\n\n4. NDCL introduces multiple intertwined losses (contrastive, reweighted CE, posterior alignment) and additional hard-negative mixup.\nThe motivation for each component is scattered and lacks concrete ablation or efficiency analysis (e.g., computational cost, convergence stability).\n\n5. Although the authors claim to enlarge margins and reduce posterior discrepancy, these quantities are not quantitatively measured or visualized beyond a brief example. The lack of deeper diagnostic analysis weakens the paper’s empirical evidence.\n\n6. The paper is lengthy and occasionally repetitive. The theoretical and methodological sections could be more concise and better connected."}, "questions": {"value": "1. How does the proposed generalization bound differ mathematically from previous DG bounds, beyond adding a posterior term?\n\n2. Can you provide empirical evidence that the NDCL loss indeed enlarges margins or reduces posterior discrepancy?\n\n3. How sensitive is NDCL to hyperparameters (e.g., α, β, ρ for Beta mixup)?\n\n4. What is the computational overhead compared with standard SupCon or BoDA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ypk6ZwotGd", "forum": "gUzdPLGq0y", "replyto": "gUzdPLGq0y", "signatures": ["ICLR.cc/2026/Conference/Submission11631/Reviewer_dTj5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11631/Reviewer_dTj5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986258511, "cdate": 1761986258511, "tmdate": 1762922702866, "mdate": 1762922702866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies Imbalanced Domain Generalization (IDG), where both domain shift and label imbalance jointly degrade model generalization. The authors first present a theoretical generalization bound under the H-divergence framework, revealing that the key to IDG lies in controlling the posterior discrepancy and decision margin across domains rather than merely aligning feature distributions.\nTo address this, the authors propose Negatives-Dominant Contrastive Learning, a contrastive learning framework emphasizing negative samples as the dominant supervision signal. Experiments on VLCS, PACS, and OfficeHome with three imbalanced setups show consistent improvements over prior domain generalization and imbalance baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly structured and easy to follow.\n2. The derived generalization bound motivates the design choices in NDCL, linking domain discrepancy and class imbalance to posterior alignment.\n3. Instead of relying on positive pair supervision, it is interesting that NDCL focuses on negative gradients to enlarge inter-class margins, which is conceptually fresh and practically effective.\n4. Several experiments validate the proposed method, which improves significantly compared to previous methods."}, "weaknesses": {"value": "1. Although the authors provide a theory, it seems not to be related to the label imbalance and domain imbalance.\n2. It is better to provide the visualization of the learned feature.\n3. The experiments should also be done on the large-scale DomainNet dataset to further demonstrate the effectiveness of the method."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Unx6gogJyA", "forum": "gUzdPLGq0y", "replyto": "gUzdPLGq0y", "signatures": ["ICLR.cc/2026/Conference/Submission11631/Reviewer_nZqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11631/Reviewer_nZqW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996238868, "cdate": 1761996238868, "tmdate": 1762922702350, "mdate": 1762922702350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to the Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their constructive feedback and insightful comments.\nWe are encouraged that the reviewers recognized the importance of the problem setting, the theoretical insight, the methodological novelty, the clarity of presentation, and the strength of the empirical results.\nWe highlight our revised text or added parts **in blue font** in the revised manuscript.\nBelow, we provide *concise responses* to the main concerns.\n\n> **1. Reclaim our Contributions**\n\nOur NDCL framework is designed as a *unified* contrastive paradigm **tailored for IDG**, with the goal of directly shaping the decision boundary.\nIts primary contribution lies in **leveraging negatives as the dominant signal to enlarge the decision margin**, a mechanism never explored in prior work (see the discussion distinguishing NDCL from SupCon and InfoNCE in **lines 210–236**, **Section B**, and **Subsection E.5**).  \nAligned with our theoretical formulation *derived by joint distributions*, NDCL further introduces **a prediction-central contrastive strategy** to align the posterior distribution, and employs **a dynamic penalty on samples near the decision boundary**, rather than reweighting minority classes, to strengthen intra-class compactness.\n\nThese components are **not incremental additions**, but rather **principled**, **mutually reinforcing mechanisms** that co-regulate the decision boundary and collectively address the challenges inherent in IDG.\n\n> **2. Dataset Statistic and Computational Cost**\n\n**As already provided in the original submission**, Subsection E.1 visualizes the domain statistics and Tab. 5 summarizes the imbalance ratios for each setting.\nSubsection E.7 also reports the computational cost in Tab. 9, showing that NDCL's cost *is comparable to competing methods*.\n\n> **3. Sensitivity of Hyperparameters**\n\n**As already provided in the original submission**, Fig. 9 in the Supplementary Material visualizes the sensitivity of the trade-off parameters $\\alpha$ and $\\beta$.\nIn **Fig. 10**, we further present the sensitivity analysis of the Beta distribution parameter $\\rho$.\n\n> **4. Quantitative Evaluation**\n\nFig. 4 in the main manuscript visualizes per-class margins and posterior discrepancies on the target domain.\nBeyond this, we further provide quantitative estimates in **Tab. 10** and **Tab. 11** of the Supplementary Material, where we evaluate the margin- and posterior-related metrics and analyze their **Pearson correlations with target accuracy**.\nThe results consistently show that both quantities are **strongly correlated with generalization performance**, supporting the theoretical claims.\n\n> **5. Significance Test**\n\nWe evaluate statistical significance using the Friedman test, as visualized in **Fig. 11** of the Supplementary Material.\nThe results show that **NDCL achieves statistically significant improvements over nearly all competing methods**, with the exception of PGrad."}}, "id": "iysP3GMEV1", "forum": "gUzdPLGq0y", "replyto": "gUzdPLGq0y", "signatures": ["ICLR.cc/2026/Conference/Submission11631/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11631/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission11631/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763627104576, "cdate": 1763627104576, "tmdate": 1763627104576, "mdate": 1763627104576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}