{"id": "W2eNfLmCHY", "number": 9506, "cdate": 1758125349687, "mdate": 1763579779527, "content": {"title": "DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining", "abstract": "Videos captured in the wild often suffer from rain streaks, blur, and noise. In addition, even slight changes in camera pose can amplify cross-frame mismatches and temporal artifacts. Existing methods rely on optical flow or heuristic alignment, which are computationally expensive and less robust. To address these challenges, Lie groups provide a principled way to represent continuous geometric transformations, making them well-suited for enforcing spatial and temporal consistency in video modeling. Building on this insight, we propose DeLiVR, an efficient video deraining method that injects spatiotemporal Lie-group differential biases directly into attention scores of the network. Specifically, the method introduces two complementary components. First, a rotation-bounded Lie relative bias predicts the in-plane  angle of each frame using a compact prediction module, which normalized coordinates are rotated and compared with base coordinates to achieve geometry-consistent alignment before feature aggregation. Second, a differential group displacement computes angular differences between adjacent frames to estimate a velocity. These biases are combined with temporal decay and a banded attention mask to emphasize short-range reliable relations while suppressing long-range noise. DeLiVR achieves sharper details, fewer rain remnants, and stronger temporal coherence on both synthetic and real rainy benchmarks.", "tldr": "", "keywords": ["Video Restoration", "Lie Groups", "Positional Bias"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71db8fb3e1893fdbb3f05df8302bc157b2a7e7bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DeLiVR, a video deraining framework that leverages spatiotemporal Lie-group differential biases to enforce geometric consistency across frames. By embedding Lie-group formulations directly into the attention mechanism, the method effectively models continuous transformations, reducing reliance on optical flow or heuristic alignment. It introduces two key components: (1) a rotation-bounded Lie relative bias for geometry-consistent spatial alignment, and (2) a differential group displacement that captures inter-frame angular velocity to guide temporal attention. Experimental results on multiple benchmarks validate the approach’s superior performance and efficiency compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces Lie group theory into video deraining for the first time, providing a principled geometric prior that replaces traditional optical flow with continuous transformation modeling, leading to more robust spatial–temporal consistency.\n2. The proposed differential spatiotemporal Lie Bias, composed of the rotation-bounded Lie relative bias and differential group displacement, is elegantly integrated into the attention mechanism. This design explicitly encodes rotational and velocity information, effectively capturing dynamic rain streak variations in real-world videos.\n3. The paper is clearly written with solid motivation, detailed methodological exposition, and comprehensive experimental validation, making the technical contributions easy to follow and well-justified."}, "weaknesses": {"value": "1. While the paper introduces Lie group theory as a prior for modeling continuous transformations, the theoretical connection between the proposed Lie biases and actual motion manifolds is not fully discussed. It would be helpful to clarify how the chosen Lie algebra parameterization ensures stability or accuracy under complex non-rigid motion.\n2. The paper claims that VDMamba (CVPR 2025) achieves slightly higher PSNR (36.29 vs. 34.06) and SSIM (0.973 vs. 0.952) but worse perceptual quality (LPIPS 0.010 vs. 0.039). However, the LPIPS metric indicates that VDMamba actually achieves better perceptual results. Therefore, the corresponding discussion in the paper is not rigorous and should be revised to ensure the accuracy of experimental interpretation.\n3. The downstream application evaluation is limited to the Rain-Syn-Complex dataset, which is synthetic. Including evaluations on real captured tasks would make the results more convincing."}, "questions": {"value": "In Table 3, both the S2VD score (26.78) and the proposed method’s score (82.52) are highlighted in bold. It is recommended to use consistent formatting or clarify the highlighting rule to avoid confusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PYaRGQ2PN2", "forum": "W2eNfLmCHY", "replyto": "W2eNfLmCHY", "signatures": ["ICLR.cc/2026/Conference/Submission9506/Reviewer_KGvX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9506/Reviewer_KGvX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530817457, "cdate": 1761530817457, "tmdate": 1762921080489, "mdate": 1762921080489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DeLiVR, a method that injects spatiotemporal Lie-group differential biases into Transformer attention scores to achieve geometry-consistent alignment without relying on optical flow. The core contributions include a rotation-bounded Lie relative bias for spatial alignment and a differential group displacement for temporal consistency. Experiments on synthetic and real-world benchmarks demonstrate improved rain removal, detail preservation, and temporal consistency compared to existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-This work addresses a salient problem in video restoration caused by camera rotation, namely spatial and temporal misalignment. The proposed Lie group based in-plane rotation estimation directly targets this challenge, and the method is well motivated with a clear and coherent rationale.\n\n-The proposed method achieves state of the art overall performance on public benchmarks and has an advantage in inference speed."}, "weaknesses": {"value": "-The in-plane rotation estimation proposed in this paper appears to target only the camera itself. In principle, the scheme applies to any video restoration task and has no obvious relevance to deraining, that is, the authors did not design for the tilt angle of rain. This makes the paper somewhat tenuous as a dedicated video deraining approach.\n\n-Camera shake is inherently 3 dimensional, raising doubts that a in-plane model can resolve the stated issue. The evidence presented in the paper is limited and does not convincingly support claims of mitigating cross frame misalignment and temporal artifacts. Since the method explicitly targets rotation, the evaluation should isolate rotation induced errors and demonstrate corresponding gains; modest and unstable aggregate improvements are not sufficient.\n\n-The authors deny the robustness of optical‑flow approaches and claim that unreliable optical flow will harm such video restoration models. This view should be supported by more solid comparative experiments, given that optical‑flow‑based methods are indeed mainstream in this field. In addition, since the authors introduce physical guidance by embedding a bias for the rotation angle, they should compare with other approaches that use bias modulation. Most straightforwardly, I believe there should be a comparison under the same baseline and modulation scheme between using optical flow for modulation and using the predicted rotation angle, which would directly show that the proposed rotation estimation is more reliable than optical flow.\n\n-The organization and presentation of this paper are not yet at a publishable level. For example, most of Section 3 consists of existing formulas used as background, with few formulas of substantive value that are newly derived by the authors, which makes it hard to grasp the key points. The figures are rather rough (in Fig. 3 many panels do not clearly show differences in details), and some figures do not match their captions (Fig. 4)."}, "questions": {"value": "-What is the specific connection between the proposed method and rain? Is the predicted rotation that of the camera or the tilt angle of the rain streaks? Overall, what I most want to know is whether the method can transfer to video restoration tasks under other degradations, what its scope of applicability is, and whether it can handle three‑dimensional camera shake.\n\n-The results of this paper require more interpretability validation. I would like to see examples where the proposed method specifically addresses cross frame misalignment and temporal artifacts, rather than only showing improvements in overall metrics.\n\n-Since the authors claim that optical‑flow‑based methods are not robust, they should conduct controlled experiments under the same baseline to demonstrate that the proposed rotation prediction provides more reliable guidance than optical flow. At present, the paper discusses this conclusion only in general terms and lacks strong experimental evidence.\n\n-The paper requires more proofreading and fine‑grained refinement in details, and both the method illustrations and the figures need a more polished presentation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0GFond9hSW", "forum": "W2eNfLmCHY", "replyto": "W2eNfLmCHY", "signatures": ["ICLR.cc/2026/Conference/Submission9506/Reviewer_ajhU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9506/Reviewer_ajhU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553089137, "cdate": 1761553089137, "tmdate": 1762921079652, "mdate": 1762921079652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Reviewer HmPv"}, "comment": {"value": "We appreciate for the insightful observations, which correctly point out key limitations and help clarify the scope of our method.\n\n**Response for Q1:**\n\nIndeed, our current formulation focuses on in-plane rotations (SO(2)), primarily motivated by two considerations:\n\n(1) Most adverse-weather video datasets exhibit camera motion dominated by small-angle in-plane rotations rather than large 3D translations;\n\n(2) Using SO(2) ensures computational efficiency and analytic tractability within attention bias computation.\n\nNevertheless, our design is **not inherently limited** to SO(2). As detailed in Appendix A.1, the use of Lie groups provides a mathematically general foundation. We view the present SO(2) instantiation as a compact and stable starting point demonstrating the feasibility and interpretability of the Lie-bias paradigm.\n\n**Response for Q2:**\n\nThank you for this critical question. Our mechanism is designed to avoid over-reliance on geometry by using it as a \"bias,\" not a \"hard constraint.\"\n\nOur core attention formula is $Logits = \\frac{QK^{\\top}}{\\sqrt{d}} + B_{total}$ . This is a hybrid system:\n\n1.  **Data-Driven ($QK^{\\top}$):** This term is based purely on content and semantics. When motion is irregular or occlusions occur—where scene semantics dominate—the resulting feature mismatch will cause $QK^{\\top}$ to produce a large negative value.\n2.  **Geometric Prior ($B_{total}$):** Our Lie bias acts as a \"guide.\"\n\nIn cases of irregular motion (like occlusions), the strong penalty (large negative value) from $QK^{\\top}$ will override the incorrect geometric bias. Consequently, the $softmax$ will still assign a near-zero attention weight to these incorrect correspondences.\n\nOur model adapts by allowing data-driven semantics ($QK^{\\top}$) to dominate whenever the geometric prior ($B_{total}$) is insufficient or incorrect."}}, "id": "KSGhXWr3MZ", "forum": "W2eNfLmCHY", "replyto": "W2eNfLmCHY", "signatures": ["ICLR.cc/2026/Conference/Submission9506/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9506/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9506/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763571205265, "cdate": 1763571205265, "tmdate": 1763571205265, "mdate": 1763571205265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DeLiVR, a video deraining method that injects spatiotemporal Lie group-based biases into Transformer attention layers, enabling geometry-consistent feature alignment and motion-aware modeling. It achieves strong performance on synthetic and real-world benchmarks, with enhanced efficiency and robustness over prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel Use of Lie Groups in Attention: Introduces a principled and efficient method to incorporate SO(2) Lie algebra into Transformer attention, improving spatiotemporal consistency without optical flow.\n\nStrong Generalization: Achieves SOTA or competitive results on synthetic datasets and real-world benchmarks like WeatherBench, with improved downstream performance.\n\nElegant Design and Efficiency: Uses lightweight geometric predictors instead of full equivariant models, maintaining inference speed and model compactness (2.64M params vs 12–58M in baselines).\n\nThorough Ablation and Theoretical Justification: Detailed experiments, regularization analysis, and Lie group derivations demonstrate rigorous understanding and contribution clarity."}, "weaknesses": {"value": "SO(2)-only Limitation: The model restricts itself to in-plane rotation (SO(2)) and may underperform on more complex real-world motion (e.g., 3D translations, SE(3)), as briefly acknowledged.\n\nOver-reliance on Geometry: The spatiotemporal bias fusion mechanism may not sufficiently adapt when motion is irregular or scene semantics dominate (e.g., occlusions, deformable objects)."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C9ij3Lv3mp", "forum": "W2eNfLmCHY", "replyto": "W2eNfLmCHY", "signatures": ["ICLR.cc/2026/Conference/Submission9506/Reviewer_HmPv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9506/Reviewer_HmPv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762222233370, "cdate": 1762222233370, "tmdate": 1762921079302, "mdate": 1762921079302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Reviewer KGvX"}, "comment": {"value": "Thank you for these insightful questions, which cuts to the core of our method. \n\n\n**Response for Q1:**\n\n1. What \"Manifold\" We Are Modeling: \n\nYou are correct that $SO(2)$ is insufficient to model the complex, non-rigid motion manifold of rain and scene dynamics. We want to clarify that this is not our claim. Our $SO(2)$ Lie bias is **not** intended to model the complex *non-rigid* motion of rain. Instead, its sole purpose is to explicitly model and correct for a much simpler, low-dimensional rigid motion manifold: the global in-plane camera rotation.\nThis camera motion is a primary *artifact* that corrupts inter-frame alignment, making it difficult for the network to distinguish rain from actual scene content. Our \"rotation-bounded Lie relative bias\" effectively \"peels off\" this rotational artifact, pre-aligning features into a geometrically-consistent space *before* the attention operation.\n\n\n2. How This Ensures Stability and Accuracy for Non-Rigid Motion:\n\n- Stability: The Lie algebra $SO(2)$ parameterization ensures the stability of the alignment step itself. As detailed in Appendix A.1, optimization occurs in the stable, linear (Euclidean) tangent space, and the exponential map guarantees the resulting transform is always a *valid* rotation by construction. This avoids the instability of optimizing unconstrained matrices.\n- Accuracy: By using the $SO(2)$ head to explicitly handle rigid camera rotation, we simplify the problem for the Transformer backbone. The attention mechanism, now operating on stabilized features, no longer has to waste capacity trying to implicitly separate camera jitter from the actual non-rigid rain dynamics. It can focus its full capacity on the non-rigid task: modeling and removing the rain streaks. This \"divide-and-conquer\" approach is precisely how we achieve stability and accuracy, even under complex non-rigid motion. \n\nIn summary, we acknowledge this is a simplified prior (as noted in our Limitations, Sec. 4.4). Howerve, our Lie group prior is not for modeling the complex scene dynamics, but for creating a stable, aligned feature space that enables the network to learn those complex dynamics more accurately.\n\n\n\n**Response for Q2:**\n\nWe appreciate the reviewer for catching this inconsistency. We have revised. Thank for highlighting this issue.\n\n\n\n**Response for Q3:**\n\nThank you for pointing out the importance of assessing downstream performance on real captured data.\nTo address this concern, we additionally evaluated our derained outputs on a real-world rainy scene using an off-the-shelf object detector. As shown **in Fig. 8**, the detector applied to the derained image identifies objects more accurately, including an additional pedestrian that is missed in the raw rainy input."}}, "id": "JkK68PulfR", "forum": "W2eNfLmCHY", "replyto": "W2eNfLmCHY", "signatures": ["ICLR.cc/2026/Conference/Submission9506/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9506/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9506/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763571836896, "cdate": 1763571836896, "tmdate": 1763571836896, "mdate": 1763571836896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Reviewer ajhU"}, "comment": {"value": "Thank you for these insightful questions, which address the core assumptions and applicability of our work.\n\n**Response for Q1:**\n\nThe predicted rotation corresponds to camera pose variation, not rain-streak tilt. By correcting global in-plane motion, the model stabilizes the background and more effectively separates transient rain streaks.\n\nAlthough developed for deraining, the geometric prior is degradation-agnostic. We therefore conducted additional experiments on dehazing and deblurring, and the results **(Appendix Fig. 7)** show that our method remains effective across these tasks.\n\nWe acknowledge that modeling only SO(2) limits the handling of full 3D motion. As noted in Sec. 4.4, our formulation is extensible to SE(3), which we identify as a promising direction for future work.\n\n\n\n**Response for Q2:**\n\nTo directly demonstrate how the proposed Lie-group differential bias addresses *cross-frame misalignment* and *temporal artifacts*, we conducted a new controlled experiment in which we artificially introduce small in-plane rotations into several frames of the input sequence. \nNew attention visualizations **(Figure 5)** show that the rotation-aware model captures richer directional dependencies. The attention-difference maps reveal that rotation bias selectively boosts attention around motion-sensitive structures, reducing flicker and ghosting. \n\nWe have included these new results and visualizations in the main text **(Sec. 4.3: Ablation Study)**, which clearly show how the method corrects cross-frame misalignment and enhances temporal consistency.\n\n\n\n**Response for Q3:**\n\n\nWe agree that a controlled comparison against optical-flow-based guidance is necessary to substantiate our claim regarding robustness. To address this, we conducted additional experiments using two representative flow-based recurrent video restoration models on the WeatherBench dataset:\n\n- Frame-Consistent Recurrent Video Deraining with Dual-Level Flow:\n\n\tPSNR 23.33 / SSIM 0.691 / LPIPS 0.455\n\n- High-Resolution Optical Flow and Frame-Recurrent Network for Video Super-Resolution and Deblurring:\n\n\tPSNR 24.10 / SSIM 0.716 / LPIPS 0.402\n\nUnder the same evaluation setting on A100, our rotation-prediction-based model surpasses both flow-guided baselines.\n\n\n\n**Response for Q4:**\n\nIn the revised version, we have revised Figure 4 to ensure full consistency between the figure content and caption.  Additionally, we have added high-resolution visualizations of Figure 3 **in the Appendix A.5** to improve readability and enable clearer comparison of fine-grained details across methods."}}, "id": "ogfHM6oHrV", "forum": "W2eNfLmCHY", "replyto": "W2eNfLmCHY", "signatures": ["ICLR.cc/2026/Conference/Submission9506/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9506/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9506/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763572347382, "cdate": 1763572347382, "tmdate": 1763572347382, "mdate": 1763572347382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All Reviewers"}, "comment": {"value": "We appreciate all reviewers’ constructive feedback, and genuinely hope that these updates help reinforce your recognition of our work."}}, "id": "kjEWamtKUs", "forum": "W2eNfLmCHY", "replyto": "W2eNfLmCHY", "signatures": ["ICLR.cc/2026/Conference/Submission9506/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9506/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9506/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763572753946, "cdate": 1763572753946, "tmdate": 1763572753946, "mdate": 1763572753946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}