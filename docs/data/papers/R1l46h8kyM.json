{"id": "R1l46h8kyM", "number": 18372, "cdate": 1758286946782, "mdate": 1759897107971, "content": {"title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver", "abstract": "While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints.", "tldr": "The paper introduces the Generalized Adversarial Solver, a simplified diffusion sampling acceleration framework combining distillation and adversarial training to achieve superior generation quality over existing methods.", "keywords": ["diffusion models", "diffusion acceleration", "diffusion distillation", "ODE solvers", "adversarial training"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7dcfdbcf4ef404af4d613d81b80a66363048844a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an ODE solver distillation method for diffusion models that augments prior work with: an adversarial loss, a better initialization inspired by theory in other ODE solvers, and deliberate over parameterization. The goal is to improve sample quality at low NFE. Experiments and ablations indicate consistent gains over prior work, with the improvements most pronounced at low NFE. Ablations also isolate the utility of the adversarial loss and the theoretically sound initial parameterization. The work also provides useful evidence on cross dataset generalization and argues for faster training relative to consistency model or progressive distillation approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work improves the performance of the ODE solver distillation paradigm.\n\n- Thorough ablations demonstrate the contribution of each component, with the adversarial loss boosting performance at low NFE.\n\n- Cross dataset generalization results in the appendix are strong and main text worthy.\n\n- The appendix argument for faster training relative to consistency models or progressive distillation strengthens its practical appeal within the solver distillation family."}, "weaknesses": {"value": "- The paper should explicitly state whether training is data free. By equations 7 and 22, no dependence on training data is apparent, yet in section 4.3 \"Dataset size\" is used and it is unclear. It seems to refer to the number of samples generated by the teacher, as parts of the appendix suggest.\n\n- Line 475 claims the generalized solver parameterization \"significantly accelerates training\" relative to existing parameterizations. Section 4.2 compares final performance to S4S but does not show convergence speed. Appendix mentions the time required is similar to S4S (line 910).\n\n- There exist methods that do not optimize the ODE solver but improve samples directly and can outperform solver distillation at very low NFE (for example Truncated Consistency Models [1]). The paper should acknowledge this limitation of the solver distillation class more clearly.\n\n- The paper candidly acknowledges two important weaknesses shared by ODE solver distillation: per NFE training and backpropagation through the whole solver inference.\n\n- Line 129 links \"Table 2\" to the table in this paper, but it seems to refer to a table in another work.\n\n- The mode collapse discussion in the appendix line 1238 is hand wavy. Consider a quantitative method like coverage from Reliable Fidelity and Diversity Metrics for Generative Models [2]. (This is a minor issue as the claim is in the appendix and the method only changes the ODE-solver and not the main model)\n\n\n[1] Lee, Sangyun, et al. \"Truncated Consistency Models.\" The Thirteenth International Conference on Learning Representations.\n\n[2] Naeem, Muhammad Ferjad, et al. \"Reliable fidelity and diversity metrics for generative models.\" International conference on machine learning. PMLR, 2020."}, "questions": {"value": "- Line 243: Please include the exact formula used for the initialization adapted to your notation. The cited paper does not provide sufficient detail on the higher order 3M variant.\n\n- In Eq. 22, the meaning of p_T Y_T is not specified. Is it to indicate a different noise is sampled from the prior for the teacher.\n\n- By Eqs. 7 and 22 it appears training does not depend on real training data. Then what does Sec. 4.3 \"Dataset size\" vary: number of noise samples, or a teacher generated dataset. Do you first generate a dataset with the teacher and then train on it? If so  are the generated samples used multiple times for training? (appendix suggests yes to both but it would be better to mention these details in main text as well)\n\n- Did you test lower NFEs  than reported (one or two)? If so, what failed or succeeded?\n\n- Do the learned time steps t_n + \\xi_n increase with n? or does training learn a mixed or non monotone schedule?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qochXePX32", "forum": "R1l46h8kyM", "replyto": "R1l46h8kyM", "signatures": ["ICLR.cc/2026/Conference/Submission18372/Reviewer_kJGM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18372/Reviewer_kJGM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610361451, "cdate": 1761610361451, "tmdate": 1762928080542, "mdate": 1762928080542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach to training solvers for fast sampling by combining a new parameterization with an adversarial loss. The proposed method enhances the alignment between the generated and teacher distributions, outperforming traditional teacher-matching losses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Effective Use of Adversarial Loss**  \n   The incorporation of an adversarial loss is both intuitive and impactful, substantially improving distribution alignment between the generated samples and the teacher model.  \n\n2. **Clarity and Organization**  \n   The paper is clearly written and well-structured, making it easy to follow the methodology and results.  \n\n3. **Flexible and Innovative Parameterization**  \n   The proposed parameterization, distinct from that of S4S [1], introduces additional flexibility to the solver. This results in measurable improvements in both performance and generalization."}, "weaknesses": {"value": "1. **High Training Cost from Adversarial Loss**  \n   While the adversarial loss improves overall quality, it significantly increases the training cost. As shown in Table 8, the training time is more than double that of GS (without adversarial loss), raising questions about computational efficiency.  \n\n2. **Questionable Initialization Strategy**  \n   The solver learns coefficients for a linear multistep method but initializes some parameters from the DPM solver [2], which belongs to the exponential integrator family. Since exponential integrators differ fundamentally from linear multistep methods, this initialization may lead to inconsistent behavior and a theoretically unsound starting point.  \n\n3. **Dependence on a Pretrained GAN**  \n   The methodâ€™s reliance on a pretrained GAN introduces unnecessary complexity and limits its generalizability. This dependency may hinder reproducibility and make it difficult to apply the method across different architectures.  \n\n4. **Unfair Comparison with S4S**  \n   The reported comparisons with S4S [1] may be misleading, as solver performance heavily depends on the teacher model used. Variations in teacher configurations can skew the results, weakening the validity of the performance claims."}, "questions": {"value": "1. **Ensure Fairness in Training Time Comparisons (Table 7)**  \n   Training times should be reported using the same GPU model to ensure fairness, as differences in hardware can significantly affect latency measurements. Additionally, comparing results under the same number of function evaluations (NFE) would provide a more accurate measure of computational efficiency.  \n\n2. **Align Training Iterations Across Methods (Appendix D.4.1)**  \n   In Appendix D.4.1, GS and GAS appear to have been trained for different numbers of iterations (e.g., 1k for GS vs. 2k for GAS in the text-to-image experiment). This inconsistency complicates fair comparison. Both methods should be trained for the same number of iterations to enable a meaningful evaluation.  \n\n3. **Clarify the Choice of Initialization Solver**  \n   The paper initializes coefficients using the DPM solver [2], which belongs to the exponential integrator family which might affect the theoretical soundess of initializing some coefficients with it.  \n\n4. **Absence of Alternating Training Strategy and Its Rationale**  \n   Unlike S4S [1], the proposed method does not employ an alternating training strategy for the timestep scheduler and solver coefficients. It would be valuable for the authors to elaborate on why their approach doesn't require alternating strategy to stabilize training.  \n\n5. **Retrain S4S for Consistent Comparison**  \n   The S4S results presented are taken directly from the original paper, yet S4S performance is highly sensitive to the choice of teacher model. For example, in the LSUN-Bedroom 256 dataset, S4S underperforms relative to other solvers due to the poor teacher performance used in S4S in the original paper. Retraining S4S under the same setup and teacher model would ensure a fair and unbiased comparison with the proposed GAS method.\n\n---\n\n## References  \n[1] **S4S: Solving for a Diffusion Model Solver.**   \n[2] **DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X614tJaNcG", "forum": "R1l46h8kyM", "replyto": "R1l46h8kyM", "signatures": ["ICLR.cc/2026/Conference/Submission18372/Reviewer_bLXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18372/Reviewer_bLXP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810157307, "cdate": 1761810157307, "tmdate": 1762928078606, "mdate": 1762928078606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the topic of distilling diffusion models into a few-step solver. This line of work focuses on the compute-efficient setting in which the parameters of the *solver*, not the score function/network itself, are learned.\n\nIt proposes a new efficient distilled solver, the generalized adversarial solver (GAS), which puts up state-of-the art results in the 4-10 NFE range on several datasets. \n\nGAS is a combination of two techniques\n- The generalized solver (GS): by parameterizing the next step $x_{n+1}$ as a linear combination of (1) the entire history of step values $\\sum_j^n a_{j, n} x_j$ and (2) the entire history of vector field evaluations $\\sum_j^n c_{j, n}v(x_j, t_j)$. The coefficients $\\{a_{j, n}, c_{j, n}\\}$  are parameterized as additive corrections to theoretically derived coefficients from past work. This contrasts to previous work, which learns only coefficients for the latter.\n- GAS is formulated by adding a relativistic GAN loss term to the standard distance loss (either LPIPS or L1) used to train GS."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think this project has the ingredients to ultimately build a strong paper.\n- The research direction seems logical to me - training the solver might be more efficient than training the model itself.\n- Its two components, GS and the GAN loss, each provide an FID boost over past methods, with the GAN loss's boost being particularly impressive in the 4-6 NFE range. \n\t- The GAN loss in particular is a very natural import from other subfields of diffusion modelling."}, "weaknesses": {"value": "However, to me the paper reads more like a technical report than a top conference paper. A couple of ideas are proposed and results are shown, but not much beyond that in terms of motivation or generalizable insight and analysis.\n-  Very little motivation is given for the way GS is designed. The motivation given (I believe L225 -> 229) is not honestly not very clear. Are you just looking for ways to add more parameters to the solver or is there something more to it? \n\t- As someone familiar with diffusion but without specific expertise in distilling diffusion solvers, I found the technical part, Section 3.1 tough to read, in large part because there is not much setup and motivation for the exposited ideas.\n\t- (However, I do think the motivation for the adversarial loss is clearer, as it is simpler and more obvious.)\n- Additionally, neither of the two components of the method are really probed or ablated to understand why they work so well. \n\t- For example, the decision to use additive vs. non-additive coefficients in GS could be ablated easily, but it is not. As could the decision to include the past history of $x_j$s (this is similar to but not the same as S4S). With little motivation and not much ablation, it's hard to glean generalizable insights from the work.\n\t- Additionally, the decision to use the relativistic GANs is not ablated over popular alternative GAN losses.\n- No code has been provided.\n\nIn summary, the two individual components of the method - the parameterization of the solver and the adversarial loss - are incremental, and little motivation, analysis, or generalizable insights are provided. Although I believe someone will find it useful, I do not believe it meets the bar for ICLR."}, "questions": {"value": "- In Table 2, you tagged S4S Alt as unfair due to a different teacher FID score. Where did you find the teacher FIDs for S4S? I did not see them in the S4S paper (but may be missing them).\n - Can you explain the advantage of this approach over PEFT-based distillation of a diffusion model? I would think PEFT was more tractable because there's no need to backpropagate through entire 4-10 step solves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UnK1Y3CBLn", "forum": "R1l46h8kyM", "replyto": "R1l46h8kyM", "signatures": ["ICLR.cc/2026/Conference/Submission18372/Reviewer_JFWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18372/Reviewer_JFWr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955252258, "cdate": 1761955252258, "tmdate": 1762928077863, "mdate": 1762928077863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}