{"id": "TfXpGqBNAQ", "number": 3669, "cdate": 1757494836954, "mdate": 1759898075844, "content": {"title": "Latent-Space Denoising for Causal Representation Learning via Free-Energy-Guided Wasserstein Particle Flows", "abstract": "Learning from corrupted observations is ubiquitous in practice, yet standard training procedures often fail under unknown nonlinear mixing and realistic noise. In causal representation learning (CRL), estimates of latent factors and their causal structure are particularly brittle to such mixing effects. We address this by denoising in a learned latent space, where the corruption approximately follows an additive noise model realized via an embedding encoder. We recover the clean latent distribution by minimizing a free-energy objective function, which couples a Kullbackâ€“Leibler divergence between the convolved clean model and the observed embedding distribution with an entropy regularizer for stability. From this objective function, we  further compute the variational derivatives, derive a weighted Wasserstein gradient, and design an explicit particle flow algorithm to carry out the latent-space denoising. The resulting denoiser functions as a drop-in module for CRL and, across noisy real-world and simulated datasets, improves overall accuracy and structural recovery relative to standard CRL baselines.", "tldr": "", "keywords": ["Additive noise model; Wasserstein gradient flow; Causal representation learning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa7b792d09af3e8fadca903970f30c1d7d939943.pdf", "supplementary_material": "/attachment/43992326c3e867277875232fa1504f66fff6a855.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a latent-space denoising method for learning from data corrupted by unknown nonlinear mixing and noise. It uses an encoder to map observations into a low-dimensional latent space, where the corruption approximately becomes additive noise. The clean latent distribution is recovered by minimizing a free-energy objective:\n$E(\\rho) = D_{\\mathrm{KL}} \\left(q_{\\rho} || \\nu\\right) + \\lambda \\mathrm{Ent}(\\rho)$,\nwhere $q_{\\rho} = \\rho * \\varphi_{H}$ and $\\varphi_{H}$ is the centered Gaussian kernel.\nThe authors derive the Wasserstein gradient flow of this objective, leading to a particle flow algorithm that \nprogressively denoises the latent representations. \n\nThe experiments show that this latent space denoiser can be used as a plug-in module for causal representation learning (CRL), improving the accuracy and stability of latent factor and causal structure recovery under realistic noise."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-developed theoretical framework supported by convincing experiments with better performance compared to the baselines."}, "weaknesses": {"value": "I am not very familiar with this field, and I found the paper quite difficult to follow. I struggled to understand several of the theoretical details and proofs, and I am still not confident that I fully grasp the details. I strongly recommend that the authors provide clearer explanations and more accessible notation to make the paper easier to follow, especially for readers who are not experts in this area."}, "questions": {"value": "1. I am curious whether the proposed method can be extended to the case of indirect and noisy \nobservations of the clean signal $X$, in the context of ill-posed inverse problems. \nSpecifically, can the framework handle models of the form  $Y = A(X) + \\epsilon$\nwhere $A$ denotes a (possibly ill-posed) forward operator? This formulation is more general than \nthe additive-noise setup presented in the paper.\n\n2. The choice of the latent dimension $d_u$ is not discussed in the experimental section. \nCould the authors clarify how $d_u$ was selected for different experiments and elaborate on how the performance depends on this parameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HrSMqshr90", "forum": "TfXpGqBNAQ", "replyto": "TfXpGqBNAQ", "signatures": ["ICLR.cc/2026/Conference/Submission3669/Reviewer_izqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3669/Reviewer_izqE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925294777, "cdate": 1761925294777, "tmdate": 1762916910451, "mdate": 1762916910451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors attempt to recover latent variable from observation to obtain a causal representation learning framework. The main approach is to model $Y=f(Z) + \\epsilon$, and the first part is to say that X=f(Z) is identifiable and then will use X to recover Z. Experiments on several datasets is ok.\n\nHowever, if my understanding is correct, the proof of identifiability of X is wrong. The basic idea of the proof is to use the fact that the characteristic function of Y equals the multiplication of characteristic of X and $\\epsilon$ and to say that the distribution of X is solely determined by the distribution of $Y$. However, first if we do not assumption the distribution of $\\epsilon$, we cannot get the distribution of $X$. Secondly, the derivation in line 658 is highly like not correct. What do you mean by law of Y? If 658 holds it may be a contradiction with the fact that $X$ and $\\epsilon$ are independent. \n\nAnother problem is to recover Z from X, generally, without further assumptions, just with the condition that $f$ is injective, this would not be possible."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "As the basic framework of the paper do have some technical issues, the only strength comes from the experimental part."}, "weaknesses": {"value": "1. The proof if proposition is highly likely to be wrong.\n\n2. Recover Z from X is in generally not possible."}, "questions": {"value": "See my previous comments about the technique issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3g5uW6yLrw", "forum": "TfXpGqBNAQ", "replyto": "TfXpGqBNAQ", "signatures": ["ICLR.cc/2026/Conference/Submission3669/Reviewer_4hGD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3669/Reviewer_4hGD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994481783, "cdate": 1761994481783, "tmdate": 1762916909949, "mdate": 1762916909949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a technique to adapt existing CRL methods to real-world observations that are corrupted. They do this by latent space denoising, where the embeddings of corrupted data are mapped to behave approximately as additive noise. Then recover a clean latent distribution via free-energy minimisation, combining KL divergence and entropy regularisation for stability. Authors demonstrate that the resulting latent-space denoiser can be seamlessly integrated into existing CRL pipelines, yielding substantial gains in accuracy and causal structure recovery on both real-world and simulated noisy datasets."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper deals with a practical problem of applying CRL methods to real-world noise data\n\n- The idea of performing denoising in the latent space using free-energy minimisation and Wasserstein particle flows is novel\n\n- The authors refer to many works in the field, in-depth literature review"}, "weaknesses": {"value": "- Hard to follow the work, the content is very dense in some parts of the paper; easing it out would help the readers\n\n-  The motivation to link Wasserstein denoising and causal mechanism recovery is mostly hand-wavy \n\n- The nonvanishing characteristic functions argument for identifiability is interesting, while the paper doesn't discuss the limitations of estimation methods \n\n- In lines 193-200 authors discuss how a trained encoder will result in an additive noise scenario, but the constraints required to achieve that are not that clear\n\n- It would be really helpful to provide intuition for your theorem"}, "questions": {"value": "See weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rjlVYdKTwV", "forum": "TfXpGqBNAQ", "replyto": "TfXpGqBNAQ", "signatures": ["ICLR.cc/2026/Conference/Submission3669/Reviewer_s7Xc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3669/Reviewer_s7Xc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762224561138, "cdate": 1762224561138, "tmdate": 1762916909725, "mdate": 1762916909725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of causal representation learning (CRL) from observations that are corrupted by both unknown nonlinear mixing and additive noise. The authors propose a new method called Latent-Space Denoising, which acts as a \"plug-and-play\" module upstream of an existing CRL model. The core idea is to first learn an encoder $h$ that maps noisy observations $Y = X + \\epsilon$ (where $X=f(Z)$ is the nonlinearly mixed clean data) into a latent space $U= h(Y)$. The authors justify that, via a first-order Taylor expansion, the latent representation of the noisy data can be approximated as the latent representation of the clean data plus an additive noise term: $h(Y) \\approx h(X) + J_h(X)\\epsilon$. Given this latent additive noise model, the paper proposes to recover the \"clean\" latent distribution $p(Z)$ from the \"noisy\" latent distribution $p(U)$ by minimizing a free-energy objective. This optimization is implemented using a free-energy-guided Wasserstein particle flow. The resulting denoised latent representations are then fed into a standard CRL method (in this case, CCRL) to identify the underlying causal system. Experiments on synthetic and semi-synthetic benchmarks show that adding the Latent-Space Denoising module improves the performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Problem Significance: The paper addresses the critical and practical limitation of CRL methods, which often assumes the observations are the noiseless mixing of latent causal variables.\n\nOriginality: The proposed  Latent-Space Denoising method, which combines a learned encoder with a free-energy-guided particle flow for denoising in the latent space to help causal identification, is a novel and interesting approach.\n\nMethodological Novelty: The application of Wasserstein particle flows to recover a clean latent distribution for a downstream discriminative task (CRL) looks like a technically non-trivial contribution."}, "weaknesses": {"value": "Weak Justification for Latent Additivity: The paper's primary assumption that additive noise in the observation space translates to an approximate additive noise model in the latent space based on a first-order Taylor approximation. This argument is not convincing to me, as it seems to rely on the component wise identifiability of the inverse of the ground truth mixing map. Even in the noise-free cases, oftentimes the component wise identifiability cannot be guaranteed (only group wise). Even under strong conditions where the component wise identifiability can be achieved, with the additional noise in the observation, the identifiability does not automatically hold. Plus, when the noise is large the first-order Taylor approximation may not perform well.  Thus, the validity of this approximation, which is essential to the method, is not sufficiently investigated and justified.\n\nIdentifiability from Noisy Data: The paper does not theoretically address how its two-stage process (denoising then identification) impacts the identifiability guarantees of the baseline CRL model. Standard CRL identifiability results are for noise-free data. The authors need to provide justification that their denoising step successfully recovers a representation from which the true causal factors are still identifiable.\n\nLimited \"Plug-and-play\" Validation: The paper claims LSD is a \"plug-and-play\" module, but this is only demonstrated by integrating it with CCRL. To substantiate this claim, the authors should show results with other, diverse CRL methods (e.g., VAE-based or other contrastive methods) to prove its general applicability. Plus, it would help to also compare against the most obvious and direct baseline: denoising in the observation space and then applying the CRL algorithm."}, "questions": {"value": "1. Could the authors provide more empirical and theoretical evidence for the validity of the first-order Taylor approximation? \n\n2. Why did the authors not compare against the more direct baseline of (1) training a denoising model (e.g., DAE) in the observation space and (2) applying the CCRL model?\n\n3. How does the proposed method provably ensure that the introduced additional steps preserves the necessary identifiability conditions for a baseline CRL model (e.g. CCRL)?\n\n4. To better support the \"plug-and-play\" claim, could the authors provide results from integrating the method with at least one other CRL baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BunEB0QN5K", "forum": "TfXpGqBNAQ", "replyto": "TfXpGqBNAQ", "signatures": ["ICLR.cc/2026/Conference/Submission3669/Reviewer_VWkd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3669/Reviewer_VWkd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762247931728, "cdate": 1762247931728, "tmdate": 1762916909372, "mdate": 1762916909372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}