{"id": "Np9HKxRMcG", "number": 9578, "cdate": 1758128473474, "mdate": 1759897711139, "content": {"title": "SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification", "abstract": "Speculative decoding accelerates LLM inference by verifying candidate tokens from a draft model against a larger target model. Recent \"judge'' decoding boosts this process by relaxing verification criteria by accepting draft tokens that may exhibit minor discrepancies from target model output, but existing methods are restricted by their reliance on human annotations or tasks with verifiable ground truths, limiting generalizability across diverse NLP tasks. We propose SelfJudge, which trains judge verifiers via self-supervision of the target model. Our method measures semantic preservation by assessing whether token-substituted responses preserve the meaning of original responses, enabling automatic verifier training across diverse NLP tasks. Our experiments show SelfJudge achieves superior inference-accuracy trade-offs than judge decoding baselines, offering a broadly applicable solution for faster LLM inference.", "tldr": "Faster speculative decoding based on judge verification. Our judge verifier is trained through the target model's self-evaluation, which enables generalizing judge verification across diverse NLP tasks.", "keywords": ["Large Language Model", "Speculative Decoding", "Efficient Inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55b8c202f34c0a54d8b1085bc06bdde897f5d22e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a model-based speculative decoding verifier training data generation method using the target model as self-judger, which can generalize across diverse tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The self-judging mechanism shows potential for better generalization and significantly reduces annotation effort."}, "weaknesses": {"value": "1.\tThe paper states that its motivation is to avoid unnecessary fallbacks and improve decoding speed. However, the proposed model-based judger introduces additional verification time, and the paper does not provide evidence of end-to-end speedup.\n2.\tThe method appears to address only token-level semantic matching. This limits its effectiveness, as phrase-level and sentence-level semantic matching remain unresolved.\n3.\tExperiments are conducted on only two relatively small models. Evaluating on larger models would strengthen claims of generalization."}, "questions": {"value": "1.\tThe accuracy seems to degrade faster on Qwen-2.5 compared to Llama-3.1. What explains this discrepancy? Does it indicate that the method is model sensitive?\n2.\tIn Figure 4, the accuracy trends differ between GSM8K and MMLU for varying suffix lengths. What factors contribute to this difference?\n3.\tHow are hyperparameters determined? Are they consistent across tasks and models, or do they require careful tuning for each scenario and model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9PBa7oOuc7", "forum": "Np9HKxRMcG", "replyto": "Np9HKxRMcG", "signatures": ["ICLR.cc/2026/Conference/Submission9578/Reviewer_8uaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9578/Reviewer_8uaU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857965915, "cdate": 1761857965915, "tmdate": 1762921130444, "mdate": 1762921130444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SelfJudge, a self-supervised framework for speculative decoding (SD).\nInstead of relying on human-annotated correctness signals or task-specific ground truths (as in AutoJudge), SelfJudge trains a lightweight verifier using semantic preservation scores computed from the target model itself. The idea is to estimate whether token substitutions preserve meaning, and to accept draft tokens based on semantic coherence rather than strict probability alignment. Experiments on Llama-3 and Qwen models across GSM8K, MATH-500, MMLU, CNN/DailyMail, and LiveCodeBench show modest speed-accuracy trade-offs compared with previous judge-based decoding methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well-structured.\n\nThe integration of self-supervision into speculative decoding is well-motivated.\n\nThe proposed semantic preservation criterion is simple and implementation-friendly, requiring no human labels or task-specific supervision.\n\nExperimental evaluation covers multiple domains and demonstrates general applicability across reasoning, coding, and summarization tasks."}, "weaknesses": {"value": "The main idea of leveraging a model’s own likelihood distribution to verify semantic coherence is conceptually very close to existing self-consistency or self-verification paradigms.\nIn essence, SelfJudge performs a token-level version of self-consistency decoding integrated into speculative decoding.\nWhile the framing as “semantic preservation” is neat, the underlying mechanism does not appear fundamentally new or theoretically distinct from prior work on self-consistency, distribution alignment, or self-reflective verification.\n\nThe reported improvement is small and possibly within experimental noise.\nCompared with AutoJudge, SelfJudge achieves only +0.1 higher accepted token length and slightly lower accuracy drop (–1.0% vs –2.7%).\nNo wall-clock runtime or throughput results are provided, making it difficult to confirm real acceleration.\nOverall, the empirical evidence does not convincingly demonstrate that the proposed semantic verifier yields a substantial or practical speedup.\n\nThe paper also lacks analysis of how the self-supervised signal differs from conventional probability alignment beyond intuitive discussion."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SwqLfXpC1O", "forum": "Np9HKxRMcG", "replyto": "Np9HKxRMcG", "signatures": ["ICLR.cc/2026/Conference/Submission9578/Reviewer_EnVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9578/Reviewer_EnVn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041269375, "cdate": 1762041269375, "tmdate": 1762921130026, "mdate": 1762921130026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SelfJudge, a verifier for speculative decoding (SD) that relaxes strict token alignment between draft and target models. Instead of requiring that the draft token exactly match the target token, SelfJudge accepts draft tokens that are semantically compatible with the target model’s own generations. This differs from some prior work, which relies on human annotations and accordingly performs well per task. The model (verifier) relies on data generated from the target model and is trained to preserve semantic compatibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using the target model itself to score token substitutions via a bidirectional likelihood difference is reasonable and avoids human annotation or task-specific ground truth. \n\n- Using the bidirectional context to score and verify the generated labels."}, "weaknesses": {"value": "- The bidirectional context is not easily understandable. The core scoring formula is non-trivial, and the paper jumps rather quickly from the conceptual definition. It would be helpful to include a concrete toy example in the main text, illustrating how the suffix likelihood behaves.\nAlso, it should be stated more explicitly that “bidirectional” is purely a training-time construction.\n\n- The two-stage verification logic is underspecified / slightly inconsistent. Section 3.2 defines verification as “Accept (d_t)​ if Verifier(​h_t) > θ, otherwise reject”, but then later states that tokens rejected by the verifier are passed to alignment-based verification.\n\n- There is no ablation on two-stage verification. It would be useful to see: (1) Judge-only verification (no fallback to alignment-based SD), (2) SelfJudge + fallback, as proposed,  to quantify how much the fallback contributes to quality and speed.\n\n- Scaling to larger target models is not demonstrated."}, "questions": {"value": "- All experiments use 7–8B-scale targets (Llama-3.1-8B, Qwen-2.5-7B), with the argument that data generation is already much more efficient than AutoJudge. It would be helpful to at least discuss: How expensive would SelfJudge data generation be for a 70B model? Whether the cost remains acceptable if you want many more labels (e.g., to train a higher-capacity verifier).\n\n- Could you add a short, concrete example in the main text showing how the semantic preservation score is computed for one token replacement, and explicitly highlight which part corresponds to the prefix vs suffix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "apO5J7k3e3", "forum": "Np9HKxRMcG", "replyto": "Np9HKxRMcG", "signatures": ["ICLR.cc/2026/Conference/Submission9578/Reviewer_TxLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9578/Reviewer_TxLa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180089168, "cdate": 1762180089168, "tmdate": 1762921128368, "mdate": 1762921128368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}