{"id": "Dd86hsSam5", "number": 8246, "cdate": 1758076133059, "mdate": 1759897796486, "content": {"title": "Seeing What’s Not There: Negation Understanding Needs More Than Training", "abstract": "Understanding the negation in a sentence is an important part of compositional\nunderstanding and logic in natural language. Many practical AI applications, such\nas autonomous driving, include precise instruction with negations. For example,\nfollowing instruction to an AI assistant ”locate a parking spot without a vehicle”\nrequires the assistant to not confuse between presence and absence of vehicles. Al-\nthough joint embedding-based Vision Language Models (VLMs) like CLIP have\nrevolutionized multi-modal tasks, they struggle to interpret negation. To address\nthis limitation, recently many works proposed to solve the problem through a data-\ncentric approach by introducing additional datasets with hard-negative samples for\nboth image and text data. Contrary to these approaches, we present a zero-shot\napproach to tackle the negation understanding problem. We probe the properties\nof CLIP text embeddings and show that they follow compositional arithmetic op-\nerations, which allow the addition or removal of semantic information directly in\nthe embedding space. We then present a rule-based approach to extract negated\ntext from given caption and then use it to explicitly remove corresponding se-\nmantic information from original embedding, improving negation understanding\nin VLMs. Our approach does not require expensive training process to induce\nnegation understanding into the model, and achieves the state-of-the-art perfor-\nmance on popular benchmark for negation understanding. We improve baseline\nCLIP model performance on NegBench from 25.5% to 67.0% for MCQ and from\n50.9% to 56.1% for retrieval tasks. Even NegCLIP model which is fine-tuned on\nnegtion datasets, our approach boosts its MCQ accuracy from 54.03% to 66.22%\nand retrieval accuracy from 59.25% to 60.1% showing strong performance.", "tldr": "", "keywords": ["Negation", "Zeroshot", "VisionlanguageModels", "MachineLearning", "ComputerVision", "DeepLearning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10308c480d8f3956a206812cae084f6cc94aa39d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- This paper investigates the persistent difficulty of VLMs in handling linguistic negation. Rather than relying on large-scale negation datasets and fine-tuning, the authors propose a training-free embedding correction method that manipulates CLIP’s embeddings directly. By identifying the negated concept via rule-based parsing and removing its semantic component through a projection-based subtraction (plus a neutral anchor addition), the approach aims to improve negation understanding. \n- Experiments on NegBench and CC-Neg show large performance gains over CLIP and fine-tuned NegCLIP baselines. The authors also provide some analyses to show intuitions onto why their proposed method works."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of improving negation understanding through direct, post-hoc manipulation of text embeddings is novel and unconventional to the more standard data-centric strategies. The approach offers a new angle for thinking about compositional semantics in VLMs.\n- The reported improvements on NegBench are substantial, even surpassing models fine-tuned on specialized negation datasets.\n- The embedding-space visualizations (PCA plots) and ablation studies provide qualitative and quantitative insights into how the correction affects embedding geometry.\n- The approach could inspire further work on embedding-space editing, compositional regularization, and interpretability."}, "weaknesses": {"value": "- The main concern is the lack of theoretical grounding. The main motivation that negation corresponds to a linear operation in embedding space is not theoretically or empirically justified. CLIP embeddings are known to be highly entangled, and the projection subtraction used here lacks formal motivation beyond analogy to word-vector arithmetic. The choice of \"neutral\" words also seem arbitrary. This is totally fine as the emperical results are good, but the authors should be careful with their claims.\n- The method’s success on simple caption pairs may not translate to more nuanced natural language understanding (more complex negation patterns)."}, "questions": {"value": "- Strange citation bug, e.g. line 143, Similarly TripletCLIP...\n- Consider using an off-the-shelf tool for negation detection. In fact, your algorithm is quite similar to NegEx (https://pypi.org/project/negspacy/).\n- Improvements of 40–50% absolute accuracy for a post-hoc method seem implausibly high. Did the authors reimplement the CLIP baselines and confirm that those results are comparable with previous works of using CLIP (and NegCLIP) on those benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NEEqlqxc1b", "forum": "Dd86hsSam5", "replyto": "Dd86hsSam5", "signatures": ["ICLR.cc/2026/Conference/Submission8246/Reviewer_aYJg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8246/Reviewer_aYJg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761022375038, "cdate": 1761022375038, "tmdate": 1762920191313, "mdate": 1762920191313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper points out that visual-language models such as CLIP have weak understanding of negation (no/not/without), often exhibiting affirmative bias. The authors propose a zero-training text embedding correction: rules extract the negated concept, estimate and subtract the \"negation direction\" from the embedding, significantly improving negation understanding without fine-tuning the model, and avoiding the decline in generality caused by fine-tuning the negation data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method requires no hard negation data collection or model fine-tuning; it only needs to correct the \"negation direction\" in the CLIP text embedding, resulting in extremely low deployment costs.\n\nOn NegBench, it improved the MCQ of CLIP from approximately 25.5% to approximately 67.0%, and the retrieval rate from approximately 50.9% to approximately 56.1%; it also brought considerable gains to the fine-tuned NegCLIP."}, "weaknesses": {"value": "The method assumes that \"negation\" is approximately linear and directional in the embedding space, but the semantics of \"missing\" different concepts/attributes are not necessarily collinear; using the global d_global may be ineffective or even lead to incorrect offsets for fine-grained concepts.\n\n\nHave you tried non-CLIP text towers (such as BLIP, SigLIP)? Even small-scale experiments should demonstrate the generality of the architecture.\n\nHow does the average overhead compare to standard CLIP text encoding? Is it easy to batch process to support large-scale retrieval? Please include a small performance table.\n\nIf λ is selected on COCO-MCQ, how would the retrieval be affected by fixing that λ to VOC/MSRVTT MCQ?\n\nRule-based negation scope is fragile. Negation scope extraction (Algorithm 1, page 6; Tables 6–8 in Appendix A.1, pages 12–14) has limited coverage of nested clauses, multiple negations, implicit negations (such as “few/barely”), quantifiers/comparison structures, etc. The main experiments are primarily based on NegBench patterns; generalization to real open-domain instructions remains uncertain."}, "questions": {"value": "see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "chtQdVAABT", "forum": "Dd86hsSam5", "replyto": "Dd86hsSam5", "signatures": ["ICLR.cc/2026/Conference/Submission8246/Reviewer_dika"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8246/Reviewer_dika"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841929834, "cdate": 1761841929834, "tmdate": 1762920190796, "mdate": 1762920190796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses VLMs like CLIP’s poor negation understanding (e.g., ignoring \"no\"/\"without\"). Unlike data-centric methods using hard-negative datasets, it proposes a zero-shot, training-free embedding correction approach. In particular, the authors propose a rule-based method to extract negated concepts, then a correction formula removes their semantic info from embeddings (using directional offset and anchor embedding).\n\nEvaluated on NegBench, it boosts CLIP’s MCQ accuracy from 25.5% to 67.0% and retrieval accuracy from 50.9% to 56.1%, outperforming fine-tuned models like NegCLIP."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. This paper is well written and is easy-to-follow.\n\nS2. The performance improvement is clear.\n\nS3. Sufficient visualizations are provided."}, "weaknesses": {"value": "W1. The performance evaluation side is insufficient. Limited methods are compared. VLMs are mentioned but no further study is conducted.\n\nW2. Typos should be corrected.\n\nW3. According to Alg. 1, a set of negator words is utilized. The correctness of these words should be discussed. Besides, the quality of the generated embedding should be studied.\n\nSee questions for more details."}, "questions": {"value": "Q1. Only CLIP is discussed. How about applying to SigLIP and SigLIP2? These encoders are more welcomed in practical use. \n\nQ2. MLLMs use CLIPs to encode features. MLLM benchmarks can also be used to evaluate the performance of VLMs. How about using your method to train a LLaVA-1.6 model. Will the general VQA performance boosted?\n\nQ3. In Alg.1, return missing ' '; ’’ should be `` ''\n\nQ4. Will synonyms influence your algorithm? What is the quality and diversity of corrected embedding?\n\nQ5. What is the general retrieval, VQA, classification of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Overall, I think this is a useful paper.\n\nI will update my rating based on the rebuttal and other reviewers' comments."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t1vyhn0eNL", "forum": "Dd86hsSam5", "replyto": "Dd86hsSam5", "signatures": ["ICLR.cc/2026/Conference/Submission8246/Reviewer_YhKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8246/Reviewer_YhKj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962769861, "cdate": 1761962769861, "tmdate": 1762920189694, "mdate": 1762920189694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper aims to improve negation understanding in embedding-based vision–language models such as CLIP, which struggle to correctly interpret negated textual inputs. Prior approaches have mainly relied on data-centric solutions that fine-tune models on curated negation datasets, but these tend to be resource-intensive and sometimes degrade general performance.\n\nThe authors propose a zero-shot, training-free approach that decomposes a given text query into affirmative and negated parts using a rule-based negation detection algorithm, then applies a linear correction in embedding space to form a corrected “negation-aware” text embedding. The method achieves strong performance improvements on NegBench, boosting CLIP’s retrieval accuracy from 50.9% to 56.1% and multiple-choice (MCQ) accuracy from 25.5% to 67.0%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1- the proposed approach is efficient, zero-shot, and training-free, and easy to integrate into existing CLIP-based pipelines.\n\n2- the paper shows consistent improvements on different model NegBench retrieval and MCQ tasks.\n\n3- the paper is well organized and easy to follow."}, "weaknesses": {"value": "1- the rule-based negation detection might overfit to the NegBench benchmark, which is synthetically generated using structured templates and paraphrasing; therefore, generalization to natural or free-form text is uncertain.\n\n2- as the authors discussed in the limitation section, the method’s coverage of negation cues is incomplete. This approach misses implicit or compositional negations (e.g. “barely visible,” “few people”, \"alone\"), and may mis-handle nested clauses or double negation.\n\n3- although the improvement on the MCQ task appears large (+50% relative), the absolute task structure (4 options) limits interpretability since a +25% absolute increase is effectively equivalent to eliminating one distractor option."}, "questions": {"value": "**Some questions and suggestions**\n\n1- have you considered replacing the rule-based negation scope detection with a learned or LLM-based decomposition approach? It might improve coverage and robustness to natural language variation.\n\n2- the NLP community has proposed several deterministic negation detection methods (e.g., NegEx, DEEPEN, or syntactic-scope models) that go beyond keyword matching. Could you compare performance to some of those?\n\n3- have you evaluated the approach on non-synthetic or human-written captions like real image descriptions to test out-of-distribution generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w5yFi4W1OP", "forum": "Dd86hsSam5", "replyto": "Dd86hsSam5", "signatures": ["ICLR.cc/2026/Conference/Submission8246/Reviewer_u7Qg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8246/Reviewer_u7Qg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982377075, "cdate": 1761982377075, "tmdate": 1762920189170, "mdate": 1762920189170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free text-embedding edit for CLIP to handle negation: detect the negated span with rules, subtract the projection of the full caption embedding onto the negated-concept embedding, and add an “anchor” vector (Eq. 2). On NegBench, MCQ accuracy jumps from 24.7/24.3/27.5 (COCO/VOC/MSRVTT) to 72.5/78.6/50.0 for CLIP; negated-caption retrieval R-Neg@5 improves modestly (COCO +5.9; MSRVTT +5.5)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simple, training-free mechanism; clear formula and pipeline (Alg. 1, Eq. 2).\n\n- Large MCQ gains on NegBench without changing positive retrieval (R@5).\n\n- Geometry story (negation axis) supported by PCA visualizations (Fig. 3 and 4, Embedding space arithmetic visualization)."}, "weaknesses": {"value": "- Missing relevant related work: prior work [1] shows inference-time steering using CLIP geometry and visual-semantic arithmetic; it should be cited and contrasted (LM-side steering vs. text-side embedding edit). \n\n- Generalization beyond CLIP text towers (e.g., SigLIP/ALIGN, BLIP text encoders, MLLMs) is untested.\n\n[1] Tewel, Yoad, et al. \"Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."}, "questions": {"value": "- It would be great to see if the proposed approach generalizes to similar but more recent models, such as SigLIP families. Is there any particular reason it was not tested on these?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sCR80P8hrK", "forum": "Dd86hsSam5", "replyto": "Dd86hsSam5", "signatures": ["ICLR.cc/2026/Conference/Submission8246/Reviewer_G9hW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8246/Reviewer_G9hW"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042266197, "cdate": 1762042266197, "tmdate": 1762920188697, "mdate": 1762920188697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}