{"id": "JAI7afWA9e", "number": 14157, "cdate": 1758229370326, "mdate": 1763697039218, "content": {"title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "abstract": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such as visual question answering, yet they can still fail even when the correct visual evidence is present. In this work, we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively. By examining layer-wise attention dynamics, we find that shallow layers focus primarily on text, while deeper layers sparsely but reliably attend to localized evidence regions. Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers, a phenomenon we term \"seeing but not believing\" that widely exists in major VLM families. Building on this, we introduce an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking. It requires no training and consistently improves accuracy across multiple families, including LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable evidence internally but under-utilize it, and that making such signals explicit can bridge the gap between perception and reasoning, advancing the diagnostic understanding and reliability of VLMs.", "tldr": "We show that VLMs often perceive the right evidence but fail to use it, and propose a simple inference-time method that highlights evidence to improve accuracy.", "keywords": ["Vision Language Model", "Multi-modal QA", "Mechanistic Interpretability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1586962a7358b5db30d0671ae862ad590f8d0af3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates the cause of hallucinations in Vision-Language Models, hypothesizing that these arise because models perceive visual information but fail to leverage it effectively. The authors analyze attention distributions across layers, observing that early layers predominantly attend to textual features, while deeper layers focus more on visual evidence, and this focus is not always used for reasoning. To address this, they propose a steering mechanism that explicitly increases attention to visual evidence regions, enhancing the model’s grounding on visual cues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper offers an insightful analysis of attention localization across layers, showing that deeper layers indeed attend more to semantically relevant image regions when forming answers.\n\n- The VEA step is methodologically well-designed, particularly with its denoising and Gaussian smoothing components to maintain spatial coherence in the attention masks. The method provides a simple yet interpretable tool for reducing hallucinations."}, "weaknesses": {"value": "- The comparison between text and image RATP values (Fig. 1) is not clearly justified, given that the two modalities operate on different attention scales. It remains unclear how a small increase in image RATP can be interpreted as a “modality shift,” especially when text attention is still orders of magnitude higher (e.g., 0.2 → 0.6 for image vs. ~20 for text). This weakens the claim that “vision plays a stronger role in later inference stages.”\n\n- Prior studies (e.g., [1], [2]) have reported different attention trends, showing that image tokens already dominate attention in early layers. The authors should better situate their findings within this literature, clarifying methodological or model-related differences that explain these discrepancies.\n\n- The interpretation of Fig. 4 could be refined: while the authors claim that evidence tokens receive higher attention even in incorrect answers, the plot also shows a drop in attention between correct and incorrect predictions, which complicates that conclusion.\n\n- The assumption that deep layers consistently capture “ground-truth” evidence regions is empirically plausible but not guaranteed. The paper would benefit from a clearer discussion on the reliability of these “evidence layers” across architectures and datasets.\n\n- It is also unclear whether the identification of such evidence layers must be repeated per model, and how sensitive the steering mechanism is to inaccuracies in this identification.\n\n[1] Amara, Kenza, et al. \"Why context matters in VQA and Reasoning: Semantic interventions for VLM input modalities.\" arXiv preprint arXiv:2410.01690 (2024).\n\n[2] Lu, Haolang, et al. \"Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control.\" arXiv preprint arXiv:2510.10285 (2025)."}, "questions": {"value": "The analysis appears to be based on single-token inference. Are the reported results averaged across all generated tokens for each answer, or do they correspond to specific tokens only?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JaDgzRyXMl", "forum": "JAI7afWA9e", "replyto": "JAI7afWA9e", "signatures": ["ICLR.cc/2026/Conference/Submission14157/Reviewer_3b67"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14157/Reviewer_3b67"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761129479824, "cdate": 1761129479824, "tmdate": 1762924621130, "mdate": 1762924621130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A highly similar work"}, "comment": {"value": "Dear authors,\n\nI would like to draw your attention to a highly similar and related work that was not cited in the current paper: “MLLMs Know Where to Look: Training-Free Perception of Small Visual Details with Multimodal LLMs,” published at ICLR 2025.\n\nIn particular, the analysis presented in RQ3 of your paper is nearly identical to Section 4 of our work, covering both the qualitative and quantitative analyses, which represent key motivations and findings in both studies. Moreover, the visual intervention approach shown in Figure 5 is also similar to our Vicrop method: both leverage the MLLM’s internal attention from the initial inference step to guide visual information enhancement, after which the refined visual input is reintroduced into the model, while we recognize the differences in implementation details, such as the highlighting strategies (cropping vs. alpha blending) and attention extraction techniques (relative vs. neighborhood filtering).\n\nGiven these conceptual and methodological similarities, I would appreciate it if you could acknowledge and discuss our work in the relevant sections.\n\nThank you"}}, "id": "bSxJLr6Vaw", "forum": "JAI7afWA9e", "replyto": "JAI7afWA9e", "signatures": ["~Jiarui_Zhang2"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Jiarui_Zhang2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14157/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762971398540, "cdate": 1762971398540, "tmdate": 1762971398540, "mdate": 1762971398540, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a phenomenon called “seeing but not believing” by studying the internal layers of vision-language models (VLMs). Through analyses of layer-wise attention dynamics and layer-wise profiling, the authors introduce a visual evidence augmentation approach that enhances regions likely to contain stronger visual evidence. The proposed method is training-free and provides both improved interpretability and better performance for VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow. The phenomenon of “seeing but not believing” is intriguing.\n\n2. The experiments and ablation studies are comprehensive.\n\n3. The proposed VEA approach provides both interpretability and performance improvement."}, "weaknesses": {"value": "1. Some experimental details are missing. For example, in Figures 2 and 3, it is unclear which models were used for attention map visualization.\n\n2. Including more visual examples could strengthen and better support the overall narrative."}, "questions": {"value": "Did the authors observe the attention sink phenomenon [1,2] during their experiments? If so, how did they handle these situations?\n\nReference:\n[1] See What You Are Told: Visual Attention Sink in Large Multimodal Models, ICLR 25\n[2] Vision Transformers Need Registers, ICLR 24"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fddHTv36Ny", "forum": "JAI7afWA9e", "replyto": "JAI7afWA9e", "signatures": ["ICLR.cc/2026/Conference/Submission14157/Reviewer_8FW4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14157/Reviewer_8FW4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761340508651, "cdate": 1761340508651, "tmdate": 1762924620552, "mdate": 1762924620552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Expect more clear clarification about some concepts."}, "comment": {"value": "I appreciate the author's diligent work, which has given me some fresh insights. However, I have some confusion about the writing.\n\n- I am confused about the introduction of RAPT, which deserves a more clear and formulaic description. \n- In RQ2, how is the visual attention derived? Many works point out that the visual attention is less interpretative due to the vision sink tokens though the `LocalizationHeads`[A] has proposed a denoising solution to some extent. Besides, the approach seems similar to `Vicrop` which has been pointed out by original authors.\n\n\n[A] Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding, CVPR2025"}}, "id": "xUBc6nMZHd", "forum": "JAI7afWA9e", "replyto": "JAI7afWA9e", "signatures": ["~Zaiquan_Yang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Zaiquan_Yang1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14157/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763056217742, "cdate": 1763056217742, "tmdate": 1763056312938, "mdate": 1763056312938, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the phenomenon of “seeing and not believing”, where vision-language models (VLMs) attend to the correct visual regions but still produce incorrect answers. To address this issue, the authors propose an inference-time method that encourages VLMs to better leverage visual information by overlaying attention-derived masks on the input image. Experiments across four VQA benchmarks and four VLM families demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors conduct a thorough investigation of how different layers in VLMs process inputs and distribute attention, providing valuable insights for the community.\n2. The proposed solution is simple yet effective, showing consistent  improvements across eight different VLMs."}, "weaknesses": {"value": "1. It is important to evaluate cases where the model does not attend to the correct regions.\nHow often does the model still answer correctly in such cases?\nDoes performance degrade when highlighting regions based on incorrect attention?\n2. The motivation is somewhat similar to [1], which also identifies this phenomenon and proposes attention-based approaches.\nThis overlap reduces the novelty of the contribution, though the improvements of the method are still appreciated.\n\n\n[1] \"Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly\", Liu et al."}, "questions": {"value": "1. In cases where the model attends to incorrect regions but still answers correctly, what happens when the attention-derived mask is applied?\nDoes it lead to performance degradation in such examples?\n2. I would be interested to see qualitative examples that explore attention behavior in more general, real-world scenes, beyond the text-centric scenarios that are (to my understanding) the main focus of the examined datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jpFmuAR5AS", "forum": "JAI7afWA9e", "replyto": "JAI7afWA9e", "signatures": ["ICLR.cc/2026/Conference/Submission14157/Reviewer_LvsH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14157/Reviewer_LvsH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964368868, "cdate": 1761964368868, "tmdate": 1762924619919, "mdate": 1762924619919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the \"seeing but not believing\" phenomenon in Vision-Language Models (VLMs): VLMs often perceive correct visual evidence in Visual Question Answering (VQA) but fail to leverage it for accurate answers. Through layer-wise attention analysis, it reveals three key findings: shallow VLMs layers focus on text, deeper layers sparsely attend to localized evidence regions, and deeper layers still lock onto evidence even when outputs are wrong. To solve this, the paper proposes VEA (Visual Evidence Augmentation), a training-free inference-time method. It first identifies \"visual-grounding layers\". During inference, VEA extracts attention from these layers, applies denoising and Gaussian smoothing to create an evidence mask, and fuses the mask with the original image to highlight evidence (weakening non-evidence regions)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an insightful observation on VLMs’ behaviors toward images. Its visualizations and analyses further reveal how text and image interactions are modeled across different layers, showing that the encoding of semantic features first emerges in deeper layers. The inconsistency between attention maps (i.e., \"seeing but not believing\") highlights limitations of current VLM architectures, which is valuable for guiding future research.\n\n2. To address this issue, the authors propose a simple yet effective algorithm. The design is straightforward and practical: it can be applied to various VLMs with zero training cost and demonstrates effectiveness across multiple benchmarks."}, "weaknesses": {"value": "1. The method improves VLM performance by overlaying a salient mask on the input image, but it does not \"fix the VLM’s attention behavior\" (as the behavior of the VLM or attention is not changed). Additionally, the design of the algorithm will introduce extra cost, and also raises the convern about multi-turn/multi-image scenarios.\n\n2. The proposed algorithm augments the brightness of different regions of the image. However, this augmentation will change the original image, causing information loss and changes (e.g., this will influence questions about brightness or color). For questions that rely on global context or beyond retrieval, the proposed mask method might cause troubles.\n\n3. Compared with visual reasoning methods that interactively retrieve key parts of the image to gather information, the proposed approach is plug-and-play, however, it also introduce limitations. This point could be further discussed in the paper."}, "questions": {"value": "1. The paper proposes to improve the network attention by casting masks on the input image. Why not apply this method to intermediate features or attention?\n2. How does this method work on multiturn conversations, multi-image QA or video?3. \n3. How does this method influence the benchmarks that rely on global context or multiple elements, such as benchmarks for general knowledge understanding or math (such as MMMU, MMStar, AI2D, MathVista)?\n4. As the author suggested, the answer from a single inference might not be accurate. The same applies to the attention map — can this method apply to the same question and image multiple times in a cascade way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xyMLDdmNxr", "forum": "JAI7afWA9e", "replyto": "JAI7afWA9e", "signatures": ["ICLR.cc/2026/Conference/Submission14157/Reviewer_RdjA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14157/Reviewer_RdjA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980687664, "cdate": 1761980687664, "tmdate": 1762924618718, "mdate": 1762924618718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}