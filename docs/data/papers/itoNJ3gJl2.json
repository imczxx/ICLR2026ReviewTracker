{"id": "itoNJ3gJl2", "number": 20573, "cdate": 1758307632242, "mdate": 1759896970617, "content": {"title": "Group Representational Position Embedding", "abstract": "We present GRAPE (Group Representational Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms:\n(i) \\emph{multiplicative} rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and\n(ii) \\emph{additive} logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$.\nIn Mul-GRAPE, a position $n\\!\\in\\!\\mathbb{Z}$ (or $t\\!\\in\\!\\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,\\omega\\,\\mathbf{L})$ with a rank‑2 skew generator $\\mathbf{L}=\\mathbf{a}\\mathbf{b}^\\top{-}\\mathbf{b}\\mathbf{a}^\\top\\in\\mathfrak{so}(d)$, yielding a relative, compositional, norm‑preserving map with a closed‑form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log‑uniform spectrum. Learned commuting subspaces and compact non‑commuting mixtures strictly extend this geometry at $O(d)$ and $O(rd)$ cost per head, respectively.\nIn Additive GRAPE, additive logits arise as rank‑1 (or low‑rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability.  Altogether, GRAPE supplies a principled design space for positional geometry in long‑context models, subsuming RoPE and ALiBi as special cases.", "tldr": "", "keywords": ["position encoding", "group theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/541ab5bb3c344aebb0096893b6f25955b0504c68.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GRAPE, a unified group-theoretic framework for positional encoding for 1D sequences that combines multiplicative rotations in SO(d) and additive unipotent actions in GL to recover and generalize methods like Rotary Position Embedding (RoPE), Attention with Linear Biases (ALiBi), and Forgetting Transformer (FoX).\nUnlike prior work that focuses only on rotations, it formalizes both multiplicative and additive mechanisms under a single algebraic structure and introduces path-integral additive biases for contextual, streaming-friendly position encoding.\nExperiments on FineWeb-Edu 100B with Llama show slightly improved stability and lower loss compared to RoPE, ALiBi, and FoX."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The generator construction L is smart and novel. \n- The combination of both multiplicative and additive mechanisms under a single algebraic structure is a contributing perspective for this field. \n- The formal description of GRAPE is complete. \n- The method is efficient and is a direct extension of Rope for 1D sequences."}, "weaknesses": {"value": "The motivation of this work could be better described. Which practical problem does GRAPE solve? What can GRAPE encode what Rope and other variants cannot encode and why is that important in practice? \n- The comparison to prior works and the related work section is incomplete.\n-- For the multiplicative GRAPE, there are several prior works like i.e. LieRE (Ostmeier et al.), STRING (Schenck et al.) that have conceptually predescribed and evaluated Lie Group structured positional encodings, where rank-2 exponentials in SO(d) and learned basis. How does GRAPE compare to just learning the 2x2 basis generators for the block diagonal rotation matrix?\n-- How does GRAPE compare to YARN (Peng et. al)?\n-- How does GRAPE compare to CoPE? \n- The paper aims to present a unified framework for positional encoding based on group actions for transformer in general, but only focuses on 1D sequence encoding and not higher dimensional inputs.\n- Although it is valuable to present the learning curves, the experimental results could be better presented i.e. confidence intervals, at least two validation sets or a test set. \n- Ablations between multiplicative and additive would enhance the understanding of the practical contributions of each of them."}, "questions": {"value": "- Is the training fully converged? Would you mind running for 50 more epochs? \n- You emphasize the importance of exact relativity and orthogonality for translation invariance in positional encodings. Could you comment on what a strict structure is enables?\n- What is the motivation for combining multiplicative and additive logit biases in your positional encoding design? Do they serve complementary roles (e.g., scaling vs shifting positional effects), and how does this impact learning stability or expressivity?\n\nMinor:\n- The abstract has undefined variables ($a$ and $b$ ... ) which are only later defined"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QVRRVzJ60D", "forum": "itoNJ3gJl2", "replyto": "itoNJ3gJl2", "signatures": ["ICLR.cc/2026/Conference/Submission20573/Reviewer_TbLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20573/Reviewer_TbLJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760641480872, "cdate": 1760641480872, "tmdate": 1762933985764, "mdate": 1762933985764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GRAPE, a unified theoretical framework for positional encoding based on group theory. The authors categorize existing methods into two families: Multiplicative GRAPE (rotations, like RoPE) and Additive GRAPE (biases, like ALiBi). The paper claims that RoPE, ALiBi, and FoX are all exact special cases or instances of this framework. The authors then propose a new, endpoint-dependent variant called Path-Integral Additive GRAPE (PI-Add-GRAPE). In a minimal experiment, this new method is shown to achieve lower (or possibly comparable) loss than baselines on a language modeling task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper provides a somehow novel viewpoint to design of positional encoding. The goal of unifying the two dominant (and seemingly different) positional encoding methods (rotations and biases) under a single mathematical framework is could be ambitious and interesting.\n\nThe proposed PI-Add-GRAPE mechanism, which introduces content-dependent biases, may be a novel concept. In theory, this dynamic approach could offer more expressive power than static position methods."}, "weaknesses": {"value": "Unclear Practical Benefit of the Theory: The paper spends significant effort on group theory formalism. However, the practical benefit of this complex formalization is unclear. It seems obvious that rotational embeddings like RoPE can be described by group theory (e.g., SO(2)). The paper does not clearly explain what new, practical advantages this complex theory provides over a simpler understanding. The claim of offering a \"design space\" is abstract and its benefit is not well-supported.\n\nInsufficient Experimental Validation: The empirical evaluation is minimal and insufficient to support the paper's claims. It consists of a single set of training curves for one model configuration. The claim of a \"persistent edge\" also appears to be an overclaim; the validation loss for ALiBi looks very competitive with PI-Add-GRAPE.\nFurthermore, the paper is missing empirical analyses to understand the proposed method. For example, there are no ablation studies, no length extrapolation tests (a key feature of ALiBi), and no analysis of attention distributions to show how the dynamic bias works. Section 7 feels aimless; it shows a result but provides no insight into why the method is good or what its specific advantages are.\n\nComputational Cost: The PI-Add-GRAPE method (Section 6) is endpoint-dependent. This implies that during inference at step t, the bias for all t−1 previous keys must be recomputed relative to the current query, right? This likely introduces a significant O(t) computational overhead per step, which is a major drawback compared to the O(d) cost of RoPE or ALiBi. This trade-off is not benchmarked empirically.\n\nLogical Gaps and Confusing Terminology: The logical connection in the introduction (from \"These observations\" to \"motivate a unified formulation\" ) is a significant jump and is not well-justified for me. In addition, there is some confusing expressions (e.g., the interchangeable use of \"exact special case\" and \"exact instance\" is confusing)."}, "questions": {"value": "Can you clarify the computational overhead of PI-Add-GRAPE during training and inference?\n\nGiven the complexity of PI-Add-GRAPE and validation of RoPE's unstable result, do you plan to release a reference implementation? This would be crucial for reproducibility and adoption by the community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WJod8GcscH", "forum": "itoNJ3gJl2", "replyto": "itoNJ3gJl2", "signatures": ["ICLR.cc/2026/Conference/Submission20573/Reviewer_MUmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20573/Reviewer_MUmm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761020883090, "cdate": 1761020883090, "tmdate": 1762933984195, "mdate": 1762933984195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for positional encoding based on group actions, dubbed as GRAPE (Group RepresentAtional Position Encoding). Specifically, the authors describe two families of positional encodings grounded in group actions: (1) Multiplicative\nGRAPE based in multiplicative rotations; (2) Additive GRAPE from unipotent actions in the general linear group. The authors describe how existing positional encodings such as RoPE, ALiBi, and FoX can be recovered as special cases within this framework. The authors also provide some empirical evidence supporting the advantages of GRAPE over existing positional encodings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper grounds the design space of positional encoding (PE) in group actions, arriving at a general framework which can possibly motivate more expressive and useful PEs."}, "weaknesses": {"value": "1. The message of the paper is confusing. For Multiplicative GRAPE, the authors stated the exact relative law in Section 2.2 which naturally leads to commuting Mul-GRAPE, but then describe non-commuting Mul-GRAPE in multiple places (e.g. abstract, related work, appendix) without any motivations.\n\n2. The paper devotes section 3 and 4 for describing Multiplicative GRAPE, but does not use it in the empirical experiment. This casts doubts on the practical utility of Multiplicative GRAPE.\n\n3. The empirical experiments are quite limited. The authors compare PI-Add-GRAPE with other baseline PEs only on their loss curves, without other metrics (e.g., perplexity) or downstream task performance, or ablations (e.g., context length, model size)."}, "questions": {"value": "1. Can the authors compare their Mul-GRAPE with the recently proposed LieRE in [1], which parameterizes the rotation as a sum of skew-symmetric matrices (followed by matrix exponential)? LieRE seems to provide a more general parameterization, so I am curious to see if this results in any computational or performance differences.\n\n2. In Prop 3.1: the equality of MS-GRAPE and ROPE only holds when the planes are the canonical coordinates pairs and the angles follow the log-uniform spectrum, right? If so, I suggest to make the statement more precise.\n\n3. The authors introduces GRAPE as a way to provide a group-theoretic view of PEs. Does GRAPE provide additional insights of the existing PEs, such as which PE one should choose over another given certain tasks in mind (e.g., length extrapolation)? \n\n References\n [1] Ostmeier et al., LieRE: Lie Rotational Positional Encodings, ICML 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F0BCGcsCSA", "forum": "itoNJ3gJl2", "replyto": "itoNJ3gJl2", "signatures": ["ICLR.cc/2026/Conference/Submission20573/Reviewer_dFym"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20573/Reviewer_dFym"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761245346154, "cdate": 1761245346154, "tmdate": 1762933983614, "mdate": 1762933983614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GRAPE, a unified group-theoretic framework for positional encoding in Transformers. It combines Multiplicative GRAPE (rotations in SO(d), generalizing RoPE) and Additive GRAPE (unipotent actions in GL, recovering ALiBi and FoX). GRAPE preserves exact relative relationships, supports streaming, and offers an extensible design space for long-context modeling."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an elegant theoretical unification of multiplicative and additive positional mechanisms within a single group-theoretic framework. It offers closed-form and computationally efficient implementations, demonstrates strong compatibility with existing Transformer architectures, and provides extensibility toward contextual, learned-basis, and non-commuting variants for more expressive positional representations."}, "weaknesses": {"value": "This paper utilizes Lie algebras. While unifying existing work with Lie algebras is natural, its drawback is that it makes the paper's contribution seem more like the superiority of Lie algebras themselves rather than the authors' contribution. I believe the authors should emphasize more on how introducing Lie algebras facilitates combining the strengths of various existing methods, explaining why each strength is beneficial, and then supplementing with corresponding ablation experiments.\n\nThe experiments in this paper are somewhat limited, lacking extrapolation experiments and comparisons with more metrics. I suggest at least supplementing with the experiments in Tab. 4 of RoPE.\n\nDue to the highly complex formulas, I cannot guarantee my complete understanding of this paper. The pseudocode in the appendix does not alleviate my concerns about reproducibility; I would appreciate to see the complete project code."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mYK77N3gfx", "forum": "itoNJ3gJl2", "replyto": "itoNJ3gJl2", "signatures": ["ICLR.cc/2026/Conference/Submission20573/Reviewer_fTQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20573/Reviewer_fTQk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716011161, "cdate": 1761716011161, "tmdate": 1762933983090, "mdate": 1762933983090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}