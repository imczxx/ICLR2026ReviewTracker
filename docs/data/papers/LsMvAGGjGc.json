{"id": "LsMvAGGjGc", "number": 7381, "cdate": 1758018876245, "mdate": 1759897856079, "content": {"title": "Benign Overfitting in Adversarial Training for Vision Transformers", "abstract": "Despite the remarkable success of Vision Transformers (ViTs) across a wide range of vision tasks, recent studies have revealed that they remain vulnerable to adversarial examples, much like Convolutional Neural Networks (CNNs). A common empirical defense strategy is adversarial training, yet the theoretical underpinnings of its robustness in ViTs remain largely unexplored. In this work, we present the first theoretical analysis of adversarial training under simplified ViT architectures. We show that, when trained under a signal-to-noise ratio that satisfies a certain condition and within a moderate $\\ell_2$ perturbation budget, adversarial training enables ViTs to achieve nearly zero robust training loss and robust generalization error under certain regimes. Remarkably, this leads to strong generalization even in the presence of overfitting, a phenomenon known as \\emph{benign overfitting}, previously only observed in CNNs (with adversarial training). Experiments on both synthetic and real-world datasets further validate our theoretical findings.", "tldr": "We theoretically and empirically demonstrate that Vision Transformers can exhibit benign overfitting under adversarial training.", "keywords": ["Learning Theory", "Benign overfitting", "Adversarial training", "ViT"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bcf29e013a000d479b14164c2bb01186cadabf7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "A theoretical analysis of a two layer vision transformer with adversarial training is done. The adversarial threat model for this work is with respect to the l-infinity norm. Empirically the method is validated on a heavily modified version of the MNIST dataset and a synthetic dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written and easy to follow."}, "weaknesses": {"value": "I am not an expert in theoretical machine learning and much of the paper‚Äôs strength lies in the fact that they have done a lot of mathematical derivations. Other reviewers will have to comment on whether that alone is enough to merit acceptance.  \n\nHowever, from an experimental perspective I find the results entirely unconvincing for the following reasons:\n\n1. The choice of datasets are too small and not impactful. The authors only test on a synthetic dataset and MNIST. Actually, even the MNIST dataset they use is not the true MNIST dataset. The dataset the authors test on is a binarized version of the MNIST dataset. These days CIFAR-10/100 experiments are the minimum for experimentally demonstrating a viable adversarial robustness technique (with Tiny-ImageNet and ImageNet also becoming norms). \n\n2. The attacks used in the paper are not SOTA. APGD would be a much better choice (instead of PGD) and an L2 version of the APGD code has been available for multiple years:\nhttps://github.com/fra31/auto-attack\n\nI also question WHY the authors only use the l2 metric. Why not also show what happens for l-inf, l-0 or l-1 attacks? Here are the related attack links:\n\nL0: https://github.com/fra31/sparse-imperceivable-attacks\nL1: https://arxiv.org/abs/2103.01208\nL2: https://arxiv.org/pdf/2003.01690\n\n There has been much work that shows it may not be enough to just prevent one norm attack, so considering multi-norm attacks (even if the results are poor) is a much more interesting scope: https://proceedings.mlr.press/v119/maini20a/maini20a.pdf\n\nAs a reviewer I cannot mandate that you do any more experiments. However, I would say that it is nearly impossible for me to be an advocate for your paper because the scope of the current work is not justifiable in my opinion. If you could extend your framework and your experiments to the multi-norm case (even if the results then become less robust), that would be a much much stronger work."}, "questions": {"value": "Please address the issues I mention in the weakness section of my review. Specifically: \n\nA. Why aren't SOTA attacks used in the experimental results?\nB. Why only focus on L2 norm? Can you give any better justification for the scope of your current work? \nC. Why aren't more complex datasets used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1lsyojlbyZ", "forum": "LsMvAGGjGc", "replyto": "LsMvAGGjGc", "signatures": ["ICLR.cc/2026/Conference/Submission7381/Reviewer_Uksd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7381/Reviewer_Uksd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761600911619, "cdate": 1761600911619, "tmdate": 1762919508455, "mdate": 1762919508455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, with a solid theoretical analysis, the authors demonstrate that the phenomenon of benign overfitting also exists for ViTs. Experiments on a real-world dataset (MNIST) highlight the correctness of the finding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 The theoretical proof is solid.\n\n2 This paper is well motivated.\n\n3 The writing is good."}, "weaknesses": {"value": "1 Acknowledging the theoretical contributions of this paper, the findings do not bring new insights to the community. For example, as listed in the second point in the contribution of this paper, the authors claim that:\n\n1) Small perturbations yield trajectories close to clean training. (According to the definition of Adversarial training, if the perturbation is small enough, AT will collapse to natural training).\n\n2) Moderate perturbations cause the attention mechanism to fail, such that the ViT collapses to a linear model; (Due to the misleading effect of the moderate adversarial samples, it will disrupt the attention mechanism.)\n\n3) Large perturbations lead to significant generalization error beyond benign overfitting.  (In this circumstance, the robust overfitting will happen and resisting attacks crafted with a large attack budget is also challenging, increasing the generalization error.)\n\n2 Theories with practical implications tend to be more appreciated. Unfortunately, this paper does not give the take-away tips on how to better perform AT on ViT Transformers.\n\n3 The verified dataset is the MNIST dataset, which is the simplest dataset in image classification. Without the experiments on more complex datasets such as CIFAR-10 and ImageNet, the correctness of the theory can not be verified in the application of ViTs in real scenarios.\n\n4 In Line 418, the signal and noise vectors is concatenated to form a new vector which is quite different from the application of AT in real-world datasets.\n\n5 The theory makes an analysis on a simple Transformer architecture, ignoring the role of the linear projection layer and the MLP head."}, "questions": {"value": "1 Why, in the verified experiment, only the samples of \"0\" and \"1\" labels are chosen to perform experiments? Can the theory be generalized to datasets with more classes?\n\n2 Can the experiments be generalized to explain the appearance of the robust overfitting in Vision Transformer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZsPOiO3SOR", "forum": "LsMvAGGjGc", "replyto": "LsMvAGGjGc", "signatures": ["ICLR.cc/2026/Conference/Submission7381/Reviewer_U4KV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7381/Reviewer_U4KV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904280183, "cdate": 1761904280183, "tmdate": 1762919508055, "mdate": 1762919508055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides the first theoretical analysis of benign overfitting under adversarial training for Vision Transformers (ViTs). The authors construct a simplified two-layer ViT model, derive convergence and generalization guarantees, and identify distinct regimes of adversarial perturbation magnitudes that influence learning dynamics. Empirical validations on synthetic and MNIST data confirm the theoretical predictions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a comprehensive and rigorous theoretical analysis of benign overfitting in the context of adversarial training for Vision Transformers (ViTs). Specifically, it extends the study of benign overfitting to the transformer architecture, offering new insights into how the interplay between attention mechanisms, signal-to-noise ratio, and perturbation magnitude determines both robust generalization and overfitting behavior."}, "weaknesses": {"value": "The main limitation of the paper lies in its experimental evaluation. The experiments are conducted only on a subset of MNIST using shallow Vision Transformers (ViTs), which is too simplistic to convincingly verify the proposed theoretical results. MNIST lacks the complexity required to test the robustness and generalization behaviors predicted by the theory. At minimum, the authors should include experiments with a standard multi-layer ViT on a more challenging dataset such as CIFAR-10, to better demonstrate the practical relevance and validity of their theoretical findings."}, "questions": {"value": "## 1. Generality beyond Vision Transformers:\nI am wondering whether the current theoretical investigation could be extended beyond the simplified two-layer ViT model to encompass more general Transformer architectures‚Äîfor example, models with multiple self-attention layers, residual connections, layer normalization, or feed-forward blocks. It would be valuable to understand whether the derived benign overfitting behavior and robustness‚Äìgeneralization relationships continue to hold under these more realistic architectural settings, and whether the theoretical scaling laws remain consistent when evaluated on larger and more complex datasets beyond MNIST.\n\n## 2. Applicability to ViT variants:\nI am also curious about how the proposed theorems and analysis apply to different variants of Vision Transformers, such as Swin Transformer, DeiT, or hierarchical ViTs that modify the attention mechanism or token structure. Do the key theoretical conditions, particularly those involving the signal-to-noise ratio and perturbation magnitude, still characterize the transition between benign and harmful overfitting in these variants? Some clarification or discussion on the generality of the theoretical framework across ViT architectures would strengthen the paper‚Äôs impact and scope.\n\n## 3. Empirical validation of theoretical regimes:\nThe paper identifies three distinct regimes of adversarial perturbation (clean-like, linear-collapse, and failure). Could the authors provide more detailed empirical evidence or visualizations to confirm these transitions‚Äîperhaps by monitoring changes in attention distributions, feature alignment, or representation collapse across varying perturbation strengths? Such results would make the theoretical phase transition more tangible and strengthen the connection between theory and practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Dm00hF3hG1", "forum": "LsMvAGGjGc", "replyto": "LsMvAGGjGc", "signatures": ["ICLR.cc/2026/Conference/Submission7381/Reviewer_zyXo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7381/Reviewer_zyXo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960304215, "cdate": 1761960304215, "tmdate": 1762919507689, "mdate": 1762919507689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper gives a theoretical account of robust benign overfitting in a simplified two‚Äëlayer ViT trained with ‚Ñì‚ÇÇ adversarial training. It proves that, under specific relationships between dataset size and signal‚Äëto‚Äënoise ratio (SNR), and for a moderate perturbation radius œÑ, the model can interpolate the training data (vanishing robust training loss) while maintaining small robust test error. Experiments on synthetic data and MNIST visualize phase transitions via heatmaps that align with the theory (e.g., boundary roughly ùëÅ‚ãÖSNR^2=Œ©(1))."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "++ The paper pinpoints when adversarially trained ViTs interpolate yet generalize robustly (e.g., N¬∑SNR¬≤ = Œ©(1) with œÑ ‚â≤ ‚ÄñŒº‚Äñ¬≤‚ÇÇ / log(dh)), and how robust error scales with d, SNR, œÑ, yielding explicit bounds and a practical ‚Äúsafe‚Äù œÑ range.\n\n++ The analysis explains why moderate œÑ can ‚Äúflatten‚Äù attention into near‚Äëuniform weights‚Äîcollapsing a ViT to a linear model‚Äîand contrasts convergence/SNR requirements with that degenerate baseline. This isolates an attention‚Äëdriven mechanism behind phase transitions.\n\n++ The paper shows that no classifier can achieve nontrivial robust accuracy when œÑ ‚â• ‚ÄñŒº‚Äñ¬≤‚ÇÇ gives a sharp ceiling on what adversarial training can achieve, cleanly bracketing the benign region.\n\n++ Heatmaps on synthetic and MNIST reproduce the predicted boundary N¬∑SNR¬≤ = Œ©(1) and show that robust gains appear only once both SNR and N clear the theoretical thresholds. The figures concretize the phase transition narrative."}, "weaknesses": {"value": "-- Validation uses synthetic data and MNIST; no CIFAR/ImageNet‚Äëscale tests or modern ViT training recipes, so the practical reach of the theory is not stress‚Äëtested under real‚Äëworld pipelines, augmentations, or stronger attacks.\n\n-- The two‚Äëlayer ViT and assumptions (e.g., multi‚Äëpatch distribution, specific œÑ/SNR scalings) help analysis but may not capture architectural and optimization nuances (depth, MHA heads, layernorm, schedules) that affect robustness in practice.\n\n-- Results emphasize l2 training/attacks and do not discuss other threat models (l‚àû, l1, corruptions) or multi-step PGD details that affect robust outcomes; generalization across norms remains open."}, "questions": {"value": "1. Do the phase boundaries or impossibility result change meaningfully for ‚Ñì‚àû or autoattack‚Äëstyle suites? Any conjecture or preliminary evidence?\n\n2. How do the conditions scale with number of heads M, head dimension, and depth? Can you extend the analysis (even heuristically) to stacked blocks or to pre‚Äënorm residual forms common in ViTs?\n\n3. Could you reproduce the heatmap phase boundary on CIFAR‚Äë10/100 with small ViTs and ‚Ñì‚ÇÇ PGD to show qualitative agreement (even if not strictly in‚Äëdistribution with the theory)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4Jhe3TVbag", "forum": "LsMvAGGjGc", "replyto": "LsMvAGGjGc", "signatures": ["ICLR.cc/2026/Conference/Submission7381/Reviewer_kd1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7381/Reviewer_kd1t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137817454, "cdate": 1762137817454, "tmdate": 1762919507303, "mdate": 1762919507303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}