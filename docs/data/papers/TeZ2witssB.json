{"id": "TeZ2witssB", "number": 6438, "cdate": 1757983896569, "mdate": 1759897914637, "content": {"title": "Information-Aware and Spectral-Preserving Quantization for Efficient Hypergraph Neural Networks", "abstract": "Hypergraph neural networks (HGNNs) capture higher-order relationships beyond pairwise graphs, yet most existing models suffer from a \\emph{uniform capacity assumption}, allocating equal resources to all node--hyperedge interactions regardless of their informativeness. This leads to inefficiencies and degraded performance, especially under compression. Moreover, current attention mechanisms and quantization methods often fail to preserve the structural and informational properties essential for hypergraph learning. We introduce \\textsc{QAdapt}, a principled framework that unifies \\emph{information-theoretic attention allocation}, \\emph{spectral-preserving fusion}, and \\emph{co-adaptive quantization}. QAdapt adaptively assigns precision based on information density, leverages spectral fusion to capture multi-scale hypergraph structure, and learns differentiable bit-allocation policies that co-optimize attention and quantization. Extensive experiments on five benchmarks show that QAdapt delivers up to $5.4\\times$ compression and $4.7\\times$ speedup while achieving consistent accuracy gains of $+6.7\\%$ to $+9.0\\%$ over state-of-the-art quantization baselines. These results demonstrate that integrating information-theoretic attention with spectral-preserving quantization enables efficient yet accurate hypergraph learning.", "tldr": "Efficient hypergraph  by using dual attention and adaptive quantization", "keywords": ["Hypergraph neural networks", "Quantization", "Attention"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c0b464d7633ca59705388695475222f10a03a6c.pdf", "supplementary_material": "/attachment/eaf4dd7723dc95b0a1e40ce3aea8b353408e0657.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces QADAPT, a framework for hypergraphs to efficiently capture structions and node-hyperedge dependencies. QADAPT estimates interaction importance through information density, aggregates neighbor information through multi-scale attention and adaptively quantizes attention values through sensitivity of attentions values, information density and Laplacian eigenvectors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces an interesting research topic to enhance efficiency of hypergraph neural network.\n\n2. The experiments are conducted with various datasets, and the ablation study is extensive.\n\n3. The paper provides reasonable motivations for each module."}, "weaknesses": {"value": "1. I can not understand how the speedup of QADAPT is achieved through the proposed module, especially \"step 3: co-adaptive precision allocation\". The attention value is quantized to different bitwidths based on a learnable MLP. The additional MLP calculation, overhead of quantization and dequantization, dispersion of different bitwidth may significantly reduce the speedup effects of the integer multiplications. And I did not found the related code from the supplement files.\n\n2. The theorem 1,2,3 lack  formal proof.\n\n3. The paper has some important formatting issues. For example, the authors should include related work in main paper, instead of appendix. The \"Conclusion\" should be a section, instead of a paragraph."}, "questions": {"value": "See Weaknesses.\n\nSome other questions:\n\n(1) In line 161, the paper mentions that \"Pe is a learnable projection associated with hyperedge e\". Does it mean that for every hyperedge e, QADAPT builds a learnable projection?\n\n(2) In line 220, the paper mentions that \"All results use 5-fold cross-validation with statistical significance testing (p < 0.01)\". I think the statistical significance testing is not defined as the paper mentions. \n\n(3) The paper does not mention how the message passing process with respect to attention coefficient with different bitwidths is formulated to improve the efficiency of Hypergraph Neural Network."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r087EssWaI", "forum": "TeZ2witssB", "replyto": "TeZ2witssB", "signatures": ["ICLR.cc/2026/Conference/Submission6438/Reviewer_zEyn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6438/Reviewer_zEyn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624369859, "cdate": 1761624369859, "tmdate": 1762918832095, "mdate": 1762918832095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two key challenges in the large-scale deployment of Hypergraph Neural Networks (HGNNs): low efficiency and performance degradation caused by uniform resource allocation, and the limitations of existing quantization methods that overlook hypergraph structure and informational properties. The authors propose QADAPT—a unified framework that integrates information-theoretic attention allocation, spectral-preserving fusion, and collaborative adaptive quantization—to enable efficient and accurate hypergraph learning. Experiments on five benchmark hypergraph datasets, covering both classification and regression tasks, demonstrate that QADAPT significantly outperforms existing approaches in terms of both accuracy and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work integrates information theory with spectral analysis for hypergraph quantization, breaking through the bottleneck of uniform resource allocation. \n\nThe framework is supported both theoretically and empirically: rigorous proofs are provided for information retention bounds, spectral preservation guarantees, and convergence, demonstrating the framework’s ability to safeguard hypergraph structures. Comprehensive ablation studies—such as isolating the contributions of information density and SpectralFusion—confirm the necessity of each component, while experiments are exhaustively designed.\n\nQADAPT achieves an excellent balance of efficiency and accuracy, maintaining superiority under both extreme compression (4-bit) and high-precision (16-bit) scenarios. Adaptive bit allocation enables the minimization of accuracy loss and maximization of efficiency."}, "weaknesses": {"value": "Although the information density computation cost is reduced by employing mini-batch contrastive learning, the second-stage hypergraph Laplacian spectral decomposition may still present an efficiency bottleneck for extremely large-scale hypergraphs. The paper does not explicitly detail time complexity optimization strategies for such large-scale scenarios.\n\nThe framework relies on several key hyperparameters (e.g., number of mutual information negative samples $N=64$, number of spectral components $K=32$, initial Gumbel-Softmax temperature $\\tau_0=2.0$. While experiments show that performance remains stable (±1.5%) with up to 25% variation in these parameters, practical deployment may require dataset-specific tuning, raising the barrier to real-world application."}, "questions": {"value": "- The mini-batch contrastive method reduces complexity from $O(|V|^2|E|)$ to a batch-dependent level, but the paper does not quantify how batch size (e.g., $B=32/64$) affects estimation accuracy. With smaller batches (e.g., B=16 in constrained settings), does mutual information approximation error rise sharply? Is there empirical evidence for the accuracy–efficiency trade-off across batch sizes?\n\n- The approach uses hypergraph Laplacian eigendecomposition with a default K=32 smallest eigenvalues. Given large structural variation across datasets (e.g., Yelp: 679,302 hyperedges vs. IMDB: 2,081), why does K=32 consistently perform best? In sparse cases (e.g., ACM), would smaller K reduce cost without hurting accuracy, and in dense cases (e.g., Yelp), is a larger K needed to capture richer spectra?\n\n- Fisher information is used to gauge attention sensitivity, relying on second-order loss derivatives. What fraction of total inference time does this computation occupy within the reported 4.7× speedup? Can simpler proxies deliver comparable accuracy with lower overhead?\n\n- The paper claims compatibility with mainstream accelerators but provides no platform-specific results. \n\n- Ablations isolate module contributions but not cross-effects. If information density estimation is noisy (e.g., small samples), could SpectralFusion amplify erroneous spectral components and mislead bit allocation? Has this error-propagation risk been analyzed or tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rW2dpr4ehx", "forum": "TeZ2witssB", "replyto": "TeZ2witssB", "signatures": ["ICLR.cc/2026/Conference/Submission6438/Reviewer_Mj1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6438/Reviewer_Mj1T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816169672, "cdate": 1761816169672, "tmdate": 1762918831650, "mdate": 1762918831650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for efficient and accurate hypergraph processing, which consists of three steps. First, a density estimation is computed as a score for each (hyperedge, node) pair, based on an estimated mutual estimation combined with structural similarities derived from spectral properties. Second, this estimated density is used as a bias term in an attention mechanism that produces learnable node-to-node coefficients. Finally, for each pair of nodes, the algorithm dynamically predicts a class indicating the quantization precision for that pair (4-, 8-, or 16-bit precision)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using mixed precision conditioned on the topology is sound and suitable for the geometric deep learning field.\n\nThe results are impressive and the paper presents several ablation study"}, "weaknesses": {"value": "- The paper lacks clarity, which makes evaluating the work difficult. Several details are either omitted, misleading, or insufficiently explained.\n\n  - In Equation (2), $P_e$ is a learnable matrix specific to each hyperedge. This scales with the dimension of the hypergraph, which is not feasible for the current benchmarks (with hundreds of thousands of hyperedges). Moreover, this choice restricts the method to a transductive setup.\n  - The shapes of the matrices are unclear. According to the definition, intra-hyperedge interactions involve an $E \\times N \\times N$  tensor (one matrix per hyperedge), whereas the node-level attention outputs a single $N \\times N$ matrix. How are these combined? Are the intra-hyperedge matrices aggregated across all hyperedges even if not explicitly mentioned in the equation? Additionally, the matrix shapes shown in Figure 1 do not match this assumption.\n  - The paper presents several theoretical results in the Appendix, but detailed proofs are not provided. Some include only three-line sketches, while others lack a proof entirely.\n   - In Figure 3, Panel B, which 12 nodes are being represented, and how does the figure illustrate hierarchical aggregation? In Figure 4, what causes the initial bias (at epoch 0) between 4-, 8-, and 16-bit quantization?\n\n- While I appreciate the positive results, the model is not particularly focused on hypergraph processing, but rather on the graph-extension side. This limits its applicability, as none of the components can be directly extended to general hypergraph networks. The quantization is applied to node-to-node pairs, which can be adapted for other graph networks but not for standard hypergraph message passing. The same limitation applies to the attention-based construction.\n\n- The training setup is not detailed in the paper. Does it follow a mini-batch setup similar to the one presented in the HyperSAGE paper, or is it trained using full-batch training like standard hypergraph approaches?\n\n- What is reported as speed in Figure 2 is unclear. To my understanding, the non-quantized rows (Step 1 and Step 2) perform much more computation compared to a simple HGNN, due to the spectral decomposition and the more expensive similarity-based attention. How is this additional computation not reflected in the speed comparison between HGNN and the proposed model (Steps 1 and 2)?\n\nOverall, my main concern relates to the clarity of the method, which makes it difficult to evaluate the model, as well as the lack of rigor in presenting both theoretical and empirical results"}, "questions": {"value": "Please see the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YmITmfLMOt", "forum": "TeZ2witssB", "replyto": "TeZ2witssB", "signatures": ["ICLR.cc/2026/Conference/Submission6438/Reviewer_t5i9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6438/Reviewer_t5i9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817723110, "cdate": 1761817723110, "tmdate": 1762918830109, "mdate": 1762918830109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes QAdapt, a framework that combines adaptive quantization and spectral fusion for efficient learning on hypergraph data. QAdapt computes an information density measure to quantify the relative importance of each node–hyperedge interaction, and then use that as a bias term to guide attention computation at both the hyperedge level and the node level. The two attention matrices are combined through linear operator involving the eigenvectors of the hypergraph Laplacian. Finally, an MLP is used to predict the optimal precision for each attention coefficient. The authors carry out comprehensive empirical evaluation on five benchmarks, and show that QAdapt outperforms existing baselines in terms of both accuracy and speed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The empirical analysis and evaluation is comprehensive."}, "weaknesses": {"value": "The QAdapt framework as a whole seems to be constructed heuristically. It is not clear how the design choices are made and where they come from. The paper did a very poor job at providing relevant background information and explaining the basic idea. I find it hard to understand why QAdapt works better in the experiments, and why QAdapt has to be designed in this way. Because of this, I find it hard to decipher a solid and concrete contribution from the current paper."}, "questions": {"value": "What happens to Section 2? Why do you call Equation (6) the standard hypergraph convolution? There are many variants of hypergraph convolution operations, and I do not think there is a commonly accepted one as the standard. If you focus on a particular hypergraph convolution operation, please cite the reference and properly describe it. Appendix A is missing a lot of background information. Please properly provide the background information on HGNN and model quantization. For example, explain the reason that you focus on the particular hypergreaph convolution in Equation (6), with proper references.\n\nTable 1 shows that QAdapt is even better than all of the full-precision methods, on all tasks. Why is that?\n\nIt looks like QAdapt can be applied to graph datasets as well. Have you tried this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NvlwX8adky", "forum": "TeZ2witssB", "replyto": "TeZ2witssB", "signatures": ["ICLR.cc/2026/Conference/Submission6438/Reviewer_vKtL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6438/Reviewer_vKtL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762740411600, "cdate": 1762740411600, "tmdate": 1762918825232, "mdate": 1762918825232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}