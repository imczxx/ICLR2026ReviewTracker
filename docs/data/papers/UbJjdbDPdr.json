{"id": "UbJjdbDPdr", "number": 7622, "cdate": 1758029595469, "mdate": 1763550858074, "content": {"title": "Learning Once, Routing Right: Information-Theoretic Gating for Online Continual Mixture-of-Experts", "abstract": "Continual Learning (CL) requires models to learn from sequential data streams while retaining previously acquired knowledge. While Mixture-of-Experts (MoE) models offer a promising solution through dynamic expert selection, a critical gap remains: their gating mechanisms lack principled design from a continual learning perspective. To bridge this gap, we investigate the impact of gating strategies on the achievable Minimum Excess Risk (MER) under the online CL setting, where data can be seen only once. Our key theoretical contribution reveals a novel connection between the MER and the mutual information between expert assignments and labels/outputs. Based on this foundation, we propose two innovative mutual information-based loss functions for both fully labeled and label-free settings. Furthermore, to ensure computational efficiency, we introduce a lightweight, matrix-based mutual information estimator with rigorous joint entropy formulation. Extensive experiments on MNIST, Fashion-MNIST, KMNIST, and EMNIST demonstrate our approach's superiority, achieving up to 12.3\\% lower overall error and 3.9\\% reduced forgetting with statistical significance.", "tldr": "", "keywords": ["Continual Learning", "Mixture-of-Experts", "Minimum Excess Risk", "Mixture-of-Experts Gating"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1a658dd7265e594859fa42e912bf618423a858e7.pdf", "supplementary_material": "/attachment/307978cfb775913018609e415acab100cebe20fc.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces an information-theoretic framework for optimizing gating mechanisms in Mixture-of-Experts (MoE) models under online continual learning. The central theoretical claim is that the Minimum Excess Risk (MER) of MoE models can be upper-bounded by terms involving mutual information between expert assignments and targets (labels or outputs). This connection motivates the design of two mutual information–based loss functions, applicable to both supervised and unsupervised continual learning. To make mutual information computation tractable, the authors propose a lightweight, matrix-based estimator with an improved joint entropy formulation. Experiments on MNIST, Fashion-MNIST, KMNIST, and EMNIST show consistent performance improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n\n2. The proposed theoretical analysis of mutual information between expert assignments and labels/outputs is interesting to understand MoE in continual learning."}, "weaknesses": {"value": "1. The proposed method is only evaluated on vary simple datasets, such as MNIST as its variants.\n\n2. The comparison baselines are also very simple, without considering sufficiently representative methods in continual learning.\n\n3. The proposed method seems to achieve very marginal improvements over the simple baselines.\n\n4. Do the theoretical analysis and the proposed method only apply to online continual learning? Is it possible to extend them to broader continual learning settings (e.g., offline continual learning)?"}, "questions": {"value": "My major concerns lie in the comparison baselines and applicability of the proposed method. Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qbkk2xO9tv", "forum": "UbJjdbDPdr", "replyto": "UbJjdbDPdr", "signatures": ["ICLR.cc/2026/Conference/Submission7622/Reviewer_s9jm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7622/Reviewer_s9jm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760883343916, "cdate": 1760883343916, "tmdate": 1762919702493, "mdate": 1762919702493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "qOkkrCuAwd", "forum": "UbJjdbDPdr", "replyto": "UbJjdbDPdr", "signatures": ["ICLR.cc/2026/Conference/Submission7622/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7622/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763550857283, "cdate": 1763550857283, "tmdate": 1763550857283, "mdate": 1763550857283, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new theoretical framework for optimizing the gating mechanism in Mixture-of-Experts (MoE) models for the Online Continual Learning (OCL) setting. The core contribution is a novel theoretical link between the Minimum Excess Risk (MER) of the MoE model and the mutual information (MI) between the expert assignments and the labels/outputs. Building on this theory, the authors derive two MI-based loss functions to optimize the gating network, one for labeled data and one for a label-free scenario. To make this practical, they also propose a lightweight, matrix-based MI estimator. The method is evaluated on a suite of MNIST-variant datasets (MNIST, Fashion-MNIST, KMNIST, EMNIST), where it shows improvements over other MoE-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and clearly motivated. The authors do a good job of identifying a specific gap—the lack of a principled, theoretical foundation for MoE gating in continual learning—and proposing a solution.\n\n2. The primary strength is the theoretical connection between MER and mutual information. Framing the gating optimization as an MI maximization problem derived from a more fundamental concept (MER) is a principled and elegant approach, moving beyond common heuristics like load-balancing.\n\n3. The authors thoughtfully bridge the gap from their theory to a practical algorithm by developing a lightweight matrix-based MI estimator. This shows a good understanding of the practical limitations of information-theoretic objectives and makes the work self-contained."}, "weaknesses": {"value": "1. My main concern is the limited scope of the experiments. The paper relies exclusively on MNIST-variant datasets (MNIST, F-MNIST, KMNIST, EMNIST). While these are acceptable for a preliminary \"proof-of-concept\" of a theoretical idea, they are relatively simple and not representative of the complex challenges in modern, large-scale CL. The community has largely moved to more challenging benchmarks (e.g., CIFAR-100, TinyImageNet, DomainNet) to demonstrate state-of-the-art performance.\n\n2. Related to the first point, the set of baselines (ToMoECL, ToMoEMEC, LB) appears to be limited to other MoE-gating or theoretical papers. It is difficult to situate the performance of this method within the broader CL landscape. The paper would be significantly stronger if it included comparisons against standard, widely-recognized CL methods (e.g., key replay-based, regularization-based, or architectural methods), even if they are not MoE-based.\n\n3. The choice of MER as the starting point is novel, but it isn't a standard framework for analyzing continual learning. The paper would benefit from a brief discussion justifying why MER is the most appropriate theoretical lens for this problem, as opposed to other common theoretical frameworks used in CL or online learning."}, "questions": {"value": "1. Could the authors comment on the scalability of their method? How do they expect the MI-based loss and its estimator to perform on more complex, higher-dimensional problems like CIFAR-100 or ImageNet-R, where the underlying distributions are far more complex than MNIST?\n\n2. Why were more standard CL baselines (e.g., EWC, GEM, ER) omitted? Even if the method is MoE-specific, these baselines would provide a crucial performance anchor to understand the practical benefit of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DqECW3YblA", "forum": "UbJjdbDPdr", "replyto": "UbJjdbDPdr", "signatures": ["ICLR.cc/2026/Conference/Submission7622/Reviewer_pQyN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7622/Reviewer_pQyN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500237774, "cdate": 1761500237774, "tmdate": 1762919702023, "mdate": 1762919702023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the relationship between Minimum Excess Risk (MER) and mutual information, and proposes two new gating functions that minimize MER through mutual information maximization. It also introduces a lightweight method to reduce the computational complexity of mutual information estimation. The experimental results demonstrate the superior performance of the proposed design."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. They studied MoE of CL from the perspective of Minimum Excess Risk, which is interesting.\n\n2. They provide a series of experiments to explain the advantage of their proposed loss design."}, "weaknesses": {"value": "1. The presentation is not clear to me. The notations they use are not defined well. Detailed questions are provided in \"Questions\".\n\n2. Though they state that \"we develop a rigorous theoretical framework to analyze MoE models in the OCL setting\", there are a lot of statement in this paper is heoristic, e.g., from L240 - L252. Detail question are provided in \"Questions\"."}, "questions": {"value": "Some definition of the core idea in this paper is not clear to me:\n1. In proposition 1, which is your main result, what is $\\psi$? Does it have an explicit expression?\n2. In L219 - L222, what is the reason of designing such a form? Why you need to normalize by $H$ rather than by dividing (norm of) $I$ itself? How to choose $\\gamma_y$ and $\\gamma_z$? \n\nOher questions:\n1. To explain the relationship between MER and mutual information, the MER is defined in L187 - L191. My question is why this MER is related to the performance of continual MoE? \n2. What is the purpose of proposing $H'$ in eq(7)? In my understanding, it is to solve the issue \"H_\\alpha(A, A) ≥H_\\alpha(A)\". But why it is an issue? If you train the model by using eq(6), will it influence the training result heavily?\n3. Can you explain that why your proposed method in 3.3 reduces time complexity?\n\nOther question about presentation and definition of this paper:\n1. In L144, what is the definition of ⊙?\n2. In L197, what is the definition of I(.,.)?\n3. In L210 - L212, how did you define \"Markov data processing chain\"?\n4. In L102 - L103, the citation Li et al. (2025) and  Li & Duan (2025) refers to the same paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZpXkvQpAAr", "forum": "UbJjdbDPdr", "replyto": "UbJjdbDPdr", "signatures": ["ICLR.cc/2026/Conference/Submission7622/Reviewer_yedf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7622/Reviewer_yedf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762485507450, "cdate": 1762485507450, "tmdate": 1762919701635, "mdate": 1762919701635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}