{"id": "itUo64aUeK", "number": 22417, "cdate": 1758330800771, "mdate": 1759896867548, "content": {"title": "Overtone: Cyclic Patch Modulation for Cleaner, Faster Physics Emulators", "abstract": "Transformer-based PDE surrogates achieve remarkable performance but face two key challenges: fixed patch sizes cause systematic error accumulation at harmonic frequencies, and computational costs remain inflexible regardless of problem complexity or available resources. We introduce Overtone, a unified solution through dynamic patch size control at inference. Overtone's key insight is that cyclically modulating patch sizes during autoregressive rollouts distributes errors across the frequency spectrum, preventing the systematic harmonic artifacts that plague fixed-patch models. We implement this through two architecture-agnostic modules—CSM (Cyclic Stride Modulator, using dynamic stride modulation) and CKM (Cyclic Kernel Modulator, using dynamic kernel resizing)—that together provide both harmonic mitigation and compute-adaptive deployment. The harmonic mitigation alone yields up to 40% error reduction in long rollouts, while the flexible tokenization allows users to trade accuracy for speed dynamically based on computational constraints. Applied to challenging 2D and 3D PDE benchmarks, a single Overtone model matches or exceeds multiple fixed-patch baselines across all compute budgets.", "tldr": "", "keywords": ["autoregressive models", "partial differential equations", "tokenization", "artifacts", "compute-adaptive inference"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2ee5e3552bc3b1edbf77ed73ff71de1aa0ca0fb.pdf", "supplementary_material": "/attachment/95515aca6cc3672a99a518fab86ed1943d4a9052.zip"}, "replies": [{"content": {"summary": {"value": "This paper, \"Overtone: Cyclic Patch Modulation for Cleaner, Faster Physics Emulators,\" introduces a novel framework named Overtone to address two critical challenges in Transformer-based partial differential equation (PDE) surrogate models during autoregressive rollouts: harmonic error accumulation caused by fixed patch sizes and the inflexibility of computational cost. The authors propose two architecture-agnostic modules—CSM (Convolutional Stride Modulation) and CKM (Convolutional Kernel Modulation)—that dynamically cycle through different patch sizes during inference. This approach effectively suppresses harmonic artifacts while enabling compute-adaptive deployment. Extensive experiments on various 2D and 3D PDE benchmarks demonstrate significant error reduction and flexible accuracy-speed trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-Defined and Significant Problem:** The paper accurately identifies a fundamental limitation in current Transformer-based PDE surrogates: systematic harmonic error accumulation due to fixed patch sizes. This issue is crucial for the long-term stability of rollouts.\n2. **Novelty of Inference-Time Cyclic Modulation:** The proposed strategy of cyclically modulating patch sizes during inference is pioneering in this field. It offers a fresh perspective on mitigating harmonic artifacts while simultaneously achieving computational adaptability.\n3. **Theoretical Underpinnings:** The mathematical analysis using a linearized error model provides theoretical support for the observed harmonic error accumulation, enhancing the rigor of the work.\n4. **Comprehensive and Solid Experimental Design:** The experimental section is thorough, encompassing multiple PDE datasets and model architectures. Extensive ablation studies robustly validate the method's effectiveness and generalizability."}, "weaknesses": {"value": "1. **Insufficient Justification for the Training-Inference Strategy Discrepancy:** A notable and somewhat counterintuitive finding is the requirement for a cyclic strategy during inference, despite using random patch size sampling during training. The observation that a random schedule significantly harms inference performance lacks deep theoretical explanation and experimental substantiation:\n   - The current explanation, primarily based on a brief comparison in Appendix H.6, states that random scheduling hurts performance but does not adequately elucidate **why randomness is beneficial during training yet detrimental during inference**.\n   - There is a lack of in-depth investigation into the **optimality of the cyclic strategy**. Why was the specific sequence `[4, 8, 16]` chosen? How would other deterministic sequences—such as non-monotonic patterns (e.g., `[4, 16, 8]`) or sequences with different periodicities (e.g., `[4, 8, 8, 16, 8, 4]`)—perform? A more systematic exploration of alternative cyclic schedules is needed.\n2. **Investigate the Impact of Training Strategy on Artifacts and Method Efficacy**:\n   - Clearly specify the training methodology used (e.g., teacher forcing vs. mixed teacher forcing/rollout training).\n   - If only teacher forcing was used, discuss the potential impact of rollout training on the emergence of harmonic artifacts and verify whether the proposed cyclic modulation remains beneficial in that regime. This would strengthen the claims regarding the method's robustness and practical utility."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MHrG31ZfnI", "forum": "itUo64aUeK", "replyto": "itUo64aUeK", "signatures": ["ICLR.cc/2026/Conference/Submission22417/Reviewer_2Q4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22417/Reviewer_2Q4X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656577471, "cdate": 1761656577471, "tmdate": 1762942209261, "mdate": 1762942209261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the systematic error accumulation during the autoregressive prediction of the fixed patch vision transformer. The authors observe that cyclically modulating patch sizes during autoregressive rollouts distributes errors across the frequency spectrum. Specifically, Overtone is presented, which contains dynamic stride modulation (CSM) and dynamic kernel resizing (CKM) to implement the cyclic patch modulation. As a result, Overtone achieves 40% error reduction in long rollouts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe authors notice and theoretically analyze an interesting problem, that is, systematic harmonic artifacts in the rollout of fixed patch vision transformers.\n\n-\tThe proposed method is verified to be effective in long-term rollout.\n\n-\tSufficient implementation details are included."}, "weaknesses": {"value": "Despite the above strengths, I think this paper has some implementation errors, which may cause the experimental results to be meaningless.\n\n### (1) Potential implementation error. \n\nIn canonical vision transformer (ViT) and its follow-ups, such as Swin Transformer, patch embedding is implemented as a flattening and linear projection. In contrast, this paper adopts convolutional layers for patch embedding and decoding. I do not think this is a correct implementation. Thus, I think the proposed method may fail in handling canonical vision transformers.\n\n### (2) A missing simple baseline.\n\nIn video prediction, a simple baseline to reduce such autoregressive error is rollout training. I think in the current experiments, all the models are trained in teacher forcing, which means during training, all the model input is ground truth. In such teacher-forced training, it is easy to observe so-called “systematic harmonic artifacts”. However, I doubt that such an accumulation error will be significantly resolved if the authors apply rollout training. Even with rollout training for about 10 steps, the model will be significantly improved.\n\nThus, I suggest that the authors rerun the experiments under a rollout training configuration. At least, provide some investigations into the effect of rollout training.\n\n### (3) More Transformer baselines.\n\nAdditionally, I think some transformer-based models should also be compared, such as [1,2].\n\n[1] Scalable Transformer for PDE Surrogate Modeling, NeurIPS 2023\n\n[2] Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers, ICML 2025\n\n### (4) Missing comparison with the ground truth spectrum.\n\nI appreciate that the authors provide the spectrum comparison in Figure 2 and the visualization comparison in Figures 4 and 5. However, I think Figure 2 should also involve the spectrum of ground truth, and Figures 4 and 5 are also expected to record the spectrum. I think this will make the comparison more direct and intuitive. Additionally, since the authors mentioned that “Overtone distributes errors across the frequency spectrum, preventing the systematic harmonic artifacts that plague fixed-patch models”, I am curious about the gap between the distributed spectrum and the ground truth. \n\n### (5) Limitation in irregular geometries.\n\nAll the patch-based transformers are limited to regular geometries. I think the authors should discuss this in the limitations section."}, "questions": {"value": "(1)\tHow to decide the hyperparameter in CSM and CKM, such as the adaptive patch size or stride size, as well as the cycle size.\n\n(2)\tIt seems that Transolver performs quite badly. Do you follow their official configuration for the Navier-Stokes benchmark?\n\n(3)\tIn Figure 4, although both CSM and CKM do not contain the harmonic artifacts, their predictions are still different from the ground truth. Do you have any interpretation about this? \n\n(4)\tHow much extra computation overload do you need in training CSM and CKM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lCKWmJCfiR", "forum": "itUo64aUeK", "replyto": "itUo64aUeK", "signatures": ["ICLR.cc/2026/Conference/Submission22417/Reviewer_u9p5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22417/Reviewer_u9p5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662068862, "cdate": 1761662068862, "tmdate": 1762942208815, "mdate": 1762942208815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a key failure mode in Transformer-based PDE surrogates: the use of fixed patch sizes in autoregressive rollouts leads to the systematic accumulation of errors at specific harmonic frequencies, manifesting as grid-like visual artifacts. The diagnosis of temporally coherent harmonic error accumulation is a valuable insight. The proposed method, implemented via two architecture-agnostic modules (CSM and CKM), not only improves the stability and accuracy of long-horizon predictions but also offers a practical way to trade computational cost for precision at inference time without retraining.\n\nDespite the good empirical results, the paper is weakened by a lack of deep theoretical grounding and leaves several critical questions about the method's underlying mechanics unanswered."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*   The paper's core strength lies in identifying and articulating the problem of harmonic error accumulation due to temporal coherence. This is a subtle but important issue that has been overlooked. The proposed cyclic modulation strategy is a simple and effective solution that directly addresses this root cause, and the empirical results demonstrate its success.\n\n*   The authors conduct a thorough and rigorous experimental evaluation. The use of diverse and challenging 2D/3D datasets from The Well benchmark ensures the findings are relevant. The comparison against not only fixed-patch baselines but also a range of other non-patch-based models (FFNO, SineNet, etc.) provides a robust contextualization of the method's performance.\n\n*   The Overtone framework is designed to be architecture-agnostic, a claim supported by its successful application to vanilla ViT, Axial ViT, and CViT."}, "weaknesses": {"value": "While the method is empirically successful, there are several areas of concern that detract from the paper's overall quality.\n\n*  The theoretical analysis in Section 2 and Appendix A provides some solid insights. But I don't quite understand how changing patch grid temporally thins and phase-misaligns injections which then shifts a portion of the error growth from quadratic to linear. (line 847~852)\n\n*  A major concern is the physical intuition behind cyclically modulating the input representation over time. I agree on the author's claim, that when the patch grid is fixed, the same locations see repeated errors step after step, causing these errors to reinforce and appear as grid-aligned patterns or “checkerboards” over long horizons. I also believe, at each autoregressive step, changing the patch size effectively changes the model's \"receptive field\", which helps improve the performance. But what doesn't make sense to me is when it's applied temporally like in this paper. This raises a fundamental question: **why should the model's perception of the physical state oscillate in this deterministic, cyclic manner?** This seems physically implausible. Furthermore, it's unclear on an implementation detail: **how are token embeddings handled with the changing patch size?** In a standard ViT, the embedding dimension is fixed to 786 as it's $c \\cdot p^2$. If the embedding dimension is kept fixed while the patch size changes, the model is cyclically fed tokens that represent vastly different amounts of spatial information. The rationale for why this is beneficial *temporally* is not well-explained.\n\n*  Missing Critical Ablations and Comparisons:\n    1.  The authors claim their method is architecture-agnostic and show that Swin Transformers suffer from similar artifacts (Appendix E). However, they do not present results for applying their method to a Swin Transformer. This is a logical omission that weakens the generality claim.\n    2.  The benefit of *temporal cycling* is not de-confounded from the benefit of simply having a model that is purely *spatially cycling*. An alternative model design could process the entire input with three different patch sizes (using three separate but smaller models) in every time step. Without this comparison, it's unclear if the cyclic schedule is the key ingredient, or if the training on diverse patch sizes is doing most of the work.\n     \n\n*   The evaluation relies solely on a single error metric (VRMSE). For physics simulation tasks, it is crucial to analyze whether the model preserves important derived physical quantities. For example, in the fluid dynamics datasets, an analysis of the predicted vorticity field (derived from the velocity) would provide much deeper insight into whether the model is learning the correct physical structures or merely achieving low pixel-wise error.\n\n*  There is no shape information in figure 3. You can just use same shape in green."}, "questions": {"value": "See questions from weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pRsjNDwbgN", "forum": "itUo64aUeK", "replyto": "itUo64aUeK", "signatures": ["ICLR.cc/2026/Conference/Submission22417/Reviewer_wD4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22417/Reviewer_wD4o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906648947, "cdate": 1761906648947, "tmdate": 1762942208494, "mdate": 1762942208494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Overtone, a framework for transformer-based PDE surrogate models. The model is trained once using randomly sampled patch sizes or strides. During inference, this single model can be deployed in two modes: (1) at a fixed patch size, which allows users to dynamically trade computational cost for accuracy, or (2) using a cyclic modulation of patch sizes during autoregressive rollouts. The authors evaluate this framework on 2D and 3D PDE benchmarks from *The Well* collection, primarily comparing against multiple, individually trained, fixed-patch-size baselines, as well as other SOTA surrogates such as FFNO and SineNet. The results indicate that the single Overtone model matches or exceeds the performance of other models, and that the cyclic rollout strategy mitigates harmonic error accumulation, leading to improved accuracy in long-horizon predictions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors analyzed the phenomenon in Transformer-based PDE prediction where fixed patch grids lead to error accumulation at specific harmonic frequencies, which manifests as spatial artifacts. To address this, they adopt strategies common in computer vision, incorporating multi-size kernels and multi-strides within a single model, thereby mitigating this problem in ViTs and achieving modest performance improvements.\n\n- The authors conduct evaluations on multiple PDE datasets from *The Well* benchmark and provide comparisons against several non-Transformer-based Neural Operators."}, "weaknesses": {"value": "- Insufficient Baselines: The paper claims flexibility but lacks critical comparisons against existing ViT variants that also process multi-scale information (e.g., U-ViT or Swin-Unet), making Overtone's relative advantages unclear. The authors are expected to add direct performance comparisons against these relevant baselines in their revision.\n\n- Limited Novelty: The core modules (CKM and CSM) are not novel. CKM is heavily based on prior work such as FlexiViT, and CSM (stride modulation) is a standard technique, limiting its contribution to an application of known methods to autoregressive PDE rollouts.\n\n- Presentation: There are some problems with the layout of the charts in the text, and the authors are expected to refine them during revisions."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aXqRd3Uj9L", "forum": "itUo64aUeK", "replyto": "itUo64aUeK", "signatures": ["ICLR.cc/2026/Conference/Submission22417/Reviewer_M73p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22417/Reviewer_M73p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977810263, "cdate": 1761977810263, "tmdate": 1762942208227, "mdate": 1762942208227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}