{"id": "XF9rcsXd1q", "number": 19492, "cdate": 1758296724361, "mdate": 1759897036015, "content": {"title": "Diverse Preference Optimization", "abstract": "Post-training of language models, either through reinforcement learning, preference optimization or supervised finetuning, tends to sharpen the output probability distribution and reduce the diversity of generated responses. This is particularly a problem for creative generative tasks where varied responses are desired. In this work we introduce Diverse Preference Optimization (DivPO), an optimization method which learns to generate much more diverse responses than standard pipelines, while maintaining the quality of the generations. In DivPO, preference pairs are selected by first considering a pool of responses, and a measure of diversity among them, and selecting chosen examples as being more rare but high quality, while rejected examples are more common, but low quality. DivPO results in generating 45.6% more diverse persona attributes, and a 74.6% increase in story diversity, while maintaining similar win rates as standard baselines. On general instruction following, DivPO results in a 46.2% increase in diversity, and a 2.4% winrate improvement compared to DPO.", "tldr": "We introduce a new method to simultaneously increase quality and diversity in preference optimization", "keywords": ["Alignment", "Diversity", "DPO"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2ebae9e90ded36b634548b1e16c95226518ec29a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the diversity collapse problem during post-training of LLMs. The authors propose Diverse Preference Optimization (DivPO), which changes how preference pairs are constructed before applying a standard DPO loss. For each prompt, the model samples a pool of responses, scores them with a reward model, and then forms two sets via a reward threshold. Instead of picking the highest-reward response like standard DPO, DivPO selects the most diverse response from the chosen set and the least diverse response from the rejected set, using one of three diversity criteria: inverse model probability, inverse word frequency, or an LLM-based diversity judge. The authors conduct experiments on structured personas, keyword/full story writing, and instruction following tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The diversity collapse issue is an important and timely problem for creative tasks.\n* The proposed idea is simple and can be easily integrated into the existing DPO pipeline.\n* The paper explores multiple diversity metrics and evaluates the method across a wide range of tasks."}, "weaknesses": {"value": "* Although the alignment collapse problem is discussed in Section 2, it would be more compelling if the authors provided additional theoretical analysis or more concrete examples/qualitative results illustrating the problem.\n* There is prior work addressing a similar problem, and the core idea of this paper appears quite similar to that of [1].\n* At the beginning of Section 3, the authors state that ”we want all high reward generations to have similar probabilities under the language model distribution”. However, the main objective of the proposed method does not seem to reflect this goal.\n* Determining the optimal threshold and diversity criterion may be challenging, as performance varies considerably across tasks and depends on several factors (dataset and reward distribution).\n* Regarding evaluation, it is difficult to verify whether the method truly improves performance, since increased diversity often comes at the cost of reduced output quality (particularly in Tables 1 and 3).\n\n[1] Chung, John Joon Young, et al. \"Modifying Large Language Model Post-Training for Diverse Creative Writing.\" arXiv preprint arXiv:2503.17126 (2025)."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wiI0ID3GvH", "forum": "XF9rcsXd1q", "replyto": "XF9rcsXd1q", "signatures": ["ICLR.cc/2026/Conference/Submission19492/Reviewer_e7H1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19492/Reviewer_e7H1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873670852, "cdate": 1761873670852, "tmdate": 1762931397197, "mdate": 1762931397197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DivPO, which incorporates diversity into the preference comparison sampling process. This approach enhances the diversity of the optimized model’s outputs without compromising performance.\n\nThe paper introduce three diversity criterion, model probability, word frequency, and LLM-as-a-diversity-judge. They validate the effectiveness of DivPO on three types of tasks: persona generation, full story generation, and instruction following."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The manuscript is clearly written and easy to understand.\n\nThe research problem is novel."}, "weaknesses": {"value": "See questions below"}, "questions": {"value": "1. The method introduces a hyperparameter ρ. How should ρ be chosen in practice?\n\n2. How generalizable is this approach? Specifically, can the diversity learned from training on Task A transfer to Task B?\n\n3. Does enhancing diversity negatively affect certain tasks, such as mathematical or programming abilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "91BWeXEW8l", "forum": "XF9rcsXd1q", "replyto": "XF9rcsXd1q", "signatures": ["ICLR.cc/2026/Conference/Submission19492/Reviewer_XygR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19492/Reviewer_XygR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939260980, "cdate": 1761939260980, "tmdate": 1762931396774, "mdate": 1762931396774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses alignment collapse from DPO by proposing Diverse Preference Optimization (DivPO), a novel training method that optimizes for both quality and diversity simultaneously. DivPO introduces a separate diversity criterion to select the most diverse response from the set of candidate responses chosen by a reward model as high-quality, and the least diverse (most common) response from the low-quality set chosen by the reward model. Then, DivPO trains the model using standard DPO on this new pair. DivPO significantly improves diversity while maintaining similar win rates as DPO and other standard baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper successfully tackles a practical problem in post-trained LLMs with a simple and intuitive, yet effective method. DivPO intervenes at the data-selection stage, which makes it easy to plug this method into existing DPO pipelines.\nThe paper shows that DivPO can increase diversity without decreasing quality, which makes the method useful for practical applications."}, "weaknesses": {"value": "The paper currently lacks comparison to other diversity methods as baselines, it would be important to add at least one method that is designed to promote diversity to compare against.\nOn experiments, there are three tasks studied, but each task experiment uses different choices of diversity criteria. Why not use all three criteria in all experiments?\nAlso the only realistic task evaluated on is instruction following, but the diversity evaluations seem brittle, so I would love to see diversity evaluated by something like LLM-as-judge or that is more correlated with human diversity perception to be fully convinced of the method's effectiveness."}, "questions": {"value": "What is the computational overhead of DivPO compared to DPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UbpvdAVJEt", "forum": "XF9rcsXd1q", "replyto": "XF9rcsXd1q", "signatures": ["ICLR.cc/2026/Conference/Submission19492/Reviewer_rSdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19492/Reviewer_rSdp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762315102853, "cdate": 1762315102853, "tmdate": 1762931396153, "mdate": 1762931396153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}