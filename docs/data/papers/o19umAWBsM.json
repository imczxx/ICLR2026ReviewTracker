{"id": "o19umAWBsM", "number": 17211, "cdate": 1758273511133, "mdate": 1759897190655, "content": {"title": "eXIAA: eXplainable Injections for Adversarial Attack", "abstract": "Post-hoc explainability methods are a subset of Machine Learning (ML) that aim to provide a reason for why a model behaves in a certain way. In this paper, we show a new black-box model-agnostic adversarial attack for post-hoc explainable Artificial Intelligence (XAI), particularly in the image domain. The goal of the attack is to modify the original explanations while being undetected by the human eye and maintain the same predicted class. In contrast to previous methods, we do not require any access to the model or its weights, but only to the model's computed predictions and explanations. Additionally, the attack is accomplished in a single step while significantly changing the provided explanations, as demonstrated by empirical evaluation. The low requirements of our method expose a critical vulnerability in current explainability methods, raising concerns about their reliability in safety-critical applications. We systematically generate attacks based on the explanations generated by post-hoc explainability methods (saliency maps, integrated gradients, and DeepLIFT SHAP) for pretrained ResNet-18 and ViT-B16 on ImageNet. \nThe results show that our attacks could lead to dramatically different explanations without changing the predictive probabilities.\nWe validate the effectiveness of our attack, compute the induced change based on the explanation with mean absolute difference, and verify the closeness of the original image and the corrupted one with the Structural Similarity Index Measure (SSIM).", "tldr": "We introduce a novel black-box model-agnostic one-step adversarial attack on image explanations generated by XAI methods, by leveraging the method itself, minimally altering the original classification prediction, and making the attack hard to detect", "keywords": ["Explainable Artificial Intelligence", "Adversarial attack"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c17fb4d95eb307d8c42aa52536ee46e54ef21f39.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose an agnostic adversarial injection method into input image of neural networks that changes the explanation of the neural networks' predictions."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "I believe the strength of the method is (1) it is simple to implement and (2) it is model agnostic."}, "weaknesses": {"value": "1) Weak baseline. As there are several other methods with the same purposes (even they are white-box or gray-box), I believe the paper needs to compare their method with SOTA method, not \"Gaussian\" baseline, which is too weak as a comparison in my opinion.\n\n2) It is not easy to interpret the impact of the work from the reported experiments. For example, I do not think the relationship between the change in explanation vs top-k or alpha (fig.2 and 3) are the most important aspect of the method. What we are interested is the relationship between \"how much the explanation changed\" vs \"how much distortion that the method create\" on the input (or more importantly, how easy to detect that perturbation). We cannot intuitively understand that from the reported results.\n\n3) It is little explanation on \"why the method work?\". For example, as the adversarial explanation is originated from another sample of different class, I would expect the adversarial explanation should be somehow related to the explanation of that other sample (like the cat image in Fig.1). However, the adversarial explanation of Fig.1 is shown to have little relationship with that other sample. I even question the validity of the explanation of the dog image in that figure.\n\n4) I am not convinced that the method can work with explanation methods based on \"segmentation\" like LIME or LIME-SHAP."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HEtEAnEokE", "forum": "o19umAWBsM", "replyto": "o19umAWBsM", "signatures": ["ICLR.cc/2026/Conference/Submission17211/Reviewer_V3hN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17211/Reviewer_V3hN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761154038165, "cdate": 1761154038165, "tmdate": 1762927178284, "mdate": 1762927178284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new method to attack explanation methods (saliency maps, etc.) without relying on access to the model internals. Their method finds the second most likely class, identifies which pixels are important for that classification (through the explenation method), and then attacks those pixels by blending those pixels with the pixels of another image of that confused class."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The method is simple"}, "weaknesses": {"value": "- You claim that your method is operating in a more realistic setting where you do not have access to model internals (e.g. gradients, weights, etc.). But; actually, you require explenations of the model. Thus; in fact, you _do_ need access to the model internals. I find this very weak - and this realization kind of defeats the novelty of this paper in my opinion. One way you could potentially get around this, is by using a surrogate model - e.g. say you train a VGG16 on Imagenet, but instead use the explenations of another model (eg your ResNet). This could make it more convincing that you don't need access to the model internals; e.g. you only need a similar model. \n\n- I find it a bit worrying that in the related work, only related methods are cited until 2019. Has there been no progress in this area of attacking explenation methods? I expect that you at least include a more recent work that is most similar to yours and highlight it. \n\n- In recent years, this field has developed quite a lot, with also now \"robust\" explenation methods. See for example this survey which is highly relevant for you: [1]. it would be very convincing if you can show your attack on more recent robust methods. \n\n- The presentation of the paper is quite weak; for example, many space is wasted, giving 6 plots that are all very similar. The writing is not concise; with quite a few repititions; the equations are also quite verbose and can be significantly shortened. When I am reading this paper I get the feeling I am reading some \"filler\" content that was added to make sure the page limit was reached. \n\n- In ICLR, I expect to see more convincing experiments. For example, is it possible to apply this framework also to NLP or other types of data? And showing more convincing examples where the explenation is significantly changed (some images! the graphs stay very abstract); other works have showed its possible to also induce certain patterns in the explenation; for example [2,3] - is that possible for your method?\n\n[1] Baniecki, H., & Biecek, P. (2024). Adversarial attacks and defenses in explainable artificial intelligence: A survey. Information Fusion, 107, 102303.\n[2] Viering, T., Wang, Z., Loog, M., & Eisemann, E. (2019). How to manipulate cnns to make them lie: the gradcam case. arXiv preprint arXiv:1907.10901.\n[3] Dombrowski, A. K., Alber, M., Anders, C., Ackermann, M., Müller, K. R., & Kessel, P. (2019). Explanations can be manipulated and geometry is to blame. Advances in neural information processing systems, 32."}, "questions": {"value": "- The baseline (adding Gaussian noise) is not very reasonable. Can you instead include another attack method, e.g. Ghorbani? Of course, they are using model internals, but at least allows us to better understand how it compares to other methods from literature\n\n- For me, the change in explenation measured is not very convincing. By changing many pixels a little, we can also achieve a large difference in explenation, but this may not be even visible to humans. I suggest to again have a look at Ghorbani at how they evaluate. How does your method compare when using other metrics; for example, distance of center of mass could be interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wPMIRseWU0", "forum": "o19umAWBsM", "replyto": "o19umAWBsM", "signatures": ["ICLR.cc/2026/Conference/Submission17211/Reviewer_hn1B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17211/Reviewer_hn1B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415262027, "cdate": 1761415262027, "tmdate": 1762927178033, "mdate": 1762927178033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel adversarial attack method that can attack the map of explanation for classification even though the model is not accessible (black-box). The flow is well explained in Figure 1. An image in the second candidate class is selected and then the attack injection is generated and integrated into the target image. As a result, the generated map will be distorted while the classification result is unchanged."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a new attack method for deep neural networks. The method is simple but effective. Experimental results show that the explanation methods such as saliency maps, integrated gradients, and DeepLIFT SHAP are affected, which can be a potential risk of AI."}, "weaknesses": {"value": "The proposed approach is very simple and seems straightforward. In other words, the proposed method seems ad-hoc and empirical with no theoretical backups. Therefore, technical depth is rather weak. The authors may want to include a mathematical analysis on why such an attack is possible. Besides, Figure 5 shows that the explanation change induced by the images of the running-up class (full lines) is always as good as or better than picking an attack image from any other class, which is also empirical and not backed up with theoretical analysis.\n\nThis paper assumes maps that explain the classification results are available (at least when attacking). I wonder whether this is a reasonable assumption. Because, in my personal opinion, such maps are not usually shown to ordinary users and will be used for researchers/developers to check the validity of the models’ outputs. But in such a case, the users can access the model. \n\nEven though defensive methods for this attack are their future work, Sensitivity analysis to defensive methods such as adversarial training or diffusion-based purification should be conducted because they are standard defense methods in this area. \n\nSome minor modification proposals (no need to reply)\n- missing commas and periods after equations, e.g., (1) and (2).\n- missing x and y axes labels in Figures 2-5. I understand they are given in the figure captions, but it is not standard."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6ilpN6JlZs", "forum": "o19umAWBsM", "replyto": "o19umAWBsM", "signatures": ["ICLR.cc/2026/Conference/Submission17211/Reviewer_8MJj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17211/Reviewer_8MJj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987400534, "cdate": 1761987400534, "tmdate": 1762931429758, "mdate": 1762931429758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a black-box adversarial attack method targeting post-hoc explainability methods (saliency maps, integrated gradients, DeepLIFT SHAP) in image classification. The attack modifies explanations while maintaining predictions and visual similarity by: (1) selecting an image from the runner-up predicted class, (2) extracting top-k positive attribution features, and (3) blending them with the original image using a weighted sum. Experiments on ImageNet with ResNet-18 and ViT-B16 demonstrate that explanations can be dramatically altered (up to 188% change for DeepLIFT SHAP on ViT-B16) while maintaining high SSIM (>0.7) and small prediction changes (<15%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and underexplored vulnerability in XAI systems. The fragility of explanations in safety-critical applications like medicine is a genuine concern, and demonstrating this vulnerability is valuable. The practical threat model is a key strength—requiring only access to predictions and explanations (no model weights or gradients) makes this attack significantly more realistic than prior work requiring white-box access or iterative optimization. The single-step nature further enhances practicality compared to multi-step attacks.\n\nThe experimental evaluation is comprehensive and well-structured. Testing across multiple explainability methods (saliency maps, integrated gradients, DeepLIFT SHAP), architectures (ResNet-18, ViT-B16), and systematic hyperparameter sweeps (α ∈ [0.03, 0.15], top-k ∈ [0.01, 0.8]) provides convincing evidence. The three-metric evaluation framework (explanation change, SSIM for detectability, prediction change) appropriately captures the attack objectives. The finding that ViT-B16 is more vulnerable to explanation attacks despite being more robust to prediction attacks (Figure 2 vs Figure 4) is particularly interesting and counterintuitive.\nThe visualization and presentation are strong. Figure 1 clearly illustrates the method, and Figures 2-4 effectively show performance trends across scenarios. The empirical validation in Figure 5 that runner-up class selection outperforms random class selection supports the design choice."}, "weaknesses": {"value": "The technical novelty is limited. The three-phase pipeline (select runner-up image, extract top features, alpha-blend) is relatively straightforward and lacks theoretical depth. The method essentially performs a weighted average of pixels guided by saliency this is more of an engineering contribution than a fundamental algorithmic advance. The paper would benefit from deeper analysis of why this simple approach works so effectively.\n\nThe threat model requires clarification. The assumption that attackers have access to model explanations but not the model itself seems narrow. In what realistic scenarios would this occur? If the model is a black-box API, would it actually expose detailed saliency maps? This concern about practical applicability isn't adequately addressed. The medical diagnosis example in the introduction (corrupted patient data producing different explanations) is compelling but isn't demonstrated experimentally on medical datasets.\nThe baseline comparison is weak. Comparing only against Gaussian noise is insufficient—the paper should compare against other explanation attack methods from the literature (Ghorbani et al. 2019, Dombrowski et al. 2019). While the authors claim their method has \"fewer requirements,\" a fair comparison adapting these methods to the black-box setting would strengthen the contribution. The lack of quantitative comparison makes it difficult to assess relative performance.\n\nThe evaluation has important gaps. The claim that attacks are \"visually undetectable\" relies solely on SSIM, but human perception studies would provide stronger evidence. SSIM values around 0.7-0.9 (Figure 3) may not actually be imperceptible to humans, especially for trained observers. The paper also lacks analysis of detection methods are there simple statistical tests that could identify these attacks? No defensive mechanisms are proposed or evaluated, limiting the practical impact.\n\nThe scope is quite limited. The experiments focus exclusively on ImageNet classification with two architectures. Extensions to other domains (medical imaging, autonomous driving mentioned in the introduction) are absent. The paper claims the approach \"could also be applied to other types of data\" (time series, text, tabular) but provides no evidence or concrete methodology for these extensions. The method is also limited to single-label classification multi-label scenarios are acknowledged as future work but not explored.\nThe writing could be more concise. Section 2 contains extensive background that could be shortened. The paper uses 15 pages including appendices when the core contribution could be presented more tightly. Some notation is introduced but not consistently used (e.g., f_j for class j probability)."}, "questions": {"value": "This paper makes a useful contribution by demonstrating a practical, low-requirement adversarial attack on XAI methods. The comprehensive empirical evaluation across multiple methods and architectures is valuable, and the findings about ViT's vulnerability to explanation attacks are interesting. However, the limited technical novelty, narrow threat model, weak baselines, lack of defenses, and missing human perceptual studies constrain the impact. The work successfully raises awareness about XAI fragility but falls short of providing deep insights into why these vulnerabilities exist or how to address them.\n\nThe paper would be significantly strengthened by: (1) comparison with adapted versions of prior explanation attack methods, (2) human perception studies validating \"undetectability\" claims, (3) theoretical or empirical analysis of why the attack works, (4) evaluation of potential detection methods and defenses, and (5) extension to at least one safety-critical domain (medical imaging) to validate the motivating use case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ztXgzdoEkt", "forum": "o19umAWBsM", "replyto": "o19umAWBsM", "signatures": ["ICLR.cc/2026/Conference/Submission17211/Reviewer_pgSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17211/Reviewer_pgSx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185014836, "cdate": 1762185014836, "tmdate": 1762927177360, "mdate": 1762927177360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}