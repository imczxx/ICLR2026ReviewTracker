{"id": "mAxz9MQFoe", "number": 12798, "cdate": 1758210395525, "mdate": 1759897484559, "content": {"title": "GAIR: A Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations", "abstract": "Vision Transformer (ViT) has been used in many computer vision tasks with excellent results by providing representations for a whole image or image patches. However, ViT lacks detailed localized image representations when applied to geospatial tasks that involve multiple geospatial data modalities, such as overhead remote sensing (RS) data, ground-level imagery, and geospatial vector data. Here high-resolution localized representations are vital for modeling geospatial relationships and alignments across modalities. We proposed to solve this representation problem with an implicit neural representation (INR) module extending ViT with Neural Implicit Local Interpolation, which produces a continuous RS image representation covering arbitrary location in the RS image. Based on the INR module, we propose GAIR, a multimodal Geo-Foundation Model (GeoFM) integrating overhead RS data, street view (SV) imagery, and their geolocation metadata. GAIR utilizes three factorized neural encoders to project different modalities into the embedding space, and the INR module is used to further align these representations geographically, which are trained with contrastive learning objectives from unlabeled data. We evaluate GAIR across 9 geospatial tasks and 22 datasets spanning RS image-based, SV image-based, and location embedding-based benchmarks. Experimental results demonstrate that GAIR outperforms state-of-the-art GeoFMs and alternative training objectives (e.g., MoCo-V2 and MAE) that do not use fine-grained geo-aligned spatial representations. Our results highlight the effectiveness of GAIR in learning generalizable geospatial representations across tasks, spatial scales, and temporal contexts.", "tldr": "", "keywords": ["Geo-Foundation Model", "Spatially Explicit Contrastive Learning", "Implicit Neural Representation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/441269a18f16aa19d073321de4e90123fd7c6e2e.pdf", "supplementary_material": "/attachment/e62f245db8e8d546fcf60dbce90db49b38e8b3ba.zip"}, "replies": [{"content": {"summary": {"value": "SUMMARY: The authors propose GAIR, a new pretrained model consisting of contrastive pretraining of geographically aligned satellite (overhead) and street view (ground level) imagery. The model include a location encoder network that allows users at inference time to query location-specific embeddings capturing location characteristics only using latitude/longitude inputs. The model is trained using CLIP-style image-image and image-location matching. This work directly expands existing work on pretrained location encoding, namely SatCLIP and GeoCLIP, into a multi-modal direction. GAIR is evaluated extensively; all three encoders (satellite image, street view image and location) are evaluated and tested separately. The authors show improvements over existing models in all 3 categories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "STRENGTHS:\nSome aspects I particularly enjoyed about this paper are:\n\n- The paper is well motivated, both from a methodological and an application perspective. Working with geo-aligned satellite and on-the-ground imagery is interesting and being able to \"translate\" between these modalities / have embeddings based on different scales could be potentially very impactful.\n\n- The experimental section of the paper is extensive! This is great to see; all 3 different encoders are evaluated. It might be a bit too expensive given that the authors sacrifice giving more details about the methodology.\n\n- It's great that this work discusses and addresses data imbalances in geospatial data, esp. the urban-rural divide."}, "weaknesses": {"value": "SHORTCOMINGS:\n\nMajor:\n\n- The paper is heavily based on the use of implicit neural representations for geospatial applications, yet fails to cite foundational work in this space: [1] are the first to propose the use of INRs to learn continuous functions on the sphere of Earth while SatCLIP (which is cited) expands this approach to learning continuous Earth embeddings. This should be fixed!\n\n- The choice of the location encoder architecture is not explored enough. The authors use a fourier-based location encoder architecture (as GeoCLIP does), however they don't go into a lot of detail on the exact design of the encoder. Furthermore, [1] also show that sin/cos transforms are not ideal for Earth data and a more specialized transform like Spherical Harmonics can perform better. It would be interesting to see different location encoder methods tested here.\n\n- There is also a great lack of detail on the design choices for the satellite image and street view image encoder. How do these models look like exactly? The authors mention that they use a pretrained satellite image encoder - which one is that? It is hard to objectively assess the experimental performance without those details.\n\nMinor:\n\n- line 224: should be \"multilayer perceptron\"\n\n- Performance gains are not huge and hard to assess without confidence intervals.\n\n- Not enough discussion of limitations; conclusion section is super short."}, "questions": {"value": "Overall this is an interesting paper with some interesting methodological advances and a great experimental section. I would ask the authors to address my concerns and questions outlined in the \"Weaknesses\" sections.\n\nReferences:\n\n[1] Rußwurm, Marc, et al. \"Geographic location encoding with spherical harmonics and sinusoidal representation networks.\" arXiv preprint arXiv:2310.06743 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zwLgFF5h4D", "forum": "mAxz9MQFoe", "replyto": "mAxz9MQFoe", "signatures": ["ICLR.cc/2026/Conference/Submission12798/Reviewer_fMPm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12798/Reviewer_fMPm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761424733205, "cdate": 1761424733205, "tmdate": 1762923607666, "mdate": 1762923607666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GAIR (Geo-Aligned Implicit Representations), a multimodal Geo-Foundation Model (GeoFM) that integrates remote sensing (RS) imagery, street-view (SV) imagery, and geolocation metadata. GAIR extends ViT-based encoders with an implicit neural representation (INR) module called Neural Implicit Local Interpolation (NILI) to achieve continuous, fine-grained spatial alignment between overhead and ground-level modalities. The model is pretrained on the Streetscapes1M dataset using contrastive self-supervised learning and evaluated on 9 tasks across 22 datasets, including RS segmentation, SV regression/classification, and location-based prediction. The authors claim GAIR outperforms existing GeoFMs such as SatMAE, CROMA, DOFA, and TaxaBind in overall performance"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Problem motivation: Addresses spatial alignment challenges in multimodal geospatial modeling by leveraging continuous representations rather than patch-level embeddings.\n\nModel integration: Combines RS, SV, and geolocation embeddings in a unified self-supervised framework, potentially useful for large-scale geospatial representation learning.\n\nComprehensive experiments: Evaluations span diverse tasks (RS, SV, location) with ablation and bias analyses, which demonstrates methodological completeness.\n\nData contribution: The Streetscapes1M dataset may be useful to the broader community if released, as few large-scale RS–SV paired datasets exist."}, "weaknesses": {"value": "Limited novelty and incremental contribution.\nThe method mainly combines existing components—ViT backbones, INR interpolation (essentially adapted from LIIF), and standard contrastive objectives—without introducing a fundamentally new mechanism. The “geo-alignment” through implicit interpolation is a direct application of known implicit coordinate mappings; no theoretical or algorithmic advancement beyond straightforward integration is shown.\n\nInsufficient comparison and contextualization.\nThe paper barely situates GAIR relative to the rich literature of multimodal foundation models (e.g., OmniSat, GeoChat, RemoteCLIP, EarthGPT, SpectralGPT). Many relevant baselines are omitted or only superficially compared. No discussion is given on how GAIR differs architecturally or conceptually from these prior GeoFMs, especially those already using spatial alignment or contrastive geo-supervision.\n\nWeak empirical evidence for innovation.\nThe performance gains over baselines are marginal and inconsistent across tasks (often within 0.5–1 mIoU or 0.05 F1), insufficient to justify a new foundation model. The “implicit alignment” advantage is not validated via spatial offset experiments or perturbation tests.\n\nPresentation quality.\nMany figures (e.g., Figures 1–3 and 5) are too small, with unreadable text, axes, and legends. Key architectural diagrams lack sufficient clarity to convey technical differences, making it difficult to follow model flow or understand parameter interactions.\n\nClarity and writing issues.\nImportant methodological descriptions (e.g., NILI equation details, coordinate projection Ψ, and training hyperparameters) are terse and sometimes ambiguous. Cross-references to appendices are excessive, and terminology such as “Geo-Foundation Model” and “implicit interpolation” is used without clear distinction from prior work."}, "questions": {"value": "How does GAIR fundamentally differ from previous GeoFMs like OmniSat, DOFA, or GeoChat that already incorporate multimodal and geo-aligned embeddings?\n\nCan the authors conduct spatial perturbation or registration-offset experiments to empirically verify that NILI indeed improves geo-alignment robustness?\n\nThe evaluation omits major multimodal FMs (e.g., RemoteCLIP, EarthGPT). Why were these not compared directly?\n\nHow sensitive is GAIR to coordinate noise and scale mismatch between RS and SV data?\n\nPlease provide enlarged, legible figures and detailed ablation visualizations to support interpretability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "I2m7Hb5H3C", "forum": "mAxz9MQFoe", "replyto": "mAxz9MQFoe", "signatures": ["ICLR.cc/2026/Conference/Submission12798/Reviewer_jEJB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12798/Reviewer_jEJB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916164223, "cdate": 1761916164223, "tmdate": 1762923607280, "mdate": 1762923607280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GAIR, a multimodal Geo-Foundation Model (GeoFM) that integrates overhead remote sensing (RS) imagery, street-view (SV) images, and geolocation metadata. The authors identify that Vision Transformers (ViTs) lack detailed localized image representations at arbitrary positions, which are essential for modeling geospatial relationships and aligning different modalities. To address this, they introduce an Implicit Neural Representation (INR) module called Neural Implicit Local Interpolation (NILI), which enables continuous spatial representations across the RS image. GAIR employs three factorized neural encoders and is trained with contrastive learning objectives on unlabeled multimodal data. The model is evaluated on 9 geospatial tasks and 22 datasets, demonstrating strong results compared with state-of-the-art GeoFMs and alternative objectives such as MoCo-v2 and MAE.\n\nThe contributions can be summarized as follows:\n1. Defined the problem: the overhead RS image needs detailed localized image representations at arbitrary positions to align with other data modalities, like ground-level imagery and geospatial vector data (like longitude and latitude). \n2. Proposed implicit neural representation (INR) module extending ViT with Neural Implicit Local Interpolation, which produces a continuous RS image representation covering arbitrary locations in the RS image \n3. Proposed contrastive learning-based method to train a multimodal GeoFM GAIR to integrate overhead RS data, street view imagery, and geolocation metadata. \n4. Evaluated across 9 geospatial tasks and 22 datasets (RS image-based, SV image-based, location embedding-based benchmarks), and achieved quite a shining result, and also included a lot of ablation studies to verify the effectiveness of their algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly defines the representation problem in multimodal geospatial learning and proposes a principled solution with the INR module.\n- The NILI module is a creative idea that extends ViTs to continuous spatial domains.\n- The experimental evaluation is extensive and well-structured, covering a wide range of tasks, modalities, and datasets.\n- Ablation studies are comprehensive and provide evidence supporting the design choices.\n- The writing is clear and the overall contribution is technically sound and reproducible."}, "weaknesses": {"value": "- In the abstract, the claim that ViT “lacks detailed localized image representations at arbitrary positions” could be better justified: is this empirically validated or based on common understanding in the community?\n- The term “generalizable geospatial representations” needs clarification — does it refer to zero-shot transfer or few-shot fine-tuning?\n- Regarding the spatial interpolation in Figure 2(a) and Section 3.2, how can we verify that it actually works as intended? Interpolation from the four nearest patch embeddings assumes local linearity in representation space; please clarify if this assumption holds.\n- Minor issue (L192–195): “path-level” should be “patch-level.”\n- In the finetuning setup: “For remote sensing benchmarks, we fine-tune only the UPerNet while keeping the pretrained backbone frozen.” What motivates this choice? Is it due to the large parameter count of the RS encoder?\n- For the street-view imagery benchmarks, why were CROMA, SatMAE, PIS, and TaxaBind chosen? Some of these (CROMA and SatMAE, for example, are trained on high resolution satellite imagery) are not pretrained on SV data, so the fairness of the comparison could be discussed.\n- From the benchmark description, TaxaBind appears quite similar to GAIR; please further clarify the differences.\n- The GAIR-MAE variant may introduce a viewpoint shift problem when treating SV images as masked RS patches. Is this pretraining setup feasible or well-justified?\n- In Table 2, including U-Net and ViT baselines would improve completeness, as prior work (e.g., Pangaea benchmark) shows their competitiveness with GeoFMs.\n- Figure 4 is unclear: what is “Masked SSI”? It is not defined in the paper.\n- Similarly, Figure 5 does not clearly demonstrate the claimed alignment. The explanation of how GAIR learns geographical alignment should be expanded."}, "questions": {"value": "Please refer to Weaknesses. Despite these questions and minor clarity issues, the submission is clear, technically correct, and experimentally rigorous. It presents a novel and well-motivated approach with strong empirical validation. I recommend acceptance, provided the authors address the raised clarifications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LljGu5Wgi1", "forum": "mAxz9MQFoe", "replyto": "mAxz9MQFoe", "signatures": ["ICLR.cc/2026/Conference/Submission12798/Reviewer_ePnY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12798/Reviewer_ePnY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991963275, "cdate": 1761991963275, "tmdate": 1762923606964, "mdate": 1762923606964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a “Geo–foundation model”, which in practice consists of three separate components: an image encoder for satellite imagery, an image encoder for street-view imagery, and a location encoder mapping coordinates to embedding vectors. All three are trained jointly via CLIP-style objectives (Eqs. 2 and 3). Conceptually, the location encoder functions as an implicit neural representation, whereas the image encoders perform standard feature extraction.\n\nHowever, the overall novelty is limited: the formulation essentially combines the geolocalization premise of GeoCLIP (street-view ↔ location) and SatCLIP (satellite image ↔ location).\n\nBecause the framework produces three different models, the experimental evaluation becomes complex and somewhat unfocused. Table 1 benchmarks the street-view component, Table 2 the satellite component, and Table 3 the location encoder. As such, the model does not seem to function as a single “foundation model”, but rather as a training framework for three separate ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Unified framework to jointly train satellite, street-view, and coordinate encoders with a CLIP-style objective. I believe massive multi-modal pre-training beyond street-aereal-location should be explored more.\n* Clear experimental setup demonstrating that each sub-model can be evaluated in isolation."}, "weaknesses": {"value": "* Limited methodological novelty — largely a combination of ideas already present in SatCLIP and GeoCLIP.\n* The “foundation model” framing is misleading: the method is effectively three separate models trained together, not a single unified model.\n* Experimental evaluation feels scattered across sub-components with no strong, coherent take-home message."}, "questions": {"value": "* How well motivated is the \"implicit representation\" aspect of this work? As I understand, only the the location encoder can be seens as an implicit neural representation, while image encoders perform regular feature extraction from image data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vN5Lej86CJ", "forum": "mAxz9MQFoe", "replyto": "mAxz9MQFoe", "signatures": ["ICLR.cc/2026/Conference/Submission12798/Reviewer_pckc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12798/Reviewer_pckc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024874341, "cdate": 1762024874341, "tmdate": 1762923606671, "mdate": 1762923606671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}