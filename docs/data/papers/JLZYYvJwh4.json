{"id": "JLZYYvJwh4", "number": 6012, "cdate": 1757950604017, "mdate": 1759897939641, "content": {"title": "Compositional Multimodal Reasoning for Long-Horizon Robotic Manipulation in Scientific Experiments", "abstract": "Long-horizon robotic manipulation in laboratory environments presents great challenges requiring strict procedural dependencies, fine-grained object manipulation, and domain expertise. While recent work integrates large language models (LLMs) with multimodal perception for task planning, they often require task-specific fine-tuning and struggle to generalize to scientific workflows that demand compositional reasoning beyond spatial grounding. To address these challenges, we propose Compositional Multimodal Planner (CoMP), a decoupled hierarchical reasoning framework for long-horizon robotic planning in scientific experiments. (1) a task-level planner using chain-of-thought prompting to capture task logic, (2) a mid-level multimodal planner that integrates future scene prediction to enable visually grounded reasoning, and (3) a low-level skill controller that executes actions via reinforcement learning. This compositional and decoupled design allows independent optimization of each component, enabling controllable and extensible planning without requiring fine-tuning of large models. To support evaluation, we introduce a benchmark dataset for scientific experiment tasks. Experiments on both our benchmark and RLBench demonstrate that CoMP consistently outperforms strong baselines, demonstrating its effectiveness and generalizability in long-horizon robotic planning.", "tldr": "", "keywords": ["long-horizon planning", "manipulation planning", "vision-language-action", "multimodal reasoning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41938eb7b27ae911cc1760e8664e92761be44f80.pdf", "supplementary_material": "/attachment/eefd479f05fe9c46e5bd47686bf1c07c09de6902.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes CoMP, a modular framework for long-horizon robotic manipulation in scientific experiments. CoMP separates (i) a task-level LLM planner using chain-of-thought, (ii) a mid-level multimodal planner that predicts future scene frames and maps subtasks to action primitives, and (iii) a low-level RL controller (e.g., DDPG). The authors also introduce a simulation suite and benchmark, report gains over several baselines (including on RLBench), and present a limited sim-to-real evaluation using detection + depth for coordinate transfer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear modularity & interpretability: Decoupling planning, grounding, and control is well-motivated for long-horizon laboratory workflows and aids debugging/verification.\n- New benchmark & analyses: A domain-specific benchmark emphasizing procedural dependencies is valuable.\n- Empirical coverage: Results on both the proposed suite and RLBench show consistent improvements on several tasks"}, "weaknesses": {"value": "- Real-robot validation is limited: The physical experiments are narrow in scope and rely on a sim-to-real coordinate mapping; robustness (sensor noise, calibration drift, safety) is under-characterized. Strengthen with more tasks, seeds, and error/failure analyses; report success variance and safety incidents.\n- Training–to–deployment gap: Clarify whether the RL policy is trained only in simulation and how it transfers (domain randomization, dynamics mismatch, contact modeling). Provide fine-tuning/none, and quantify performance drop from sim to real.\n- Efficiency & latency: The pipeline appears complex (CoT planning → visual prediction → MLM primitives → RL). There is no analysis of runtime (per-step latency, FPS), computational cost, or throughput—critical for long-horizon tasks.\n- Baseline completeness and fairness: Include comparisons to simpler Dual-System (System-1 + System-2) paradigms (e.g., “Hi Robot”-style simple planner + skills) and rule-based/task-graph planners to demonstrate that gains require the full CoMP stack. Explicitly discuss fairness for re-implemented baselines (training data, hyperparameters)."}, "questions": {"value": "- RL training & transfer: Was the RL policy trained exclusively in simulation? What domain randomizations were used? Any real-robot fine-tuning? How is stability ensured under perception errors?\n- Calibration pipeline: How is the YOLO + depth → simulation mapping calibrated and maintained over time? What is the failure rate due to calibration drift or occlusions?\n- Runtime profile: What are the end-to-end latencies for (a) CoT planning, (b) future-frame prediction, (c) MLM primitive generation, and (d) RL control? Where is the bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yEPrS5oqRV", "forum": "JLZYYvJwh4", "replyto": "JLZYYvJwh4", "signatures": ["ICLR.cc/2026/Conference/Submission6012/Reviewer_7GdJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6012/Reviewer_7GdJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760881158822, "cdate": 1760881158822, "tmdate": 1762918414602, "mdate": 1762918414602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CoMP, a modular framework for long-horizon robotic manipulation in scientific lab settings. It decouples: (i) a task-level LLM planner using chain-of-thought (CoT) with a verification–correction loop, (ii) a mid-level multimodal planner combining a conditional diffusion–based future frame predictor and a multimodal LLM to turn predicted goals into action primitives, and (iii) a low-level RL controller (DDPG). The authors also introduce a small simulation benchmark in CoppeliaSim and report higher success rates than several baselines on their benchmark and mixed results on RLBench, plus limited sim-to-real tests using YOLOv8 + depth mapping."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The modular decomposition is easy to understand: a language-based task planner decomposes goals into symbolic subtasks, a mid-level component grounds each subtask using the current and predicted goal images, and a low-level controller executes the resulting action primitives. The paper’s overview figure and method section make this pipeline intuitive.\n2. Clear ablations illustrate the role of key modules: the paper varies the LLM and the MLM and reports CoT step-wise ablations, which together help attribute where gains come from.\n3. The descriptions of each module are clear in both the main text and the appendix, with concrete prompt templates, training details, and a breakdown of the verification–correction operations; this level of disclosure makes the system easier to understand and (partially) reproduce."}, "weaknesses": {"value": "1. **Overly complicated pipeline.** The system performs subtask decomposition twice—first via a task-level LLM planner and then again via GPT-4o in the mid-level. It’s unclear why a single VLM could not directly produce the decomposition and grounding, and what unique value the initial LLM adds.\n\n2. **Limited novelty.** The core ideas—LLM/VLM-based subtask decomposition and future image prediction—are well-trodden areas with substantial prior art. The paper does not sufficiently articulate what is new beyond combining known components.\n\n3. **Missing imitation-learning setting.** Most competitive VLA systems and many baselines in the literature rely on imitation learning. The paper uses only RL, making comparisons incomplete and potentially unfavorable to the method’s practicality.\n\n4. **Incomplete empirical validation.** Experiments are almost entirely in simulation with only minimal real-robot results. Claims about long-horizon manipulation and “scientific workflow” utility are not substantiated on physical hardware.\n\n5. **Scalability concerns.** The many moving parts (planner LLM, GPT-4o grounding, predictor, RL controller) raise questions about data efficiency, engineering overhead, and whether the framework scales to large, diverse task suites without prohibitive complexity."}, "questions": {"value": "1. **On the dual decomposition:** Why is a separate task-level LLM needed if a modern VLM can directly output grounded subgoals/action primitives?\n\n2. **On novelty and positioning:** What specific technical contributions differentiate this work from prior LLM/VLM planning and future-prediction pipelines? Beyond integration, are there new algorithms, training objectives, or guarantees? A related-work table mapping differences would help.\n\n3. **On imitation learning:** Can you include IL baselines and/or an IL variant of your method? If not, please justify why RL is necessary here and report how many demonstrations or episodes would be needed to match RL performance.\n\n4. **On real-world validation:** Can you expand physical-robot evaluation (more tasks, repetitions, failure breakdowns) and report success metrics with confidence intervals? What are the dominant real-world failure modes (perception vs. planning vs. control)?\n\n5. **On scalability and cost:** What is the training/inference cost and data requirement of each module, and how do these scale with task count and horizon length? Any evidence that the architecture remains tractable and reliable when moving to a large multi-task setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "78nrMXZ8m7", "forum": "JLZYYvJwh4", "replyto": "JLZYYvJwh4", "signatures": ["ICLR.cc/2026/Conference/Submission6012/Reviewer_dB6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6012/Reviewer_dB6F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889357575, "cdate": 1761889357575, "tmdate": 1762918414243, "mdate": 1762918414243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a non end-to-end pipeline for long-horizon robotic experimentation.\n\nSpecifically, the framework first uses a large language model (LLM) to decompose a high-level experimental goal into a sequence of symbolic subtasks {s}, then employs a multimodal language model (MLM) to further break down each subtask into fine-grained action primitives. Finally, a reinforcement learning (RL) controller is used to train and execute the corresponding physical actions.\n\nThe paper also introduces a simple benchmark and training dataset containing basic chemical operations (e.g., pouring, stirring, mixing).\nExperimental results show that the proposed pipeline, CoMP, achieves state-of-the-art performance on both the proposed benchmark and public datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear motivation: the authors convincingly argue that end-to-end approaches fail to generalize in long-horizon scenarios (due to catastrophic forgetting), and therefore adopt a hierarchical decomposition strategy (LLM -> MLM -> RL). The logic of this design is coherent and well-motivated.\n\n* Dataset contribution: the paper provides a compact but useful benchmark simulating chemical laboratory operations.\n\n* Strong results: CoMP achieves SOTA performance across evaluated benchmarks."}, "weaknesses": {"value": "* In the public benchmark (Table 4), CoMP performs poorly on the \"pick cup\" task. This is surprising since a long-horizon planner should, in theory, handle short-horizon tasks more easily, i.e., it shouldn't hurt the original capability of short-horizon tasks.\n\n* The dataset is overly simple, covering only a limited set of basic chemical actions.\n\n* It is unclear how fine-grained CoMP’s control actually is, i.e., in the “pour” task, does the system control liquid volume (e.g., in milliliters), or is the simulation limited to a symbolic pouring gesture without real fluid modeling?\n\n* The RL training procedure is insufficiently described: is the controller trained from scratch, or initialized via imitation learning?"}, "questions": {"value": "The questions are included in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7cUHhrZqP7", "forum": "JLZYYvJwh4", "replyto": "JLZYYvJwh4", "signatures": ["ICLR.cc/2026/Conference/Submission6012/Reviewer_LRa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6012/Reviewer_LRa6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998882201, "cdate": 1761998882201, "tmdate": 1762918413949, "mdate": 1762918413949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper presents CoMP, a compositional and decoupled framework for long-horizon robotic planning in scientific experiments.\n\n- CoMP combines task-level CoT decomposition, multimodal visual prediction, and RL-based control for robotic planning and control.\n\n- This paper also introduces a benchmark dataset for scientific experiment tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is sound to employ a modular approach rather than an end-to-end method for long-horizon robotic manipulation in laboratory environments.\n\n- Focusing on autonomous experimental systems for science is highly insightful."}, "weaknesses": {"value": "- A primary concern is the potential for the entire system to be overly complex, redundant, and time-consuming. Why did the authors choose not to leverage a single powerful Vision-Language Model (VLM), such as GPT-4o or Gemini, to handle both task-level planning and mid-level planning concurrently?\n\n- Is the visual prediction module essential for planning? For a fairer comparison in Table 3, comparing MLM (LLaMA3.2)+RL and MLM+RL is insufficient due to the different base MLLMs used. The authors should have conducted an ablation using MLM (GPT-4o, without vision input) + RL as a baseline. Furthermore, I argue that visual prediction may not be strictly necessary for sub-task planning, as similar grounding could potentially be achieved by employing highly detailed text prompts.\n\n- The visual prediction module necessitates the use of expert demonstration samples. Therefore, the authors' claim of operating \"without trajectory-level supervision\" is arguably misleading. Although action information is not explicitly used, the cost of acquiring the required demonstration data is comparable to that of standard Imitation Learning (IL) methods.\n\n- The comparison against several IL based works may not be entirely fair, as those baselines do not require a separate visual prediction module. What would be the performance outcome if the authors trained the IL policy models using the exact same demonstration data?"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EQHJ2gVoKO", "forum": "JLZYYvJwh4", "replyto": "JLZYYvJwh4", "signatures": ["ICLR.cc/2026/Conference/Submission6012/Reviewer_wT92"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6012/Reviewer_wT92"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762166357110, "cdate": 1762166357110, "tmdate": 1762918413712, "mdate": 1762918413712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}