{"id": "vGk4D0fUzv", "number": 16270, "cdate": 1758262548425, "mdate": 1759897251019, "content": {"title": "GraphShield: Graph-Theoretic Modeling of Network-Level Dynamics for Robust Jailbreak Detection", "abstract": "Large language models (LLMs) are increasingly deployed in real-world applications but remain highly vulnerable to jailbreak prompts that bypass safety guardrails and elicit harmful outputs. We propose GraphShield, a graph-theoretic jailbreak detector that models information routing inside the LLM as token--layer graphs. Unlike prior defenses that rely on surface cues or costly gradient signals, GraphShield captures network-level dynamics in a lightweight and model-agnostic way by extracting multi-scale structural and semantic features that reveal jailbreak signatures. Extensive experiments on LLaMA-2-7B-Chat and Vicuna-7B-v1.5 show that GraphShield reduces attack success rates to 1.9% and 7.8%, respectively, while keeping refusal rates on benign prompts at 7.1% and 6.8%, significantly improving the robustness–utility trade-off compared to strong baselines. These results demonstrate that graph-theoretic modeling of network-level dynamics provides a principled and effective framework for robust jailbreak detection in LLMs.", "tldr": "", "keywords": ["Jailbreak Detection", "Graph-Based Features", "Large Language Models (LLMs)", "Safety and Robustness in LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92fbbe74a462b1054ce4982a965f3fcca95b3a98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel way of defending against jailbreaks using internal LLM signals, that don't just look at token-level signals (like linear probes), but also look at the flow of information. I think the idea is very interesting and nice. However, I have concerns about the exposition, framing, baselines, and strength of results. I lean towards rejection due to the concerns, but I expect I could change my mind."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea is very interesting and as far as I know, novel. My intuitive understanding (which could be wrong) is basically not looking at just a probe, but understanding more of a sense of how information is being used in the network. This is an interesting idea. I think it would be really great to have a paragraph in the introduction explaining this at a high level."}, "weaknesses": {"value": "- **Improve the exposition and writing**. In general, I think the exposition of the paper could be greatly improved.\n    - For a machine learning reader that is less familiar with neuroscience, I recommend framing more and adding more explanatory information. e.g., \"jailbreak behavior in LLMs is best understood as an emergent property of information routing rather than an isolated token- or activation level phenomenon.\". What does this mean, precisely? I understood this eventually, but adding figures in the introductoin would be helpful. I think Figure 1 is fairly good—adding it earlier would be good.\n    - Introduction claims \"prior attempts rely on surface level signals.\" I don't think this is quite true—in what way do classifiers rely on surface level signals? The concern there is they are \"tied to training taxonomies\"—what jailbreak defense isn't? This framing seems to be off to me. What is exactly the limitation of a training taxonomy? In fact, it can be helpful—e.g., Sharma et al. (2025) Constitutional Classifiers paper shows how a constitution can be used to train. I'd also suggest citing this paper.\n    - On page 4, where the maths is introduced, I suggest adding more exposition and in general explaining what is happening.\n    - Adding an extra figure on page 4 to explain the different maths terms would also help, and understand the sparsified graph. I think you have this graph later.\n    - On page 5, explain the motivation behind different features for the graph, and ideally in some easier to understand way. I would guess that jailbreaks break the refusal edge connectivity, so that etiher the refusal never comes up, or it comes up and it it isn't routed to the output. Where is that captured here?\n    - On page 7, adding some bold and some flow going through the results would improve how easily the results are understood.\n- Judging procedure\n    - I don't understand why you use a keyword pattern filter? I would prefer something stronger, like the rubric approach used in the StrongReject paper.\n- Easier comparison\n    - It would be easier to compare results in Table 1 if we fixed the thresholds to have a fixed benign refusal rate.\n- Baseline\n    - I'd like to see a baseline of a simple linear probe to see the benefit of using connectivity.\n- I'm concerned about the strength of defences, given that decipher attacks and some adaptive attacks (without adaptive defences) seem to beat the method. In fact, given this, this suggests that these defences, as they are, might simply not be that robust. I think the issue lies with the use of anchor tokens that are pain english (see questions below). So I am concerned about this."}, "questions": {"value": "- I want to understand use of anchor tokens—it seems like you're probing for indication of refusal. But I don't fully get it. \n- And it's not clear whether the anchor tokens generalize e.g., across languages? I'd like to see a simple experiment where we try to break the use of anchor tokens by translating text into another language. Indeed, I think the poor performance of Cipher might be due to this (i.e., the relevant token similarities are very different, so we get poor generalization)\n- Did you consider alternative approaches for anchor token? E.g., you could find refusal vectors at each layer (more like training a linear probe). This might help with the above issues.\n- I'd love to understand the edge weights better. Are we looking at the edge weights layer to layer (i.e., understanding if information related to the anchor tokens progagates?)\n- I don't follow why the ASR is so different for Llama-2 and Vicuna for e.g., Llama guard in Table 1? Llama guard is an external classifier, so it should classifier the same? Or is the idea that vicuna has a lower refusal rate in general?\n- In figure 3, I am confused how the last layer tokens for criminals have no edges but high node scores. I thought the high node score indicated that the token affects the output. However, the lack of edges might suggest it doesn't affect the output that much?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IwDjAVdyUt", "forum": "vGk4D0fUzv", "replyto": "vGk4D0fUzv", "signatures": ["ICLR.cc/2026/Conference/Submission16270/Reviewer_fKnw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16270/Reviewer_fKnw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761349895037, "cdate": 1761349895037, "tmdate": 1762926419797, "mdate": 1762926419797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GraphShield, a novel neuroscience inspired approach to detecting jailbreak attacks by analyzing graph-theoretic network level semantics instead of shallow surface level indicators, unlike past work. Their method leads to a light-weight, model-agnostic classifier that achieves the best trade-off on defense and preserving utility (responding to benign prompts) when compared to gradient-based, hidden-state based, perplexity/pattern based defenses etc. and also works across two model families. They also perform crucial ablations on the importance of the chosen set of features, showing their importance for maximizing true positives while keeping a low false positive rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Proposes a novel, neuroscience-inspired, network-level approach for jailbreak detection that is lightweight, needing only a single forward pass and simple classifiers. Also, compared to past methods, it relies on network-level semantics instead of focusing on single-point, localized, or surface-level indicators of semantics (e.g., hidden states, specific patterns or tokens, or specific training taxonomies).\n2. Outperforms prior pattern-based, gradient-based, hidden-state-based, and multi-pass methods that rely on gradients, taxonomies, model-specific alignment, or multiple passes.\n3. The method is model-agnostic and requires no fine-tuning.\n4. Method achieves a good trade-off between benign refusal rate (BRR), a measure of utility, and attack success rate (ASR), a measure of how many harmful prompts succeed in eliciting a harmful response. It has a favorable trade-off between harmlessness and harmfulness.\n5. The two chosen models demonstrate a huge reduction in ASR through their defense, and they also achieve the best or second-best reduction in ASR, and the most favorable trade-off between ASR and BRR."}, "weaknesses": {"value": "1. **Limited generalizability across attack types:** The leave-one-out attack ablations show that their detection method struggles across rare or obfuscated attacks (e.g., Deciper). While the authors claim that this could be mitigated by ensuring diverse attack types are present in the training distribution by leveraging synthetic jailbreaks, this seems hard to achieve in practice since new attacks keep emerging all the time, and this would require something like an active learning setup with their detector. **Also, this means their method has the same limitation as the classifier-based methods since they effectively require a taxonomy of attacks to ensure diverse attack types are covered.**\n2. Do the network-level patterns associated with harmfulness for a given set of harmful scenarios match the patterns in new scenarios with similar kinds/definitions of harm but completely different settings? I recommend using something like WildJailBreak [1] or InjecAgent [2] for different settings. I also have some general concerns about the small size of the test set (only 36 prompts/scenarios per attack family) despite the claim of low variance found in their bootstrap-style testing. I would be more reassured about the robustness of their method if these suggested evaluations were performed.\n3. **Concerns about generalizability across model scales:** While the paper does a good job of choosing models from different families (Vicuna and Llama), my main concern is that both are 7B models. Inverse scaling is a well-known trend in the alignment literature [3] that suggests that models of varying scales exhibit significantly different behavior to harmful prompts. My intuition is that this would lead to the emergence of different and potentially more complex patterns across model scales. While the authors’ intent is to target larger and closed-source models in future work, I would highly recommend having at least 1 open-source model in the 32B-70B parameter range in their experiments for this paper as a way to show that their detector can capture jailbreak-related patterns for larger models. \n\n[1] Jiang, Liwei, et al. \"Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models.\" Advances in Neural Information Processing Systems 37 (2024): 47094-47165.  \n[2] Zhan, Qiusi, et al. \"Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents.\" arXiv preprint arXiv:2403.02691 (2024).  \n[3] McKenzie, Ian R., et al. \"Inverse scaling: When bigger isn't better.\" arXiv preprint arXiv:2306.09479 (2023)."}, "questions": {"value": "1. Why do you sample 120 instances from JailbreakBench instead of using the whole dataset?\n2. Since you are using JBB-Behaviors (https://huggingface.co/datasets/JailbreakBench/JBB-Behaviors), which already has 100 harmful and benign behaviors, why do you source the benign prompts from the AlpacaEval dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AUq7MFIXP1", "forum": "vGk4D0fUzv", "replyto": "vGk4D0fUzv", "signatures": ["ICLR.cc/2026/Conference/Submission16270/Reviewer_VHh2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16270/Reviewer_VHh2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761499989291, "cdate": 1761499989291, "tmdate": 1762926419259, "mdate": 1762926419259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GraphShield, a jailbreak detection method that constructs token–layer graphs from model hidden states and attentions, tracks semantic routing around selected anchor tokens, and classifies extracted graph features with an RBF-SVM to block harmful prompts before generation. Evaluated on LLaMA-2-7B-Chat and Vicuna-7B-v1.5 across seven jailbreak attack families and a benign set, it reports low attack success rates with modest benign refusal rates, demonstrates speed advantages over multi-pass defenses, and analyzes performance under unseen and adaptive attack scenarios using a mixed heuristic-plus-human evaluation protocol."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a novel framing of jailbreak detection through network-level routing using token–layer graphs, supported by principled rollout, sparsification, clear mathematical grounding, and reproducible pseudocode. The writing is clear, detailed, and easy to follow throughout.\n* GraphShield achieves a strong utility–robustness trade-off, obtaining ASR and BSR that are competitive with or outperforming prior defenses.\n* The experiments include clear and extensive ablations, where most design choicesare validated.\n* The method has favorable runtime, leveraging single-pass feature reuse to operate faster than multi-generation defenses which takes an order of magnitude longer.\n* The paper is transparent about limitations, explicitly reporting seen vs. leave-one-out and adaptive failures, and providing comprehensive appendices and baseline implementation details.\n* The evaluation includes a broad selection of jailbreak attack families and defense baselines, contributing to a thorough comparative analysis."}, "weaknesses": {"value": "1. The model-agnosticity claim would be more convincing with experiments on a broader set of models, including additional architecture families such as Qwen.\n2. Although the paper provides ablations on the selected anchor words, the reasoning behind their specific choice is not fully explained. It would be helpful to evaluate other commonly used refusal-related tokens (e.g., “sorry,” “as an AI…”) as potential anchors.\n3. Incorporating more diverse benign datasets could further strengthen generalization and provide a more reliable estimate of false refusals in real-world usage.\n4. Some hyperparameter settings, such as those described in Appendix D, lack justification. Ablations or sensitivity analyses on these choices would improve clarity and reproducibility.\n5. While the keyword-based refusal heuristic shows over 90% agreement with humans, evaluating the approach with an LLM-based judge would provide an additional perspective on labeling quality and robustness.\n\n***Minor remark:***\n1. The numerical results reported in the abstract would be more meaningful if accompanied by a brief comparison point or baseline reference to contextualize their significance."}, "questions": {"value": "1. As Figure 1 indicates clearly lower performance on Decipher and JOOD attacks, I would like to see the ASR of each baseline defense specifically on these two attack families to better understand how GraphShield compares at.\n2. In the unseen attack setting, performance drops noticeably for some attack families. So, could you provide an analysis of how performance scales with the number of training examples for these attack types, for example by reporting ASR/TPR as a function of sample size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h4A395bePx", "forum": "vGk4D0fUzv", "replyto": "vGk4D0fUzv", "signatures": ["ICLR.cc/2026/Conference/Submission16270/Reviewer_p9ne"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16270/Reviewer_p9ne"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858823670, "cdate": 1761858823670, "tmdate": 1762926418860, "mdate": 1762926418860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GraphShield, a graph-theoretic framework for detecting jailbreak prompts in large language models. Its main idea is to construct token-layer graphs from hidden states and attention weights, use anchor tokens as probes for refusal-related semantics, extract multi-scale structural and semantic features from these graphs, and use them within a lightweight classifier to predict jailbreak attempts. Experiments on LLaMA-2-7B-Chat and Vicuna-7B-v1.5 demonstrate lower attack success rates and strong benign refusal performance, achieving a better robustness-utility balance than previous defenses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a graph-theoretic approach to model information flow within LLMs, moving beyond surface-level or gradient-based detectors. This shift captures emergent routing patterns in jailbreak detection.\n2. GraphShield achieves higher F1 and TPR at low FPR compared with baselines such as PromptGuard and PPL-based filters. It conducts detailed ablations on anchor tokens and feature groups to pinpoint which components most influence detection performance.\n3. The token-layer graph visualizations clearly illustrate how refusal semantics propagate, offering intuitive explanations for detection outcomes."}, "weaknesses": {"value": "1. Reliance on Supervised Classification: The method depends on a supervised SVM trained on labeled benign and attack prompts. Its performance may degrade as new jailbreak strategies or paraphrased anchors emerge, requiring retraining to maintain coverage.\n2. Anchor Token Dependency: GraphShield is highly vulnerable to adaptive attacks that suppress or paaraphrase its fixed anchor tokens (“can”, “cannot”, “help”, “I”, “else”). Although retraining with such samples restores robustness, this reliance on continuous adversarial updates poses long-term scalability and maintenance challenges.\n3. GraphShield’s pipeline remains intricate, involving multi-stage sparsification (permutation nulls, top-k pruning) and extensive feature extraction across structural, community, and centrality metrics for each anchor. Despite being described as lightweight, the resulting feature space is opaque. The link between specific feature groups and jailbreak likelihood is not straightforward. This black-box nature may limit trust and adoption in safety-critical contexts that require transparent post-hoc auditing.\n4. Although the paper includes ablations on feature groups and anchor tokens, it lacks direct comparisons to alternative graph formulations, such as sequence-level attention graphs without anchor conditioning, or to simpler network-informed but non-graph-based features. Consequently, it remains unclear whether the specific anchor-probe graph design is essential for the reported performance gains."}, "questions": {"value": "1. Since adaptive attacks target the anchor tokens (“can”, “cannot”, “help”, “I”, “else”), have the authors explored dynamic or learned anchor selection rather than fixed tokens?\n2. How well does GraphShield generalize to unseen jailbreak types or paraphrasing styles without retraining?\n3. Refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pU8ip8OOJA", "forum": "vGk4D0fUzv", "replyto": "vGk4D0fUzv", "signatures": ["ICLR.cc/2026/Conference/Submission16270/Reviewer_ZSEn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16270/Reviewer_ZSEn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985569905, "cdate": 1761985569905, "tmdate": 1762926418260, "mdate": 1762926418260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}