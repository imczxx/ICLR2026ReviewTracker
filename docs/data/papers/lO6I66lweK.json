{"id": "lO6I66lweK", "number": 10905, "cdate": 1758184502063, "mdate": 1763637333370, "content": {"title": "Taming Hierarchical Image Coding Optimization: A Spectral Regularization Perspective", "abstract": "Hierarchical coding offers distinct advantages for learned image compression by capturing multi-scale representations to support scale-wise modeling and enable flexible quality scalability, making it a promising alternative to single-scale models. However, its practical performance remains limited. Through spectral analysis of training dynamics, we reveal that existing hierarchical image coding approaches suffer from cross-scale energy dispersion and spectral aliasing, resulting in optimization inefficiency and performance bottlenecks. To address this, we propose explicit spectral regularization schemes for hierarchical image coding, consisting of (i) intra-scale frequency regularization, which encourages a smooth low‑to‑high frequency buildup as scales increase, and (ii) inter-scale similarity regularization, which suppresses spectral aliasing across scales. Both regularizers are applied only during training and impose no overhead at inference. Extensive experiments demonstrate that our method accelerates the training of the vanilla model by 2.3$\\times$, delivers an average 20.65\\% rate–distortion gain over the latest VTM-22.0 on public datasets, and outperforms existing single-scale approaches, thereby setting a new state of the art in learned image compression.", "tldr": "", "keywords": ["Learned Image Compression", "Hierarchical Variational Autoencoders", "Spectral Regularization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2590ae2b3c6a6bfad2e8514d27132eb5ec9b7ce4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "They propose explicit spectral regularization (intra-scale frequency regularization for smooth frequency buildup across scales, inter-scale similarity regularization to suppress cross-scale aliasing) — applied only in training with no inference overhead. Experiments show the method accelerates vanilla model training by 2.3x, achieves 20.65% average rate–distortion gain over VTM-22.0 on public datasets, outperforms single-scale approaches, and sets a new SOTA in learned image compression."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This manuscript tries to extract disentangled features from images using spectral regularization.\n\n2. The motivation is clear.\n\n3. The results about the performance of the proposed method are convincing in Kodak, CLIC datasets."}, "weaknesses": {"value": "1. The related details of the proposed method are not described clearly. For example, how to obtain the y-axis of Figure 1 ? What does it mean about the lines with different color of Figure 8?\n\n2. The training of inter-scale regularization as shown in Figure 5 may not be stable since $z_1, z_2$ maybe the noise at the early stage and the corresponding regularization makes no sense.\n\n3. There exists no analysis or discussion about \"Different scales converge to their respective frequency bands at different rates\" at line 183. It could be better if this manuscript provides some clues.\n\n4. For visualization in Figure 6, it is hard to totally distinguish the frequencies of the details without any aliasing. Besides the demonstration of Figure 6(b) has little color shift compared with 6(c), can the regularization be more effective for color representation?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oQy8bdBSnx", "forum": "lO6I66lweK", "replyto": "lO6I66lweK", "signatures": ["ICLR.cc/2026/Conference/Submission10905/Reviewer_RfuU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10905/Reviewer_RfuU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291119439, "cdate": 1761291119439, "tmdate": 1762922109603, "mdate": 1762922109603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall response to all reviewers"}, "comment": {"value": "We thank all reviewers for their constructive reviews and recognition of our spectral-analysis idea (**NjpX,QW9U,HdPe,RfuU**), state-of-the-art performance (**NjpX,QW9U,RfuU**), and sufficient experiments (**NjpX,QW9U,HdPe,RfuU**). In response to their comments, we have added relative explanations, experiments, and analyses, and updated the revisited manuscript accordingly (**changes are marked in blue**). Concretely, we have:\n\n1. **Provided further explanation of our method’s mechanism and justification**, especially clarifying how the inter-scale regularization alleviates spectral aliasing (Sec. 3.3 and Appendix A.1).\n2. **Added the suggested ablations and analyses**. We appreciate that these suggestions are insightful and believe they can assist in further improving clarity. Also, we emphasize that our manuscript focuses on spectral analysis and regularized training for hierarchical coding  structures; the added ablations are targeted adjustments around the network structure and do not change our main contributions and conclusions.\n\n   a. **Clarified the role and necessity of the FSP module**. FSP acts like a skip-connection module between different scales in our hierarchical coding  design, which intends to aid training. However, it is not intrinsic to the proposed regularization. Additional experiments show that the small BD-rate gain (≈0.68%) attributable to FSP originates from more thorough finetuning under a correct learning-rate scheduler setup; removing FSP has negligible effect on final performance and training speed. Hence, we will add another version of network design without the FSP module as reference in the revisited version. After peer review, we will then update relative results in the final paper.\n\n   b. **Quantified the effect of the re-parameterized block design**. We emphasize that the baseline networks (without our spectral regularization) also used re-parameterization. This is related to the design of the basic model, which provides good basic performance. Our proposed regularization method further achieves performance improvement.\n3. **Explored integrating our regularization into single-scale architectures with various context modeling (e.g., ELIC, MLIC++, HPCM)**. Although our regularization schemes are designed for hierarchical architectures, we still included preliminary experiments to examine applicability to single-scale designs. From both theory and experiments we explain why naively applying our regularization to single-scale models is inappropriate: unlike hierarchical models—where multiscale latents naturally emerge from input transformations—single-scale models produce a single-scale latent that lacks explicit frequency separation. Without architectural guidance or frequency-aware constraints, the single-scale latent cannot be reliably decomposed into multi-frequency components, and blindly introducing our regularizers may disrupt the original context-modeling design.\n4. **Revisited and refined various statements, figures and formulas, and corrected several minor typos to improve readability.**\n\nAdditional explanations and results have been incorporated in the responses to each reviewers. After peer review, these revisions and results will be incorporated into the final paper."}}, "id": "gDttp9G5xG", "forum": "lO6I66lweK", "replyto": "lO6I66lweK", "signatures": ["ICLR.cc/2026/Conference/Submission10905/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10905/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10905/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763634872581, "cdate": 1763634872581, "tmdate": 1763634872581, "mdate": 1763634872581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed two techniques to address the inefficiency in training in hierarchical coding. The first is for intra-scale frequency regularization, which is to guide the training frequencies by progressive truncation. The second for inter-scale similarity regularization, which is to suppress the similarity between neighboring scales (?). Experimental results demonstrate the successfulness of the proposed method.\n\nFor the weakness, 1. The inter-scale similarity regularization part needs more explanation. It looks like that Eq.(6) encourages similarity instead of suppressing it. 2. Some figures are not helpful for explaining the method, e.g. Figs 4 and 5."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors proposed two techniques to address the inefficiency in training in hierarchical coding. The first is for intra-scale frequency regularization, which is to guide the training frequencies by progressive truncation. The second for inter-scale similarity regularization, which is to suppress the similarity between neighboring scales (?). Experimental results demonstrate the successfulness of the proposed method."}, "weaknesses": {"value": "1. The inter-scale similarity regularization part needs more explanation. It looks like that Eq.(6) encourages similarity instead of suppressing it. \n2. Some figures are not helpful for explaining the method, e.g. Figs 4 and 5."}, "questions": {"value": "1.\tThe inter-scale similarity regularization part needs more explanation. It looks like that Eq.(6) encourages similarity instead of suppressing it.\n2.\tSome figures are not helpful for explaining the method, e.g. Figs 4 and 5. \n3.\tWhat is the meaning of “time t”?\n4.\tThe inner minimum in Eq.(4) seems not necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Df69jLUotd", "forum": "lO6I66lweK", "replyto": "lO6I66lweK", "signatures": ["ICLR.cc/2026/Conference/Submission10905/Reviewer_HdPe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10905/Reviewer_HdPe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749899502, "cdate": 1761749899502, "tmdate": 1762922109073, "mdate": 1762922109073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles optimization bottlenecks in hierarchical image compression by identifying that standard training suffers from \"spectral aliasing\" and \"energy dispersion\" across scales. To resolve this, the authors introduce two training-only spectral regularizers: an intra-scale DCT-based curriculum for progressive low-to-high frequency learning, and an inter-scale similarity penalty to reduce redundancy. This approach accelerates training by 2.3x and sets a new state-of-the-art, achieving a 20.65% average BD-Rate saving over the VTM-22.0 video codec. I like the paper's insightful analysis and strong empirical results. However, its methodological soundness is weakened by a lack of theoretical justification connecting the proposed regularization schemes to their stated goal of spectral separation."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper provides a novel and insightful spectral analysis, identifying \"energy dispersion\" and \"spectral aliasing\" as the root causes of training difficulties in hierarchical compression models. The compelling visualizations strongly support this diagnosis.\n2.The proposed regularization techniques are well-designed and directly target the diagnosed spectral issues. The approach is rigorous, logically sound, and adds no inference overhead, making it practical.\n3.The method achieves state-of-the-art performance with substantial gains, including a 20.65% average BD-Rate saving over VTM-22.0. The 2.3x training acceleration is a significant practical advantage.\n4.The paper is supported by thorough experiments, including extensive SOTA comparisons, detailed ablation studies, and proven generalizability to other architectures, which robustly validate the authors' claims."}, "weaknesses": {"value": "1.The paper's design, guided by the \"frequency principle,\" assigns low-frequency content to semantically deeper scales and high-frequency content to shallower scales. This creates an apparent tension with the common understanding that more challenging, high-frequency details often benefit from the greater expressive power of deeper network layers. While the empirical results are strong, the paper's methodological contribution would be further strengthened by a clearer theoretical justification for this seemingly counter-traditional design, explaining why assigning the most complex information to relatively shallower structures is advantageous in this context.\n\n2.The paper lacks a theoretical basis for its claim that an L2 latent penalty (Eq. 6) enforces spectral separation between scales. The connection between latent distance and spectral orthogonality is not established. Furthermore, the implementation uses a 1x1 convolution, a feature-mixing operation that could potentially contradict the goal of suppressing spectral aliasing. The authors should clarify the true source of this regularizer's effectiveness."}, "questions": {"value": "1.Regarding Figure 1:\n Could you provide a more detailed technical explanation of how these heatmaps were generated? Specifically, how is the \"spectral overlap\" between a scale's contribution and the input image quantitatively defined and computed? The current description in the Appendix is a bit brief for full reproducibility.\n\n2.Regarding Figure 9:\nCould you elaborate on the specific design purpose and function of the FSP?\nWhat is the relationship between the FSP and the proposed Inter-Scale Latent Regularization? Do they work synergistically, or are they independent components?\nI noticed a similar shortcut structure in a recent related work, AuxT [R1]. Could you clarify the key differences and relationships between your FSP and the auxiliary structure in AuxT?\n\n[R1] Li, et. al, On Disentangled Training for Nonlinear Transform in Learned Image Compression. ICLR 2025.\n\n3.Can the proposed spectral regularization methods be adapted for non-hierarchical (single-scale) compression frameworks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EvFxNxXq6s", "forum": "lO6I66lweK", "replyto": "lO6I66lweK", "signatures": ["ICLR.cc/2026/Conference/Submission10905/Reviewer_QW9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10905/Reviewer_QW9U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818143116, "cdate": 1761818143116, "tmdate": 1762922108498, "mdate": 1762922108498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the optimization challenges in hierarchical learned image compression via spectral analysis. The authors identify two major issues—cross-scale energy dispersion and spectral aliasing—and introduce two regularization strategies to mitigate them: (1) intra-scale frequency regularization through progressive DCT truncation, and (2) inter-scale latent regularization using similarity penalties. Experimental results demonstrate strong performance, achieving a 20.65% BD-Rate improvement over VTM-22.0 and a 2.3× training speedup without adding inference complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The spectral interpretation of hierarchical training dynamics provides an intuitive understanding of cross-scale interactions and training instability.\n2. Strong empirical results: 1) State-of-the-art compression performance across multiple datasets, 2) Significant 2.3× training acceleration without inference overhead, 3) Robust performance across diverse resolutions (480p–4K).\n3. The proposed regularizers are training-only and do not increase inference complexity, making the method easy to deploy.\n4. The identification of spectral dispersion and aliasing as key bottlenecks adds valuable understanding to hierarchical compression models."}, "weaknesses": {"value": "1. The inter-scale regularization term minimizes $ L2(z_{l-1}, Conv(DWT(z_l)))$, which encourages similarity between adjacent scales. However, the text claims the objective is to make features “as distant as possible”. This appears contradictory and should be clarified.\nThe paper lacks a formal explanation of how minimizing latent similarity mitigates spectral aliasing. The relationship between spatial-domain similarity and frequency-domain decoupling needs stronger theoretical grounding.\n2. The model diagram includes the FSP component, but it is not sufficiently discussed in the text. Compared to QARV, it remains unclear how much FSP contributes to convergence speed and rate–distortion performance. FSP seems conceptually close to AuxT [1], yet no detailed comparison or ablation is provided, despite citing AuxT.\n3. The reparameterized block structure is mentioned but not thoroughly evaluated and its impact on performance should be quantified.\n4. Hierarchical VAE has been previously applied to image compression (e.g., by Yueyu Hu et al. [2-3] ); this line of work should be properly cited and discussed.\n5. It would be valuable to analyze whether the proposed regularization strategies could be extended to models such as HPCM or MLIC++, where complex context modeling divides single-scale features into multiple slices. Would regularizing these slices be similarly effective?\n\n[1] Li, Han, et al. \"On disentangled training for nonlinear transform in learned image compression.\" arXiv preprint arXiv:2501.13751 (2025).\n\n[2] Hu, Yueyu, et al. \"Learning end-to-end lossy image compression: A benchmark.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.8 (2021): 4194-4211.\n\n[3] Hu, Yueyu, Wenhan Yang, and Jiaying Liu. \"Coarse-to-fine hyper-prior modeling for learned image compression.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 07. 2020."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MN7InDNETR", "forum": "lO6I66lweK", "replyto": "lO6I66lweK", "signatures": ["ICLR.cc/2026/Conference/Submission10905/Reviewer_NjpX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10905/Reviewer_NjpX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922327357, "cdate": 1761922327357, "tmdate": 1762922108158, "mdate": 1762922108158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}