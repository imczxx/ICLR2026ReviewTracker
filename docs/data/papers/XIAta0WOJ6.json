{"id": "XIAta0WOJ6", "number": 367, "cdate": 1756736670833, "mdate": 1759898264852, "content": {"title": "Faster Gradient Methods for Highly-smooth Stochastic Bilevel Optimization", "abstract": "This paper studies the complexity of finding an $\\epsilon$-stationary point for stochastic bilevel optimization when the upper-level problem is nonconvex and the lower-level problem is strongly convex.\nRecent work proposed the first-order method, F${}^2$SA, achieving the $\\tilde{\\mathcal{O}}(\\epsilon^{-6})$ upper complexity bound for first-order smooth problems. This is slower than the optimal $\\Omega(\\epsilon^{-4})$ complexity lower bound in its single-level counterpart. \nIn this work, we show that faster rates are achievable for higher-order smooth problems. We first reformulate F$^2$SA as approximating the hyper-gradient with a forward difference. Based on this observation, we propose a class of methods F${}^2$SA-$p$ that uses $p$th-order finite difference for hyper-gradient approximation and improves the upper bound to $\\tilde{\\mathcal{O}}(p \\epsilon^{-4-2/p})$ for $p$th-order smooth problems. Finally, we demonstrate that the\n$\\Omega(\\epsilon^{-4})$ lower bound\nalso holds for stochastic bilevel problems when the high-order smoothness holds for the lower-level variable, indicating that the upper bound of F${}^2$SA-$p$ is nearly optimal in the highly smooth region $p = \\Omega( \\log \\epsilon^{-1} / \\log \\log \\epsilon^{-1})$.", "tldr": "", "keywords": ["bilevel optimization", "stochastic acceleration"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9e61b903f0af2dfe9967d1f105284b79a0ea2b3.pdf", "supplementary_material": "/attachment/9034a0f896ff576bddaeef7fda4e8a11a99f3b9b.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a conceptually elegant and mathematically rigorous advancement in stochastic bilevel optimization. By linking finite-difference theory with $p$th-order first-order hyper-gradient estimation, it achieves improved convergence without Hessian information. The analysis is solid and the experiments validate both theory and practicality, though assumptions on high-order smoothness and complexity constants limit immediate applicability to all real-world problems. Nonetheless, it represents a notable theoretical milestone toward closing the gap between first-order bilevel optimization and its single-level counterpart."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This paper provides a novel and elegant finite-difference interpretation of first-order bilevel optimization, enabling the F2SA-p method to achieve near-optimal convergence without any Hessian computation. It successfully bridges theory and practice by offering a fully first-order, scalable, and provably faster framework for highly smooth stochastic bilevel problems."}, "weaknesses": {"value": "This paper relies on a new assumption of high-order smoothness in the lower-level variable $y$. The improved convergence rate largely stems from this assumption; however, since the method requires a relatively large $p$, the assumption may be quite strong in practice."}, "questions": {"value": "Could the authors clarify how much Assumption 2.5 (high-order smoothness) contributes to the improvement in the convergence rate? Additionally, could the authors provide justification or empirical evidence regarding the strength or practicality of this assumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jS5it1n9wI", "forum": "XIAta0WOJ6", "replyto": "XIAta0WOJ6", "signatures": ["ICLR.cc/2026/Conference/Submission367/Reviewer_mbBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission367/Reviewer_mbBx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618345959, "cdate": 1761618345959, "tmdate": 1762915504463, "mdate": 1762915504463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the stochastic bilevel optimization problem in which the upper-level objective is nonconvex and the lower-level objective is strongly convex. Building on the first-order F2SA algorithm, which achieves $\\tilde{O}(\\epsilon^{-6})$ stochastic oracle complexity, the authors propose a family of methods F2SA-p that leverage p-th order finite-difference hyper-gradient approximation. They prove that under p-th order smoothness in the lower-level variable, the algorithm attains a tighter complexity bound $\\tilde{O}(p \\kappa^{9+2/p} \\epsilon^{-4 - 2/p})$. Moreover, they establish a matching lower bound $\\Omega(\\epsilon^{-4})$ under similar smoothness assumptions, showing near-optimality when $p = \\Omega(\\log(1/\\epsilon)/\\log\\log(1/\\epsilon))$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The finite-difference modification of F2SA is both novel and conceptually elegant, offering a unified view that also tightens and refines previous analyses.\n2. The paper provides an explicit comparison of smoothness assumptions with related works and includes a thorough and contextualized literature review.\n3. A new lower-bound result demonstrates that the upper bound is nearly optimal in the regime $p = \\Omega(\\log(1/\\epsilon)/\\log\\log(1/\\epsilon))$."}, "weaknesses": {"value": "1. The experiments are limited to a single small-scale dataset, with no evaluation on modern large-scale or deep bilevel tasks (e.g., hyperparameter tuning or meta-learning).\n2. The high-order smoothness assumption on the lower-level variable may be restrictive or difficult to verify in practical ML settings. It would strengthen the paper to discuss or cite works examining when such smoothness assumptions hold in real-world problems."}, "questions": {"value": "1. Regarding the convergence guarantee in Theorem 3.1, is the result stated in expectation or with high probability?\n2. I wonder whether the proposed algorithm can be combined with variance-reduction or momentum techniques to further improve the convergence rate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SOKnUIehPH", "forum": "XIAta0WOJ6", "replyto": "XIAta0WOJ6", "signatures": ["ICLR.cc/2026/Conference/Submission367/Reviewer_pQay"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission367/Reviewer_pQay"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856874298, "cdate": 1761856874298, "tmdate": 1762915504316, "mdate": 1762915504316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors study a stochastic bilevel optimization problem where the upper-level objective is non-convex and the lower-level objective is strongly convex. Instead of relying on Hessian-vector products, they focus on first-order methods that use only gradient information. The key insight is that the state-of-the-art first-order method, $\\textsf{F}^2\\textsf{SA}$, can be viewed as approximating the hypergradient via a forward difference of a univariate function. Building on this idea, the authors employ higher-order finite difference schemes and, under proper smoothness assumptions, establish a complexity bound of $\\tilde{O}(p \\kappa^{9+2/p} \\epsilon^{-4-2/p})$. When $p=1$, their result improves the previous analysis by a factor of $\\kappa$, and when $p = \\Omega(\\log \\epsilon^{-1} / \\log\\log \\epsilon^{-1})$, the complexity improves to $\\tilde{O}(\\kappa^9 \\epsilon^{-4})$, matching the optimal rate for single level problems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The $\\textsf{F}^2\\textsf{SA}$ method was originally proposed as a penalty-based approach. In this paper, the authors offer a novel interpretation by viewing it as a forward difference approximation of the hypergradient. This perspective naturally extends to higher-order finite-difference schemes and provides a clear pathway to improve first-order methods.  \n- The authors also perform a tight analysis to bound the Lipschitz constants of the higher-order derivatives of the perturbed function in terms of those of the original objectives. Under additional smoothness assumptions, they derive a sharper complexity bound than existing first-order methods."}, "weaknesses": {"value": "- **Dimension dependence in Remark 3.1**. The remark states that Lemma 3.1 can be applied to each dimension and thus the overall error can be controlled in Euclidean norm. However, I think a direct coordinate-wise argument would incur an extra $\\sqrt{d}$ factor in the approximation error, which could be problematic. Could you please clarify whether your analysis avoids this factor, e.g., by analyzing the error in different coordinates jointly?\n- **Lower bound in Section 4**. The construction effectively considers a bilevel problem where the upper-level and lower-level objectives are independent, thus reducing to a single-level non-convex problem. While I am not aware of a prior formal statement in exactly this form, the resulting conclusion feels fairly direct.\n- The presentation can be improved, and several points would benefit from additional detail: \n    - In the context of the penalty-based method, $\\nu$ plays the role of the inverse of the penalty parameter $\\lambda$, so I find it somewhat counterintuitive that the algorithm considers negative $\\nu$ when $p$ is even. This also raises the question whether $\\min_{y} g_{\\nu}(x,y)$ can be solved efficiently. My understanding is that taking $\\nu$ sufficiently small preserves smoothness and strong convexity in $y$, so the subproblem can still be solved in a standard way.\n   - On a related note, the relationship between the symmetric penalty problem in (4), the penalty problem in (3), and the original bilevel objective is not immediately clear to me. A brief discussion clarifying their connections will be useful. \n    - Typos:\n         - Page 5, Line 262. The identity $\\frac{\\partial}{\\partial \\nu} \\ell_{\\nu}(x) \\vert_{\\nu=0} = \\phi(x)$ is cited to Chen et al. (2025b), Lemma B.3. However, that lemma appears to bound the gap between the penalized function and the hyperobjective.  \n         - In Lemma 3.1, the coefficients for even $p$ should satisify $\\alpha_j = -\\alpha_{-j}$. Also, for the $p$th-order forward difference estimator, the indices should run $j=0, \\dots, p$. \n         - In Lemma D.2, the SGD update is written as the normalized SGD by mistake."}, "questions": {"value": "Please see the \"Weaknesses\" Section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QQStodqeBl", "forum": "XIAta0WOJ6", "replyto": "XIAta0WOJ6", "signatures": ["ICLR.cc/2026/Conference/Submission367/Reviewer_MZzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission367/Reviewer_MZzi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762227968558, "cdate": 1762227968558, "tmdate": 1762915504110, "mdate": 1762915504110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a variant of the first-order method, F2SA, which uses higher-order central/forward finite difference to approximate the hypergradient. Therefore, the proposed method is called ‌F2SA-p, which uses p points to construct an estimator to the derivative of a unitary function (e.g., p = 1 or 2)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n\n1. The work is ‌well-motivated‌, and provides both upper and lower bound analysis. \n\n2. For ‌pth-order smooth bilevel problems‌, the authors sated that the proposed method achieves an improved lower bound."}, "weaknesses": {"value": "Weakness:\n\n1. What's the difference between the proposed method with F2SA in (Kwon et al., 2024a) and (Chen et al., 2025b)? \n\n2. The ‌problem setup‌ in Section 2 aligns with prior works like Chen et al. (2025b) and Kwon et al. (2024a), with the addition of ‌Assumption 2.5‌ (high-order smoothness).\n\n3. Fully first-order methods are ‌more scalable‌ than Hessian-vector-product-based approaches, particularly for large-scale problems like LLM training. What's the advantage of the proposed method with p= 2 or 3?\n\n4. The experimental results are not convincing. For instance, the authors should report the performance of the proposed method when p= 3 or 4."}, "questions": {"value": "1. What's the difference between the proposed method with F2SA in (Kwon et al., 2024a) and (Chen et al., 2025b)? \n\n2. The ‌problem setup‌ in Section 2 aligns with prior works like Chen et al. (2025b) and Kwon et al. (2024a), with the addition of ‌Assumption 2.5‌ (high-order smoothness).\n\n3. Fully first-order methods are ‌more scalable‌ than Hessian-vector-product-based approaches, particularly for large-scale problems like LLM training. What's the advantage of the proposed method with p= 2 or 3?\n\n4. The experimental results are not convincing. For instance, the authors should report the performance of the proposed method when p= 3 or 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BlARFgVWf2", "forum": "XIAta0WOJ6", "replyto": "XIAta0WOJ6", "signatures": ["ICLR.cc/2026/Conference/Submission367/Reviewer_Rwzc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission367/Reviewer_Rwzc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762230737496, "cdate": 1762230737496, "tmdate": 1762915503914, "mdate": 1762915503914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}