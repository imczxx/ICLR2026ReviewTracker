{"id": "3JOrru3pHG", "number": 1815, "cdate": 1756942315345, "mdate": 1759898184537, "content": {"title": "Informing Reinforcement Learning Agents by Grounding Language to Markov Decision Processes", "abstract": "Natural language advice has the potential to accelerate reinforcement learning, but utilizing diverse and highly detailed forms of language efficiently remains unsolved. Existing methods focus on mapping natural language to individual elements of MDPs such as reward functions or policies, but such approaches limit the scope of language they consider to make such mappings possible. We propose to leverage language advice by translating sentences to a grounded formal language for expressing information about every element of an MDP and its solution, including policies, plans, reward functions, and transition functions. We also introduce a new model-based reinforcement learning algorithm, RLang-Dyna-Q, capable of leveraging all such advice, and demonstrate in two sets of experiments that grounding language to every element of an MDP leads to significant performance gains. In additional symbol-grounding demonstrations we show how vision-language models can annotate important structure in the environment in the form of RLang vocabulary files, eliminating the need for human labels.", "tldr": "", "keywords": ["language advice", "human in the loop", "human interation", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5aad3ce5521332cc018580e7b93af3086eeb8d18.pdf", "supplementary_material": "/attachment/9b9081bd75f983eec1c9219fcf1deb1491d445ec.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents RLang-Dyna-Q, a novel model-based reinforcement learning framework designed to leverage natural language advice. The core contribution is a system that translates human language into a formal, grounded language (RLang) capable of specifying information about all core components of an MDP—policies, rewards, transition functions, and plans. By integrating this structured advice into a Dyna-Q-style algorithm, the agent's learning is significantly accelerated. The authors validate their approach with strong empirical results and further demonstrate a promising method for automating the symbol-grounding process using Vision-Language Models (VLMs), reducing reliance on manual annotation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core concept of grounding language to all MDP elements is original.\n2. The experiments clearly show significant gains in sample efficiency and final performance, validating the framework's effectiveness."}, "weaknesses": {"value": "1.  The paper could benefit from more details on the natural language to RLang translator. Its robustness to varied or ambiguous phrasing is critical, and more information on its architecture, training needs, and failure modes would strengthen the work.\n2. The experiments are conducted in environments with relatively low-dimensional state spaces. A more thorough discussion or experimentation on scaling the grounding approach to high-dimensional inputs (e.g., raw pixels) would be valuable.\n3. While powerful, the RLang formalism might impose a significant cognitive load on human users. The practicality of authoring complex advice by hand could be a limitation and warrants discussion."}, "questions": {"value": "1.  Could you elaborate on the architecture of the natural language-to-RLang translator? What are its data requirements for effective training?\n2.  How does the framework handle incorrect or conflicting advice? Can the agent learn to override flawed advice based on its own environmental experience?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lueVe9phF2", "forum": "3JOrru3pHG", "replyto": "3JOrru3pHG", "signatures": ["ICLR.cc/2026/Conference/Submission1815/Reviewer_ouWK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1815/Reviewer_ouWK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761150549475, "cdate": 1761150549475, "tmdate": 1762915898172, "mdate": 1762915898172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to ground natural language advices into RLang, a formal language for specifying Markov Decision Processes (MDPs). The main hypothesis is that pieces of advice can map to different components of the MDP such as policies, reward functions, transition functions, and plans whereas most existing methods map all advice to a single element. The authors introduce a two-step LLM-based pipeline that (i) classifies each piece of advice according to its most suitable MDP component and (ii) translates it into an executable RLang program. They further present RLang-Dyna-Q, a modified version of Dyna-Q capable of integrating RLang-derived specifications. Experiments are conducted on several environments, and ablations are provided to test the main hypothesis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The central hypothesis that different forms of natural language advices naturally correspond to different MDP components is conceptually sound and practically relevant.\n- The proposed framework provides a novel perspective on how symbolic formal languages like RLang can be integrated with LLMs to inform reinforcement learning."}, "weaknesses": {"value": "- The background section reads more as a related works section than as a formal introduction to notation and core concepts. While RLang is introduced, its explanation is insufficient for readers unfamiliar with it to fully grasp how RLang programs are structured or what components such as “vocabularies” represent.    \n- The main algorithm is not clearly presented. The pseudocode omits explanations for several hyperparameters and integration mechanisms. For example, many symbols (e.g., $N_1, N_2$) are undefined, and their purpose or selection criteria are not discussed.\n- The technical contribution appears modest. Much of the implementation relies on prompting large language models for translation, with limited algorithmic innovation beyond that.\n- Limitations are not discussed properly. Translation via LLMs introduces hallucination and instability. The only mitigation provided consists in restricting vocabulary through prompting, while is helpful it is insufficient to ensure reliable grounding.\n- Experiments are limited to small, discrete environments and are sometimes under-explained. There is no evaluation of translation accuracy, only indirect validation through task performance. Also, the central hypothesis is verified only in two experiments (MidMazeLava, FoodSafety) in the other two ablation is missing (HardMaze) or other baselines surpass it (CouchPotato).\n- The method still depends heavily on hand-specified vocabularies, which raises questions about automation and scalability, even though the authors later explore partial automation using vision-language models."}, "questions": {"value": "- Can you comment on how to extend this approach beyond tabular settings to continuous or real-world domains?\n- How do you ensure safety in a real-world deployement of this method, especially when LLM generated translations may hallucinate?\n- How does the system handle inconsistent or conflicting pieces of advice?\n- Why in HardMaze there is no ablation with other variants?\n\nThe paper presents a promising direction by connecting natural language grounding with the internal structure of MDPs. However, the current implementation and evaluation feels immature. The approach heavily relies on prompting, and several methodological details are missing. Suggested improvements are: a clearer presentation of the algorithm, a comprehensive discussion of limitations, and stronger experimental validation for instance considerin translation accuracy and extension beyond tabular."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "H1MEF2V0dt", "forum": "3JOrru3pHG", "replyto": "3JOrru3pHG", "signatures": ["ICLR.cc/2026/Conference/Submission1815/Reviewer_6awY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1815/Reviewer_6awY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945452031, "cdate": 1761945452031, "tmdate": 1762915898047, "mdate": 1762915898047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a method to provide natural language advice to RL agents by mapping to RLang as an intermediary step. The work also ensures that the the mapping to RLang incorporates all elements of an MDP, differing from prior work which typically incorporate advice for portion of the MDP, like the transition dynamics. The paper the presents two important steps from this, 1) by showing a clear method to incorporate this RLang advice into Dyna-Q which demonstrates its utility, 2) showing that vision-language models can be used to structure the RLang vocabulary files to avoid needing human supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Originality\nThe work is fairly original - it is clearly grounded in the prior work on RLang and the literature on incorporating feedback into pieces of the MDP. However, this is clearly mentioned and ultimately this clarity supports the originality of the work. At the very least I do not believe it should be help against this work. The final experiments connecting vision-language models also supports the originality as it takes on slight step further and incorporates other ideas like foundation models.\n\n## Quality\nThe hypothesis of the work and the problem it aims to solve is clearly stated. Experiments are designed well to directly evaluate the core claims of the work and the results are evaluated fairly.\n\n## Clarity\nOverall the paper is well written, figures are clear and the examples used are chosen well to demonstrate the utility of the proposed method. The paper is also well structured to be a natural read and gradually introduces new idea or addresses lingering concerns (like the need for supervision). I appreciate the effort and clarity of Algorithm 1.\n\n## Significance\nThe work addresses a core and widely applicable problem - how to incorporate guiding language into the RL pipeline. This has implications for research in safety and other key areas of concern beyond just academia."}, "weaknesses": {"value": "## Quality\nI have two primary concerns for quality: 1) the experiments shown in Figures 2, 3 and 4 only compare to Dyna-Q. While this is certainly an obvious baseline but means that the experimental design of the work is ablation which does not compare to different ideas or approaches. It is predictable that the model with the most information will perform the best in this ablation. Especially since the paper does go to great lengths to justify the approach relative to other works, it would have been appropriate to compare to these other works. 2) the need for supervision is acknowledge but it is a bit ask. Additionally, and my bigger concern, the section that aims to counter the need for supervision suggests that a foundation vision-language model is needed. This is not given sufficient consideration as the need for a foundation model to assist with data labelling brings in many important concerns (such as reliability and training costs) which are ignored. While I appreciate the authors aiming to remove the need for human labelling, it is still necessary to fully discuss the implications for using a foundation model as this is at best a partial solution.\n\n## Clarity\nTwo fairly minor things: 1) the bottom of page 3 has a sentence which stop half-way through. The caption of Figure 6 is not rendered properly.\n\n## Significance\nFootnote 1 bothers me slightly as this does not seem trivial to solve and is (to my intuitive understanding) the biggest problem with trying to incorporate natural language into all pieces of an MDP: there is ambiguity in this process and it becomes more difficult to extract the key pieces and represent more text in terms of RLang. So this does limit the significance of the work to me, since how to extend to a case where each piece of advice grounds to multiple types, is not explored."}, "questions": {"value": "How would subsequent work go about addressing the limitation of Footnote 1?\n\nHow robust is the proposed model against noise or errors from the vision-language model labelling?\n\nHow does the RLang Dyna-Q compare to SOTA models for the domains in Figures 2 to 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Mt6jZUoc2n", "forum": "3JOrru3pHG", "replyto": "3JOrru3pHG", "signatures": ["ICLR.cc/2026/Conference/Submission1815/Reviewer_BVMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1815/Reviewer_BVMZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993796659, "cdate": 1761993796659, "tmdate": 1762915897910, "mdate": 1762915897910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper maps natural-language advice into a formal DSL (RLang) that can express multiple MDP components, plans/policies, rewards, and transition effects, and plugs those components into a Dyna-Q–style planner (RLang-Dyna-Q). A two-stage LLM pipeline “grounds” free-form text into executable RLang. A lightweight VLM step helps disambiguate object references. On MiniGrid and VirtualHome tasks, injecting RLang advice improves sample efficiency and final performance versus vanilla Dyna-Q, with ablations showing different gains from plan/policy/effect advice. A small user study suggests non-experts can produce useful advice when guided by the DSL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A clear, unified way to use language beyond rewards or policies alone, covering multiple MDP elements. Simple integration with model-based planning (Dyna-Q) that yields consistent empirical gains. Interpretable advice that can be inspected, edited, and ablated. Practical grounding pipeline (LLM + optional VLM) that reduces manual engineering, and clean experiments that isolate the contribution of plan/policy/effect advice, highlighting when language helps most."}, "weaknesses": {"value": "Q1: The method is built around Dyna-Q/Q-learning. There are no head-to-head comparisons against strong, commonly used algorithms (e.g., PPO, SAC, TD3/A2C). Without modern baselines, external validity is limited.\n\nQ2: While plan/policy/effect are separated, the paper lacks a principled study of when each type helps (task difficulty, horizon, sparsity) and how types interact. There’s no prescriptive guidance for choosing or composing advice across tasks.\n\nQ3: Language grounding relies on fixed in-context prompting. There is no trainable module (fine-tuning, preference optimization, RLHF, self-correction) and no comparison showing when learning-based alignment would outperform ICL.\n\nQ4: The contribution largely extends prior RLang by plugging language-derived components into Dyna-Q planning. The algorithmic novelty feels incremental.\n\nQ5: The paper lacks controlled curves of environment steps vs. performance with matched compute. Model-call counts, planning steps, and replay budgets are not normalized, risking unfair comparisons.\n\nQ6: Evaluations focus on discrete/grid or constrained household tasks. There are no results for continuous control, high-dimensional vision with long horizons/sparse rewards, or cross-task transfer (zero/low-shot).\n\nQ7: Results emphasize success/return but lack analyses of variance/stability (seed sensitivity), exploration efficiency, convergence speed, policy complexity, interpretability, and quantitative “advice dependence” (usage rates, ablations)."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ykDo0qE43", "forum": "3JOrru3pHG", "replyto": "3JOrru3pHG", "signatures": ["ICLR.cc/2026/Conference/Submission1815/Reviewer_pEzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1815/Reviewer_pEzx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040955947, "cdate": 1762040955947, "tmdate": 1762915897712, "mdate": 1762915897712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}