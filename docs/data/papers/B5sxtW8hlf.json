{"id": "B5sxtW8hlf", "number": 17751, "cdate": 1758280142574, "mdate": 1759897156099, "content": {"title": "QuantGen: Parameter Generation for Controllable Model Quantization", "abstract": "Parameter generation has recently emerged as a novel and effective paradigm in the pursuit of efficient AI, offering a fundamentally different perspective from conventional deep learning by directly synthesizing high-quality model parameters. Despite its promise, existing parameter generation methods are typically constrained to producing parameters aligned with the task objectives present in their training data. This limitation significantly restricts their applicability and practical utility across diverse real-world scenarios. In this work, we introduce a dedicated parameter generation method specifically designed for model quantization—a critical step in deploying deep learning models on resource-constrained devices. We propose the first recurrent-based generator capable of directly producing quantized model parameters that retain performance comparable to their full-precision counterparts, without requiring any additional data or retraining. Furthermore, our framework supports controllable quantization, enabling the generation of parameters that satisfy varying precision and deployment requirements. Extensive experiments across multiple datasets and model architectures demonstrate that our method achieves strong generalization and robustness under a wide range of quantization settings. These findings underscore the potential of parameter generation as a powerful and flexible tool for efficient model compression, training, and deployment.", "tldr": "We extend the capabilities of parameter generation by applying it to the domain of model quantization", "keywords": ["parameter generation", "quantization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c48de3dee942b03831b5165c64185ccd3d67afa9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces *QuantGen*, a recurrent parameter generator that directly maps full-precision weights to quantized weights in a single step with claims that it does so without access to target task data. The core design of QuantGen treats weights as tokens, and injects quantization metadata $(S,Z)$ into the token stream, and trains with an MSE loss to regress PTQ/QAT-produced ground truths. The paper shows that the model supports controllable precision and granularity via one-hot condition embeddings. The paper conducts comprehensive experiments on image models, which show modest gains over PTQ with near-QAT results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors formulate *parameter generation* as an interesting alternative to quantization, showing better performance than PTQ and less computational resources than QAT.\n- One-hot condition embeddings for precision and granularity make the generator generalizable, with consistent gains on multiple models.\n- The writing is clear and easy to follow."}, "weaknesses": {"value": "1. The core equations mentioned in the paper are the usual affine quantization and dequantization. The novelty is mainly system-level (tokenization + recurrent generator + simple mse loss) which makes the contribution feel incremental compared to methods to push PTQ/QAT.\n2. To my understanding, the generator must be trained with PTQ/QAT ground-truths per architecture–dataset configuration, as it learns only within that distribution of checkpoints. This design limits the “data-free” and “amortized” efficiency claims since each new architecture or dataset still requires collecting PTQ/QAT checkpoints and retraining the generator from scratch. The method thus behaves more like a model-specific quantization accelerator than a general-purpose quantizer.\n3. As the generator is dependent on PTQ/QAT targets rather than optimizing downstream metrics directly. This likely upper bounds the performance gain in cases where QAT is suboptimal (like in large language models), which makes QuantGen an approximation of PTQ at best."}, "questions": {"value": "1. Instead of regressing to PTQ/QAT parameters, did you try optimizing a proxy like layer-wise calibration loss or output-space MSE directly from fp-32 to quantized (still data-free via synthetic stats)? If not, what blocked it?\n2. Is the generator trained and reused across different architectures and datasets, or does it have to be retrained with PTQ/QAT ground truths for each new model architecture + dataset pair? If retraining is required, can you quantify this cost and clarify how it affects the claimed amortization and scalability benefits? What exactly is shared and what is retrained?\n3. Is it possible to remove the QAT dependency by training the generator only from PTQ (or data-free) teacher tokens and still match QAT accuracy? That would strengthen the “no data” claim.\n4. As it is now, the main advantage of QuantGen seems to come from the ability to quantize large numbers of the *same* model architecture on a *single* task/domain. What are some practical use-cases of this approach, which cannot be mitigated by PTQ/QAT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iVAkN9ddTg", "forum": "B5sxtW8hlf", "replyto": "B5sxtW8hlf", "signatures": ["ICLR.cc/2026/Conference/Submission17751/Reviewer_rtmi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17751/Reviewer_rtmi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761275534886, "cdate": 1761275534886, "tmdate": 1762927592981, "mdate": 1762927592981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient method for parameter generation in model quantization. It builds upon recent work, RPG, which combines a recurrent neural network learning approach with a diffusion model. The paper addresses the challenges of current quantization methods by proposing a conditionally controllable and flexible process. The paper focuses on the recurrent neural network part of RPG, i.e. using Mamba for generating quantized parameter tokens. This approach avoids the diffusion-based model, resulting in a more efficient method. The output of the recurrent model is then compared to the target parameter tokens, which are quantized by current quantization techniques, post-training quantization (PTQ) and quantization-aware training (QAT). The proposed method is evaluated on CIFAR-10/100 and ImageNet-1k using two different transformer models, VIT and MLP-Mixer. The comparison to PTQ and QAT demonstrates improvements in most quantization rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents good motivation why model quantization is important \n- This paper presents an important approach of parameter generation, elevating efficiency and more generalizability for actual applications \n- The paper also shows that including the additional overhead of a diffusion model is not needed to generate strong parameters"}, "weaknesses": {"value": "- This approach is somehow limited in novelty because Mamba is already known in the context of parameter generation. \n- While the topic is important, it relies on a generated dataset (here 50 checkpoints) of fully trained networks and their quantized counterparts. This raises the question of its applicability to new networks. \n- The claim that the performance in line 345 is superior feels a bit too strong here.\n- How do the generated parameters differ from the trained ones? This comparison helps assess the method’s ability to generate novel parameters rather than memorising the trained ones.\n- Diffusion models are primarily used to generate a diverse range of data points, so it’s concerning that it fails in this context. Any explanation for the model’s failure in Table 5 should be provided. \n- The training data is based on the same network topology but trained differently. This suggests that each network topology is handled individually, which seems contradictory to the claim that the proposed method is generalisable to a wide range of networks in lines 096-097.\n\n*Minor*: \n- Text in figure 3 is too small"}, "questions": {"value": "- The difference between PTQ and Gen is quite small, so what is the computational difference? For Gen, one first needs to generate a complete dataset to also perform post-training quantization. What is the trade-off in terms of compute? \n- Is it correct, that at inference time the model uses full-precision networks and aims to generate quanzited versions, i.e. the overall goal is to quantize already trained networks? \n- Could we generate quantized parameters for untrained networks?\n- So far, the paper has shown experiments on two different networks individually. How about generating parameters for unseen, novel networks? Alternatively, could we train on MLP-Mixer and generate parameters for VIT?\n- Is the simulated quantizations also a post-processing step to evaluate the performance of generated parameters? What is the overhead involved for that post-processing step then (Line 335ff)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MIrHKIFMyY", "forum": "B5sxtW8hlf", "replyto": "B5sxtW8hlf", "signatures": ["ICLR.cc/2026/Conference/Submission17751/Reviewer_rmco"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17751/Reviewer_rmco"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824640143, "cdate": 1761824640143, "tmdate": 1762927592092, "mdate": 1762927592092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes QuantGen, a recurrent-based framework for generating quantized model parameters directly from full-precision counterparts, bypassing traditional quantization pipelines that require retraining or calibration data. The method emphasizes controllability across precision levels and quantization schemes, while maintaining performance parity with conventional techniques. Experiments validate its efficiency, generalization, and privacy-preserving advantages."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework supports dynamic adjustments to precision (e.g., 8-bit to 3-bit) and quantization granularity (channel vs. group), validated through conditional generation experiments (Table 3). This aligns with real-world deployment needs for hardware compatibility. \n\n\n2. Experiments across ImageNet, CIFAR datasets demonstrate competitive performance. For example, on ImageNet-1K with ViT-Tiny, QuantGen achieves 75.73% accuracy at 8-bit precision, marginally outperforming QAT (75.61%)."}, "weaknesses": {"value": "1. The proposed method builds upon conventional quantization models but incorporates additional data processing and training steps. However, it only achieves marginal improvement in accuracy over standard quantization techniques. What distinct advantages does this method offer compared to existing quantization approaches?\n\n2. The method’s tokenization strategy (e.g., token length 16,384 for ViT-Base) may struggle with larger architectures like ViT-Large or modern LLMs. The computational trade-offs for billion-parameter models remain unaddressed.\n\n3. Evaluations focus on classification and detection, but segmentation results (e.g., UperNet on ADE20K) show inconsistent gains (Table 8). The generalizability to generative tasks (e.g., diffusion models) is untested."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s1Cmfkulxg", "forum": "B5sxtW8hlf", "replyto": "B5sxtW8hlf", "signatures": ["ICLR.cc/2026/Conference/Submission17751/Reviewer_EscZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17751/Reviewer_EscZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903083783, "cdate": 1761903083783, "tmdate": 1762927591414, "mdate": 1762927591414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on model quantization problems and proposes a new framework for data-free and direct weight generation. The proposed method outperforms QAT in classification tasks and other tasks (in appendix)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written, with benchmarks with classification tasks and other tasks (in appendix). The proposed approach is fast and easy to use in practice since it does not require any data and retraining."}, "weaknesses": {"value": "In this paper, QUANTGEN is compared to QAT with (a kind of limited) improvement. It would be better to expand the experimental section to induce more comparisons, which might include some recent advanced QAT result like https://arxiv.org/pdf/2407.11062\n\nConsidering the novelty and contribution, I lean towards rejection."}, "questions": {"value": "1. Note that QUANTGEN fails or degrades in low-bit or complex-task settings. I would suggest that the authors provide more insight and analysis into the reasons behind this degradation.\n2. While the major benefit of QUANTGEN is its ability to operate without task or data awareness, I am concerned that this approach might miss essential data-driven subtleties that affect end-task accuracy, especially in less standard network architectures. Do the authors believe that the lack of task or data awareness could limit QUANTGEN’s effectiveness in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xWx63ah1mq", "forum": "B5sxtW8hlf", "replyto": "B5sxtW8hlf", "signatures": ["ICLR.cc/2026/Conference/Submission17751/Reviewer_dbQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17751/Reviewer_dbQd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124628997, "cdate": 1762124628997, "tmdate": 1762927590707, "mdate": 1762927590707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}