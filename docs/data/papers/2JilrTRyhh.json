{"id": "2JilrTRyhh", "number": 6677, "cdate": 1757991884510, "mdate": 1763706446049, "content": {"title": "Transformers as a Measure-Theoretic Associative Memory: A Statistical Perspective", "abstract": "Transformers excel through content-addressable retrieval and the ability to exploit contexts of, in principle, unbounded length.\nWe recast associative memory at the level of probability measures, treating a context as a distribution over tokens and viewing attention as an integral operator on measures. \nConcretely, for mixture contexts $\\nu = I^{-1} \\sum_{i=1}^I \\mu^{(i)}$ and a query $x_{\\mathrm{q}}(i^\\*)$, the task decomposes into (i) recall of the relevant component $\\mu^{(i^\\*)}$ and (ii) prediction from $(\\mu_{i^\\*},x_{\\mathrm{q}})$.\nWe study learned softmax attention (not a frozen kernel) trained by empirical risk minimization and show that a shallow measure-theoretic Transformer composed with an MLP learns the recall-and-predict map under a spectral assumption on the input densities. We further establish a matching minimax lower bound with the same rate exponent (up to multiplicative constants), proving sharpness of the convergence order. The framework offers a principled recipe for designing and analyzing Transformers that recall from arbitrarily long, distributional contexts with provable generalization guarantees.", "tldr": "", "keywords": ["associative memory", "learning theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17bae70387b6dccebf794665060d45a406f0f492.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In the paper, the authors cast associative memory in transformers as a problem in retrieving the correct component from a mixture distribution via querying. The authors show that a (measure-theoretic) transformer trained on empirical risk minimization is able to implement via an approximately one-hot softmax the optimal target function with sub-polynomial rate."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors carry out a sophisticated theoretical analysis of the problem. Particularly nice is the tight characterization of the excess-risk rate. Despite the complex nature of the work, the paper is relatively well written and organized. I especially like that the authors reserved some space to give a quick proof sketch for theorems in the main body. The proofs seem reasonable, even though I could not check the appendix in detail."}, "weaknesses": {"value": "The limit of the work is obviously the lack of clear practical implications, being a purely theoretical work. The lack of experimental results is also a consequence of the same fact. Nonetheless, the results are interesting and a worthy theoretical contribution.\n\nOn the minor side:\n1. Lines 176-177 are repeated at lines 178-179;\n2. The definition at line 211 is confusing: there is no F in the argument;\n3. I would take a little space to explain what RKHS in Sec. 3.1.1;\n4. There is an extra \"that\" at line 406."}, "questions": {"value": "1.Do you think that this work could be relevant and practically useful for the community in any way? More specifically, why do you think that an infinite-dimensional context in the form of a mixture distribution can be meaningful in practice?\n2. The assumptions are not discussed extensively and they seem quite strict. In the conclusion you mention the possibility to extend your results to broader regimes such as polynomial instead of exponential regimes. What is currently the main difficulty in carrying out such a generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D2Gz1Zm6ZE", "forum": "2JilrTRyhh", "replyto": "2JilrTRyhh", "signatures": ["ICLR.cc/2026/Conference/Submission6677/Reviewer_coHc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6677/Reviewer_coHc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629647632, "cdate": 1761629647632, "tmdate": 1762918978694, "mdate": 1762918978694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment"}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely thank all reviewers for their thoughtful and detailed feedback. We have revised the paper accordingly, and below we summarize the main clarifications and changes that are common across the reviews.\n\n### Measure-theoretic settings\n\nOur use of probability measures is **not** to make the setting artificially abstract, but to formalize in a clean way what is already implicit in practice.\n\n- Conceptually, representing a token sequence as a measure is analogous to representing a whole document by its empirical word distribution, essentially a document-level embedding. If we want a mathematically precise notion of \"context embedding\" that does **not** depend on the context length, using probability measures is a natural and robust choice.\n- Technically, prior theoretical work already treats self-attention as an **integral operator** on function spaces to study expressivity and generalization independently of sequence length (e.g., Vuckovic et al., 2020). Our framework follows this line rather than introducing measure theory for its own sake.\n- In our paper, the measure appears as the **limit of empirical distributions** of a finite token multiset: as the number of tokens $N$ grows, the law of large numbers allows us to represent a very large token collection by a single measure, so the problem can be formulated independently of $N$.\n\nThe advantages of working directly with measures are:\n\n1. We can define similarity between contexts via distances between measures (e.g., Wasserstein, MMD), instead of ad hoc sequence-level metrics.\n2. We can exploit smooth densities and their spectral (Mercer) expansions.\n3. In the unmasked setting, permutation invariance of the context is built in: a measure is inherently insensitive to token order.\n\nBy staying entirely in the \"world of measures,\" we can safely introduce distances between sets of measures and notions such as covering numbers and effective dimension, and use them to **quantify**, both:\n- the smoothness of functions that take token sets as input, and \n- the intrinsic difficulty (minimax rates) of the associated estimation problem.\n\n### Spectral decay\n\nRegarding the spectral decay assumption, we now clarify its role and give concrete examples.\n\n- As a first-step setting, we assume **exponential eigenvalue decay**, inspired by widely used kernels such as the **Gaussian kernel**. In our bounded-domain context, this corresponds to truncated Gaussian-type kernels.\n- As a rigorous example compatible with our assumptions, we emphasize the **heat kernel** on $[0,1]$. The Laplace operator $d^2/dx^2$ has trigonometric eigenfunctions with eigenvalues proportional to $j^2$ , and the associated heat kernel has spectrum $\\exp(-c j^2)$, i.e., $\\lambda_j \\simeq \\exp(-c j^\\alpha)$ with $\\alpha = 2$ . This is a standard textbook example (Grigor’yan, 2006) and serves as a natural toy model for our theory.\n- In this work we deliberately focus on the **exponential** eigen-decay case as a clean first step. For **polynomial** decay, $\\lambda_j \\simeq j^{-2\\alpha}$, the covering entropy of the model class in Section 3 is already known to satisfy a lower bound of order $\\exp(\\Omega(\\varepsilon^{-1/(\\alpha+1)}))$ (Lanthaler, 2024) and an upper bound of order $\\exp(O(\\varepsilon^{-1/\\alpha}))$ (via Boissard, 2011). We expect the upper-bound argument in **Theorem 1** to extend to this setting with moderate changes, but closing this gap to obtain a matching **minimax lower bound** (Theorem 2) is technically nontrivial, so we leave a full treatment of the polynomial regime to future work.\n\n### Experiments\n\nOur main contribution is theoretical, but we add a **small synthetic experiment (Appendix D)** as a sanity check. In a controlled setting, we vary the spectral decay parameter $\\alpha$ and observe faster excess-risk decay for larger $\\alpha$, and we inspect the learned softmax attention, which tends to concentrate on tokens associated with the queried component.\n\n### Typos and notation\n\nWe thank the reviewers for carefully pointing out several typographical and notational issues. We have corrected all such issues and streamlined the notation to improve readability.\n\nOnce again, we are very grateful to all reviewers for their insightful comments and suggestions, which have significantly improved the clarity and presentation of our work.\n\n---\n\n- Vuckovic, J., Baratin, A., & Combes, R. T. D. (2020). A mathematical theory of attention. arXiv preprint arXiv:2007.02876.\n- Grigor’yan, A. (2006). *Heat kernels on weighted manifolds and applications*. Contemp. Math. 398.\n- Lanthaler, S. (2024). Operator learning of lipschitz operators: An information-theoretic perspective. arXiv preprint arXiv:2406.18794.\n- Boissard, E. (2011). Simple bounds for the convergence of empirical and occupation measures in 1-Wasserstein distance. *Electronic Journal of Probability, 16*."}}, "id": "51nFcGiC1w", "forum": "2JilrTRyhh", "replyto": "2JilrTRyhh", "signatures": ["ICLR.cc/2026/Conference/Submission6677/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6677/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6677/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763700232820, "cdate": 1763700232820, "tmdate": 1763700232820, "mdate": 1763700232820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a measure-theoretic view of Transformers for \nassociative recall.\nInstead of treating a context as a finite token sequence, the input is a \nmixture of probability measures over tokens; a query identifies the \nrelevant component measure (recall), and the predictor maps that \nrecalled measure (plus the query) to an output (predict).\nTechnically, attention is lifted to an integral operator on measures, so \nsums over tokens become integrals.\nUnder a spectral assumption, i.e. Mercer eigenvalues of the RKHS\nkernel decay exponentially, the authors construct a shallow (depth-2) \nTransformer+MLP and prove a sub-polynomial excess-risk bound of the \nform\n$\\exp\\{-\\Theta((\\log n)^{\\alpha/(\\alpha+1)})\\}$ for ERM, and provide a \nmatching minimax lower bound (same exponent, up to constants).\nThey argue learned softmax attention can implement near one-hot, \nquery-adaptive recall that isolates the correct component measure and \naggregates a small set of its Mercer coefficients for prediction."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "To the extent of my understanding, which is limited, the modeling of contexts as probability distributions seems an interesting line of research, with several other recent papers treating it."}, "weaknesses": {"value": "Clarity:\nI found the paper extremely hard to parse (which may be my fault, and not a weakness, I let the area chair judge). For example:\n- line 73: it is not clear what is meant by \"context\"\n- line 74: it is not clear on which space do the measuere nu, mu^(i) live in\n- line 75: it is not clear which space the query x_q lives in\n- line 76: what is the \"target map\". Is it some sort of ground-truth function? This should be defined clearly\n- etc...\nWhile stated as \"informal\", I find this introductory section quite confusing.\nThe paragraph in line 149-167 is not illuminating either on this, and one needs to reach line 169 to finally know what are the objects we are speaking about.\nWhile the paragraph starting at line 169 provides definition, they are hard to understand: I would suggest providing examples here!\n\nMore generally, important intuitions (effective dimension $(\\log n)^{1/(1+\\alpha)}$, role of the second attention block, sensitivity to query–key geometry) are \nscattered across sections and appendices, hampering readability for a \nbroader audience.\n\nModel:\nThe query explicitly contains the relevant tag $v^{(i^\\*)}$ and the tags \nare well-separated (inner products $\\le 0$), making recall potentially substantially \neasier than in natural setting.\n \nLack of any empirical validation:\nThe work is purely theoretical. Even small \nsynthetic experiments would help test the sharpness/spikiness of learned \nsoftmax and test the assumptions (e.g., performance deterioration \nwhen spectra are polynomial)."}, "questions": {"value": "Polynomial spectra. Can you extend (even heuristically) the analysis to \nkernels with polynomial eigen-decay? Which proof steps break and what \nrates would you expect?\n\nQuery robustness. What happens if $x_q$ does explicitly contain \n$v^{(i^\\*)}$ or if the tags are weakly separated/noisy? Can the softmax \ngate still become sufficiently spiky under ERM?\n\nEmpirics. Could you add synthetic experiments varying $\\alpha$ and \n$I$, and compare learned softmax vs. linear attention / frozen kernels, to \nillustrate the theory?\n\ntypos:\n- line 179 repeats line 176\n- line 180: \\Chi_0 not defined\n- line 211: inside the argmin should be F? Same in line 214"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CFUomIZaYr", "forum": "2JilrTRyhh", "replyto": "2JilrTRyhh", "signatures": ["ICLR.cc/2026/Conference/Submission6677/Reviewer_wjf3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6677/Reviewer_wjf3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839835123, "cdate": 1761839835123, "tmdate": 1762918978316, "mdate": 1762918978316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempts to study the associative memory problem theoretically. It formulates the \"context\" to a transformer as a mixture of measures.  The goal of the transformer is, on reading a query corresponding to one of these constituent measures, to identify the associated measure."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper attempts a theoretical analysis of a practically relevant setup in transformers, which is always welcome and appreciated.  It presents matching upper and lower bounds to the risk of a statistical estimation problem that they pose as a proxy to the associative memory problem."}, "weaknesses": {"value": "The biggest issue of this paper is that the mathematical notation is (a) far too dense for a conference such as ICLR, and (b) just poor, objectively (there are too many to list, but see Questions for a few).  At a high level, the paper reads as follows to me: the paper spend 7 pages (!) setting up notation and making several assumptions (some of which, admittedly, do seem reasonable, but, e.g., the eigenvalue decay rate, just show up out of nowhere) and then obtains some results that likely do not hold if any of the assumptions are violated even slightly (e.g., if the eigenvalue decay rate is different).  \n\nIs there really a need for the entire paper to be measure-theoretic?  What would be different if we just assumed these were just a linear combinations \"discrete distributions\" of some finite set and then went with it?  The insistence on measure theory and functional-analytic analysis makes the paper very specialized, so if it could be simplified at all, it would be greatly appreciated (I understand that often we do actually require this level of sophistication to analyze some setups, but this setup feels like a more interesting analysis could be made with simpler mathematics).\n\nWhat exactly is the statistical estimation problem introduced in Section 3?  As far as I understand, it is an abstraction of the memory recall problem (I might be wrong, but this is what I understood from the paper).  In this case, what exactly is the value of such an abstraction and analysis?  I do not think there is value in knowing at exactly what rate it decays, given that the problem itself is a \"fake\" problem invented only a a proxy.  I think there would be value if the paper were able to provide insights/explain some curious phenomena on the general problem of associate memory and recall, but I do not see any such insights."}, "questions": {"value": "1. Why this choice of eigenvalue decay (lambda_j = exp(-c j^alpha))?  This seems rather arbitrary and I do not see any justification/explanation for it.  More importantly, this alpha is crucial and shows up in the final results, making them very fragile (not robust to this assumption).\n\n2. Example 1 lists some examples satisfying the assumptions, but it is not clear if these examples satisfy all or some subset of the assumptions, and whether these examples are interesting for the setup being considered (e.g., one could list any arbitrary assumption and say that there are functions satisfying that assumption, but whether it makes sense to consider that example for that setup is important).  Comments?\n\nMinor comments:\n1. Equation at line 211: should be F, not F^\\star\n2.  Section 3.2, one of the expressions for either SAttn or MSAttn must be wrong: both have a V^h x at the end, and MSAttn does not have a W^h  (guessing that SAttn is right and MSAttn has W instead of V)?\n3. Definition 5, the equation: what is nu_1, did you mean mu_1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sq2uj3QxSE", "forum": "2JilrTRyhh", "replyto": "2JilrTRyhh", "signatures": ["ICLR.cc/2026/Conference/Submission6677/Reviewer_rkiJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6677/Reviewer_rkiJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859352528, "cdate": 1761859352528, "tmdate": 1762918977994, "mdate": 1762918977994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}