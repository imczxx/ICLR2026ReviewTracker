{"id": "hC53SFmUMe", "number": 20015, "cdate": 1758301510598, "mdate": 1759897006111, "content": {"title": "Learning Structured Dependencies using Generative Computational Units", "abstract": "The ability of neural networks to generalize from data is fundamentally shaped by the design of their computational units. Common examples include the perceptron and radial basis function (RBF) units, each of which provides useful inductive biases. In this work, we introduce a new computational unit, the generative matching unit (GMU), which is designed to naturally capture structured dependencies in data. Each GMU contains an internal generative model that infers latent parameters specific to an input instance $X$, and then outputs a non-linear function of the generative error using these parameters. By incorporating generative mechanisms into the unit itself, GMUs offer a complementary approach to existing computational units. In this work, we focus on linear GMUs, where the internal generative models are linear latent variable models, yielding a function form that shares some similarities with RBF units. We show that linear GMUs are universal approximators like RBFs, while being able to convey richer information and lessen the impact of the curse of dimensionality compared to RBFs.  Like perceptrons and RBF units, a linear GMU has its set of weights and biases, and has a closed-form analytical expression, enabling fast computation. To evaluate the performance of linear GMUs, we conduct a set of comprehensive experiments and compare them to multi-layer perceptrons (MLPs), RBF networks, and ResNets. We construct GMU-ResNets, where the first feedforward layer is replaced by GMUs, and test on 27 tabular datasets, observing improved generalization over standard ResNets and competitive performance with other benchmarks. We also construct GMU-CNNs, which contain convolutional GMUs in their first layer. Across five vision datasets, GMU-CNNs  exhibit better generalization and significantly better robustness to test-time corruptions. We also empirically compare linear GMUs, to benchmark networks across more than 30 synthetic classification tasks, encompassing both structured and unstructured data distributions. We find that GMUs consistently demonstrate superior generalization to out-of-distribution samples, especially for the structured cases.", "tldr": "We show that a different optimal computational unit emerges when assuming a switching-based structural causal model for data generation.", "keywords": ["Computational Units", "Supervised Learning", "Generalization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66037d832babbf8f36e5ae0b373bcb53b3abc327.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Generative Matching Unit (GMU), a new neural computational unit designed to capture structured dependencies in data through an internal generative model. Key contributions include: (1) formalizing GMU variants (e.g., linear GMUs) with closed-form expressions and theoretical guarantees of universal approximation and Bayesian optimality under structured sampling processes; (2) demonstrating GMU's robustness to the curse of dimensionality compared to RBF units via k-subspace distance metrics; and (3) empirical validation showing improved generalization and out-of-distribution robustness in tabular (e.g., GMU-ResNets) and vision tasks (e.g., GMU-CNNs)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is novel.\n2. The paper is very well-written and clearly structured.\n3. The paper is completed with theory, analysis, and experiments."}, "weaknesses": {"value": "1. The paper's most significant limitation is the decision to place GMUs only in the first layer of the networks tested. While justified by computational concerns, this severely restricts the assessment of GMUs as a general-purpose computational unit.\n2. While testing on 27 tabular datasets is thorough, the vision benchmarks are relatively standard and small-scale. The paper does not demonstrate that GMUs scale effectively to larger, more complex datasets, such as ImageNet.\n3. The paper would benefit from a more systematic ablation of the GMU's hyperparameters, particularly the order $k$ and the activation function $\\phi$."}, "questions": {"value": "See the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ib55KkU4mm", "forum": "hC53SFmUMe", "replyto": "hC53SFmUMe", "signatures": ["ICLR.cc/2026/Conference/Submission20015/Reviewer_cDvd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20015/Reviewer_cDvd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761228356372, "cdate": 1761228356372, "tmdate": 1762932917479, "mdate": 1762932917479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Generative Matching Unit (GMU), a new type of neural computational unit that embeds generative model\nwithin each neuron. Instead of performing a linear mapping Wx+b as in a perceptron, a GMU infers latent parameters through an\ninternal function f(θ,W) and computes its activation from the minimal reconstruction error between the generated and observed\ninputs. The authors argue that this design allows GMUs to capture structured dependencies in data and achieve “Bayes-optimal”\nbehavior under class-wise structural causal models (CL-SCMs). The authors primarily study linear GMUs, a simplified version that\nshows analytical similarity to RBF units while claiming richer representational capacity. They further integrate these GMUs into\nstandard neural architectures, constructing MLP- and CNN-based networks in which conventional layers are replaced by GMU\nlayers to evaluate their effectiveness on both tabular and vision benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of reinterpreting neural computation units from a Bayesian–generative perspective is conceptually interesting.\n2. The definition of the GMU is clearly structured and easy to follow. The mathematical formulation and analysis are\ngenerally sound and clearly presented."}, "weaknesses": {"value": "1. The proposed GMU is functionally similar to RBF units —it computes a distance between the input and an internal template, followed by a nonlinear mapping. The internal “latent generative model” serves only as a reparameterized projection, not a true generative process.\n2. The proposed GMU introduces a significantly higher computational cost compared to standard RBF units. While this theoretically allows\nmore expressive representations, the paper does not demonstrate any scenario where such additional complexity leads to substantial benefits.\n3. The theoretical discussion of Bayes-optimality, ECI, and SCM essentially reformulates the Naive Bayes assumption without offering\nsubstantial new insight. This section feels overly long and contributes little to the actual development of the GMU.\n4. The evaluation is limited. Comparisons are restricted to RBF networks and standard CNN/ResNet baselines, without including more recent probabilistic or energy-based architectures. The experiments are conducted only on small-scale tabular and vision datasets, which limits the generality of the results and makes it difficult to assess the scalability of the proposed approach.\n5. Furthermore, the experiments replace only the first layer of standard networks with GMUs, limiting the evaluation to shallow settings and preventing a deeper investigation of whether the proposed generative properties hold in multi-layer architectures."}, "questions": {"value": "1. Could the proposed GMU behavior be approximated by stacking conventional neurons or RBF units to form equivalent subspace\nprojections? If so, what unique representational benefit does GMU provide beyond reparameterizing existing architectures?\n\n2. The GMU introduces an internal pseudo-inverse computation, which can be expensive. Could the authors provide complexity analysis or\ndiscuss practical settings where the additional cost is justified?\n\n3. All experiments use only the linear version of the GMU. Have the authors explored nonlinear variants, and if so, what challenges were\nencountered in optimization or computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SLpbhE7Ndm", "forum": "hC53SFmUMe", "replyto": "hC53SFmUMe", "signatures": ["ICLR.cc/2026/Conference/Submission20015/Reviewer_ZBLM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20015/Reviewer_ZBLM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652712139, "cdate": 1761652712139, "tmdate": 1762932916981, "mdate": 1762932916981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the author proposed generative matching units (GMU) as a computational unit in deep learning models to enhance the generalizability and capture structural dependency in data. The authors also proved that GMU are universal approximators and are efficient and robust. GMUs are also designed to avoid the curse of dimensionality. On tabular datasets and vision datasets, GMU also demonstrate higher classification accuracy than baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This work is clearly presented. The logic is linear and the core mechanism of proposed GMUs is well articulated. \n\n2. Theories developed and discussed in this work are compact. How GMU can resolve the curse of dimensionality issue is sufficiently scrutinized in theory. Extensive experiments demonstrate the effectiveness of proposed GMU to avoid the curse of dimensionality issue. They indicate the comprehensiveness and coherence of this work. Theoretically and empirically, the discussion of the curse of dimensionality issue is of high quality."}, "weaknesses": {"value": "1. Although attention units are mentioned in Introduction, it is not clearly and comprehensively discussed in related works and methodologies. As an important and ubiquitous computational unit, the attention unit should be considered as a baseline in terms of generalizability, robustness to OOD cases and efficiency for the proposed GMU.\n\n2. The significance of this work definitely needs to be clearly and explicitly studied and articulated. Nowadays, computational units are not of such high significance in the era of LLMs, if the computational units proposed cannot answer the pressing questions, such as security, reasoning, etc.\n\n3. The technical novelty and originality of this work also requires to be highlighted and stated with clarity. The generative Bayesian computation methods in recent two years are not referred and compared, such as [1]. \n\n[1] Maria Nareklishvili, Nicholas Polson, and Vadim Sokolov, Generative Bayesian Computation for Causal Inference, arXiv, vol. 2306.16096, 2024."}, "questions": {"value": "1. Why is it still necessary to propose and study GMU as universal approximators in this era on top of self-attention? Is it possible to answer the question why foundation models can hallucinate and solve this problem? Can GMU explain and improve the interpretability of foundation models? \n\n2. Compared with newly proposed computational units such as self-attention, KAN or capsule networks, does GMU demonstrate higher generalizability and robustness theoretically or empirically, especially against OOD scenarios?\n\n3. For GMU, could the authors give an example of nonlinear GMU? Why in this paper nonlinear GMU is not chosen as a computational unit in deep learning models for study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aaVGWTKP1o", "forum": "hC53SFmUMe", "replyto": "hC53SFmUMe", "signatures": ["ICLR.cc/2026/Conference/Submission20015/Reviewer_7BWj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20015/Reviewer_7BWj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725696862, "cdate": 1761725696862, "tmdate": 1762932916303, "mdate": 1762932916303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A new computational unit (a kind of layer of a network) is proposed. The idea is to use the distance of a data point from the manifold corresponding to a structural causal model as a feature. The theoretical results presented in the paper show that such a model is reasonable because it can achieve the Bayes optimal classifier when the data come from the structural causal model. The experimental results show the strong generalization capability of the proposed model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is nicely structured. The motivation of the proposed computational unit, theoretical analysis, and the experiments are presented in a well-organized manner.\n\nThe idea of the proposed computational unit is reasonable.\n\nThe experiments show the strong capability of the proposed model."}, "weaknesses": {"value": "I have no major concerns. Below are minor comments.\n- Having a separate section merely for listing the contributions looks strange. It is usually a part of the introduction section. (I even think such a list is of no use though)\n- In Eq. (3) (and relevant equations in the appendix), why there is $\\cdot^\\top$ in the second occasion of $\\frac{X-b}\\eta$? No transpose seems needed.\n- Direct comparison of the numbers of the parameters of the baseline and proposed models would be informative."}, "questions": {"value": "The empirical result in the part of Curse of Dimensionality is not necessarily straightforward to interpret. Especially in Figure 3(a), the CV values of higher dimensionalities (say $d>40$?) seem very close to each other. Does it suggest that such dimensionalities are still hard to deal with? Or does it look so just because of the visualization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f0rnGt9hvQ", "forum": "hC53SFmUMe", "replyto": "hC53SFmUMe", "signatures": ["ICLR.cc/2026/Conference/Submission20015/Reviewer_R3fg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20015/Reviewer_R3fg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919197938, "cdate": 1761919197938, "tmdate": 1762932915948, "mdate": 1762932915948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}