{"id": "uW9FoHBuoQ", "number": 12260, "cdate": 1758206679840, "mdate": 1759897521964, "content": {"title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning", "abstract": "Inverse Reinforcement Learning (IRL) aims to recover Reward Models from expert demonstrations, but traditional methods yield \"black-box\" models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using code Large Language Models (LLMs) within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically demonstrate that GRACE can efficiently learn highly accurate rewards in the multi-task setups as defined by two benchmarks, BabyAI and AndroidWorld. Further, we demonstrate that the resulting reward leads to strong policies compared to both competitive Imitation Learning and online RL approaches with groundtruth rewards. Finally, we show that \\method{} is able to build complex reward APIs in mulit-task setups.", "tldr": "We introduce GRACE, a framework that uses code-generating language models within an evolutionary search to learn an interpretable reward function as executable Python code directly from expert demonstrations", "keywords": ["inverse reinforcement learning", "large language models", "evolution"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00dab441d27acb142ed266e7eddd4983b65b273a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GRACE, a framework for inferring interpretable reward functions as Python code from expert demonstrations. The approaches iteratively optimizes a population over LLM-generated code-level reward functions via evolutionary search. The initial population is generated from LLM-labelled goal states, and then refined using new samples gathered from an inner RL loop. The data collection and context provided to the LLM in the evolutionary search depend on the goal label provided by an LLM. The proposed method consistently solves BabyAI and AndroidWorld tasks, and outperforms GAIL and a (forward-RL) PPO baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using an evolutionary framework where an LLM mutates code-as-a-reward functions based on achieved goal states and failures is a novel and highly promising approach to IRL.\n- By design, the method produces human-readable reward functions, which is a significant advantage over black-box IRL.\n- The paper is generally well-written and easy to follow. The approach is presented in a clear and concise manner, and most hyperparameters and design choices are listed and justified.\n- GRACE shows strong performance on the selected tasks outperforming GAIL with a fraction of the data and producing rewards that are on par with the ground truth in terms of downstream RL performance."}, "weaknesses": {"value": "- The entire framework seems to depend on an LLM's ability to correctly identify goal states from demonstrations and, more crucially, from new agent trajectories of the inner RL loop. This is a rather strong assumption that deviates from the standard IRL setup. The (potential lack of) robustness of this approach is not analyzed, and it seems to limit GRACE to finite-horizon RL settings.\n- Similarly, the context provided for the LLM-based mutation seems a bit arbitrary and could be better motivated or experimented with.\n- The related work section lists other \"Code as Reward\" methods but does not explain how GRACE compares to or improves upon them. None of these methods are included as baselines in the experiments, and this choice is not justified or explained. This omission makes it difficult to assess GRACE’s position in the research landscape, and consequently the novelty and significance of the method. GAIL is chosen as the only Imitation Learning/IRL baseline, which may be insufficient given a lot of recent advances in, e.g., diffusion-based imitation learning.\n- The experimental results are a bit thin. For example, Figure 3 uses error bars over 3 seeds, which are absent for, e.g., Figure 2. Similarly, Figure 5 reports the mean over three different environments, and so on. Some details, such as the number of demonstrations used for the AndroidWorld tasks, are missing. GRACE solves both presented benchmarks, which makes it hard to judge where the boundaries of the method lie and where it would fail. There is no qualitative visualization, which makes it difficult to appreciate the difficulty of the tasks."}, "questions": {"value": "I think the present draft is promising, but have some small problems with its rigour and presentation. I’m happy to positively re-assess my rating if the below questions are adequately discussed and addressed.\n\n1. The reliance on LLM-labeled goal states seems to be a major design choice, especially since this labeling is also used for the RL-in-the-loop part of the algorithm. Could the authors provide more justification for this? A discussion on its limitations and potential failure modes, or an ablation showing how performance degrades with less accurate labeling (either via artificial noise or, preferably, from a less powerful LLM), would be very valuable.\n2. The related work section mentions other methods that generate rewards-as-code. Could the authors provide a more detailed distinction between these methods and GRACE and, ideally, add at least one of these methods as a baseline?\n3. Could the authors clarify the experimental setup? How many demonstrations are used for the AndroidWorld tasks, how many seeds are used for the different results, and why?\n4. The notation for demonstrations and their goal state is somewhat overloaded, and a demonstration is essentially only characterized by its goal state. (How) can the method use intermediate states of these trajectories to its advantage? Is it necessary to provide negative trajectories, or can intermediate states act as “negatives” for the LLM prompt? An ablation study\non this behavior would help clarify the capacities and limitations of GRACE"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gAf64donJs", "forum": "uW9FoHBuoQ", "replyto": "uW9FoHBuoQ", "signatures": ["ICLR.cc/2026/Conference/Submission12260/Reviewer_R5gj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12260/Reviewer_R5gj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761150531191, "cdate": 1761150531191, "tmdate": 1762923199704, "mdate": 1762923199704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRACE (Generating Rewards As CodE), a framework that uses code-generating Large Language Models within an evolutionary search procedure to learn interpretable reward functions from expert demonstrations. The method operates in three phases: (1) identifying goal states from expert trajectories, (2) refining reward functions through evolutionary search guided by fitness on goal/non-goal state classification, and (3) expanding the dataset through RL-trained policy rollouts. The authors evaluate GRACE on BabyAI navigation tasks and AndroidWorld device control tasks, demonstrating that it can recover accurate rewards from few demonstrations and outperform GAIL while matching oracle PPO performance. The code-based representation enables interpretability and natural emergence of reusable reward APIs across multiple tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important problem of interpretability in Inverse Reinforcement Learning by representing rewards as executable code rather than opaque neural networks.\n- Using code as rewards is interesting and likely enables faster RL training compared to querying LLM reward models at each step.\n- The approach demonstrates strong sample efficiency, achieving high performance with only 8 expert trajectories compared to GAIL's requirement of 2000 demonstrations.\n- The multi-task experiments on BabyAI showing emergent code reuse and API formation are compelling evidence of the benefits of symbolic reward representations.\n- The writing is generally clear and the method is presented systematically."}, "weaknesses": {"value": "- Fundamental evaluation concerns: As acknowledged in limitations, the LLM serves as the final judge of success in Phase 3, which compromises evaluation validity since the same model generates and evaluates rewards.\n- Missing critical baselines: The paper lacks comparison to prompted LLMs as reward judges (mentioned in related work but not benchmarked), which would be the most obvious baseline. Similarly, no comparison to using LLMs directly as policies with demonstrations in context.\n- Limited problem scope: The approach reduces reward learning to classifying goal states, which only covers tasks with distinguishable terminal success states. This excludes many RL problems like learning to backflip where initial and final states are identical, or continuous control tasks requiring trajectory-level rewards.\n- Theoretical issues: Proposition 1 is unclear—the relationship between the mask function m and the original IRL objective (Eq. 2) is not explained. How does flipping rewards on goal/non-goal states relate to the max-margin formulation of classical IRL?\n- Statistical rigor: Results appear to be from single runs without error bars or significance tests (e.g., Figure 4 shows single curves). Figure 3 mentions \"3 seeds\" but most other results lack this. Table 1 shows single numbers without variance.\n- Confounded claims: The claim that Phase 3 generates \"well-shaped\" rewards conflates reward shaping with improved coverage beyond expert demonstrations. The performance gains might simply come from seeing more diverse states rather than better shaping.\n- Limited scope: Evaluation restricted to relatively simple domains (BabyAI mazes, basic Android tasks)."}, "questions": {"value": "- How does GRACE handle continuous states/actions? All experiments use discrete or structured representations. Can it -work with raw continuous sensor data?\n- What is the computational cost (LLM queries, wall-clock time) compared to baselines?\n- Why only GAIL as a baseline? How does GRACE compare to other IL methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CoxHbQw0MM", "forum": "uW9FoHBuoQ", "replyto": "uW9FoHBuoQ", "signatures": ["ICLR.cc/2026/Conference/Submission12260/Reviewer_JQHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12260/Reviewer_JQHA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864971611, "cdate": 1761864971611, "tmdate": 1762923199188, "mdate": 1762923199188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Manual reward design for RL is challenging and impractical. To address this, inverse reinforcement learning approaches recovers reward functions from expert demonstrations, but these recovered reward functions are traditionally non-interpretable. Recently, code-generating LLMs recover explainable reward functions but require human-curated task descriptions, goal states or feedback from a trained policy. This paper proposes to recover reward functions purely without task description or goal-specific design specifications by leveraging code-LLMs. First, using the expert demonstrations and random trajectories, initial reward design is created as code. Then, iteratively, through an evolutionary search and a fitness score, reward is refined; using this reward, a policy is trained and additional data is collected, and this process is repeated for fixed number of steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-**Interesting Approach**: Using evolutionary search with a fitness score to generate reward functions looks promising when the size of the expert data is limited. \n\n-**Ablations**: It is clearly presented how the method improves with more expert trajectories and negative trajectories. \n\n-**Analysis of Training**: The proposed approach is well evaluated and how it changes during the training is well presented."}, "weaknesses": {"value": "- **Computational Overhead**: Computationally the proposed method looks significantly complex. It requires retraining the policy for each reward refinement, and this looks inefficient even with the limited budget $N$  they use for PPO training. The computational overhead of the proposed approach must be clearly evaluated against Adversarial IL (GAIL [c] etc.) and IRL (MaxEnt IRL [d], GCL [e], etc.). Wall clock times during trainings or total number of gradient steps can be compared. \n\n- **Contributions and Novelty**: The main contribution of the paperis claimed to be not using any information other than expert demonstrations. However, in line 161, it is stated that extra information about the environment is used as query to the LLM for initial reward design. Although this is not human-designed, this contradicts the claim of purely using expert data. Please clarify how important this extra environment information is via an experimental analysis. More importantly, the proposed approach is fundamentally very similar to [a], and both of them use the performance of the trained policy as feedback for reward learning. Therefore, the contribution over [a] must be clarified. Please compare both the performance and computational efficiency with [a] extensively.\n\n- **Limited Experimental Setting**: Experimental setting is limited to selected BabyAI environments. Please employ more standardized reinforcement learning and imitation learning benchmarks covering continuous control, locomotion and dexterous manipulation tasks. It would be great if you could evaluate GRACE in the IsaacGym [f] and Bi-dexterous Manipulation [g] benchmarks as in [a].\n\n- **Unclear Design Choices**: In Eq. 3, authors state that they bound the reward function. However, instead of just bounding, they employ binary rewards in Eq. 3 with a threshold. The reason behind this selection is unclear, and not explained. \n\n- **Further Analysis and Clarification**: In Figure 3, the x-axis is unclear, it should be clearly stated whether it is $K$ or not. Also, in addition to the effect o $K$, the effect of $M$ (number of refinement steps) and $N$ (number of PPO training steps *for each refined reward function* should be analyzed. \n\n- **Comparisons with IRL**: It is claimed that the proposed method is more sample efficient than IRL approaches. Please clearly demonstrate empirically that GRACE outperforms IRL (d-e, not adversarial imitation learning which is different than IRL). \n\n[a]: Ma, Yecheng Jason, et al. \"Eureka: Human-Level Reward Design via Coding Large Language Models.\" The Twelfth International Conference on Learning Representations.\n\n[b]: Venuto, David, et al. \"Code as Reward: Empowering Reinforcement Learning with VLMs.\" International Conference on Machine Learning. PMLR, 2024.\n\n[c]: Ho, Jonathan, and Stefano Ermon. \"Generative adversarial imitation learning.\" Advances in neural information processing systems 29 (2016).\n\n[d]: Ziebart, Brian D., et al. \"Maximum entropy inverse reinforcement learning.\" Aaai. Vol. 8. 2008.\n\n[e]: Finn, Chelsea, Sergey Levine, and Pieter Abbeel. \"Guided cost learning: Deep inverse optimal control via policy optimization.\" International conference on machine learning. PMLR, 2016.\n\n[f]: Makoviychuk, Viktor, et al. \"Isaac Gym: High Performance GPU Based Physics Simulation For Robot Learning.\" NeurIPS Datasets and Benchmarks. 2021.\n\n[g]: Chen, Yuanpei, et al. \"Towards human-level bimanual dexterous manipulation with reinforcement learning.\" Advances in Neural Information Processing Systems 35 (2022): 5150-5163."}, "questions": {"value": "- Please compare the performance and computational efficiency of the proposed approach with LLM-based reward generation methods in the literature (a,b). \n\n- Please discuss this design selection of using binary reward values in Eq. 3\n\n- Can you please examine the impact of providing environment information to LLM in initial reward design step? \n\n- It is interesting that the goal module is more and more used in later stages of the training, and go_to_reward component is less and less used as training goes on. Just because of curiosity, it would be great to hear the authors' interpretation of this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zvJJkBrRtY", "forum": "uW9FoHBuoQ", "replyto": "uW9FoHBuoQ", "signatures": ["ICLR.cc/2026/Conference/Submission12260/Reviewer_2iny"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12260/Reviewer_2iny"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951342393, "cdate": 1761951342393, "tmdate": 1762923198606, "mdate": 1762923198606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GRACE is an IRL framework that uses code-generating LLMs and evolutionary search to infer executable Python reward functions from expert demonstrations. It operates in three phases: (1) LLM identifies goal states and generates initial rewards from positive/negative trajectories; (2) evolutionary refinement via LLM mutations on misclassified states to maximize fitness; (3) online RL with PPO expands data, enabling further reward improvement. Evaluated on BabyAI and AndroidWorld, GRACE recovers accurate, well-shaped rewards from few demos, outperforms GAIL, matches oracle PPO, and produces reusable multi-task reward APIs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Achieves good reward recovery and generalization with 1–8 expert trajectories, far surpassing neural IRL methods in sample efficiency.\n2. Produces dense, interpretable rewards that enable strong policy learning—matching or exceeding ground-truth PPO in BabyAI and outperforming GAIL.\n3. Demonstrates emergence of modular, reusable reward code across tasks, offering a practical path toward composable reward design.\n4. Clean theoretical link to classical IRL via fitness function, with clear algorithm and reproducible structure."}, "weaknesses": {"value": "1. Heavy reliance on strong proprietary LLMs (e.g., gpt-4o) without ablation on weaker or open-source models; performance may degrade significantly in practice. The evaluation on other models could be conducted.\n2. No compute cost analysis—evolutionary search with 100+ LLM calls per task is likely expensive and slow, limiting scalability.\n3. Goal state identification by LLM is a critical weak point; no quantification of labeling errors or robustness to hallucinations.\n4. Evaluation scope is narrow: BabyAI is synthetic and low-dimensional; AndroidWorld uses only Clock app with curated negatives—real-world generalization untested.\n5. Baselines (e.g., GAIL) are not compared under identical low-data settings, and multi-task claims lack strong IRL benchmarks."}, "questions": {"value": "1. How does GRACE perform with open-source code LLMs (e.g., CodeLlama, DeepSeek)? Any ablation on model size or quality?\n2. What is the total LLM inference cost (tokens, time, $) per task? How does it scale with environment complexity?\n3. How reliable is LLM-based goal detection in Phase 1 and 3? Any error rate analysis on false positives/negatives?\n4. Why limit AndroidWorld to Clock app? Have you tested cross-app generalization or real-device deployment?\n5. The method assumes access to environment state (grid, XML)—how does it handle partial observability or raw pixels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wp7LZDdIaV", "forum": "uW9FoHBuoQ", "replyto": "uW9FoHBuoQ", "signatures": ["ICLR.cc/2026/Conference/Submission12260/Reviewer_ZUCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12260/Reviewer_ZUCH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989315347, "cdate": 1761989315347, "tmdate": 1762923198194, "mdate": 1762923198194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}