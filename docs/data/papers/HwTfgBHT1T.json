{"id": "HwTfgBHT1T", "number": 2203, "cdate": 1757025024752, "mdate": 1759898163009, "content": {"title": "Why Do Unlearnable Examples Work: A Novel Perspective of Mutual Information", "abstract": "The volume of freely scraped data on the Internet has driven the tremendous success of deep learning. Along with this comes the rising concern about data privacy and security. Numerous methods for generating unlearnable examples have been proposed to prevent data from being illicitly learned by unauthorized deep models by impeding generalization. However, the existing approaches primarily rely on\nempirical heuristics, making it challenging to enhance unlearnable examples with solid explanations. In this paper, we analyze and improve unlearnable examples from a novel perspective: mutual information reduction. We demonstrate that effective unlearnable examples always decrease mutual information between clean features and poisoned features, and when the network gets deeper, the unlearnability goes better together with lower mutual information. Further, we prove from a covariance reduction perspective that minimizing the conditional covariance of intra-class poisoned features reduces the mutual information between distributions. Based on the theoretical results, we propose a novel unlearnable method called Mutual Information Unlearnable Examples (MI-UE) that reduces covariance by maximizing the cosine similarity among intra-class features, thus impeding the generalization effectively. Extensive experiments demonstrate that our approach significantly outperforms previous state-of-the-art methods, even under defense mechanisms.", "tldr": "", "keywords": ["mutual information", "unlearnable examples"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c678fd05e4d8607fe5445a1d6605ecfdea2706e.pdf", "supplementary_material": "/attachment/cd2458e18def83d3f599c7171753e4922b9ebeb5.zip"}, "replies": [{"content": {"summary": {"value": "The paper argues that the effectiveness of unlearnable examples is heuristic and not well explained, and they set out to show that the mutual information gap presented by unlearnable datasets can better explain the effectiveness of reducing test accuracy. The authors propose a new optimization objective which reduces the covariance of intraclass features given the labels. Extensive evaluations show this approach is better than prior approaches in terms of its reduction of test accuracy, effectiveness against defenses, and transferability."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. MI-UE is evaluated on 3 classic datasets and compared with classic unlearnable examples, demonstrating that it works best across datasets (Table 2), transfers best from RN-18 to other models (Table 3), works the best across adversarial training (Table 4), and across other defense methods (Table 5). Overall, I am impressed with how consistently this approach works.\n2. The fact that these unlearnable examples work so well lend credence to their hypothesis that mutual information plays a key role (though, this can be improved and is discussed in weaknesses section)\n3. Good ablations show that the similarity term in their loss is more important than the distance term (Table 6)."}, "weaknesses": {"value": "1. Motivation: The author's argument around linear separability is that \"we find that linear classifiers trained on UEs can achieve certain generalization...interpreting UEs merely as linear shortcuts does not fully account for why such examples result in worse generalization\". But prior studies like [1] study the linear separability of **perturbations**, not of the poisoned images. The language implies these linear models were not trained on just perturbations.\n2. Using Table 1, the authors compute a correlation between the MI gap and Acc Gap of 0.78. This is good but in order to really show that \"effective unlearnable examples **always decrease** mutual information\" other alternatives should also be considered. For example: one could consider the linear separability of the perturbation (e.g. train a linear layer on perturbations) and correlate that to the Acc Gap. Is that correlation higher or lower? This would be a start (and necessary since the authors talk about linear separability in motivation), but it would be nice to have another metric if possible too. The authors could use prior work to find other \"explanations\" for why UEs work.\n3. L251 \"the reduction of MI is the primary cause\". I think I agree that MI could be a cause, but to show it is the primary cause the authors will need to consider alternatives (See previous point above).\n4. Regarding ISS [3], more details need to be given as to how the evaluation was performed. ISS paper proposed JPEG and grayscale as separate augmentations that broke a lot of unlearnable methods. I am particularly interested in an evaluation of *just ISS* (different JPEG compression qualities must be tried: 0.9, 0.8, 0.7, etc) instead of all the other \"defenses\" (cutout, cutmix, mixup, OP, etc.) because JPEG was shown to be so effective.\n\nTypos:\n- \"correlation score be 0.7818\" -> \"of 0.7818\"\n\n[1] Availability attacks create shortcuts. Yu et al., 2022\n\n[3] Image shortcut squeezing: Countering perturbative. Liu et al. 2023"}, "questions": {"value": "1. Above all, my suggestion is to work on the weaknesses described above.\n2. I don't understand the message/point of Figure 1. For example, what is the histogram-based estimator estimating? The paragraph starting at L206 says that Fig. 1 is \"To further ensure the confidence of MI estimation\" but how? Should be described at least at a high level - what does this figure add?\n3. The MI-UE noise in G.2. looks interesting. I wonder if these are linearly separable: if one takes the perturbations only (for each class) and trains a linear classifier on the perturbations, what accuracy is achieved? \n4. I'm curious what the train loss / test accuracy curves for MI-UE look like when training on CIFAR-10. In particular, I wonder if there are points during training when test accuracy peaks like some poisons do in [2]\n\n[2] What can we learn from unlearnable datasets?, Sandoval-Segura et al., 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VLSTB7xoP6", "forum": "HwTfgBHT1T", "replyto": "HwTfgBHT1T", "signatures": ["ICLR.cc/2026/Conference/Submission2203/Reviewer_L5MP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2203/Reviewer_L5MP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760740927311, "cdate": 1760740927311, "tmdate": 1762916135309, "mdate": 1762916135309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to generate unlearnable examples better. The authors propose to reduce the mutual information between the original examples and the generated data. The paper claims that the method is supported by a theorem. Extensive experimental results are presented.\n\nMy first concern is the novelty and significance of reducing the mutual information between original examples and generated data to generate unlearnable examples. The same idea has been in [1]. A large volume of literature is on similar methods based on KL divergence.\n\nSecond, the theory relies on an unjustified assumption that the poison distribution is \"is close to a Gaussian mixture distribution,\" which makes the theory unconvincing.\n\nIn the experiments, many settings are not justified; e.g., \"we set the total poison budget at 8/255,\" \"a step size of 0.2/255,\" \"the step size of PGD be 0.4/255,\" etc. I would suggest that the authors give results on the hyperparameter sensitivity.\n\nI didn't find how many trials the test run. If the reported results are from a single run, I would like to question their credibility. From my experiences, the results likely have considerable randomness. I would suggest the authors report (a) averages and (b) standard deviations of multiple (ideally over 5) trials.\n\nThe presentation can be improved. \n\n[1] Wang, B., Tian, J., Wang, X., Yuan, X. and Li, J., 2025. Reversible Unlearnable Examples: Towards the Copyright Protection in Deep Learning Era. IEEE Transactions on Circuits and Systems for Video Technology."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Please see above."}, "weaknesses": {"value": "Please see above."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hcfsTMprbA", "forum": "HwTfgBHT1T", "replyto": "HwTfgBHT1T", "signatures": ["ICLR.cc/2026/Conference/Submission2203/Reviewer_nUBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2203/Reviewer_nUBW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774706362, "cdate": 1761774706362, "tmdate": 1762916135121, "mdate": 1762916135121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach for generating unlearnable examples, focusing on mutual information (MI) reduction as the mechanism behind their effectiveness. The authors analyze the relationship between MI reduction and the unlearnability of examples, providing both theoretical and experimental results. They propose a new method called Mutual Information Unlearnable Examples (MI-UE), which aims to reduce MI by optimizing the covariance between poisoned and clean features. The paper presents experiments comparing MI-UE to state-of-the-art methods and demonstrates its superior performance, particularly in reducing model generalization ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a interesting perspective on unlearnable examples, introducing MI reduction as the key mechanism. This approach is novel and provides a theoretical understanding of how UEs work, going beyond empirical heuristics.\n\n- The paper includes theoretical grounding, demonstrating a connection between MI reduction and unlearnability. The experiments are well-designed, comparing MI-UE with several baseline methods and across different model architectures.\n\n- The paper is well-written, with clear definitions, logical flow, and accessible explanations. Complex concepts like MI and covariance reduction are explained systematically."}, "weaknesses": {"value": "- The paper acknowledges the challenges in optimizing MI and covariance, but the proposed solution still relies on a relatively simple optimization process. Further exploration into alternative optimization strategies could strengthen the paper's impact.\n\n- The bi-level optimization lacks sufficient theoretical analysis. Given that first-order gradient-based methods for solving bi-level optimization problems face well-known convergence challenges in non-convex scenarios, the absence of theoretical or empirical investigation diminishes the soundness of the method. It would be beneficial for the authors to substantiate the convergence of the proposed method through theoretical analysis or empirical validation.\n\n- Lack of novelty in experimental setups. While the theoretical contribution is novel, the experiments are primarily based on standard benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet-subset), which are commonly used in related work. This reduces the perceived novelty of the empirical evaluation.\n\n- While the authors claim that MI-UE significantly outperforms existing methods, the computational complexity of the proposed method is not fully discussed. It would be useful to have a more detailed analysis of how MI-UE scales with large datasets and models."}, "questions": {"value": "1. Could the authors elaborate on the scalability of MI-UE when applied to large datasets or deep models beyond the tested architectures? (e.g. LLMs)\n\n2. How does MI-UE perform in terms of convergence? Does MI-UE deliver a superior computation–utility trade-off compared with existing methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FrYMe08yG0", "forum": "HwTfgBHT1T", "replyto": "HwTfgBHT1T", "signatures": ["ICLR.cc/2026/Conference/Submission2203/Reviewer_i275"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2203/Reviewer_i275"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986528926, "cdate": 1761986528926, "tmdate": 1762916134853, "mdate": 1762916134853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why unlearnable examples (UEs) succeed at preventing unauthorized model training and introduces a mutual-information (MI)–based perspective to explain and enhance UE effectiveness. The authors empirically show that effective UEs consistently reduce the mutual information between clean and poisoned representations, and that deeper models suffer greater generalization degradation when trained on such data. \nBuilding on this insight, they propose MI-UE, a new poisoning method that explicitly minimizes intra-class feature covariance and maximizes cosine similarity among poisoned features. Across CIFAR-10/100 and ImageNet-subset, MI-UE outperforms prior UE approaches under standard and adversarial training, and demonstrates improved transferability to various architectures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel theoretical lens: Introduces mutual-information reduction as a unified explanation for UE effectiveness, filling a conceptual gap in a field often driven by heuristics.\n\n2. Strong empirical evidence: Comprehensive experiments across datasets, models (CNNs and ViTs), and defense scenarios support claims.\n\n3. Clear practical contributions: Proposed MI-UE method is simple, effective, and broadly transferable across architectures.\n\n4. Good analysis of model depth effect: Demonstrates that deeper models are more vulnerable to UEs, which aligns with empirical observations and provides useful insight.\n\n5. Solid ablations: Shows benefit of MI-driven optimization beyond simple feature clustering."}, "weaknesses": {"value": "1. Limited analysis of failure modes and defenses.\nWhile MI-UE achieves strong performance under many settings, it still degrades against certain tailored UE defenses (e.g., ISS, AVA) and does not universally dominate all specialized defense configurations. The paper acknowledges this but does not deeply examine why MI-UE fails in these cases, what properties of MI-based poisoning are being countered, or whether failure arises from optimization constraints, feature collapse dynamics, or assumptions about representation geometry. A more systematic characterization of failures and conditions under which MI-UE breaks down would strengthen the contribution.\n\n2. High computational overhead and unclear scalability.\nThe proposed approach introduces bi-level optimization with inner-loop shadow model updates and iterative PGD steps, which may impose substantially higher compute cost than classic error-minimization-based UEs. \nAlthough effective, this cost may limit usability in large-scale settings (e.g., ImageNet or web-scale self-supervised pipelines) where training even a single model is expensive. The paper does not report runtime cost, memory overhead, or scalability curves relative to baselines. It would be better to assess such practicality for real-world privacy preservation at large data scale."}, "questions": {"value": "Please respond to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vKjFExzWy2", "forum": "HwTfgBHT1T", "replyto": "HwTfgBHT1T", "signatures": ["ICLR.cc/2026/Conference/Submission2203/Reviewer_uCnz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2203/Reviewer_uCnz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125140679, "cdate": 1762125140679, "tmdate": 1762916134575, "mdate": 1762916134575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}