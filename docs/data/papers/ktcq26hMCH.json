{"id": "ktcq26hMCH", "number": 5163, "cdate": 1757858144523, "mdate": 1759897991125, "content": {"title": "RESA: Bringing Back What Sparse Attention Ignores with Residual Estimation", "abstract": "Large Language Models (LLM) have gained significant attention.\n    KV cache, stored to avoid quadratic complexity of attention, becomes a bottleneck due to the demands for long-context.\n    Sparse attention (SA) has been proposed to address this by only selecting critical KVs for attention, which may degrade model quality in less sparse scenarios.\n    To improve quality, rather than selecting more KVs, this paper reveals another perspective by estimating the contributions of remaining KVs, called Residual Estimation. \n    We find that attention logits (before softmax) exhibit substantial redundancy due to its inherent low-rank nature.\n    We perform Singular Value Decomposition (SVD) on logits matrix in prefilling and find the spectral dominance of principal singular value and its linearly scaling property with sequence length.\n    These imply that increasing sequence length leads to replication-like logits growth with significant redundancy.\n    However, it is impossible to perform SVD at each decoding step in practice due to its heavy costs.\n    To this end, we propose RESA, a training-free framework compensating SA's output with an estimated low-rank prior of logits.\n    RESA introduces (i) a Prior Estimator that derives a prior distribution from a typical query as a rank-1 approximation at the end of prefilling, and (ii) an Online Aggregator that fuses the prior with SA at each decoding step via lightweight scaling and merging.\n    Besides, we further show that RESA's effect comes from priors being used as attention bias for knowledge injection.\n    Extensive experiments show that without extra overheads, RESA improves model quality by up to 26\\% across various tasks with the same KV budget compared to state-of-the-art.\n    Moreover, RESA maintains the same quality with up to 33.2\\% KV budget reduction and 1.23$\\times$ attention throughput improvement.", "tldr": "We propose RESA to compensate the results of sparse attention for a more accurate output.", "keywords": ["Sparse Attention", "Attention Redundancy", "Low-rank Approximation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/040056a501ad230fa1be11f559946bc4b311035d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes RESA (Residual Estimation for Sparse Attention), a novel, training-free framework to improve the accuracy of sparse attention (SA) during LLM inference. The authors argue that standard SA methods, which select a small subset of KVs and ignore the rest, suffer from accuracy degradation by effectively zeroing out the contribution of unselected tokens.\n\nRESA's core idea is to *estimate* the contribution of these ignored KVs rather than discarding them. This is motivated by the empirical observation that the attention logit matrix ($QK^T$) is inherently low-rank and is dominated by its principal (rank-1) singular value, which accounts for a significant portion (40-50%) of the total energy.\n\nInstead of performing a costly SVD, RESA proposes a lightweight approximation for this rank-1 component. This approximation is used by a \"Prior Estimator\" to compute a global \"prior output\" ($O_{Est}$) once at the end of the prefill stage. Then, at each decoding step, an \"Online Aggregator\" merges this pre-computed $O_{Est}$ with the output of a standard SA method (e.g., Quest, ArkVale). The paper presents a key algorithmic contribution in the form of a \"delta merging\" formula (Eq 5), which allows this aggregation to be performed in $O(|\\mathcal{I}|)$ time—the same asymptotic complexity as the original SA method—by cleverly rescaling and combining the pre-computed and online components.\n\nExperiments across multiple models (Llama 3.1, Mistral) and benchmarks (RULER, LongBench) demonstrate that RESA consistently improves the accuracy of existing SA methods, or alternatively, allows for a significant reduction in the KV budget (up to 33.2%) while maintaining the same accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novel and Valuable Perspective:** The core idea of \"residual estimation\" is a significant conceptual strength. It reframes the problem of information loss in SA and provides a new, orthogonal axis for improvement.\n2.  **Strong Empirical Motivation:** The SVD analysis in Section 3.2 (Figure 2) is clear, convincing, and provides a strong, data-driven justification for why a low-rank (specifically, rank-1) approximation is a sensible approach.\n3.  **Algorithmically Efficient Design:** The \"delta merging\" technique in Section 4.3 is a key technical contribution. Deriving a formula (Eq 5) that correctly merges the pre-computed prior with the online sparse logits, all while maintaining the $O(|\\mathcal{I}|)$ asymptotic complexity of vanilla SA, is non-trivial and essential for the method's practicality.\n4.  **Training-Free and General:** The fact that RESA requires no re-training and can be applied as a wrapper around existing SA methods (Quest, ArkVale, etc.) makes it highly general and easy to adopt.\n5.  **Consistent Positive Results:** The experiments compellingly show that RESA provides a clear benefit, either by improving accuracy at a fixed budget (Tables 1, 2) or by enabling significant budget reduction (up to 33.2%) for a fixed accuracy target (Figure 7b).\nMarkdow"}, "weaknesses": {"value": "1.  **Missing Derivation for the Core Approximation (Eq 2):** This is the paper's most significant weakness. The \"Prior Estimator\" is built entirely on the approximation $q_{i}\\cdot K^{T}\\approx\\mu_{Q}\\cdot K^{T}+(q_{i}-\\mu_{Q})\\cdot\\mu_{K}^{T}$. The paper provides no derivation for this formula or theoretical justification for why it should approximate the rank-1 SVD component $\\sigma_{1}u_{1,i}v_{1}^{T}$. It is simply presented, and its validity is only asserted empirically (Figure 4). This leaves a critical gap in the paper's technical story.\n2.  **Speculative RoPE Analysis:** The connection made in Section 5.2 between the $\\mu_Q$/$\\mu_K$ terms and the \"low-frequency semantic carriers\" of RoPE (Figure 5b) is interesting but highly speculative. It reads as a post-hoc justification rather than a core part of the method's design, and it's not strictly necessary to validate the method's performance.\n3.  **Ambiguity in \"Overhead\":** The paper claims \"without extra overheads\" and \"maintains the same complexity $O(|\\mathcal{I}|)$ as SA.\" While asymptotically true for the *decoding step*, this glosses over:\n    * (a) The $O(L \\cdot d)$ computation and $O(L)$ softmax for $O_{Est}$ at the end of prefill (though this one-time cost is likely amortized and acceptable).\n    * (b) The *constant factor* overhead of the delta-merge (Eq 5) at *every* decoding step. The paper shows throughput gains *after* reducing the budget (Fig 7b), but it never shows a direct, per-token latency comparison of SA vs. SA+RESA at the *same* budget."}, "questions": {"value": "1.  Could the authors please provide a derivation or at least a more formal justification for Equation 2? How is the expression $\\mu_{Q}\\cdot K^{T}+(q_{i}-\\mu_{Q})\\cdot\\mu_{K}^{T}$ formally linked to the principal SVD component $\\sigma_{1}u_{1,i}v_{1}^{T}$? \n2.  To clarify the true overhead, what is the measured, per-token wall-clock latency (at the *same* KV budget, e.g., 5%) for a baseline like Quest versus Quest+RESA? This would quantify the constant-factor overhead of the Online Aggregator.\n3.  What is the additional *memory* overhead of RESA? The $O_{Est}$ vector (size $d$) and the prior scores $S$ (size $L$) must be stored for every head and every layer, correct? How does this extra storage compare to the size of the KV cache itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tv1FFsf41z", "forum": "ktcq26hMCH", "replyto": "ktcq26hMCH", "signatures": ["ICLR.cc/2026/Conference/Submission5163/Reviewer_5DL8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5163/Reviewer_5DL8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291765211, "cdate": 1761291765211, "tmdate": 1762917918110, "mdate": 1762917918110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel and well-motivated approach to improving sparse attention (SA) mechanisms by leveraging the low-rank nature of attention logits. The proposed method, RESA, is training-free and introduces a lightweight residual estimation framework that compensates for the contributions of unselected key-value pairs. The idea is innovative and supported by solid theoretical and empirical analysis. However, the evaluation could be strengthened by more comprehensive comparisons and deeper ablations to better understand the method's generality and limitations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a fresh perspective on the redundancy in attention logits due to their inherent low-rank structure, supported by singular value decomposition (SVD) analysis and visualizations. This offers a principled foundation for residual estimation in sparse attention.  \n- RESA is a training-free framework that effectively captures low-rank priors during prefilling and integrates them during decoding via a lightweight online aggregation mechanism. This makes it practical and easy to deploy.  \n- The method demonstrates consistent improvements across multiple models and benchmarks (RULER and LongBench), with significant gains (up to 26% on certain tasks) under the same KV budget.  \n- The authors further interpret RESA’s mechanism as a form of attention bias for knowledge injection, opening up interesting directions for training-time optimizations."}, "weaknesses": {"value": "1. **Comparison with Recent SA Methods**:  \n   The paper primarily compares against Quest, ArkVale, and an Ideal Top-K baseline. However, several recent and relevant SA methods such as SnapKV, HeadKV, PSA, and Ada-KV are not included in the main evaluation. It would be beneficial to see how RESA performs when integrated with these more recent approaches, especially on the same benchmarks.\n\n2. **Efficiency Evaluation**:  \n   While the method claims to be lightweight, there is no concrete reporting of prefilling time, inference latency, or memory footprint. Including such metrics would help readers better assess the practical overhead of RESA, especially in resource-constrained settings.\n\n3. **Evaluation on Complex Tasks**:  \n   The current experiments focus heavily on retrieval and simple QA tasks. To better demonstrate the robustness of RESA, we suggest evaluating on more challenging benchmarks such as InfiniteBench or complex reasoning tasks from lm-eval, which involve few-shot learning, long-context reasoning, or multi-step inference.\n\n4. **Prefilling Overhead**:  \n   The authors mention that the prior estimation is computed once at the end of prefilling, but the computational and memory cost of this step is not quantified. A brief analysis of the prefilling stage overhead would help clarify the trade-offs.\n\n5. **Ablation Studies**:  \n   Although a hyperparameter \\( \\lambda \\) is introduced to control the prior's influence, more thorough ablations are needed. For instance:  \n   - How does the choice of rank-1 approximation affect performance? Would a higher-rank prior help?  \n   - Are there alternative weighting schemes (e.g., recency-based) that could improve the prior?  \n   - How sensitive are the scaling factors \\( \\alpha \\) and \\( \\beta \\) in Eq. 5?  \n   These experiments would provide deeper insights into the design choices and their impact."}, "questions": {"value": "See the weaknesses section for details"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RlAFSLy28b", "forum": "ktcq26hMCH", "replyto": "ktcq26hMCH", "signatures": ["ICLR.cc/2026/Conference/Submission5163/Reviewer_WmEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5163/Reviewer_WmEf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807412971, "cdate": 1761807412971, "tmdate": 1762917917713, "mdate": 1762917917713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RESA, a training-free residual estimation framework that complements sparse attention by estimating the contributions of the key–value pairs that sparse attention discards. The key insight is that attention logits are strongly low-rank, allowing a rank-1 prior (computed from the mean query and key vectors) to approximate the global structure of attention. During decoding, RESA merges this prior with sparse attention outputs through an rescaling and delta update, keeping computational cost unchanged. Experiments on LongBench and RULER show that RESA can improve task accuracy and reduce KV budget while maintaining comparable performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a simple yet effective approach to compensate for the information loss in sparse attention using a low-rank residual estimation. It is training-free, computationally efficient, and integrates seamlessly with existing sparse attention mechanisms. The empirical results are consistent across different models and benchmarks, demonstrating tangible performance and throughput improvements. The low-rank analysis provides interesting insight into the structure of attention logits, which may inspire future theoretical or practical extensions."}, "weaknesses": {"value": "1. The theoretical justification is incomplete. The proposed estimator is intuitive and empirically validated but lacks formal analysis or error bounds. Providing theoretical guarantees or conditions for the rank-1 prior to hold would greatly strengthen the claims.\n\n2. The computation of the prior under chunked prefill remains ambiguous. It is unclear whether the prior is aggregated across chunks or derived from the last chunk only, and the paper lacks a detailed runtime breakdown to justify the claimed negligible overhead.\n3. The performance is sensitive to the hyperparameter λ, which controls the influence of the prior. Larger λ values can hurt summarization or comprehension tasks, but there is no guidance on automatic or adaptive tuning strategies.\n4. Some tables contain minor formatting errors and inconsistencies. (Table 7 and Table 8, \"quert\" should be \"quest\")"}, "questions": {"value": "1. How is the prior computed when prefilling is chunked? Is the prior accumulated across all chunks, or only computed at the final chunk? Can you quantify the prefill-time overhead relative to standard sparse attention?\n\n2. Have you explored an adaptive λ schedule, for example, using entropy or sparsity of the attention logits as a signal to modulate prior strength dynamically?\n\n3. Can you compare RESA with low-rank inference baselines such as a learned rank-1 bias or lin-attention variant to clarify its novelty relative to existing low-rank approaches?\n\n4. Summarization and comprehension tasks appear to degrade at larger λ. Can you analyze failure cases and propose mitigation strategies beyond manually reducing λ?\n\n5. For numerical stability, since α and β are computed via log-sum-exp normalization, did you observe pathological heads or tokens with extreme α/β? Were any clipping or smoothing techniques applied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LN8LfdsKUY", "forum": "ktcq26hMCH", "replyto": "ktcq26hMCH", "signatures": ["ICLR.cc/2026/Conference/Submission5163/Reviewer_WqTC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5163/Reviewer_WqTC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839104493, "cdate": 1761839104493, "tmdate": 1762917917267, "mdate": 1762917917267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose RESA, a training-free residual estimation framework. THe framework alleviate the drawback bring by sparse attention by estimating the attention scores that discard by sparse attention. \n\nThey getting insights from the attention logits are naturelly low-rank. So they use rank-1 prior to approximate the global structure of attenition. \n\nDuring the decoding stage, RESA use residual attention estimation through rescaling, delta update, and final merge it into final attention computation. \n\nThe experiments conducted on LongBench and RULER have shown that RESA improve the tasks accuracy and reduce the memory budget, specifically the KV budget."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The RESA introduced by this paper is quite simple and effective. It is training-free, computational friendly, and can plug-and-play into any existing framework. \n\nThe empirical results also validate the effectiveness of this method. The low-rank analysis of the residual attention score is also quite interesting and inspiring."}, "weaknesses": {"value": "1 More theoretical analyze is needed. The author intuitively and empirically discover that the residual attention score has rank-1 nature. It would be better if author can provide theoretical analysis for why those residual attention score has those kind of rank-1 nature. \n\n2 The author claimed the overhead is negligible under chunked prefill. Does the prior aggregate across chunks or derived from only the last chunk? Could author explain more about this ?\n\n3 How to automatically chose the hyperparameter $\\lambda$ . The performance is highly depend on the hyperparameter. So is there a automatic way, or recipe for how to choose the hyper-parameter lambda? \n\n4 Some table contains typos. (Table 7 and Table 8, \"quert\" should be \"quest\")"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LN8LfdsKUY", "forum": "ktcq26hMCH", "replyto": "ktcq26hMCH", "signatures": ["ICLR.cc/2026/Conference/Submission5163/Reviewer_WqTC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5163/Reviewer_WqTC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839104493, "cdate": 1761839104493, "tmdate": 1763680567393, "mdate": 1763680567393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key limitation of Sparse Attention (SA) methods used in LLM inference. While SA accelerates decoding by selecting and computing only a small subset of \"critical\" Key-Value (KV) pairs, it completely ignores the contribution of all unselected KVs, leading to a degradation in model quality.\n\nThe authors propose RESA (Residual Estimation for Sparse Attention), a novel, training-free framework that \"brings back what sparse attention ignores\". Instead of just ignoring the unselected KVs, RESA estimates their collective contribution (the \"residual\") and adds it back to the sparse attention output, thereby improving accuracy.\n\nThe core insight is that the attention logits matrix (pre-softmax $QK^T$) is inherently low-rank. The authors show via Singular Value Decomposition (SVD) that the effective rank is upper-bounded by the head dimension (e.g., 128) regardless of sequence length (e.g., 8k), and that the principal singular value (rank-1) is dominant, accounting for 40-50% of the matrix's energy. SA methods excel at capturing fine-grained sparse peaks but discard this global low-rank structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality and Significance: The paper's core idea is highly original. The vast majority of SA research focuses on \"how to select the top-k KVs more accurately or efficiently\". This work introduces a completely orthogonal and complementary perspective: \"how to efficiently account for the $N-k$ KVs that were not selected\". This reframes the problem from simple sparsity to a more robust sparse + low-rank approximation. The resulting improvements are very significant: boosting accuracy (up to 26%) or saving 33.2% of the KV budget for free is a major practical contribution.\n\n2. RESA's effectiveness is demonstrated across multiple models (Llama-3, Mistral, LWM) and multiple challenging long-context benchmarks (RULER, LongBench)."}, "weaknesses": {"value": "Please see Questions below."}, "questions": {"value": "1. Prior Estimator Cost: Could you please quantify the one-time wall-clock latency (in ms) of the \"Prior Estimator\" step at the end of prefill? How does this cost scale as sequence length L increases from 8k to 128k?\n\n2. Could you please provide a precise definition for the \"typical query\" $\\mu_Q$? Is it computed as the simple average of all $L$ query vectors in the prefill context? How sensitive is the model's accuracy to this choice (e.g., mean of all queries vs. just the last query)?\n\n3. The paper's motivation (low-rank logits) applies to the prefill stage as well (Figure 2a). While the proposed mechanism is for the decoding stage, could a similar \"residual estimation\" idea be used to accelerate the $O(N^2)$ prefill stage itself?\n\n4. The error of the logit estimation (Fig 6b) shows a clear periodic, mean-reverting pattern. You note this \"temporal structure\" is worth exploring. Do you have a hypothesis for what causes this periodicity? Does it align with any known properties of RoPE or the model architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m2FIKDHeAR", "forum": "ktcq26hMCH", "replyto": "ktcq26hMCH", "signatures": ["ICLR.cc/2026/Conference/Submission5163/Reviewer_2Wxj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5163/Reviewer_2Wxj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954904966, "cdate": 1761954904966, "tmdate": 1762917916972, "mdate": 1762917916972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}