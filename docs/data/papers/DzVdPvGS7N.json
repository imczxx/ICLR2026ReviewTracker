{"id": "DzVdPvGS7N", "number": 21420, "cdate": 1758317375039, "mdate": 1759896922945, "content": {"title": "Sparse Deep Additive Model with Interactions: Enhancing Interpretability and Predictability", "abstract": "Recent advances in deep learning highlight the need for personalized models that can learn from small or moderate samples, handle high-dimensional features, and remain interpretable. To address this challenge, we propose the Sparse Deep Additive Model with Interactions (SDAMI), a framework that combines sparsity-driven feature selection with deep subnetworks for flexible function approximation. Unlike conventional deep learning models, which often function as black boxes, SDAMI explicitly disentangles main effects and interaction effects to enhance interpretability. At the same time, its deep additive structure achieves higher predictive accuracy than classical additive models. Central to SDAMI is the concept of an Effect Footprint, which assumes that higher-order interactions project marginally onto main effects. Guided by this principle, SDAMI adopts a two-stage strategy: first, identify strong main effects that implicitly carry information about important interactions; second, exploit this information—through structured regularization such as group lasso—to distinguish genuine main effects from interaction effects. For each selected main effect, SDAMI constructs a dedicated subnetwork, enabling nonlinear function approximation while preserving interpretability and providing a structured foundation for modeling interactions. Extensive simulations with comparisons confirm SDAMI’s ability to recover effect structures across diverse scenarios. Applications in reliability analysis, neuroscience, and medical diagnostics further demonstrate its versatility in addressing real-world high-dimensional modeling challenges.", "tldr": "Sparse Deep Additive Model with Interactions (SDAMI)", "keywords": ["Interpretable deep learning", "feature selection", "High-dimensional data analysis"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/944a1a1d084af247e2096c666572034af855ab97.pdf", "supplementary_material": "/attachment/a5f2bf436b762d8e38d8abd08601d22065672dea.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Sparse Deep Additive Model with Interactions (SDAMI), a framework that separates main effects (a sum of univariate functions) from interaction effects for greater interpretability. The paper focuses on the challenging case of small n and large p, where the risk of overfitting is significant for conventional deep learning models.  The authors introduce the concept of Effect Footprint: interaction effects leave a signal that can be detected via screening. They propose a mathematical formulation of the problem and a three-step approach to solving it:\n- feature selection via a sparse additive screening procedure \n- group lasso to distinguish between main and interaction effects\n- training deep learning models to model main and interaction effects.\n\nThe experimental results are on synthetic and real-world datasets. They also provide theoretical results:\n- Conditions under which the effect footprint disappears, and screening may fail\n- Asymptotic convergence results for SDAMI (feature selection and convergence of the probability estimator)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes the new concept of effect footprint and provides theoretical results to justify the use of screening and illustrate its limitations.\n- Asymptotic convergence results provide a theoretical justification for the framework.\n- The paper shows how SDAMI can be used to extract interpretable signals and understand the contribution of main and interaction effects."}, "weaknesses": {"value": "1) The writing of the paper could be improved:\n- The contribution section is somewhat repetitive (the list is a summary of the text just above).\n- Line 157: Is “additive” the right word? I think “main effect” is more appropriate.\n- Line 214: the word “partitioning” seems to indicate that there cannot be the same feature in the main and interaction effects.\n- In the contribution, you mention that the simulations focus on p>>n. However, line 310 indicates that n is always greater than p (n >= p = 150).\n\n2) As you mentioned in the limitations, the theoretical results are asymptotic. However, your motivation, small n and large p, corresponds to a very different setting.\n\n3) The synthetic simulations correspond to very simple and somewhat too perfect cases (very similar to your formulation of the problem).\n\n4) Experiments on real-world datasets are interesting. However, the number of datasets and baselines is very limited. You should include results on other Generalized Additive Models baselines such as NAM (Neural Additive Models) [1], GAMI-Net [2], and NODE-GAM [3]. \n\n5) Your formulation of the problem (equation 2) attempts to find a sum of main effects and an interaction model that takes all “interaction features” as input. The formulation is unclear because the model itself becomes a black box model (because of the interaction “model”). Are you then focusing on second-order interactions only?\n\n6) Your framework corresponds to a case of Generalized Additive Models with interactions. You should indicate this in the paper and mention more literature on the subject.\n\n[1]: Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, and\nGeoffrey Hinton. Neural additive models: Interpretable machine learning with neural nets. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing\nSystems, 2021.\n\n[2]: Zebin Yang, Aijun Zhang, and A. Sudjianto. Gami-net: An explainable neural network based on generalized\nadditive models with structured interactions. Pattern Recognit., 120:108192, 2021\n\n[3]: Chun-Hao Chang, Rich Caruana, and Anna Goldenberg. NODE-GAM: Neural generalized additive model\nfor interpretable deep learning. In International Conference on Learning Representations, 2022."}, "questions": {"value": "- Could you test your method on other baselines (NAM, GAMI-Net, NODE-GAM) and other datasets? The datasets from these papers would be interesting to study.\n- How much computing time does your method require compared to the other baselines?\n- Are you focusing solely on second-order interaction effects? It would be good to clarify this further in the paper. \n- Do you think you can derive non-asymptotic guarantees for your framework? This would be more interesting for the case you are interested in (small n, large p)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gZMAhRxJQ2", "forum": "DzVdPvGS7N", "replyto": "DzVdPvGS7N", "signatures": ["ICLR.cc/2026/Conference/Submission21420/Reviewer_HNEK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21420/Reviewer_HNEK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857009471, "cdate": 1761857009471, "tmdate": 1762941757627, "mdate": 1762941757627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed work attempts to introduces a new neural additive model through equations (1) and (2), but after later implicit assumptions, reduces to training a neural network with feature selection.  The work then introduces a novel two-stage learning structure based on the idea of first learning the one-dimensional main effects before selecting the higher-order interactions using heredity.  The work then applies another training procedure to the specific neural network and applies to synthetic and real-world datasets. Recovery of additive models is demonstrated on several synthetic datasets and competitive performance on three real-world datasets is shown compared to LASSO, DNN, and fSpAM."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The structural equation in Equation 1 has the potential to be novel if the authors do not implicitly assume that M⊂I\n- The learning procedure has the potential to encourage soft pruning of irrelevant features"}, "weaknesses": {"value": "- Glosses over closely related NAM work as irrelevant due to lack of interaction modeling [1] and then ignores other NAM works which include interactions [2,3,4,5].\n- The effect footprint does not seem to be novel and seems to be the same as existing works applying heredity.\n- There are very few baselines compared against and relatively few datasets are used for comparison\n- The impact of the important hyperparameters introduced by the paper are not explored \n- Figure 1 and Figure 4 are not given much explanation in the main paper\n\n\n[1] Shiyun Xu, Zhiqi Bu, Pratik Chaudhari, and Ian J Barnett. Sparse neural additive model: Interpretable deep learning with feature selection via group sparsity\n[2] Zebin Yang, Aijun Zhang, Agus Sudjianto.  GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions\n[3] Chun-Hao Chang, Rich Caruana, Anna Goldenberg.  NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning\n[4] James Enouen and Yan Liu. Sparse interaction additive networks via feature interaction detection and sparse selection.\n[5] Minkyu Kim, Hyun-Soo Choi, and Jinho Kim. Higher-order Neural Additive Models: An Interpretable Machine Learning Model with Feature Interactions"}, "questions": {"value": "- What are the hyperparameter choices used for neural network training and other learning algorithms?\n- What is the novel aspect of the effect footprint compared with existing works using heredity?\n- Why is the implicit assumption M⊂I used?  What prevents this from reducing to the feature selection case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w2Y82djEkV", "forum": "DzVdPvGS7N", "replyto": "DzVdPvGS7N", "signatures": ["ICLR.cc/2026/Conference/Submission21420/Reviewer_uvQg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21420/Reviewer_uvQg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886445720, "cdate": 1761886445720, "tmdate": 1762941757245, "mdate": 1762941757245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method for creating interpretable deep learning models when the number of training examples is much smaller than the number of dimensions.  The key idea is to characterize marginal effects of interactions and use those effects to guide the construction of a network with dedicated components for main effects and variable effects.  The paper provides theoretical support for the method, as well as extensive simulations to illustrate its utility in practice relative to baseline methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "A key concept proposed here is the notion of the \"Effect Footprint,\" in which \"higher-order interactions leave detectable marginal signatures on main effects.\"  This concept seems like a useful abstraction.\n\nI like how the related work section clearly lays out the two different categories of methods for addressing this problem, and then goes on to explain how the proposed method brings these two categories together.\n\nThe empirical analyes using simulations and real data are impressive, showing that SDAMI outperforms baseline methods both in terms of predictive accuracy and in accurately recovering the true underlying structure of the data."}, "weaknesses": {"value": "I think the tension described in line 44 should be restated to include the obvious candidate class of models here, which are not \"classical deep models\" but classical linear models.  It's not obvious why we should bother with deep models when n << p.  These models are alluded to in line 77, though of course there are classical sparse models that do allow for higher-order associations.  These should be discussed.\n\nAlong those lines, the introduction doesn't do a great job of explaining how pre-training can address the problems outlined here; i.e., when n << p, but when you have some other collection of (potentially unlabeled) n' examples, where n' > p.\n\nFigure 2 is confusing.  Based on the description in lines 197-209, it seems like there are two sets of inputs: main effect variables of size p and footprint variables of size q.  I think the input layer should have 1..p going into main effect blocks, and then 1...p+q going into the interaction block.\n\nLine 258: It seems like you also have to address the other failure mode, when a variable leaves a misleading (i.e., false positive) footprint trace.\n\nI thought Figure 5 was pretty difficult to interpret.  We are basically asked to take the authors word for it that these panels \"reveal synergistic patterns consistent with cortical pooling.\"\n\nMinor points:\n\nline 120: Missing cite for SpAM.\n\nLine 215: Give a cite for Mallow's C_p.\n\nLine 432: diabete -> diabetes"}, "questions": {"value": "What would Figure 5 look like if the method did not work well?\n\nWhy does Figure 2 show q variables going into the main effect subnetworks?  Where are the footprint variables in this figure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JGbRtXZoya", "forum": "DzVdPvGS7N", "replyto": "DzVdPvGS7N", "signatures": ["ICLR.cc/2026/Conference/Submission21420/Reviewer_Mqyj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21420/Reviewer_Mqyj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932690984, "cdate": 1761932690984, "tmdate": 1762941756989, "mdate": 1762941756989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposes a deep learning model that is effective for datasets with a small number of samples and a large number of features. While conventional methods often suffer from overfitting and difficulties in feature selection, the proposed approach enhances model explainability and interpretability by exploiting feature sparsity and identifying interaction features. In particular, a novel measure called \"Effect Footprint\" is introduced to extract features that have no main effects but play an important role through interaction effects. This measure provides a framework for identifying indirect dependencies among features that were previously difficult to detect. Furthermore, the theoretical properties of the proposed model are analyzed, and its effectiveness is demonstrated through numerical experiments using both synthetic and real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This study addresses the important challenge of improving the explainability and interpretability of deep learning models. In particular, it introduces a unique framework that sparsely connects multiple subnetworks, enabling clear identification of feature contributions even in high-dimensional datasets.\n\nThe study proposes a novel measure called \"Effect Footprint\" to detect features that have no main effects but play important roles through interactions. This theoretical framework seems highly original, as it enables the identification of indirect feature dependencies that were difficult to capture with conventional methods.\n\nThe effectiveness of the proposed method is demonstrated not only through numerical experiments using synthetic and real-world datasets but also through theoretical analysis. The effort to validate the method from both theoretical and experimental perspectives enhances the credibility of this research."}, "weaknesses": {"value": "The overall description in the paper is somewhat ambiguous, and the mathematical and algorithmic details of the proposed method are not sufficiently presented. In particular, the selection process of the feature set with interactions, $\\mathcal I$, as well as the specific computation and utilization of the \"Effect Footprint\", are unclear, raising concerns about the reproducibility and comprehensibility of the method.\n\nWhile the paper claims that identifying interaction features enhances the interpretability of the model, there appears to be no explicit mechanism for extracting such features from the actual model structure. I feel that there is a noticeable gap between the claims made in the Abstract and Introduction and what is actually demonstrated in the main text.\n\nThe necessity of using deep learning for small-sample, high-dimensional problems is not well justified. Numerous prior studies, particularly those based on linear models and traditional statistical approaches, have addressed interaction effects (e.g., the works listed below). The lack of discussion or comparison with these studies weakens the overall persuasiveness of the paper:\n\n  Choi et al. Variable selection with the strong heredity constraint and its oracle property. JASA, vol.105, no.489, 2010.\n\n  Bien et al. A lasso for hierarchical interactions. Annals of Statistics, vol.41, no.3, 2013.\n\n  Nakagawa et al. Safe pattern pruning: An efficient approach for predictive pattern mining. KDD 2016."}, "questions": {"value": "The paper's main contribution lies in the introduction of the Effect Footprint; however, it is unclear how this measure is specifically computed and utilized within the model. According to the Algorithm in the Appendix, it appears that SpAM is simply applied, but it is not well explained why this procedure enables the identification of Interaction-Only features. A more concrete theoretical and intuitive explanation would be helpful.\n\nThe proposed method constructs a deep learning model that handles interaction terms, but it remains unclear by what criteria or procedure the important interaction terms are finally extracted. The authors are requested to clarify how the network structure or learned weights are related to the selection of significant features.\n\nThe paper also presents a theoretical discussion on feature selection consistency, but it is questionable whether this property is independent of the learning performance of the deep neural network. It seems that the argument implicitly assumes that the model is correctly trained; therefore, such assumptions and dependencies should be explicitly described."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9iwFHNORoH", "forum": "DzVdPvGS7N", "replyto": "DzVdPvGS7N", "signatures": ["ICLR.cc/2026/Conference/Submission21420/Reviewer_kDzt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21420/Reviewer_kDzt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037381724, "cdate": 1762037381724, "tmdate": 1762941756576, "mdate": 1762941756576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}