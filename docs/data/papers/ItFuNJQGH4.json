{"id": "ItFuNJQGH4", "number": 21455, "cdate": 1758317727825, "mdate": 1759896921170, "content": {"title": "$p\\textrm{-less}$ Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding", "abstract": "Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p\\textrm{-less}$ sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p\\textrm{-less}$ sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p\\textrm{-less}$ sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy.\nFinally, we provide analyses to highlight the benefits of $p\\textrm{-less}$ through qualitative examples, case studies, and diversity assessments.", "tldr": "P-less Sampling: A parameterless sampling strategy grounded in information theory, where the truncation threshold adapts to the entire token probability distribution, is bounded and valid, and dynamically adjusts with temperature.", "keywords": ["LLM", "decoding", "sampling", "truncation", "inference", "information-theoretic", "information-theory", "hyperparameterless", "hyperparameter-free", "entropy", "entropy-aware", "distribution-aware", "adaptive", "efficient", "generation"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c4a68003ebb7d583b5fca1f5d2d1f4bfc9f5b17.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **p-less sampling**, a new way to generate text from large language models. It does not need any hyperparameters. Instead, it decides what tokens to sample using information from the whole probability distribution. The method is based on R√©nyi entropy.The authors test p-less on math, logic, and creative writing tasks. It works better and more stable than top-p, min-p, or mirostat, especially at high temperatures. It is also faster because it skips sorting steps. Results show higher accuracy, shorter outputs, and smoother behavior across models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents p-less sampling, a hyperparameter-free decoding strategy for large language models grounded in information theory. A key strength of this approach is its adaptability: it dynamically adjusts truncation thresholds based on the full token probability distribution, which allows it to maintain robustness and quality across a wide range of temperatures. Empirical results show that p-less consistently outperforms existing methods such as top-p, min-p, and mirostat in both reasoning and creative writing tasks, while also improving inference-time efficiency. The method‚Äôs theoretical grounding in R√©nyi entropy and its parameter-free nature make it both principled and practical for diverse LLM applications"}, "weaknesses": {"value": "the  evaluation focuses mainly on a limited set of benchmark tasks and may not fully represent more complex or real-world text generation scenarios. The theoretical framing, while elegant, could benefit from better explanations and a polished presentation. Additionally, while p-less shows competitive or superior accuracy, its reduced diversity at higher temperatures could limit its utility for tasks where creative variation is essential. Further exploration into broader model architectures, languages, and user-centric evaluations would strengthen the generalizability and practical applicability of the method. Moreover, the discussion of prior and related work is somewhat superficial, see below. Generally, th e methods leans hard on the model‚Äôs own probabilities. If calibration drifts, the threshold can wobble and cut good tokens. The independence assumption is neat, but the real loop is messy. Order-2 entropy is elegant, yet it can miss rare tails that matter. Human evaluation is small and narrow; long-form and multilingual remain thin. O(N) sounds great, but the authors do not discuss constant factors and memory traffic."}, "questions": {"value": "1. **Threshold mechanics:** Plot admitted-set size vs. entropy/temperature for synthetic distributions (unimodal, bimodal, near-uniform) to show how $L[P]=\\sum p_i^2$ scales with $|V|$ and $\\tau$.\n\n2. **Efficiency:** You claim $O(N)$ per step (no sorting). Share kernel-level profiles (CPU/GPU) and memory footprints vs. top-p/min-p across sequence lengths 32‚Üí2048.\n\n3. **Robustness:** Where does p-less fail? List concrete failure patterns (e.g., long-range coreference, tool-use prompts) and the corresponding entropy/threshold traces.\n\n4. **Diversity vs. quality:** At $\\tau \\ge 1.5$, p-less sometimes shows lower diversity. Ablate p-less vs. p-lessnorm with n-gram repetition, MAUVE, and human Likert scores on open-ended prompts.\n\n5. **Relation to prior work:** Could you compare p-less to **Adaptive Contrastive Search (ACS)** and to **GUARD**, noting that GUARD is also hyperparameter-free? Contrast theory, temperature regimes, and compute costs.\n   ‚Äì ACS paper: [https://aclanthology.org/2024.findings-emnlp.885/](https://aclanthology.org/2024.findings-emnlp.885/)\n   ‚Äì GUARD paper: [https://arxiv.org/abs/2508.20757](https://arxiv.org/abs/2508.20757)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "90uV0v8HsC", "forum": "ItFuNJQGH4", "replyto": "ItFuNJQGH4", "signatures": ["ICLR.cc/2026/Conference/Submission21455/Reviewer_zLU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21455/Reviewer_zLU6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919325787, "cdate": 1761919325787, "tmdate": 1762941786412, "mdate": 1762941786412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new threshold truncation method that does not require hyper parameter tuning that performs well on QA tasks and is stable at different temperatures. The method for choosing the threshold is theoretically motivated by Renyi entropy, and intuitively by the idea of estimating the probability of sampling the ground truth from the estimated distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide a truncation sampling method which requires no hyper parameter tuning. \nThe truncation threshold has a nice theoretical motivation.\nThe writing is easy enough to understand."}, "weaknesses": {"value": "In Table 1 there is no baseline for greedy sampling. In math tasks and other question answering tasks, low entropy decoding methods like greedy/beam search often excel. It could be that your method is simply lower entropy than the compared methods and so does better. This is exacerbated by the fact that the other methods have hyper parameters that may not be properly set, leading to lower scores. This is somewhat confirmed in Table 7 where we see that the lower the temperature, the higher the (and closer to one another) the scores get.\n\nIn terms of efficiency, I would expect to see a standard error on the mean and a significance test to support the claim that the method is faster in practice. As it stands, the standard deviations are quite large compared to the difference in means, making it unclear how large the gains are. A $O(n\\log n)$ to $O(n)$ speedup seems like a marginal gain, and some threshold methods like $\\eta$ and $\\epsilon$ sampling can get away without sorting via rejection sampling.\n\nL378 It is not clear why the shorter generations appear for p-less sampling, or if this would hold for other models, so this point seems somewhat tenuous.\n\nI cannot tell from the method description whether the probabilities used to calculate the threshold are taken from before or after temperature adjustment, or if the method is theoretically connected to doing that. If the thresholds are applied before temperature, then it would be important to compare this method to threshold methods that reject tokens based on the temperature 1 distribution. As a concrete example, take top-p sampling, choose the valid tokens based on temp-1 probs, then renormalize the truncated distribution based on the temperature 2 probabilities. This is effectively what they do in the paper [Top-nùúé: Eliminating Noise in Logit Space for Robust Token Sampling of LLM](https://aclanthology.org/2025.acl-long.528/) (Tang et al., ACL 2025).\n\nRelatedly, it seems that your method does not allow significant increases in diversity based on temperature. The diversity measures stay fairly close to their low-temperature values regardless of temperature."}, "questions": {"value": "Refer to my weaknesses section for needed clarifications. \n\nI understand how L[P] estimates the probability randomly sampling the ground truth, but I'm not totally clear on why this should be interpreted as a threshold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QUkKvfJsFm", "forum": "ItFuNJQGH4", "replyto": "ItFuNJQGH4", "signatures": ["ICLR.cc/2026/Conference/Submission21455/Reviewer_QeAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21455/Reviewer_QeAK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956154425, "cdate": 1761956154425, "tmdate": 1762941786026, "mdate": 1762941786026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces p-less sampling, a truncation sampling strategy in which (1) a threshold is determined by summing the squared likelihoods of all words‚Äô probabilities, and (2) words below that threshold are truncated ‚Äì their probabilities are set to zero, and all accepted words‚Äô probabilities are renormalized. The paper makes connections between this strategy and various things, including the likelihood that a randomly selected token is also the ‚Äúcorrect‚Äù (under an implicit assumption that there‚Äôs a correct next token) next token. This sampling strategy has no diversity/accuracy tradeoff hyperparameter, like the p in top-p.\n\nThe paper supports this sampling strategy with a range of experiments, including mathematics and other verifiable problems ‚Äî GSM8k, GPQA ‚Äì and a writing prompts open-ended evaluation. They run experiments testing truncation under different temperatures ‚Äì 0.5, 1, 1.5, 2 ‚Äì finding that their method performs well and does not decay in quality under high temperatures like the other methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper has a nice sampling heuristic intuition, and makes some interesting mathematical connections, all based on the premise that there‚Äôs a single correct token at each step, and we should be truncating tokens relative to the likelihood of getting that token correct. It‚Äôs nice to have a method without a hyperparameter one has to mess with, as well. Different tasks require different p in top-p for best performance, and basically all the other sampling methods require some hyperparameter optimization.\n\nThe range of experiments performed here is nice (though, there are some concerning issues I go over in the weaknesses section.)\n\nCore to this work seems to be the removal of the assumption that we‚Äôre trying to recreate some ‚Äútrue‚Äù potentially high-entropy distribution that the language model defined; we‚Äôre instead trying to perform something much closer to an argmax-seeking procedure, and we‚Äôre admitting some error in the search space which admits multiple possible generations. I‚Äôm putting this in the strengths, but I think it‚Äôs really key that the authors discuss this more."}, "weaknesses": {"value": "I think the choice of hyperparameters for the baseline methods, and the lack of a greedy decoding baseline, make for a weaker paper. This is important because p-less sampling seems (from plots like Figures 8,9) to be a relatively stringent truncation strategy, truncating much more than, say, top-p for p=0.9, and behaving much closer to greedy decoding. The old ‚Äúneural text degeneration‚Äù problems with greedy decoding just aren‚Äôt around nearly as much with modern strong instruction-tuned models, so not comparing to greedy decoding is a real problem. More evidence for this, while the numbers in the main text compare p-less sampling to very high-diversity sampling(say, top-p with p=0.9), which is not what one would use in that setting. In Table 7 where they try a more stringent truncation hyperparameter for top-p (p=0.4) it‚Äôs really not clear that p-less sampling is any better than top-p. In summary, despite the nice properties of p-less sampling not having a hyperparameter, one should compare to reasonable hyperparameters for the other methods for each problem (low p for math, high p for open-ended story generation.)\n\nLikewise, one should compare to greedy decoding, which likely will work for all the settings compared. Other methods in the past have tried to measure distribution coverage, not just quality ‚Äì i.e., through MAUVE. I‚Äôm not saying the authors need to run MAUVE experiments, but top-p and its variants explicitly allow for a range of tradeoffs between coverage and quality, and p-less sampling seems to perform better than (high-diversity hyperparameter settings of) top-p; since we‚Äôre not measuring any notion of coverage, it‚Äôs necessary to compare p-less sampling to beam search and greedy decoding.\n\nI don‚Äôt think the author-run human study should be included in the final draft. Authors working on a sampling strategy get a strong feeling for the kinds of texts that their algorithm generates, and so it‚Äôs not possible to have a blind study here if it‚Äôs run by the authors."}, "questions": {"value": "- Can you give an intuitive reason why p-less sampling can be faster than epsilon-sampling? Contrastively to what‚Äôs stated in line 357, epsilon-sampling does not sort the vocabulary. Finding the argmax, likewise, does not require sorting. \n- Why is it good to generate shorter sequences via sampling compared to training a model to prefer shorter sequences, or having a sampling algorithm that better-replicates whatever the length distribution the model defines?\n- The authors haven‚Äôt motivated why we care about sampling at temperature=2. Increasing the temperature and then truncating is just sampling from a more-uniform version of the accepted-token set. One could instead first truncate with top-p sampling or any other method and then increase the temperature over that truncated set. The accuracy numbers don‚Äôt ever seem to be better at temperature=2. Could you explain why we want this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NXVaJDDv9R", "forum": "ItFuNJQGH4", "replyto": "ItFuNJQGH4", "signatures": ["ICLR.cc/2026/Conference/Submission21455/Reviewer_1KUh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21455/Reviewer_1KUh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022605004, "cdate": 1762022605004, "tmdate": 1762941785637, "mdate": 1762941785637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}