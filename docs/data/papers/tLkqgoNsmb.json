{"id": "tLkqgoNsmb", "number": 22605, "cdate": 1758333412997, "mdate": 1759896857202, "content": {"title": "StatQAT: Statistical Quantizer Optimization for Deep Networks", "abstract": "Quantization is essential for reducing the computational cost and memory usage of deep neural networks, enabling efficient inference on low-precision hardware. Despite the growing adoption of uniform and floating-point quantization schemes, selecting optimal quantization parameters remains a key challenge, particularly for diverse data distributions encountered during training and inference. This work presents a novel statistical error analysis framework for uniform and floating-point quantization, providing theoretical insight into error behavior across quantization configurations. Building on this analysis, we propose iterative quantizers designed for arbitrary data distributions and analytic quantizers tailored for Gaussian-like weight distributions. These methods enable efficient, low-error quantization suitable for both activations and weights. We incorporate our quantizers into quantization-aware training and evaluate them across integer and floating-point formats. Experiments demonstrate improved accuracy and stability, highlighting the effectiveness of our approach for training low-precision neural networks.", "tldr": "", "keywords": ["quantization aware training", "float quantization", "uniform quantization", "fp4", "int4"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c8369b4ff570e07f1edd6d10953907881df25aa.pdf", "supplementary_material": "/attachment/f2246465eb746ab259262545705dab11e641176e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes utilizing analytic quantizers (for weights) and iterative one (for activation) in quantization aware training.\nGiven the statistics (of weights during QAT), the analytic quantizer for the target data format like FP4 chooses the best quantization parameters (clipping threshold and zero) from pre-computed ones assuming Gaussian distributions.\nThe experiments show SOTA results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experimental results are quite impressive.\nEspecially, it looks nice that the proposed method is effective in tensor-wise cases where quantization gets simpler than in channel-wise ones."}, "weaknesses": {"value": "Pre-computing quantization parameters based on known distributions and utilizing them (based on the statistics obtained) during QAT is not new.\nFor instance, in [1], the authors propose an idea called statistics-aware weight binning (SAWB) which quantizes weights based on Gaussian assumption during QAT.\n\n[1] Bridging the Accuracy Gap for 2-bit Quantized Neural Networks (QNN), https://arxiv.org/pdf/1807.06964\n\nCompared with [1], one key difference will be FP4 since it was not available when [1] was published.\nIt would be nice to discuss what are 'fundamentally' new problems, in case of FP4, for statistics-aware quantization.\nProposing a statistics-aware method for FP4 based on a floating point-aware analytical analysis (which looks similar to the one in [1] in principle) looks rather weak to me as novelty \nsince only number representations are different between Int [1] and Float while the main idea of utilizing the pre-computed results (optimal quantization parameters for a given distribution) looks the same."}, "questions": {"value": "It would be nice to explain how the proposed work advances QAT in terms of analytic analyzer \nin comparison with existing QAT works based on analytic quantizers and Gaussian (or pre-determined) distribution assumption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v7DZyStVpL", "forum": "tLkqgoNsmb", "replyto": "tLkqgoNsmb", "signatures": ["ICLR.cc/2026/Conference/Submission22605/Reviewer_2cs9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22605/Reviewer_2cs9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740107598, "cdate": 1761740107598, "tmdate": 1762942300044, "mdate": 1762942300044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors discuss weight and activation quantization in NNs. They analyze the errors of different quantization choices (uniform, FP) and derive a k-means style approach to find appropriately-spaced cluster centers given observed data (weights & activations). They derive iterative and - if data (weights) can be assumed normally distributed - analytical update rules for those cluster centers. \nThe authors perform QAT experiments with their proposed methods and compare against baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work is well-motivated, the problem of QAT is highly relevant to deploy large models efficiently and the exposition is well-done.\nConsidering the latency of the quantizer is an important contribution, especially since interative and analytic perform comparably."}, "weaknesses": {"value": "The only weakness I can identify is the limited empirical evaluation. Looking at e.g. ParetoQ, there is an opportunity to study <4bit quantization - as well as the opportunity to include more models & datasets to improve the robustness of this work."}, "questions": {"value": "I believe in EQ 7 the lower limit of the second integral should be $s(N-5/2) + z$\nsubstituting $t_{N-2}=s(N-2-1/2+z)=s(N-4/2-1/2+z) = s(N-5/2+z)$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7iW4FKE0zp", "forum": "tLkqgoNsmb", "replyto": "tLkqgoNsmb", "signatures": ["ICLR.cc/2026/Conference/Submission22605/Reviewer_mHUY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22605/Reviewer_mHUY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854707145, "cdate": 1761854707145, "tmdate": 1762942299783, "mdate": 1762942299783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes StatQAT, a statistical framework for choosing quantization parameters for both uniform and FP schemes.\nThe core is an analytical decomposition of quantization error into clipping + stepping terms, iterative updates for scale/shift and updates that precompute an optimal clipping constant via SNR maximization, then set per-tensor scales in one shot.\nVarius experiments on llms as well as resnet18."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "### Clear error model\nfor both uniform and floating-point quantizers, including a tractable stepping-error mixture for FP formats (Sec. 3.2, Eqs. 15–16), which is rarely derived cleanly in QAT papers. The SNR-based perspective is useful (Fig. 1, p. 7). \n\n### Simple, practical update rules\nAlso shown in appendix how faster the analitical compared to iterative\n\n## Broad Evaluation\nTwo models with two sizes make the impact broad across models and datasets."}, "weaknesses": {"value": "1. The iterative uniform update is essentially a **constrained 1-D k-means** (Lloyd–Max) with fixed spacing , and the FP version mirrors that idea on a scaled FP grid. This feels like a straightforward specialization of classic quantizer optimization to the uniform/FP grids rather than a new algorithmic principle.\n\n2. The “first closed-form analytic solution for FP quantizers in QAT” (Abstract, Intro) seems overstated. optimal **C** is chosen by numerical search on a derived SNR expression (eq 22). Please tone down the claim or clarify precisely what is closed-form.\n\n3. **Positioning vs. prior work** - Prior art already optimizes clipping/scale during QAT (e.g., LSQ/PACT) and analyzes error (Lloyd–Max, optimal clipping). you list  them in the intro, but the manuscript should articulate what is new beyond applying an SNR-driven selection and  demonstrait that a single-step analytic update matches iterative methods under QAT constraints.\n\n4. **Clarity & presentation** - Section 2 is long background, Section 3 sometimes re-derives known pieces (for example, uniform steping error). While i understand that it's improtant for deriving your decomposed error, you can simply reffer to literuture or move to the appendix (similar to some of the solved known integrals you've put there), as it take major part of the paper lenghts.\n**Figures** should be PDF and not images, so the quality remain high when zooming, as currently the quality of the figures are poor.\n\n5. Reproducability - I failed to find detalis about the datased you used for fine-tuning the LLMs, with details about it (some QAT uses only the data without labels, or with artificial labels, the size matter a lot since it's much heavier on the saving side of your non iterative suggestion).\n\n6. Where does the gain come from? I would be happy to see more ablations that explain that.. the SNR curves (Fig. 1) suggest robustness/benefit from better clipping. It would help to add per-layer scale histograms, SNR per layer, or an analysis of failure cases vs min-max/LSQ to better understand the effects."}, "questions": {"value": "I incorporated my questions in the Weaknesses section.\n\nBut regardless, can you provide even a very small scale correlation study why ablation over resnet and cifar translate into LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mQWMO3RWsR", "forum": "tLkqgoNsmb", "replyto": "tLkqgoNsmb", "signatures": ["ICLR.cc/2026/Conference/Submission22605/Reviewer_JukX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22605/Reviewer_JukX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762074493188, "cdate": 1762074493188, "tmdate": 1762942299349, "mdate": 1762942299349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates a statistical framework for optimizing quantization parameters for uniform and floating-point quantization and proposes two quantization procedures, namely, iterative quantizers for arbitrary data distributions (activations) via alternating updates of scale and shift for each time step, and analytic quantizers using a closed form solution for Gaussian-like weight distributions. These methods can be integrated into Quantization-Aware Training (QAT). The method is validated using ResNet, MobileLLM, and Llama 3.2 and the quantizers are compared to minmax and normal quanitzers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Nice to see inclusion of both integer and floating-point formats, and both activations and weights.\n2. The writing is clear and easy to follow.\n3. The statistical foundation for quantizer design is interesting."}, "weaknesses": {"value": "1. Limited evaluation - the models evaluated are very limited, not clear if the technique will scale to larger models used today beyond ResNet-18 and larger than 3B parameter LLMs. Broader domain coverage could strengthen claims.\n2. Limited comparison - I don't see comparisons with more recent SOTA techniques like SpinQuant for PTQ, LSQ for QAT, etc. \n3. Runs on CIFAR-10 can be noisy, I don't see any characterization of noise."}, "questions": {"value": "1. The labels used for Fig 2 (left) seem to be incorrect.\n2. Can you comment on how this can be combined with existing QAT techniques like LSQ, and include comparisons with some of the techniques mentioned above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "86jn5dNdH5", "forum": "tLkqgoNsmb", "replyto": "tLkqgoNsmb", "signatures": ["ICLR.cc/2026/Conference/Submission22605/Reviewer_XqCB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22605/Reviewer_XqCB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762215113054, "cdate": 1762215113054, "tmdate": 1762942299042, "mdate": 1762942299042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}