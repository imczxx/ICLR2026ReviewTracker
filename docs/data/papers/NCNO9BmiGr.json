{"id": "NCNO9BmiGr", "number": 14307, "cdate": 1758232578431, "mdate": 1759897377695, "content": {"title": "Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models", "abstract": "Data visualizations are vital components of many scientific articles and news stories. Current vision-language models (VLMs) still struggle on basic data visualization understanding tasks, but the causes of failure remain unclear. Are VLM failures attributable to limitations in how visual information in the data visualization is encoded, how information is transferred between the vision and language modules, or how information is processed within the language module? We developed $\\texttt{FUGU}$, a suite of data visualization understanding tasks, to precisely characterize potential sources of difficulty (e.g., extracting the position of data points, distances between them, and other summary statistics). We used $\\texttt{FUGU}$ to investigate three widely used VLMs ( LLaMA-3.2, LLaVA-OneVision , and InternVL3). To diagnose the sources of errors produced by these models, we used activation patching and linear probes to trace information flow through models across a variety of prompting strategies. We found that some models fail to generate the coordinates of individual data points correctly, and these initial errors often lead to erroneous final responses. When these models are provided with the correct coordinates, performance improves substantially, suggesting that the downstream mathematical reasoning steps performed in the language module are sound. Moreover, even when the model generates an incorrect response, the correct coordinates can be successfully read out from the latent representations in the vision encoder, suggesting that the source of these errors lies in the vision-language handoff. We further found that while providing correct coordinates helps with tasks involving one or a small number of data points, it generally worsens performance for tasks that require extracting statistical relationships across many data points (e.g., correlations, clusters). Fine-tuning models on $\\texttt{FUGU}$ also fails to yield ceiling performance. These findings point to fundamental architectural constraints in current VLMs that might pose significant challenges for reliable data visualization understanding.", "tldr": "We leverage tools from mechanistic interpretability to investigate the causes of failure by three vision-language model architectures on basic data visualization understanding tasks.", "keywords": ["Interpretability", "vision-language models", "multimodal reasoning", "chart understanding", "mathematical reasoning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f80a3fb64360780e1585befe95870861335f6bed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper seeks to investigate the bottleneck in Vision-Language Models (VLMs) for data visualization understanding. To this end, the authors introduce FUGU, a data visualization understanding benchmark consists of synthetic scatter plots and 9 tasks (5 basic and 4 ensemble). They evaluate three modern VLMs with mechanistic interpretability techniques (activation patching and linear probes). Based on the evaluation results, they argue that the bottleneck is caused by the vision-language handoff."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe flexibility and granularity (controlled visual attributes, and variable plot complexity) of FUGU enable precise probing of model weaknesses. The experiments are thorough, comparing three contemporary VLMs and systematically evaluating their performance with different visualization complexity and tasks.\n•\tThe paper is well written and clearly introduces the motivation and experimental findings."}, "weaknesses": {"value": "- Limited generalizability of the dataset\nBecause FUGU comprises only scatter plots, the generalizability of the paper’s conclusions is limited. Real-world data visualizations are more complex and diverse, incorporating various chart types (e.g., bar, line, pie charts), dense textual annotations, legends, and visual noise. The bottlenecks identified in this study may be specific to FUGU. It is important to evaluate whether the bottlenecks vary by visualization types and scale with both visualization complexity (from simple charts to rich infographics) and question complexity (from basic queries to complex data insight queries such as those in ChartQAPro).\n\n-  The nature of the identified bottleneck seems task-dependent\nThe claim of the VLM architectural bottleneck is weakened by the fact that this bottleneck task-dependent. In Section 4.6, one experiment demonstrates that providing ground-truth coordinates harms performance on ensemble tasks. This suggests that the language part of VLMs probably faces a bottleneck in this context. Thus, the bottleneck is not a fixed architectural limitation but a task-specific capacity mismatch between model components and task demands.  This weakens the claim of a fundamental architectural limitation and shifts the contribution from identifying a fundamental architectural constraint to characterizing a capacity limitation in current VLMs. This is a more modest result than the paper suggests.\n\n- Results on fine-tuning do not fully support the claim\nIn Sections 3.1 and 4.7, the authors claim that FUGU is extremely difficult for current VLMs and cannot be fully solved directly through fine-tuning. However, the fine-tuned InternVL3 14B performs very well on simple tasks (count 100, position 99.2). This suggests, first, that the bottleneck may shift after fine-tuning. Second, if a 14B model can solve simple tasks with fine-tuning, larger fine-tuned models may also handle additional FUGU tasks."}, "questions": {"value": "- Why are the values of Gemini 2.5 Pro in Tables 2 and 3 missing? \n- Considering the good performance of Gemini 2.5 Flash in Tables 2 and 3, is FUGU not difficult for Gemini 2.5 Pro?\n- line 150: VLMS -> VLMs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vFZAXwwMFA", "forum": "NCNO9BmiGr", "replyto": "NCNO9BmiGr", "signatures": ["ICLR.cc/2026/Conference/Submission14307/Reviewer_oyTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14307/Reviewer_oyTB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739568169, "cdate": 1761739568169, "tmdate": 1762924747252, "mdate": 1762924747252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FUGU, a controlled benchmark to diagnose where VLMs fail to understand basic charts. FUGU focuses on highly controlled, synthetic scatter plot tasks designed to probe models' abilities to extract and reason about quantitative information at multiple levels of complexity. The paper systematically evaluates three widely used VLM architectures (LLaMA-3.2, LLaVA-OneVision, InternVL3) using behavioral analysis, causal interventions (activation patching), and linear probes to track information flow and identify bottlenecks. The key findings implicate the hand-off between vision encoders and language modules as a major source of failure, rather than the underlying representation or reasoning capacities of either module in isolation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The dataset design is simple, clear, and well-controlled, allowing for a clean analysis of specific model behaviors and error sources\n+ The combination of multiple analysis methods provides different aspects to assess model behavior\n+ The conclusion brought by linear probs experiments is interesting and convincing"}, "weaknesses": {"value": "- The dataset scope is narrow. FUGU focuses on scatterplots with limited data points, no occlusion, and fixed glyphs. Real charts usually include bars/lines, partial occlusion, diverse scaled axes, and additional legends/annotations. It remains unclear whether the experimental results and conclusions would still hold for other chart types or in-the-wild data."}, "questions": {"value": "- How robust are the conclusions if the scatterplot contains more points, and the axes are non-integer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9jQnMWTOP9", "forum": "NCNO9BmiGr", "replyto": "NCNO9BmiGr", "signatures": ["ICLR.cc/2026/Conference/Submission14307/Reviewer_4DYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14307/Reviewer_4DYu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850944415, "cdate": 1761850944415, "tmdate": 1762924746668, "mdate": 1762924746668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why modern vision-language models (VLMs) fail to understand data visualizations, arguing it's unclear if the fault lies in the visual encoder, language module, or their interface. To diagnose this, the authors introduce FUGU, a new benchmark of fine-grained \"unit tests\" for chart-understanding capabilities like extracting data points or calculating statistics. Evaluating three VLMs (LLaMA-3.2, LLaVA-OneVision, InternVL3), they find poor performance that degrades rapidly as the number of data points increases. Using diagnostic tools like linear probes and activation patching, the authors pinpoint a key bottleneck. They find that while the visual encoder does successfully capture low-level information (like coordinates), this information is \"scrambled\" or lost at the vision-language connector and in the early layers of the language model. This core architectural flaw, rather than a failure of visual perception, is the primary source of error and persists even after fine-tuning. The work's primary limitations are its focus on synthetic scatter/bar charts and the computational cost of its diagnostic methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces FUGU, a diagnostic benchmark designed to \"unit test\" the fine-grained capabilities of VLMs on data visualizations.\n2. Provides a clear and localized diagnosis for VLM failures, identifying the vision-language connector and early LM layers as the primary bottleneck.\n3. The work appears reproducible due to clear descriptions of the FUGU benchmark tasks and the diagnostic methods."}, "weaknesses": {"value": "* The FUGU benchmark's scope is currently narrow, focusing on synthetic scatter plots (Sec 3.1). This makes it unclear if the findings and the identified bottleneck generalize to other common chart families (e.g., bar charts, line graphs, histograms) or to more complex, real-world visualizations with varied aesthetics, occlusions, or multiple panels.\n* The paper's contribution is primarily diagnostic. While it successfully identifies the location of the information bottleneck (Sec 5.3), it does not proceed to propose or empirically test any architectural solutions or mitigation strategies to address this flaw.\n* The evaluation relies on a judge prompt and regex-based checks for scoring (Sec 4.1). While the high agreement rate is noted, the lack of a human-rated validation subset with inter-rater agreement means the robustness of the automated scoring is not fully confirmed.\n* The tasks within FUGU (e.g., counting, position, distance, mean) appear to rely heavily on a single core skill: accurately extracting all (x,y) coordinates. This might over-weight one specific failure mode rather than testing a diverse set of reasoning capabilities."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZnOkIwCi9e", "forum": "NCNO9BmiGr", "replyto": "NCNO9BmiGr", "signatures": ["ICLR.cc/2026/Conference/Submission14307/Reviewer_jg4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14307/Reviewer_jg4B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944412120, "cdate": 1761944412120, "tmdate": 1762924746277, "mdate": 1762924746277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why current vision-language models (VLMs) struggle with understanding data visualizations such as scatter plots. The authors introduce FUGU (Fundamentals of Graph Understanding) — a new diagnostic benchmark designed to systematically test foundational spatial and mathematical reasoning skills necessary for chart interpretation, including counting, locating points, measuring distances, finding extrema, and computing means. Further proposed tasks include correlation, cluster, function, outlier.\nUsing three representative VLMs (LLaMA-3.2, LLaVA-OneVision, and InternVL3), the study combines behavioral evaluation with activation patching and linear probing to trace information flow through model components."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear motivation and positioning:**  \n  The paper addresses a relevant and timely question regarding the ability of VLMs to understand charts and data visualizations. The research problem is articulated clearly, focusing on identifying where failures in chart understanding may originate.\n\n- **Well-scoped contributions:**  \n  The proposed FUGU tasks are thoughtfully designed and cover a useful range of chart-understanding skills (e.g., counting, coordinates, extrema). The use of both causal interventions and linear probes provides an informative way to study model behavior beyond surface-level accuracy.\n\n- **Comprehensive experimental setup:**  \n  The experiments include several representative models (e.g., LLaMA-3.2, LLaVA-OneVision, InternVL3) and explore different task conditions. The findings offer a plausible explanation that the vision–language interface poses a key challenge, supported by a range of analyses. The visualizations and ablations help illustrate the results.\n\n- **Clarity and reproducibility:**  \n  The paper is generally well-organized and provides sufficient implementation detail, including appendices that describe task construction and probe configurations. This level of transparency should support reproducibility and future research on multimodal reasoning."}, "weaknesses": {"value": "- **Limited coverage of visualization types:**  \n  The analysis focuses mainly on Cartesian point-based charts (e.g., line and bar charts), where positional relationships naturally reflect values. However, for non-Cartesian visualizations such as pie charts or radar charts, angular information is equally critical. The current framework does not appear to account for these cases, limiting its generality across broader visualization types.\n\n- **Insufficient consideration of visual encoder scale and capacity:**  \n  The conclusion that visual encoders preserve nearly 100% of the relevant information is based on a single encoder configuration. The impact of encoder size, architecture, and pre-training strategy on this finding is not examined. Incorporating comparisons across encoder scales would strengthen the validity of this conclusion.\n\n- **Limited mechanistic insight into the vision–language bottleneck:**  \n  While the work identifies the vision–language interface as the main source of degradation, the analysis does not delve into *why* this interface fails. Potential factors—such as attention bottlenecks, misalignment in cross-modal token fusion, or representational loss during projection—are not explored. A more detailed mechanistic investigation would make the diagnosis more robust."}, "questions": {"value": "- **Nature of the vision–language bottleneck:**  \n  The paper attributes the core failure to the vision–language interface, but the underlying mechanism remains unclear. Could the authors elaborate on what aspects of attention or projection layers may cause information loss at this stage? Additionally, if information is already lost at this layer, how do the observed performance fluctuations across subsequent layers arise?\n\n- **Encoder size and representation capacity:**  \n  The results suggest that positional information is fully preserved across tested vision encoders. How sensitive is this finding to encoder scale or architecture? Would smaller or differently pre-trained encoders yield similar preservation patterns?\n\n- **Scope of FUGU task diversity:**  \n  The benchmark focuses on scatter plots in Cartesian coordinates. Do the authors expect comparable failure behaviors in charts with non-Cartesian or hierarchical structures (e.g., polar plots, radar charts, treemaps)? Clarification on how the diagnostic approach would generalize to such cases would be helpful.\n\n- **Implications for future model design:**  \n  Since fine-tuning does not appear to resolve the bottleneck, have the authors considered alternative or hybrid architectural directions that could mitigate the issue? Insight into how these findings might inform future multimodal model design would strengthen the discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nUvmR5mdhi", "forum": "NCNO9BmiGr", "replyto": "NCNO9BmiGr", "signatures": ["ICLR.cc/2026/Conference/Submission14307/Reviewer_cLBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14307/Reviewer_cLBW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997019981, "cdate": 1761997019981, "tmdate": 1762924745843, "mdate": 1762924745843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}