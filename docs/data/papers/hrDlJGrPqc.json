{"id": "hrDlJGrPqc", "number": 20330, "cdate": 1758304833156, "mdate": 1759896983511, "content": {"title": "QC-Bench: What Do Language Models Know About Quantum Computing?", "abstract": "Language models increasingly interact with quantum computing content through theoretical exploration, paper summarization, and educational assistance, yet their factual accuracy on quantum computing concepts remains unmeasured. QC-Bench addresses this gap with 6,237 questions covering quantum algorithms, error correction, security protocols, circuit design, and theoretical foundations. We designed expert-level questions informed by over 200 peer-reviewed papers from four decades of quantum computing research to construct the benchmark. Evaluation across 31 models from OpenAI, Anthropic, Google, Meta, and others reveals strong performance on established theory contrasted with systematic failure on advanced topics such as quantum security and recent attack vectors. We compared model performance against quantum computing experts and practitioners who achieved scores ranging from 26.7\\% to 86.7\\%. Notably, 8 models outperformed the human expert average of 83.3\\%, yet all models struggled with questions about recent developments in advanced quantum computing topics. Top performers Claude Sonnet 4 and GPT-5 achieved 88\\% overall accuracy but drop to 76\\% on security questions. Cross-format testing shows models achieve high multiple-choice scores but struggle with generating coherent explanations without answer options, with some models dropping 20 percentage points. Multilingual testing revealed an interesting pattern: models consistently performed best in English, maintained reasonable accuracy in French (11.2\\% degradation), but showed notably larger performance drops in Spanish (16.2\\% degradation), indicating that quantum computing knowledge does not transfer uniformly across languages. As language models become integral to scientific workflows and even peer review processes where quantum computing research is evaluated, ensuring their domain accuracy is critical for the AI community. QC-Bench offers a reliable benchmark for developing and validating AI systems at the intersection of quantum computing and machine learning.", "tldr": "We introduce QC-Bench, a human-authored benchmark designed to evaluate large language models on core topics in quantum computing.", "keywords": ["Quantum Computing", "Large Language Models", "Model Reliability"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/adfbf3abb5bf0f39737d111164cf584398d6a92e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work studies how LLMs perform with quantum computing knowledge. The work generates thousands of questions with multiple choices using LLMs and then hundreds of true/false and open-ended questions. These questions are then posed to multiple well-known LLMs and 16 human subjects. The results show that some LLMs perform better than human experts, and most do not. Models especially perform really well in English as opposed to other languages."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work establishes a baseline for what current LLMs actually “know” about quantum computing concepts (e.g., superposition, entanglement, variational circuits, security, etc.).\n\nThe work actually undertakes a very large and extensive study. Significant effort was put into running the experimentation and surveys for this work.\n\nExcept for the weaknesses mentioned below, the methodology seems sound."}, "weaknesses": {"value": "The questions are generated by the LLMs, of course, storing the context of the questions. These LLMs are thus likely to get the questions generated by them right. This is because they are generating the multiple-choice questions such that there would only be one correct choice; thus, they know what the correct answer is. It would be much better to test them using human-generated questions.\n\nI think the human sample is too small to make any conclusive statements or draw any statistically significant evidence from the results. Specifically, there are too few \"experts\" to determine if the LLMs were better than the experts or not.\n\nThis kind of survey runs the risk of becoming stale before it is even published because of how fast LLM knowledge is moving. In fact, I'm pretty confident that the LLMs will be much better with quantum knowledge by the time this work is published. Just asking these questions to the LLMs helped improve their quality for the next time someone asks them. So in that sense, the work has little and fleeting \"research\" value."}, "questions": {"value": "Why were only 16 humans surveyed, and why were only 30 questions asked of them? Can this be expanded to get statistically significant results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9ibY3QNQHu", "forum": "hrDlJGrPqc", "replyto": "hrDlJGrPqc", "signatures": ["ICLR.cc/2026/Conference/Submission20330/Reviewer_U3tu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20330/Reviewer_U3tu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760461850572, "cdate": 1760461850572, "tmdate": 1762933788755, "mdate": 1762933788755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces QC-Bench, a large, domain-specific benchmark designed to evaluate LLM knowledge of quantum computing across seven areas. The benchmark contains three types of questions: multiple-choice, true/false, and open-ended. It comprises a human-authored core QC1000, a multilingual QC500 subset, and 4,400 additional questions mined from ~200+ papers using several LLMs. Manual rewriting and multi-stage filtering were applied to improve quality and avoid verbatim overlap. The authors evaluate 31 closed and open models and include a human baseline to contextualize results. Key findings include (1) top models score ~88–92% on basic topics but drop to ~76% on quantum security; (2) open-ended performance lags multiple-choice; (3) multilingual evaluation shows performance degradation in French and Spanish compared to English; (4) fine-tuning smaller models with LoRA gives mixed, modest improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Meaningful topics & substantial human effort. The authors plainly invested considerable manual labor to collect/curate items spanning core quantum computing areas with both foundational and advanced topics. \n2. Multi-faced evaluation framework. The inclusion of open-ended questions reveals reasoning gaps that MCQ/T-F obscure and helps with more nuanced evaluation. Multilingual analysis shows clear evidence of non-uniform cross-lingual transfer.\n3. Thorough benchmarking results. The experiments section runs many models and formats, providing a useful snapshot of the current landscape. Human baseline serves as a useful component to contextualize LLM performance.\n4. Figures and tables are nicely designed and informative."}, "weaknesses": {"value": "1. Limited difficulty as a benchmark. This is my biggest concern. The paper’s own results show SOTA models scoring very high on many categories, which limits diagnostic value and reduces competitiveness as a research benchmark. This contrasts with recent quantum code generation benchmarks that intentionally stress executable and semantic correctness (e.g., QCircuitBench, QuanBench), suggesting QC-Bench presently under-challenges frontier models.\n\n2. Insufficient dataset description. The main text should document design choices (topic taxonomy, difficulty calibration), collection pipeline (sources, authoring guidelines), and filtering criteria (duplicate removal, human inspection) with detailed explanations and concrete statistics. Regarding the filtering process, the paper mentioned \"removing duplicate questions, filtering irrelevant content, and conducting manual quality review\". Are all 4400 machine-mined questions examined by human researchers?  For open-ended items, please report grading criteria. Moreover, manual assessment hinders automatic verification procedure, which is impractical for standard benchmarking.\n\n3. Human baseline scope. The baseline uses 30 questions, which may be too few to support strong conclusions about “expert average” versus top models. Topic balance and item difficulty calibration for the chosen set are not detailed.\n\n4. Related-work coverage is too narrow. Beyond the two cited papers (QuantumLLMInstruct, GroverGPT), the paper omits several relevant benchmarks/datasets: \n\n   - Qiskit HumanEval: hand-curated tasks with canonical solutions & tests for QC SDK code generation.\n   - QuanBench: LLM-based quantum code generation across 44 tasks with functional correctness and quantum semantic equivalence.\n   - QCircuitNet: Large-scale datasets focusing on quantum algorithm design and circuit implementation with automatic syntax and semantic verification.\n   - Classical QC benchmarking suites: QASMBench, a low-level OpenQASM benchmark suite for NISQ evaluation. \n\n   While these works target at different goals from QC-Bench, it is necessary to discuss them to contextualize the contribution of QC-Bench.\n\n5. The benchmark is not open-sourced, which makes it hard to evaluate the quality of the data entries.  Consider releasing the full dataset or a small subset to enable reviewers' assessment.\n\n6. Writing can be improved. The abstract is tediously long. In the caption of Table 2, I believe it should be \"Accuracy above 90%\" instead of 95%."}, "questions": {"value": "See Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gzQ0BRUEek", "forum": "hrDlJGrPqc", "replyto": "hrDlJGrPqc", "signatures": ["ICLR.cc/2026/Conference/Submission20330/Reviewer_sSTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20330/Reviewer_sSTx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744565939, "cdate": 1761744565939, "tmdate": 1762933788186, "mdate": 1762933788186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce QC-Bench, the first LLM benchmark dataset for evaluating LLM knowledge of quantum computing. QC-Bench contains both human-authored and LLM-generated questions, using a variety of formats including multiple choice, true and false, and open-ended questions. The benchmark also includes 500 questions that have been translated into Spanish and French. \n\nThe authors evaluate a number of LLMs on QC-Bench, finding that newer, larger models outperform older and smaller models. Efforts are made to analyze LLM performance along a variety of axes, including question content. \n\nThe authors also show how to use QC-Bench for model fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Standard Benchmarking Approach: The paper checks many standard boxes for creating a benchmark, including the use of human and machine-generated questions grounded in the outputs of subject matter experts (SMEs).\n2. Multilingual Question Set: The inclusion of a multilingual question set enhances the accessibility and applicability of the benchmark across different language speakers.\n3. Diverse Question Formats: The benchmark incorporates open-form questions, allowing for a broader range of responses and insights into the capabilities of large language models (LLMs).\n4. Domain-Specific Focus: The QC-Bench dataset is built on a review of over 200 peer-reviewed research papers, ensuring that the questions are relevant to the field of quantum computing and not just general science topics.\n5. Human-Authored Evaluation Questions: The inclusion of 1200 human-authored evaluation questions adds credibility and relevance to the benchmark, as they are designed to reflect foundational questions and state-of-the-art research in quantum computing."}, "weaknesses": {"value": "1. Lack of Novelty in Benchmarking: The paper fails to advance the science of benchmarking, offering no new approaches or methodologies for evaluating LLMs in the context of quantum computing.\n2. Unclear Value Proposition: The benchmark raises questions about its actual value and whether it will encourage critical examination of LLM responses to quantum computing questions.\n3. Limited Analysis Capabilities: The benchmark does not enable uncertainty quantification or more in-depth analysis of responses beyond traditional scoring methods, limiting its utility for comprehensive evaluation (cf. https://arxiv.org/pdf/2411.00640).\n4. No Real-World Performance Correlation: There is a failure to tie performance on the benchmark to real-world performance or utility, which diminishes the relevance of the findings.\n5. Insufficient Data for Conclusions: The conclusions regarding model performance on basic versus advanced topics lack supporting data on the distribution of these concepts within each question category, making it difficult to critically assess the results. Moreover, it is unclear if conclusions such as \"they lack understanding of cutting-edge developments that define the current research frontier\" are truly supported by models performing less well on quantum security questions (a relatively niche sub-field of quantum computing).\n6. Questionable Relevance of Topics: The large focus on \"quantum security\" and lack of clear room for questions on noise characterization, quantum hardware, and quantum control questions whether the benchmark truly reflects the broader literature and raises concerns about the representativeness of the benchmark questions.\n7. Lack of Human Subject Details: The paper does not provide summary statistics on the human participants, making it difficult to evaluate the expertise of the human subjects involved in the study.\n8. Statistical Power Concerns: The administration of only 30 questions to 16 human subjects raises questions about the statistical power of the study and the reliability of the results.\n9. Absence of Ethical Review Information: There are no details regarding the human subject review board process, which is a significant oversight given the use of human participants in the research.\n10. Potential Bias in Grading: The paper does not address the possibility that human graders may have a generic preference for answers from larger models, regardless of the answers factual content, which could skew the evaluation results when comparing the performance of larger models to smaller models."}, "questions": {"value": "Can you provide clarity on what each subject area (e.g., quantum security, gates & circuits) covers?\n\nCan you provide more in-depth analysis supporting the claim that models perform better on well-established fundamentals and less well on cutting-edge research questions?\n\nHow can you enable a more statistically rigorous analysis of an LLM's capabilities using the QC-Bench (e.g., along the lines of https://arxiv.org/pdf/2411.00640)?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The study included 16 human survey respondents, but doesn't include any information about an ethical review process or, for example, institutional review board oversight."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8ByPZc9elG", "forum": "hrDlJGrPqc", "replyto": "hrDlJGrPqc", "signatures": ["ICLR.cc/2026/Conference/Submission20330/Reviewer_xnJY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20330/Reviewer_xnJY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941422647, "cdate": 1761941422647, "tmdate": 1762933787870, "mdate": 1762933787870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QC-Bench, a comprehensive benchmark for evaluating LLM's knowledge of quantum computing. The benchmark contains 6,237 questions across seven core topics of quantum computing and are collected from over 200 peer-reviewed papers. The authors evaluate more than 30 frontier LLMs and compare their performance against human experts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. I appreciate effort of including human quantum experts in both data curation and evaluation processes. \n\n2. The benchmark does a good job in balancing both fundamental quantum theory and more recent research. \n\n3. The paper is well-written."}, "weaknesses": {"value": "1. I think the paper fails to convince the reader the significance of the benchmark. First, it's unclear whether QC-Bench evaluates simple quantum knowledge retrieval or reasoning like a quantum scientist. Will the modern LLMs with web search abilities solve these questions by looking up information online? If yes, I am not sure that the benchmark will stand test of time. More critically, the paper does not convince me that \"acing in this benchmark will mean the model will be a good quantum scientist\".\n\n2. The memorization analysis is inadequate. The authors claim to address memorization concerns by testing if the model can reproduce the answers verbatim, but this approach has serious problems. it does not test whether models have memorized the concepts and facts from source papers, only whether they've memorized exact phrasings. Given that many papers are quite old and famous, it's pretty safe to assume they indeed apper in the training set.\n\n3. The multilingual evaluation lacks clear motivation. The multilingual experiments present French and Spanish translations of 500 questions, but it's unclear what relevant research question this addresses. If anything, this might show that LLMs might be memorizing the source materials. \n\n4. The fine-tuning experiments feel underdeveloped. Section 4.4 presents fine-tuning results on 5 models but offers limited analysis. Why do some models improve (Llama-3.1-8B: +5%) while others decline (gpt-j-6b: -7%)? The conclusions offered in the paper is not satisfying."}, "questions": {"value": "1. Will LLM ace QC-bench by doing internet search?\n\n2. Can you provide analysis showing whether model performance correlates with source paper citation counts or publication dates?\n\n3. What hypothesis were you testing with the French and Spanish translations?\n\n4. Do you think the performance decrease in a differnet language implies memorization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yF81Qaklxd", "forum": "hrDlJGrPqc", "replyto": "hrDlJGrPqc", "signatures": ["ICLR.cc/2026/Conference/Submission20330/Reviewer_bYba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20330/Reviewer_bYba"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947731950, "cdate": 1761947731950, "tmdate": 1762933787573, "mdate": 1762933787573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}