{"id": "wQ2jyFz18H", "number": 7797, "cdate": 1758036672406, "mdate": 1759897831904, "content": {"title": "LeanForPhysics: Comprehensive Reasoning Framework for University-level Physics in Lean4", "abstract": "We present **Lean4PHYS**, a comprehensive reasoning framework for college-level physics problems in Lean4. **Lean4PHYS** includes *LeanPhysBench*, a college-level benchmark for Lean4 formal physics reasoning, which contains 200 hand-crafted and peer-reviewed statements formalized from university textbooks and physics competition problems. To establish a solid foundation for formal reasoning in physics, we also launch *PhysLib*, a community-driven repository that includes fundamental unit systems and theorems essential for formal physics reasoning. Based on the benchmark and Lean4 repository we composed in **Lean4PHYS**, we report baseline results using major expert Math Lean4 provers and state-of-the-art closed-source models, and provide an analysis of their performance. In the experiment, we identify that most expert provers do not outperform general models as they did in the math domain. This indicates a potential overfitting of the math domain rather than learning formal reasoning. We also conduct a comprehensive experiment showing that with *PhysLib* in the context, LLMs' performance on*LeanPhysBench* increases by 11.88\\% on average, proving the effectiveness of our repository in assisting LLMs to solve the Lean4 physics problem.  To the best of our knowledge, we are the first study to provide a physics benchmark in Lean4.", "tldr": "This paper presents Lean4PHYS, a reasoning framework for college-level physics problems in Lean4. It includes LeanPhysBench, the first benchmark in the field, and PhysLib, a community-driven repository that sets the foundation for the field.", "keywords": ["Lean4", "Reasoning", "AIforScience"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16d370e66fb1b28cc96d0bc0ca90a42ad459628f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Lean4PHYS, a Lean4-based framework for formalizing college-level physics problems.   It includes PhysLib, a modular library with a systematic unit system and reusable theorems, and LeanPhysBench, a benchmark of 200 formalized physics problems.   The authors propose a pipeline to convert natural language physics questions into Lean4 proofs.   Experiments compare Lean-oriented provers with general-purpose LLMs on LeanPhysBench.   Results show LLMs outperform provers, with improved performance around 40.5% accuracy when using PhysLib context, highlighting the need for domain-specific knowledge in formal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper pioneers formal physics reasoning in Lean and introduces the first large-scale Lean4 physics benchmark covering topics from mechanics to modern physics. \n\n- PhysLib provides a modular, SI-based unit system and topic-structured theorems, ensuring dimensional consistency and extendability for accurate physical reasoning.\n\n- Comprehensive experiments show that PhysLib context consistently boosts model performance.  LLMs outperform specialized Lean provers, revealing that current Lean provers, trained for math, struggle with physics tasks."}, "weaknesses": {"value": "- The paper lacks an explanation of the advantages of PhysLib's modular structure over organizing theorems by specific physics domains.  It also does not clarify how this hierarchical organization aids in retrieval and reasoning.\n- What defines the boundary between mathematical and physical problems, and why do Lean provers, which perform well in mathematics, fail to transfer their capabilities to the physics domain?\n- Would including non-competition problems, such as physics questions from middle school or high school exams, in the experiments provide a more comprehensive comparison?"}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HE7KZlDAN2", "forum": "wQ2jyFz18H", "replyto": "wQ2jyFz18H", "signatures": ["ICLR.cc/2026/Conference/Submission7797/Reviewer_VJbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7797/Reviewer_VJbK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640833727, "cdate": 1761640833727, "tmdate": 1762919841693, "mdate": 1762919841693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a comprehensive framework named Lean4PHYS, designed for formal reasoning on university-level physics problems using the Lean4 proof assistant. The framework consists of two core components: PhysLib, a community-driven library that provides a foundational unit system and commonly used theorems for formal physics reasoning; and LeanPhysBench, a benchmark dataset of 200 problems manually constructed and formalized from university textbooks and physics competitions. Based on this framework, the authors evaluate the performance of several mainstream large language models (including both general-purpose models and those specialized in Lean mathematical proofs). The experimental results show that general-purpose LLMs generally outperform math-specialized models on physics reasoning tasks, revealing a potential overfitting issue of the latter to the mathematics domain. Furthermore, the study demonstrates that using PhysLib as contextual information significantly improves the performance of all tested models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "*   **Significant Contribution to the Research Community:** This paper contributes two extremely valuable resources: **PhysLib**, a modular and extensible foundational library for physics, and **LeanPhysBench**, the first benchmark dedicated to evaluating formal reasoning capabilities in physics. These two achievements provide a solid infrastructure and a fair evaluation standard for subsequent researchers to enter and work in this field, which will undoubtedly promote the development of the entire community.\n*   **Exhaustive Experiments and Deep Insights:** The paper's experimental design is very comprehensive, not only testing multiple top-tier general-purpose LLMs but also comparing them with several Lean-specialized models that excel in mathematics. The results reveal an important finding that \"specialized models have limited cross-domain (from math to physics) generalization ability,\" prompting deep reflection on model generalization and domain overfitting. At the same time, the experiments clearly quantify the effectiveness of the PhysLib library in assisting models with physics reasoning, proving its design value."}, "weaknesses": {"value": "*   **Insufficient Discussion of Related Work:** The \"Related Work\" section mentions Lean's application in physics and other non-mathematical fields but fails to deeply discuss the differences and connections with some directly related works. For example, the paper mentions the `PhysLean` [Tooby-Smith & contributors (2024)] project but only briefly describes it as \"theorem-specific, small-scale, and non-modular.\" Considering that `PhysLean` also aims to formalize physics in Lean4, the authors should have elaborated more on the fundamental differences and specific advantages of Lean4PHYS in terms of design philosophy, implementation methods (such as the construction of the unit system), coverage, and modular design compared to `PhysLean`. Adding such in-depth comparative analysis would better highlight the uniqueness and irreplaceable contribution of this work."}, "questions": {"value": "1.  The experimental results indicate that providing PhysLib as context to the models significantly improves their performance. Given that PhysLib, as a foundational library, could be quite large, the paper seems to lack a specific description of how it was effectively integrated into the model's prompt context window. Did the authors provide the entire library's content, or was some form of retrieval mechanism used to select relevant theorems and definitions? Clarifying this implementation detail is crucial for the reproducibility and understanding of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HPaYTuYLd2", "forum": "wQ2jyFz18H", "replyto": "wQ2jyFz18H", "signatures": ["ICLR.cc/2026/Conference/Submission7797/Reviewer_SHVy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7797/Reviewer_SHVy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729154624, "cdate": 1761729154624, "tmdate": 1762919841236, "mdate": 1762919841236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Lean4PHYS, which is a Lean4-based framework for formal physics. Lean4PHYS includes PhysLib (a repository of a physics unit system and commonly used theorems) and LeanPhysBench (a benchmark of 200 hand-crafted theorems from high school competitions to elementary college level)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The first framework for physics problems in Lean4 for LLM. This may be of significant interest to the community that studies LLM applications to physics. The PhysLib library and LeanPhysBench test dataset can be a useful artefact for future studies."}, "weaknesses": {"value": "- Copyright infringement.\n    - **Due to this issue, I decide to assign a very low score to the paper albeit the important contribution. However, I am very open to changing my score once this issue is clarified/addressed.**\n    - The authors mentioned that “rather than copying the questions verbatim, we reformulated and rephrased them based on the underlying physics ideas\", however, I am not certain that this is sufficient. The key issue hinges on \"substantial similarity\" and whether the original work's creative expression has been copied, even in a modified form. Given that the underlying physics idea of the questions are copied (perhaps to the point that there exists a one-to-one mapping between the textbooks and the dataset questions), this seems to constitute substantial similarity. The authors may need to ask for **explicit permission** from the publishers.\n- Statistical robustness\n    - Given that the evaluations were done with non-zero temperature, the authors should report the stochasticity of the results (e.g., standard error).\n- Lack of implementation elaboration\n    - How do the authors present PhysLib to the model? Would it fit into the context window?\n    - For non-experts, it is challenging to understand what the task looks like, particularly because the prompt asks the model to “complete the following Lean4 code” instead of the commonly known question-answering setup."}, "questions": {"value": "- Did the authors attempt to compare the accuracy of the models in this Lean 4-based setup vs natural setup (i.e., asking the model the question in natural language)?\n    - This would be important to check if the bottleneck is in the physics understanding or in the Lean 4 code understanding\n- I would expect PhysLib to be used by the model in a tool-calling fashion such that we do not need to present all the available concepts to the model in the context window. Is that the case?\n- L377-379: “models with weaker in-context learning perform reatively badly on this level of problems. It is because they cannot infer the new out-of-distribution syntax or unit-handling rules from context.” → This seems to be an overclaiming since none of the experiments are checking the in-context capabilities. Not to mention, we cannot confidently say that this data is OOD because we do not have access to the pretraining data of the models. Am I understanding the sentence properly?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper uses copyrighted materials from publishers such as Pearson and Science Press. The authors noted that they \"reformulated and rephrased\" the materials; however, I am not certain that this sufficiently addresses the copyright limitations."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C5VFMjlGNr", "forum": "wQ2jyFz18H", "replyto": "wQ2jyFz18H", "signatures": ["ICLR.cc/2026/Conference/Submission7797/Reviewer_6F2Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7797/Reviewer_6F2Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862920991, "cdate": 1761862920991, "tmdate": 1762919840476, "mdate": 1762919840476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}