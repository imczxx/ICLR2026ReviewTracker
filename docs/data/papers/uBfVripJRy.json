{"id": "uBfVripJRy", "number": 17065, "cdate": 1758271755429, "mdate": 1759897200900, "content": {"title": "DuConTE: Dual-Granularity Text Encoder with Topology-Constrained Attention for Text-attributed Graphs", "abstract": "Text-attributed graphs integrate semantic information of node texts with topological structure, offering significant value in various applications such as document classification and information extraction. Existing approaches typically encode textual content using language models (LMs), followed by graph neural networks (GNNs) to process structural information. However, during the LM-based text encoding phase, most methods not only perform semantic interaction solely at the word-token granularity, but also neglect the structural dependencies among texts from different nodes. In this work, we propose DuConTE, a dual-granularity text encoder with topology-constrained attention. The model employs a cascaded architecture of two pretrained LMs, encoding semantics first at the word-token granularity and then at the node granularity. During the self-attention computation in each LM, we dynamically adjust the attention mask matrix based on node connectivity, guiding the model to learn semantic correlations informed by the graph structure. Furthermore, when composing node representations from word-token embeddings, we separately evaluate the importance of tokens under the center-node context and the neighborhood context, enabling the capture of more contextually relevant semantic information. Extensive experiments on multiple benchmark datasets demonstrate that DuConTE achieves state-of-the-art performance on the majority of them.", "tldr": "We propose DuConTE, a dual-granularity text encoder that integrates graph structure into LM-based text encoding via topology-constrained attention, improving semantic modeling for text-attributed graphs.", "keywords": ["Text-Attributed Graph", "Topological structure", "Language models", "Attention mechanism"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5b2f7b59f991fd9c0bc31595871affc9dfd2267.pdf", "supplementary_material": "/attachment/48217b73a66b2ee77a0abade52fd74b463bead33.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces DuConTE, a novel model designed for text-attributed graphs, which integrate both semantic information from node texts and topological graph structure. Text-attributed graphs have significant applications in tasks such as document classification and information extraction. Traditional methods often use language models (LMs) to encode text and graph neural networks (GNNs) for processing the graph’s structural information. However, these methods usually focus on word-token granularity during the text encoding phase and fail to capture the structural dependencies between texts from different nodes.\n\nDuConTE addresses these issues with a dual-granularity text encoder and a topology-constrained attention mechanism. The model employs a cascaded architecture of two pretrained LMs. The first LM encodes semantics at the word-token granularity, while the second operates at the node granularity. During self-attention computation in each LM, DuConTE dynamically adjusts the attention mask matrix based on node connectivity. This adjustment guides the model to better capture semantic correlations that are informed by the graph structure. Additionally, DuConTE evaluates the importance of tokens under two different contexts: the center-node context and the neighborhood context, enabling the model to capture more relevant semantic information that is contextually informed by both the individual node and its graph neighbors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. By dynamically adjusting the attention mask based on node connectivity, DuConTE incorporates graph structure into the semantic encoding process. This ensures that the model better understands how nodes are connected, which is a key feature for text-attributed graph tasks.\n\n2. The use of dual-granularity encoding, which operates at both the word-token and node levels, allows for a richer semantic understanding of the nodes and their relationships within the graph.\n\n3. This paper is clear and easy to follow."}, "weaknesses": {"value": "1. Complexity of Cascaded Architecture: The dual-granularity approach, especially the use of a cascaded architecture of two pretrained LMs, introduces additional computational complexity. This may lead to higher training times and resource consumption, potentially making the model less scalable for large datasets.\n\n2. Traditional LM embedding methods typically first obtain word embeddings, then assist with GNNs to perform node propagation. This cascaded approach in DuConTE is neither novel nor practical.\n\n3. Many existing papers have already addressed the interaction problem, such as GLEM and they only used a single LLM to achieve word-level learning. \n\n4. The paper contains many formulas, but they are relatively trivial. The overall framework is quite simple and does not provide particularly insightful ideas.\n\n5. The performance on ogbn-product is not that impressive but it's a minor issue as most datasets show better improvements."}, "questions": {"value": "I suggest authors can consider reducing the complexity of the cascaded two-LM structure. A more streamlined approach could improve both computational efficiency and practical applicability, especially for large-scale datasets. \n\nIt would be helpful to better highlight the novel aspects of the model. While the current method builds on existing approaches, offering deeper insights or introducing more innovative techniques could strengthen the paper’s contribution to the field."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ibSvwsaKSg", "forum": "uBfVripJRy", "replyto": "uBfVripJRy", "signatures": ["ICLR.cc/2026/Conference/Submission17065/Reviewer_pNQe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17065/Reviewer_pNQe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760931882566, "cdate": 1760931882566, "tmdate": 1762927076023, "mdate": 1762927076023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed DuConTeE, a framework that encode text rich graph using topology constrained attention, in the granularity of text and node. A compose layer is also proposed to encode the texts in node into single node representation.  \n\nComprehensive experiment and ablation study is performed and experiments demonstrated the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow and understand \n\n2. The experiment and ablation study is comprehensive. \n\n3. The proposed approach reached state-of-the-art performance on various dataset."}, "weaknesses": {"value": "1. The proposed approach is not novel, especially for \"topology-constrained attention mechanism\". Previous impactful work like K-BERT [1], has already proposed similar idea and not cited in this paper. And there are a lot of similar works during that time. Previous work before 2020 is not comprehensively surveyed.  \n\n2. The effectiveness of different modules are incremental, according to ablation study in Table 2. The improvement of Mask and dual representation is small, it seems that only K-BERT like architecture have already reached state-of-the-art performance. This further reduce the novelty of this paper. \n\n[1] K-BERT: Enabling Language Representation with Knowledge Graph"}, "questions": {"value": "Refer to  the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vuONhreTFl", "forum": "uBfVripJRy", "replyto": "uBfVripJRy", "signatures": ["ICLR.cc/2026/Conference/Submission17065/Reviewer_SSXe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17065/Reviewer_SSXe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761432294408, "cdate": 1761432294408, "tmdate": 1762927075787, "mdate": 1762927075787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the text encoder component in text-attributed graphs and observes that most existing works overlook the incorporation of structural information during text encoding. To address this, the authors propose DuConTE, a dual-granularity text encoder that integrates graph structure into language model-based text encoding through topology-constrained attention, thereby enhancing semantic modeling for text-attributed graphs. Experimental results on five commonly used datasets demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper is clearly written, and the proposed method is easy to follow and understand.\n\nS2: Experimental results on multiple datasets demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "W1: The experimental evaluation could be improved. The authors only conduct experiments on the node classification task and report Accuracy as the sole metric. Moreover, on the non-citation datasets (WikiCS and OGBN-Products), the proposed method does not show a significant improvement. It is recommended that the authors further validate the generalizability of their approach on e-commerce networks such as those used in the CS-TAG [1] work.\n\n[1] A Comprehensive Study on Text-attributed Graphs: Benchmarking and Rethinking. NeurIPS 2023.\n\nW2: Some implementation details of the proposed method are not clearly described. For instance, the setting of the key parameter k is not explicitly reported for each dataset, and it remains unclear how the authors handle longer contexts when feeding data into the RoBERTa model (whose maximum context length is 512).\n\nW3: The proposed approach involves multiple attention operations, but the overall time complexity analysis is missing. The authors are encouraged to provide both theoretical complexity and empirical runtime comparisons.\n\nW4: It would be helpful if the authors could include a brief description of the overall training process at the end of the methodology section, as this would improve the reader’s understanding of the proposed framework.\n\nW5: Mechanisms similar to the proposed Topology-Constrained Attention are already quite common in the Graph Transformer literature. Therefore, the methodological novelty of this work may be somewhat limited.\n\nW6: There are several typographical errors in the paper. For example, in Section 2.2, lines 131–132, the phrase “Another work, Revisiting, generates...” contains a typo."}, "questions": {"value": "Q1： I am curious about the composer component of the proposed method. Would replacing it with a simpler operation, such as mean pooling, lead to a substantial drop in performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4zd7j3Zbew", "forum": "uBfVripJRy", "replyto": "uBfVripJRy", "signatures": ["ICLR.cc/2026/Conference/Submission17065/Reviewer_FoCe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17065/Reviewer_FoCe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492916559, "cdate": 1761492916559, "tmdate": 1762927075326, "mdate": 1762927075326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DuConTE (Dual-Granularity Conceptualization for Text-Attributed Graphs), a new framework designed to enhance text-attributed graph (TAG) learning by jointly leveraging coarse- and fine-grained semantics. The authors observe that prior GNN–LLM fusion models often focus on single-level representations, which limits the model’s ability to generalize across heterogeneous node semantics. To address this, DuConTE introduces two complementary components: a coarse-grained concept encoder that abstracts global node semantics via clustering-driven conceptualization, and a fine-grained text encoder that captures token-level contextual nuances using large language models. The framework then fuses the two representations through a dual-level contrastive alignment mechanism, encouraging the model to maintain consistency between local linguistic details and high-level structural abstractions. Extensive experiments on six text-attributed graph benchmarks show that DuConTE outperforms both GNN-based and hybrid GNN–LLM baselines on node classification and transfer learning tasks. The authors also demonstrate superior efficiency compared to deep fusion approaches, as DuConTE reduces redundant token usage while preserving expressiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s strengths are threefold: 1) it proposes a conceptually elegant and practically useful dual-granularity design that integrates fine linguistic cues and abstract conceptual features, addressing a key limitation in prior single-granularity fusion methods; 2) the method strikes a strong balance between performance and efficiency, offering comparable or superior accuracy to deep GNN–LLM models but with reduced computational overhead thanks to compact conceptual representations and shared alignment objectives; and 3) the empirical evaluation is thorough and convincing, spanning six datasets, diverse baselines, ablation studies, and zero-shot transfer tests, all of which consistently validate the effectiveness of the dual-level alignment scheme in capturing multi-scale semantics and improving generalization."}, "weaknesses": {"value": "The paper also has several weaknesses: 1) while well-structured, the novelty is somewhat incremental, as the core idea of combining coarse- and fine-grained semantic representations echoes hierarchical or multi-level contrastive learning paradigms already common in multimodal and text–vision literature, adapted here for TAGs rather than newly invented; 2) the fusion mechanism remains under-analyzed, as the paper provides limited interpretability or diagnostic insight into how the two granularity levels interact or when one dominates the other, leaving questions about robustness to noisy clustering or overly abstract concept nodes; and 3) the scalability discussion is lacking, as the model’s clustering step and dual contrastive objectives may become costly for large-scale graphs or streaming scenarios, yet these practical concerns are not empirically evaluated or theoretically bounded."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eozCzeJjTH", "forum": "uBfVripJRy", "replyto": "uBfVripJRy", "signatures": ["ICLR.cc/2026/Conference/Submission17065/Reviewer_RnYD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17065/Reviewer_RnYD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920162966, "cdate": 1761920162966, "tmdate": 1762927074973, "mdate": 1762927074973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}