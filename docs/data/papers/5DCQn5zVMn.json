{"id": "5DCQn5zVMn", "number": 18605, "cdate": 1758289439655, "mdate": 1763637658737, "content": {"title": "ILPG: Instance-Level Prototype Generation for Zero-Shot Learning", "abstract": "Zero-shot learning (ZSL) aims to recognize unseen classes by transferring knowledge from seen ones through shared semantic attributes. However, existing methods typically align image features with static, class-level prototypes, which ignore intra-class diversity, lack adaptivity to individual samples, and often suffer from semantic drift. We propose the Instance-Level Prototype Generation (ILPG) network, a lightweight framework that dynamically refines semantic prototypes on a per-instance basis. ILPG combines an attention-based attribute localization module, which highlights discriminative visual regions, with a semantic adjustment pathway that personalizes class prototypes to capture instance-specific variations. This design achieves fine-grained alignment between image features and class semantics while mitigating prototype rigidity. To further enhance robustness, we introduce a synergistic loss formulation that balances discriminability and semantic consistency, ensuring dynamically adjusted prototypes remain semantically faithful. Extensive experiments on three widely used benchmarks (CUB, SUN, and AWA2) demonstrate that ILPG consistently outperforms competitive baselines. ILPG not only establishes new state-of-the-art performance in both conventional and generalized ZSL but also provides interpretable attribute‚Äìfeature associations.", "tldr": "", "keywords": ["Instance-Level Prototype Generation;Zero-Shot Learning;Dynamic Instance Semantic Vectors;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9a1c3130ca4ac0945282eb9fdaef709e7dbf465.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new prototype-based zero-shot learning method. The proposed method proposes attribute-to-visual localization and prototype personalization modules to produce attribute-augmented, instance-personalized prototypes, addressing the issue of intra-class variability. Experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well-written and well-organized. \n- The motivation is clearly articulated, and the three proposed modules are well-explained and easy to understand. \n- The ablation study demonstrates the effectiveness of the proposed approach."}, "weaknesses": {"value": "- Given the prevalence of large language models, I am wondering what‚Äôs the effect of using LLMs to extract semantic attributes for each class or each sample, rather than using the attribute vectors predefined in each dataset. \n\n- What‚Äôs the difference between instance-specific signature and the common instance features? Does this mean the method would a set of class prototypes for each instance? Could authors provide computational efficiency comparison with previous sota methods?\n\n- It would be helpful to clarify how this work compares with recent visual-language models (VLMs), such as CLIP and its variants, which also exhibit strong zero-shot recognition capabilities. The authors may further discuss the advantages or unique contributions of this approach relative to VLMs. Additionally, relying solely on pure ViT models and GloVe vectors may appear somewhat outdated in light of recent advancements in VLMs.\n\n- There are a few minor typos: incorrect letter captions in line 166, missing punctuation at the end of some equations, and an issue in line 193."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KGVSo6rHDS", "forum": "5DCQn5zVMn", "replyto": "5DCQn5zVMn", "signatures": ["ICLR.cc/2026/Conference/Submission18605/Reviewer_wmZ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18605/Reviewer_wmZ7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901701306, "cdate": 1761901701306, "tmdate": 1762928322058, "mdate": 1762928322058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Instance-Level Prototype Generation (ILPG) network for Zero-Shot Learning (ZSL). The core motivation is to overcome the limitations of static, class-level prototypes, which fail to capture intra-class diversity and suffer from semantic drift. ILPG addresses this by dynamically generating a \"personalized prototype\" for each instance. This is achieved by combining three novel components: Attribute-Localized Visual features (AVL), an Instance Signature derived from a frozen DINOv2 encoder, and Instance-Dependent Prototype Personalization (IDPP). The framework is optimized using a comprehensive loss function that includes four distinct terms: $L_{IPM}$, $L_{SAS}$, $L_{VSR}$, and $L_{PCR}$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong Motivation: The concept of dynamically generating instance-level prototypes to address intra-class variance and semantic drift in ZSL is a highly compelling and novel idea. This is a significant step beyond standard class-prototype-based ZSL methods.\n\nEffective Component Integration: The framework thoughtfully integrates several powerful modern components. Leveraging a frozen, self-supervised encoder (DINOv2) for robust, high-fidelity instance signatures is a clever design choice that provides a stable, rich signal for prototype personalization.\n\nAttribute Localization: The inclusion of the AVL module, which uses attention to highlight discriminative attribute-specific visual regions, addresses the fine-grained nature of ZSL."}, "weaknesses": {"value": "Loss Function Complexity and Justification: The proposed loss function, $L = L_{IPM} + \\lambda_{SAS}L_{SAS} + \\lambda_{VSR}L_{VSR} + \\lambda_{PCR}L_{PCR}$, is highly complex with four terms and three weighting hyper-parameters ($\\lambda$'s). A thorough ablation study is absolutely critical to justify the necessity and contribution of each term, especially $L_{SAS}$, $L_{VSR}$, and $L_{PCR}$, and to analyze the sensitivity to the $\\lambda$ values.\n\n\"Lightweight\" Claim: While the paper claims to be lightweight, the addition of the AVL cross-attention module and the IDPP personalization network (MLP $h(\\cdot)$) introduces overhead. A rigorous analysis and comparison of model complexity (FLOPs, parameter count) against existing competitive ZSL baselines is necessary to validate this claim.\n\nRole of DINOv2: The reliance on a frozen DINOv2 encoder for the Instance Signature $g$ might introduce a bottleneck or limit the model's ability to learn task-specific features, although it ensures stability."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KkR4FXM3bf", "forum": "5DCQn5zVMn", "replyto": "5DCQn5zVMn", "signatures": ["ICLR.cc/2026/Conference/Submission18605/Reviewer_Suxq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18605/Reviewer_Suxq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000462586, "cdate": 1762000462586, "tmdate": 1762928321641, "mdate": 1762928321641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes ILPG for zero-shot learning, which replaces a single class-level prototype with a per-instance personalized prototype. The pipeline has three parts: AVL uses cross-attention between attribute embeddings and visual features to localize attribute-relevant tokens; IDPP injects an instance signature into a dual-path tuning scheme to produce a personalized prototype; APC matches the instance‚Äôs localized feature against its own generated prototype set for classification. Experiments on CUB, SUN, and AWA2 claim SOTA or competitive results, plus ablations, occlusion robustness, and hyper-parameter analyses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation. Static class-level prototypes struggle to capture intra-class heterogeneity and are prone to ‚Äúsemantic drift.‚Äù Introducing instance-level prototypes directly targets core pain points in zero-shot learning (ZSL).\n\n2. Method aligned with the goal. AVL uses attribute embeddings to guide cross-attention for localized alignment; IDPP leverages a frozen DINOv2 to extract an instance signature that personalizes the semantic prototype; PCR/SAS/VSR respectively enforce consistency with anchors, inter-class separability, and visual‚Äìsemantic geometric alignment. The combination of personalization and regularization is logically sound.\n\n3. Broad experimental coverage and analysis. The paper reports CZSL/GZSL comparisons on three standard benchmarks (CUB/SUN/AWA2), complemented by ablations, t-SNE visualizations, hyperparameter sensitivity, and occlusion robustness analyses."}, "weaknesses": {"value": "1. Lack of fairness in SOTA comparisons. Table 1 states ‚ÄúBackbone column removed,‚Äù yet the ILPG implementation explicitly uses DINOv2 (a self-supervised ViT) as the visual encoder, whereas most compared methods in their original papers use ResNet-101 or their own default backbones. If the numbers are taken from the literature without re-training under a unified backbone, the comparison may be biased by backbone differences. The authors should report results under a shared backbone for all methods, or at least disclose the backbone used for each compared method.\n\n2. Limited testability of the theoretical part. The robustness radius and unseen-class risk bounds rely on strong assumptions (e.g., Lipschitzness and margin) and mainly yield ‚Äúno-worse-than‚Äù type conclusions. There is no clear, estimable link to properties of the actually trained networks, which limits the practical verifiability of the theory.\n\n3. Inconsistencies in notation, formulas, and naming.\n\n(1) The tensor shapes and naming for the class semantics $S$, attribute embeddings $A$, the cross-attention output $\\tilde{E}$, and the APC scoring are inconsistent and ambiguous, which impedes faithful reproduction. For example, the paper alternately treats the instance-specific weight $W$ used in $S_W = W \\odot S + S$ as either $W \\in \\mathbb{R}^{N}$ (row-wise broadcasting) or $W \\in \\mathbb{R}^{N \\times d_s}$ (per-dimension scaling), while leaving unspecified how $S_W$, defined in the semantic space $S \\in \\mathbb{R}^{N \\times d_s}$ with $N$ classes and semantic dimension $d_s$, is aligned with the attribute-guided visual representation $\\tilde{E} \\in \\mathbb{R}^{K \\times d}$ produced by AVL from $A \\in \\mathbb{R}^{K \\times d_a}$ ($K$ attributes, visual dimension $d$). The manuscript should fix one consistent specification and clarify whether $S_W$ is projected into the $d$-dimensional visual space (e.g., via a linear map) or, alternatively, whether $\\tilde{E}$ is pooled or reshaped so that the APC score is well-defined; otherwise the compared vectors may not be shape-compatible and the evaluation protocol remains underspecified.\n\n\n(2) In Equation (3), the mapped feature $tilde{E}$ is mistakenly labeled as ùê∏ in Figure 2.\n\n(3) Loss naming is inconsistent (the text first defines LIPM, but the overall loss is summarized as LCE).\n\n(4) Main method/component names are inconsistent: the appendix/tables use abbreviations WSC/PA/PC, whereas the main text uses SAS/VSR/PCR; additionally, ‚ÄúCSA‚Äù appears in a row of Table 4 and seems to be a typo."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NgYAhDVpuZ", "forum": "5DCQn5zVMn", "replyto": "5DCQn5zVMn", "signatures": ["ICLR.cc/2026/Conference/Submission18605/Reviewer_AyBe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18605/Reviewer_AyBe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001368513, "cdate": 1762001368513, "tmdate": 1762928321309, "mdate": 1762928321309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ILPG, a framework for zero-shot learning that dynamically generates instance-specific prototypes instead of relying on static, class-level semantic prototypes. The model proposes a suite of complementary loss functions Instance Prototype Matching (IPM), Semantic- Aware Self-Calibration (SAS), Visual‚ÄìSemantic Residual Alignment (VSR), and Prototype Con- trast Refinement (PCR)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative Instance-Level Adaptation:\nILPG advances beyond existing multi-prototype approaches by generating prototypes per instance, effectively capturing intra-class variation such as pose, viewpoint, and color differences.\n\n2. Comprehensive Experiments:\nThe paper includes both quantitative (accuracy, harmonic mean, ablations) and qualitative analyses (t-SNE plots, attention maps, occlusion tests), offering a convincing demonstration of the model‚Äôs capabilities"}, "weaknesses": {"value": "1. Using prototypes for Zero-shot categories is not a new idea. The authors are encouraged to specify the difference of this paper comparing with other models.\n\n2. The performance gain of the proposed model is marginal. For instance, comparing with ReZSL, the proposed method only improves 0.2 on CUB~(CZSL) and 0.6 on AWA2. Thought the performence improvement on SUN dataset is significant, the author did not explicitly explain the reason."}, "questions": {"value": "How does ILPG ensure that the instance-driven prototype personalization (IDPP) doesn‚Äôt overfit to noise or spurious instance cues (e.g., background artifacts) rather than meaningful attributes?\n\nSince prototypes are dynamically adjusted per instance, how are these personalized prototypes still interpretable as semantic representations? Is there any visualization of how the semantic space evolves after personalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mcYqMxQwSf", "forum": "5DCQn5zVMn", "replyto": "5DCQn5zVMn", "signatures": ["ICLR.cc/2026/Conference/Submission18605/Reviewer_WppD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18605/Reviewer_WppD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008382100, "cdate": 1762008382100, "tmdate": 1762928320811, "mdate": 1762928320811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}