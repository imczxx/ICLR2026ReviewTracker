{"id": "84OJ2WRyC2", "number": 7323, "cdate": 1758015877447, "mdate": 1759897859869, "content": {"title": "Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints", "abstract": "Multi-agent reinforcement learning (MARL) often assumes that agents can reliably execute their intended actions. In practice, however, malfunctions and unexpected failures are inevitable, leading to severe coordination breakdowns. We propose the \\emph{Multi-Agent Robust Training Algorithm (MARTA)}, a plug-and-play framework for training MARL policies that remain effective under agent failures. MARTA introduces a novel adversarial game in which a \\emph{Switcher} learns when and where to activate malfunctions in high risk-states, while an \\emph{Adversary} controls the faulty agents. The remaining agents are trained to \\emph{jointly} best-respond to such targeted malfunctions, yielding policies that are robust to critical failures. We provide theoretical guarantees that MARTA converges to a Markov perfect equilibrium, ensuring robustness against worst-case malfunctions under both cost and budget formulations. Empirically, MARTA achieves state-of-the-art fault tolerance across diverse MARL benchmarks, including Traffic Junction, Level-Based Foraging, and Multi-Agent Particle Environments, substantially improving performance under both aligned and distribution-shifted failure scenarios. Our results highlight MARTA as a principled and general approach for fault-tolerant MARL.", "tldr": "", "keywords": ["multi-agent reinforcement learning", "robust", "fault-tolerance", "multi-agent systems"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5375b49c315646efe2a5ce659e4ed1bb3b150a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an adversarial training framework for robust multi-agent reinforcement learning (MARL). The approach, called the Multi-Agent Robust Training Algorithm (MARTA), introduces a Switcher agent that decides at every time step when and how to intervene with an adversarial decision-maker. Therefore, each agent has an adversarial counterpart that can replace its corresponding functional agent. The functional agents and the adversarial agents are trained alternatingly using different QMIX instances, whereas the Switcher is trained with soft actor-critic. MARTA is evaluated in different benchmark environments with adversarial agents and compared to the standard MARL methods QMIX and VDN, which are not adversarial themselves."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an important (yet well-studied) problem in cooperative MARL.\n\nThe proposed method, MARTA, and setting are built on well-established foundations, such as zero-sum Markov games, and are therefore sound.\n\nI appreciate the diversity of environments used to demonstrate the effectiveness of MARTA."}, "weaknesses": {"value": "**Novelty**\n\nThe problem of faulty agents or where each agent has an adversarial counterpart has been addressed in prior work, which has neither been discussed in related work nor compared with in the experiments [1,2]. The problem setting defined in Section 2.1 has been formalized as a zero-sum Markov game or, more generally, as a mixed cooperative-competitive game in the literature [3,4].\n\nThe adversarial training scheme of two opposing QMIX instances has also been introduced in [1].\n\nThis leaves the Switcher agent being the only new addition to the overall adversarial framework, making the technical contribution somewhat incremental.\n\n**Soundness**\n\nThe theoretical analysis may be sound, but I do not consider it a major contribution, since the whole setting is a straightforward instantiation of zero-sum Markov games [4], where robustness properties have been shown long before.\n\nMinor comment: Despite focusing on a partially observable setup, the paper assumes observations to be Markov as the policy conditions on them. To make the definition theoretically sound, the policies should condition on the action-observation history [5].\n\n**Significance**\n\nThe paper aims to evaluate the robustness by integrating faulty agents. However, I couldn't find any information on how these faulty agents are created (e.g., from MARTA?) to check for potential biases during testing.\n\nDespite discussing some prior work on robust MARL, the experimental comparison does not include any of them. Instead, the work is only compared with QMIX and VDN, which are standard MARL methods that are known to fail in adversarial settings [1,2]. Without any comparison with alternative adversarial MARL methods, I cannot confirm the claim that MARTA is less conservative and more effective than prior work.\n\n**Literature**\n\n[1] Phan et al., \"Learning and Testing Resilience in Cooperative Multi-Agent Systems\", AAMAS-20\n\n[2] Li et al., \"Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game\", ICLR-24\n\n[3] Lowe et al., \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\", NeurIPS-17\n\n[4] Littman et al., \"Markov Games as a Framework for Multi-Agent Reinforcement Learning\", ICML-94\n\n[5] Oliehoek et al., \"A Concise Introduction to Decentralized POMDPs\", 2015"}, "questions": {"value": "How are the faulty test agents created for the experimental test?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GTC47H6MNN", "forum": "84OJ2WRyC2", "replyto": "84OJ2WRyC2", "signatures": ["ICLR.cc/2026/Conference/Submission7323/Reviewer_PSdH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7323/Reviewer_PSdH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760881091888, "cdate": 1760881091888, "tmdate": 1762919441759, "mdate": 1762919441759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of fault tolerance in multi-agent reinforcement learning (MARL). The authors propose MARTA, a modular and plug-and-play training framework that enables MARL systems to remain robust when some agents malfunction. The framework introduces two additional learners: a Switcher, which decides when and which agent to trigger a malfunction on, and an Adversary, which determines the faulty agent’s behavior. The interaction between the Switcher, Adversary, and cooperative agents is formulated as a stochastic game with either a per-activation cost or a fixed malfunction budget. Through theoretical analysis, the authors demonstrate that the learning dynamics converge to a Markov perfect equilibrium. Extensive experiments across several benchmark environments show consistent improvements in robustness without altering the underlying MARL architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important and relatively unexplored problem of fault tolerance in multi-agent reinforcement learning, with a clear and well-motivated problem statement.\n\n2. The proposed MARTA framework introduces a novel Switcher–Adversary formulation that models both the timing and behavior of agent malfunctions in a unified way.\n\n3. The work offers theoretical guarantees and consistent empirical results across various MARLs, demonstrating clear practical benefits and robustness improvements."}, "weaknesses": {"value": "1. The presentation is quite dry and mathematically heavy, making the paper difficult to follow in parts. The figures are limited and do not clearly illustrate the key intuitions behind the proposed framework. \n\n2. The title and abstract do not accurately convey the main emphasis of the work. They make the paper appear as a system or framework for fault-tolerant MARL, whereas the actual contribution lies more in the theoretical formulation and convergence analysis of the proposed approach. \n\n3. Although the framework is described as involving both a Switcher and an Adversary, the analysis and experiments mainly focus on the Switcher component, leaving the Adversary’s role and interaction underexplored."}, "questions": {"value": "1. Some modeling choices appear primarily made for mathematical tractability, like allowing only one agent to malfunction at a time. While understandable, it would be valuable to include a short discussion or limitation section on how these assumptions affect real-world applicability and whether extensions to multiple concurrent failures are feasible.\n\n2. Although the framework introduces both the Switcher and Adversary learners, the paper effectively fixes the Adversary during analysis and training. Given the symmetry of roles, would it be possible to alternatively fix the Switcher and optimize the Adversary, then combine the two solutions for better equilibrium behavior?\n\n3. In the MARTA-B variant, the budget constraint is expressed as a fixed number n of allowed activations. This formulation raises several questions: \n(1) Does the learned policy tend to exhaust its budget regardless of context? \n(2) If the remaining steps become fewer than the remaining budget, does the Switcher always activate faults? \n(3) Conceptually, MARTA encourages selective fault triggering to balance cost and robustness, whereas MARTA-B seems to encourage full budget utilization. Would it be more consistent to introduce a total-cost constraint (e.g., ∑cᵢ ≤ B) with a penalty beyond B rather than a hard count-based budget?\n\n4. It is unclear whether MARTA-B is empirically evaluated. The experimental section seems to focus solely on MARTA; providing results for MARTA-B would strengthen the overall validation of the proposed variants.\n\n5. While the paper claims modularity and “plug-and-play” usability, there is limited discussion of the computational overhead. The appendix briefly reports total CPU hours, but a more explicit analysis of computational cost would help substantiate the claimed practicality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6ZR9qXE7wv", "forum": "84OJ2WRyC2", "replyto": "84OJ2WRyC2", "signatures": ["ICLR.cc/2026/Conference/Submission7323/Reviewer_ry1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7323/Reviewer_ry1d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100137045, "cdate": 1761100137045, "tmdate": 1762919441146, "mdate": 1762919441146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MARTA, a framework for training MARL policies that are robust to an adversary that can at any step adversarial control one of the agents. A Bellman operator for the game is proposed, and various theoretical properties are shown. A variant of the framework with a budget on the number of malfunctions is also considered. Experiments on three simple MARL problems show that the addition of the proposed method improves fault tolerance compared to unconstrained MARL methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The exact problem setting tackled seems to be novel, to the best of my knowledge, although there are other works (though uncited) that tackle very similar problems\n- Experimental results suggest the proposed framework improves robustness compared to without applying the proposed framework"}, "weaknesses": {"value": "- Related works [A, B, C] tackling very similar attack vectors as the one proposed in this paper are not cited or discussed.\n- The experimental section is a bit lacking. In particular, the proposed method is not compared to any other MARL methods that try to improve fault tolerance, including both methods discussed in the paper (e.g., M3DDPG) as well as ones that the authors did not identify but are highly relevant (e.g., [A]).\n- The domains that are tested on seem quite simple\n- Also, details on the environments tested are lacking\n    - How many agents does each environment have?\n- Line 429: The paper claims that shielding and backup policy methods “often sacrifice performance and scale poorly as the number of agents increases.” Does the proposed method improve upon the scalability of these methods? For example, the cited (Qin et al., 2021) has experiments with up to 1024 agents. Was the method deployed on even larger scale environments compared to prior work that “scaled poorly”?\n- Many parts of the proofs in the appendix are poorly written are difficult to read\n    - All the math in Proposition 3 uses inline mathematics which is too excessive. This hampers readability and made it very difficult to follow the logical flow of the proof.\n\n[A] Li, Simin, et al. \"Byzantine robust cooperative multi-agent reinforcement learning as a bayesian game.\" ICLR 2024.\n\n[B] Zhou, Ziyuan, and Guanjun Liu. \"Robustness testing for multi-agent \nreinforcement learning: State perturbations on critical agents.\" *arXiv preprint arXiv:2306.06136* (2023).\n\n[C] Zheng, Haibin, et al. \"One4all: Manipulate one agent to poison the cooperative multi-agent reinforcement learning.\" *Computers & Security* 124 (2023): 103005."}, "questions": {"value": "- How do existing methods of fault tolerant MARL perform in terms of robustness compared to the original VDN/QMIX baseline even though they do not target the exact same problem formulation?\n- The citations for switching controls (Mguni, 2018a; Mguni et al., 2023) seem very strange\n    - In Mguni (2018a), the term “switching controls” does not even appear. Instead, it references very old and established literature on **impulse control**, as well cites the appropriate literature such as [$\\alpha$]\n    - It’s not clear why the term “switching controls” was chosen instead of the more established “impulse control”\n- I’m confused by how / whether the existence of the value $v^*$ is shown. Specifically, I’m not sure where it is proven that $\\min_{g} \\max_{\\pi} v(s | \\pi, g) = \\max_{\\pi} \\min_{g} v(s | \\pi, g)$.\n\n[$\\alpha$] Bensoussan, A. \"Contrôle Impulsionnel et Inéquations Quasi Variationelles.\" International Congress Of Mathematicians. 1975."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rfnfx4Kv9o", "forum": "84OJ2WRyC2", "replyto": "84OJ2WRyC2", "signatures": ["ICLR.cc/2026/Conference/Submission7323/Reviewer_YpZz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7323/Reviewer_YpZz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941274852, "cdate": 1761941274852, "tmdate": 1762919440634, "mdate": 1762919440634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response to all reviewers"}, "comment": {"value": "**We thank all reviewers** for recognising the importance of fault tolerance in MARL, the novelty of the Switcher-Adversary formulation, and the strength of our empirical results and theoretical guarantees.\n\n**A shared misconception** across reviews is that MARTA resembles prior adversarial or diagnostic frameworks ([A], [B], [1], [2]).\nIn fact, these works evaluate or attack policies, or model strategic adversaries.\n\nMARTA instead produces robust cooperative policies under true actuator malfunctions—a fundamentally different robustness axis.\n\nSeveral reviewers overlooked that MARTA is plug-and-play: it robustifies QMIX, VDN, MAPPO and MADDPG without architectural modification. This property is not shared by any cited baselines.\n\nAcross reviews, there was underappreciation of MARTA’s theoretical contributions:\n(i) a new fault-switching Markov game with budgeted failure dynamics,\n\n(ii) a contraction-based proof of existence and uniqueness of the minimax value,\n\n(iii) policy convergence guarantees even with function approximation.\n\nNone of the referenced works provide these results for fault-inducing controllers.\n\nReviewers requested clarity on modelling choices (single-fault regime, fixed Adversary); these will be addressed with added discussion and extended experiments (including MARTA-B).\n\nOverall, MARTA provides the community with the first unified, theoretically grounded and practical mechanism for training multi-agent policies robust to hardware-like malfunctions, addressing challenges that prior adversarial MARL methods do not cover.\n\n[A] [2] Li, Simin, et al. “Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game.”\nICLR 2024.\n\n[B] Zhou, Ziyuan, and Guanjun Liu. “Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents.”\narXiv:2306.06136 (2023).\n\n[1] Phan, Tom, et al. “Learning and Testing Resilience in Cooperative Multi-Agent Systems.” AAMAS 2020."}}, "id": "DNojwQ5i10", "forum": "84OJ2WRyC2", "replyto": "84OJ2WRyC2", "signatures": ["ICLR.cc/2026/Conference/Submission7323/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7323/Authors"], "number": 16, "invitations": ["ICLR.cc/2026/Conference/Submission7323/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643842550, "cdate": 1763643842550, "tmdate": 1763643842550, "mdate": 1763643842550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}