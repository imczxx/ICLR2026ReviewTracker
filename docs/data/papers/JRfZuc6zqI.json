{"id": "JRfZuc6zqI", "number": 8479, "cdate": 1758085749917, "mdate": 1759897781798, "content": {"title": "Flash-DD: An Ultra Parameter-Efficient Approach to Dataset Distillation", "abstract": "Dataset distillation (DD) aims to create a smaller dataset that encapsulates the essential knowledge of a larger dataset, thereby reducing storage demands and accelerating downstream training. For large-scale dataset distillation, state-of-the-art methods achieve satisfactory performance by using soft labels generated by well-trained teacher models during downstream training. However, it will cause some issues: (1) a substantial amount of additional storage is required to retain the teacher models, often significantly exceeding the storage needed for the synthetic images; (2) generating labels through these teacher models slows down the downstream training process, counteracting the efficiency goals of dataset distillation; and (3) downstream training guided by these teacher models, according to our studies, yields suboptimal performance. Focusing on these drawbacks, in this paper, we propose plug-and-play parameter-efficient label generation techniques for dataset distillation, which maximizes the benefits of limited model parameters and can be generalized to different DD methods, datasets, and settings. Specifically, we propose a DD-oriented model parameter reduction method that automatically determines the optimal capacity of teacher models and eliminates redundant parameters for dataset distillation tasks. Furthermore, for additional parameter space, we turn to model ensemble strategies and propose guidelines to optimize the utilization efficiency of the additional space. Compared to the state-of-the-art methods, Flash-DD requires only 0.03% of the additional storage and significantly accelerates downstream label generation by 843.81x while maintaining comparable performance. Alternatively, with a mere 1.8% storage budget, it can boost accuracy by up to 13.4% over previous leading methods. Our code will be available.", "tldr": "We propose a DD-oriented model parameter reduction method that automatically determines the optimal capacity of teacher models and eliminates redundant parameters for dataset distillation tasks.", "keywords": ["Dataset Distillation", "Dataset Condensation", "Efficient Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8f10d98c0ba8569b7c36fa52a089a57ab53629b.pdf", "supplementary_material": "/attachment/14fa71c80c0359d5806100840ac7e89c7f1d48ed.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents FLASH-DD, a plug-and-play and parameter-efficient technique for label generation in dataset distillation. The method aims to improve the teacher model’s storage and effectiveness by reducing redundant parameters and employs an ensemble of models to generate soft labels for training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed FLASH-DD requires only 0.03% additional storage and accelerates downstream label generation by 843.81×, while maintaining comparable performance.\n2. The proposed method outperforms previous state-of-the-art approaches in all settings and substantially reduces resource usage."}, "weaknesses": {"value": "1. The explanation of Equation (2) on page 4 lacks sufficient detail. It is unclear how the importance score β is calculated, why certain parameters are considered redundant, and what algorithm this step is based on as the theory support.\n2. The proof of Proposition 1 on page 5 is insufficiently explained. The notation in Equation (3) is not well introduced, and the underlying theory is missing. It is unclear what Equation (3) represents and what each term denotes.\n3. The description of the ensembling strategy in lines 316–317 on page 6 is incomplete. It is unclear whether multiple teacher models are used to generate labels or if submodules from different models are combined to form a new teacher model."}, "questions": {"value": "Does the parameter reduction refer to removing a portion of the parameters from the teacher model? If so, how is the remaining part of the teacher model utilized after this reduction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FMaaRuJam4", "forum": "JRfZuc6zqI", "replyto": "JRfZuc6zqI", "signatures": ["ICLR.cc/2026/Conference/Submission8479/Reviewer_z3eu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8479/Reviewer_z3eu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761382518923, "cdate": 1761382518923, "tmdate": 1762920357052, "mdate": 1762920357052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Flash-DD, a parameter-efficient dataset distillation framework that reduces teacher model size via automated parameter pruning and uses ensemble-based label generation to optimize knowledge transfer. It achieves up to 843× faster label generation and comparable or superior performance with only 0.03–1.8% of the storage required by prior methods on large-scale datasets like ImageNet-1K."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research direction of improving inference efficiency for soft label generation is important, and the authors present an intuitive and effective approach to address this problem.\n\n2. The paper is well structured and clearly written, making it easy to follow.\n\n3. The experimental evaluation is comprehensive and solid, including cross-architecture generalization analysis."}, "weaknesses": {"value": "1. Algorithm 1 lacks clarity in the definition and initialization of the threshold $\\zeta$.  The variable $\\zeta$ is used in the comparison $Dis(\\theta_{T_i}) \\ge \\zeta$ before being explicitly defined,  which introduces ambiguity in the algorithm's logical flow and practical implementation.\n\n2. The theoretical analysis in Section 3.2, particularly the definition of the threshold $\\zeta$ via the second derivative of $Dis(\\theta_T)$, lacks mathematical rigor.  Since $Dis(\\theta_T)$ is empirically measured rather than analytically defined, the condition $\\frac{\\partial^2 Dis(\\theta_T)}{\\partial|\\theta_T|^2} = 0$ is largely heuristic.  While identifying an inflection point in teaching effectiveness is intuitively reasonable, the formal expression appears ad hoc and may overstate the theoretical grounding of the method.\n\n3. Code is not provided.\n\n4. (Minor) Figure 1 is somewhat difficult to read; improving its visual clarity would help.\n\n5. (Minor) The legend in most of the figures (Figure 4 and Figure 6) in this paper is so small and hard to read, and the caption doesn't contain full information for understanding the figure itself."}, "questions": {"value": "I like the paper overall and would be happy to raise my score if the authors address the few weaknesses and questions.\n\n1. It is unclear whether the training configurations of the baseline methods (SRe2L, G-VBSM, and RDED) are identical to those used when applying Flash-DD. By “training configurations,” I refer to the hyperparameters and the number of teacher models involved in generating soft labels. For example, SRe2L appears to employ a single ResNet-18 as the teacher model, whereas Flash-DD utilizes multiple teacher models for soft label generation. This raises concerns that the observed performance improvement may partially result from the ensemble effect rather than the proposed parameter-efficient design.\n\n2. A similar concern arises in the comparison with RDED, where RDED uses a single ResNet-18 for label generation, while Flash-DD adopts multiple teacher models. It would be helpful if the authors could include an ablation study or additional analysis to clarify whether the performance gains are primarily due to the ensemble setup or the proposed optimization strategy.\n\n3. The algorithm defines the threshold $\\zeta$ using the condition  $\\frac{\\partial^2 Dis(\\theta_T)}{\\partial|\\theta_T|^2}=0$,  but in practice $Dis(\\theta_T)$ is empirically measured rather than analytically defined.  Could the authors elaborate on how $\\zeta$ is computed numerically and whether this threshold is robust to noise in the measured $Dis$ values?\n\n4. How is Acc in Algorithm 1 defined? Is it the accuracy of the model on the distilled dataset?\n\n5. How sensitive is the optimal teacher capacity to dataset domain or model architecture? Would a teacher optimized on ImageNet-1K remain near-optimal when transferred to a related dataset such as ImageNet-100?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RUxaetfnhe", "forum": "JRfZuc6zqI", "replyto": "JRfZuc6zqI", "signatures": ["ICLR.cc/2026/Conference/Submission8479/Reviewer_fTh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8479/Reviewer_fTh4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486511712, "cdate": 1761486511712, "tmdate": 1762920356679, "mdate": 1762920356679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Flash-DD, a method to address a significant overhead in large-scale dataset distillation (DD). State-of-the-art (SOTA) DD methods often rely on soft labels from large, pre-trained \"teacher models,\" which introduces substantial storage and computational costs, undermining the efficiency goals of DD. The authors' core contribution is the counter-intuitive finding that the full-sized, most accurate teacher model is suboptimal for guiding downstream training.\n\nFlash-DD proposes a two-part solution:\n\n1. DD-Oriented Parameter Reduction: A method to prune the teacher model not just to save space, but to find an \"optimal capacity\". This smaller teacher is shown to provide clearer class boundaries (Fig. 3) and leads to better downstream performance (Fig. 2). The method uses an iterative pruning process guided by a heuristic based on early-stage training loss.\n\n2. Ensemble Label Generation: Guidelines for using any remaining storage budget to ensemble multiple, small, capacity-matched teachers, which further boosts performance by providing diverse perspectives.\n\nExperiments show that Flash-DD can match SOTA performance using only 0.03% of the original teacher's storage while accelerating label generation by 843.81x. Alternatively, using a small 1.8% storage budget, it can improve SOTA accuracy by up to 13.4%. The method is shown to be a plug-and-play improvement for several existing DD methods (Table 1) and generalizes well to other student architectures and continual learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Critical Problem: Addresses a fundamental and practical flaw in SOTA large-scale DD methods: the massive storage and compute overhead of teacher models, which this paper correctly identifies as counter-productive.\n\n2. Novel Core Insight: The paper's key contribution is the empirical discovery that downstream DD accuracy is a non-monotonic function of teacher capacity. The full-sized teacher is shown to be suboptimal, and a pruned, lower-capacity teacher provides superior guidance.\n\n3. Exceptional Empirical Results: The efficiency gains are not marginal; they are orders of magnitude. Matching SOTA performance with 0.03% extra storage and an 843.81x speedup is a stellar result. Boosting SOTA accuracy by 13.4% with only 1.8% storage is equally powerful.\n\n4. Generality: The method is demonstrated to be \"plug-and-play,\" successfully improving the performance and efficiency of three different SOTA DD methods (Table 1).\n\n5. Robustness: The authors provide strong evidence of generalization, showing their method works well when training diverse student architectures (including Transformers) and in a continual learning setting ."}, "weaknesses": {"value": "1. Weak Theoretical Justification: The paper's primary weakness. Proposition 1 is backed by a \"proof\" that is merely an intuition for a simplified linear case and holds no water for DNNs . This attempt at formalism is unconvincing and should be removed or heavily reframed as an \"intuition.\"\n\n2. Missing Ablation on Pruning Method: The method relies exclusively on $l_{1}$-norm magnitude pruning. This is a simple, unstructured method. It's an open question if the type of pruning matters. Would a more advanced, structured, or gradient-based pruning method find a better (or worse) teacher? The paper makes a tacit assumption that $l_1$ pruning is the right tool for this \"knowledge-shaping\" job, which is not ablated."}, "questions": {"value": "1. Pruning Method Sensitivity: Have you investigated whether the choice of pruning method (e.g., $l_1$-norm vs. $l_2$-norm, or vs. a gradient-based method) impacts the \"optimal capacity\" or the final downstream performance? Is it possible that $l_1$ magnitude pruning is uniquely suited for this task?\n\n2. Ensemble Saturation: The ensemble results (Fig. 6) are strong, showing 3 teachers are better than 2, which are better than 1 . Is there a point of diminishing returns? What happens if you use 5 or 10 teachers, constrained by the same total memory budget (meaning each teacher is even smaller)?\n\n**Note to Authors: I am willing to raise my score if the authors can satisfactorily address the weaknesses and questions raised above in their rebuttal.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fLpBmz7zjy", "forum": "JRfZuc6zqI", "replyto": "JRfZuc6zqI", "signatures": ["ICLR.cc/2026/Conference/Submission8479/Reviewer_Y37M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8479/Reviewer_Y37M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940046799, "cdate": 1761940046799, "tmdate": 1762920356262, "mdate": 1762920356262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Flash-DD offers plug-and-play techniques for ultra-efficient soft label generation in dataset distillation (DD), countering teacher-model inefficiencies in SOTA large-scale methods (e.g., SRe2L, GVBSM, RDED). Innovations include DD-specific pruning to auto-determine teacher capacity (removing irrelevant params) and ensemble guidelines to repurpose extra space for stage-adapted knowledge diversity. This forms a storage-tunable paradigm: 0.03% extra storage yields 843.81× faster labeling at SOTA parity; 1.8% delivers up to 13.4% accuracy gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The findings in Figures 2 and 3 are interesting and inspiring.\n    \n2. The experimental results are promising."}, "weaknesses": {"value": "Major Weakness: \n1. The discussion in lines 247–251 regarding Figure 3 could be clarified. From the figure, it is somewhat difficult to discern which type of teacher model leads to better downstream performance.\n\n2. The paper introduces several hyperparameters (e.g., $\\lambda$), but the selection process and robustness analysis are not sufficiently discussed.\n\n3. The motivation for Equation (10) is not clearly explained.\n\n4. The paper lacks comparison with methods specifically designed for soft label compression (e.g., [1]), which would be important for a fair and comprehensive evaluation.\n\nMinor Weakness: \n1. Figure placement could be improved. For example, Figures 2 and 3 appear on page 3 but are first referenced on page 5. Similar inconsistencies occur elsewhere; a thorough check is recommended.\n\n2. Figure and table captions are somewhat lengthy; using bold text to emphasize key points could improve readability.\n\n3. Some captions contain minor inaccuracies—for instance, Figure 6 does not have “first row” and “second row.”\n\n[1] Yu, Ruonan, et al. \"Heavy labels out! dataset distillation with label space lightening.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025."}, "questions": {"value": "1. Could the authors further elaborate on the observations and implications presented in Figure 3?\n\n2. Could the authors provide more details on how the hyperparameters were chosen and whether the method is robust?\n\n3. What is the underlying motivation behind Equation (10)? \n\n4. Since label compression is an essential aspect of this work, could the authors include comparisons with label compression methods to strengthen the evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IfPaoeth2D", "forum": "JRfZuc6zqI", "replyto": "JRfZuc6zqI", "signatures": ["ICLR.cc/2026/Conference/Submission8479/Reviewer_94Eb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8479/Reviewer_94Eb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984131864, "cdate": 1761984131864, "tmdate": 1762920355893, "mdate": 1762920355893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}