{"id": "RMT6CZquly", "number": 11453, "cdate": 1758199417581, "mdate": 1759897574665, "content": {"title": "One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers", "abstract": "Pretraining massively multilingual Large Language Models (LLMs) on corpora from many languages at once is challenging due to limited model capacity, scarce high-quality data in many languages, and compute constraints. This has led to a multilingual gap in language coverage. Moreover, the lack of language coverage of the tokenizer, makes this harder to address purely at the post-training stage. In this work, we study what relatively cheap interventions early on in training improve \"language plasticity\", or adaptation capabilities of the model post-training to new languages. We focus on tokenizer design and propose using a universal tokenizer that is trained for more languages than the primary pretraining languages to enable nimble and efficient adaptation in expanding language coverage after pretraining. We conduct systematic experiments across 63 languages belonging to diverse typological and lexicographic language groups. Across different training strategies, we show that models with a universal tokenizer achieve significantly higher language adaptation, with up to 20.2% increase in win rates compared to the model with more conservative language group specific tokenizer. Furthermore, a universal tokenizer also enables better plasticity towards languages that are completely unseen in the tokenizer and pretraining, by up to 5% win rate gain. We achieve this adaptation to an expanded set of languages with minimal compromise in performance on the majority of languages included in pre-training.", "tldr": "", "keywords": ["tokenizer", "multilingual pre-training", "less-resourced languages", "cross-lingual transfer", "multilingual representations", "tokenization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fbdd0f0c25e0ac252675d5759a1838be7bac998a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors train LLMs using larger-than-usual tokenizers, and show that this added capacity does not benefit much popular languages, but can benefit greatly less represented languages if they are upsampled properly. They also show that adding tokens post-training is not fully as effective as training with them from the start, if done with baseline methods (random init, mean embedding init). These initializations are however far from the existing SOTA, and it's not clear that this conclusion would still hold with more recent initialization strategies such as (FOCUS, WECHSEL, or Trans-Tokenization)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The authors aim to solve a very important problem, the poor performance of language models in less-represented languages.\n* The proposed solution is reasonable, although this type of language reweighting is fairly standard for all LLM tokenizers these days.\n* The article is globally well-written and accessible."}, "weaknesses": {"value": "* The paper only shows that increasing the vocab size to 250k yields little to no benefit once the target language is properly modeled (e.g. about 100k tokens), while using that added capacity for other languages is helpful for later finetuning the model on those languages. This is pretty obvious, and well-documented. Even the first multilingual llama paper discusses this, to some extent. The reason why tokenizer size is usually kept smaller than 250k is the cost involved in a larger modeling head + softmax, which is not discussed at all in the paper. It is somehow assumed that chaging the vocab size is a free operation, which isn't true at all in practice. I would refer to Apple's paper on Hierachical softmax for possible ways to alleviate this issue.\n* Some of the languages chosen as \"completely unseen\" are not reasonable choices for this category. For example, the first one (written Afrikaans) is extremely similar to written Dutch, which is a seen language. The second one (Belarussian) is very close from Russian and Ukrainian, especially in writing. Those languages will naturally vastly benefit from an appropriate tokenizer with their tokens trained on similar languages. This isn't a surprising or novel finding.\n* More globally, no tokenizer is universal. The claim in this paper to have invented a universal tokenizer is baseless, as for instance the \"universal\" tokenizer will not be able to perform well on languages like Armenian if they weren't seen during the tokenization training, since the language uses unique code points not shared with other languages, which would then be decomposed in byte sequences by BPE. Any language not seen during the tokenizer training would suffer the same issue, and not all languages are in the training set."}, "questions": {"value": "* Can you justify your statement that `Overall, these results show that it is more effective to use a UNIVERSAL tokenizer from the start,\n rather than substituting it in after pretraining` in the light of the significant cost caused by a large tokenizer during training of the model?\n* Why did you only compare your method with pretty basic baseline CVA techniques, rather than more recent ones which have been shown to provide huge benefits over these baselines at none to very low training costs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YwR8dR9udu", "forum": "RMT6CZquly", "replyto": "RMT6CZquly", "signatures": ["ICLR.cc/2026/Conference/Submission11453/Reviewer_JwKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11453/Reviewer_JwKn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761425577508, "cdate": 1761425577508, "tmdate": 1762922563945, "mdate": 1762922563945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how tokenizer design affects the language plasticity of multilingual LLMs -- i.e., their ability to adapt to new or unseen languages after pretraining.\nThe authors propose a \"Universal\" Tokenizer, trained on a much broader set of languages than those used during pretraining, as a low-cost intervention to improve post-training adaptability. \nIn their setup, the authors consider 62 languages, which are broken up into three clusters: (1) European languages, (2) Asian languages, and (3) Middle-Eastern and Indic languages. \nThe authors then conducted controlled experiments by pretraining models in the \"primary\" cluster, while using the other clusters as \"expanded\" subsets (there are also 7 unseen languages) for plasticity adaptation evaluation with multiple strategies (continued pretraining, targeted fine-tuning, and unseen-language adaptation).\nThe authors show that the universal tokenizer enables higher win rates in adaptation to expanded languages and unseen ones, while maintaining comparable performance on pretraining languages.\nAdditionally, the experiments indicate that the universal tokenizer yields faster adaptation and better compression efficiency.\nThe authors also show that the universal tokenizer outperforms cross-lingual vocabulary adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The experiments are very extensive and well-controlled (69 languages, 3 language clusters + 7 unseen languages, multiple adaptation regimes).\n\n- There is clear evidence of improvement in plasticity and adaptation efficiency without trade-offs on primary languages.\n\n- The authors provide clear evidence that a universal tokenizer is a simple, yet scalable and low-cost method to enhance multilingual coverage."}, "weaknesses": {"value": "- The use of LLM-as-a-Judge for open-ended generation is reasonable but subjective. Given that 15 adaptation languages are evaluated, including even a small-scale human verification subset would strengthen the credibility of the win-rate results.\n\n- Experiments are conducted on a 3.3B model. Since multilingual capability often scales with model size and inference budget [1], it remains unclear whether the Universal Tokenizer's benefits persist, or diminish, at larger scales.\n\nI understand the difficulty of evaluating open-ended generation. The authors use LLM-as-a-Judge and report the win rates. Since the authors use 15 adaptation languages in this test, I think it would be more convincing if a small-scale human evaluation is included.\n\n- The model scale (3.3B) is not very large. Since the model capability (including multilinguality) has shown to be improved quite a bit with scaling up when giving enough inference budget [1], I am wondering if the benefit of using universal tokenizer decreases when the model size scales up.\n\n- The CVA comparisons use relatively simple mean and random initialization methods. Stronger baselines (e.g., from [2, 3, 4]) could provide a fairer reference point, though this is not critical given the paper's focus on tokenizer design rather than adaptation algorithms.\n\n\n[1] https://arxiv.org/abs/2505.05408\n[2] https://arxiv.org/abs/2112.06598\n[3] https://arxiv.org/abs/2305.14481\n[4] https://arxiv.org/abs/2311.08849"}, "questions": {"value": "- Could you provide a quantitative breakdown of token overlap ratios between Universal and Cluster tokenizers across scripts?\n\n- Have you considered multilingual reasoning tasks (e.g., MGSM) to test if improved plasticity translates to reasoning?\n\n- Since the Universal Tokenizer covers languages unseen during pretraining, many subword embeddings might be undertrained or even untrained, especially for low-resource or distinct-script languages. Can the authors provide intuition for why these embeddings still facilitate adaptation in post-training stages?\n\n- Relatedly, could you quantify the proportion of subwords that are (1) untrained and (2) undertrained after pretraining? This would help explain the mechanism behind the Universal Tokenizer's effectiveness.\n\n- OFA [1] is another wise embedding initialization strategy for CVA; including this could further strengthen the completeness of the related work..\n\n[1] https://arxiv.org/abs/2311.08849"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jsox6I15ni", "forum": "RMT6CZquly", "replyto": "RMT6CZquly", "signatures": ["ICLR.cc/2026/Conference/Submission11453/Reviewer_UXUT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11453/Reviewer_UXUT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568821852, "cdate": 1761568821852, "tmdate": 1762922563495, "mdate": 1762922563495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main contribution of this paper is to propose training the tokenizer on a large variety of language corpora before pretraining (on a pretraining dataset of lower diversity). The paper shows that such a tokenizer leads to negligible drops in performance on the pretraining languages, while yielding significant improvements on the expanded (seen-by-tokenizer) languages and smaller but notable gains on expanded (unseen-by-tokenizer) languages. The experiments are extensive and cover multiple languages and setups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses an important challenge in LLMs regarding the utility of tokenizers for diverse languages.\n2. Experiments are extensive, and those sections are well written and clearly explained.\n3. An improved multilingual tokenizer would be of broad interest to the multilingual LLM community."}, "weaknesses": {"value": "1. The expanded (seen languages) setup, which is the main focus of the paper, feels somewhat artificial. Since the Universal tokenizer sees data from all languages, it seems like an unfair comparison because if pretraining data for these languages are already available to build the tokenizers, one could simply create a single tokenizer using an upweighting strategy and then pretrain on all available data. That would likely be the practical approach. It is difficult to imagine a real-world scenario where corpora from two languages are available, yet pretraining is only done on one of them (while the tokenizer is trained on both).\n2. The paper would benefit from a deeper analysis of why the UNIVERSAL tokenizer helps during pretraining. Section 5.4 suggests benefits even when 0% of the language is included in pretraining, but it is unclear why a token for an expanded language would be useful for the Universal tokenizer unless it undergoes some “hits” during training. Otherwise, those token embeddings would remain close to their initialization. One hypothesis is that the observed improvements occur primarily for expanded languages sharing the same script, while another is that the filtering method still fails to fully remove contamination. A clearer analysis would strengthen the paper’s contribution and improve understanding of the benefits of the Universal tokenizer.\n3. Overall, the method is slightly unclear. While the analyses are strong, the lack of sufficient discussion about the differences between UNIVERSAL and CLUSTER makes it difficult to understand where the gains are coming from.\n\nI lean toward a reject in its current form, mainly because the seen-by-tokenizer setup which largely comprises the paper feels somewhat artificial based on my understanding."}, "questions": {"value": "My questions below reflect what I found unclear.\n1. I don’t fully understand what the CLUSTER tokenizer is and how it differs from UNIVERSAL. Is it that UNIVERSAL sees both expanded and cluster-specific languages for each of the three clusters, while CLUSTER does not?\n2. **L127** *“Half of the training mix consists of an even distribution of all languages in the instruction finetuning data.”* This may be a nitpick, but I’m not sure this qualifies as “CPT,” given that it uses instruction tuning data. Could the high quality of this finetuning data (relative to typical pretraining data) be contributing to the tokenizer’s effectiveness? Additionally, why was an even distribution chosen?\n3.  **L246** *“Where all languages are uniformly weighted except English”*: does this mean that weights are proportional to data availability?\n4. Regarding the CLUSTER tokenizer: is this a unique tokenizer specific to each cluster (i.e., three separate tokenizers, each of size 250k)? If so, does Figure 2 represent three separate language models trained on these clusters, and then CPT without any tokenizer intervention on the expanded languages? This setup seems somewhat unintuitive, as one would expect a baseline such as CVA applied to the expanded languages on top of the cluster tokenizer in that figure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k3nnTIbx24", "forum": "RMT6CZquly", "replyto": "RMT6CZquly", "signatures": ["ICLR.cc/2026/Conference/Submission11453/Reviewer_5iVL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11453/Reviewer_5iVL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969751236, "cdate": 1761969751236, "tmdate": 1762922563009, "mdate": 1762922563009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how tokenizer design affects the multilingual adaptability of large language models. The authors propose using a universal multilingual tokenizer trained over a wide range of languages during pretraining, rather than adapting tokenizers post hoc. Through experiments spanning more than 60 languages and multiple adaptation settings, they find that models pretrained with a universal tokenizer achieve strong gains on low-resource and unseen languages, while maintaining performance on high-resource ones. The approach yields faster adaptation, improved translation and instruction-following quality, and outperforms cross-lingual vocabulary adaptation baselines. The study highlights tokenizer choice as a simple yet powerful lever for enhancing multilingual plasticity (Chen et al 2023) in language models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a clear and thoughtful study of how tokenizer design influences multilingual adaptability. \n\n- Its originality lies in approaching language plasticity (Chen et al 2023) through a simple, pretraining-time intervention rather than architectural or post-hoc changes. \n\n- The experiments are broad, well controlled, and convincingly demonstrate that a universal tokenizer improves cross-lingual transfer without harming high-resource performance."}, "weaknesses": {"value": "- The experiments, though broad, are confined to mid-sized models; it remains unclear whether the observed gains in adaptation speed and multilingual coverage hold at larger scales. \n\n- The evaluation relies heavily on LLM-as-judge metrics. Adding human or cross-judge validation, especially for languages with distinct scripts, would bolster confidence in the reported improvements. \n\n- The paper’s discussion of prior work on language plasticity is limited. It briefly cites Chen et al. (2023) to define the term but does not meaningfully compare its tokenizer-based approach with training-time interventions such as active forgetting or embedding resets. A fuller discussion would help clarify what is novel about influencing plasticity through tokenizer design rather than optimization dynamics.  Both works address the question of improving multilingual plasticity through pretraining-time interventions, but via very different mechanisms. So it would strengthen the related work section to discuss this connection more _explicitly_."}, "questions": {"value": "1. Could the authors elaborate on how their tokenizer-based intervention differs conceptually from prior training-time approaches to improving language plasticity, such as active forgetting (Chen et al., 2023)? \n\n2. Do the observed multilingual gains and faster adaptation trends hold at larger scales or in more compute-intensive settings? \n\n3. How robust are the results to the choice of evaluation method? For instance, would human evaluation or alternative judges confirm the same improvements, especially for languages with distinct scripts or tokenization structures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UzTtFnfy6m", "forum": "RMT6CZquly", "replyto": "RMT6CZquly", "signatures": ["ICLR.cc/2026/Conference/Submission11453/Reviewer_WPps"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11453/Reviewer_WPps"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994271048, "cdate": 1761994271048, "tmdate": 1762922562418, "mdate": 1762922562418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}