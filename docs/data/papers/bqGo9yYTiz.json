{"id": "bqGo9yYTiz", "number": 7724, "cdate": 1758033804262, "mdate": 1759897837217, "content": {"title": "Quality and Diversity Optimization Even From Offline Homogeneous Dataset", "abstract": "We investigate the challenge of promoting diversity in offline reinforcement learning (RL), where agents must develop diverse strategies despite being trained on homogeneous datasets with limited behavioral variation. Existing offline RL approaches, including those leveraging expectation-maximization algorithms for unsupervised clustering, often struggle with either insufficient diversity or performance degradation in such settings. To overcome these limitations, we introduce a novel Unique Behavior objective function that can be \\emph{directly computed to quantify the distinctiveness between agents}, eliminating the need for additional estimators and reducing potential estimation errors. By maximizing uniqueness, our approach encourages agents to learn diverse behaviors effectively, even when the training data lacks variety. Extensive experiments on D4RL MuJoCo and Atari benchmarks demonstrate that our method achieves significant behavioral diversity while maintaining strong performance, even from homogeneous training data.", "tldr": "We introduce an intrinsic reward mechanism that enhances behavioral diversity without sacrificing performance in offline reinforcement learning.", "keywords": ["Offline Reinforcement Learning", "Diversity and Performance", "Homogeneous Dataset"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/94f014358268e2b94e8ae39494a53f4d6b7f223f.pdf", "supplementary_material": "/attachment/dceee2b91afa9006eb9c2ec7d7db880dd5af3b90.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the significant challenge of ​​Quality-Diversity (QD) optimization in offline reinforcement learning​​. The core problem is learning a set of high-performing yet diverse policies from a static, pre-collected dataset that may be ​​homogeneous​​ (containing limited behavioral variation). Existing offline QD methods, such as those based on Expectation-Maximization (EM) clustering or variational mutual information (MI) estimation, often fail in such settings because they rely on inherent dataset diversity or introduce approximation errors.\nThe authors' primary contribution is a novel ​​Unique Behavior (UB) objective function​​. They theoretically reformulate the intractable problem of \"path diversity\" by decomposing it into \"behavior diversity\" (difference in action choices) and \"state visitation diversity.\" They prove that behavior diversity is a tractable lower bound for the full path diversity. The UB objective directly maximizes the distinctiveness of action distributions across different policies under shared states, calculated explicitly without needing additional estimators. To ensure that the diverse behaviors are also high-quality, the method integrates a conservative Q-learning penalty to avoid unreliable out-of-distribution actions. Extensive experiments on D4RL MuJoCo and Atari benchmarks demonstrate that this approach successfully generates significant behavioral diversity while maintaining strong performance, even when trained on homogeneous data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of the Unique Behavior objective is a key innovation. It provides a fresh perspective on inducing diversity by directly maximizing the difference in action distributions, which seems to be both simple and powerful.\n\n2. The research is exceptionally thorough. It combines a theoretical insight (the lower bound) with a practical algorithm, backed by extensive and well-designed experiments that validate the method's effectiveness across multiple dimensions.\n\n​3. The core idea is intuitive and well-illustrated. The visual comparisons in the figures effectively highlight the method's advantages."}, "weaknesses": {"value": "1. While the lower bound is provided, a deeper analysis of its tightnessunder different conditions would strengthen the theoretical contribution. It remains an empirical observation that optimizing this lower bound is sufficient for trajectory-level diversity.\n\n2. The comparison could be strengthened by including a simpler baseline, such as training multiple independent offline RL agents (e.g., with CQL or IQL) with different random seeds. This would help quantify how much diversity is specifically due to the UB objective versus the randomness of optimization.\n\n3. The approach requires training M policies and computing pairwise action distribution differences. A discussion of the computational overhead compared to training a single policy or other baseline methods would provide a more complete picture of the method's practicality.\n\n4. This paper lacks sufficient investigation of related work about QD in Reinforcement Learning. For example, to my best knowledge in this domain, the [1] and [2] are most recent papers about Quality Diversity Techniques for reinforcement learning or learning from datasets.\n\n[1] Batra, S., Tjanaka, B., Fontaine, M. C., Petrenko, A., Nikolaidis, S., & Sukhatme, G. (2023). Proximal policy gradient arborescence for quality diversity reinforcement learning. arXiv preprint arXiv:2305.13795.\n\n[2] Wan, Z., Yu, X., Bossens, D. M., Lyu, Y., Guo, Q., Fan, F. X., ... & Tsang, I. Diversifying Robot Locomotion Behaviors with Extrinsic Behavioral Curiosity. In Forty-second International Conference on Machine Learning."}, "questions": {"value": "​​1. You theoretically establish that behavior diversity is a lower bound on path diversity. Could you comment on the empirical tightness of this bound in the environments you tested? Are there conditions under which optimizing this lower bound is provably sufficient for generating full trajectory-level diversity?\n\n​​2. The value of λ is set to 1.0 for most environments but reduced to 0.5 for Walker2d-medium-expert. What is the specific characteristic of this environment that makes it more sensitive to the diversity penalty? Could you suggest a more principled, data-driven approach for setting λ rather than empirical tuning?\n\n3. In addition to comparisons with SORL and DIVEOFF, did you consider ablating against a baseline of simply training multiple independent conservative agents (e.g., using CQL) with different random initializations? This would help isolate the contribution of the UB objective from the inherent stochasticity of training multiple agents.\n\n4. There are recent works that seem to address similar problem with your work (e.g. [1] and [2]). If the settings are indeed the same, the authors should incorporate them as baselines for comparison. If the settings are not the same, the authors should discuss the difference in detail.\n\n[1] Batra, S., Tjanaka, B., Fontaine, M. C., Petrenko, A., Nikolaidis, S., & Sukhatme, G. (2023). Proximal policy gradient arborescence for quality diversity reinforcement learning. arXiv preprint arXiv:2305.13795.\n\n[2] Wan, Z., Yu, X., Bossens, D. M., Lyu, Y., Guo, Q., Fan, F. X., ... & Tsang, I. Diversifying Robot Locomotion Behaviors with Extrinsic Behavioral Curiosity. In Forty-second International Conference on Machine Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OVmMpO7Kv5", "forum": "bqGo9yYTiz", "replyto": "bqGo9yYTiz", "signatures": ["ICLR.cc/2026/Conference/Submission7724/Reviewer_x3GS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7724/Reviewer_x3GS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712442829, "cdate": 1761712442829, "tmdate": 1762919778925, "mdate": 1762919778925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Unique Behavior (UB) objective to promote policy diversity in offline RL, even when training data are homogeneous. It reformulates mutual information into a tractable action-level objective and integrates it into the EDAC framework with conservative Q-learning. Experiments on D4RL benchmarks show that UB achieves higher diversity and strong performance compared to SORL, CLUE, and DIVEOff."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation: tackles diversity learning from homogeneous offline data; Simple and tractable formulation.\n\n2. Strong results on both performance and diversity.\n\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Novelty is incremental: While the lower-bound formulation is elegant, it can be viewed as a natural extension of existing MI-based methods rather than a fundamentally new framework.\n\n2. Theoretical guarantees are light compared to empirical results.\n\n3. Hyperparameter sensitivity: Although λ is claimed to work across settings, there is no detailed exploration of its impact on performance–diversity tradeoff beyond a single ablation.\n\n4. Another related work that also focuses on promoting diversity in offline datasets is [1], which appears to leverage mutual information as well. Could the authors provide a comparison or discussion highlighting the differences between their approach and [1]?\n\n[1] Diverse Policies Recovering via Pointwise Mutual Information Weighted Imitation Learning."}, "questions": {"value": "1. How sensitive is the performance/diversity tradeoff to the number of policies (M)? Is there a diminishing return beyond a certain number?\n\n2. The use of conservative Q-learning is mentioned, but how does it interact with the UB reward? Could UB inadvertently encourage unsafe extrapolation?\n\n3. How does training multiple policies with UB scale compare to EM-based methods like SORL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GXpMUXVG2S", "forum": "bqGo9yYTiz", "replyto": "bqGo9yYTiz", "signatures": ["ICLR.cc/2026/Conference/Submission7724/Reviewer_c4eh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7724/Reviewer_c4eh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820947051, "cdate": 1761820947051, "tmdate": 1762919778452, "mdate": 1762919778452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles offline quality–diversity (QD) in RL when the dataset is homogeneous. The paper proposes a new Unique Behavior (UB) objective, which directly measures behavioral distinctiveness across agents through tractable, action-level uniqueness terms. Theoretically, they show this “behavior diversity” provides a lower bound on the intractable full trajectory diversity (path-level MI). Practically, they integrate UB into the EDAC offline RL framework, using ensemble critics and reward shaping to balance quality and diversity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper observes that most offline QD approaches assume heterogeneous data and thus break down when the dataset has only one style of behavior\n- The decomposition of trajectory-level MI into (i) action-level distinctiveness and (ii) state-visitation distinctiveness, and the observation that the former is directly computable from policy densities while the latter is not\n- Plugging the UB term in as reward shaping on top of EDAC (and using the minimum over Q-ensembles) is a sensible design to prevent the policy to go OOD"}, "weaknesses": {"value": "- The central inequality (“behavior uniqueness is a lower bound on path uniqueness”) is basically a consequence of MI non-negativity after splitting trajectory MI into action- and state-terms. No argument is given about tightness or when optimizing the surrogate will significantly move the intractable target.\n- Experiments are largely confined to MuJoCo and locomotion benchmarks. More diverse domains (e.g., discrete games) could demonstrate broader applicability.\n- The results have high variance with significant overlap with the baselines. Including statistical analysis would strengthen the empirical results.\n- The diversity evaluation uses the determinant of behavioral embeddings (as in DIVEOFF), which can be sensitive to specific kernel implementation especially in the continuous control domains. Alternative or more interpretable diversity metrics could better ground the findings."}, "questions": {"value": "- How sensitive is the UB objective to the number of agents (M)? Could fewer agents  still yield meaningful diversity?\n- Does the method generalize well to discrete environments where policy outputs are categorical or joint distributions?\n- Can the UB term conflict with the conservative Q-learning penalty, e.g., by pushing policies into low-data regions?\n- Currently the resulting behaviors seem the be locally different but largely the same at a high-level, e.g., changing the stride lengths in the locomotion task. What extensions would be needed for a more high-level diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DnamNGWgQ8", "forum": "bqGo9yYTiz", "replyto": "bqGo9yYTiz", "signatures": ["ICLR.cc/2026/Conference/Submission7724/Reviewer_q2Zh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7724/Reviewer_q2Zh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995245551, "cdate": 1761995245551, "tmdate": 1762919778037, "mdate": 1762919778037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an algorithm that can learn diverse behaviours even from a homogeneous offline dataset. It aims to maximise the behaviour uniqueness, which is a lower bound of the overall path-level uniqueness. By adding the behaviour uniqueness reward to the external reward, the algorithm can successfully learn policies that are diverse yet high-performing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on the problem of learning diverse behaviours, which is very important for real-world applications of offline RL. Unlike most existing work that assumes the dataset to already contain trajectories sampled from multiple different policies, the proposed algorithm can learn diverse behaviours even from a homogeneous dataset. Empirical analysis on multiple benchmarks clearly demonstrate the effectiveness of the algorithm."}, "weaknesses": {"value": "The usage of up to 30 critic networks for each behaviour policy seems like an overkill. Also, the usage of tinyurls in the document is very dangerous from the perspective of cybersecurity, so please consider changing them to their original URLs. Finally, the significance of learning diverse behaviours in offline RL is not very convincing. Providing concrete examples, backed by experiments, on how the learned diverse behaviours can be used would be helpful for the readers to get a better understanding on why learning diverse behaviours is important for offline RL."}, "questions": {"value": "(5) presents a loss where the behaviour uniqueness is directly optimised, whereas the actual algorithm was implemented in a way where it is indirectly done through intrinsic rewards. Is there a particular reason for this indirect optimisation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0Nwck668kI", "forum": "bqGo9yYTiz", "replyto": "bqGo9yYTiz", "signatures": ["ICLR.cc/2026/Conference/Submission7724/Reviewer_kkmz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7724/Reviewer_kkmz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999747221, "cdate": 1761999747221, "tmdate": 1762919777256, "mdate": 1762919777256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}