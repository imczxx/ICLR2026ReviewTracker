{"id": "WgRyl6fzLx", "number": 6662, "cdate": 1757991597062, "mdate": 1763710731344, "content": {"title": "SSRL: Self-Search Reinforcement Learning", "abstract": "We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions:  1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.", "tldr": "We propose an reinforcement learning method called SSRL which achieves comparable performance with previous baselines. We further propose Sim2Real in agent tasks and gain better results.", "keywords": ["Search Agent", "Reinforcement Learning", "World Knowledge", "Sim2Real"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2fde0c012c69b25e8617ae0c310d1bedfa1bc762.pdf", "supplementary_material": "/attachment/7a131cc93402b83d23c656a749676fdceef95513.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes SSRL (Self-Search Reinforcement Learning), a method that trains language models to perform self-generated search before answering questions.\nInstead of using a real search engine, the model creates its own “search” and “information” steps inside the output, and then receives a reward based on the answer quality and output format.\nThe goal is to teach the model to reason and retrieve information more systematically, while reducing the need for real web queries.\nThe authors evaluate SSRL on several QA benchmarks using models from 3B to 70B parameters, and also test a Sim-to-Real setting where the model interacts with real search results."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) Novelty:\nThe paper combines reinforcement learning with self-constructed search reasoning.\nThis idea of training an LLM in a simulated search environment is novel and interesting.\n\n2) Cost efficiency:\nBecause it does not depend on real API calls or human feedback, the method is low-cost and scalable for small and medium-sized models.\n\n3) Sim-to-Real transfer:\nThe results show that a model trained only with self-generated searches can still work reasonably well when connected to a real search engine.\nThis indicates the learned policy is relatively stable and generalizable.\n\n4) Practical relevance:\nThe work explores how smaller models can approach the performance of larger ones through structured reasoning and sampling, which is useful for efficient deployment."}, "weaknesses": {"value": "1) Methodology clarity:\nThe paper does not clearly define the RL setup — it is unclear what the exact states, actions, and rewards are.\nThe training pipeline (sampling, reward, update) is not fully described, and the “self-search” mechanism is hard to reproduce.\nAlthough the code is provided, it is difficult to directly examine every detail in code. \n\n2) Reward design limitations:\nThe reward is simple and rule-based, combining a binary “outcome reward” (correct / incorrect) and a “format reward” for structural tags.\nThis design is very discrete and task-specific, which can make learning unstable and limit generalization.\nIt makes the knowledge of LLM is trapped in its own training environment, and implicitly put a requirement on the training dataset.\n\n3) Claims without quantitative proof:\nThe paper often claims that SSRL reduces hallucination or improves reasoning quality, but there is no direct measurement (e.g., factual consistency, hallucination rate, or human study).\nThe evidence is only indirect from QA accuracy, which is not enough.\n\n4) Lack of ablation and analysis:\nThere is no ablation to test the importance of each reward term or design choice.\nIt is unclear how much improvement comes from SSRL itself versus the sampling or instruction tuning.\n\n5) Task-specific scope:\nThe method is only evaluated on QA datasets, and it is uncertain whether the same idea can generalize to open-ended reasoning or dialogue tasks. \nIt's necessary to quantify the scope."}, "questions": {"value": "Can the authors give more details about how the RL process is implemented?\nFor example, how are advantages computed, how is KL controlled, and how many updates per batch?\n\nHow sensitive is SSRL to the reward coefficients (e.g., λf)?\nWould smoother or continuous rewards improve stability?\n\nDid the authors try any direct hallucination or factual consistency evaluation?\nThis would help verify the claimed benefits.\n\nHow much of the gain comes from the structured format reward versus the self-search policy itself?\nAn ablation would clarify this.\n\nCould this method be applied to other tasks beyond QA, such as summarization or dialogue?\nIf so, what changes would be needed in the reward or structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yAvm1o2UsV", "forum": "WgRyl6fzLx", "replyto": "WgRyl6fzLx", "signatures": ["ICLR.cc/2026/Conference/Submission6662/Reviewer_Xsoz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6662/Reviewer_Xsoz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528376962, "cdate": 1761528376962, "tmdate": 1762918973034, "mdate": 1762918973034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is about training LLMs reinforcement learning (RL) on tasks that require agentic search. The authors propose to approximate costly external search queries with the intrinsic search ability of LLMs (which they call Self-Search) during training. The authors first evaluate the Self-Search ability of some open-source LLMs from three families (Qwen2.5, Qwen3, and Llama3) showing that search result accuracy on six benchmarks (e.g., General QA, Multi-hop QA, Vague QA) increases with repeated sampling (i.e., pass@k). Then, the authors introduce Self-Search Reinforcement Learning (SSRL), a custom RL objective that enables models to progressively enhance their internal use of knowledge. The authors empirically show that SSRL-trained policies are cost-effective and stable for doing RL training with agentic search queries. While SSRL reduces reliance on external search engines during training, it still allows for sim-to-real transfer as evidenced by empirical results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of leveraging the intrinsic search ability of LLMs to reduce reliance on external search engines during RL training is interesting and might be novel in this particular setting. I recognize that it addresses a practical challenge in training LLMs for agentic tasks.\n- The empirical evaluation is comprehensive, covering multiple LLM families and benchmarks. The results demonstrate the effectiveness of SSRL in improving search accuracy and reducing external search costs. Also, the Appendix contains many ablation studies to validate the effectiveness of each component."}, "weaknesses": {"value": "- My biggest concern is about the clarity of the proposed approach. While the high-level idea of Self-Search and SSRL is understandable, some details are unclear to me. I believe the paper would benefit from more polishing to improve clarity.\n  - Starting from Figure 1 (left), at first glance, it was not clear what the dotted box represented for the full-sim search section.\n  - What is the instructions/prompts used. I'm assuming they are different when studying the Self-Search ability (Section 2) versus when training with SSRL (Section 3). The prompt design discussed in the Appendix B.1.1 seems rather important to me to fully understand the proposed approach. I would integrate that in the main text.\n  - What does iterative refinement mean? How does that relates to the repeated sampling? Are those the same thing? It is not clear in the main text what iterative refinement means in the context of Self-Search. When reading the Appendix, I understood that it refers to simply continuing the CoT generation alternating <think>, <search> and<information>. Section B.3 mentions 10 as the maximum number of iterations, but it is unclear to me how many iterations are typically needed to get good results.\n  - Is the k in pass@k the same as the number of iterations in iterative refinement? Typically pass@k refers to generating k independent full-inferences.\n  - Some of this information is only available in the Appendix and the reader has to put all the scattered parts together.\n\n- How do you ensure diversity in the multiple samples generated during SSRL? If the samples are too similar, the benefit of repeated sampling might be limited.\n- Sampling temperature can significantly impact the quality of the samples. How sensitive is the Self-Search performance to the choice of sampling temperature? Have you experimented with different temperature settings other than 1 during training?\n\n#### Minor\n- This is confusing \"The instruction used is shown in Appendix B.1.2. The prompt used is listed in Appendix B.1.1.\". When looking at Table 5 and 6 they are very similar. The only different is one is asking for the model to fill in the top search results. Where does \"k\" comes into play in Table 6?"}, "questions": {"value": "- Sampling temperature can significantly impact the quality of the samples. How sensitive is the Self-Search performance to the choice of sampling temperature? Have you experimented with different temperature settings other than 1 during training?\n- In Table 1, where are the results for Self-Search with Search Engine -/G ?\n- Are the results in Table 1 represent pass@k? If not how do you aggregate the multiple samples generated during Self-Search to produce a final answer? Are you using majority voting as discussed in Section 2.4?\n- In Table 2, which results correspond to SSRL-trained models with external search engines at inference time versus those that retrieve K responses from local corpora?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JHbJiImcwT", "forum": "WgRyl6fzLx", "replyto": "WgRyl6fzLx", "signatures": ["ICLR.cc/2026/Conference/Submission6662/Reviewer_vvYs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6662/Reviewer_vvYs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752593939, "cdate": 1761752593939, "tmdate": 1762918972661, "mdate": 1762918972661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates an interesting problem of training a self-search agent with reinforcement learning. Existing search agents rely on calling real-time search agents during training and inference, which leads to latency. In this work, the authors propose to use the policy model itself as the search engine and conduct a self-search agent with reasoning and “self-search”. They first conduct experiments to study the test-time scaling performance of the self-search agent and propose a reinforcement learning solution to further improve it. Extensive experiments on several benchmarks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well written and easy to understand.\n2. The inference time scaling experiments are interesting and insightful.\n3. The study of “self-search” reinforcement learning is novel, and it is great to see that the model trained with “self-search” RL can generalize to adopting tools during inference.\n4. The authors conduct extensive experiments to demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Lack of an ablation study for the format reward. In Section 3.2, the authors propose conducting outcome reward and format reward functions. However, there is no ablation study to verify the effectiveness of the format reward.\n\n2. Is the method only suitable for a specific type of LLM? The main results in Table 1 are based on Llama models. It is questionable whether SSRL can still outperform other methods on other types of LLMs, such as Qwen. From Figure 5, it seems that the performance on Qwen2.5 is not very good with SSRL.\n\n3. It requires further explanation on why information token masking is still useful here. The difference between SSRL and Search-R1 is that here the information tokens are on-policy, thus masking may not be desired."}, "questions": {"value": "1. What is the performance if we do ablation for the format reward?\n2. Is the method only suitable for a specific type of LLM?\n3. Why is it still important to conduct information token masking here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Gx0mXSZYO6", "forum": "WgRyl6fzLx", "replyto": "WgRyl6fzLx", "signatures": ["ICLR.cc/2026/Conference/Submission6662/Reviewer_bKdV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6662/Reviewer_bKdV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850642864, "cdate": 1761850642864, "tmdate": 1762918972263, "mdate": 1762918972263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces self-search reinforcement learning (SSRL), a RL-based framework that trains Large Language Models (LLMs) to perform search tasks by levering their own internal knowledge rather than relying on external search engines. The authors first investigate self-search and found that the performance scale with increasing sample size k, indicating that LLM's intrinsic knowledge may be sufficient for such benchmarks. As such, the authors propose SSRL with format- and outcome-based rewards to enhance such capabilities with RL training, allowing models to refine their internal knowledge utilization through long-form reasoning and self-search. This approach creates a cost-effective training framework compared to search-based ones, and enables the trained models to perform similarly to LLMs trained with real-world search engines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors performed extensive experiement on the sample size of search agents and argue that the performance can be improved by scaling sample sizes even without retrieving from external knowledge. Motivated by such observations, the proposed SSRL method eliminates the search API costs and improves the performance of search agents by training models to exploit their own internal knowledge.\n\n2. Through extensive experiemnts, the authors show that models trained with SSRL can both work with integrated knowledge or with real search engines. In addition, the experiment results show that SSRL trained models show comparable performance to the models trained with real search engines."}, "weaknesses": {"value": "1. The authors did not propose novel insights or new learning framework. Instead, the work seems to be an improvement on the Search-R1  baseline, featuring more refined RL training techniques. Therefore resulting method rather seems to be an improved \"R1-Base / Instruct\" baseline without searching.\n\n2. Although the experiment results show that LLM performance can match search agents with SSRL training, the model does not access external information or facts that may not exist within the embedded knowledge, which could result in over-confidence or hallucinations, but the authors do not discuss these aspects in detail.\n\n3. As shown in the appendix, when applied to challenging tasks like SimpleQA, models using real search still significantly outperform SSRL. This suggests that SSRL is only effective in scenarios where the model has already internalized the necessary knowledge (e.g., Wikipedia) during pretraining, but it struggles when faced with tasks that require external knowledge it does not possess."}, "questions": {"value": "1. The authors mention that multi-turn self-search and self-search with reflection hurt performance compared to naive repeated sampling. Does this imply that SSRL is less about improving the model's reasoning process and more about optimizing the simple, one-step extraction of its existing parametric knowledge?\n\n2. Since authors are motivated by strong TTS results, where increasing the sample size at inference time significantly improves performance. However, the ablation in Appendix C.3.8 shows that increasing group size at training time for the GRPO algorithm provides limited to no benefit. Why do the benefits of a larger sample size diminish during training, when TTS proves that a wide range of high-reward trajectories already exist within the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gGV4ZtV2bM", "forum": "WgRyl6fzLx", "replyto": "WgRyl6fzLx", "signatures": ["ICLR.cc/2026/Conference/Submission6662/Reviewer_MNtC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6662/Reviewer_MNtC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046234735, "cdate": 1762046234735, "tmdate": 1762918971900, "mdate": 1762918971900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}