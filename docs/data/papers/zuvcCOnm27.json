{"id": "zuvcCOnm27", "number": 2043, "cdate": 1756981019649, "mdate": 1759898172615, "content": {"title": "Data Efficacy for Language Model Training", "abstract": "Data is fundamental to the training of language models (LM). \nRecent research has focused on data efficiency, aiming to reduce data scale without compromising model performance.\nHowever, **data efficacy**, emphasizes improving model performance by optimizing the utilization of training data, is an area that remains underexplored.\nTo enhance it, we propose novel methods for both data ordering and data scoring. \nFor data ordering, we design *Folding Ordering (FO)*, which addresses challenges such as data distribution bias and model forgetting introduced by traditional curriculum learning.\nFor data scoring, we present *Learnability-Quality Scoring (LQS)*, the first method specifically designed to support both data ordering and selection.\nTo further establish the foundation for data efficacy, a general paradigm, **DELT** (**D**ata **E**fficacy for **L**M **T**raining), is introduced to underscore the importance of training data utilization.  It comprises two essential modules: data scoring and data ordering, along with one optional module of data selection.\nThis primarily enables DELT to improve data efficacy as well as efficiency.\nComprehensive experiments validate our approach, demonstrating that FO and LQS significantly improve LM performance across various settings, consistently surpassing existing baselines.\nWe believe that data efficacy, which aims to fully harness data value without altering data scale and model size to benefit model performance, is a promising foundational area in LM training.", "tldr": "", "keywords": ["data efficacy; language models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9946feceecccd8ac708606d3d17d52246895d7b6.pdf", "supplementary_material": "/attachment/ddb78b12426cee1be679cd08d03fe9a1f8834e71.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a two-step framework, DLET, designed to improve data efficiency in LLM pre-training. It first scores a small subset of the training data, then trains a scoring model (similar to the reward model in RLHF), and finally employs this model to sort the training data using a novel folding approach.\n\nThe first contribution is the proposed scoring function, LQS, which evaluates the reliability, quality, and learnability of each training sample $x_n$ as $\\gamma_n = \\mathbf{R}(\\boldsymbol{\\theta}_{t+1}) \\cdot \\mathbf{Q}(x_n) \\cdot \\mathbf{L}(x_n)$.\nThe reliability function $R$ corresponds to the magnitude of the target vector $\\lambda$ introduced by PDS. The quality function $Q$ measures the cosine similarity between the target vector $\\lambda$ and the current gradient. Finally, the learnability function $L$ is defined as the cumulative fraction of gradients between the current and next steps.\n\nThe second contribution is the folding learning strategy, in which training samples are randomly divided, typically into three parts, and within each part, the data are greedily sorted according to their assigned scores."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The experiments are conducted in a comprehensive and convincing manner. Specifically, the comparisons cover three model scales, 160M, 470M, and 1B parameters, as well as strong baselines, such as PDS. The evaluation includes both pre-training and post-training results on the code and math domains. The ablation studies are also appropriately designed, focusing on (1) the folding learning strategy and (2) the proposed scoring function. Overall, I find the experimental evaluation to be thorough and persuasive."}, "weaknesses": {"value": "**W1** The organization of the paper is somewhat confusing to me.\n\n(1) The proposed scoring function, **LQS**, is composed of three key elements. However, only two of them—the quality and learnability functions—are described in the main body, while the reliability function is deferred to the appendix (page 16). It is unclear why such an important component is omitted from the main discussion, and this may lead to misunderstandings about the formulation of the proposed scoring function (e.g., Eq. 5).\n\n(2) I personally find that **Section 4, “DELT Paradigm,”** adds limited clarity or new insight. In contrast, some critical implementation details are missing from the main body, such as the fact that the scoring model is trained on a small subset of the training data, and that most score annotations are generated by this model’s predictions.\n\n**W2** The **motivation for the folding learning** component also appears weak. While I appreciate the comprehensive experimental evaluation that supports its performance gains, the paper’s motivation for this technique is rather superficial. The single-sentence justification, that “they often face challenges such as data distribution bias and model forgetting”, does not sufficiently explain the conceptual reasoning behind folding learning. Moreover, I find the evidence presented in Table 14 (page 21), which claims that folding enhances data diversity, unconvincing.\n\n**W3** The novelty of the proposed quality and reliability functions seems limited. The quality function is defined as the (cumulative) cosine distance between the target vector and the current gradient, while the reliability function corresponds to the norm of the target vector. Since the target vector itself is directly borrowed from the well-known PDS paper, it is difficult to regard these components as novel contributions."}, "questions": {"value": "I do not have any questions at this stage. Although the organization of the paper is somewhat weak, it is clear that the authors have presented the technical details and experimental results thoroughly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AQBEvODVyG", "forum": "zuvcCOnm27", "replyto": "zuvcCOnm27", "signatures": ["ICLR.cc/2026/Conference/Submission2043/Reviewer_dGm4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2043/Reviewer_dGm4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760978608897, "cdate": 1760978608897, "tmdate": 1762916003759, "mdate": 1762916003759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on data efficacy in LLM training—optimizing data utilization to boost performance, distinct from data efficiency . It proposes Folding Ordering (FO) to mitigate distribution bias and model forgetting of traditional curriculum learning by reorganizing sorted data at fixed intervals ; Learnability-Quality Scoring (LQS), the first method unifying ordering and selection, via gradient-based learnability and quality metrics ; and the DELT paradigm (integrating scoring, ordering, optional selection) to harness data value without altering data/model . Experiments across model sizes (160M–1.7B), data scales (1B–50B tokens), and domains (general NLP, math, code) show DELT outperforms baselines by up to 1.65% in average accuracy, excelling in efficacy and efficiency . It concludes data efficacy is a promising LLM training area ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It proposes innovative, practical methods: Folding Ordering  mitigates data distribution bias and model forgetting of traditional curriculum learning ; Learnability-Quality Scoring , the first method unifying data ordering and selection, uses gradient-based metrics for robust sample evaluation . The DELT paradigm integrates these with optional selection, enhancing efficacy/efficiency without altering data or model . Comprehensive experiments (160M–1.7B models, 1B–50B tokens, general NLP/math/code) show DELT outperforms baselines by 1.65% in average accuracy, proving broad robustness . These make it a impactful foundation for LLM training."}, "weaknesses": {"value": "1. LQS relies on small, high-quality datasets to compute downstream loss for scoring.\n2. It lacks validation on larger LMs (e.g., 10B+ parameters) or exascale datasets, leaving uncertainty about scalability in state-of-the-art LLM training.\n3. The optimal folding layer L=3 for FO is empirically determined but lacks mechanistic explanation, increasing adoption friction for practitioners . These gaps restrict its generalizability."}, "questions": {"value": "As shown in the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gDHJ3AT4M9", "forum": "zuvcCOnm27", "replyto": "zuvcCOnm27", "signatures": ["ICLR.cc/2026/Conference/Submission2043/Reviewer_RCEA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2043/Reviewer_RCEA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830010411, "cdate": 1761830010411, "tmdate": 1762916003510, "mdate": 1762916003510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new combination of data scoring and data ordering for LLM pretraining. Each datapoint is scored based on its learnability, i.e. how much it decreases loss during training, and its quality, i.e. how much the gradient of the loss w/r/t this datapoint aligns with the average gradient of the loss. For data ordering, the paper proposes \"folding ordering\", which takes the data scores and repeats curriculum learning several times rather than sorting data in ascending order by score. Comprehensive experiments show improvements over random shuffling and data selection baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a new combination of data scoring and data ordering methods and comprehensively evaluates them for LLM pretraining.\n- The main results in Table 1 show that compared to randomly shuffled data, the proposed combination of data scoring and folding ordering yields improvements of up to 1.7 points of accuracy on 8 downstream tasks.\n- The results showing that folding ordering outperforms random shuffling (Table 4) are very compelling, with folding outperforming shuffling in every evaluated setting, regardless of scoring method."}, "weaknesses": {"value": "I have no major concerns about this paper. The primary weakness is that the improvements in data efficiency and efficacy are small beyond baselines. The results in Table 1 do show improvements over random shuffling, but it's not clear whether they warrant the added complexity of the approach.  For another example, in Table 3 where different scoring methods used with the same ordering method, the proposed method outperforms baselines for 4 out of 8 tasks, with an average improvement of 0.69 points of accuracy over the next best baseline.\n\nGiven the small improvements in general, the conclusions would be strengthened by replications in order to determine statistical significance, though I understand other papers on this topic typically do not do so because of the compute requirements. At the very least, including the standard deviation over the three random seeds for conventional random shuffling in Table 1 would help contextualize those numbers."}, "questions": {"value": "- It seems like the learnability score in Equation 2 depends on the data ordering used for training. Do you account for this at all?\n- The general form of the experimental setup in Section 5.1 should be described more clearly. My understanding is that 1) most experiments pre-train the Mistral architecture at various parameter sizes from scratch on subsets of the RedPajama dataset, and 2) Table 5 alone shows the effect of the method for post-training Qwen1.5-0.5B. What model and dataset size combinations are used for the pre-training results in Tables 2, 3, and 4?\n\nSmall comments:\n- Figure 1 is so zoomed in that it exaggerates the differences between methods, in my opinion, by not showing the 0 point on the y-axis.\n- Line 76: \"Folding Learning (FO)\" -> \"Folding Ordering (FO)\"\n- Use `\\citet{}` instead of `\\citep{}` for in-text citations, like those in the first paragraph of Section 2.3.\n- Line 219: \"N is the quantity of \\theta\" -> I think you mean \"N is the dimension of \\theta\"\n- Line 380: \"We presents\" -> \"We present\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aFxpSkzr28", "forum": "zuvcCOnm27", "replyto": "zuvcCOnm27", "signatures": ["ICLR.cc/2026/Conference/Submission2043/Reviewer_WZPA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2043/Reviewer_WZPA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890886926, "cdate": 1761890886926, "tmdate": 1762916003195, "mdate": 1762916003195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies data efficacy for language model training with aim at improving model performance by optimizing the way of utilizing data. It features two strategies for data scoring and ordering, called folding ordering and learnability-quality scoring, respectively. Optionally, data selection, for efficiency, can be incorporated between the applications of two strategies in the proposed pipeline called DELT. Extensive experiments are conducted to demonstrate the superiority of this strategy combination."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The two strategies proposed for improving data utilization looks simple yet effective\n2. Propose a general framework for training data utilization\n3. Extensive experiments are conducted to verify the effectiveness of the proposed strategies for utilizing data"}, "weaknesses": {"value": "1. It remains unknown at all levels except for experiments why folding ordering is better than curriculum learning.\n2. The data scoring strategy lacks theoretical support, and is more like heuristics\n3. LQS is calculated only using a small number of data points, which raises news questions on how to select these data points and how to ensure no bias\n4. The experimental setting could be made clearer, e.g., by drawing the whole workflow that covers all about the setting\n5. In Line 432, \"For the LQS method, it outperforms other baselines across different ordering methods and selection ratios\". This claim seems incorrect. For example, LQS performs worst for folding ordering  and selection ratio .9 in Table 4"}, "questions": {"value": "1. In Line 190-191, \"Folding learning not only inherits the benefits from curriculum learning but also mitigates issues of model forgetting, and data distribution bias\". Why?\n2. In Line 232 \"Conversely, noisy samples or those with unstable gradients yield a low learnability score\". Why? \n3. In Line 244, the target vector $\\lambda$ is calculated by following Gu et al. (2025). What's the connection of your strategies to this work and why?\n4. For data selection, the score vector is sorted in ascending order and the last K samples are discarded. But    it is mentioned in Line 254 that \"samples with large score values have higher quality and significant contributions to reducing the downstream loss J(θ), especially when introduced during later training stages\". Does this mean that the top-K samples are discarded?\n5. In Section 5, what does \"conventional\" mean exactly in Table 1? It hides a lot of details. What's the metric for results there?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4GIVAxOiH7", "forum": "zuvcCOnm27", "replyto": "zuvcCOnm27", "signatures": ["ICLR.cc/2026/Conference/Submission2043/Reviewer_xC4U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2043/Reviewer_xC4U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999963083, "cdate": 1761999963083, "tmdate": 1762916003052, "mdate": 1762916003052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}