{"id": "9uChwgWHX9", "number": 24085, "cdate": 1758352575768, "mdate": 1759896782353, "content": {"title": "Design Principles for Sequence Models via Coefficient Dynamics", "abstract": "Deep sequence models, ranging from Transformers and State Space Models  (SSMs) to more recent approaches such as gated linear RNNs, fundamentally compute outputs as linear combinations of past value vectors. To draw insights and systematically compare such architectures, we develop a unified framework that makes this output operation explicit, by casting the linear combination coefficients as the outputs of autonomous linear dynamical systems driven by impulse inputs. This viewpoint, in spirit substantially different from approaches focusing on connecting linear RNNs with linear attention, reveals a common mathematical theme across diverse architectures and crucially captures softmax attention, on top of RNNs, SSMs, and related models. In contrast to new model proposals that are commonly evaluated on benchmarks, we derive design principles linking architectural choices to model properties. Thereby identifying tradeoffs between expressivity and efficient implementation, geometric constraints on input selectivity, and stability conditions for numerically stable training and information retention. By connecting several insights and observations from recent literature, the framework both explains empirical successes of recent designs and provides guiding principles for systematically designing new sequence model architectures.", "tldr": "Mixer blocks compute linear combinations of value vectors; we model their coefficients as outputs of autonomous linear dynamics with impulse inputs, unifying models from attention to SSMs and yielding design principles for sequence model design.", "keywords": ["Sequence models", "Design principles", "Coefficient dynamics"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ba7218772bdf1386ec46825ed03f3b506371c5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified theoretical framework for viewing sequence models (including Transformers, recurrent neural networks (RNNs), and state-space models (SSMs)) through the lens of **coefficient dynamics**.\nThe authors model the coefficients alpha in the linear combination as outputs of an autonomous linear dynamical system driven by *impulse inputs*.\nThey claim this representation reveals shared mathematical structure among diverse sequence architectures, enabling the derivation of six **“design principles”** related to:\n\n1. Linear vs. nonlinear readout maps and their efficiency tradeoffs\n2. Input selectivity through the geometry of zero set\n3. Encoding positional information via non-identity evolution matrices\n4. Structured choices of A_t matrix (e.g., diagonal or Householder)\n5. Proper scaling of injection parameters (b_j)\n6. Normalization factors to ensure stability.\n\nExperiments on synthetic tasks (MAD benchmark) empirically test these principles, showing expected patterns such as improved selectivity with larger zero sets and the sufficiency of (A_t that is not equal to I) to encode position without embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Clear and mathematically consistent formulation.\n* Provides a clean pedagogical summary connecting RNNs, attention, and SSMs under one algebraic form.\n* Well-presented with readable equations and illustrative diagrams.\n* Serves as a potential tutorial reference for newcomers to the field."}, "weaknesses": {"value": "* **No novel theoretical result:** All lemmas rederive existing intuitions without advancing formal understanding.\n* **Weak experimental validation:** Evaluations on simple synthetic tasks (MAD) do not test scalability, language modeling, or real-world data.\n* **Limited empirical novelty:** Trends (e.g., gating helps, non-identity (A_t) adds positional info) are already widely known.\n* **Incomplete discussion of prior work:** The paper fails to acknowledge prior theoretical frameworks that make similar connections. No point is not already known. I don't see anything in this paper that is novel and that we did not already know from training deep sequence models."}, "questions": {"value": "1. **Scope of derivations:** Are any of your truly lemmas new, or are they reinterpretations of existing SSM analyses?\n2. **Experimental depth:** Can you test your design principles on real-world data (e.g., language modeling or speech) to demonstrate their usefulness beyond toy MAD benchmarks?\n3. **Relation to Mamba and Selective SSMs:** Since these models already embody your principles (learned (A_t,b_j), stability constraints), how does your framework offer new insights beyond theirs?\n4. **Extension to multi-layer architectures:** The study is single-layer. How do the principles scale in multi-layer or cross-attention settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3fszABTcv0", "forum": "9uChwgWHX9", "replyto": "9uChwgWHX9", "signatures": ["ICLR.cc/2026/Conference/Submission24085/Reviewer_8c4e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24085/Reviewer_8c4e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760649330590, "cdate": 1760649330590, "tmdate": 1762942930445, "mdate": 1762942930445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a number of principles for designing sequence modeling layers. To come up with such proposals, the paper provides a survey of previous works, and mechanistically separate parts into A, b, \\phi, \\eta, and \\alpha, highlighting major different parts. Coming up with six principles, the authors provide ablations for each, validating their claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well structured, proposing several ideas that not only help designing new architectures but also understand existing methods. The principles that are introduced in the paper are simple, thus can be easily adopted. Each proposal is backed up with an experiment."}, "weaknesses": {"value": "My major concern of this paper is that the ideas are only validated using synthetic benchmarks as these benchmarks cannot model all real-world issues, and quite vulnerable to training setups such as weight decay and learning rates. Since the paper comes up with the principles that each should improve the model, I believe the authors should have used some real-world datasets (say language modeling) and benchmark at least their best model that all the principles are applied.\n\nAdditionally, I also have concerns with some principles. For example, Principle 1 and 5 are quite trivial which are already well known among the community, and Principle 6 is not specific enough (what specifically is an unstable A?). Also, I wonder if Principle 3 is actually correct: for instance, attention-based autoregressive models without positional embeddings (i.e., NoPE) has shown promising results."}, "questions": {"value": "- What would be the model with all the principles applied, and how does it perform on language modeling?\n\n- Is Principle 3 true?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zU9I1V784T", "forum": "9uChwgWHX9", "replyto": "9uChwgWHX9", "signatures": ["ICLR.cc/2026/Conference/Submission24085/Reviewer_gmof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24085/Reviewer_gmof"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948001876, "cdate": 1761948001876, "tmdate": 1762942930149, "mdate": 1762942930149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified theoretical framework to describe sequence models including Transformer and RNNs, and puts forward a series of design principles and experimental verifications to guide sequence modeling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The design principles for sequence modeling architecture proposed in this paper are instructive.\n2. The analysis combining theory and experiment is convincing."}, "weaknesses": {"value": "A unified sequence modeling framework for transformer and RNN model architectures has been mentioned in various works, such as MetaLA [1], PaTH Attention [2], and log-linear-attention [3], which may diminish the contribution of this paper. Therefore, further comparison and discussion with similar related works will help highlight the contribution of this paper.\n\n[1] Yuhong Chou, et al. MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map. NeurIPS, 2024.\n[2] Songlin Yang, et al. PaTH Attention: Position Encoding via Accumulating Householder Transformations. NeurIPS, 2025.\n[3] Han Guo, et al. Log-Linear Attention. arXiv, 2025."}, "questions": {"value": "1. Based on the design principles of the sequence modeling architecture proposed in the paper, what characteristics should the optimal model architecture for sequence modeling?\n2. Although these design principles can guide the design of the optimal sequence modeling architecture, for linear RNN architectures, the design of some components may not be conducive to hardware efficient parallel training of RNN. In this case, would a more general and expressive matrix gate be less useful than a simpler, more efficiently trained diagonal/scaler gate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0Yz95wChzM", "forum": "9uChwgWHX9", "replyto": "9uChwgWHX9", "signatures": ["ICLR.cc/2026/Conference/Submission24085/Reviewer_gPgH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24085/Reviewer_gPgH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992425360, "cdate": 1761992425360, "tmdate": 1762942929609, "mdate": 1762942929609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper proposes a unified theoretical framework (Coefficient Dynamics) for analyzing sequence models, including Transformers, linear attention, and State Space Models (SSMs). The framework formalizes the fact that these models compute outputs as linear combinations of past tokens, and interprets the outputs of *linear dynamical systems driven by impulse inputs*.\n\n2. Unlike prior unification approaches, this formulation explicitly introduces a per-token index, j, representing each previous key/value, conceptually similar to a KV-cache, and shows Transformers, SSMs, Linear Attention-like architectures can be expressed as special cases.\n\n\nBuilding on this framework, the authors derive a set of design principles:\n\n  1. **Linearity of $\\phi$:** Only linear readout maps permit parallel recurrent computation.\n  2. **Input selectivity:** Nonlinear $\\phi$ enables sparse coefficients aka selectivity.\n  3. **Positional information:** Setting $A_t \\neq I$ embeds position into coefficients; $A_t = I$ requires positional embeddings.\n  4. **Evolution structure:** The choice of $A_t$ (scalar, diagonal, Householder) allows for transformations such as scaling or rotation.\n  5. **Scaling of $b_j$:** Proper scaling of $b_j$, prevents variance blow-up with increasing hidden state.\n  6. **Normalization $\\eta_i$:** Stable training requires normalization of coefficient magnitudes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and the mathematical formulation is rigorous.\n2. Unifies existing insights on SSMs, RNNs, and attention into a single framework summarizing core principles like linearity, efficiency, input selectivity, normalization, and stability. This is pedagogically useful for newcomers to the field."}, "weaknesses": {"value": "### **On the framework**\n\nThe “coefficient dynamics” construction, builds on standard frameworks like Dynamical Systems Framework with the new ingredient being the explicit *per token j index*, which is equivalent to maintaining a key–value (KV) cache. In my opinion this viewpoint is not novel in a theoretical sense: prior work (e.g., Dao & Gu, 2024; Sieber et al., 2024) already expresses attention and SSMs as linear recurrences or matrix multiplications over past states. The main change, vis-a-vis previous works, is the KV cache-like formulation to unify attention without using infinite state sizes. \n\n---\n\n### **On Principle 1 ($\\phi$ must be linear for parallelization)**\n\nIn my opinion, this is a well-known result in the subquadratic. Specifically, it is known that only linear readouts permit associative-scan like formulations required for efficient recurrent computation. This principle has been used in multiple works (Linear Attention, Mamba, Mamba-2, Gated Deltanet). While the lemma is correct, its inclusion as a “new principle” adds little beyond reiterating that *linear functions yield linear complexity*.\n\n---\n\n### **On Principle 2 (Input selectivity and geometry of $\\phi$)**\n\nThe main idea—that nonlinear ϕ enables suppression of uninformative tokens while linear ϕ limits selectivity—is sound and nicely presented. However, this observation is well known and connects to classic results on **associative memory capacity**, where linear associative memories can store only ~n patterns in dimension n. \n\nAs a nit remark: the follow-up discussion \"Can learnable parameters save us?\" is technically correct but does not logically follow from the principle, since modifying $A_t$ or $b_j$ changes only the key–value dynamics, not the query-dependent coefficients $\\alpha_{ij}$.\n\nRemark: Authors claim that Linear Attention has a readout of the form $\\psi(\\cdot)\\psi(\\cdot)$, but this is not really a function acting post the readout as defined and hence does not fit with the framework definition. It is better viewed as a preprocessing trick rather than a part of the framework.\n\n---\n\n### **On Principle 3 (Positional Information)**\n\nThe result that $A_t = I$, which implies that per-query token the sequence mixing process is permutation invariant and hence requires position embeddings is also well known and has been discussed in prior works. Authors of Mamba mention that due to the decay, the operation is no longer permutation invariant and hence does not require position embeddings. In my opinion, the lemma correctly states this but adds no new insight.\n\n---\n\n### **On Principle 4 (Structure of $A_t$)**\n\nThe fact that the structure of the state transition matrix $A_t$ (scalar, diagonal, Householder) limits the operation that can be performed on the keys is a tautological statement for me as the state transition matrix is what acts on the keys to produce the output. In the Lemma, authors simple summarize the actions performed by scalar, diagonal, or Householder matrix operators on the keys being acted upon.\n\n---\n\n### **On Principles 5 & 6 (Scaling and normalization)**\n\nThe discussion on $b_j = O(1/\\sqrt{n})$ to maintain $O(1)$ variance and the normalization of coefficients $\\alpha_{ij}$ to avoid exploding norms repeats standard initialization theory (Glorot & Bengio, 2010; Vaswani et al., 2017). While correctly stated, these are rules of thumbs which are widely used in architecture design to ensure that variance remains bounded in deep models. In my opinion, their inclusion as novel “principles” is overstated.\n\n---\n\nMy overall assessment of novelty is that the paper’s strength lies in gathering well-known rules of thumbs under a shared formalism, but every individual principle has been studied or applied before."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YrvKNNK1FE", "forum": "9uChwgWHX9", "replyto": "9uChwgWHX9", "signatures": ["ICLR.cc/2026/Conference/Submission24085/Reviewer_SX4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24085/Reviewer_SX4M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762238376203, "cdate": 1762238376203, "tmdate": 1762942928802, "mdate": 1762942928802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}