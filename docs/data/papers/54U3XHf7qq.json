{"id": "54U3XHf7qq", "number": 1717, "cdate": 1756911138365, "mdate": 1759898192513, "content": {"title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation", "abstract": "Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and π0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline.", "tldr": "We propose MemoryVLA, a Cognition-Memory-Action framework for robotic manipulation, inspired by human hippocampus mechanism.", "keywords": ["Embodied AI", "Vision-Language-Action Models", "robotic manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd7a1f25e9118e29df73c06c00665ddfe065be86.pdf", "supplementary_material": "/attachment/1c9b7aa947c48e3f3ffc3128f34121e387d82e40.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MemoryVLA, a Cognition-Memory-Action framework for robotic manipulation that addresses the limitation of mainstream VLA models in handling long-horizon, temporally dependent tasks. Inspired by human memory systems, the approach features a Perceptual-Cognitive Memory Bank that consolidates low-level visual details and high-level semantic information, which working memory retrieves and fuses with current observations to condition a diffusion-based action expert."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a concrete limitation in existing VLA models (ignoring temporal context) and proposes a practical memory bank mechanism with retrieval, fusion, and consolidation operations. The dual-stream design (perceptual + cognitive tokens) with gated fusion is technically sound and the memory consolidation strategy effectively manages computational costs.\n\n2. The method achieves substantial improvements over strong baselines, with comprehensive experiments across 3 robots, and thorough ablations validating each design choice. The real-world deployment on both Franka and WidowX robots demonstrates practical applicability beyond simulation."}, "weaknesses": {"value": "1. The paper does not report inference time and memory footprint comparison with baselines. The memory retrieval via cross-attention at each timestep, especially with memory lengths up to 256, likely incurs significant computational overhead that could limit real-time deployment.\n\n2. The paper lacks visualization or analysis of retrieved memory contents. It's unclear whether the memory bank actually retrieves semantically/temporally relevant contexts or if the gains simply come from having more visual history. Attention weight visualization or case studies showing retrieved frames would strengthen the claims.\n\n3. Ablations (Tables 5-6) are only conducted on SimplerEnv-Bridge. It's unclear if the design choices (e.g., memory length=16, gate fusion, token merge) generalize to other benchmarks like LIBERO or real-world tasks where temporal dependencies may differ.\n\n4. Limited generalization analysis:\n* Task generalization: While OOD robustness is tested with visual variations (backgrounds, lighting, occlusion), there's no evaluation of zero-shot generalization to unseen task categories, which is crucial for a general-purpose VLA model.\n\n* Memory capacity analysis: For long-horizon tasks, is memory length of 256 sufficient? The paper doesn't analyze what happens when task horizon exceeds memory capacity, or whether the consolidation strategy causes information loss for very long sequences.\n\n* Benchmark selection for ablations: Tables 5-6 conduct ablations only on SimplerEnv-Bridge, which arguably doesn't require strong temporal dependencies (as the tasks are relatively simple pick-and-place). Ablations should be conducted on benchmarks where temporal reasoning is critical (e.g., real-world long-horizon tasks) to validate design choices."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JYoGamsT4w", "forum": "54U3XHf7qq", "replyto": "54U3XHf7qq", "signatures": ["ICLR.cc/2026/Conference/Submission1717/Reviewer_MmT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1717/Reviewer_MmT3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875706548, "cdate": 1761875706548, "tmdate": 1762915866392, "mdate": 1762915866392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles core gap in VLA which is weak temporal reasoning for long-horizon, non-markovian manipulation, and they do so by drawing on cognition-memory-action architecture with two complementary memory systems. A pretrained VLM converts observations into (i) perceptual tokens and (ii) higher-level cognitive tokens that together serve as working memory for immediate control. In parallel, a Perceptual-Cognitive Memory Bank accumulates both low-level details and high-level semantic “gist.” At each step, working memory retrieves decision-relevant entries from the bank, adapts/fuses them with current tokens, and updates the bank by merging redundancies. A memory-conditioned diffusion action expert then produces temporally aware action sequences."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The working-memory vs. long-term (episodic + semantic) split is directly inspired by human memory and mapped cleanly to a VLA stack (perceptual/cognitive tokens + Perceptual-Cognitive Memory Bank). This makes the temporal modeling choice easy to justify and reason about.\n\n2. Converting observations into perceptual and cogntiive tokens enable lightweight retrieval, fusion and consolidations.\n\n3. Good performance on SimplerEnv-Bridge benchmark and LIBERO."}, "weaknesses": {"value": "1,Benchmark mismatch (memory not actually required).\n\nFundamentally, the simulation benchmark used does not evaluate memory: the tasks appear in-distribution, short-horizon, and solvable without non-Markovian reasoning. I recommend evaluating on a benchmark that explicitly requires memory, such as Memory-Bench (from SAM2Act), to substantiate the paper’s claims.\n\n2.Inadequate baselines (no memory or long-context retrieval).\n\nThe chosen baselines are not memory-enhanced and do not leverage long context or retrieval, making it difficult to attribute gains to the proposed method. Please compare against baselines that incorporate memory or retrieval to fairly assess effectiveness."}, "questions": {"value": "The paper should address its two major weaknesses; without doing so, its claims will remain insufficiently supported. I would consider change the rating if my questions can be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EuQj2ed0jx", "forum": "54U3XHf7qq", "replyto": "54U3XHf7qq", "signatures": ["ICLR.cc/2026/Conference/Submission1717/Reviewer_5ryq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1717/Reviewer_5ryq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933479986, "cdate": 1761933479986, "tmdate": 1762915866206, "mdate": 1762915866206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new model, MemoryVLA, which employs a specialized memory bank designed to better handle temporal dependencies."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Incorporating memory mechanisms into VLAs is a highly relevant and important research direction.\n2. The paper presents a large number of experiments, including those conducted on a real robot.\n3. The work is well written and easy to read."}, "weaknesses": {"value": "1. Despite the large number of experiments, the main drawback of the paper is that most of the tasks used do not actually require a memory mechanism.  The authors should conduct comparisons on specialized robotics benchmarks focused on memory-based tasks, such as Mikasa-Robo [1] and MemoryBench [2]. Without these experiments, it is impossible to properly evaluate the effectiveness of the proposed memory mechanism.\n2. The results on LIBERO outperform Discrete Diffusion VLA [3] by only 0.3, even though the latter does not use any memory mechanisms. This again raises questions about the suitability of the chosen benchmarks for evaluation.\n3. The functioning of the memory mechanism is not demonstrated clearly, it can only be inferred indirectly from the overall model performance. It is important to show that the memory bank retrieves relevant elements, providing direct evidence of how the mechanism contributes to task solving.\n\nIn its current state, I believe that, despite addressing an important direction, the paper does not sufficiently demonstrate that the proposed memory mechanism truly helps solve complex memory-dependent tasks. This is primarily due to testing mostly on simple tasks that do not require memory, as well as the lack of an in-depth analysis of the memory mechanism itself.\n\nI am willing to reconsider my evaluation if these shortcomings are addressed.\n\nReferences:\n1. Cherepanov, Egor, et al. \"Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning.\" arXiv preprint arXiv:2502.10550 (2025). \n2. Fang, Haoquan, et al. \"Sam2act: Integrating visual foundation model with a memory architecture for robotic manipulation.\" arXiv preprint arXiv:2501.18564 (2025).\n3. Liang, Zhixuan, et al. \"Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies.\" arXiv preprint arXiv:2508.20072 (2025)."}, "questions": {"value": "1. The main question is how the proposed model performs on tasks from Mikasa-Robo [1] and MemoryBench [2].\n2. In [1], it was shown that using an action chunk of large size can circumvent the need for a memory mechanism to solve tasks. In MemoryVLA, a chunk of size T = 16 is used, predicting 16 steps ahead. How would the model behave if this value were reduced? In which cases does performance improvement come from using a long chunk, and in which cases from the memory mechanism itself?\n3. What is the number of steps (actions) required to solve the tasks (mean, minimum, maximum, and median), including on the real robot?\n4. How do the elements retrieved from the memory bank correspond to the task being solved at a given moment?\n5. Why does the proposed model perform so much better than CogACT in real-robot experiments compared to simulation? Were the models trained under comparable conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CFXg9Zfcf0", "forum": "54U3XHf7qq", "replyto": "54U3XHf7qq", "signatures": ["ICLR.cc/2026/Conference/Submission1717/Reviewer_PZt6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1717/Reviewer_PZt6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977735168, "cdate": 1761977735168, "tmdate": 1762915866073, "mdate": 1762915866073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles non-Markovian, long-horizon robotic manipulation where single-frame VLAs fail. It proposes MemoryVLA, a Cognition-Memory–Action framework that (i) converts a single RGB frame + instruction into perceptual tokens (DINOv2+SigLIP compressed via SE-bottleneck) and a single cognitive token (EOS from LLaMA-7B), (ii) stores both streams in a Perceptual–Cognitive Memory Bank (PCMB), and (iii) performs retrieval (cross-attention + timestep PE), gate fusion, and consolidation (adjacent-pair merge by cosine similarity) before a DiT+DDIM action head predicts a 16-step continuous 7-DoF trajectory. The authors conduct an extensive evaluation across 150+ tasks in simulation (SimplerEnv, LIBERO) and the real world."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Clear structure with good motivation: the task of handling of non-Markovian tasks is interesting and significant in robotics, where the motivation fused memory term within the architecture design.\n* Extensive evaluation: the authors evaluate MemoryVLA across three different robots, three distinct simulation benchmarks (SimplerEnv-Bridge, SimplerEnv-Fractal, LIBERO), and a set of 12 real-world tasks. This comprehensive evaluation on 150+ tasks with 500+ variations provides high confidence in the method's effectiveness and generalizability.\n* Significant performance gain: the model achieves state-of-the-art performance across all benchmarks. The standout result is the +26 point improvement over the next-best baseline on real-world long-horizon tasks."}, "weaknesses": {"value": "* The ambiguity of optimal memory length: from the ablation study in Table 5, it suggests that a memory length of $L=16$ is optimal (71.9% success), while the performance worsens at $L=64$ (67.7%). However, in the Appendix, the authors state that a memory length of $L=256$ was used for real-world long-horizon tasks. There lacks of in-depth analysis of how memory length is associated with the actual performance.\n* The mechanism of using single cognitive token: for complex tasks require multiple latent hypotheses, one EOS token could be lossy. The paper doesn’t probe whether more cognitive capacity helps or hurts."}, "questions": {"value": "* Although the model shows strong robustness to many OOD variations, the performance drops sharply when viewpoint changes in Appendix (Sec. C). Does that suggest the learned perceptual features are highly view-dependent and that the memory module may be memorizing visual details rather than an abstracted spatial representation?\n* How does the memory length affect the performance of the model?\n* How does different memory modules work independently? Can you provide a qualitative example of a task failure that occurs with only cognitive memory (i.e., lacking perceptual detail) and a failure that occurs with only perceptual memory (i.e., lacking cognitive gist)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cMhA83ROXP", "forum": "54U3XHf7qq", "replyto": "54U3XHf7qq", "signatures": ["ICLR.cc/2026/Conference/Submission1717/Reviewer_F2kP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1717/Reviewer_F2kP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998626542, "cdate": 1761998626542, "tmdate": 1762915865860, "mdate": 1762915865860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}