{"id": "E7JzkZCofa", "number": 7052, "cdate": 1758006114586, "mdate": 1759897874999, "content": {"title": "Sat3DGen: Comprehensive Street-Level 3D Scene Generation from Single Satellite Image", "abstract": "Street‑level 3D generated from a single satellite image enables mapping, robotics, simulation, and media creation at scale, but existing methods trade off semantics and geometry: geometry‑colorization pipelines yield clean yet building‑only reconstructions, while proxy‑based renderers preserve satellite semantics but produce coarse, unstable geometry. We propose Sat3DGen, a feed‑forward satellite‑to‑3D framework that learns a differentiable 3D field under 2D supervision, with a focus on scene‑level geometric quality. The method introduces three components to address warped geometry, boundary errors, and rooftop ambiguity: a gravity‑based density variation loss that biases the volumetric field toward gravity‑aligned structures; a spatial token that expands the effective ground‑plane extent to stabilize the peripheral layout; and a monocular relative‑depth prior that constrains satellite‑view depth. Besides, we further strengthen the supervision, without extra data sources, by jointly training on high‑resolution panoramas and their projected perspective views to increase viewpoint coverage and photometric consistency. Although Sat3DGen includes no tailored image‑quality modules, it reduces FID on the unseen‑city split of VIGOR‑OOD from about 40 to 19 compared to state-of-the-art satellite-to-street-view video model Sat2Density++, with improvements attributable to more accurate, view‑consistent geometry. The resulting 3D assets support downstream applications, including semantic‑map‑to‑3D synthesis, surround‑view multi‑camera video generation from satellite imagery, large‑area mesh construction, and single‑image DSM estimation without depth ground truth.", "tldr": "", "keywords": ["3D generation", "novel view synthesis", "satellite to street-view generation", "feed-forward image to 3D", "outdoor scene generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afcd7e6aaacffe3cf99f4cdd77805ae1c80854cb.pdf", "supplementary_material": "/attachment/389974374195156303ea26c648735b8b7014a253.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Sat3DGen, a framework designed to generate comprehensive street-level 3D scenes from a single satellite image. The method incorporates several techniques — a gravity-based density variation loss, spatial token padding, and a monocular relative-depth prior — to enhance the performance of Sat2Density++. However, the overall architecture remains almost identical to Sat2Density, leading to concerns about the lack of novelty."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors tackle a meaningful and challenging task: generating realistic ground-level 3D scenes from a single satellite image.\n2. The proposed gravity-based density variation loss, spatial token padding, and monocular relative-depth prior improve upon the previous Sat2Density++ framework."}, "weaknesses": {"value": "**1. The use of DINO-v3**\n\nThe model employs DINO-v3, which is computationally expensive. It is unclear how the inference speed and GPU memory consumption are affected. Moreover, it remains uncertain whether the performance gains primarily come from DINO-v3, rather than the proposed method itself.\n\n**2. Unclear explanation of the Gravity-based Density Variation Loss (Lines 241–254)**\n\n* The mathematical formulation is ambiguous. Is x defined in Line 197 as a 3D point?\n* Why does δx (a scaled 3D point) represent “along gravity”?\n* The statement “lower-altitude points usually have density that is no smaller than higher-altitude points” seems empirical — is there any theoretical justification?\n\n**3. Depth estimation and supervision issues (Line 258–265)**\n* How accurate is the depth estimated by Depth Anything v2 when applied to satellite imagery?\n* What happens if the estimated depth is inaccurate?\n* Why is depth only used as a loss term instead of being fused into the network representation?\n* The supervision on spatial gradients (Line 265) might oversmooth regions that should exhibit sharp depth changes (e.g., building-ground boundaries).\n\n**4. Inconsistent baseline comparison**\n\n* In Figure 4(c), the comparison is made against Sat2Density instead of Sat2Density++. Why not compare with the most recent and stronger baseline?\n* The paper lacks quantitative comparisons with related methods such as ControlNet, ControlS2S, or Canonical Image-to-3D.\n\n**5. Limited novelty**\n\nThe overall architecture is nearly identical to Sat2Density++, with improvements mainly stemming from new loss terms and a stronger backbone."}, "questions": {"value": "The problem corresponds one-to-one with the content of the weakness:\n1. How does the use of DINO-v3 affect inference speed and memory consumption? Are the improvements mainly due to the stronger feature extractor?\n2. Could the authors provide clearer mathematical formulations and theoretical justification for the Gravity-based Density Variation Loss?\n3. How accurate is Depth Anything v2 on satellite imagery, and how sensitive is the model to potential depth estimation errors? Why not integrate depth directly into the model rather than using it as a loss? Does the spatial gradient supervision risk over-smoothing sharp depth transitions?\n4. Why is Sat2Density used as the baseline in Figure 4(c) instead of Sat2Density++? Why are quantitative comparisons with ControlNet, ControlS2S, or Canonical Image-to-3D missing?\n5. How do the authors justify the novelty of Sat3DGen given its high similarity to Sat2Density++?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Non-existent"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dHG4s488fu", "forum": "E7JzkZCofa", "replyto": "E7JzkZCofa", "signatures": ["ICLR.cc/2026/Conference/Submission7052/Reviewer_tZHZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7052/Reviewer_tZHZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761046715442, "cdate": 1761046715442, "tmdate": 1762919246744, "mdate": 1762919246744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Sat3DGen, a novel framework for generating high-fidelity, street-level 3D reconstructions from a single satellite image. The method introduces three geometry-focused contributions: (1) Gravity-based density variation loss, (2) Spatial token padding, (3) Monocular satellite-view depth regularization. It demonstrates substantial improvements over prior state-of-the-art approaches like Sat2Density++ and Sat2Scene, particularly in scene-level 3D geometry consistency, semantic fidelity, and rendered view realism, across both qualitative and quantitative benchmarks (e.g., FID, DINO similarity)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly motivated and well-structured paper with substantial methodological contributions.\n- The paper achieves strong improvements in empirical results (e.g., FID drops from 40.8 to 19.2) and demonstrates practical utility in applications such as DSM estimation and multi-view video synthesis.\n- Effective ablation studies and transparent discussion of limitations are provided."}, "weaknesses": {"value": "- The paper lacks evaluation against metric 3D ground truth (e.g., DSM or city-scale LiDAR) and could be strengthened with controlled experiments on public datasets and analysis of DSM or mesh error.\n- There is insufficient discussion of major failure modes (such as occlusions or challenging geometry), and additional metrics beyond FID/LPIPS—like multi-view photometric consistency or temporal flicker—would offer a fuller assessment of 3D and video realism.\n- Robustness and generalization should be further explored, including evaluation on non-VIGOR data, handling of noisy or missing illumination inputs, and clarifications on methodological details and reproducibility (e.g., pretrained model release, citation updates, and clear diagram separation)."}, "questions": {"value": "1. Are the generated meshes watertight and suitable for downstream simulation tasks (such as physics simulation or driving simulation)?\n2. How robust is the model to varying lighting conditions or the absence of panorama-derived illumination codes, and does it offer controllable rendering for different times of day?\n3. Does the method generalize well to images outside the VIGOR dataset, including rural or non-urban areas?\n4. It would be nice to see more analysis on the failure cases and the robustness of the method (e.g., non-planar surfaces, complex geometry, occlusion, etc.)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S9cUpoWbjV", "forum": "E7JzkZCofa", "replyto": "E7JzkZCofa", "signatures": ["ICLR.cc/2026/Conference/Submission7052/Reviewer_Quvg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7052/Reviewer_Quvg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870131181, "cdate": 1761870131181, "tmdate": 1762919246307, "mdate": 1762919246307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for efficiently generating high-quality street-level 3D scenes from a single satellite image. Specifically, this method designs an end-to-end framework from satellite image to 3D. This framework first uses DINO v3 to encode the features of the input satellite image, and then decodes the encoded features into a Triplane feature field. Subsequently, volume rendering of this feature field is performed using an MLP to reconstruct the 3D scene. To effectively address potential issues such as edge artifacts, geometric distortions, and roof errors during scene reconstruction, this paper introduces various optimization strategies, including physical constraints and depth constraints. Experimental results show that, compared to existing methods such as Sat2Density++, Sat3DGen can generate street-level 3D scenes with more accurate geometric information and more detailed rendering results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-written: The paper has a clear organizational structure, is well-written, and has a logical flow.\n2. Targeted improvements to geometric stability: Gravity-based Density Variation Loss is proposed, which modulates the volume density along the direction of gravity, significantly alleviating the common \"floating layers/holes\" problem, making the reconstruction more coherent and more renderable.\n3. Simultaneous improvement in rendering quality and cross-view consistency: Combining depth prior with multi-view supervision of panorama/perspective, covering a wider field of view and strengthening geometric constraints, resulting in more stable reconstruction and higher rendering fidelity, especially more reliable in details such as boundaries and roofs."}, "weaknesses": {"value": "1. The contribution is unclear: Based on my understanding of this article, its basic framework is quite similar to Sat2Density++, with the core contribution being the introduction of depth conditions as training constraints. The authors need to better clarify the differences between this and the Sat2Density++ framework.\n\n2. The experimental validation is insufficient. The authors only conducted experiments on VIGOR-OOD. To my knowledge, VIGOR-OOD is mainly designed for urban scene acquisition. For scenes that are more suburban or rural (e.g.,CVACT[1]), will the authors' method still have significant robustness?\n\n3. Lack of evaluation baseline: As I understand it, based on the contribution of the paper, the focus is on optimizing the 3D geometry generated by the baseline method. Therefore, this paper should add a comparison with general Image-3D methods [2, 3].\n\n[1] Liu, Liu, and Hongdong Li. \"Lending orientation to neural networks for cross-view geo-localization.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n[2] Xiang, Jianfeng, et al. \"Structured 3d latents for scalable and versatile 3d generation.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n[3] Hunyuan3D, Team, et al. \"Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material.\" arXiv preprint arXiv:2506.15442 (2025)."}, "questions": {"value": "1. Regarding the comparison of Figure 4(c), currently only Sat2Density is compared, excluding Sat2Density++. Given that Sat2Density++ clearly demonstrates a quality improvement over Sat2Density in its original paper, could you explain why this baseline was not included? Furthermore, since this paper achieves higher quantitative metrics, it is recommended to supplement the comparison with Sat2Density++ through parallel rendering, providing qualitative results under the same settings, to more comprehensively evaluate the differences between the two in terms of geometric consistency and texture fidelity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OC7AOlusrN", "forum": "E7JzkZCofa", "replyto": "E7JzkZCofa", "signatures": ["ICLR.cc/2026/Conference/Submission7052/Reviewer_udS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7052/Reviewer_udS7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994398747, "cdate": 1761994398747, "tmdate": 1762919245894, "mdate": 1762919245894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Sat3DGen proposes a feed-forward framework to generate street-level 3D scenes from a single satellite image, addressing the trade-off between semantics and geometry in existing methods. Built on a tri-plane NeRF backbone, it introduces three key components: a gravity-based density variation loss to suppress floating artifacts and voids, spatial tokens to stabilize boundary geometry, and satellite-view depth regularization to resolve rooftop ambiguity. Additionally, it strengthens supervision by jointly training on panoramas and their projected perspective views. Experiments on VIGOR-OOD demonstrate superior performance and support downstream applications like DSM estimation and large-area mesh generation. The work’s core strength lies in targeted geometric optimizations, though it relies on combinations of existing techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear framework design**: The pipeline (satellite encoding → tri-plane lifting → illumination-adaptive rendering) is logically coherent, with sufficient details for reproducibility.\n- **effective geometric optimizations**: The gravity-based loss directly addresses volumetric field voids and floaters, which is a critical problem in scene-level NeRF-based generation, with clear qualitative and quantitative improvements.\n- **Strong practical value**: Supports multiple downstream tasks (e.g. DSM estimation, multi-camera video generation, semantic-map-to-3D) without extra supervision, enhancing real-world applicability.\n- **Comprehensive experimental validation**: Includes ablation studies for key components, cross-method comparisons, and qualitative/quantitative evaluations, ensuring result credibility."}, "weaknesses": {"value": "- **relying on existing method combinations**: The core framework (tri-plane NeRF + 2D supervision) is borrowed from prior works (e.g., Sat2Density++). No breakthrough in methodology or framework design is presented.\n- **Unclear motivation and lack of ablation for DINOv3 encoder**: The paper uses a frozen DINOv3 ViT encoder for satellite tokenization but provides no justification for choosing DINOv3 over other encoders. There is no ablation to verify whether DINOv3 contributes to performance gains, or if simpler encoders could achieve similar results, or if the model can generalize to out-of-distribution scenarios.\n- **Lack of quantitative 3D geometric evaluation**: All geometric assessments are qualitative (mesh visualizations), with no quantitative metrics for 3D quality. This makes it hard to rigorously validate the claimed \"superior geometric quality\" compared to baselines like Sat2Density++."}, "questions": {"value": "- What was the motivation for selecting DINOv3 as the satellite encoder? Have you conducted ablation studies comparing it with other encoders in terms of performance, computational cost, or token quality? If DINOv3 is replaced with a simpler encoder, how much performance degradation would occur? \n- Could you supplement quantitative 3D metrics to objectively validate geometric improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dYVCodlM9H", "forum": "E7JzkZCofa", "replyto": "E7JzkZCofa", "signatures": ["ICLR.cc/2026/Conference/Submission7052/Reviewer_zCTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7052/Reviewer_zCTK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073260850, "cdate": 1762073260850, "tmdate": 1762919245477, "mdate": 1762919245477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}