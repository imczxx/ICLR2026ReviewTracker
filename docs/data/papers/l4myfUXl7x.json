{"id": "l4myfUXl7x", "number": 10219, "cdate": 1758164310354, "mdate": 1762950182994, "content": {"title": "Certified Signed Graph Unlearning", "abstract": "Data protection has become increasingly stringent, and the reliance on personal behavioral data for model training introduces substantial privacy risks, rendering the ability to selectively remove information from models a fundamental requirement. This issue is particularly challenging in signed graphs, which incorporate both positive and negative edges to model privacy information, with applications in social networks, recommendation systems, and financial analysis. While graph unlearning seeks to remove the influence of specific data from Graph Neural Networks (GNNs), existing methods are designed for conventional GNNs and fail to capture the heterogeneous properties of signed graphs. Direct application to Signed Graph Neural Networks (SGNNs) leads to the neglect of negative edges, which undermines the semantics of signed structures. To address this gap, we introduce $\\textbf{Certified Signed Graph Unlearning}$ (CSGU), a method that provides provable privacy guarantees underlying SGNNs. CSGU consists of three stages: (1) efficiently identifying minimally affected neighborhoods through triangular structures, (2) quantifying node importance for optimal privacy budget allocation by leveraging the sociological theories of SGNNs, and (3) performing weighted parameter updates to enable certified modifications with minimal utility loss. Extensive experiments show that CSGU outperforms existing methods and achieves more reliable unlearning effects on SGNNs, which demonstrates the effectiveness of integrating privacy guarantees with signed graph semantic preservation. Codes and datasets are available at https://anonymous.4open.science/r/CSGU-94AF.", "tldr": "", "keywords": ["Signed Graph", "Certified Unlearning", "Privacy Protection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2b74b3a4496e716769a0c2992101abb5f3a4e1a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses certified unlearning in signed graph neural networks. The authors propose a spectral truncation framework that leverages the signed Laplacian decomposition to achieve approximate unlearning guarantees. By bounding the effect of deleted data through the top-k eigencomponents, they obtain explicit certification bounds on node representation drift. They further derive guarantees under linearized SGNNs and present experiments on several signed graph benchmarks showing improved trade-offs between unlearning fidelity and accuracy compared to retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposed the first certified unlearning method for SGNNs. Extending to signed graphs and defining suitable certificates under the signed Laplacian is technically nontrivial.\n\n- The authors proposed a novel upper bound on node influence of signed graphs and a weighting scheme that allocates the privacy budget based on the sociological importance of each edge. \n\n- Experiments are consistent across datasets, metrics, and removal settings. The ablations (e.g., the effect of truncation rank k) convincingly support the theoretical trends.\n\n- The paper is well-organized and easy to follow."}, "weaknesses": {"value": "- The unlearning certification in the theoretical analysis seems to be standard. It would be helpful to highlight the challenges of certified unlearning on signed graphs.\n\n- Some important baselines are missing [1].\n\n- The ablation of the privacy budget weighting seems to be missing in the ablation study. This appears to be one of the major contributions of the method, and it would be helpful to clarify this further.\n\n[1] Wu K, Shen J, Ning Y, et al. Certified edge unlearning for graph neural networks[C]//Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023: 2606-2617."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2Qrek6hlaJ", "forum": "l4myfUXl7x", "replyto": "l4myfUXl7x", "signatures": ["ICLR.cc/2026/Conference/Submission10219/Reviewer_C9s5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10219/Reviewer_C9s5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888692706, "cdate": 1761888692706, "tmdate": 1762921578706, "mdate": 1762921578706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "G3ULSoXq0i", "forum": "l4myfUXl7x", "replyto": "l4myfUXl7x", "signatures": ["ICLR.cc/2026/Conference/Submission10219/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10219/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762950180816, "cdate": 1762950180816, "tmdate": 1762950180816, "mdate": 1762950180816, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for certified machine unlearning in the context of signed graph neural networks (GNNs) where both positive and negative edges are used to model complex relations. The authors recognize that unlearning in GNNs is particularly challenging due to information entanglement among nodes via the message-passing mechanism, which propagates features across multiple hops.\n\nTo address this, the paper proposes a certified unlearning method that provides theoretical guarantees on the removal of specified data (nodes, edges, or features) from the trained model. Specifically, the method first constructs triadic influence neighborhoods to capture influence propagation via triangular structures, then ituse sociological theory to model weights of edges; at leaast, it introduces weighted certified unlearning.\n\nThe framework is evaluated on standard signed graph benchmarks (e.g., Bitcoin-Alpha, Wikipedia-RFA, Epinions) and demonstrates strong empirical performance in terms of unlearning effectiveness, model fidelity, and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1 Originality: The paper tackles machine unlearning in the signed graph domain, a niche yet practically important area where relationships can be both positive and negative.\n\nS2 Quality: The theoretical analysis is rigorous, with formal derivations of the certification bound and proofs ensuring robustness of the unlearning process.\n\nS3 Clarity: The paper is clearly structured, progressing logically from problem motivation to theoretical formulation, algorithm, and experiments. Figures (particularly Figure 2 on the mechanism of signed message passing and Figure 4 on efficiency comparison) are intuitive and helpful for understanding.\n\nS4 Significance: Extending certified unlearning to signed GNNs broadens applicability to trust networks, recommendation systems, and fraud detection."}, "weaknesses": {"value": "W1 While the method is well-motivated for signed GNNs, its broader applicability to general (unsigned) GNNs or heterogeneous graphs is not fully demonstrated. More importantly, it would be helpful to discuss techniques that normalize the signed weights on graphs. \n\nW2 The certification bound relies on the assumption that node influence diminishes exponentially with hop distance. While this is practical, it might not hold for densely connected or long-range dependency graphs.\n\nW3 Although results on signed datasets are comprehensive, the experiments could include ablation studies analyzing the effect of graph sparsity, sign ratio, or degree distribution on unlearning performance.\n\nW4 The theoretical runtime analysis is not fully fleshed out. While empirical results show reduced computation, the paper lacks an explicit asymptotic complexity comparison between full retraining and the proposed method."}, "questions": {"value": "Q1: The paper claims that partition-based methods failed maintain the distinction between positive and negative edges, but Graph Eraser achieves very competitive results in many datasets and setups. Are there any theoretical or empirical studies to show the importance of distinction between positive and negative edges in learning or unlearning processes?\n\nQ2: How does certified unlearning relate theoretically or practically to graph differential privacy, especially when both aim to limit the effect of specific data points?\n\nQ3: Besides bitcoin networks, are there any other applications in real-world that need separate representation of positive and negative edges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EoqHWLRGiU", "forum": "l4myfUXl7x", "replyto": "l4myfUXl7x", "signatures": ["ICLR.cc/2026/Conference/Submission10219/Reviewer_rcUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10219/Reviewer_rcUU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116422822, "cdate": 1762116422822, "tmdate": 1762921577118, "mdate": 1762921577118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a certified unlearning method for signed graphs, CSGU. Due to the heterogeneous nature of edges in signed graphs, the authors claim that existing unlearning methods are not suitable. The proposed CSGU first constructs triadic influence neighborhoods to compute the influence propagation of the unlearned edges, and then weights the edges by sociological influence quantification, and finally injects noise to achieve weighted certified unlearning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper addresses an important and underexplored problem—unlearning in signed graphs—which is both timely and meaningful.\n\nS2. The proposed components—Triadic Influence Neighborhood and Sociological Influence Quantification—are conceptually intuitive and well-motivated."}, "weaknesses": {"value": "W1. My primary concern is that CSGU does not truly achieve certified unlearning, as the introduction of edge weights fundamentally modifies the original loss function. Specifically, CSGU guarantees similarity between the unlearned model and the retrained model with respect to the weighted loss defined in Eq. (7), rather than the original unweighted loss.\n\nW2. The computation of triadic closure is computationally expensive and poses a significant scalability bottleneck for real-world graph applications. Moreover, this computation must be reperformed for every unlearning request, making the method impractical for large or dynamic graphs.\n\nW3. Although the paper claims that CSGU achieves certified unlearning, the experimental setup does not satisfy the theoretical conditions required for such certification—for example, the assumption of a strongly convex loss. In addition, CSGU should be compared against existing certified unlearning baselines, such as Certified Graph Unlearning [R1] and ScaleGUN [R2], to provide a fair and rigorous evaluation.\n\n[R1] Certified graph unlearning.\n\n[R2] Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier."}, "questions": {"value": "Q1.\nWhat exactly is the edge sign prediction task considered in this paper?\nA more detailed problem formulation would be helpful.\n\nQ2. In the definition of certified unlearning, why is the inequality written in only one direction, but not including the left-hand side $e^{\\epsilon}Pr(\\cdot)+\\delta\\le Pr(\\cdot)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "neSSMyuh58", "forum": "l4myfUXl7x", "replyto": "l4myfUXl7x", "signatures": ["ICLR.cc/2026/Conference/Submission10219/Reviewer_BLbE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10219/Reviewer_BLbE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141818597, "cdate": 1762141818597, "tmdate": 1762921576427, "mdate": 1762921576427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}