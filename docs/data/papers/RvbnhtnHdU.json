{"id": "RvbnhtnHdU", "number": 7706, "cdate": 1758032956389, "mdate": 1759897838070, "content": {"title": "TransAbs: Taming Absolute Interaction for Efficient Relative Motion Prediction", "abstract": "Accurate motion prediction in complex social scenarios requires capturing effective spatial interactions among traffic participants. Limited by the unsatisfactory accuracy of absolute interaction, recent methods transform the absolute scenario into multiple relative scenarios to obtain high-quality predictions. However, this relative prediction suffers from re-encoding relative motion information (spatial position and participants interaction), leading to computational inefficiency compared to absolute interaction. In this paper, we present TransAbs, which tames the absolute interaction to achieve relative effects, while maintaining the computational efficiency. The core idea of TransAbs is to map the Hadamard representation between a pair of absolute positions to their relative position. Incorporating the absolute positional embedding to the attention formulation (Hadamard product first and then sum up), TransAbs achieves relative positional embedding and spatial interaction simultaneously by leveraging a post-multiplication positional encoding. To align the Hadamard presentation and attention scores, TransAbs is optimized jointly with the motion predictor. We evaluate the effectiveness of TransAbs by integrating it into the transformer-based motion predictor commonly employed in motion prediction. Extensive experiments on a large scale public benchmark, Waymo Open Motion Dataset, demonstrate that TransAbs successfully balances prediction accuracy and computational efficiency with minimal overhead—achieving comparable accuracy while eliminating the redundant re-encoding introduced by relative interaction. The pretrained weights and code implementations will be released upon acceptance.", "tldr": "", "keywords": ["Absolute Position", "Relative Position", "Motion Prediction"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf96b4875f2bddb2f537ccb3b065d4921426812b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes TransAbs, a module that performs relative position generation into the self-attention process for efficient multi-agent motion prediction in autonomous driving. Instead of repeatedly re-encoding relative coordinates for each agent, TransAbs introduces a Hadamard-based Auto-Transformer that learns to convert absolute positional embeddings into relative ones through a self-supervised alignment mechanism. By injecting these tamed embeddings into the attention computation through post-multiplicative encoding, the model achieves relative spatial reasoning with lite computations. Integrated into transformer predictors such as MTR orMTR++, TransAbs delivers comparable or slightly improved accuracy on the Waymo Open Motion Dataset while reducing memory and latency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Reformulating relative transformation as an implicit Hadamard alignment between absolute position embeddings and attention computation is conceptually appealing and potentially generalizable;\n\n2. The insights to tackle relative interactions inside attention seems appealing."}, "weaknesses": {"value": "1. The proposed transformation is described heuristically, without formal analysis of when or why the Hadamard mapping preserves relative geometric consistency.\n\n2. Testing result of MTR+TransAbs on WOMD Leaderboard is missing. Recent efficient or unified-interaction frameworks such as Wayformer, ModeSeq, or QCNet are not quantitatively evaluated. \n\n3. It’s unclear how TransAbs scales to larger agent counts.\n\n4. More explanation on stability, convergence, and whether this loss is weighted or balanced against the main prediction loss would improve clarity\n\n5. Qualitative analysis is pretty limited. In depth analysis of the mechanism should also be added"}, "questions": {"value": "1. How does TransAbs behave in extremely dense urban scenes?does memory scale linearly or quadratically after pruning?\n\n2. How sensitive is performance to the number of sampled pairs in terms of self-supervised learning objective?\n\n3. Have you tested TransAbs on other benchmarks like Argoverse2 or nuScenes to confirm dataset generalization?\n\n4. How is the performance combining TransAbs on other predictors / planners such as QCNet or DeMo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PYIZUZqzIi", "forum": "RvbnhtnHdU", "replyto": "RvbnhtnHdU", "signatures": ["ICLR.cc/2026/Conference/Submission7706/Reviewer_hsDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7706/Reviewer_hsDV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724988796, "cdate": 1761724988796, "tmdate": 1762919763303, "mdate": 1762919763303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TransAbs, a transformer-based module designed to make absolute spatial interactions behave like relative ones without explicit re-encoding. The key idea is to incorporate relative positional effects directly into the attention mechanism through a Hadamard representation between pairs of absolute positions. By aligning this representation with attention scores through a self-supervised auto-transformer, the method aims to retain the efficiency of absolute interaction while capturing the precision of relative modeling.\nExperiments on the Waymo Open Motion Dataset (WOMD) show that TransAbs, when integrated into MTR++, achieves slightly higher accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper provides a clear motivation: relative interaction methods yield higher accuracy but are computationally expensive, and TransAbs attempts to combine the benefits of both paradigms.\n\n2.\tThe integration of a self-supervised auto-transformer into the attention mechanism is theoretically sound.\n\n3.\tThe presentation of the Hadamard-based formulation is clear, and the efficiency analysis is helpful for understanding potential runtime benefits."}, "weaknesses": {"value": "1.\tInsufficient experimental validation.\n\nThe evaluation relies almost entirely on MTR++ as the backbone, which is not the best-performing model on WOMD and is not open-sourced. Using only this backbone is restrictive and does not prove the generality of TransAbs. Integrating the method into stronger backbones or into a variety of architectures would be essential for validating the claim of broad applicability.\n\n2.\tLimited comparative scope.\n\nTables 1 and 2 compare TransAbs only with a few positional encodings (APE, RPE, RPB, RoPE). In Section 2.0.2, the paper discusses other relative-encoding frameworks such as HiVT, QCNet, and Simpl, but no experiments or quantitative results against these approaches are presented. Without these comparisons, it is unclear whether TransAbs offers a meaningful accuracy improvement over established relative-encoding models.\n\n3.\tWeak empirical gains.\n\nThe mAP improvements shown in Tables 1–2 are very small, often within the margin of training variance. The claim that TransAbs “balances prediction accuracy and computational efficiency” is not strongly supported by the reported numbers. A more comprehensive set of ablations and statistical confidence intervals would be required to establish significance.\n\n4.\tOverstated claims of generality.\nThe method is presented as a general way to replace relative interaction, but all results are restricted to one dataset and one baseline. There is no evidence that the technique would transfer to other prediction settings or benchmarks."}, "questions": {"value": "Why was MTR++ selected as the sole backbone? Did you attempt integration with stronger or more recent models?\n\nWould the proposed Hadamard-based positional encoding still be effective when applied to non-transformer motion predictors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cF4LsrDdBX", "forum": "RvbnhtnHdU", "replyto": "RvbnhtnHdU", "signatures": ["ICLR.cc/2026/Conference/Submission7706/Reviewer_9dxQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7706/Reviewer_9dxQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095909845, "cdate": 1762095909845, "tmdate": 1762919762896, "mdate": 1762919762896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TransAbs, a transformer-based motion prediction framework that tackles the efficiency-accuracy trade-off in spatial interaction modeling. The approach addresses two pain points: (1) an Auto-Transformer module that learns to map Hadamard representations of absolute position pairs to relative positions via self-supervised learning, avoiding the expensive coordinate re-encoding required by traditional relative methods; and (2) a post-multiplication positional encoding strategy that integrates the learned relative information directly into the attention mechanism, enabling simultaneous spatial interaction and relative position encoding in a single forward pass. The model is trained end-to-end with both self-supervised loss and trajectory prediction loss. Experiments on Waymo Open Motion Dataset show that when integrated into MTR and MTR++ baselines, TransAbs achieves comparable or marginally improved accuracy (~1-2% mAP gain) with reduced memory usage and inference latency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- TransAbs explicitly addresses the re-encoding overhead of relative interaction methods through its Auto-Transformer module, which learns to encode relative positions from Hadamard representations of absolute positions. The post-multiplication integration directly eliminates redundant coordinate transformations while maintaining relative positional effects—addressing a known computational bottleneck in motion prediction.\n- TransAbs is positioned as a single approach that (i) operates on absolute coordinates for efficiency, (ii) achieves relative positional modeling through learned representations, and (iii) integrates seamlessly into existing transformer architectures.\n- TransAbs employs a training-inference decoupling design. The transform decoder and Hadamard representation generation are used only during training for self-supervised learning, then discarded at inference."}, "weaknesses": {"value": "- Insufficient theoretical justification for the core claim. The paper asserts that the Hadamard product encodes relative position information (Eq. 9-10), but this is not rigorously justified. Element-wise multiplication followed by summation does not obviously capture the geometric transformations (translation + rotation) that define relative positions. The Auto-Transformer learns a mapping via self-supervised loss, but there's no analysis of (i) what representations it actually learns, (ii) why Hadamard products specifically enable this.\n- Marginal performance gains with trade-offs in other metrics. While TransAbs shows modest mAP improvements (+1.8% on MTR++: 0.4379 vs 0.4303), Table 1 reveals that other important metrics actually degrade or show mixed results. For MTR++, minADE increases from 0.6087 to 0.6120, minFDE increases from 1.2265 to 1.2366, indicating worse prediction accuracy in terms of displacement errors.\n- Questionable efficiency claims. Table 3 shows a paradox: TransAbs+MTR++ has more parameters yet uses less memory and has lower latency. This counterintuitive result is not explained. Without clarification, it's unclear whether the efficiency gains are genuine or artifacts of implementation choices.\n- Inconsistent baseline comparisons and reproducibility concerns. Table 3 shows TransAbs+MTR has fewer parameters (65.2M) than MTR alone (65.7M), which is counterintuitive given that TransAbs adds components. This suggests either an error or undisclosed architectural changes. Additionally, critical hyperparameters are missing: loss weights ($\\lambda_1$, $\\lambda_2$ in Eq. 13)​, learning rates, and training schedules are not specified, making reproduction difficult.\n- Limited scope of evaluation. The method is only tested on WOMD and only integrated into two baselines (MTR/MTR++). Testing on other datasets (Argoverse 2 Motion Forecasting Dataset) would demonstrate generalizability.\n- Presentation clarity issues. (i) Figure 3's \"Hadamard Representation\" visualization is hard to understand. Showing actual tensor shapes and data flow would help, (ii) Section 3.3.1's claim that \"relative position generation is similar with self-attention\" requires more rigorous comparison before introducing the Auto-Transformer;"}, "questions": {"value": "- Parameter inconsistency. Why does TransAbs+MTR (65.2M) have fewer parameters than MTR (65.7M) in Table 3? Are there architectural modifications not mentioned in the text? Please clarify what accounts for this difference.\n- Hyperparameter specifications. What are the values of $\\lambda_1$​, $\\lambda_2$ in Eq. 13? How is the relative weighting between $L_{ssl}$ and $L_{tp}$​ chosen? How sensitive is performance to these choices? Please report the final values used.\n- Generalization beyond WOMD. Can you test TransAbs on other motion prediction datasets (Argoverse 2 Motion Forecasting Dataset) or integrate it into other transformer-based architectures to demonstrate broader applicability?\n- Theoretical justification and learned representations. Provide (i) a mathematical proof or deeper analysis showing why the Hadamard product encodes relative position, (ii) visualizations of the decoded relative positions vs. ground truth,  and (c) analysis of what geometric or algebraic properties this representation preserves. Providing this analysis would strengthen the theoretical foundation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RAFV9A79KA", "forum": "RvbnhtnHdU", "replyto": "RvbnhtnHdU", "signatures": ["ICLR.cc/2026/Conference/Submission7706/Reviewer_3WY1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7706/Reviewer_3WY1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762268765626, "cdate": 1762268765626, "tmdate": 1762919762523, "mdate": 1762919762523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}