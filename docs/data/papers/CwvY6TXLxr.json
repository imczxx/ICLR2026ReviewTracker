{"id": "CwvY6TXLxr", "number": 21320, "cdate": 1758316212917, "mdate": 1763619376693, "content": {"title": "Draft, Verify, \\& Improve: Toward Training-Aware Speculative Decoding", "abstract": "Autoregressive (AR) decoding is a major latency bottleneck for large language models. Speculative decoding (SD) accelerates AR by letting a drafter propose multi-token blocks that a verifier accepts or rejects. However, many SD systems require heavy offline training or extra components. These choices raise data/compute cost and can yield brittle drafters under distribution drift.\nWe introduce \\emph{Draft, Verify, \\& Improve (DVI)}, a training-aware self-speculative framework that combines inference with continual online learning. We partition an LLM into a drafter and a verifier, and during generation, verifier accept/reject decisions are converted into supervision signals and used to update the drafter head. \nA simple \\emph{KL$\\rightarrow$RL} schedule bootstraps calibration via online distillation and then adds reward-masked cross-entropy with a on-policy policy-gradient term, preserving lossless, single model deployment.\nOn Spec-Bench, DVI achieves a $2.16\\times$ wall-time speedup, on par with SoTA approaches like EAGLE-2, while orders of magnitude less data for training, and ablations show that DVI outperforms KL-only online distillation. \nDVI demonstrates that \\emph{training-aware} self-speculation can deliver state-of-the-art, lossless speedups with minimal training overhead.", "tldr": "Train a self-speculation model on its own verifier feedback, matching SoTA performance with less data and training.", "keywords": ["Speculation", "Speculative Decoding", "acceleration", "llm", "large langauge model", "inference", "efficient training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b32b4ede5201156c0f47a4413be623581ce0e056.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an interesting idea of integrating online learning into speculative decoding. However, the manuscript suffers from three major weaknesses that significantly undermine its contributions: limited performance gains, failure to address the core inference bottleneck, and outdated comparisons that ignore recent advancements like EAGLE-3."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "​1. Novel Integration of Online Learning:​​ The core idea of closing the loop between speculative inference and online learning is innovative. Treating the verifier's commit decisions as real-time, self-supervised feedback for the drafter is a compelling approach to continual adaptation, potentially mitigating drafter brittleness under distribution drift without requiring separate offline datasets.\n\n2. High Data and Training Efficiency:​​ A significant advantage of DVI is its minimal data requirement. The paper demonstrates that effective speedups can be achieved after exposure to only 2,000 prompts, which is substantially less than the millions of prompts required by methods like Medusa or EAGLE. This makes DVI a highly cost-effective and practical option for scenarios with limited training data or the need for rapid deployment."}, "weaknesses": {"value": "1. Marginal Performance Improvements​\nThe claimed 2.16× average speedup appears modest when examined closely. As shown in Table 2, DVI's performance is actually ​inferior to EAGLE-2​ on several tasks (MT-Bench and Summarization), while the advantages in other tasks are minimal (e.g., only 0.07× faster in QA). Such marginal gains raise questions about the practical significance of the proposed method.\n\n2. Misplaced Focus: Training Efficiency ≠ Inference Speed\nThe paper heavily emphasizes reduced training cost(using only 2,000 prompts) as a key advantage. However, this addresses a secondary concern while overlooking the primary challenge in LLM deployment: ​maximizing inference speed and minimizing latency.\n\n3. Timeliness Issue: Missing Comparison with EAGLE-3\nThe most serious flaw is the omission of ​EAGLE-3​ (Li et al., 2024b), which represents the current state-of-the-art in speculative decoding."}, "questions": {"value": "1 ​Include EAGLE-3 Comparisons: Essential experiments comparing DVI with EAGLE-3 under identical settings must be conducted.\n2 Broaden Experimental Scope: Extend evaluations to larger models (e.g., 70B parameters) and different decoding strategies to demonstrate generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EAE1JKoxmK", "forum": "CwvY6TXLxr", "replyto": "CwvY6TXLxr", "signatures": ["ICLR.cc/2026/Conference/Submission21320/Reviewer_r38y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21320/Reviewer_r38y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761225345883, "cdate": 1761225345883, "tmdate": 1762941692613, "mdate": 1762941692613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-aware self-speculative decoding framework that partitions a single LLM into shallow drafting layers and deep verification layers, then converts verify accept/reject signals into online supervision for the drafter. \n\nA KL -> RL schedule warms up via online distillation to the frozen verifier and then adds reward-masked cross-entropy plus a light on-policy policy-gradient term, keeping speculation lossless under the verifier sampler. \n\nOn Spec-Bench with Vicuna-7B, DVI reports ~2.16X average wall-time speedup, competitive with EAGLE-2, while training on only 2k prompts and requiring no auxiliary drafter.\n\nOverall, I think this is a good piece of work that provides insights in how to build novel speculative decoding frameworks and is worthy of acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall design is simple and deployment-friendly. The entire DIV consists of one backbone, a LoRA drafter head, and a frozen verifier. \n\n2. The training-aware self-speculation turns commit decisions into online supervision, and is able to adapt the drafter to live traffic and mitigating distribution drift.\n\n3. DVI is both data and computation efficient through empirical experiments. It achieves competitive speedups with a tiny online budget (e.g. 2k prompts; single-GPU setup), compared to orders-of-magnitude larger offline training for baselines."}, "weaknesses": {"value": "Presentation needs to be improved. In fact that is the only factor that prevents this work from being published. For example, all equations are not numbered and reviewers are not able to refer to them. Some references are not in standard format, e.g. L74."}, "questions": {"value": "1. How sensitive are results to split index k and proposal depth k_spec?\n\n2. Can you report mean +- stdev over seeds for the main Spec-Bench tables? That should be the common practice for methods evaluated on Spec-Bench."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NH9ap4JGN0", "forum": "CwvY6TXLxr", "replyto": "CwvY6TXLxr", "signatures": ["ICLR.cc/2026/Conference/Submission21320/Reviewer_GD6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21320/Reviewer_GD6V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892142824, "cdate": 1761892142824, "tmdate": 1762941692088, "mdate": 1762941692088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduce Draft, Verify, & Improve (DVI), a training-aware self-speculative framework that combines inference with continual online learning to tackle the training overhead of speculative decoding (SD) methods. DVI incorporates a frozen verifier and an online-learned drafter head that converts commit decisions of SD into self-supervision, making the SD model data-efficient without separate offline datasets or long pre-training. Experimental results demonstrate competitive speedups compared to other SOTA SD methods with minimal training overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using a frozen verifier and an online-learned drafter head to save training overhead of SD models is interesting and promising. The presentation of the proposed method is clear and easy to follow.\n\n2. The experimental results demonstrate the proposed method achieves competitive speedups compared to other SOTA SD methods with minimal training overhead."}, "weaknesses": {"value": "1. The experiments is based on a small-scale, outdated LLMs Vicuna-7B. Further experiments on larger models (30B or 70B parameters) are expected to yield improved value of this work."}, "questions": {"value": "Although the speedup metrics are competitive compared to other SOTA SD methods, there are still gaps between the proposed methods and SOTA methods in the mean accepted tokens metrics. How to understand this gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3QhhRXVkG0", "forum": "CwvY6TXLxr", "replyto": "CwvY6TXLxr", "signatures": ["ICLR.cc/2026/Conference/Submission21320/Reviewer_STyx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21320/Reviewer_STyx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981981019, "cdate": 1761981981019, "tmdate": 1762941691834, "mdate": 1762941691834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The algorithm proposes Draft, Verify, & Improve (DVI) which is a self-speculative decoding method with online training. Specifically, it adopts first few layers as drafter and add lora head and use rest of the layers as verifier with verifier head. Then, it trains both lora heads in an online manner where supervision is given from the token acceptance reward. The result shows improved performance on Spec-Bench with smaller number of trainig data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The proposed algorithm introduces new types of online SD combining with self-speculative decoding.\n* The motivation of the paper is clear.\n* Proper ablations are conducted with sound presentations."}, "weaknesses": {"value": "* **Novelty** : Prposed method combines self-speculative decoding with online training, but utilizing first few layers as drafter is already investigated as in [1]. Also, even DVI differs Online speculative decoding [2] in utilizing only accepted tokens, and combine it as a reward-signal in training, the effect of adding reward-suprevision is not independently investigated which limits the contributions of the paper. \n\n* **Train time scaling** : While the proposed algorithm shows decent performance with only a samll amount of the train data, often one might need better drafter with more computes for training but no experiment is done.\n\n* **Tree decdoing and stronger baselines** : The baseline should contain stronger baselines like EAGLE-3 [3] for fair comparison. Moreover, the result on tree-decoding of the drafter ([4], [5]) should be tested which generally shows improved speed-ups while the experiments are done only with single trajectory decoding.\n\n* **Limited details** : Experiment details like warm-up steps or training hyper-parameters seems like being omitted."}, "questions": {"value": "* Can authors show the performance of the SD along the number of trained tokens (i suspect the training might saturate earlier than other methods)?\n\n* Can you test the trained model on tree-decoding scenario?\n\n* Can authors evaluate the trained models on OOD dataset? I think RL-type component might hinder generalizability.\n\n\n[1] (Liu et al.) Kangaroo: Lossless self-speculative decoding via double early exiting.\n\n[2] (Liu et al.) Online Speculative Decoding.\n\n[3] (Li et al.) EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test\n\n[4] (Cai et al.) MEDUSA: Simple LLM Inference Acceleration Framework with Multiple\nDecoding Heads\n\n[5] (Li et al.) EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FAUVFN6WMc", "forum": "CwvY6TXLxr", "replyto": "CwvY6TXLxr", "signatures": ["ICLR.cc/2026/Conference/Submission21320/Reviewer_s2jc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21320/Reviewer_s2jc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762324376729, "cdate": 1762324376729, "tmdate": 1762941691504, "mdate": 1762941691504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}