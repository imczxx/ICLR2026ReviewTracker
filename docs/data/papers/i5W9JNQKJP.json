{"id": "i5W9JNQKJP", "number": 20855, "cdate": 1758310992492, "mdate": 1759896955150, "content": {"title": "Memory as State Abstraction Over History", "abstract": "Reinforcement learning is provably difficult in non-Markovian environments, which motivates identifying useful environment classes. Previous work has identified classes such as regular decision processes and approximate information states. While these works address essential properties such as tractability, they do not answer how the classes relate, or when users should prefer one class over another. We resolve this by defining finer POMDP classes in terms of memory and state abstractions. Considering agent memory as a temporally-extended abstraction over the agent's observation-action history, we prove that POMDP classes can be defined using traditional state abstractions, such as model-preservation, optimal value $Q^{\\*}$ preservation, and optimal policy $\\pi^{\\*}$ preservation. In the process, we extend state abstraction to \"soft\" (stochastic) abstractions and show how this kind of abstraction relates to stochastic memory. Reinterpreting existing POMDP classes using our unified framework enables us to prove new relationships between existing classes and generalize these classes to approximate variants.", "tldr": "We view memory as temporally-extended state abstraction to organize and extend POMDP classes in the literature.", "keywords": ["reinforcement learning", "memory", "rl", "sequential decision making", "state abstraction", "abstraction", "hierarchy", "pomdp"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d7d31cd11b37a8bf08900b0d3428ab96d41fdaf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to deepen our understanding of history abstraction in POMDPs. It analyzes abstraction classes, contrasts deterministic versus stochastic memory, and examines the role of memory capacity. The paper contributes theoretical insights into how memory functions can serve as abstractions over history, connecting perspectives from RL, control, and automata theory."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors make commendable efforts to unify viewpoints from reinforcement learning, control theory, and automata theory. This interdisciplinary framing is valuable for understanding the conceptual foundations of history, abstraction, and memory.  \n- The derivations appear mathematically rigorous (though detailed checking would require reviewing the appendix). The formalization of memory functions and abstraction types is well structured.   \n- The paper is well motivated by the need to formalize the notion of memory in POMDPs, which is a critical and underexplored area."}, "weaknesses": {"value": "**Limited novelty and missing contextualization.** The discussion of related work is insufficient to situate the paper’s originality. \n\nFor example, the concept of approximate history abstraction in Table 2 (Optimal column) directly parallels the Approximate Information State (AIS) framework, though the terminology differs. The connection becomes clear in *“Bridging State and History Representations: Understanding Self-Predictive RL”* (Ni et al., 2024), which extends exact abstraction notions ($\\pi^*$-, $Q^*$-, and model-preserving) to POMDPs and unifies abstraction from a history-based perspective. Since that work, not discussed in this paper, already treats trajectory MDPs as a unifying view of POMDP abstraction, the novelty claimed here seems overstated. \n\nIn Section 6: it is ambiguous whether Tables 3 and 4 summarize prior work or present new theoretical results.  \n   - In *Ni et al., 2023*, Theorem 1 establishes the relationship between memory lengths and that paper also covers $k$-order MDPs.  \n   - In AIS and *Ni et al., 2024*, abstraction types in POMDPs are systematically derived.  \n\nCould you clarify whether your tables reproduce, extend, or reinterpret these results, and explain what the directional arrows denote?  \n\nMissing discussion of other relevant POMDP theory.  The paper overlooks works like *“When Is Partially Observable Reinforcement Learning Not Scary?”* (2022), which also classifies POMDP learnability and provides complementary theoretical framing. Including this would strengthen the contextualization and highlight the distinct contribution of this work.\n\n**Clarity and accessibility issues.**  Figures such as Fig. 3 and Fig. 11 are difficult to interpret on first read. Simplifying the visuals or providing a more guided explanation could make the insights clearer to readers less familiar with the formalism.\n\n**Limited significance for the ICLR threshold.**  While the theory is developed, the paper’s contribution is purely conceptual. To reach the ICLR standard, it would help to demonstrate empirical implications—e.g., experiments showing how the proposed theory informs representation design, memory capacity choice, or policy learning efficiency."}, "questions": {"value": "- Section 5.1’s novelty seems to lie in extending the deterministic memory function from AIS to stochastic, correct?  \n- What exactly is “memory capacity”? Can it be formally defined?  \n- Could you elaborate on why there is no 2-state memory sufficient to distinguish strings `1000` and `0010` (for instance, by storing the first character)?  \n- Intuitively, the optimal memory can be deterministic if memory capacity is unbounded. Is the core message that stochasticity becomes essential under *finite* memory capacity constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hVVx4vnyP0", "forum": "i5W9JNQKJP", "replyto": "i5W9JNQKJP", "signatures": ["ICLR.cc/2026/Conference/Submission20855/Reviewer_Y5MW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20855/Reviewer_Y5MW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756217376, "cdate": 1761756217376, "tmdate": 1762999986194, "mdate": 1762999986194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework of POMDP subclasses in terms of memory functions and state abstractions. First, the authors show how memory functions induce state abstractions over trajectory and effective MDPs and recontextualize traditional state abstraction relationships in this framework. Then, the authors present additional results in their framework for stochastic and deterministic memory functions. Finally, their work is shown to further unify and expand upon the existing literature."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides a novel perspective and understanding on existing concepts in the POMDP litterature that is grounded in mathematical rigor.\n- The paper gives novel insights around the stochastic-deterministic axes of memory functions, giving grounded examples and theorems around when a deterministic memory can be as powerful as stochastic ones, and how stochastic memories can be more expressive than deterministic ones\n- Their framework helps unify and synthesize the existing literature in POMDPs, providing a comprehensive overview of different POMDP subclasses and how they relate to each other."}, "weaknesses": {"value": "- The paper can be quite dense in mathematical concepts, making it difficult to follow and read, especially if the reader is not familiar with all concepts. It can be beneficial to provide more examples that ground certain concepts for a more intuitive understanding.\n- While the results presented throughout the paper are interesting, it is difficult to follow the motivation and general story of each result, which seem to exist more or less in a vacuum.\n- Following the previous two points, there is a lack of experimental evidence. Experiments are not only useful to empirically validate the results of a paper, but they help ground theoretical results into something that is simpler to visualize and understand for the reader, as well as provide concrete evidence as to how theoretical results can be beneficial and impact practitioners. As is, the authors provide no clear path forward for practitioners to build upon their work. \n- Some of the results, for instance Table 2, appear to mostly be a re-derivation of existing results. While this can be important, especially if new results are elucidated from this re-derivation, it is not clear how these re-derivations provide additional insights beyond their original works."}, "questions": {"value": "- The purpose of the results in Table 2 are not made clear. In line 245, the motivation seems to be to create an analogous table to Table 1, yet the columns Optimal and Improving in Table 2 are not 1-to-1 analogies to the exact state abstractions and approximate state abstractions in Table 1. The definitions here seem unnecessarily complex as well when discussing $\\epsilon$-optimal $\\epsilon$-improving, or $\\epsilon$-nonimproving, since the results in either column are mostly the complementary side of the other column. Why is it necessary for the results in the other section to define the optimality conditions with respect to both the trajectory MDP and the effective MDP? \n- The results on the stochasticity of a memory function in section 5.2 are interesting, and could benefit from empirical results, even if just in a toy environment. Does that mean that a stochastic memory function can learn to solve the environment in Figure 5 given a smaller capacity than a deterministic one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0Q4YHhWhtX", "forum": "i5W9JNQKJP", "replyto": "i5W9JNQKJP", "signatures": ["ICLR.cc/2026/Conference/Submission20855/Reviewer_ychT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20855/Reviewer_ychT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977721164, "cdate": 1761977721164, "tmdate": 1762999986092, "mdate": 1762999986092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Missing reviews"}, "comment": {"value": "Dear Area Chair,\n\nOur paper has not received any reviews as of time of writing this message. We just wanted to flag this so that it's on your radar. We are looking forward to the discussion."}}, "id": "vXuaE1Upfv", "forum": "i5W9JNQKJP", "replyto": "i5W9JNQKJP", "signatures": ["ICLR.cc/2026/Conference/Submission20855/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20855/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20855/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762969149516, "cdate": 1762969149516, "tmdate": 1762969149516, "mdate": 1762969149516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel view of memory in POMDPs as state abstraction in trajectory space – namely, a memory function based on a finite state machine is used to define state abstractions in memory space. The advantage of doing this is that you can then show that the typical model/Q*/pi* abstraction hierarchy holds on the memory space. The authors partition the space of memory functions into those which are optimal and improving, which describes whether the memory function represents the POMDP perfectly (in the trajectory MDP) or simply better than having no memory (relative to the effective MDP). The framework further distinguishes memory functions by their stochasticity (deterministic vs stochastic) and size (number of memory states), creating a taxonomy of POMDP subclasses. The main theorems establish relationships between these classes, and unify existing POMDP subclasses from the literature through the state abstraction lens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel unifying framework which is conceptually elegant and allows us to view prior work through a single conceptual lens.\n- The writing and mathematical exposition is mostly clear throughout, and the figures and diagrams are illustrative.\n- The related work section is extensive, thorough, and helps contextualize the paper's objective.\n- I believe this paper has potential to have a significant impact on both POMDP theory and the theory of abstraction, although my understanding may be a bit outdated here."}, "weaknesses": {"value": "- Definition 4.2 lacks detail, particularly around the effective MDP construction and the role of policies (see Questions).\n- The paper has no experimental results. Do the authors think it would be possible to conduct an experiment which takes some (more) realistic POMDP, e.g. a simulated environment or game, and measures what \"class\" it falls into? I'm not sure this is completely well-formed, but I'd be curious to understand if there's a result where we would control some salient aspect of the environment (transition structure? number of states?) and measure how the state dimensionality of the optimal memory function varies. Understanding the learnability of such a memory function could be interesting too, although it is likely beyond the scope of this work.\n- As the authors point out in the limitations, only the \\epsilon = 0 case is studied. It would indeed be interesting to understand the propagation of errors in the hierarchy for the \\epsilon > 0 case."}, "questions": {"value": "- On line 223, the authors state that $\\mathbb{P}(s | \\omega)$ is policy-dependent. This is confusing to me. What policy is being considered here, and why is it not written as part of the definition of the effective MDP? How does the choice of policy affect the downstream results?\n- \"We use FSM-based memory in this work as it produces a good model of systems like RNNs.\" What would happen if we just defined the memory function as a general mapping from full trajectories to abstract states? My impression is that the structure of the FSM is convenient for the analysis, but practical implementations might use transformers (as in Ni et al.), which have a different structure.\n- Line 238: \"improving memory functions that improve over having no memory at all, and optimal memory functions improve.\" – I think the second half of the sentence is missing.\n- I was a little confused on first read about the term \"target\" which is used throughout. I now take this to be a placeholder for model/Q*/pi*, but this could probably be stated a bit more explicitly.\n- I recognize that this paper is foundational theory work, but given the broader current interest in memory for systems such as LLMs or deep RL agents, the authors could perhaps discuss the relation between their insights and contemporary empirical problems in the field. What can we take away about the design of effective agents with memory? Where might we expect real-world POMDPs to fall within this taxonomy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gWQ7SOoGWc", "forum": "i5W9JNQKJP", "replyto": "i5W9JNQKJP", "signatures": ["ICLR.cc/2026/Conference/Submission20855/Reviewer_NyUx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20855/Reviewer_NyUx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978691336, "cdate": 1761978691336, "tmdate": 1762999986223, "mdate": 1762999986223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified theory that treats an agent’s memory as a state abstraction over history, formalizing how finite-state (deterministic or stochastic) memory functions induce abstractions on a trajectory MDP and how a memoryless effective MDP serves as the baseline for improvement. This lets the authors define POMDP subclasses by whether a memory exists that preserves or improves classic abstraction targets, including model, Q*, and π*. The framework yields new inclusion relationships among classes (e.g., model → Q* → π* preservation under memory-induced abstractions) and sharp results on randomness vs. capacity: a finite stochastic memory can be ε-approximated by a larger deterministic memory with an explicit size bound, while for fixed capacity stochastic memory can strictly outperform deterministic (constructive example). Finally, the taxonomy systematizes and connects many prior POMDP notions, e.g., regular decision processes, -order Markov structure, finite-memory policies, and approximate information states, within one lens centered on memory-induced abstractions, clarifying when each subclass applies and how they inter-relate."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper provides a clear classification scheme for POMDPs based on memory functions, which elegantly connects many disjoint concepts in the literature. By using memory as a temporally extended state abstraction, it becomes possible to compare previously unrelated subclasses in a single framework.\n\n2.The paper is thorough in formally defining each concept and proving relationships. All major claims are supported either by rigorous proofs or by well-chosen counterexamples.\n\n3.The work’s perspective on treating memory as a first-class abstraction is novel in that it merges two traditionally separate lines of research: state abstraction in fully observable MDPs and memory design in POMDPs.\n\n4.The authors systematically consider all combinations of key factors (target type, optimal vs improving, deterministic vs stochastic, memory size of 2 vs finite k vs unbounded), covering a wide range of corner cases."}, "weaknesses": {"value": "1.the effective MDP (Definition 4.2) is defined by marginalizing states to get a model P̂(ω'|ω, a), but because P(s| ω) depends on the agent’s policy, this effective MDP is not a fixed property of the environment but rather a derived construct given a policy (or policy class). The authors intend it as the baseline of a memoryless agent’s best achievable model, but a reader might be confused about how P(s| ω) is obtained and whether the effective MDP is defined w.r.t. an optimal memoryless policy or an arbitrary one.\n\n2.This is primarily a theoretical paper which is appropriate for the contributions, but one weakness is the absence of any empirical illustration or case study demonstrating the usefulness of the classification. For instance, it would have been insightful to take a known POMDP benchmark (maybe a simple partially observable gridworld or a Tiger problem variant) and analyze which memory class it falls into, or how an algorithm might exploit that knowledge.\n\n3.Following the above, a brief discussion on how one might test or infer the memory needs of an environment (perhaps via training performance with increasing memory sizes, or by recognizing structural clues) would have added practical relevance.\n\n4.Minor grammatical and spelling issues:\n(1)Line 076: the case that an agents’ (agent’s) environment is fully … \n(2)Line 137: Those authors allowing (allow) the k parameter…\n(3)Line 160: the controllers rather than analyzing their expressability (expressibility)…\n(4)Line 292: is fed in to (into) a policy function…\n(5)Line 239: and optimal memory functions improve. (improve what?)\n(6)Line 323: determining if results monotically (monotonically) improve…\n(7)Line 1084: for a confidence of at east (least)…\n(8)Line 1085: nature of the agents (agent’s) memory…\n(9)Line 1286: gives an (a) Q∗ error of…\n(10)Line 1086: all agents with (will) receive positive/negation…"}, "questions": {"value": "1.The framework introduces many subclasses (e.g. stochastic 2-memory model-optimal, deterministic k-memory Q*-improving, etc.). In a real application, determining which category a given problem falls into might be challenging. Do the authors have thoughts on practical tests or diagnostic methods to infer an environment’s memory class? For instance, is there a way to detect that a POMDP has a finite model-preserving memory without actually discovering the memory function?\n\n2.Could the authors provide more intuition or a simple example of a soft abstraction? For instance, is there a scenario in a small POMDP where the optimal abstraction naturally must be stochastic (perhaps due to symmetric states that need random tie-breaking)? Understanding this would help readers appreciate why a stochastic memory function can do better than a deterministic one.\n\n3.In Section 5.3, the authors demonstrate that “improving” does not imply “optimal” by constructing a POMDP where a small memory yields some reward but a larger memory is needed for the maximum reward. The converse question is also interesting: does having an optimal memory imply the existence of a smaller improving memory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pH7BGVlvQf", "forum": "i5W9JNQKJP", "replyto": "i5W9JNQKJP", "signatures": ["ICLR.cc/2026/Conference/Submission20855/Reviewer_V1cN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20855/Reviewer_V1cN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002306481, "cdate": 1762002306481, "tmdate": 1762999986163, "mdate": 1762999986163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}