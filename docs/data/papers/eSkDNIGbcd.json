{"id": "eSkDNIGbcd", "number": 18935, "cdate": 1758292124732, "mdate": 1763757346006, "content": {"title": "Hierarchical Value-Decomposed Offline Reinforcement Learning for Whole-Body Control", "abstract": "Scaling imitation learning to high-DoF whole-body robots is fundamentally limited by the \\textbf{curse of dimensionality} and the prohibitive cost of collecting expert demonstrations. We argue that the core bottleneck is paradigmatic: real-world supervision for whole-body control is inherently imperfect, yet most methods assume expert data. To overcome this, we propose \\textbf{HVD} (Hierarchical Value-Decomposed Offline Reinforcement Learning), an offline reinforcement learning framework that learns effective policies directly from suboptimal, reward-labeled trajectories. HVD structures the value function along the robot’s kinematic hierarchy and over temporal chunks, enabling precise credit assignment in long-horizon, high-dimensional tasks. Built on a Transformer-based architecture, HVD supports \\textit{multi-modal} and \\textit{multi-task} learning, allowing flexible integration of diverse sensory inputs for policy learning. To enable realistic evaluation and training, we further introduce \\textbf{WB-50}, a 50-hour dataset of teleoperated and policy rollout trajectories annotated with rewards and preserving natural imperfections — including partial successes, corrections, and failures. Experiments show HVD significantly outperforms existing baselines in success rate across complex whole-body tasks. Our results suggest that effective policy learning for high-DoF systems can emerge not from perfect demonstrations, but from structured learning over realistic, imperfect data.", "tldr": "", "keywords": ["robot policy learning", "offline reinforcement learning", "whole-body control"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a569d950220681519dd4ecc3648eac15aef4076a.pdf", "supplementary_material": "/attachment/bf80232a756214185477d67afc3f7044c4b4a059.zip"}, "replies": [{"content": {"summary": {"value": "This work presents HVD (Hierarchical Value-Decomposed Offline RL), a framework that learns high-DoF whole-body robot control from imperfect, reward-labeled data. By decomposing value functions along the robot’s kinematic hierarchy and temporal structure, HVD enables precise credit assignment in long-horizon tasks. Built on a Transformer architecture, it supports multi-modal, multi-task learning. The authors also release WB-50, a 50-hour teleoperation and policy rollout dataset capturing natural imperfections. Experiments show HVD significantly outperforms baselines, demonstrating that effective whole-body control can emerge from structured learning on imperfect data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an interesting and challenging problem of humanoid whole-body control in high-dimensional settings. It proposes a hierarchical decomposition of the Q-function to effectively handle the high degrees of freedom.\n\n2. The empirical validation includes real-world experiments across multiple tasks, demonstrating the effectiveness of the proposed HVD approach. In addition to the method, the paper also introduces a new dataset used for training.\n\n3. The experimental details are clearly presented and appear to be sufficient for reproducibility."}, "weaknesses": {"value": "1. **Figure clarity**: The pipeline figure (Figure 2) lacks captions, making it difficult to follow and interpret. Clear labeling of modules would improve readability.\n\n2. **Theoretical analysis**: The analysis in Proposition 3.1 is limited to the tabular case and cannot be directly applied to humanoid whole-body control, which involves continuous, partially observed states (e.g., image inputs). The justification should be revised, possibly using covering number arguments for general function approximation. The overall intuition—that higher observation dimensionality increases the need for expert data—is valid. However, the proposed hierarchical method decomposes the action space, not the state space, which may not align with the theoretical bound based solely on state-space cardinality.\n\n3. **Experimental fairness**: The experiments compare the proposed offline RL method (which uses reward information) with pure imitation learning, which does not. This comparison is potentially unfair. The authors should include more offline RL baselines based on generative policies [2, 3] that also utilize rewards for a fair evaluation.\n\n4. **Related work**: Several relevant studies on humanoid whole-body control that also exploit hierarchical structures (e.g., [1]) are missing from the discussion and should be cited for completeness.\n\n\n[1] Hansen, N., Jyothir, S. V., Sobal, V., LeCun, Y., Wang, X., & Su, H. Hierarchical World Models as Visual Whole-Body Humanoid Controllers. In The Thirteenth International Conference on Learning Representations.\n[2] Lu, C., Chen, H., Chen, J., Su, H., Li, C., & Zhu, J. (2023, July). Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. In International Conference on Machine Learning (pp. 22825-22855). PMLR.\n[3] Zhang, S., Zhang, W., & Gu, Q. Energy-Weighted Flow Matching for Offline Reinforcement Learning. In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. Could you revise Figure 2 and refine the theoretical analysis section to strengthen the justifications and improve clarity?\n\n2. Could you include additional comparisons with offline RL methods that also utilize reward signals for a fairer evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mM87Yf2v1K", "forum": "eSkDNIGbcd", "replyto": "eSkDNIGbcd", "signatures": ["ICLR.cc/2026/Conference/Submission18935/Reviewer_xcm5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18935/Reviewer_xcm5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760563026388, "cdate": 1760563026388, "tmdate": 1762930992792, "mdate": 1762930992792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents HVD (Hierarchical Value-Decomposed Offline RL), a framework that learns high-DoF whole-body robot control from imperfect, reward-labeled data. By decomposing value functions along the robot’s kinematic hierarchy and temporal structure, HVD enables precise credit assignment in long-horizon tasks. Built on a Transformer architecture, it supports multi-modal, multi-task learning. The authors also release WB-50, a 50-hour teleoperation and policy rollout dataset capturing natural imperfections. Experiments show HVD significantly outperforms baselines, demonstrating that effective whole-body control can emerge from structured learning on imperfect data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an interesting and challenging problem of humanoid whole-body control in high-dimensional settings. It proposes a hierarchical decomposition of the Q-function to effectively handle the high degrees of freedom.\n\n2. The empirical validation includes real-world experiments across multiple tasks, demonstrating the effectiveness of the proposed HVD approach. In addition to the method, the paper also introduces a new dataset used for training.\n\n3. The experimental details are clearly presented and appear to be sufficient for reproducibility."}, "weaknesses": {"value": "1. **Figure clarity**: The pipeline figure (Figure 2) lacks captions, making it difficult to follow and interpret. Clear labeling of modules would improve readability.\n\n2. **Theoretical analysis**: The analysis in Proposition 3.1 is limited to the tabular case and cannot be directly applied to humanoid whole-body control, which involves continuous, partially observed states (e.g., image inputs). The justification should be revised, possibly using covering number arguments for general function approximation. The overall intuition—that higher observation dimensionality increases the need for expert data—is valid. However, the proposed hierarchical method decomposes the action space, not the state space, which may not align with the theoretical bound based solely on state-space cardinality.\n\n3. **Experimental fairness**: The experiments compare the proposed offline RL method (which uses reward information) with pure imitation learning, which does not. This comparison is potentially unfair. The authors should include more offline RL baselines based on generative policies [2, 3] that also utilize rewards for a fair evaluation.\n\n4. **Related work**: Several relevant studies on humanoid whole-body control that also exploit hierarchical structures (e.g., [1]) are missing from the discussion and should be cited for completeness.\n\n\n[1] Hansen, N., Jyothir, S. V., Sobal, V., LeCun, Y., Wang, X., & Su, H. Hierarchical World Models as Visual Whole-Body Humanoid Controllers. In The Thirteenth International Conference on Learning Representations.\n[2] Lu, C., Chen, H., Chen, J., Su, H., Li, C., & Zhu, J. (2023, July). Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. In International Conference on Machine Learning (pp. 22825-22855). PMLR.\n[3] Zhang, S., Zhang, W., & Gu, Q. Energy-Weighted Flow Matching for Offline Reinforcement Learning. In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. Could you revise Figure 2 and refine the theoretical analysis section to strengthen the justifications and improve clarity?\n\n2. Could you include additional comparisons with offline RL methods that also utilize reward signals for a fairer evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mM87Yf2v1K", "forum": "eSkDNIGbcd", "replyto": "eSkDNIGbcd", "signatures": ["ICLR.cc/2026/Conference/Submission18935/Reviewer_xcm5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18935/Reviewer_xcm5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760563026388, "cdate": 1760563026388, "tmdate": 1763760500416, "mdate": 1763760500416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper indicates that imitation-learning approaches suffer from state-action-space explosion when applied to whole-body control, as a consequence it's unrealistic to collect the exponentially-growing expert data.\nOn the other hand, suboptimal trajectories are more readily available, motivating the use of offline reinforcement learning.\nThe paper proposes to use hierarchical value deomposition based on the embodiment (body, torso, arm) to improve credit assignment.\nTo demonstrate the improvement, the paper provides a curated dataset consists of optimal and suboptimal demonstrations, and policy rollouts, for multiple tasks such as pen insert, cup upright, wipe board, basket carry, and trash dispose.\nOn this dataet, the paper shows that hierarchical-value deomposition outperforms the standard imitation-learning approach on three vision-language-action models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clear and easy-to-follow.\n- The experiments are conducted on a real-world robot that requires substantial efforts."}, "weaknesses": {"value": "- Motivation\n\t- In the abstract, the paper states that scaling imitation learning (IL) to high DoF is limited by non-stationary observation transition. My concern with this statement is that if this is indeed true, the paper should not be focusing on formulating whole-body control as a Markov decision process (MDP), since MDPs do assume stationary transitions.\n\t- The paper indicates, on lines 38-42, that scaling IL approaches is fundamentally limited by the number of state-actions exploding exponentially. I agree with this statement, but I also agree that offline reinforcement learning (RL) approaches also suffer from this problem. In fact, one can argue that offline RL scales worse without data coverage and appropriate realizability assumptions [1].\n- Method\n\t- Eq. 1: Why this choice of ordering the base, torso, and arm? Is there any ablation to demonstrate that this is the \"best\" way to decompose the Q-function?\n\t- Eq. 2: It seems like for the later Q-functions, we can simply regress based on, e.g.,  $Q_{base} - Q_{torso}$. That is, why not $Q(s, a) - Q(s, a, a')$? From one perspective we can view $Q_{base}$ as the baseline for $Q_{torso}$ and so on.\n- Experiments\n\t- While I appreicate the challenges in producing results on a real robot, I strongly believe reproducibility is important---as a result, it would be great if the paper can provide even a single simulated environment, perhaps MuJoCo humanoid, that this approach can indeed improve upon existing methods.\n\t\t- I am unsure where the dataset, WB-50, will be provided---that will be greatly appreciated for reproducibility purposes as well.\n\t- What is the number of rollouts for evaluation?\n\t- How is \"better credit assignment\" analyzed? I was hoping for analysis on Q-values being more accurately propagated with the decomposition.\n\t- While the theory suggests the exponential explosion due to increasing DoF, it would be nice if empirically the paper demonstrates this problem with ablation on dataset size vs performance on increasing number of DoF.\n\nReferences:  \n[1] Zhan, Wenhao, et al. \"Offline reinforcement learning with realizability and single-policy concentrability.\" Conference on Learning Theory. PMLR, 2022."}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kEwwPODfeu", "forum": "eSkDNIGbcd", "replyto": "eSkDNIGbcd", "signatures": ["ICLR.cc/2026/Conference/Submission18935/Reviewer_ruzg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18935/Reviewer_ruzg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566754735, "cdate": 1761566754735, "tmdate": 1762930926809, "mdate": 1762930926809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to apply offline RL methods to whole-body robotic control. In general, whole-body control is more complicated than only manipulation control, due to larger degrees of freedom, increased variation from adjusting robot position, etc. These increase the complexity of the RL problem.\n\nTo address this, the authors propose using a hierarchical offline RL approach. Instead of treating the action $a$ as a monolithic action, it is segmented into three components, $a_{base}, a_{torso}, a_{arm}$. A Q-function is trained with each conditioning on all prior modalities, giving 3 Q-functions $Q(s, a_{base})$, $Q(s, a_{base}, a_{torso})$, and $Q(s, a_{base}, a_{torso}, a_{arm})$. Each is fit with its own TD loss term  but shares base architecture. For learning the Q-function and extracting a policy at the end, the authors use IDQL from prior work (fit Q-function with TD-learning then apply diffusion AWR to extract a policy). Each action component $a_{base}, a_{torso}, a_{arm}$ is extracted using advantage weights computed from the corresponding Q-function.\n\nTo evaluate this, the authors collect a 50 hour dataset of whole body control on the Galaxea platform, made of a mix of expert demonstrations, suboptimal demonstrations from operators that are worse at teleoperation, and policy rollouts from learned policies. The authors show their offline RL outperforms pure IL, and that the multi-level Q-function performs better than using no hierarchy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important problem of learning from suboptimal offline data. Collecting and labeling a 50 hour dataset is potentially quite useful. The paper acknowledges some of the weaknesses of the approach as well (specifically the need to do this labeling). Ablations suggest there are gains from doing this more hierarchical setup, and visualization of advantage weights over the subtask are compelling."}, "weaknesses": {"value": "The theoretical analysis seems entirely unnecessary to me. The cited Rajaraman et al 2020 bound is based on a worst case assumption of environment dynamics and generalization, and given that empirical dataset sizes are significantly smaller than this bound, it doesn't really seem to apply and just seems like an overly complicated way to state that higher dimensionality problems may need more data to fit.\n\nThe specific decomposition into base, torso, and arm is pretty specific to whole-body control. It seems like a special case of autoregressively fitting + extracting a Q-function 1 action dimension at a time, similar to Q-Transformer (although Q-transformer does not do any AWR weighting / diffusion step).\n\nIt was unclear to me how many trials were used to generate the task success rate tables of Table 1 / Table 2, or how much the hierarchical setup affects action inference time."}, "questions": {"value": "Are expert demonstrations vs suboptimal rollouts evenly split among tasks, or are some tasks more biased towards one or the other?\n\nHow is inference speed affected by dividing the action space this way?\n\nWhy is so much of the appendix spent defining quantitative measures of task complexity that don't seem to be used anywhere else?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QgzJ2AxUaf", "forum": "eSkDNIGbcd", "replyto": "eSkDNIGbcd", "signatures": ["ICLR.cc/2026/Conference/Submission18935/Reviewer_87vt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18935/Reviewer_87vt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699838103, "cdate": 1761699838103, "tmdate": 1762930925283, "mdate": 1762930925283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Hierarchical Value-Decomposed (HVD) Offline Reinforcement Learning, a framework designed for high-dimensional, whole-body control in robotic systems. The authors motivate their approach through empirical and theoretical challenges in whole-body control. The authors then introduce the value decomposition framework, utilized within a transformer architecture, and achieve more accurate credit assignment across long-horizon behaviors. The authors present numerous results that show HVD significantly improves imitation performance, credit assignment, and can scale to multi-task settings."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ Paper presents ample contributions, including a novel offline RL method for whole-body control, a dataset of whole-body behavior, and an implementation that support multi-modal and multi-task learning.\n+ Paper is very well-written."}, "weaknesses": {"value": "- The authors note that HVD relies on human-annotated rewards. Could you provide further information on where these come from and whether these are noisy reward signals? Further, does the framework depend on a specific distribution of data? For example, if 80% were imperfect demonstrations, would the framework still be able to learn behaviors well?"}, "questions": {"value": "Please address the weakness noted above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BXrRIes7fS", "forum": "eSkDNIGbcd", "replyto": "eSkDNIGbcd", "signatures": ["ICLR.cc/2026/Conference/Submission18935/Reviewer_qv84"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18935/Reviewer_qv84"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971968661, "cdate": 1761971968661, "tmdate": 1762930924647, "mdate": 1762930924647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}