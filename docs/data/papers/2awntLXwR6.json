{"id": "2awntLXwR6", "number": 7891, "cdate": 1758041291989, "mdate": 1759897824008, "content": {"title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy with dynamic entropy regulation, progressively teaching the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs. Notably, it achieves an average performance gain of 23.6\\% over strong baselines.", "tldr": "", "keywords": ["med-vlm", "multi-agent collaboration", "multimodal medical reasoning", "medical vqa", "reinforcement learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7e3b92925c07d9eef5d0e60f292a78c131dc06f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MMedAgent-RL, a multi-agent framework based on RL designed to optimize collaboration for multimodal medical diagnostic reasoning. The framework trains a triage doctor agent to assign cases and an attending physician agent, which uses a curriculum learning strategy, to integrate opinions from various specialist agents and make a final decision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed curriculum-based RL strategy to train an attending agent to evaluate and integrate potentially noisy or conflicting advice from specialists is a novel and well-motivated approach to improving the robustness of multi-agent systems."}, "weaknesses": {"value": "My major comments are:\n\n1. The specialist agents that provide the core domain knowledge are powerful proprietary models like GPT-4o and o3. This makes the framework's performance heavily dependent on external, closed-source models. The experiments do not clearly show how the system performs when only open-source models are used as specialists.\n2.  The paper claims a significant performance gain over an SFT method. However, this SFT baseline is not clearly defined in the main text. It is uncertain if this is a model fine-tuned on ground-truth answers, on specialist responses, or another configuration. \n3. The paper states the triage doctor is optimized using GRPO. However, the ablation study in Table 2 only presents a \"w/o Triage\" condition, which removes the step entirely. It fails to compare the RL-optimized triage agent against a simpler, non-RL baseline (e.g., a standard SFT-trained classifier). Given the near-perfect triage accuracy reported, the necessity of using RL for this component is not well-justified.\n4.  The curriculum learning strategy splits training data based on specialist accuracy (easy, medium, hard). The paper then presents an analysis of performance on test data that is also split by difficulty. The method for splitting this test data is not explained. If it is based on the specialists' accuracy on the test set, this constitutes data leakage, as it uses information about the test answers to perform the analysis."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YJxzQ5zLQ5", "forum": "2awntLXwR6", "replyto": "2awntLXwR6", "signatures": ["ICLR.cc/2026/Conference/Submission7891/Reviewer_A9FJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7891/Reviewer_A9FJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877822204, "cdate": 1761877822204, "tmdate": 1762919923706, "mdate": 1762919923706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a RL-based framework, namely MMedAgent-RL, for optimizing multi-agent collaboration in medical VLM reasoning. The proposed framework is well-motivated and empirically strong, with evaluations on both in-domain and out-of-distribution datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "(1) This framework develops a machine that can adjust collaboration policies based on task difficulty. The integration of C-MARL for entropy control is theoretically motivated.\n(2) The evaluation and experimental design are comprehensive. The proposed framework shows good performance in 5 public datasets, including both in-domain and out-of-distribution datasets."}, "weaknesses": {"value": "(1) The framework is inspired by the 'triage–specialist–attending'. The authors need to find more evidence to demonstrate that this aligns with the real hospitalization process. Within different sections in a hospital, the workflow may differ.\n(2) This work lacks the involvement of human experts. \n(3) Some of the technical details are missing. For example, are the first GP and the second GP updated simultaneously?"}, "questions": {"value": "The authors use Qwen2.5-VL as the base model. Is this framework transferable to other base models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ygwAWgRO7G", "forum": "2awntLXwR6", "replyto": "2awntLXwR6", "signatures": ["ICLR.cc/2026/Conference/Submission7891/Reviewer_dwrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7891/Reviewer_dwrJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882232494, "cdate": 1761882232494, "tmdate": 1762919923363, "mdate": 1762919923363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that overcomes the rigidity of existing collaboration systems by enabling dynamic and optimized cooperation among medical agents. The framework mimics a clinical \"triage-and-referral\" system, utilizing a curriculum RL strategy with dynamic entropy regulation to train a primary model to intelligently integrate and resolve noisy or conflicting inputs from various specialist agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, logically clear, and easy to follow.\n2. The theoretical derivations are fairly sound.\n3. Extensive experiments demonstrate the superiority of the proposed MMedAgent-RL."}, "weaknesses": {"value": "1. The middle part of Figure 1(a) does not reflect the practical workflow of Multi-Agent collaboration; it seems to lack representation of the General Practitioner, which leads to ambiguity.\n2. Section 3.1 mentions optimizing the triage doctor using GRPO, so it would be worthwhile to discuss the triage doctor's capability (quantitatively) as well as its reasoning process.\n3. The underlying mechanism for the entropy regularization term in Equation 3.1 needs to be explained, and the rationale behind the choice and range of values for $\\gamma_s$ should also be elaborated.\n4. Figure 3 mentions the selection of three 'o3' models as specialist doctors? Does this mean each specialty uses three 'o3's? If so, there seems to be no differentiation between the specialties. Why was 'o3' chosen over a specially designed medical MLLM?\n5. The experimental setup in Figure 4 is not clearly described, and more details should be provided."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DyGw2qX4m1", "forum": "2awntLXwR6", "replyto": "2awntLXwR6", "signatures": ["ICLR.cc/2026/Conference/Submission7891/Reviewer_noLG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7891/Reviewer_noLG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934254876, "cdate": 1761934254876, "tmdate": 1762919922992, "mdate": 1762919922992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMedAgent-RL, a reinforcement learning framework for multi-agent medical reasoning. The system simulates clinical workflows with a triage doctor routing cases to specialists (proprietary LVLMs), then an attending physician trained via curriculum RL to aggregate specialist opinions. The key innovation is a curriculum learning strategy with dynamic entropy regulation that progressively teaches the model to handle specialist outputs of varying reliability. Experiments on 5 medical VQA benchmarks show significant gains over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Framing multi-agent medical reasoning as a curriculum RL problem with dynamic entropy control is well-motivated by the reality of imperfect expert judgments.\n\n2. Strong empirical results, 23.6% average gain over baselines and excellent OOD generalization (72.6% on MMMU/OmniMedVQA) demonstrate effectiveness.\n\n3. The three-stage curriculum (easy/medium/hard based on specialist accuracy) with corresponding entropy coefficients (0.0001/0.005/0.03) is principled and clearly explained."}, "weaknesses": {"value": "1. Missing critical baselines: No comparison with simpler alternatives that could  possible achieve similar results, eg. single GPT-4o or Qwen2.5-VL sampling N diverse outputs using different prompts or high temperatures → majority voting or trained aggregator. These would test if the complex triage+multi-expert pipeline is necessary.\n\n2. The paper claims the attending physician learns to \"correct specialist mistakes,\" but provides no quantitative evidence on hard cases where all specialists fail. A fairer and more rigorous evaluation is needed to determine how much of the performance gain is due to routing and how much is due to aggregation. \n\n3. Best performance requires OpenAI llms, but medical data cannot be sent to external APIs in privacy-critical environments. Where is the evaluation with deployable open-source specialists ?\n\nI will reconsider my rating if the author addresses these questions well."}, "questions": {"value": "1. In \"w/o Triage\" (Table 2), what exactly happens? Random specialist? All specialists? Please clarify and consider: single model with diverse sampling might achieve similar diversity without routing.\n\n2. Can you provide results progressively adding components (base → +triage → +multi-expert → +curriculum RL) to quantify each contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "diK9Fu0Sb2", "forum": "2awntLXwR6", "replyto": "2awntLXwR6", "signatures": ["ICLR.cc/2026/Conference/Submission7891/Reviewer_NzMH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7891/Reviewer_NzMH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134620080, "cdate": 1762134620080, "tmdate": 1762919922495, "mdate": 1762919922495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}