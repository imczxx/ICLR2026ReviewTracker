{"id": "5Y8PMEeAkv", "number": 18041, "cdate": 1758283234404, "mdate": 1759897137239, "content": {"title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models", "abstract": "As the capabilities of Vision-Language Models (VLMs) advance, they can process increasingly large inputs, which, unlike in LLMs, generates significant visual token redundancy and leads to prohibitive inference costs. While many methods aim to reduce these costs by pruning visual tokens, existing approaches, whether based on attention or diversity, typically neglect the guidance of the text prompt and thus fail to prioritize task relevance. In this work, we propose a novel, zero-shot method that reframes the problem by introducing a prompt-aware perspective, explicitly modeling visual token pruning as a balance between task relevance and information diversity. Our hierarchical approach first selects a core set of task-relevant visual tokens and then supplements them with diversity tokens to preserve broader context. Experiments across multiple models and benchmarks show that our method achieves performance that matches or surpasses the state-of-the-art with only minimal accuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these gains are accompanied by significant reductions in GPU memory footprint and inference latency.", "tldr": "", "keywords": ["VLM", "Prompt-Aware", "Zero-Shot", "Visual token pruning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b934374716f30aaa602dc9f8adf10a939e4a0805.pdf", "supplementary_material": "/attachment/d5f400bfcf8521072140b3aaa9f32a1675042e58.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ZSPAPrune, a zero-shot, plug-and-play visual token pruning method that accelerates inference in vision-language models without any fine-tuning. The approach selects a small budget of visual tokens in two stages: first choosing the tokens most relevant to the text prompt (task-relevant core set), then adding tokens that are maximally diverse to preserve global context. Evaluated on LLaVA-1.5-7B and Qwen2.5-VL-7B-Instruct under aggressive 90% pruning, ZSPAPrune matches or improves accuracy on benchmarks such as MMMU, GQA, AI2D, POPE, TextVQA, and ChartQA compared to strong baselines like DivPrune. The paper also reports modest latency and memory reductions at inference time and emphasizes that the method is model-agnostic and easy to integrate."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear, zero-shot pruning method that balances prompt relevance and visual diversity, which prior work did not.\n\nExperiments across strong VLMs and multiple benchmarks show it maintains or improves accuracy under extreme pruning while reducing cost.\n\nThe method is practically significant because it can be dropped into existing VLMs without any retraining or architectural changes."}, "weaknesses": {"value": "The paper does not report direct quantitative comparisons against strong prompt-aware pruning baselines (e.g., GlimpsePrune), so it is hard to verify that the proposed approach is actually better than the closest prior work.\n\n\nThe efficiency claims are based on a single model/setting and only at an extreme 90% pruning ratio, with limited analysis of where latency and memory savings come from or how they scale with pruning level.\n\nThe method is essentially heuristic and lacks a clear formal objective or robustness analysis (e.g., failure cases when relevance vs. diversity is misbalanced).\n\nThe evaluation is limited to ~7B-scale VLMs, and there is no evidence that the proposed pruning strategy remains effective or stable for larger vision-language models, where attention structure and token redundancy may differ."}, "questions": {"value": "How stable is ZSPAPrune across different prompt styles (e.g., long multi-step reasoning questions vs. short factual queries), and does the same relevance/diversity ratio work across them without retuning?\n\nHave you investigated automatically selecting the relevanceâ€“diversity ratio at inference time (e.g., predicting it from the prompt or task type), rather than setting it manually per dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WAEexHL1tP", "forum": "5Y8PMEeAkv", "replyto": "5Y8PMEeAkv", "signatures": ["ICLR.cc/2026/Conference/Submission18041/Reviewer_aMaK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18041/Reviewer_aMaK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003369403, "cdate": 1762003369403, "tmdate": 1762927827965, "mdate": 1762927827965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ZSPAPrune, a zero-shot, prompt-aware token pruning framework for Vision-Language Models (VLMs). Existing pruning methods are often prompt-agnostic, ignoring text guidance and thus failing to prioritize task-relevant visual information. ZSPAPrune addresses this by reframing pruning as a balance between task relevance and information diversity, achieved through a hierarchical process: Prompt Simplification, Prompt-Aware Selection, and Diversity Balance. The method selects core visual tokens most relevant to the prompt and augments them with diverse tokens to retain global context. Experiments on multiple benchmarks and models show that ZSPAPrune achieves state-of-the-art or comparable performance with minimal accuracy loss even when pruning up to 90% of tokens, while significantly reducing GPU memory usage and inference latency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. From a perspective of prompt-aware token selection to balance task relevance and information diversity in visual representations.\n2. Introducing a hierarchical pruning mechanism composed of Prompt Simplification, Prompt-Aware Selection, and Diversity Balance to achieve controllable token reduction.\n3. Achieving significant inference efficiency improvements with minimal accuracy loss under zero-shot settings across multiple Vision-Language Models and benchmarks."}, "weaknesses": {"value": "1. The paper lacks comparison with other methods that explicitly address the trade-off between task relevance and information diversity. Without such comparison, it remains unclear whether the proposed balance strategy is superior or merely heuristic.\n2. As a plug-and-play method, ZSPAPrune should be validated on more models with different parameter scales to confirm its general applicability. The current experiments are limited to a narrow range of architectures, reducing the evidence of scalability.\n3. The comparison with task-relevance-based approaches appears potentially unfair. Some baselines are reimplemented without clear alignment in training setup or hyperparameter tuning, which may bias the reported results.\n4. The proposed method is overly simple and lacks crucial theoretical analysis. No formal justification or complexity discussion is provided to explain why the hierarchical prompt-aware pruning mechanism should work effectively.\n5. The framework figure (i.e., Figure 2) is overly general and resembles a process diagram rather than an architectural framework. It fails to visually highlight the innovation and importance of the proposed components, and a more informative figure is recommended."}, "questions": {"value": "null"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mA02kDIBIO", "forum": "5Y8PMEeAkv", "replyto": "5Y8PMEeAkv", "signatures": ["ICLR.cc/2026/Conference/Submission18041/Reviewer_85xg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18041/Reviewer_85xg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239034685, "cdate": 1762239034685, "tmdate": 1762927827516, "mdate": 1762927827516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies token pruning issue in vision large language models. Specifically, it takes token pruning in vLLMs as  a tunable balance between task relevance and information diversity. In implementation, the prompt-aware score by calculating the relevance between prompts and token embeddings, while the diversity balance is calculated by selecting the token most disimilar to all previously selected tokens. Experiments are done on several benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The strengths are as follows:  \n1.The paper is easy to read and the method is easy to follow.  \n2.Evaluated datasets and vLLMs are diverse."}, "weaknesses": {"value": "The weakness are as follows:  \n1.There are many existing works on task relevance of token pruning for vLLMs. This work additionally considers the information diversity, which seems incremental novelty. Meanwhile, in Figure 1, it is not easy to understand why the information diversity is useful for token pruning task.   \n2.Missing related works. Recently, there are many other token pruning methods[1,2,3,4], which are not analyzed and discussed in this work. These works should also be added for comparison.  \n3.In the method design, I have some concerns:  \n(1) In Eq.4, averge pooling is applied on the prompt token embeddings. It is not quite reasonable since the prompt text may involve many not task-releted tokens.  \n(2) The diversity balance is performed by selecting some tokens dissimilar to previously selected ones. Probably, this could select some useless tokens and background tokens. I am not sure this motivation is correct.  \n\n\n[1] Boosting multimodal large language models with visual tokens withdrawal for rapid inference  \n[2] Dynamic-llava: Efficient multimodal large language models via dynamic vision-language context sparsification.  \n[3] Visionzip: Longer is better but not necessary in vision language models  \n[4] Folder: Accelerating multi-modal large language models with enhanced performance"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l7YfWTBXbN", "forum": "5Y8PMEeAkv", "replyto": "5Y8PMEeAkv", "signatures": ["ICLR.cc/2026/Conference/Submission18041/Reviewer_faFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18041/Reviewer_faFY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762260995705, "cdate": 1762260995705, "tmdate": 1762927827034, "mdate": 1762927827034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}