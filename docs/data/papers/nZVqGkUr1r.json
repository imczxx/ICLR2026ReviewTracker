{"id": "nZVqGkUr1r", "number": 8923, "cdate": 1758102710378, "mdate": 1763697231147, "content": {"title": "Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries", "abstract": "Large language models (LLMs) are increasingly used to generate code, yet they continue to hallucinate, often inventing non-existent libraries.\nSuch library hallucinations are not just benign errors: they can mislead developers, break builds, and expose systems to supply chain threats such as slopsquatting.\nDespite increasing awareness of these risks, little is known about how real-world prompt variations affect hallucination rates.\nTherefore, we present the first systematic study of how user-level prompt variations impact library hallucinations in LLM-generated code.\nWe evaluate six diverse LLMs across two hallucination types: library name hallucinations (invalid imports) and library member hallucinations (invalid calls from valid libraries).\nWe investigate how realistic user language extracted from developer forums and how user errors of varying degrees (one- or multi-character misspellings and completely fake names/members) affect LLM hallucination rates.\nOur findings reveal systemic vulnerabilities: one-character misspellings trigger hallucinations in up to 26% of tasks, fake libraries are accepted in up to 99% of tasks, and time-related prompts lead to hallucinations in up to 84% of tasks.\nPrompt engineering shows promise for mitigating hallucinations, but remains inconsistent and LLM-dependent.\nOur results underscore the fragility of LLMs to natural prompt variation and highlight the urgent need for safeguards against library-related hallucinations and their potential exploitation.", "tldr": "A deep dive into how prompt variations can effect library hallucinations rates in LLMs, including user descriptions of libraries, misspellings in the library name, and popular prompting strategies.", "keywords": ["llms", "code-generation", "llm for code", "llm hallucinations"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc26c26fa7b3899a0a21cb36611e2daa5ca7f6c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an empirical analysis of the extent to which language models for code hallucinate concerning dependent software libraries.\n\nThe paper investigates six language models (GPT4, GPT5, Ministral, Qwen, Llama, and DeepSeek). The dataset used starts from BigCodeBench, yielding a dataset of 356 tasks using 30 distinct libraries. Three experiments are conducted:\n\n1. To what extent do variations in user descriptions of libraries to be used influence hallucinations?\n2. To what extent do misspellings in prompts lead to hallucinations?\n3. To what extent do prompt engineering techniques (such as chain of thought, or step back) lead to hallucinations?\n\nThe experiments find that:\n\n1. Adjectives (like fast, modern) to libary instructions generally have no influence; adding a year (\"from 2025\") does increase presence of hallucinations\n2. Simple spelling mistakes leads to hallucinations in up to 26% of the tasks, with all LLMs resorting to fake library names.\n3. Prompt engineering techniques like self-analysis and explicit checks can reduce hallucinations, but chanin of thought and step-back prompting often worsen hallucination rates.\n\nThe paper discusses how asking for lesser known libraries can further increase hallucinations, and whether hallucinations could be considered as sources of inspiration for desired functionality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- A very well written paper\n- Rigorous, well conducted experiment\n- Clear and understandable results from the experiments\n- A relevant problem"}, "weaknesses": {"value": "The findings are clear and the experiments are very well conducted. I was, however, somewhat disappointed by the discussion, which seems to accept the results at face value and just presents two somewhat unrelated ideas. I was hoping for a more rigorous treatment of underlying principles, consequences and implications, also to strengthen the sense of urgency from this paper. Why are we getting these results? What should be done about them?\n\nFor example, the finding that typos can have detrimental effects is a nice one -- but how could it be addressed? Could it be circumvented by having a tool check anything that might be a library, and spellcheck the prompt to use correct library names? Or: could LLMs be made more robust by adding 'adversarial' finetuning with likely typos?\n\nThe setup could also be viewed as a way to evaluate the effect of countermeasures. In practice, new library versions will continue to arrive after an LLM has been created, so RAG-based solutions or frequent fine tuning approaches are inevitable. But how well would they work? Might the setup proposed in this paper help?"}, "questions": {"value": "- What would you include in a bolder, more urgent discussion section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PW3Wkzgemk", "forum": "nZVqGkUr1r", "replyto": "nZVqGkUr1r", "signatures": ["ICLR.cc/2026/Conference/Submission8923/Reviewer_BPXC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8923/Reviewer_BPXC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031529355, "cdate": 1762031529355, "tmdate": 1762920673118, "mdate": 1762920673118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic study of how user-level prompt variations impact library hallucinations in LLM-generated code, evaluating six diverse LLMs (GPT-4o-mini, GPT-5-mini, Ministral-8B, Qwen2.5-Coder, Llama-3.3, DeepSeek-V3.1) across 356 Python coding tasks from BigCodeBench. The authors investigate two hallucination types, library name hallucinations and library member hallucinations, through three systematic experiments examining realistic user language from developer forums, user errors of varying severity, and prompt engineering mitigation strategies. Key findings reveal systemic vulnerabilities: while adjective-based descriptions like \"fast\" or \"lightweight\" are largely ignored, year-based prompts asking for libraries \"from 2025\" trigger hallucinations in up to 84% of tasks; one-character misspellings cause hallucinations in up to 26% of tasks, multi-character misspellings in up to 79%, and fake library names are accepted in up to 99% of tasks, demonstrating extreme model sycophancy. Prompt engineering shows mixed results, self-analysis and explicit-check strategies reduce hallucinations in ~80% of cases, while popular reasoning prompts like cot often increase hallucination rates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors provide a precise definition of \"library hallucinations\" as two types of measurable failures (non-existent package names and invalid member calls from valid packages), and systematically measure them using three groups of realistic prompt factors (descriptive words/year-based prompts, user spelling errors, low-barrier prompt engineering). The narrative structure is clear and well-organized.\n\n2. The experimental design is rigorous and comprehensive: it evaluates 6 diverse LLMs (covering open/closed source, general/code-specific, 8B-671B parameters) on 356 carefully filtered tasks from BigCodeBench, generating 3 responses per task using fresh API sessions to avoid caching effects.\n\n3. The paper establishes a particularly insightful novel connection between LLM hallucinations and supply chain attacks (typosquatting/slopsquatting), revealing how models' sycophantic behavior, accepting fake library names in up to 99% of tasks, creates exploitable vulnerabilities.\n\n4. The paper systematically compares four low-cost mitigation strategies (CoT, Step-back, Self-analysis, Explicit-check), providing immediately usable \"lightweight prompt modification\" baselines for practical engineering teams."}, "weaknesses": {"value": "1. The Python-only scope with merely 30 libraries may not represent broader ecosystems. Results may not generalize to JavaScript, Java, or Rust with different packaging conventions, and the paper does not analyze whether findings vary by library popularity, domain, or API complexity, despite BigCodeBench containing 7 distinct domains.\n\n2. The hallucination detection methodology is problematic: checking only against the latest documentation fails to distinguish true hallucinations from outdated knowledge or version mismatches, which is critical since models trained on older versions may correctly reference deprecated APIs. The manual review process lacks details on inter-rater reliability.\n\n3. The most fundamental problem is the lack of methodological innovation, this is primarily a measurement study that does not propose any novel solutions. The paper only tests 4 existing prompt engineering techniques (chain-of-thought, step-back, etc.), which essentially reflects an absence of methodological contribution. For instance, the interesting finding in Experiment 3 that chain-of-thought prompting increases hallucination rates in 50% of cases is not leveraged to design better methods."}, "questions": {"value": "See the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mfjiO0v5vg", "forum": "nZVqGkUr1r", "replyto": "nZVqGkUr1r", "signatures": ["ICLR.cc/2026/Conference/Submission8923/Reviewer_KMKa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8923/Reviewer_KMKa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233570805, "cdate": 1762233570805, "tmdate": 1762920672674, "mdate": 1762920672674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts the comprehensive analysis of how realistic prompt variations affect hallucinations of library names and functions in LLM-generated Python code. The authors discover that while descriptive adjectives in prompts have little effect, subtle phrasing changesâ€”such as year-based descriptions, or one-character misspelling-can significantly increase hallucination frequency. Although some prompt-engineering strategies can reduce these errors, their effectiveness is not consistent. The future work should focus more on this issue for building a more reliable coding model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(s1) The studied problem is important and realistic.\n\n(s2) The findings in this paper may provide some new insights to the community.\n\n(s3) The empirical results support some claims in the paper."}, "weaknesses": {"value": "(w1) The writing can be improved. For example, a figure of general description can be provided for the better understanding of readers.\n\n(w2) I would like to see the evaluation results of more advanced coding LLMs, such as Claude. The selected models in the current version do not yet represent the best available coding capability..\n\n(w3) The authors only conduct evaluations on Python language on one specific dataset (BigCodeBench). More results on other datasets would strengthen the paper. \n\n(w4) The prompting strategies show limited effectiveness in mitigating the hallucinations. I would suggest the authors try some training-based method. \n\n(w5) The Broader Impact section is missing, and I think this paper should include a discussion on some potential social impacts."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The findings of this paper might potentially be exploited for malicious purposes, posing risks in LLM-based code generation scenarios."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "haiOtNlZ5X", "forum": "nZVqGkUr1r", "replyto": "nZVqGkUr1r", "signatures": ["ICLR.cc/2026/Conference/Submission8923/Reviewer_DLB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8923/Reviewer_DLB3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762421535024, "cdate": 1762421535024, "tmdate": 1762920672244, "mdate": 1762920672244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Dear Reviewers DLB3, KMKa and BPXC, and Area Chair,"}, "comment": {"value": "We sincerely appreciate the time and effort you have dedicated to reviewing our work. We thank all reviewers for their thoughtful and constructive feedback, and are encouraged by the many positive comments also given. We have responded to all of your queries in your individual responses below, and have revised the paper to address all points raised.\n\nA summary of all enhancements made to the paper during the discussion period, inspired by your valuable comments, is as follows:\n\n- Added a diagram to aid in general understanding of our pipeline. (p2)\n- Included full results for Claude-4.5-Haiku. (p6,7,9)\n- Emphasized the published benchmark dataset, that allows readers to evaluate their own models, in the main text. (p10)\n- Added a discussion of the practical implications and broader impacts. (p8)\n- Domain analysis appendix, showing the results from the main paper aggregated over the task domains. (p25)\n- Finetuning appendix, containing an initial investigation into how a simple finetuning strategy could help mitigate hallucinations. (p25)\n- Generalisability appendix, initial results indicating that our findings generalise to other real-world tasks, and a wider range of libraries. (p26)\n- Various improvements to the writing, with added detail for certain topics, such manual result analysis (p5) and version checking (p6).\n\nWe look forward to hearing more from you, and further discussing our paper and the revisions we have made."}}, "id": "lu1f3V998h", "forum": "nZVqGkUr1r", "replyto": "nZVqGkUr1r", "signatures": ["ICLR.cc/2026/Conference/Submission8923/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8923/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission8923/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763698192673, "cdate": 1763698192673, "tmdate": 1763698192673, "mdate": 1763698192673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}