{"id": "rhD7ZuFAjU", "number": 14745, "cdate": 1758242956460, "mdate": 1759897351621, "content": {"title": "Balancing the Experts: Unlocking LoRA-MoE for GRPO via Mechanism-Aware Rewards", "abstract": "Parameter-efficient Mixture-of-Experts (MoE) architectures, such as LoRA-MoE, enable strong and generalizable fine-tuning. However, a critical problem arises when fine-tuning these architectures with advanced reinforcement learning algorithms such as Group Relative Policy Optimization (GRPO). Traditional supervised techniques, such as auxiliary losses, are incompatible with the GRPO process, while the external task reward is blind to the internal routing mechanism. This disconnect leads to routing collapse and severe underutilization of MoE adapter parameters. To resolve this disconnect, we introduce Routing-Optimized Group Relative Policy Optimization (RO-GRPO), a mechanism-aware framework. It turns internal expert routing statistics collected during training into a direct reward signal, seamlessly integrating routing supervision into the reinforcement fine-tuning (RFT) process. This enables effective optimization of parameter utilization and improves performance on both unimodal and multimodal mathematical reasoning tasks, all without extra training stages. Our work provides the first demonstration that a scalar reward in GRPO can be engineered from a model's own internal mechanics to explicitly guide their optimization, extending alignment from mere behavior tuning to holistic mechanism alignment.", "tldr": "We introduce RO-GRPO, a method that prevents routing collapse in MoE models during GRPO by transforming internal routing statistics into a reward signal, enabling the simultaneous alignment of a model's behavior and its internal mechanisms.", "keywords": ["Large Language Models (LLMs)", "Reinforcement Learning from Human Feedback (RLHF)", "Mixture-of-Experts (MoE)", "Parameter-Efficient Fine-Tuning (PEFT)", "Group Relative Policy Optimization (GRPO)"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38caf14b1b28e1314442c380f8de2483eef6d73a.pdf", "supplementary_material": "/attachment/9d9c9adc08031114b6c666fecb4a1062dbe9ff0f.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a technique called RO-GRPO, which maximizes utility of experts of MoE through LLM-RL training. RO-GRPO adds an additional rewards based on routing entropy and expert utilization to explicitly encourage the MoE model to efficiently use experts. The paper introduces two variations, RO-GRPO (Smooth) and RO-GRPO (Relative). Smooth variation schedules weights of additional reward terms while Relative variation computes the reward signal based on improvements over historical baselines, which eliminates the manual scheduling mechanics. In experiments, both variations outperforms a baseline and especially the Relative variation shows strong performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed idea is simple and easy to integrate to existing MoE training. And, the Relative variation eliminates manual balancing to tune performance, which makes this approach very practical.\n- The performance improvement is significant especially when increasing the number of experts, which proves its effect.\n- Through extensive analysis, how individual changes contribute behavior of MoE training is clarified in the paper."}, "weaknesses": {"value": "- Although the proposed idea sounds sane, a straightforward alternative approach would be to use the routing entropy and the expert utilization as loss functions, which could directly optimize those metrics. I found the similar description in Appendix E.1, however this seems vital baselines to makes more sense to use the proposed idea. Lack of this analysis keeps my score as it is.\n- Analysis of models' outputs with the proposed change should be interesting, but there isn't much observation provided in the paper except Appendix D. Since the proposed model can utilize experts more, it would be interesting to observe different trends in output texts. This analysis will improve the manuscript."}, "questions": {"value": "- Computing the route reward adds any additional computing overheads?\n- Do we need to balance scaling between the main task reward and the route reward? Can we just use the route reward with its raw scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DV3foVIGXW", "forum": "rhD7ZuFAjU", "replyto": "rhD7ZuFAjU", "signatures": ["ICLR.cc/2026/Conference/Submission14745/Reviewer_CGAv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14745/Reviewer_CGAv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760923252804, "cdate": 1760923252804, "tmdate": 1762925104540, "mdate": 1762925104540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the expert collapse and underutilization problem in reinforcement fine tuning of LORA-MOE architectures. The authors proposed a routing confidence and a load balancing reward. The two rewards are combined and summed with the task reward using a weight curriculum. Then the combined reward is optimized using the GRPO algorithm. Experiments on conducted on several math reasoning benchmarks. The authors show the effectiveness of their method for controlling expert confidence and load balance. It was further shown in ablations that removing the proposed mechanisms degrade the performance of LORA-MOE fine tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The presentation of the idea is pretty clear. \n* The ability of the proposed method to control entropy and load balance is clearly pretty effective from the plots."}, "weaknesses": {"value": "* While the method is effective at controlling entropy and load balance, it is not clear that it improves performance for more experts, which I belief is the ultimate goal for this family of methods. In fact from the results tables, 8 experts is almost never the overall best. \n* While integrating these objectives into the reward function seems to unify the training method, it is not clear from the paper that simply adding auxiliary losses to the RL objective is fundamentally flawed or significantly worse."}, "questions": {"value": "* Have you considered adding auxiliary loss to the simple RL objective as a baseline? Could you discuss this option or show some experiments?\n* Although it's likely intuitive, could you explain why applying the combined reward on the entire policy rather than applying the entropy and load balancing reward to only the routers is so effective? And why not the latter option?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0MbB4UYDsg", "forum": "rhD7ZuFAjU", "replyto": "rhD7ZuFAjU", "signatures": ["ICLR.cc/2026/Conference/Submission14745/Reviewer_eaSP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14745/Reviewer_eaSP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761365641645, "cdate": 1761365641645, "tmdate": 1762925104027, "mdate": 1762925104027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of applying LoRA-MoE architectures to reinforcement learning fine-tuning RFT, specifically GRPO. The authors state that traditional auxiliary losses used to guide expert routing in supervised settings are incompatible with GRPO's scalar reward framework, leading to routing collapse and poor expert utilization. They propose RO-GRPO, which transforms internal routing statistics into reward signals through two components: a confidence reward encouraging low-entropy routing and a balance reward promoting uniform expert utilization. Two variants are introduced, (i) Smooth (curriculum-based scheduling) and (ii) Relative (binary reward for simultaneous improvement). Experiments on mathematical reasoning tasks using Qwen2.5 models demonstrate improvements in both task performance and routing efficiency across numerous expert counts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a meaningful problem at the intersection of parameter-efficient fine-tuning and reinforcement learning, with practical implications for deploying and adapting large models.\n- The proposed approach is technically sound, integrating routing supervision directly into the reward signal without requiring architectural modifications or additional training stages.\n- The experimental design is comprehensive, covering both unimodal and multimodal mathematical reasoning tasks with consistent methodology across multiple benchmarks.\n- The theoretical grounding connecting the approach to constrained optimization and information bottleneck principles adds depth to the empirical contributions."}, "weaknesses": {"value": "- The scope of evaluation is limited to adapter-based MoE architectures, leaving unclear whether the identified routing pathologies and proposed solutions generalize to full MoE models such as Phi-3.5-MoE or OLMoE.\n\n- The design choice to formulate routing supervision as rewards rather than auxiliary losses is not adequately justified. Given that GRPO already employs KL divergence as a regularization term, it is unclear why routing constraints cannot be incorporated similarly as loss terms.\n\n- The experimental comparisons lack a crucial baseline: a standard LoRA configuration with parameter count matching the LoRA-MoE variants, making it difficult to isolate whether performance gains stem from the routing-aware rewards or simply from increased model capacity.\n\n- Tables 2 reveal several instances where the vanilla GRPO (LoRA) baseline with approximately one-quarter the trainable parameters outperforms RO-GRPO variants, undermining claims of consistent improvement."}, "questions": {"value": "- Could you provide experimental results on full MoE architectures rather than adapter-based variants? This would clarify whether routing collapse is inherent to MoE structures under GRPO or specific to the LoRA-MoE configuration.\n\n- What happens when you include a GRPO baseline using standard LoRA with trainable parameters matching the LoRA-MoE models (e.g., approximately 127M parameters for the E=8 configuration)?\n\n- Can you provide qualitative analysis or case studies demonstrating what different experts actually learn? Do they specialize in different mathematical reasoning strategies, problem types, or computational roles?\n\n- Why must routing supervision be formulated as a reward signal rather than an auxiliary loss term? Could you compare your approach against a variant that incorporates routing entropy and load balancing as loss terms alongside the GRPO objective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7uvAFFFpAz", "forum": "rhD7ZuFAjU", "replyto": "rhD7ZuFAjU", "signatures": ["ICLR.cc/2026/Conference/Submission14745/Reviewer_xfQ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14745/Reviewer_xfQ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979899715, "cdate": 1761979899715, "tmdate": 1762925103613, "mdate": 1762925103613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a mismatch when fine-tuning LoRA-MoE models with GRPO: the external task reward is unaware of internal expert routing, leading to routing collapse and poor MoE utilization. The proposed RO-GRPO augments the reward with a mechanism-aware term $R_{\\text{routing}}$ added to the task reward $R_{\\text{task}}$. $R_{\\text{routing}}$ combines (i) routing confidence (mean per-token routing entropy; lower is better) and (ii) load balance (MSE to uniform expert usage; lower is better). Two strategies are evaluated: a smooth curriculum that shifts emphasis from confidence to balance, and a relative scheme that rewards only when both metrics improve over a historical baseline. On uni- and multimodal math reasoning tasks, RO-GRPO improves accuracy and yields healthier expert utilization versus vanilla LoRA-MoE. Ablations indicate both components matter; a shuffled-control (permuting $R_{\\text{routing}}$ within a batch) removes gains, supporting causal attribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple, novel, and practical: Converts internal routing statistics into a scalar reward, no architectural changes or auxiliary losses.  \n2. Credible evidence: Informative ablations and a strong shuffled-control isolate the effect of routing-aware feedback.  \n3. Clear exposition & tangible impact: Clean figures and a persuasive qualitative case link improved routing to better reasoning."}, "weaknesses": {"value": "1. Parameter-budget confound (major): Baselines compare ~30M LoRA vs RO-GRPO variants up to ~127M trainables; needs budget-matched or FLOPs-normalized comparisons.  \n2. Narrow task scope: All benchmarks are math-centric; generality to instruction following, summarization, dialogue, or safety is untested.  \n3. Limited algorithmic scope & reporting gaps: Only GRPO is shown; PPO/DPO not demonstrated. Clarify entropy/balance aggregation and sensitivity; consider risks of “confidence” gaming."}, "questions": {"value": "1. Budget parity: How does RO-GRPO compare to a high-rank LoRA baseline matched to the $E{=}8$ trainables (~127M)? Please report accuracy and routing metrics under parameter/FLOPs parity.  \n2. Beyond math: Any preliminary results on instruction-following or summarization, and observations on expert specialization there?  \n3. PPO/DPO adaptation: For PPO, can $R_{\\text{routing}}$ be used as standard reward shaping without instability (need annealing/curriculum)? For DPO, is reweighting by $\\Delta$routing (preferred minus rejected) viable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iKv255cH1O", "forum": "rhD7ZuFAjU", "replyto": "rhD7ZuFAjU", "signatures": ["ICLR.cc/2026/Conference/Submission14745/Reviewer_rDj1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14745/Reviewer_rDj1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244857603, "cdate": 1762244857603, "tmdate": 1762925103212, "mdate": 1762925103212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}