{"id": "afO3vnSNsS", "number": 16702, "cdate": 1758267824772, "mdate": 1759897223995, "content": {"title": "ClarifyVC: Clarifying Ambiguous Commands in Vehicle Control with a Hybrid Data Augmentation Pipeline", "abstract": "Natural language interfaces for vehicle control must contend with vague commands, evolving dialogue context, and strict protocol constraints. \nWe introduce ClarifyVC, a unified framework that integrates a hybrid data-augmentation pipeline (ClarifyVC-Data), reference models trained on the data (ClarifyVC-Models)\nand a evaluation protocol (ClarifyVC-Eval). \nThe agent-orchestrated pipeline generates diverse, ambiguity-rich dialogues from real-world seeded queries under schema and safety constraints, while the evaluation protocol systematically probes single-turn parsing, conservative clarification under extreme fuzziness, and multi-turn grounding. \nFine-tuning on ClarifyVC-Data yields consistent gains—up to 15\\% higher parsing accuracy, 20\\% stronger ambiguity resolution, and 98\\% protocol compliance—across realistic in-cabin scenarios, with human-in-the-loop assessments confirming high realism, coherence, and applicability. \nClarifyVC thus advances beyond simulation-only datasets by tightly coupling real-world grounding with scalable generation and standardized evaluation, and provides a generalizable pipeline for broader interactive control domains.", "tldr": "", "keywords": ["Interactive Control Systems", "Clarification-First Dialogue", "Ambiguity Resolution", "Hybrid Data Augmentation", "Function-Calling Language Models", "Human Validation and Robustness"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/274ef767740d6a67cf1fa750ae3a196dc51f1ba3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ClarifyVC, a framework unifying data augmentation, reference models, and an evaluation protocol for in-car language control. This paper’s agent-driven pipeline creates diverse, ambiguity-rich dialogues from real-world seeded queries under schema and safety constraints. This paper evaluates single-turn parsing, conservative clarification under fuzziness, and multi-turn grounding. This paper reports gains up to 15% in parsing accuracy, 20% in ambiguity resolution, and 98% protocol compliance, supported by human evaluations of realism and applicability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes new data, evaluation methods, and models for improving an in-car assistant—an important application.\n- It presents a set of metrics to evaluate data and model quality.\n- Extensive experiments demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "- The task specification and the descriptions of data augmentation, training data, and test data are not detailed enough for a proper assessment.\n\n- It’s difficult to discern the exact task the paper targets from the current presentation.\n\n- The dataset is not described: the types of tasks, their variations, and their intended purposes are unclear. In particular, the real-world in-vehicle logs used in the experiments are not characterized (e.g., distribution, types, lengths).\n\n- The basic instruction-following evaluation uses both Talk2Car and real-world in-vehicle control logs, but those real-world logs are also used for training. Even if the splits differ, the distributions may be similar (the paper doesn’t specify), which could bias results in favor of the trained model.\n\n- The benchmarks used for the “Evaluation on Advanced Scenarios” are not clearly identified."}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0dCHNyQUbB", "forum": "afO3vnSNsS", "replyto": "afO3vnSNsS", "signatures": ["ICLR.cc/2026/Conference/Submission16702/Reviewer_NcpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16702/Reviewer_NcpB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961911866, "cdate": 1761961911866, "tmdate": 1762926754306, "mdate": 1762926754306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is inspired by the ambiguity of natural-language commands for in-car voice assistants, and proposes a pipeline to augment data for the purposes of command clarification. The paper also proposes an evaluation protocol and metrics to evaluate the quality of such data and any models trained on it, including ambiguity detection and resolution in single and multi-turn dialogue.\n\nThe paper provides rigorous experimentation, examining both the efficacy of the proposed augmentation process and related work datasets (under the proposed evaluation framework and metrics), and the downstream effect of the data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- While not the first dataset to address command ambiguity in an in-car setting, it combines data-augmentation towards ambiguity resolution (seeded from real world scenarios) and multi-turn grounding.\n- The proposed evaluation protocol should be useful to the community moving forward, and it clearly shows the efficacy of the proposed data-augmentation process.\n- Further rigorous analysis in the paper supports the usefulness of the augmented data in fine-tuning models, including comparisons of the dataset against previous ones (under the proposed evaluation framework), evaluation of models fine-tuned with the dataset, and comparisons against closed-source models."}, "weaknesses": {"value": "- The scope of the paper is limited to in-car voice assistants. While the paper claims that the approach is domain-agnostic, such is not demonstrated in this paper.\n- Unclear whether the data will be released, as they seem to be based on proprietary seeds. This could hurt both reproducibility and the impact of this work."}, "questions": {"value": "- The paper uses the low public perception of self-driving cars as a motivating factor for this work, but it is unclear how addressing in-car voice assistants helps address this, please elaborate.\n- Please provide more details on how AD, PC and R are measured in the main body of the paper.\n- Please elaborate on the human evaluation of Section 4.1. Consulting the appendix as well, it seems that these evaluation did not consider any alternative baseline o related work. Is this assumption accurate? Without such a reference, this evaluation could be considered biased, and does not validate the method against previous work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Cjx90K5Hzr", "forum": "afO3vnSNsS", "replyto": "afO3vnSNsS", "signatures": ["ICLR.cc/2026/Conference/Submission16702/Reviewer_S7bf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16702/Reviewer_S7bf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001519117, "cdate": 1762001519117, "tmdate": 1762926753762, "mdate": 1762926753762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ClarifyVC for clarifying ambiguous natural language commands in vehicle control, combining a dataset, fine-tuned reference models, and a multi-turn dialogue safety-aware evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-structured and clearly presented.\n- The benchmark is more challenging than existing open datasets, with 20k real logs with multi-turn in-vehicle commands and dialogues.\n- The improvements achieved by the model fine-tuned on the proposed dataset are strong and well-demonstrated through clear presentation."}, "weaknesses": {"value": "- As the authors mentioned in the limitation section, the current benchmark focuses solely on textual command understanding. However, real in-vehicle interactions often require multimodal comprehension, where interpreting a command may depend on visual context or environmental cues."}, "questions": {"value": "1. How does the framework ensure that “ambiguity injection” does not distort the underlying intent distribution of real user commands?\n2. How can causal relationships in model performance be evaluated and validated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GuxMlIZ4sf", "forum": "afO3vnSNsS", "replyto": "afO3vnSNsS", "signatures": ["ICLR.cc/2026/Conference/Submission16702/Reviewer_UDzZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16702/Reviewer_UDzZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762894371860, "cdate": 1762894371860, "tmdate": 1762926753409, "mdate": 1762926753409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces 3 novel contributions, a framework for clarifying ambiguous commands, a dataset, and an evaluation protocol. Previous works have worked with single-turn parsing, but the proposed pipeline maintains a focus on multi-turn grounding, for interaction with the user. The benchmark dataset and evaluation protocol outperform other models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Original ideas with novel approaches, high quality of the work presented and incredibly detailed, strong contributions with excellent evaluation benchmarks, the figures are very well made"}, "weaknesses": {"value": "Some of the comparison evaluations seem redundant and make the flow of the paper harder to read"}, "questions": {"value": "1.  In table 2b, why was human validation conducted on ClarifyVC-Data only? \n2. Under Function Hit Rate on Page 8, what is the gold standard referring to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1kvi6tKYe4", "forum": "afO3vnSNsS", "replyto": "afO3vnSNsS", "signatures": ["ICLR.cc/2026/Conference/Submission16702/Reviewer_TGRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16702/Reviewer_TGRH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762902544887, "cdate": 1762902544887, "tmdate": 1762926752297, "mdate": 1762926752297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}