{"id": "c2dqbma1pj", "number": 20731, "cdate": 1758309419151, "mdate": 1759896961315, "content": {"title": "Resolving Extreme Data Scarcity by Explicit Physics Integration: An Application to Groundwater Heat Transport", "abstract": "Machine learning methods often struggle with real-world applications in science and engineering due to an insufficient amount or quality of training data. In this work, the example of subsurface porous media flow is considered; this corresponds to advection-diffusion processes under heterogeneous flow conditions, i.e., for spatially varying material parameters, and a large number of spatially distributed source terms. This challenge comes at high computing costs for classical simulation methods due to the required high spatio-temporal resolution and large domains. Machine learning-based surrogate models seem to offer a computationally efficient alternative. However, faced with real-world data-limitations, purely data-driven approaches face difficulties in predicting the advection process, which is highly sensitive to input variations and involves long-range interactions. Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN) approach is introduced, that combines a lightweight numerical surrogate for the global transport process with convolutional neural networks (CNNs) for the local processes. With the LGCNN, we model a city-wide subsurface temperature field, involving a heterogeneous groundwater flow field and one hundred groundwater heat pump injection points forming interacting heat plumes over long distances. In order to first systematically analyze the method, random subsurface input fields are employed. Then, the model is trained on a few cut-outs from a real-world subsurface map of the Munich region in Germany. Our model scales to larger cut-outs without retraining by accounting for the global effects with numerical physics models. All datasets, our code, and trained models are published for reproducibility.", "tldr": "We propose modeling complex scenarios like groundwater flow simulations around dozens of heat pumps with a hybrid, physics-inspired, staged CNN-based approach \"Local Global CNN\", that is trained on just a few data points.", "keywords": ["UNet", "CNN", "physics-aware", "scalability", "real-world data", "few-shot learning", "domain transfer", "surrogate modeling", "heat transport", "groundwater flow", "heat pumps"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56be402a9f47861b757dc164a3f2ee7cc5d7ffb6.pdf", "supplementary_material": "/attachment/6286b0e06da1e01da8db172a5ea3dfe36691bfd7.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes LGCNN, a hybrid approach for predicting subsurface temperature fields from groundwater heat pumps. The key idea is to split the problem into three steps: use CNNs to predict velocity from pressure/permeability (local physics), compute streamlines numerically (global transport), then use another CNN to predict temperatures. They test on synthetic fields and some real Munich data, claiming this reduces data requirements and scales to larger domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The operator splitting concept is sensible - recognising that CNNs struggle with long-range advective transport and handling it with a numerical surrogate is the right intuition. The physics decomposition is clean."}, "weaknesses": {"value": "1. Training on 3 datapoints (literally 1 train, 1 val, 1 test) and claiming \"strongly reduced data requirements\" doesn't pass the smell test. Even with the 101-sample comparison, there's no proper cross-validation or statistical validation. \n\n2. You cite this work, Pelzer et al. (2024), which describes essentially the same two-stage idea (numerical surrogate for global transport + CNN for local processes). What exactly is new here beyond that paper? The three-step breakdown feels like an implementation detail rather than a conceptual advance."}, "questions": {"value": "- Could you expand on the novelty of your work in relation to Pelzer et al?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Cob9B41vSm", "forum": "c2dqbma1pj", "replyto": "c2dqbma1pj", "signatures": ["ICLR.cc/2026/Conference/Submission20731/Reviewer_jMn3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20731/Reviewer_jMn3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298820154, "cdate": 1761298820154, "tmdate": 1762934149970, "mdate": 1762934149970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Local-Global CNN (LGCNN), which is a deep learning model for subsurface temperature and velocity fields for Ground Water Heat Pumps. LGCNN combines CNNs and a numerical solver for the initial value problem to leverage the advantages of both. More specifically, a CNN (U-Net) predicts velocity values from local relations, the numeric solver computes the streamlines, and a second CNN computes the temperatures. The proposed method is compared against two U-Net variations (purely data-driven), including one tailored for large domains (DDUNet). The datasets applied in the evaluation are based on real and synthetic permeability fields and include synthetic smaller and larger scale datasets and a larger dataset with measurements from Munich. Results using multiple metrics (MAE, MSE, Maximum Absolute Error, etc.) show that the proposed model can scale to large datasets and achieves lower error rates than the data-driven alternatives."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is motivated by a real and relevant application\n\n- The idea of combining ML and classical methods to speed up simulations is promising\n\n- The experiments are based on both real and synthetic datasets"}, "weaknesses": {"value": "- The contributions of the paper to ML are not clear: the paper focuses on a narrow application and proposes a simple solution integrating CNNs and a numerical solver. It is not clear how the proposed approach will lead to advances in ML for simulations. Moreover, the paper doesn’t contextualize the work within the large related literature on simulations recently published at ICLR, NeurIPS, ICML, AAAI, IJCAI, etc. A quick look at the citations shows that the paper is much more focused on the application domain, which is pretty narrow compared to other problems that have attracted the interest of the ML community, such as weather forecasting. \n\n\n- The writing of the paper can be greatly improved: the paper is not well-written. The experiments are divided into two different sections, and the reader has to move back and forth to compare the results. The captions of the figures are not helpful in describing the take-home message from the results. Moreover, the problem could be made more abstract for an ML audience, since most readers will not have expertise on groundwater heat transport. Details about the application domain could be moved to the experiments, and maybe a motivation section. Potential impact on other related applications should be discussed in more detail.\n\n- The baselines considered in the paper seem weak: the baselines considered in the paper are just U-Nets. These might be the approaches considered by practitioners, but in the ML community, there are many alternatives that are well-known, such as Neural Operators, Graph Neural Networks, Physics-Informed Neural Networks, Universal Differential Equations, etc. Some of these approaches were only discussed in text, but that is not sufficient."}, "questions": {"value": "1) What are the fundamental contributions of this paper to ML beyond the specific application considered?\n\n2) Why other ML approaches for physics-based simulations (see Weaknesses) have not been considered in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QbAddJ6Tnk", "forum": "c2dqbma1pj", "replyto": "c2dqbma1pj", "signatures": ["ICLR.cc/2026/Conference/Submission20731/Reviewer_T6Sk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20731/Reviewer_T6Sk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937204842, "cdate": 1761937204842, "tmdate": 1762934149021, "mdate": 1762934149021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose LGCNN, a physics-inspired CNN hybrid architecture for the estimation of groundwater heat transport. It seems to be a non-trivial application due to a multitude of environmental factors and long-range dependencies. Evaluations show LGCNN out-performs other tested methods on the chosen dataset, both on training and test set. Authors claim their method also scales to larger regions: analysis is done by training on smaller regions of the dataset and validating on larger ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Results sound promising: the model seems to learn and generalize well from few training samples. \n- Models and data will be available after publication for reference.\n- The architecture and general idea of physical formulas inside/between CNN seem interesting"}, "weaknesses": {"value": "- Comparisons are lacking: as far as I can tell the proposed LGCNN is never compared directly to other state of the art solutions or even non-deep learning solutions, this makes it difficult to judge how useful LGCNN is in practice. \n- Also, the novelty is not entirely clear: physics informed neural networks are known and applied to a variety of domains, what makes LGCNN special in this regard? Could LGCNN be compared to other, established, physics informed neural network architectures? Comparisons seem to focus on variants of LGCNN with different backbones and training hyperparameters but not different architectures. \n- Figures are not explained well. For example, Figure 1: what are I, P, T, s and s_o? There are some formulas in the caption, but these should also be explained in more detail in the text as this is the main contribution/idea of the paper and should thus be clear and understandable even for people outside the domain. \n- The visual assessment might make sense, but it should also be explained better what is shown, for example Figure 2: the LGCNN seems to have a white border around the data, is this an effect of the model or the presentation? Also, the input data and result should be explained more."}, "questions": {"value": "- What is the size of the model?\n- The dataset seems relatively small, if the model is large, how is overfitting addressed? Just via weight decay? It’s not addressed in the main part of the paper at all."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wq6rjtmMXb", "forum": "c2dqbma1pj", "replyto": "c2dqbma1pj", "signatures": ["ICLR.cc/2026/Conference/Submission20731/Reviewer_Nyb5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20731/Reviewer_Nyb5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007521264, "cdate": 1762007521264, "tmdate": 1762934148418, "mdate": 1762934148418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Local–Global CNN (LGCNN) framework to model advection-dominated subsurface heat transport under extreme data scarcity. The method decomposes the problem into: (1) a CNN that predicts steady-state flow fields, (2) a numerical solver that integrates streamlines to capture global advection, and (3) another CNN that predicts temperature fields conditioned on local features and streamline embeddings. This hybrid design leverages physical priors to achieve strong accuracy with as few as one or two training simulations and scales to larger spatial domains without retraining. Experiments on synthetic and real groundwater data show that LGCNN matches or surpasses standard UNet-based surrogates trained with significantly more data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Well-motivated physics factorization. The advection-dominated regime (Pe ≫ 1) motivates separating global transport from local effects, with streamlines serving as an informative, compact conditioning signal. This reduces the burden on CNN receptive fields and training data. \n\n2) Data efficiency under scarcity. Matching UNet/DDUNet trained on ~73–101 datapoints using only 1–3 simulations for LGCNN in critical steps is impressive and practically important for scientific settings with expensive labels. \n\n3) Robust scaling to larger domains. The method scales to 4× larger domains (synthetic) and extended domains (real Munich maps) without retraining, owing to the global step handling long-range transport and patch-based training for local steps.\n\n4) Careful ablations. The paper tests training sequence (simulated vs predicted v for Step 3), zero-padding, data partitioning, and the role of central vs offset streamlines (value fading vs binary masks), giving insight into what makes the pipeline work.\n\n5) Real-world transfer. Demonstrates applicability on measured permeability maps with many interacting heat sources—beyond pairwise/isolated pump scenarios common in prior work."}, "weaknesses": {"value": "1) Pipeline error propagation & calibration. The full pipeline’s error notably increases vs. Step-3-isolated results (using simulated v). There’s limited analysis of where velocity errors matter most (e.g., bifurcations), how sensitive temperature predictions are to streamline integration tolerances, or whether outputs are calibrated (e.g., reliability vs PAT/SSIM). A quantitative uncertainty or sensitivity analysis is missing. \n\n2) Global surrogate design choices. The IVP solver and 2D raster embedding (with linear fading) are somewhat ad-hoc; accuracy vs. step size/tolerance, integrator choice (Radau vs RK4), and the width/number of offset streamlines are not systematically optimized or analyzed across regimes. \n\n3) Comparative breadth. While baselines include UNet, DDUNet, and brief notes on PINNs/FNOs, there’s no head-to-head with global receptive field architectures like Swin-UNet variants, dilated/atrous stacks, or modern neural operator variants designed for multi-source fields (e.g., localized operator kernels) under equal data budgets; the FNO discussion is qualitative and hardware-limited. \n\n4) Physical scope limitations. The method is steady-state, 2D, and one-way coupled (no thermal feedback on flow). The authors acknowledge this, but it limits immediate deployment where transient or 3D effects and thermo-hydraulic coupling matter. Benchmarks on even coarse 3D or short-horizon transients would strengthen claims of generality. \n\n5) Metrics vs application risk. PAT at 0.1°C and SSIM are helpful, but siting/interaction decisions could depend on tail behavior (e.g., maximum plume extents, constraint violations). More task-level metrics (e.g., false-negative plume overlap beyond regulation thresholds) and cost/benefit versus full simulators would improve practical relevance."}, "questions": {"value": "1) Uncertainty & sensitivity. Can you provide calibrated uncertainty for T (e.g., via ensemble of streamline perturbations or dropout), and a sensitivity study for (a) streamline integrator step/tolerance, (b) number and offset of auxiliary streamlines, and (c) pressure-gradient mis-specification? This would contextualize PAT under real deployment noise.\n\n2) Where do velocity errors hurt? Please quantify how local v errors map to downstream temperature errors—e.g., plot error vs. distance along principal streamlines; analyze bifurcation regions where small k variations flip paths. Could a corrective local refinement near bifurcations reduce the pipeline gap?\n\n3) Alternative global modules. Did you try (or could you include) comparisons with (i) dilated CNN backbones, (ii) attention-augmented UNets, or (iii) localized neural-operator kernels trained under the same 3-datapoint budget? Even a scaled-down study would position LGCNN more broadly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BhCV18QcFT", "forum": "c2dqbma1pj", "replyto": "c2dqbma1pj", "signatures": ["ICLR.cc/2026/Conference/Submission20731/Reviewer_zSSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20731/Reviewer_zSSc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237898337, "cdate": 1762237898337, "tmdate": 1762934146676, "mdate": 1762934146676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}