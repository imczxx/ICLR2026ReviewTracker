{"id": "aWiC2e2wjK", "number": 21145, "cdate": 1758314245317, "mdate": 1759896939823, "content": {"title": "GRIPedge: Heterophily-aware graph learning via attentional feature-spectral neighbour propagation in dense graphs", "abstract": "Node classification in heterophilic graphs remains a challenging task, as connected\nnodes often belong to different classes and exhibit heterogeneous features. The assumption\nof homophily, which is typical in GNNs, encounters problems such as\noversmoothing and reduced separability and leads to low performance on dense\nheterophilic benchmarks such as Squirrel and Chameleon. We therefore propose\na unified framework that improves feature representation, structural learning and\nspectral aggregation. Our approach combines attention-based mechanisms to integrate\nlocal and global neighborhood information, spectral modulation to capture\noscillatory node–edge patterns and edge augmentation inspired by structure\nlearning to refine graph connectivity. Extensive experiments demonstrate that our\nmodel consistently offers robust and discriminative node embeddings and outperforms\nstate-of-the-art methods on the task of node classification in dense graphs.", "tldr": "", "keywords": ["graph learning", "node classification", "heterophilic dense graphs"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dfc66d999d20b0c9c2e83ddf26d78c901d1c5084.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GRIPedge, a heterophily-aware graph learning framework that integrates self-attention, spectral modulation, and edge augmentation for node classification on dense graphs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method seems effective on the selected datasets.\n2. It elegantly combines local feature learning, spectral filtering, and edge refinement, giving it conceptual and architectural coherence."}, "weaknesses": {"value": "1. Novelty is limited. The proposed challenges are old. For example, [1] also utilizes the attention mechanism to combine local and global information, thereby addressing the heterophily problem. Besides, limiting the attention to the edges is similar to [1] and [2].\n\n2. This paper claims the scalability of dense graphs. However, the complexity seems to increase linearly with respect to the number of edges. Thus, I’m confused why it’s fast on dense graphs. Besides, from my understanding of the proposed method, it runs slower than traditional Transformer since it inputs nodes one by one. The author didn’t provide the code for me to test the running time (this also raises concerns about the reproducibility).\n\n3. While the spectral modulation is intuitively motivated, the paper does not provide a rigorous theoretical justification for how modulation improves heterophilic signal separation.\n\n4. Comparisons to newer high-pass or adaptive spectral models are limited in depth.\n\n[1] Buterez, David, et al. \"An end-to-end attention-based approach for learning on graphs.\" Nature Communications 16.1 (2025): 5244.\n[2] M. S. Hussain, M. J. Zaki, and D. Subramanian, “Global self-attention as a replacement for graph convolution,” in Proceedings of the 28th ACMSIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 655–665."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mB22MrjTRC", "forum": "aWiC2e2wjK", "replyto": "aWiC2e2wjK", "signatures": ["ICLR.cc/2026/Conference/Submission21145/Reviewer_HrHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21145/Reviewer_HrHJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552111803, "cdate": 1761552111803, "tmdate": 1762941502272, "mdate": 1762941502272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GRIP/GRIPedge for node classification in heterogeneous graphs. It first performs attention-optimized node representations on one-hop neighbors, then introduces a spectral neighbor propagation strategy into GCN propagation. This strategy stacks p-hop and q-hop information and fuses them via cross-attention. GRIPedge further employs KDGA-style structure learning for edge enhancement, achieving more robust and discriminative node embeddings on dense heterogeneous graphs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. GRIPedge combines multiple techniques including attention mechanisms, spectral aggregation, and local-global fusion. Its design is highly intuitive, forming a unique framework that demonstrates innovation in hybrid architectures.\n2. The feature-spectral neighbor propagation mechanism proposed by GRIPedge appears to be a novel node representation learning approach. This method not only considers node feature similarity but also captures high-frequency oscillation patterns between nodes through spectral modulation, thereby better distinguishing nodes across different categories.\n3. GRIPedge employs a multi-hop cross-attention mechanism to fuse local and global information, yielding node embeddings with enhanced discriminative power and robustness. This mechanism theoretically captures short-, medium-, and long-range dependencies between nodes effectively.\n4. In experiments, The GRIPedge model achieves higher performance than state-of-the-art methods on dense heterophilic datasets, aligning with its core claims."}, "weaknesses": {"value": "1. The clarity of this manuscript is generally average. Spectral aggregation appears to be its core innovation, but the technical and theoretical descriptions are insufficient. Without prior knowledge of the underlying principles, readers will struggle to grasp the paper's ideas and methodological details. Furthermore, I believe Section 4.2 should not appear in Chapter 4 but rather within the Methods section. It remains unclear how the spectral aggregation technique employed here differs from previous approaches.\n2. Experimental results indicate GRIPedge shows no significant advantage, particularly compared to NLSFs. While GRIPedge performs marginally better on two heterophilic datasets, NLSFs outperform GRIPedge on all three homophilic datasets (see Table 2 in the manuscript).\n3. The paper lacks any discussion of computational costs.\n4. The paper currently does not provide publicly available code. While the authors commit to releasing it upon acceptance, this undoubtedly undermines the paper's credibility."}, "questions": {"value": "For different graphs, the choice of p/q remains the same. Is this reasonable?\nSee Weekness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qKylo0Jf4i", "forum": "aWiC2e2wjK", "replyto": "aWiC2e2wjK", "signatures": ["ICLR.cc/2026/Conference/Submission21145/Reviewer_chJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21145/Reviewer_chJb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956421546, "cdate": 1761956421546, "tmdate": 1762941501098, "mdate": 1762941501098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GRIP/GRIPedge, a heterophily-aware node classification framework that combines structual and feature aggregation. The architecture is well designed with focusing on many important problems on graph heterophily."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear problem framing (heterophily) and a clean modular pipeline (attention + spectral modulation + structure augmentation).\n\n2. Empirical gains on dense heterophilic benchmarks; ablations indicate each module contributes."}, "weaknesses": {"value": "1. Datasets are a little toy and classic for heterophily domain. The heterophily evaluation is effectively limited to Chameleon/Squirrel/Actor (plus classic homophily sets). The heterophily literature now includes larger, more diverse benchmarks (e.g., ogbn-papers100M heterophily slices, Pokec variants, Penn94/ArXiv-year, WebKB-cleaned/Telegram/Roman-empirical heterophily sets, etc.). I would expect ≥3 additional, large heterophilic datasets and head-to-head comparisons to the latest baselines under unified splits to substantiate generality.\n\n2. Incremental architecture with unclear framing. The method largely assembles known components (local/global attention, spectral filters, edge augmentation). For a community already saturated with architecture variants, the paper needs a crisper framework figure and concise conceptual narrative that articulate why this specific composition is necessary and what is fundamentally new (beyond combining parts).\n\n3. The paper aggregates results from multiple prior sources with different splits/protocols. It is then unclear which baselines were re-run, which were copied, and under what hyper-parameters. This undercuts fairness of the comparison. Please create an appendix section to clearify ."}, "questions": {"value": "Listed in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZuS4C22GTH", "forum": "aWiC2e2wjK", "replyto": "aWiC2e2wjK", "signatures": ["ICLR.cc/2026/Conference/Submission21145/Reviewer_ojkr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21145/Reviewer_ojkr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046867012, "cdate": 1762046867012, "tmdate": 1762941499328, "mdate": 1762941499328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified framework, GRIP, for node classification under heterophily, an area where homophily-biased GNNs struggle due to oversmoothing and reduced separability. GRIP preprocesses features with a one-hop self-attention block to create context-aware local embeddings. It then performs feature-spectral neighbor propagation, fusing a feature-based edge importance term with a spectral “oscillation” term derived from Laplacian eigenpairs via a smooth modulation that emphasizes informative high-frequency differences and dampens noise. GRIP aggregates information over multi-hop paths using parallel p-hop and q-hop stacks, then cross-attends to fuse local and global signals adaptively. GRIPedge extends GRIP with a KDGA-style teacher-student structure learning stage to augment edges with a multi-view scorer, mixing the original and learned adjacencies over training. Empirically, GRIPedge outperforms strong baselines on dense heterophilic datasets like Chameleon and Squirrel, while being less competitive on sparse graphs and homophilic benchmarks. Ablations indicate each major design choice contributes nontrivially to performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper’s synthesis of three strands—local self‑attention, spectral modulation of edge importance, and KDGA‑style structure augmentation—is thoughtfully motivated for heterophily. The specific choice to modulate feature-based attention with a spectrally informed oscillation term is a neat conceptual bridge between feature‑driven and frequency‑domain views.\n\n- The method is evaluated across six canonical datasets with multiple splits and confidence intervals, and includes ablations isolating the contributions of attention, spectral modulation, and multi‑hop fusion. On dense heterophilic graphs, GRIPedge achieves state‑of‑the‑art or near‑SOTA performance, with credible robustness under edge/node sparsification scenarios.\n\n- The high-level narrative—why heterophily is hard, which families of methods exist, and how GRIPedge integrates them—is clear and well contextualized."}, "weaknesses": {"value": "- Fairness and comparability of baselines: Results are aggregated from multiple sources with mixed split protocols and reporting formats (“No data / split”, mixing 95% CI and standard deviations). This can skew comparisons. \n\n- Empirics. The authors did not provide theoretical analysis or gurantee on why GRIP can perform well on heterophic graphs. Also the empirical performance of GRIP is not good apart from the Chameleon and Squirrel, it performs poorly on homophilic graphs and is even outperformed by MLP on Actors\n\n- Methodological details missing/ambiguous: The form of ‎`f_\\theta` in the spectral term is not fully specified (parameterization, nonlinearity, normalization), and some equation references are inconsistent (e.g., COMBINE “Equation (2)” mislabels).\n\n- Computational considerations: Computing up to K=32 Laplacian eigenvectors can be expensive for larger graphs; multi‑hop stacks and cross‑attention add further overhead. The paper does not quantify runtime or memory footprint.\n\n- Generalization beyond dense graphs: Performance degrades notably on Actor and homophilic datasets; the method seems tailored to dense heterophily.\n\n- Structure learning stage evaluation protocol: Selecting the best of teacher or student for each split can inflate reported performance relative to single‑model baselines."}, "questions": {"value": "- What is the exact parameterization of ‎$f_\\theta$? Is it a scalar MLP, a gated mechanism, or a bounded mapping designed to stabilize gradients? How sensitive is performance to its architecture and initialization?\n\n- Frequency band sensitivity: Have you analyzed which eigenvalue bands contribute most? For instance, does performance peak at certain K, and does a learned polynomial (à la GPR‑GNN) approximate your modulation?\n\n- Scalability: What are runtimes and GPU memory usage compared to GCN, GAT, NLSFs, and Specformer on PubMed or larger OGB datasets? Can GRIPedge be made OGB‑scale via approximations?\n\n- Ablations under unified settings: Could you provide ablations for each component (self‑attention, spectral modulation, cross‑attention, KDGA augmentation) under identical splits and training budgets across all three heterophilic datasets?\n\n- Why is the performance so bad on homophilic graphs? Do you have any guesses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FyU1SmDpP7", "forum": "aWiC2e2wjK", "replyto": "aWiC2e2wjK", "signatures": ["ICLR.cc/2026/Conference/Submission21145/Reviewer_Gmgx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21145/Reviewer_Gmgx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144309560, "cdate": 1762144309560, "tmdate": 1762945701854, "mdate": 1762945701854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}