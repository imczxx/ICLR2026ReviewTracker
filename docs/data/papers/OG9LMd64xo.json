{"id": "OG9LMd64xo", "number": 13104, "cdate": 1758213622304, "mdate": 1763720251183, "content": {"title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction", "abstract": "Can large language models (LLMs) admit their mistakes when they should know better? In this work, we study when and why LLMs choose to retract, i.e., spontaneously and immediately acknowledge their errors. Using model-specific testbeds, we find that while LLMs are capable of retraction, they do so only rarely, even when they can recognize their mistakes when asked in a separate interaction. We identify a reliable predictor of retraction: the model’s \\emph{momentary belief}, as measured by a probe on its internal states that is trained to predict correctness on external datasets unrelated to retraction. A model retracts only when it \"believes\" its answers to be incorrect \\emph{during generation}; these beliefs frequently diverge from models' parametric knowledge as measured by factoid questions. Steering experiments further demonstrate that model belief causally drives retraction. In particular, when the model believes its answer to be incorrect, this not only encourages the model to attempt further verification, but also alters attention dynamics. Finally, we show that supervised fine-tuning improves retraction performance by helping the model learn more accurate internal belief.", "tldr": "", "keywords": ["Large Language Models", "Internal Representations", "Factual QA"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81f28c24e3ce758ce76eac42867de0857bff6163.pdf", "supplementary_material": "/attachment/7aea52c07fb05a1e51fbffd93382957e92be72ff.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies when and why large language models (LLMs) retract their mistakes—defined as spontaneous acknowledgments of incorrect answers. The authors construct model-specific continuation datasets from WIKIDATA and CELEBRITY benchmarks, where models generate potentially wrong answers and are then observed to see if they retract them without external prompting. Overall, the paper provides a principled, mechanistic understanding of why models sometimes refuse to retract even when they “know better.”"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Positions “retraction” as a measurable, meaningful behavioral metric for model reliability.\n\n- Uses steering and patching methods to demonstrate directional control over behavior.\n\n- Provides concrete evidence that attention value vectors, not just weights, mediate belief propagation.\n\n- Evaluates multiple model families, increasing robustness.\n\n- Links findings to SFT improvements, suggesting paths for aligning model introspection with truthfulness.\n\n- Extensive appendices, code availability, and transparent methodology."}, "weaknesses": {"value": "- Focuses on factual QA; results may differ for open-ended or reasoning-heavy tasks (e.g., math or multi-step reasoning).\n\n- While linear probes capture useful signals, belief may conflate confidence, calibration, and factual recall.\n\n- Although consistent, evaluation could benefit from human verification for robustness.\n\n- Steering effects may differ across architectures or prompt styles.\n\n- Retracting only ~25% of wrong answers initially limits downstream applicability, even if mechanisms are well-understood."}, "questions": {"value": "- How robust are belief-based probes to changes in prompt style or model sampling temperature?\n\n- Could non-linear or attention-based probes (e.g., small MLPs) capture belief signals more accurately?\n\n- How does belief steering generalize to reasoning or creative tasks, where “wrong” answers are less well-defined?\n\n- Does the causal influence of belief persist in RLHF-tuned models, which are already calibrated for truthfulness?\n\n- Have you examined whether cross-model belief vectors (e.g., between Qwen and Llama) share a universal direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The datasets (WIKIDATA, CELEBRITY, UTQA) are public and non-sensitive. The study focuses purely on model behavior and interpretability. There are no human subjects or privacy issues."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nmM9GAUojI", "forum": "OG9LMd64xo", "replyto": "OG9LMd64xo", "signatures": ["ICLR.cc/2026/Conference/Submission13104/Reviewer_VBRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13104/Reviewer_VBRM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761086402011, "cdate": 1761086402011, "tmdate": 1762923831539, "mdate": 1762923831539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates when and why LLMs spontaneously retract their own incorrect answers. For systematic evaluation, the authors propose continuation testbeds by prefilling the answers (e.g., Hillary Clinton [Model generation continues from here...]). The main observation based on this testbed is LLM can retract, but infrequently. By using probing and activation steering, it shows that the LLM’s internal belief reflects when the model internally “believes” it is wrong, it is more likely to retract. Besides, it shows that supervised fine-tuning improves retraction by aligning internal beliefs with factual correctness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new framework for studying LLM reliability as the testbeds for retraction behaviour. \n\n2. This paper proposed a warning for SFT. The connection between belief and retraction holds even after supervised fine-tuning, showing that improved belief calibration enhances factual alignment and transparency."}, "weaknesses": {"value": "1. For table 2, I am curious if the phenomenon is consistent for larger LLM (e.g., meta-llama/Llama-3.1-70B-Instruct).\n\n2. For the training procedure of the linear prob, I think the training dataset (UTQA dataset) is not a continuation dataset. The linear prob trained on this dataset aims to reflect factual correctness but not the retract behaviour, right? The interesting observation is that the linear prob is  highly predictive of whether the model will retract its answer. Any additional and deep explanation for this phenomenon will be very interesting.\n\n3. For activation steering, it shows that changing internal belief alone is enough to change whether the model admits a mistake. I believe some ablation studies should be added to support this claim. For example, we should first split the testbed into two groups by the criterion as if the LLM can do retraction originally, and then study how negative/positive belief steering can change the retraction rate for these two different groups. \n\n4. For supervised fine-tuning, figure 4 shows that the probing of factual correctness can achieve significantly higher accuracies for middle layers (12-24). It is very interesting. Any explanation for this phenomenon?\n\n5. There are several interesting observations in this paper, but most of them are quite facial. Besides, the relation between factual correctness and retraction behaviour is still not explained clearly. \n\n6. The novelty is quite limited. The methods used for analysis, such as linear probing, steering, and patching are not novel."}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jQzytlMXjY", "forum": "OG9LMd64xo", "replyto": "OG9LMd64xo", "signatures": ["ICLR.cc/2026/Conference/Submission13104/Reviewer_rHvH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13104/Reviewer_rHvH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761437340902, "cdate": 1761437340902, "tmdate": 1762923831263, "mdate": 1762923831263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies spontaneous retraction—cases where an LLM, after giving an answer, immediately acknowledges it is wrong without being prompted. The authors build model-specific “continuation” testbeds from two knowledge tasks (WIKIDATA; CELEBRITY) and evaluate three 7–8B instruction-tuned models. They find 1. models can retract but do so rarely (low recall), even when separate verification questions show the model “knows” the answer is wrong; 2. a linear probe trained on external true/false datasets provides a belief signal that predicts retraction much better than correctness; 3. activation steering along belief directions causally modulates retraction (negative steering → frequent retraction; positive steering → almost none); and 4. SFT improves in-distribution retraction and aligns internal belief more closely with factual correctness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, focused problem: Spontaneous retraction is practically relevant and distinct from multi-turn self-correction.\n\n- Neat empirical finding: Linear probes of hidden states correlate strongly with retraction but less with ground-truth correctness, clarifying what such probes actually capture.\n\n- Causality evidence: Activation steering gives credible leverage that belief directions are not merely correlates but drivers of retraction behavior.\n\n- Mechanistic analysis: Patching experiments suggest attention value vectors (not only attention weights) are a primary pathway by which belief influences retraction.\n\n- Training connection: Showing that SFT improves retraction by aligning internal belief bridges interpretability results with standard training practice."}, "weaknesses": {"value": "- **External validity / scope** All core experiments use three small instruction models and two knowledge-centric datasets. It’s unclear if belief→retraction generalizes to larger or reasoning-style models, to non-factoid tasks, or to tool-augmented settings.\n\n- **Evaluation dependence on an LLM judge** Retraction is judged automatically (Llama-3.3-70B). Although convenient, relying on a single judge risks systematic bias and false positives/negatives (e.g., hedged text vs true retraction). Human audits or multi-judge agreement would strengthen claims.\n\n- **Prompt / decoding sensitivity** The phenomenon may hinge on stopping behavior and continuation prompts (the “is-appended” setting materially changes results). More thorough robustness to decoding parameters, prompt templates, and stop-criteria would help.\n\n- **Probe training / generalization** Belief probes trained on UTQA then applied OOD to continuation data work well for retraction but less for correctness. This invites concerns about dataset-specific artifacts (style, length, lexical cues). More ablations on token position, layer selection, and context length would clarify.\n\n- **Steering hyperparameters** Steering strength/layer ranges are manually searched. The narrative would benefit from systematic sweeps with confidence intervals and controls for side effects (fluency, length, off-topic drift).\n\n- **Statistical reporting** Many results are presented as point metrics. Please include CIs, random-seed variance, and significance tests for retraction precision/recall and AUROC curves."}, "questions": {"value": "- How consistent are retraction labels across multiple judges or human raters? Any inter-annotator stats?\n\n- How does temperature, top-p, and max-tokens affect retraction rates independently of belief steering?\n\n- Can you quantify textual markers of retraction (e.g., “I was wrong,” “Correction: …”) and report precision/recall for each subtype?\n\n- Do belief directions found on one model family transfer to another? Any cross-model steering experiments?\n\n- Beyond attention values, did you test MLP value patching or logit lens analyses to localize belief-to-retraction pathways?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xyy0w76d1q", "forum": "OG9LMd64xo", "replyto": "OG9LMd64xo", "signatures": ["ICLR.cc/2026/Conference/Submission13104/Reviewer_vuwy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13104/Reviewer_vuwy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957899643, "cdate": 1761957899643, "tmdate": 1762923830950, "mdate": 1762923830950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reliable predictor of retraction: the model’s momentary belief, as measured by a probe on its internal states that is trained to predict correctness on external datasets unrelated to retraction. Supervised fine-tuning shows to improve retraction performance by helping a model learn more accurate internal belief."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Regarding originality, momentary belief appears to be an interesting concept. See questions/weaknesses below so I can better judge the novelty.\n\n2. I like the supervised fine-tuning part, which connects the idea of LLMs' internal belief."}, "weaknesses": {"value": "1. Soundness needs to be improved by running experiments on larger models. The largest model studied is 8B, so I am not sure how the findings generalize to larger models. It is necessary to discuss the similarity and difference between large and smaller models in terms of your momentary belief concept.\n\n2. I am not sure the utility of momentary belief. What are the final metrics of measuring the utility of momentary belief? The clarity needs to be improved.\n\n3. I am not sure the efficiency of computing momentary belief. If your concept of momentary belief is indeed helpful, it is necessary to show the comparison on performance, computation costs, and inference latency across a wide range of baselines.\n\n4. Writing needs to be improved in order to reach a publishable state. For example, I am not sure how your definition of \"retraction\" differs from \"backtracking\" as the question asked below. This is my concern, because I use it to judge the novelty of this paper."}, "questions": {"value": "1. In your abstract, can you explain \"Steering experiments further demonstrate that model belief causally drives retraction\"? You used the word \"causally\" throughout your paper, but I still find a hard time understanding it.\n\n2. Since you define retraction in Section 3.1, is it the same as \"backtracking\" that is more commonly used in other literature such as [1]?\n\n[1] \"To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wpm8lAQvA5", "forum": "OG9LMd64xo", "replyto": "OG9LMd64xo", "signatures": ["ICLR.cc/2026/Conference/Submission13104/Reviewer_eA1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13104/Reviewer_eA1T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998236252, "cdate": 1761998236252, "tmdate": 1762923830506, "mdate": 1762923830506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}