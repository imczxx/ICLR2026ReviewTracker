{"id": "vqaz8rlRUk", "number": 21136, "cdate": 1758314115783, "mdate": 1759896940225, "content": {"title": "MetaCluster: Enabling Deep Compression of Kolmogorov-Arnold Network", "abstract": "Kolmogorov-Arnold Networks (KANs) replace scalar weights with per-edge vectors of basis coefficients, thereby boosting expressivity and accuracy but at the same time resulting in a multiplicative increase in parameters and memory. We propose MetaCluster, a framework that makes KANs highly compressible without sacrificing accuracy. Specifically, a lightweight meta‑learner, trained jointly with the KAN, is used to map low‑dimensional embedding to coefficient vectors, shaping them to lie on a low‑dimensional manifold that is amenable to clustering. We then run K-means in coefficient space and replace per‑edge vectors with shared centroids.  Afterwards, the meta‑learner can be discarded, and a brief fine‑tuning of the centroid codebook recovers any residual accuracy loss. The resulting model stores only a small codebook and per-edge indices, exploiting the vector nature of KAN parameters to amortize storage across multiple coefficients. On MNIST, CIFAR-10, and CIFAR-100, across standard KANs and ConvKANs using multiple basis functions, MetaCluster achieves a reduction of up to $80\\times$ in parameter storage, with no loss in accuracy. Code will be released upon publication.", "tldr": "", "keywords": ["KAN", "Weight Sharing", "Compression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/221982e8aad939b3f164834fc4fcad65436aaa8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the memory overhead of Kolmogorov–Arnold Networks (KANs), where each edge carries a vector of basis coefficients rather than a scalar. It proposes MetaCluster, a three-stage pipeline: (1) train a lightweight meta-learner that maps low-dimensional embeddings to coefficient vectors so that coefficients lie on a clusterable low-dimensional manifold; (2) perform K-means in coefficient space and replace per-edge vectors with shared centroids plus indices; (3) discard the meta-learner and briefly fine-tune the centroid codebook. On MNIST, CIFAR-10/100, and both fully-connected KANs and ConvKANs with several basis families, MetaCluster reports up to 80× parameter-storage reduction without accuracy loss relative to the uncompressed KAN."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Leveraging the vectorized per-edge structure of KANs makes codebook amortization particularly effective compared to scalar-weight MLPs. The method is simple to integrate: the meta-learner is used only during training and removed at inference.\n- Clear articulation of why naïve weight sharing fails on KANs  and how manifold shaping mitigates this.\n- Solid empirical coverage for small/medium scale: Results span FC-KAN and ConvKAN, with B-spline/RBF/Gram bases on MNIST/CIFAR-10/100."}, "weaknesses": {"value": "- Experiments are limited to MNIST/CIFAR and relatively small KAN/ConvKANs; there is no large-scale vision or transformer-style model demonstration.\n- The paper emphasizes zero inference overhead from removing the meta-learner, but provides no wall-clock or FLOPs comparison of training cost vs. baselines for the meta-learner + clustering + fine-tuning stages.\n- While related work mentions Hessian-weighted K-means and differentiable K-means (DKM), the paper does not evaluate these variants or other clustering families (e.g., hierarchical or agglomerative).\n- The paper uses a single global K per model family (e.g., FC-KAN K=16; ConvKAN K=256 per hyperparameter tables) and does not explore layer-wise varying K."}, "questions": {"value": "1. Recent post-training, clustering-based methods (e.g., model folding[1], IFM[2] ) share the theme of parameter sharing/tying and low-dimensional structure. How does MetaCluster compare conceptually and empirically? Would it be possible to perform post-training clustering on a KAN model trained without metaclustering?\n\n2. The authors state that quantization is complementary. Do you anticipate non-trivial accuracy loss when combining MetaCluster with 8-bit / 4-bit quantization of centroids and/or indices? Any preliminary data?\n\n[1] Wang, Dong, et al. \"Forget the data and fine-tuning! just fold the network to compress.\" arXiv preprint arXiv:2502.10216 (2025).\n[2] Chen, Yiting, Zhanpeng Zhou, and Junchi Yan. \"Going beyond neural network feature similarity: The network feature complexity and its interpretation using category theory.\" arXiv preprint arXiv:2310.06756 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qZRP4HKwYk", "forum": "vqaz8rlRUk", "replyto": "vqaz8rlRUk", "signatures": ["ICLR.cc/2026/Conference/Submission21136/Reviewer_VcBJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21136/Reviewer_VcBJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561107072, "cdate": 1761561107072, "tmdate": 1762941413182, "mdate": 1762941413182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MetaCluster, a compression framework for Kolmogorov–Arnold Networks (KANs). Although KANs have demonstrated stronger performance than MLPs, they require significantly more parameters. This motivates a compression strategy based on weight sharing, where parameters are clustered into a small codebook, and only compact indices are stored. However, applying standard clustering directly to KANs is not straightforward. To address this, the authors train a small meta-learner that maps each edge’s coefficient vector onto a low-dimensional manifold, after which the vectors are clustered using K-means. The per-edge coefficients are then replaced by shared centroids (plus indices), followed by brief centroid fine-tuning. MetaCluster achieves up to an 80× reduction in parameter storage with no loss in accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, and its motivation is clear. The main strength lies in the impressive experimental results, as demonstrated in Tables 1 and 2. Additionally, the authors provide thorough ablation studies to validate their design choices, as shown in Section 4.3."}, "weaknesses": {"value": "- The authors do not benchmark against non-KAN compression baselines. Given the extensive literature on model compression, it would be valuable to compare MetaCluster with common techniques (e.g., pruning, quantization, or weight sharing) applied to MLPs or CNNs. This would help clarify whether MetaCluster is state-of-the-art relative to general compression methods. If those methods are not easily extendable to KANs, a discussion explaining why would strengthen the paper.\n\n- The evaluation is conducted only on relatively simple datasets (MNIST and CIFAR). It remains unclear how MetaCluster performs on more challenging or large-scale datasets.\n\n- The meta-learner does induce a bit more of training complexity, since this adds engineering steps and hyperparameters (e.g., number of clusters, embedding dimnesions, etc) which can complicate the process. Can we also not jointly train but perhaps find a way to do post-training compression, i.e., learn a meta-learner afterwards? Perhaps decoupling these can ease the process a bit.\n\n- The ablation results indicate that performance is sensitive to the meta-embedding dimension: clustering becomes more difficult as the embedding dimension increases. This suggests that finding a suitable dimension may require tuning and makes the method less plug-and-play."}, "questions": {"value": "I have a few simple questions:\n\n- Why choose K-means instead of other potential alternatives? Perhaps there are other choices that could boost the performance of MetaCluster. \n\n- Does your compression ratio / memory account for codebook overhead + index storage + bit packing? \n\nI apologize in advance if these were already answered in the manuscript and I missed them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iBn9BPYqU3", "forum": "vqaz8rlRUk", "replyto": "vqaz8rlRUk", "signatures": ["ICLR.cc/2026/Conference/Submission21136/Reviewer_eTyr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21136/Reviewer_eTyr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940576896, "cdate": 1761940576896, "tmdate": 1762941411819, "mdate": 1762941411819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MetaCluster, a three-stage compression framework for Kolmogorov-Arnold Networks (KANs) that combines meta-learning with weight sharing. In the first stage, a small meta-learner maps low-dimensional embeddings into coefficient vectors, constraining them to lie on a manifold that is highly clusterable. Then, K-means clustering is applied to replace per-edge coefficient vectors with shared codebook centroids. Finally, the centroids are fine-tuned to recover accuracy. The authors report up to 80× reduction in parameter storage with minimal accuracy loss across multiple architectures and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly identifies KAN’s memory inefficiency and proposes a targeted solution.\n- This is the first successful application of weight sharing specifically designed for KANs.\n- The paper is well organized and easy to follow."}, "weaknesses": {"value": "- Complete absence of vector quantization literature. The proposed method is fundamentally vector quantization (VQ): mapping high-dimensional vectors to discrete codebook entries via clustering. However, the paper never mentions \"vector quantization\" and ignores relevant research.\n- Lack of comparison with established vector quantization methods. The paper employs standard K-means with Euclidean distance but provides no comparison against advanced VQ techniques. For example, Product Quantization [1], which decomposes vectors into subvectors quantized independently, could achieve superior compression ratios. The choice of Euclidean distance over alternatives (cosine similarity, learned metrics) is also unjustified—for coefficient vectors representing basis functions, cosine similarity might better preserve functional shape. Without these comparisons, we cannot assess whether the meta-learner genuinely adds value over simpler VQ baselines.\n- The paper motivates KAN compression with the references of KNN’s advantages in scientific tasks. However, all experiments are conducted on computer vision tasks (MNIST, CIFAR-10, CIFAR-100). This is largely different from the scientific tasks where KNN is explored. It would strengthen the paper to include evaluations on domains that better reflect the stated motivation, such as scientific or physical modeling tasks.\n\n[1] Jégou et al. (2010) proposed product quantization for efficient nearest neighbor search."}, "questions": {"value": "- Can you provide results on scientific computing tasks (equation modeling, PDE solving) where KANs have demonstrated their primary advantages, rather than only vision tasks?\n- How does your method compare against Product Quantization, which decomposes vectors into independently quantized subvectors and could achieve smaller codebook sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VTV7njIFPL", "forum": "vqaz8rlRUk", "replyto": "vqaz8rlRUk", "signatures": ["ICLR.cc/2026/Conference/Submission21136/Reviewer_PLUa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21136/Reviewer_PLUa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984435491, "cdate": 1761984435491, "tmdate": 1762941409961, "mdate": 1762941409961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a three stage weight sharing method for KAN. The first stage involves mapping low dimensional embeddings to per-edge coefficient vectors. The second stage involves k-means clustering to replace per-edge vectors with centroids. The final stage involves  finetuning centroid codebook to recover accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed weight sharing method greatly reduces the amount of trainable parameters in KAN."}, "weaknesses": {"value": "The proposed method crucially relies on k-means clustering to provide reasonable good centroids. However, k-means clustering assumes the data is spherically shaped, which may not be true in practice. Could the authors replace the K-means clustering by other clustering methods (e.g. gaussian mixture model) to illustrate the proposed method can be used together with different clustering algorithms?"}, "questions": {"value": "Could the authors theoretically quantify the accuracy loss from weight sharing, compared to vanilla KAN?\nWhat is the complexity of the proposed 3-stage approach and how does it compare to vanilla KAN?\nCould the authors report the standard error in experiments (table 1-3)? The standard error can potentially be obtained by using different train/validation splits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dxCaNeYUT6", "forum": "vqaz8rlRUk", "replyto": "vqaz8rlRUk", "signatures": ["ICLR.cc/2026/Conference/Submission21136/Reviewer_Rvza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21136/Reviewer_Rvza"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762461389691, "cdate": 1762461389691, "tmdate": 1762941408817, "mdate": 1762941408817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}