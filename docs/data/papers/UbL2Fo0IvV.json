{"id": "UbL2Fo0IvV", "number": 11173, "cdate": 1758192045094, "mdate": 1759897602817, "content": {"title": "A Study of Posterior Stability in Time-Series Latent Diffusion", "abstract": "Latent diffusion has achieved remarkable success in image generation, with high sampling efficiency. However, this framework might suffer from posterior collapse when applied to time series. In this work, we first show that latent diffusion with a collapsed posterior degenerates into a much weaker generative model: variational autoencoder (VAE). This finding highlights the significance of addressing the problem. We then introduce a principled method: dependency measures, which quantify the sensitivity of a recurrent decoder to input variables. Through this method, we confirm that posterior collapse seriously affects latent time-series diffusion on real time series. For example, the latent variable has an exponentially decreasing impact on the decoder over time. Building on our theoretical and empirical studies, we finally introduce a new framework: posterior-stable latent diffusion, which interprets the diffusion process as a type of variational inference. In this way, it eliminates the use of risky KL regularization and penalizes decoder insensitivity. Extensive experiments on multiple real time-series datasets show that our new framework is with a highly stable posterior and notably outperforms previous baselines in time series synthesis.", "tldr": "Conducted a solid analysis of posterior collapse in time-series latent diffusion, and presented a new framework that is free from the problem.", "keywords": ["Latent Diffusion", "Time Series", "Posterior Collapse"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f6d760f0cbbabe4feb79924c750769921055542.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates posterior collapse in time-series latent diffusion models. It first argues that, under a collapsed posterior, latent diffusion effectively degenerates into a weaker VAE-like formulation, motivating a careful re-examination of the learning objective. The authors propose dependency-based diagnostics to quantify decoder sensitivity to the latent variable versus past observations and report evidence of both collapse and a ``dependency illusion'' on real datasets. Building on this analysis, the paper introduces a posterior-stable latent diffusion framework that removes KL regularization by reinterpreting early diffusion steps as variational inference and adds a collapse-simulation loss to encourage decoder sensitivity. Experiments on multiple time-series datasets show consistent improvements over latent-diffusion baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Problem framing and clarity.** The paper clearly articulates an important and under-explored failure mode of latent diffusion for time series, identifies likely causes, and presents them in a well-structured manner.\n\n2. **Diagnostic contribution.** While I cannot fully assess the novelty of the proposed dependency measure, it appears useful and practically informative for characterizing posterior collapse and decoder reliance. The empirical analyses derived from this diagnostic are convincing.\n\n3. **Methodological direction.** The proposed loss regime, interpreting early diffusion steps as variational inference and adding a collapse-simulation term, is intriguing. Beyond the present setting, the idea may also be applicable to addressing the well-known over-smoothing tendencies in VAEs."}, "weaknesses": {"value": "1. **Related work coverage and baselines.** The coverage of prior work on time-series generative modeling is too narrow. Several state-of-the-art diffusion-based and alternative generative approaches for time series [1, 2, 3, 4] are missing.\n\n2. **Experimental setting.** The experimental section does not compare against these stronger baselines on the standard generation task, making it difficult to gauge relative progress. A comparison is expected to be in a broader context and in a standard manner in unconditional generative modeling.\n\n3. **Positioning w.r.t. KL-related remedies in images.** Prior work in images has also linked KL terms to posterior collapse and proposed mitigations such as vector-quantized autoencoders or very small KL weights [5, 6]. Empirically, latent spaces in these models exhibit non-Gaussian structure despite the nominal prior. The paper should better delineate what is fundamentally new here (beyond the sequence-specific pathologies) and why the proposed approach is preferable to these alternatives in the time-series setting.\n\n[1] On the constrained time-series generation problem\n\n[2] Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series.\n\n[3] A Non-Isotropic Time Series Diffusion Model with Moving Average Transitions.\n\n[4] Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs.\n\n[5] Taming transformers for high-resolution image synthesis\n\n[6] High-resolution image synthesis with latent diffusion models"}, "questions": {"value": "1. **Status of “diffusion as variational inference.”** Is the connection theoretically exact under stated assumptions (and if so, which), or primarily a heuristic that performs well in practice? Can you please specify the conditions under which objective functions align and where gaps remain ?\n\n2. **Sensitivity of dependency measures.** How robust are the proposed dependence metrics to decoder architecture and training choices? Could particular design decisions (e.g., capacity, regularization, normalization) spuriously inflate measured reliance on the latent variable? Ablations or controls would help.\n\n3. **Beyond time series: image latents.** Do the diagnostics detect collapse in image latent diffusion as well, or is the phenomenon predominantly temporal? A brief analysis, or a clear negative result, would contextualize the time-series specificity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xwL4tHcZen", "forum": "UbL2Fo0IvV", "replyto": "UbL2Fo0IvV", "signatures": ["ICLR.cc/2026/Conference/Submission11173/Reviewer_iFvJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11173/Reviewer_iFvJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291122338, "cdate": 1761291122338, "tmdate": 1762922330911, "mdate": 1762922330911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of posterior collapse in latent diffusion models when applied to time-series data, claiming that the autoregressive nature of time-series decoders makes them uniquely susceptible to this issue. The authors' key contributions include a theoretical argument that a fully collapsed posterior degenerates the powerful latent diffusion model into a much weaker Variational Autoencoder, and a novel diagnostic tool called \"dependency measures\" to empirically quantify this effect. Using this tool, the paper provides empirical evidence that standard latent diffusion does suffer from a vanishing latent variable impact on time-series data. To solve this, the authors propose a \"posterior-stable latent diffusion\" framework which removes the standard KL regularization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The introduction of \"dependency measures\"  is a principled and interesting contribution. Basing this metric on integrated gradients  provides a solid theoretical grounding for quantifying the type of decoder insensitivity (global vs. local) that leads to posterior collapse in sequence models.\n\n- The proposed solution is technically sound."}, "weaknesses": {"value": "- Insufficient Motivation from Time-Series Literature: The most significant concern is the lack of motivation for this problem within the time-series domain. The introduction (Section 1, Paragraph 1) frames the success of latent diffusion by citing image synthesis papers (Rombach et al., 2022; Podell et al., 2024) and the problem of posterior collapse by citing general VAE and NLP papers (e.g., Bowman et al., 2016). The paper fails to cite any existing work that first applies latent diffusion to time series and then identifies posterior collapse as a critical failure mode.\n\n- The paper fails to benchmark against a sufficiently broad set of recent, state-of-the-art (SOTA) time-series generation models. Table 2 does include some more recent and relevant baselines, such as \"Neural Latent Dynamic\" (Li et al., 2024) and \"Frequency Diffusion\" (Crabbé et al., 2024), which is commendable. However, this list is far from comprehensive. To make a convincing claim of superiority for a top-tier conference like ICLR, the method should be compared against a wider array of SOTA models such as [1, 2, 3, 4, 5]. \n\n\n- One of the paper's baselines is \"Latent Diffusion (Rombach et al., 2022)\". This model was designed for and is famous for image synthesis. The paper (e.g., in Table 2) directly cites this image paper when applying it to time series data. It is unclear if the authors adapted this model for sequential data or are applying an image-generation architecture directly. This choice is confusing.\n\n\n[1] Galib, Asadullah Hill, Pang-Ning Tan, and Lifeng Luo. \"FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation.\" NeurIPS 2024.\n\n[2] Naiman, Ilan, et al. \"Utilizing image transforms and diffusion models for generative modeling of short and long time series.\" NeurIPS 2024\n\n[3] Jeon, Jinsung, et al. \"GT-GAN: General purpose time series synthesis with generative adversarial networks.\" NeurIPS 2022\n\n[4] Naiman, Ilan, et al. \"Generative modeling of regular and irregular time series data via Koopman VAEs.\" ICLR 2024.\n\n[5] Coletta, Andrea, et al. \"On the constrained time-series generation problem.\" NeurIPS 2023."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zb2ofQxFCc", "forum": "UbL2Fo0IvV", "replyto": "UbL2Fo0IvV", "signatures": ["ICLR.cc/2026/Conference/Submission11173/Reviewer_pXBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11173/Reviewer_pXBK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713082805, "cdate": 1761713082805, "tmdate": 1762922330362, "mdate": 1762922330362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the effect of posterior collapse in latent diffusion models for time series data. The authors show that under posterior collapse a latent diffusion model collapses to a simple VAE. The authors introduce dependency measures which quantify the impact of the latent variables on the recurrent decoder for time-series data. The authors then leverage these findings to propose the so-called posterior-stable latent diffusion where the VAE decoder is free from the impact of posterior collapse."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I found the formulation of the dependency measure quite intuitive. However this intuition could be better clarified in the main text. See my comments on the presentation in the weaknesses section. I also liked Figure 1 which clearly justifies the intuition presented by the authors.\n\n2. I like the idea of penalizing the decoder for large noise levels in the diffusion model."}, "weaknesses": {"value": "I dont have major comments about the method itself but some comments on the presentation itself and some minor issues.\n\n**Presentation issues**\n\n1. The impact of the VAE posterior collapse on latent diffusion is trivial to understand. Its totally fine if the authors dont stress on this aspect too much in their introduction and instead just keep the current discussion in Section 3.1. I dont think this impacts the significance of the contributions.\n\n2. The dependence on autoregressive time t in the decoder $f^{dec}$ is missing and could be made more explicit starting Section 3.2.\n\n3. The notation in Eqs 8 and 9 can be vectorized for better readability.\n\n4. The definition of the dependency measure in Eq. 9 at a first glance looks quite messy and unintuitive. The authors should provide more intuition around this definition in the main text for better clarity for the readers. For instance something like: “the first component in the dot product denotes the overall change in the output of the time-series decoder while the second component denotes the overall impact of the jth context vector as s is modulated from 0 to 1”. Moreover, the equation itself can be cleaned up a bit as highlighted in the point (c) also.\n\n5. The idea of dependency illusion is nice but not surprising as some work in image classification also shows that deep neural nets can learn arbitrary mappings between images and class labels with high accuracy. Maybe the authors can clarify this in a bit more detail in the main text around the results in Fig. 1\n\n6. I think the authors can clarify the key differences between their approach and the classic latent diffusion approach in terms of training and sampling in more details in Section 4.2 or Section 5.\n\n**Unconvincing arguments**: The second point in Section 4.1 regarding the strong decoder in time-series data is not convincing. More specifically, it is not clear to me why the decoder used in time-series data inherently strong? Can the authors elaborate more on this?\n\n**Minor**: Is there a typo in Line 6 of the Training algorithm? I think the superscript over the variable z should be i and not j. Otherwise there is a discrepancy between the time embedding passed in the diffusion model and the actual noise level added."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K0hgifVqhu", "forum": "UbL2Fo0IvV", "replyto": "UbL2Fo0IvV", "signatures": ["ICLR.cc/2026/Conference/Submission11173/Reviewer_uFie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11173/Reviewer_uFie"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949449239, "cdate": 1761949449239, "tmdate": 1762922329932, "mdate": 1762922329932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies posterior collapse in latent diffusion models (LDMs) for time-series generation. It argues that, when paired with recurrent decoders, the latent variable’s influence decays (nearly exponentially) across time, leading the decoder to ignore latents. The paper identifies two main causes: unnecessary KL regularization in the latent path and vulnerability of time-series decoders. To handle this, this paper proposes new framework that treating the diffusion process as variational inference with regularization losses. Experiments show clear gains across several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-articulated background on posterior collapse and its time-series specifics.\n- Empirical improvements appear consistent across datasets."}, "weaknesses": {"value": "- The paper reports no experimental results on WARDS in Table 2, despite its inclusion in Table 1. Please clarify this.\n- The baselines shown in Figure 2 are insufficiently explained in the text, and the related-work section lacks a discussion of how these baselines address posterior collapse. Given that posterior collapse is already a well-recognized problem in time-series generative models, the paper should more clearly articulate what differentiates its approach."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FQJZQKwFoS", "forum": "UbL2Fo0IvV", "replyto": "UbL2Fo0IvV", "signatures": ["ICLR.cc/2026/Conference/Submission11173/Reviewer_xYR6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11173/Reviewer_xYR6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981818258, "cdate": 1761981818258, "tmdate": 1762922329391, "mdate": 1762922329391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}