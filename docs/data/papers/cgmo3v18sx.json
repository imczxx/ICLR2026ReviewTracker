{"id": "cgmo3v18sx", "number": 13636, "cdate": 1758220194807, "mdate": 1759897423384, "content": {"title": "Matched Data, Better Models: Target Aligned Data Filtering with Sparse Features", "abstract": "Data filtering plays a central role in improving model performance, particularly for vision language models that are pretrained on large, noisy, and redundant image-caption datasets. Existing filtering techniques assess every sample individually and retain those that exceed a certain quality threshold, but such strategies fail to capture higher-order interactions. In this work, we propose a novel submodular framework for data selection that addresses this limitation. Our method, Submodular Distribution Matching (SDM), selects a subset by: (1) training a type of sparse autoencoder to learn disentangled and \\emph{monotone} features; (2) estimating a target feature distribution from a target dataset; and (3) selecting a subset of samples whose feature distribution closely matches the target via submodular maximization. Given the DataComp-medium training set and no external models, SDM achieves state-of-the-art accuracy on both ImageNet-1K and average performance across 38 downstream tasks. On the full DataComp-medium benchmark, SDM delivers performance within 1\\% of the state-of-the-art results while using over \\textbf{\\emph{5×}} fewer GPU hours than the leading approach.", "tldr": "We use a submodular function instantiated on sparse autoencoder features to curate high quality + diverse datasets", "keywords": ["data filtering", "submodular", "sparse autoencoders"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/165eb3cdf5eae8e68434ad54a663770bd40e3f2f.pdf", "supplementary_material": "/attachment/b13144a0d22472adfe6fd36e5bfa799379cd4de5.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of filtering large-scale image-text datasets (like those scraped from the web) to improve model training. Such datasets are often noisy (containing irrelevant or low-quality data) and redundant (many very similar examples), which can hurt model performance. Traditional filtering methods usually score each data sample for quality and drop the low-scoring ones, but fail to capture the 'high-level feature. This paper proposed SDM to capture the high-level features and estimate the target distribution from ImageNet as a standard. Follow the standard, SDM can select high-quality samples from new datasets that follow the standard distribution at a high conceptual level."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposed SDM to capture high-level interactions (features), which go beyond single-sample selection. \n2. SDM uses high-quality datasets as standard to estimate the target distribution of high-level features\n3. This data selection strategy is verified in rich downstream tasks."}, "weaknesses": {"value": "1. It highly relies on the high-quality dataset, the high-level features learned from it are treated as the target distribution.\n2. The interpretability of the high-level features is a big concern as I listed in the questions"}, "questions": {"value": "1.  In line 154, why the sparse dimension $d_{sparse} >> d_{in}$. For the learned sparse features, how can we verify what the features are? Why are they informative to represent the information shown in the Image or text?\n2. For estimating the target distribution of 'high-level' features, this paper uses ImageNet as the standard. If all datasets share the same 'high-level' features? \n3. Although ImageNet is comprehensive in images, different datasets may have different features. This paper uses the same sparse autoencoder to align the learned high-level features. What if there are new important features in new datasets? \n4. Moreover, the proportion of different classes of images may affect the target distribution. If the gained performance is because the autoencoder is pre-trained on ImageNet, the data selection process uses ImageNet as a standard, which makes the selected data fit the model. \n5. For the comparison of different model sizes, does the best baseline have the same results across the different sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hb0vOn2mBQ", "forum": "cgmo3v18sx", "replyto": "cgmo3v18sx", "signatures": ["ICLR.cc/2026/Conference/Submission13636/Reviewer_aMFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13636/Reviewer_aMFk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825641037, "cdate": 1761825641037, "tmdate": 1762924213486, "mdate": 1762924213486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses data filtering for vision-language models pretrained on large, noisy datasets. The authors propose Submodular Distribution Matching (SDM), a submodular framework that selects samples by matching feature distributions learned via sparse autoencoders. Experiments on DataComp-medium demonstrate that SDM achieves state-of-the-art or near state-of-the-art accuracy across ImageNet-1K and 38 downstream tasks, while significantly reducing computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed Submodular Distribution Matching (SDM) presents a novel and promising approach to data filtering. Extensive experiments and superior performance compared with baseline methods convincingly demonstrate its effectiveness.\n\n2. The theoretical analysis linking the designed submodular maximization objective to the distribution-matching target strengthens the rationale behind the proposed method.\n\n3. The paper is well organized and clearly written, making it easy to follow and understand."}, "weaknesses": {"value": "From a model training perspective, selecting a data subset that precisely matches or aligns with the target distribution may critically influence the model’s out-of-distribution (OOD) generalization capability. Providing additional empirical evaluation of OOD performance using the filtered data would help clarify the practical impact of the proposed method and further highlight its contribution to improving generalization beyond the training distribution."}, "questions": {"value": "1. My main concern is that selecting a data subset that exactly matches or aligns with the target distribution may critically affect the model’s out-of-distribution generalization. How to balance the goal of distribution matching with maintaining data diversity during filtering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6caUuq1PxS", "forum": "cgmo3v18sx", "replyto": "cgmo3v18sx", "signatures": ["ICLR.cc/2026/Conference/Submission13636/Reviewer_7pgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13636/Reviewer_7pgs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952633338, "cdate": 1761952633338, "tmdate": 1762924212909, "mdate": 1762924212909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SDM (Submodular Distribution Matching), a data filtering framework for vision-language models that combines sparse autoencoders (SAEs) with submodular optimization. The method learns disentangled and monotone features through SAEs with a novel monotonicity loss, then selects data subsets that match a target distribution while considering sample quality. The authors claim state-of-the-art results on DataComp-medium benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel framework integration: First to combine SAEs with submodular optimization for data selection, providing both interpretability and theoretical guarantees\n2. Theoretical contribution: Establishes connection between KL divergence minimization and submodular maximization (Theorem 2.3), enabling efficient algorithms\n3. Practical impact: Achieves competitive performance on DataComp-medium with reasonable computational budget compared to alternatives\n4. Comprehensive evaluation: Tests across 38 downstream tasks, showing consistent (if modest) improvements\n5. Monotonicity loss innovation: Novel loss term (Eq. 3) for encouraging monotone features in SAEs could be valuable for interpretability community"}, "weaknesses": {"value": "Major\n1 Mathematical Soundness: \n- The objective \\log m_i(A) is undefined when mi​(A)=0. Add explicit ε-smoothing: log(mi​(A)+ϵ) with sensitivity analysis.\n- Unverified bound: The proof of Lemma 2.4 relies on∥h∥∞​≤β which the SAE architecture doesn't guarantee.\n2. Statistical Validation: All results lack error bars. Re-run with ≥3 seeds, report mean±std for all tables, and provide significance tests.\n3. Computational reporting: Should clarify total pipeline costs including encoding time\n\n\nMinor\n\n1. No ablation separating component contributions\n2. Provide more details on Algorithm 1 (distance metrics, buffer size)\n3. Explain the 5-run intersection choice"}, "questions": {"value": "1. How sensitive are results to ε-smoothing value? Please provide ablation.\n2. Can you guarantee the β bound in practice? What's the actual max activation value observed?\n3. Why take intersection of 5 greedy runs rather than union or single run?\n4. What's the breakdown of improvements from SAE features vs submodular selection?\n5. How does performance vary with different target distributions?\n\nThis paper makes a solid contribution to data selection for large-scale training. The idea of using SAEs to obtain interpretable features for submodular selection is clever and well-executed. While there are technical details to clarify, the core contribution is valuable and the experimental results support the claims.\n\nThe 0.7% average improvement may seem modest, but in the context of large-scale training where compute costs are substantial, even small improvements are valuable. The framework is also general and could be applied to other domains beyond vision-language models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tTPdvCVbBZ", "forum": "cgmo3v18sx", "replyto": "cgmo3v18sx", "signatures": ["ICLR.cc/2026/Conference/Submission13636/Reviewer_hCWT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13636/Reviewer_hCWT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006795787, "cdate": 1762006795787, "tmdate": 1762924212506, "mdate": 1762924212506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}