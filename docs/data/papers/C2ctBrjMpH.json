{"id": "C2ctBrjMpH", "number": 19009, "cdate": 1758292710397, "mdate": 1763741293989, "content": {"title": "FLAME: Reducing Computation in Federated Learning via Sample-Adaptive Multi-Exit Training", "abstract": "Federated learning (FL) enables a group of clients to collaboratively train a global machine learning model without sharing raw data. It is particularly suited to Internet-of-Things and similar environments involving small, heterogeneous devices. However, these clients often lack the computational resources needed to train the full global model locally, as the FL pipeline conventionally expects. Prior work addresses this challenge by assigning smaller sub-networks to resource-constrained clients, but such approaches have a key limitation: they do not adapt computational effort based on the needs of individual input samples. In this work, we introduce Federated Learning with sample-Adaptive Multi-Exiting (FLAME), the first method to incorporate sample-adaptive early exiting into local training for efficient FL. FLAME allows each training sample to exit at the earliest layer at which the model can confidently predict the sample’s output, which improves efficiency without sacrificing accuracy. We show that this use of sample-adaptiveness leads to better AUC than existing solutions because instead of uniformly saving computation across all samples, it strategically saves it on easier samples and preserves it for harder ones. Our empirical results demonstrate FLAME's ability to reduce per-client computation by up to 50% while maintaining or even improving model accuracy, and to outperform existing solutions in practical settings. We also show how FLAME’s success stems from FL’s collaborative nature and propose two optimizations that further enhance its efficiency and performance. Overall, this work introduces the novel concept of training-time sample-adaptiveness in the FL domain, which opens new avenues for improving the utilization of heterogeneous clients and for enhancing the FL paradigm.", "tldr": "", "keywords": ["federated learning", "multi-exit models", "efficient deep learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ca57615452b09f8f16654410d03f06d11bf9b43.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces FLAME, a novel sample-adaptive multi-exit training framework for Federated Learning (FL). Unlike traditional methods that allocate fixed or client-specific subnetworks, FLAME dynamically adjusts computation per sample through early-exit classifiers. Each input can terminate forward propagation early if successive internal classifiers agree, thereby saving compute while maintaining accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The work is well motivated, addressing a genuine bottleneck in FL: computation heterogeneity rather than storage limitations. By focusing on resource-constrained clients such as those in IoT environments, the paper demonstrates clear practical relevance and positions its contribution as directly applicable to real-world federated systems.\n\n2. The work is supported by comprehensive experiments conducted on multiple NLP benchmarks, including SST-2, MRPC, MNLI, and Sent140. These experiments are complemented by detailed ablation and scalability studies that validate the approach and highlight its effectiveness in reducing computation without degrading performance.\n\n3. The inclusion of a convergence proof adapted from FedPMT provides solid theoretical grounding, ensuring that the proposed method is not only empirically effective but also theoretically sound. This strengthens the work’s rigor and reassures readers about its stability in federated optimization."}, "weaknesses": {"value": "1. The idea of incorporating Multi-Exit mechanisms into federated learning is not novel. Several prior works have already explored this direction [1–3]. However, the authors neither compare FLAME with these existing approaches nor introduce any substantive refinement to the Multi-Exit paradigm to better adapt it to the federated learning setting.\n[1] Lee, Royson, et al. \"Recurrent Early Exits for Federated Learning with Heterogeneous Clients.\" Forty-first International Conference on Machine Learning.\n[2] Qu, Lehao, et al. \"DarkDistill: Difficulty-Aligned Federated Early-Exit Network Training on Heterogeneous Devices.\" Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2. 2025.\n[3] Ilhan, Fatih, Gong Su, and Ling Liu. \"Scalefl: Resource-adaptive federated learning with heterogeneous clients.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n2. The overall presentation of the paper requires improvement. The organization is confusing. For example, the authors introduce experimental settings before presenting the design of their method, which disrupts the logical flow of the paper. In addition, the design section includes several experimental results that would be more appropriately placed in the evaluation section. Furthermore, there are visible blank spaces in Table 2, suggesting incomplete or improperly formatted content. These issues collectively reduce the paper’s readability and professionalism.\n\n3. The experimental scope of the paper is quite limited, as all evaluations are confined to BERT-based NLP tasks. It remains unclear whether the sample-adaptive multi-exit strategy would function effectively in architectures such as CNNs or Vision Transformers (ViTs), where layer semantics and computational dynamics differ significantly from those in Transformer-based NLP models. Extending the evaluation to diverse modalities would greatly strengthen the paper’s claims about the broad applicability and robustness of FLAME.\n\n4. The literature review in this paper is insufficient, as most of the cited works are relatively outdated. In addition, the chosen baseline methods are also based on older approaches, which limits the paper’s ability to demonstrate progress over the current state of the art. Incorporating more recent studies and stronger baselines would significantly improve the completeness and credibility of the work.\n\n5. The authors propose the use of a multi-exit mechanism based on device resource limitations; however, they only consider computational constraints while neglecting storage constraints. In practice, each device is still required to deploy the entire large-scale machine learning model, which undermines the claimed advantage for resource-limited clients. A truly resource-efficient design should also account for memory and storage requirements, not just computation."}, "questions": {"value": "1. The claimed contribution lacks clear differentiation from prior research. The integration of multi-exit mechanisms into federated learning has already been explored in several earlier studies. The authors should clearly explain how their approach advances beyond these previous efforts and specify what unique advantages or improvements FLAME provides compared with existing multi-exit-based FL methods.\n\n2. The comparison baselines used in the paper are relatively outdated. The authors primarily evaluate against earlier methods rather than the latest advances in federated learning efficiency or adaptive computation. To provide a fair and convincing assessment of FLAME’s effectiveness, the paper should include comparisons with more recent and competitive baselines that reflect the current state of the art."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XY4qmPGZsd", "forum": "C2ctBrjMpH", "replyto": "C2ctBrjMpH", "signatures": ["ICLR.cc/2026/Conference/Submission19009/Reviewer_fRv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19009/Reviewer_fRv4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761200940185, "cdate": 1761200940185, "tmdate": 1762931058481, "mdate": 1762931058481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the limitation in existing solutions to system heterogeneity in federated learning, where clients train local models of a fixed size regardless of the input sample. To overcome this issue, it introduces a mechanism that dynamically determines when to perform an early exit based on each sample. Specifically, the method allows a sample to exit early when consecutive internal classifiers agree on the prediction; otherwise, the model continues training deeper layers. Through this process, the framework adaptively adjusts the effective size of each sub-model, leading to more efficient computation.  Through a series of ablation studies, the paper attempts to demonstrate the effectiveness of this sample-adaptive multi-exiting mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of adjusting early exits based on each sample, rather than applying the same sub-model to all data, is plausible.\n\nThe paper provides several ablation studies to demonstrate the effectiveness of the proposed sample-adaptive multi-exiting approach.\n\nThe paper provides a detailed analysis of the proposed method in terms of implementation details such as batch size and the aggregation rule."}, "weaknesses": {"value": "**The explanation of the experimental setup is not sufficiently detailed, making it difficult to interpret and trust the reported results.** For example, in Table 2, the baseline does not employ early exit, which suggests that it trains the full model. If so, it should represent the upper bound of performance. It is therefore unclear why its performance is lower than that of FLAME. More specific questions are listed in the Questions below.\n\n**The achievable computation cost savings are limited, and there are no communication cost savings.** Unlike the baselines, the proposed method still communicates the entire full model, resulting in no reduction in communication overhead. As shown in Table 3, even the computation cost savings are relatively modest, up to around 50% at best.\n\n**The necessity of deeper layers is questionable.** In Table 4, the baseline, despite having no early exit, shows a lower late-exiting AUC. Moreover, when early exit is allowed, deeper layers are trained with fewer samples, which further amplifies this discrepancy. Wouldn’t it be more stable and efficient to have all clients train smaller models without the deeper layers altogether?"}, "questions": {"value": "In Table 2, why is the avg. exit value for the baselines without early exiting not 12? If early exit is not applied, inference should always be performed using the full model. Isn’t the avg. exit therefore expected to be equal to the maximum depth (i.e., 12)?\n\nIn Table 5, it’s unclear why the performance of the fixed exit layer setting is so low. In particular, if all clients exit at layer 11, that effectively means they have trained the full model. How can this configuration perform worse than FLAME?\n\nIn Figure 2, the difference between the higher-patience client (b) and the lower-patience client (a) does not appear to increase noticeably in the later layers. What could be the reason for this?\n\nIn Table 8, HeteroFL does not incorporate early exit. How, then, was its evaluation conducted in this setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D9ztJUdZzW", "forum": "C2ctBrjMpH", "replyto": "C2ctBrjMpH", "signatures": ["ICLR.cc/2026/Conference/Submission19009/Reviewer_agcu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19009/Reviewer_agcu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401373040, "cdate": 1761401373040, "tmdate": 1762931057975, "mdate": 1762931057975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FLAME (Federated Learning with sample-Adaptive Multi-Exiting), which reduces the training cost at clients of Federated Learning by using sample-adaptive early stopping in local training of clients. The method allows each sample to exit the neural network at the earliest layer where the model can confidently predict its output, thus saving computational resources without sacrificing accuracy. That is, after one or more hidden layers of the model, the sample is passed through an internal classifier (IC). The forward pass will stop at the point where p successive ICs have the same classification. Although the method does not reduce the memory footprint at each client, it could reduce the computation cost for easy samples since they are performed in a smaller subnetwork."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. FLAME help to improve the performance, e.g., AUC, and reduce the training cost, yet reduce the time per iterations.\nS2. The introduction of sample-adaptive early exits in FL is a novel approach."}, "weaknesses": {"value": "W1. FLAME requires clients to store the entire model, even though they may not utilize all of its layers. Consequently, FLAME may still be infeasible for training large models on resource-constrained clients, as the overall memory footprint is not reduced. Specifically, although the authors mentioned the memory reduction in Table 14 (Appendix), the reduction is trivial.\n\nW2. The findings presented in the paper are primarily based on empirical results, without any accompanying theoretical analysis. Moreover, the empirical evaluation focuses on a limited range of settings (in terms of models and datasets) and employs a non-typical federated learning setup—with only 10 clients and unclear details on how the local datasets are constructed. As a result, the proposed idea appears to lack generality."}, "questions": {"value": "Q1. Please provide a detailed discussion on how the model is updated in FLAME with grouped backpropagation. Explain why the grouping strategy leads to a reduction in AUC. Additionally, in Table 6, the authors should compare FLAME with the setting (No FLAME + b > 1). In this case, does FLAME help reduce the total local training time? Furthermore, note that performing the forward pass sample by sample (instead of by batch) may increase the training time—this point should also be discussed.\n\nQ2. The performance of FLAME largely depends on how the parameter p is chosen. However, in the 10 settings presented in the paper (Table 1), p is fixed for each client. Moreover, using the same fixed value of p across all clients may not be optimal. Please discuss how p should be selected in practice.\n\nQ3. The results in Table 2 show that FLAME under-performs in certain cases. Please discuss which datasets or tasks FLAME performs better or worse on compared to the baseline methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oUbZ5OAiyH", "forum": "C2ctBrjMpH", "replyto": "C2ctBrjMpH", "signatures": ["ICLR.cc/2026/Conference/Submission19009/Reviewer_ejMK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19009/Reviewer_ejMK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958553031, "cdate": 1761958553031, "tmdate": 1762931057613, "mdate": 1762931057613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FLAME (Federated Learning with sample-Adaptive Multi-Exiting) which applies multi-exit training to federated learning for reducing client-side computational costs. The key insight of FLAME is to dynamically adapt computation at the sample level by allowing easier samples to exit early during training while preserving computational resources for harder examples. Using a patience-based mechanism where samples exit when consecutive internal classifiers agree, FLAME achieves up to 50% reduction in training computation while maintaining or improving model accuracy across multiple NLP benchmarks. The authors provide theoretical convergence guarantees (O(1/T) rate), demonstrate through ablation studies that FL's collaborative nature is essential for FLAME's success by preventing under-training of deeper layers, and show that sample-adaptiveness leads to better AUC than fixed sub-network methods. Additionally, they propose grouped backpropagation to enable minibatch training and three tailored aggregation algorithms. Experiments on GLUE tasks and Sentiment140 demonstrate FLAME outperforms state-of-the-art baselines (HeteroFL, ScaleFL, AFD) under matched computational budgets, particularly on non-IID data distributions, establishing sample-level adaptiveness as a promising new paradigm for efficient federated learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel sample-adaptive paradigm: Introduces a compelling alternative to existing client-level sub-network approaches by dynamically allocating computation based on individual sample difficulty rather than uniform resource distribution.\n\n2. Strong empirical results: Achieves substantial computational savings (up to 50%) while maintaining or improving accuracy across multiple benchmarks. The comparisons with HeteroFL, ScaleFL, and AFD under matched compute budgets are thorough and convincing.\n\n3. Insightful ablation studies: The experiments in Section 5 effectively demonstrate why FLAME works - particularly the role of collaboration in mitigating under-training and the value of sample-level adaptation over fixed exits.\n\n4. Solid theoretical grounding: Provides convergence guarantees and includes practical extensions (grouped backpropagation, aggregation variants) that address real implementation challenges.\n\n5. Clear presentation: Well-structured paper with effective visualizations and comprehensive appendices that aid understanding and reproducibility."}, "weaknesses": {"value": "1. Memory requirements contradict resource-constrained motivation: FLAME requires storing the full global model plus 12 internal classifiers (one per layer), which undermines claims about enabling FL on IoT/mobile devices. While the paper claims ICs add \"negligible FLOPs\" (Appendix B), there's no analysis of their memory footprint or cumulative parameter overhead. The ~200MB RAM savings (16.0GB → 15.8GB) are marginal, and all experiments run on A100 GPUs with 40GB RAM rather than actual resource-constrained hardware. No evidence supports the claim that \"computation is the primary bottleneck\" over memory in target deployment scenarios.\n\n2. Limited scalability and practical deployment analysis: Experiments use only 10-25 clients, far from realistic deployments with hundreds/thousands of clients. No analysis of communication costs (clients must download full model + all ICs), stragglers, partial participation, or interaction with privacy mechanisms. Wall-clock improvements are measured on controlled GPU setups rather than heterogeneous devices, and there's no guidance on choosing patience distributions in practice.\n\n3. Batch size limitation and inconsistent task performance: Standard FLAME requires batch size 1, which is inefficient for modern hardware. Grouped backpropagation addresses this but shows significant accuracy drops (0.981 → 0.913 AUC on SST-2). Performance is also inconsistent across tasks - FLAME degrades on MRPC (0.78 vs 0.84 baseline) with insufficient analysis of when/why it works versus when it doesn't.\n\n4. Narrow experimental scope: All experiments are BERT-based text classification tasks - no evaluation on vision models, other architectures, or non-NLP domains. The comparison omits several recent methods mentioned in related work (FedDSE, PriSM, FjORD, FLANC), and convergence behavior is not empirically validated despite theoretical analysis.Retry"}, "questions": {"value": "1. Memory overhead and deployment on actual resource-constrained devices: While Table 14 shows ~100-200MB RAM savings from reduced activations, what is the actual memory overhead of storing 12 internal classifiers? Each IC is a linear layer projecting hidden states to class logits - for BERT-base with 768 hidden dims and varying output classes, this could be substantial. Also - can you provide benchmarks on actual edge devices (Raspberry Pi, mobile phones with <4GB RAM) rather than A100 GPUs? How does total model size (base + all ICs) compare to methods like HeteroFL where clients store smaller sub-networks?\n\n2. Communication costs analysis: What is the total communication overhead per round when clients must download the full model plus all 12 ICs? How does this compare quantitatively to baselines (HeteroFL, ScaleFL) where clients download only sub-networks, especially for the 25-client Sentiment140 experiment?\n\n3. Task-specific failure modes and practical guidance: FLAME works well on SST-2 (0.98 AUC) but degrades on MRPC (0.78 vs 0.84 baseline). Can you characterize what task properties determine success/failure? How should practitioners decide if FLAME is suitable for their task, and how should they select patience distributions across clients?\n\n4. How does FLAME perform with 100+ clients and partial participation? Can you provide results beyond BERT-based text classification - e.g., vision models (CNNs/ViTs), other architectures, or regression tasks where the patience-based exit criterion may not directly apply?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LFqDjSTKr3", "forum": "C2ctBrjMpH", "replyto": "C2ctBrjMpH", "signatures": ["ICLR.cc/2026/Conference/Submission19009/Reviewer_veS5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19009/Reviewer_veS5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008702562, "cdate": 1762008702562, "tmdate": 1762931057172, "mdate": 1762931057172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}