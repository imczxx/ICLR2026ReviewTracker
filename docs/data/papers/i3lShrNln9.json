{"id": "i3lShrNln9", "number": 2788, "cdate": 1757249522472, "mdate": 1759898127188, "content": {"title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning", "abstract": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks to execute classic algorithms by supervised learning. Despite its successes, important limitations remain: inability to construct valid solutions without post-processing and to reason about multiple correct ones, poor performance on combinatorial NP-hard problems, and inapplicability to problems for which strong algorithms are not yet known. To address these limitations, we reframe the problem of learning algorithm trajectories as a Markov Decision Process, which imposes structure on the solution construction procedure and unlocks the powerful tools of imitation and reinforcement learning (RL). We propose the GNARL framework, encompassing the methodology to translate problem formulations from NAR to RL and a learning architecture suitable for a wide range of graph-based problems. We achieve very high graph accuracy results on several CLRS-30 problems, performance matching or exceeding much narrower NAR approaches for NP-hard problems and, remarkably, applicability even when lacking an expert algorithm.", "tldr": "", "keywords": ["reinforcement learning", "imitation learning", "neural algorithmic reasoning", "graph neural networks", "combinatorial optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/805476d89e9041bf5d6877d97b935896083dc4dc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reframes the NAR as an RL process and builds an architecture that is applicable to problems with multiple correct answers, NP-hard problems, and problems without expert algorithms. Performances are evaluated on several CLRS-30 problems and the robust graph construction task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Redefining the algorithm learning process as a trajectory optimization problem is a promising paradigm. Experiments show that the proposed method solves key problems of NAR methods."}, "weaknesses": {"value": "1. The introduction of the proposed method, model variants, and experimental metrics is not clear enough. Sometimes it's hard to understand the content due to a loss of details.\n\n2. The time consumption compared with other methods is lacking.\n\n3. The performance of the proposed method is still inferior compared to advanced Non-NAR methods."}, "questions": {"value": "1. For problems without expert algorithms, how do you compute the rewards?\n\n2. In your framework, how do you train GNARL-BC and GNARL-PPO in detail? I think it should be presented in a concentrated manner rather than scattered across different sections.\n\n3. Line 316 says \"meaning that not all CLRS-30 graph problems are representable for the time being\". Can you show some examples in CLRS-30 that GNARL can not handle, and is this a limitation of GNARL?\n\n4. How much computational overhead is brought by training the critic model? Can you compare the training time consumption of GNARL and other NAR methods?\n\n5. What is the meaning of the metric \"TSP percentage above optimal objective\"? Is it the lower the better?\n\n6. In the Limitation Section, it says \"GNARL relies on the environment during execution, creating a performance bottleneck. \". Can you explain this in detail?\n\n7. On evaluation of CLRS-30, it says \"Rodionov & Prokhorenkova (2025) report 100% graph accuracy on the BFS, DFS, and MST-Prim problems. \". Why is this baseline removed from the comparison in Table 2?\n\n8. Why is GNALR-PPO not considered in Table 2? \n\n9. In Table 3, why do you use only 10% of the training data? What is the performance comparison when using the full training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LWn52gPcLX", "forum": "i3lShrNln9", "replyto": "i3lShrNln9", "signatures": ["ICLR.cc/2026/Conference/Submission2788/Reviewer_dAeN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2788/Reviewer_dAeN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760588155842, "cdate": 1760588155842, "tmdate": 1762916377301, "mdate": 1762916377301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work the authors propose the Graph Neural Algorithm Reasoning with RL (GNARL) framework. This approach casts algorithmic processes as trajectories in MDPs. By formulating the problem as an MDP, they leverage RL techniques to improved performance in replicating performance on algorithmic reasoning tasks. In contrast to other NAR approaches, the authors use MDPs to ensure valid solutions, cast P and NP graph problems into the same formalism, and learn examples without an expert algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of this paper is good, and appears quite promising.\n\nThe description of the need the authors hope to fill is clear and well-motivated.\n\nThe paper’s results are very even-handed, with a clear assessment of the limitations of the current work."}, "weaknesses": {"value": "If my understanding is correct, the key insight is the formulation of the problem as an MDP. Thus, section 1 is a critical section of the paper. This section was difficult to parse. It seems that the transition function is algorithm-dependent, but many of the other elements are not. Perhaps the authors could provide a compact definition of a GNARL MDP that is similar to the MDP tuple definition found at the beginning of 3.2. Then the GNARL MDP tuple could be defined in terms of graph elements, with notes about which elements are the same for all problems, and which vary according to problems.\n\nIn general, it strikes me as difficult to think about how to formulate an algorithmic problem as an MDP, even after reading this paper. It seems to me that a more general framing of the approach would be helpful. Or perhaps this difficulty is the core trade-off, as indicated in the limitations section?\n\nFor me, the second and third paragraphs of 3.2 felt superfluous, except perhaps the definitions of the acronyms BC and IL. If the authors need more room to describe their method, I think this section could be trimmed substantially."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EsvP0AbPNV", "forum": "i3lShrNln9", "replyto": "i3lShrNln9", "signatures": ["ICLR.cc/2026/Conference/Submission2788/Reviewer_svJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2788/Reviewer_svJy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576903404, "cdate": 1761576903404, "tmdate": 1762916377067, "mdate": 1762916377067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Neural Algorithmic Reasoning (NAR) involves training neural networks to learn to exectue algorithms. This is typically done by executing the algorithm and collecting  supervision signal (e.g., intermediate variable values) at every step of the algorithm. This data can then be used to train the neural network by supervising the output at every step. This paper proposes a new approach which views algorithms as Markov Decision Processes with a state, a set of actions, and a transition function. This then allows to train NAR models to produce the right action at every step, hence moving the supervision signal from the domain of outputs to those of actions. Training then happens though reinforcement learning methods. In particular the authors propose to use either imitation learning (when an expert policy is available) or proximal policy optimization.\nThe proposed method is evaluated on standard algorithms for NAR from the CLRS dataset, a combinatorial optimization algorithm (travelling salesman), and an NP algorithm (minimum vertex cover). The baselines are provided by popular NAR methods for CLRS, and specialized approaches for the other scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Viewing NAR as an MDP is a very intuitive yet novel idea\n- The method can in principle be applied to several graph algorithms\n- The experimental results show important improvements over previous methods"}, "weaknesses": {"value": "- There are some existing methods which train NAR models without supervising at every step that have not been considered (e.g., \"Deep Equilibrium Algorithmic Reasoning\", Georgiev et al., 2024l \"Deep equilibrium models for\nalgorithmic reasoning\", Xhonneux et al. 2024). These works should be cited at the very least \n- I found some parts of the text a bit unclear (see questions below)\n- defining the MDP for a given algorithm is not always trivial and there may be more possible MDPs for any given algorithm"}, "questions": {"value": "- Are there ways to extend the method to algorithms in which the Markov property doesn't hold?\n- Could you explain how the proto action works? I found it quite unclear from the current text\n- Does the translation from \"encode-process-decode\" to \"encode-process-act\" lead to a much higher number of \"steps\"? From what is shown in Appendix B it seems that this is the case. Can this be an issue? I feel like there should be a discussion about this aspect in the paper\n- Could the authors please expand on the statement that the proposed method \"can handle multiple correct solutions\"? Is this because instead of having an end-to-end \"gold\" trajectory, one can just apply 1 step from the current state and then use it for supervision?\n- Reinforcement learning is notoriously challenging to apply in practice. Could you please add plots on the training stability of the proposed method and some discussion on the number of training samples required with respect to supervised training?\n- Could the authors please expand on what is meant with \"we estimate the graph accuracy as micro-F1|\"? This last paragraph in Page 6 is quite unclear to me\n- Is there an \"automatic\" procedure that can be used to define an MDP from a given algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concern"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GNp2VrgDP8", "forum": "i3lShrNln9", "replyto": "i3lShrNln9", "signatures": ["ICLR.cc/2026/Conference/Submission2788/Reviewer_vU6f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2788/Reviewer_vU6f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587912968, "cdate": 1761587912968, "tmdate": 1762916376404, "mdate": 1762916376404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a rephrasing of the reinforcement learning or Markov Decision Process in the language of Neural Algorithmic Reasoning (NAR).\n\nThe authors present the general mapping and experiments on the CLRS-30 benchmark on Depth-first or Breadth-First Searches, on TSP instances, on Minimum Vertex Cover (MVC), and robust graph construction (RGC). \n\nThe model can be trained similarly to RL, using Imitation training or using proper RL methods (e.g.PPO)\n\nThe paper's contributions are:\n1. to show the mapping between NAR and MDP\n2. to evaluate and compare the performance on 4 learning tasks"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written, the related works are mostly covered, and a relatively good number of evaluations."}, "weaknesses": {"value": "There is no reference in the related work at least to the GFlowNet approach.\n\nThe approach is meant to be NAR, but the mapping depends on the problem: \"A(s) are specified for each problem\", \"the horizon $h$ is defined by the problem\".\n\nIn general, it is hard for me to really understand the difference with respect to using RL. The \"framework\" is to define the State space $\\mathcal S=\\mathcal T \\times \\mathcal F$, with $\\mathcal T$ the imposta space and $\\mathcal F$ the state space of the graph. \n\nI probably also have a problem understanding what is different in the general NAR framework, which seems to be the idea of having an encoder, a decoder, and some iteration in the latent space. \n\nWhile I find CO really fascinating, I am not sure I really appreciated the NAR perspective; therefore, it is hard for me to evaluate this work."}, "questions": {"value": "Based on my previous analysis, the author shall therefore clarify better 1) what the actual contribution is compared to RL, 2), explain the difference to RL, and 3) also explain the difference with GFlowNet, which also defines an MDP over a graph of states."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YIbG6Hp7xi", "forum": "i3lShrNln9", "replyto": "i3lShrNln9", "signatures": ["ICLR.cc/2026/Conference/Submission2788/Reviewer_qSb5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2788/Reviewer_qSb5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915895049, "cdate": 1761915895049, "tmdate": 1762916376059, "mdate": 1762916376059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}