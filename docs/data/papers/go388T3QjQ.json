{"id": "go388T3QjQ", "number": 10542, "cdate": 1758175125400, "mdate": 1759897644508, "content": {"title": "Long-tailed Learning with Muon Optimizer", "abstract": "Long-tailed recognition poses a significant challenge in deep learning, as models tend to be biased towards head classes, leading to poor generalization on underrepresented tail classes. A key factor contributing to this issue is that the optimization process for tail classes often stalls in sharp regions of the loss landscape. In this work, we investigate this problem from an optimization perspective and leverage the recently proposed Muon optimizer. We provide new theoretical insights, demonstrating that Muon's gradient orthogonalization enhances the update's projection along directions of negative curvature, thereby facilitating a more effective escape from sharp minima. To further mitigate the additional computational overhead of Muon, we propose Progressive Muon Optimizer (ProMO), a novel hybrid optimization approach that balances performance with efficiency. Specifically, ProMO employs a sinusoidal probability schedule to dynamically alternate between SGD and Muon. This method predominantly uses computationally efficient SGD in the early stages of training and gradually increases the use of Muon as the model approaches convergence when escaping sharp minima becomes critical for tail-class generalization. Extensive experiments on large-scale long-tailed benchmarks demonstrate that ProMO consistently outperforms existing long-tailed recognition methods. These results validate that ProMO effectively improves generalization on tail classes without incurring significant computational costs, highlighting its potential as a practical and effective solution for long-tailed learning.", "tldr": "", "keywords": ["machine learning", "imbalanced learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/692dd677dcebd8da4f8b1c5a09325fd7a3abc3e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates long-tailed learning from the perspective of the sharpness of the loss landscape. It argues that previous sharpness-aware minimization (SAM) methods and their variants incur high computational costs. Similarly, although the Muon optimizer can help models escape sharp regions, it is also computationally expensive. To address these issues, the paper proposes ProMO, a method that applies the Muon optimizer following a sinusoidal scheduling strategy to balance effectiveness and efficiency. Experimental results on several mainstream long-tailed datasets demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized, and the overall writing flow is clear and logical, making it easy to follow.\n2. The proposed solution is simple yet technically sound.\n3. The theoretical and computational analyses provide valuable insights that strengthen the motivation and understanding of the proposed approach."}, "weaknesses": {"value": "1. The technical novelty of the proposed method appears limited, as it primarily combines Muon and SGD in a hybrid scheduling manner. To strengthen the contribution, the authors are encouraged to provide deeper insights or analyses on the underlying training dynamics that motivate this design.\n2. The paper does not include comparisons with several strong SAM variants, such as ImbSAM [1] and CC-SAM [2], which are highly relevant baselines in this context.\n3. Experimental results on the iNaturalist-2018 dataset are missing, which limits the completeness of the evaluation.\n4. According to the reported results, ProMO exhibits inconsistent performance improvements across different datasets.\n\nReference:\n\n[1] ImbSAM: ACloser Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition. ICCV 2023.\n\n[2] Class-Conditional Sharpness Aware Minimization for Deep Long-Tailed Recognition. CVPR 2023."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ohbe4akQoy", "forum": "go388T3QjQ", "replyto": "go388T3QjQ", "signatures": ["ICLR.cc/2026/Conference/Submission10542/Reviewer_MTSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10542/Reviewer_MTSZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554347100, "cdate": 1761554347100, "tmdate": 1762921821042, "mdate": 1762921821042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of long-tailed learning from an optimization perspective by leveraging the Muon optimizer.\nThe main contributions are as follows:\n\n1. It demonstrates that Muon’s gradient orthogonalization enhances updates along negative curvature directions, enabling the optimizer to escape sharp minima more effectively.\n\n2. It introduces the Progressive Muon Optimizer (ProMO), a hybrid approach that dynamically alternates between SGD and Muon using a sinusoidal probability schedule to balance performance and computational cost.\n\nIn summary, the proposed Muon and ProMO optimizers show potential as replacements for conventional SGD-based methods, achieving improved overall performance at the cost of higher computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a solid theoretical analysis of the Muon optimizer, with Theorem 1 proving that it amplifies gradient projections along negative curvature directions under the Correlated Negative Curvature (CNC) assumption. This provides a sound theoretical foundation for understanding how Muon effectively escapes sharp minima.\n\n2. The experimental evaluation is comprehensive and convincing, covering a wide range of mainstream loss functions and optimization algorithms."}, "weaknesses": {"value": "1. The Muon optimizer appears to be a general optimization method applicable to existing models, but this paper lacks specific algorithmic design or adaptation specialized for long-tailed learning.\n\n2. The comparison of fine-tuning strategies is missing. For example, recent methods such as LIFT and LPT are not included. \nIn addition, the paper does not consider decoupled training strategies. \nIt is also unclear whether using a re-balancing classifier would offset the reported improvements.\n\n3. The Muon optimizer sometimes incurs higher computational overhead than SAM and does not consistently achieve superior performance, which may limit its practical effectiveness. (Nevertheless, the proposed ProMO variant shows better efficiency and performance trade-offs. Why not directly treat ProMO as the main part of the proposed mehtod rather than as an auxiliary extension?)\n\n4. The paper does not provide code or implementation details, which prevents further verification and reproducibility of the proposed method."}, "questions": {"value": "1. What does the deep blue line in the tables represent?\n\n2. Will the authors release the code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fSqC5OokqT", "forum": "go388T3QjQ", "replyto": "go388T3QjQ", "signatures": ["ICLR.cc/2026/Conference/Submission10542/Reviewer_ZKRh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10542/Reviewer_ZKRh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568453905, "cdate": 1761568453905, "tmdate": 1762921820427, "mdate": 1762921820427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the challenge of poor generalization in tail classes within long-tailed learning by examining the optimization process through the lens of loss landscape geometry. It demonstrates that tail classes are often prone to converging to sharper minima in the loss landscape. The authors further show that the recently proposed Muon optimizer enhances gradient projections along directions of negative curvature, enabling faster escape from sharp minima. However, Muon introduces considerable computational overhead. To address this issue, the paper proposes ProMO, a progressive hybrid optimizer that alternates between SGD and Muon according to a sinusoidal probability schedule, effectively balancing computational efficiency and generalization. Extensive experiments validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-written and easy to follow.\n- This paper explores the application of the Muon optimizer in long-tailed learning and shows that it can help escape sharp minima, which is both interesting and meaningful.\n- The experiments are fairly comprehensive, covering four mainstream benchmarks and comparing multiple methods and optimizers."}, "weaknesses": {"value": "- As far as I know, the CNC assumption describes the property that determines an optimization algorithm’s ability to escape saddle points. However, a saddle point is not equivalent to a sharp minimum. In fact, [1] shows that the loss landscape of tail classes has a highly negative minimum eigenvalue, indicating convergence to saddle points, which hinders generalization. Therefore, it would be more informative to further examine whether the minimum eigenvalue under Muon is larger than that under SGD, to demonstrate that Muon facilitates escaping saddle points. In Fig. 2, this trend appears plausible, but it would be better to report the exact eigenvalue values for verification.\n- More representative long-tailed learning methods, such as GPaCo [2], CC-SAM [3] and DirMixE [4], should be included in the related work for a more comprehensive review.\n- There are also some typos, such as in Figure 1, where \"Hessain metric\" should be \"Hessian metric\".\n\n-----\n\n[1] Escaping saddle points for effective generalization on class-imbalanced data, NeurIPS 2022\n\n[2] Generalized Parametric Contrastive Learning, TPAMI 2023\n\n[3] Class-Conditional Sharpness-Aware Minimization for Deep Long-Tailed Recognition, CVPR 2023\n\n[4] Harnessing Hierarchical Label Distribution Variations in Test Agnostic Long-tail Recognition, ICML 2024"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zkgLmZJmqP", "forum": "go388T3QjQ", "replyto": "go388T3QjQ", "signatures": ["ICLR.cc/2026/Conference/Submission10542/Reviewer_7gu8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10542/Reviewer_7gu8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623099860, "cdate": 1761623099860, "tmdate": 1762921820022, "mdate": 1762921820022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a rigorous theoretical analysis of the Muon optimizer, demonstrating its ability to escape sharp minima by enhancing gradient projection along negative curvature directions. This bridges optimization theory with practical long-tailed learning challenges. The proposed ProMO balances computational efficiency and performance, which has been empirically validated on real datasets. In summary, this paper addresses a significant long-tailed learning problem, offering a promising tool for the whole long-tailed learning community. However, the impact of the proposed optimizer on learned representations is somewhat unclear, as the paper focuses more on the efficiency and accuracy issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed ProMO addresses a critical gap in long-tailed learning by balancing computational efficiency and performance. The sinusoidal probability schedule is simple yet empirically effective, outperforming alternatives like linear or exponential schedules.\n\n2. The paper follows a clear structure and is easy to follow. The appendices provide pseudo-code and additional proofs, which enhance reproducibility. The figures and tables are well-designed and support key claims.\n\n3. Evaluation is performed on multiple datasets and loss functions, demonstrating the robustness of the proposed method."}, "weaknesses": {"value": "1. This paper focuses on optimizer design. However, for ICLR, which focuses more on the principles of learning representations, a stronger emphasis on how the method fundamentally improves representation learning beyond just faster convergence is preferable. Although the conference's subject areas include 'large-scale learning and non-convex optimization', my understanding is that it focuses more on how to address the fundamental challenges these issues pose to representation learning, rather than the efficiency limitations imposed by them.\n\n2. The paper does not explore optimizations like gradient approximation or parallelization to further mitigate overhead.\n\n3. The CNC assumption is not validated in a broader context. The claim that Muon’s orthogonalization always enhances negative curvature projection lacks edge-case analysis, e.g., near-convex regions.\n\n4. The source code is not yet publicly available."}, "questions": {"value": "1. The paper focuses on optimizer design. How does Muon fundamentally change feature learning, beyond just better convergence/efficiency compared to the existing optimizers for long-tailed learning?\n\n2. Have the authors compared adaptive methods, e.g., Adam variants?\n\n3. The experiments focus on vision tasks. Is this proposed optimizer promising on NLP or multimodal long-tailed datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kl26DW5hFw", "forum": "go388T3QjQ", "replyto": "go388T3QjQ", "signatures": ["ICLR.cc/2026/Conference/Submission10542/Reviewer_oSTn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10542/Reviewer_oSTn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924225265, "cdate": 1761924225265, "tmdate": 1762921819611, "mdate": 1762921819611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}