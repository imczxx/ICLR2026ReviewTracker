{"id": "LmJoLn04iL", "number": 19357, "cdate": 1758295625065, "mdate": 1759897043548, "content": {"title": "IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs", "abstract": "Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.", "tldr": "We introduce IndicVisionBench, a large-scale cultural benchmark covering English and 10 Indian languages across 3 multimodal tasks including OCR, VQA and Multimodal Machine Translation (MMT).", "keywords": ["Vision Language Models", "VLMs", "Multimodal models", "Cultural VLMs", "Mutlimodal Evaluation", "OCR", "Cultural VQA", "Mutlimodal Machine Translation", "MMT"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cbcdc06ee017f597f0532d506ceaae12e86a968.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces IndicVisionBench which is a culturally grounded multimodal benchmark focused on Indian languages. The benchmark covers three tasks-- VQA, multimodal MT, and OCR. It has ~5k images and 37k+ QA pairs in English and 10 Indic languages. The questions include six types including MCQs, True/False, Short Answers (two versions), long answers and adversarial questions with opposite assumptions. Several multimodal LLMs have been evaluated against the developed benchmark and a comparison has been provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses three tasks by presenting a benchmark for 10 low-resource Indic languages. Such benchmarks are crucial to evaluate capabilities of multimodal LLMs on less represented languages.\n- The benchmark covers the annotation of 13 cultural topics and three tasks--VQA, multimodal MT and OCR. Question have further six categories showing a comparative aspect and abilities of LLMs. \n- Eight LLMs have been evaluated covering open and closed source models that provides an inside of performance compared to different Indic cultures and languages.\n- Parallel slice for controlled analysis, the 106-image VQA-Parallel + MMT subset is well designed for cross-lingual/model-vs-no-image comparisons.\n- Adding adversarial questions with false assumptions provides realistic stress test and it really exposes even strong models."}, "weaknesses": {"value": "- Synthetic data creation: many QAs are first generated by the LLM and then corrected by human. What was the error rate and it should be quantified.\n- Many images are Commons Google-search cultural images which are highly likely to have been seen by pretraining corpora."}, "questions": {"value": "As most of the images in the benchmark were collected from Google search under creative common license -- is there any correlation with collecting images from Google search and achieving highest performance from Gemini-2.5 (from Google's DeepMind)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fns1yZgH9G", "forum": "LmJoLn04iL", "replyto": "LmJoLn04iL", "signatures": ["ICLR.cc/2026/Conference/Submission19357/Reviewer_w7Rf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19357/Reviewer_w7Rf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839322494, "cdate": 1761839322494, "tmdate": 1762931292806, "mdate": 1762931292806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents IndicVisionBench, a large-scale benchmark designed to evaluate Vision-Language Models (VLMs) on culturally grounded and multilingual tasks focused on the Indian subcontinent. The benchmark comprises 5,000 unique images and over 37,000 question-answer pairs spanning 13 cultural topics across English and 10 Indian languages. The evaluation framework encompasses three primary tasks: Visual Question Answering (VQA) with six question types, Optical Character Recognition (OCR), and Multimodal Machine Translation (MMT). The authors evaluate models, including both proprietary systems (Gemini-2.5, GPT-4o) and open-source variants, revealing substantial performance gaps particularly for low-resource languages and culturally specific content.​"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Comprehensive Multi-task Framework**: The benchmark's tri-modal evaluation approach (VQA, OCR, MMT) provides a holistic assessment of VLM capabilities beyond simple question-answering. The inclusion of adversarial questions represents a good approach to probing deeper cultural knowledge beyond surface-level recognition.\n\n-  **Linguistic Coverage** : The benchmark covers 10 Indic languages with diverse scripts."}, "weaknesses": {"value": "- **Limited Methodological Novelty**:    \nWhile the cultural focus is valuable, the benchmark construction methodology largely follows established paradigms without introducing novel evaluation frameworks. The reliance on synthetic question generation using commercial models (Gemini-1.5-Flash, Gemini-2.5-Flash) raises concerns about potential biases inherited from these models.\n\n- **Self-Preferential Bias and Evaluation Circularity**:   \nThe most severe methodological flaw is the systematic use of Gemini models throughout the benchmark construction pipeline, followed by its evaluation on the same benchmark. Gemini-1.5-Flash and Gemini-2.5-Flash generate synthetic captions and QA pairs​ and translations across languages​. This creates a circular evaluation where Gemini is assessed on data it helped generate, violating fundamental evaluation principles established in contamination literature. The consistently superior performance of Gemini-2.5 across all tasks becomes suspect given this methodological flaw.​  \n\n- **Absence of Design Choice Validation**:   \nThe paper provides no systematic benchmarking for the choice of Gemini variants. While mentioning a \"small pilot study and cost considerations,\" no details, quality assessments, or comparative analysis with alternative generation models are provided. Table 6 shows only cost comparisons without quality metrics.  The benchmark's coverage appears skewed toward certain linguistic groups (Hindi at 26.8% of QA pairs) without adequate justification for this distribution.\n\n - **Complete Lack of Quality Control Statistics**:   \nCaption Quality: No validation of synthetic caption accuracy against image content using multimodal embeddings or human verification statistics​.  \nHuman Refinement Rates: The claim that \"Human reviewers refined all outputs for factual accuracy and cultural alignment\" provides zero statistics on edit rates, acceptance rates, or extent of modifications required​.  \nInter-Annotator Agreement: Despite being standard practice for evaluation benchmarks, no IAA scores or methodology details are provided.  \n\n- **Inadequately Documented Subset Selection**:  \nVQA-Indic: The selection of which subset from \"4K+ images\" was translated lacks stratification methodology, selection criteria, or coverage statistics​\nVQA-Parallel: The choice of 106 images for multilingual translation provides no justification for representativeness or selection rationale​\nStatistical Significance: With Hindi dominating 26.8% of QA pairs and other languages having minimal representation, the paper lacks analysis of whether minority language subsets support reliable conclusions\n \n- **Statistical Significance Testing**  \nThe paper lacks statistical significance testing for reported performance differences, provides no confidence intervals, and offers insufficient error analysis across the diverse evaluation scenarios. With Hindi dominating 26.8% of QA pairs and other languages having minimal representation, the paper lacks analysis of whether minority language subsets support reliable conclusions"}, "questions": {"value": "- Why was **Surya OCR** excluded from the OCR evaluation? Given that Surya has demonstrated superior performance on Indic languages, its absence represents a significant gap in baseline coverage\n\n- Authors mention adversarial questions \"incorporate false assumptions\". What methodology ensured these questions were appropriately challenging? Even Gemini-2.5 scores only 5.79/10 on adversarial questions - does this indicate poor question design or fundamental model limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xbyxannz6g", "forum": "LmJoLn04iL", "replyto": "LmJoLn04iL", "signatures": ["ICLR.cc/2026/Conference/Submission19357/Reviewer_iVmL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19357/Reviewer_iVmL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939810931, "cdate": 1761939810931, "tmdate": 1762931292287, "mdate": 1762931292287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents IndicVisionBench, a large-scale benchmark designed to evaluate Vision-Language Models (VLMs) on culturally grounded and multilingual tasks focused on the Indian subcontinent. The benchmark comprises 5,000 unique images and over 37,000 question-answer pairs spanning 13 cultural topics across English and 10 Indian languages. The evaluation framework encompasses three primary tasks: Visual Question Answering (VQA) with six question types, Optical Character Recognition (OCR), and Multimodal Machine Translation (MMT). The authors evaluate models, including both proprietary systems (Gemini-2.5, GPT-4o) and open-source variants, revealing substantial performance gaps particularly for low-resource languages and culturally specific content.​"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Comprehensive Multi-task Framework**: The benchmark's tri-modal evaluation approach (VQA, OCR, MMT) provides a holistic assessment of VLM capabilities beyond simple question-answering. The inclusion of adversarial questions represents a good approach to probing deeper cultural knowledge beyond surface-level recognition.\n\n-  **Linguistic Coverage** : The benchmark covers 10 Indic languages with diverse scripts."}, "weaknesses": {"value": "- **Limited Methodological Novelty**:    \nWhile the cultural focus is valuable, the benchmark construction methodology largely follows established paradigms without introducing novel evaluation frameworks. The reliance on synthetic question generation using commercial models (Gemini-1.5-Flash, Gemini-2.5-Flash) raises concerns about potential biases inherited from these models.\n\n- **Self-Preferential Bias and Evaluation Circularity**:   \nThe most severe methodological flaw is the systematic use of Gemini models throughout the benchmark construction pipeline, followed by its evaluation on the same benchmark. Gemini-1.5-Flash and Gemini-2.5-Flash generate synthetic captions and QA pairs​ and translations across languages​. This creates a circular evaluation where Gemini is assessed on data it helped generate, violating fundamental evaluation principles established in contamination literature. The consistently superior performance of Gemini-2.5 across all tasks becomes suspect given this methodological flaw.​  \n\n- **Absence of Design Choice Validation**:   \nThe paper provides no systematic benchmarking for the choice of Gemini variants. While mentioning a \"small pilot study and cost considerations,\" no details, quality assessments, or comparative analysis with alternative generation models are provided. Table 6 shows only cost comparisons without quality metrics.  The benchmark's coverage appears skewed toward certain linguistic groups (Hindi at 26.8% of QA pairs) without adequate justification for this distribution.\n\n - **Complete Lack of Quality Control Statistics**:   \nCaption Quality: No validation of synthetic caption accuracy against image content using multimodal embeddings or human verification statistics​.  \nHuman Refinement Rates: The claim that \"Human reviewers refined all outputs for factual accuracy and cultural alignment\" provides zero statistics on edit rates, acceptance rates, or extent of modifications required​.  \nInter-Annotator Agreement: Despite being standard practice for evaluation benchmarks, no IAA scores or methodology details are provided.  \n\n- **Inadequately Documented Subset Selection**:  \n  - VQA-Indic: The selection of which subset from \"4K+ images\" was translated lacks stratification methodology, selection criteria, or coverage statistics​\n  - VQA-Parallel: The choice of 106 images for multilingual translation provides no justification for representativeness or selection rationale​\n \n- **Statistical Significance Testing**  \nThe paper lacks statistical significance testing for reported performance differences, provides no confidence intervals, and offers insufficient error analysis across the diverse evaluation scenarios. With Hindi dominating 26.8% of QA pairs and other languages having minimal representation, the paper lacks analysis of whether minority language subsets support reliable conclusions."}, "questions": {"value": "- Why was **Surya OCR** excluded from the OCR evaluation? Given that Surya has demonstrated superior performance on Indic languages, its absence represents a significant gap in baseline coverage\n\n- Authors mention adversarial questions \"incorporate false assumptions\". What methodology ensured these questions were appropriately challenging? Even Gemini-2.5 scores only 5.79/10 on adversarial questions - does this indicate poor question design or fundamental model limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xbyxannz6g", "forum": "LmJoLn04iL", "replyto": "LmJoLn04iL", "signatures": ["ICLR.cc/2026/Conference/Submission19357/Reviewer_iVmL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19357/Reviewer_iVmL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939810931, "cdate": 1761939810931, "tmdate": 1763263052826, "mdate": 1763263052826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a human-annotated multimodal benchmark for Indic languages. The benchmark includes tasks such as OCR, MT, and visual QA. The authors evaluated several vision-language models, including both open-source and commercial ones, and identified a notable performance gap."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work addresses an underrepresented language and provides a valuable resource for the multilingual AI community, particularly for the Indic community. In the current era of LLM-generated and synthetic data, I appreciate the thorough effort to involve humans in data creation. The resulting dataset covers multiple tasks and languages. The annotation guidelines and interface are also clearly described, further demonstrating the authors’ commitment to transparency."}, "weaknesses": {"value": "Annotator demography is missing, which will be useful information to add in this type of dataset."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YQLuDoPMr8", "forum": "LmJoLn04iL", "replyto": "LmJoLn04iL", "signatures": ["ICLR.cc/2026/Conference/Submission19357/Reviewer_f28F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19357/Reviewer_f28F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982686749, "cdate": 1761982686749, "tmdate": 1762931291456, "mdate": 1762931291456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the Western-centric bias in existing vision-language model evaluation benchmarks by introducing IndicVisionBench. This new benchmark is the first of its kind to focus on the cultural and linguistic diversity of the Indian subcontinent, covering English and 10 Indic languages. The benchmark is composed of 5K images and over 37K question-answer pairs, structured into three distinct tasks: VQA, OCR, and multimodal MT. The data curation process involved a combination of web crawling, crowdsourcing, and rigorous human verification to ensure cultural and linguistic fidelity. The authors evaluate eight prominent VLMs, from proprietary systems to open-weight models, and reveal significant performance deficits, particularly for low-resource languages and culturally nuanced queries. The analysis delves into regional biases, cross-lingual performance variations, and topic-specific capabilities, demonstrating the benchmark's utility in uncovering the limitations of current state-of-the-art multimodal systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors' motivation for this work isintuitive They address a well-known but often-neglected limitation in our field: the Western-centric nature of most vision-language evaluation benchmarks. The paper makes a very convincing argument for the necessity of developing resources that capture greater cultural and linguistic diversity, a direction of research that is becoming increasingly critical.\n\n- A major strength of this paper is the comprehensive design of IndicVisionBench. It is not limited to a single task but integrates three distinct multimodal evaluations (VQA, MMT, and OCR) especially the MT part, which makes it a very comprehensive resource. The VQA component is especially well-designed, featuring six different question formats. The use of \"adversarial questions\" to probe for the ability to reject false cultural assumptions is a particularly novel and insightful method for assessing model robustness.\n\n- The experimental evaluation presented is comprehensive, covering a wide range of current vision-language models. Importantly, the analysis goes beyond a simple ranking of models. The authors provide valuable insights through detailed breakdowns of performance, for example by investigating regional-language biases and performance variations across different cultural topics."}, "weaknesses": {"value": "- There is a potential for bias in the data generation process. The VQA pairs were first generated using Gemini models and then corrected by human annotators. While this is a practical and common approach, it introduces a risk of \"self-enhancement\" bias, where the models used for generation might produce content that is easier for them to evaluate later. The paper would be strengthened by acknowledging and discussing this potential limitation.\n\n- The evaluation of open-ended questions relies on GPT-4o as a judge. Such judges can have their own biases and may not fully capture all cultural nuances, introducing a layer of uncertainty. It would be beneficial to include a brief discussion on these known limitations. Reporting inter-annotator agreement from a small subset of human judges, to calibrate the LLM judge's performance, would also add significant value.\n\n- The benchmark is presented as \"large-scale,\" but the amount of data for some specific languages and tasks is quite small. For instance, the VQA-Parallel and MMT tracks are built upon only 106 images. For some of the lowest-resource Indic languages, the number of samples is also limited. This can affect the statistical significance of the conclusions for these specific languages. The authors should be cautious with making very strong claims about performance on languages with a small number of samples.\n\n- A very interesting point is made in Section 6 regarding the limitations of WER/CER metrics for OCR evaluation, and the authors make a good case for using ANLS instead. This discussion is valuable. However, it appears quite late in the paper. The impact would be greater if this challenge were introduced earlier, in the experimental setup (Section 4), to provide better context for the results presented in Table"}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c08PsOp9GT", "forum": "LmJoLn04iL", "replyto": "LmJoLn04iL", "signatures": ["ICLR.cc/2026/Conference/Submission19357/Reviewer_HKVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19357/Reviewer_HKVC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990211934, "cdate": 1761990211934, "tmdate": 1762931291086, "mdate": 1762931291086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}