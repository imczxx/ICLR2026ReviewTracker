{"id": "jEFKRMLwff", "number": 15352, "cdate": 1758250480277, "mdate": 1763733784149, "content": {"title": "Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT", "abstract": "Understanding the physical world—governed by laws of motion, spatial relations, and causality—poses a fundamental challenge for multimodal large language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o demonstrate impressive perceptual and reasoning capabilities, our investigation reveals these models struggle profoundly with visual physical reasoning, failing to grasp basic physical laws, spatial interactions, and causal effects in complex scenes. More importantly, they often fail to follow coherent reasoning chains grounded in visual evidence, especially when multiple steps are needed to arrive at the correct answer. To rigorously evaluate this capability, we introduce MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual chain-of-thought (CoT). Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. This setup mirrors how humans reason through real-world physical processes over time. To ensure fine-grained evaluation, we introduce a graph-based CoT consistency metric that verifies whether the reasoning path of model adheres to valid physical logic. Additionally, we minimize shortcut exploitation from text priors, encouraging models to rely on visual understanding. Experimental results reveal a concerning trend: even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. Surprisingly, RL-based post-training alignment—commonly believed to improve visual reasoning performance—often harms spatial reasoning, suggesting a need to rethink current fine-tuning practices.", "tldr": "MVPBench: a benchmark for multi-image reasoning with step-by-step visual grounding", "keywords": ["Spatial Reasoning; Physics Benchmark; Chain of Thought"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/642c9d29cde4135d4ac980103bd8b2d39bc1aef4.pdf", "supplementary_material": "/attachment/53a2ea9dece8b4c138d82090e1c5f194265b0773.zip"}, "replies": [{"content": {"summary": {"value": "This paper tries to address the problem: even state-of-the-art MLLMs like GPT-4o and O3 fail at visual physical reasoning. To address the shortcomings of existing benchmarks (e.g., unrealistic data, text shortcuts), the authors propose MVPBench, a new benchmark based on real-world physics scenarios featuring multi-image inputs (visual CoT) and multi-path correct answers. Furthermore, the paper introduces a graph-based evaluation suite to assess the process of reasoning across quality, diversity, and efficiency. A key finding is that RL-based alignment post-training (like MPO) can actively harm the model's spatial reasoning abilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The target issues of the paper are meaningful and worth exploring and the motivation is clear.\n2. The paper is well written and easy to follow.\n3. The design of MVPBench directly targets the weaknesses of current benchmarks, forcing models to ground their reasoning in dynamic visual evidence rather than relying on linguistic priors.\n4. The discovery that RL post-training impairs physical reasoning is a significant and non-obvious contribution that has important implications for future MLLM alignment strategies."}, "weaknesses": {"value": "1. The new graph-based CoT evaluation metric (Sec 4.2) is excessively complex. It relies on Graph Edit Distance (GED) and multiple hand-tuned hyperparameters, making the evaluation difficult to reproduce and its superiority over simpler metrics unproven.\n2. Key metrics for CoT Quality and Efficiency depend on GPT-4o as the judge. This is highly circular, as GPT-4o is one of the models being evaluated, and this method may systematically favor models with a similar style to GPT-4o.\n3. The benchmark's scale is relatively small (1,211 samples). More importantly, the data is scraped from sources like Bilibili and Gaokao exams (Sec 3.1), raising concerns about annotation quality and noise that are not discussed.\n4. The \"multi-path CoT\" is a core feature, but this annotation is extremely expensive and subjective (how to define \"all\" valid paths?). The paper does not report Inter-Annotator Agreement for this process, making the reliability and scalability of this feature questionable."}, "questions": {"value": "1. Line 988, Chinses -> Chinese\n2. The claim that \"RL post-training... harms spatial reasoning\" is very strong. The evidence is limited to a few models, which may be an artifact of specific RL implementations (like MPO) rather than a fundamental flaw of RL itself. Can authors discuss more about it?\n3. The human performance evaluation is very weak. It only assesses the \"diversity\" metric, and the subjects were given the key steps, which is not comparable to the model's task of generating CoT from scratch. This makes the human scores in Table 5 not very meaningful.\n4. See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RMoAzzTI8y", "forum": "jEFKRMLwff", "replyto": "jEFKRMLwff", "signatures": ["ICLR.cc/2026/Conference/Submission15352/Reviewer_xjhL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15352/Reviewer_xjhL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442863000, "cdate": 1761442863000, "tmdate": 1762925639843, "mdate": 1762925639843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MVPBench, a benchmark designed to evaluate the visual physical reasoning abilities of multimodal large language models (MLLMs). It contains 1,211 examples across physics experiments, physics problems, spatial relations, and dynamic prediction, each annotated with multiple valid chain-of-thought (CoT) paths. To evaluate the reasoning process, the authors propose a graph-based CoT evaluation framework from three dimensions: quality, diversity, and efficiency. Experiments show that multi-image inputs improve reasoning, while reinforcement learning–based post-training surprisingly reduces general reasoning performance, indicating the misalignment between fine-tuning and genuine physical understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper jointly assesses CoT quality, diversity, and efficiency, enabling a fine-grained measurement of model behavior.\n- MVPBench provides multiple annotated reasoning paths for each question. This design better reflects real-world human problem-solving patterns and allows graph-based metrics.\n- The benchmark removes the textual cues to force the model to use visual evidence for reasoning. This helps reduce the text priors as a shortcut and isolates the visual reasoning ability."}, "weaknesses": {"value": "- Figure 1 incorrectly labels GPT-4o’s Step 1 as wrong. However, step 1 in textual CoT is wrong but is considered correct.\n- The first subsection in the related work, “Limitations of Multi-modal Large Language Models”, is too broad and unfocused. The authors only discuss the limitations of physical understanding.\n- The same symbol \\alpha is reused in both CRS and Path Coverage Scores without clear differentiation. Moreover, the paper first uses “DAG-based matching” (L299–300) before defining.\n- Although multi-image input improves performance for most models, the paper does not analyze the reasons. Besides, why the QVQ-72B model uniquely degrades with more images still remains unclear.\n- The weighting coefficient hyperparameters are fixed without sensitivity testing. This limits the confidence in metric stability across different setups.\n- The claim that RL post-training harms spatial reasoning is attributed to distributional bias or overfitting, but no empirical evidence is presented to support this hypothesis.\n- The dataset is unbalanced. It consists of only 100 Dynamic Prediction problems. Additionally, half of the Physics Experiments are Mechanics problems. This may bias performance averages and limit conclusions on underrepresented subdomains.\n- The SAS and SRS metrics rely heavily on GPT-4o for evaluation, which not only requires high computational and financial cost but may also introduce evaluation bias, potentially affecting the reliability of the assessment."}, "questions": {"value": "- Could the authors correct Figure 1 and clarify why GPT-4o’s reasoning is considered wrong? Also, please ensure consistent weight notation and define DAG before its first use.\n- Could the authors refine the first subsection in Related Work on a more specific scope?\n- Could the authors analyze the reasons for performance improvement from multi-image inputs and the degradation in QVQ-72B with empirical evidence?\n- Could the authors conduct experiments to confirm the cause of the generalizability reduction of RL post-training in physical visual reasoning?\n- Could the authors include a sensitivity analysis of the weighting hyperparameters to assess the robustness of the proposed metrics?\n- Given the unbalanced dataset, could the authors evaluate whether the performance remains consistent when balanced subsets are sampled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iP1xyRLwW3", "forum": "jEFKRMLwff", "replyto": "jEFKRMLwff", "signatures": ["ICLR.cc/2026/Conference/Submission15352/Reviewer_t9yx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15352/Reviewer_t9yx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941064354, "cdate": 1761941064354, "tmdate": 1762925638857, "mdate": 1762925638857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the limitations of multimodal large language models (MLLMs) in performing visual physical reasoning—understanding and reasoning about motion, spatial relations, and causality from visual inputs. Despite recent advancements exemplified by models such as OpenAI o3 and GPT-4o, the study finds that these systems exhibit fundamental weaknesses in applying physical laws and maintaining coherent, visually grounded reasoning chains, particularly across multi-step inference tasks. To address this gap, the authors introduce MVPBench, a benchmark designed to evaluate visual chain-of-thought (CoT) reasoning using multi-image inputs that require temporally grounded, step-by-step reasoning. A graph-based CoT consistency metric is proposed to assess the logical validity of models’ reasoning processes while mitigating reliance on textual shortcuts. Experimental evaluations reveal that even state-of-the-art MLLMs demonstrate poor performance in visual physical reasoning and weak image–text alignment, and that reinforcement learning–based alignment methods may inadvertently impair spatial reasoning. These findings underscore the need to revisit current fine-tuning practices for multimodal reasoning models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The introduction of MVPBench, a new benchmark for evaluating visual physical reasoning with multi-image inputs and a graph-based consistency metric, provides a rigorous framework for assessing multimodal models’ reasoning capabilities.\n- The paper offers valuable insights into the current weaknesses of state-of-the-art models in visual physical reasoning, particularly in understanding basic physical laws and spatial interactions, and challenges existing practices in reinforcement learning-based alignment."}, "weaknesses": {"value": "- In Table 3, it appears that closed-source LLMs generally outperform their open-source counterparts. This raises the question of whether the advanced reasoning capabilities of new models are already (or partially) addressing the tasks in question. To investigate, I reviewed the example in Figure 1 for both GPT-5 (thinking) and Gemini-Pro 2.5, and found that both models were able to correctly answer the question. Notably, Gemini-Pro 2.5 even demonstrated the ability to correctly identify the movement trajectory in the image.\n- Some insights presented in the paper seem to echo findings from previous studies. For example, the claim that \"Providing models with the full image sequence boosts performance by up to 21% points-evidence that temporal context matters\" partially aligns with \"thinking with images.\" that once models are capable of processing multiple images, leveraging more images for reasoning should enhance performance across many multimodal tasks.\n- Given the title of the submission, \"Visual Physical CoT\", I had anticipated a broader exploration of \"physical content\" beyond simple physics experiments. For instance, it would be valuable to see reasoning applied to more complex dynamic systems, such as those found in earth science (e.g., weather prediction), or other scientific domains, where physical reasoning plays a crucial role in understanding complex phenomena."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "viQCv0BBDS", "forum": "jEFKRMLwff", "replyto": "jEFKRMLwff", "signatures": ["ICLR.cc/2026/Conference/Submission15352/Reviewer_nCwg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15352/Reviewer_nCwg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095304259, "cdate": 1762095304259, "tmdate": 1762925638529, "mdate": 1762925638529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MVPBench a benchmark to assess the ability of the multimodal large language models to reason about how physical world works (such as motion and causality). MVPBench includes different physical problem tasks, and includes assessing the reasoing steps (cots) rather than only the final answer. Each example includes multiple valid reasoning paths, and the authors propose a graph-based evaluation method that checks whether a model’s thought process follows physically consistent logic. Testing a wide range of open- and closed-source models, they find that even top systems struggle with spatial and causal understanding, and that reinforcement learning–based fine-tuning can actually make it worse."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Evaluating the reasoning steps (and not just the final answer) is an important aspect of reasoning capabilities assessment. The authors proposed method and benchmark fills this important gap.\n- Annotating and manually checking the reasoning steps is a time consuming task for more than 1k samples in the becnhmark.\n- The experiment results on reinforcement learning–based fine-tuned models is interesting."}, "weaknesses": {"value": "Although the benchmark and tasks seem interesting, I am not convinced of the actual quality of the samples, which is the most crucial aspect of a benchmark. For instance, the very first example shown in the paper (Figure 1) seems to be flawed. The correct answer is that the bus is moving downwards (so the front is at the bottom) but the step_1 in Textual CoT says \"the tail is near the\nbottom of the picture\". Also, Step 1 in Model reasoning (GPT-4o), correctly says \"The front of the\nvehicle is at the bottom part of the vehicle (closer to the camera).\" but is marked as incorrect.\n\nIn other examples of Figure 2, the questions do not seem to be clear, and the answers too. For instance, the Physical State example, \"What results from A and B contacting C– F under gravity?\" is very confusing even for humans. \n\nOverall, although the focus area of paper is important, the execution is not satisfactory."}, "questions": {"value": "Could you please provide more quantitive details on how the manual annotations of CoTs where performed? How many hours was spent on it, and by how many people each sample was manually cross checked? \n\nProviding a human performance baseline for the tasks is crucial, as it will determine the actual do ability and quality of the benchmark."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "This work includes a team of 31 human annotators."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DwzEOSTMD3", "forum": "jEFKRMLwff", "replyto": "jEFKRMLwff", "signatures": ["ICLR.cc/2026/Conference/Submission15352/Reviewer_8qam"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15352/Reviewer_8qam"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153734915, "cdate": 1762153734915, "tmdate": 1762925638178, "mdate": 1762925638178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}