{"id": "Q8HRE2E5wp", "number": 24977, "cdate": 1758362703837, "mdate": 1759896739684, "content": {"title": "Can Recommender Systems Teach Themselves? A Recursive Self-Improving Framework with Fidelity Control", "abstract": "The scarcity of high-quality training data presents a fundamental bottleneck to scaling machine learning models. This challenge is particularly acute in recommendation systems, where extreme sparsity in user interactions leads to rugged optimization landscapes and poor generalization. We propose the Recursive Self-Improving Recommendation (RSIR) framework, a paradigm in which a model bootstraps its own performance without reliance on external data or teacher models. RSIR operates in a closed loop: the current model generates plausible user interaction sequences, a fidelity-based quality control mechanism filters them for consistency with true user preferences, and a successor model is retrained on the enriched dataset. Our theoretical analysis shows that RSIR acts as a data-driven implicit regularizer, smoothing the optimization landscape and guiding models toward more robust solutions. Empirically, RSIR yields consistent, cumulative gains across multiple benchmarks and architectures. Notably, even smaller models benefit, and weak models can generate effective training curricula for stronger ones. These results demonstrate that recursive self-improvement is a general, model-agnostic approach to overcoming data sparsity, suggesting a scalable path forward for recommender systems and beyond. Our anonymized code is available at https://anonymous.4open.science/status/RSIR-7C5B.", "tldr": "", "keywords": ["Self-improving; Recommendation System; Data Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3cf9423a98df87df8aeb6580e53c3fa60427fc47.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the RSIR framework to address data sparsity in recommender systems, enabling models to self-improve via a closed loop—generating interaction sequences, filtering them with fidelity control, and retraining on enriched data without external reliance. Theoretically, RSIR acts as a data-driven implicit regularizer to smooth the optimization landscape ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. RSIR significantly addresses the core issue of extreme data sparsity in recommender systems. \n\n2. Unlike traditional methods that rely on external curated data (expensive and domain-specific) or teacher models (risk of distribution mismatch), RSIR operates in a closed loop. It leverages the current model’s own understanding of user preferences to generate and filter synthetic sequences, eliminating reliance on external resources ."}, "weaknesses": {"value": "1. The iterative self-improvement loop involves repeated steps of sequence generation (generating m synthetic sequences per user), fidelity filtering, and model retraining. For large datasets or multiple iterations, this process increases computational costs — e.g., generating 20 synthetic sequences per user (m=20) and retraining models from scratch each iteration adds significant time and resource burdens .\n\n2. Why does each iteration require the recommender system to be retrained from scratch? Why not employ incremental training? In practical industrial settings, incremental training is the more common practice, since retraining from scratch is prohibitively expensive."}, "questions": {"value": "1. In each iteration, when randomly sampling from the candidate item set, does the set include all items from the training split or all items from the entire dataset?\n\n2. In real-world deployments, the candidate pool is enormous. With random sampling, is there a risk of not retrieving items that satisfy the quality-control criteria? Alternatively, would many sampling attempts be required to find a suitable item, thereby greatly increasing computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ouuPnJV6za", "forum": "Q8HRE2E5wp", "replyto": "Q8HRE2E5wp", "signatures": ["ICLR.cc/2026/Conference/Submission24977/Reviewer_jDuY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24977/Reviewer_jDuY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589146553, "cdate": 1761589146553, "tmdate": 1762943269164, "mdate": 1762943269164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of extreme user interaction sparsity in recommender systems, which leads to optimization difficulty and poor generalization. The authors propose a Recursive Self-Improving Recommendation (RSIR) framework that enables a model to continuously improve itself without external data or teacher models. RSIR operates by generating plausible user–item interaction sequences with the current model, filtering them through a fidelity-based quality control mechanism to retain only high-quality samples aligned with real user preferences, and using these to train the next-generation model. Experimental results on multiple benchmarks and architectures demonstrate consistent and cumulative performance improvements across iterations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The user sparsity issue in recommender systems is important and highly relevant to the community.\n* The proposed three-stage “generate–filter–retrain” pipeline is conceptually simple, broadly applicable, and uses transparent hyperparameters, which facilitates reproducibility.\n* The authors conduct extensive experiments, showing improvements on multiple datasets and model backbones, with multi-round benefits until convergence or saturation."}, "weaknesses": {"value": "* The fidelity-based quality control mechanism is tightly coupled to the *future ground truth*, which may limit generalization and realism.\n\n  * Specifically, the filter accepts a generated step only if, after insertion, at least one *real* future item remains highly ranked (≤ τ).\n  * This effectively constrains the generation process to small perturbations around the known future sequence, making many “new sequences” closer to alternative prefixes of the same label rather than genuinely novel user trajectories.\n  * It would strengthen the work if the authors could propose ways to address this issue or introduce alternative criteria that decouple fidelity assessment from future ground truth.\n* The baseline coverage is relatively narrow. Current comparisons focus only on simple insertion and reordering augmentations, but stronger and more recent baselines—such as data regeneration, diffusion-based, or generative augmentation methods that actively synthesize sequential data—are missing.\n* The theoretical discussion is mostly intuitive and lacks formal guarantees or conditions.\n\n  * The idea of “implicit regularization” or “landscape smoothing” is interesting but remains qualitative; no formal bounds or conditions are provided to clarify when RSIR helps versus hurts.\n  * Moreover, the technical components (especially *self-training*) have already been explored in other recommendation contexts. Since the novelty here largely lies in the generation criterion, a deeper and more rigorous theoretical analysis would be crucial to justify the claimed insights. I strongly suggest the authors expand or formalize the theoretical section in their rebuttal.\n* Efficiency reporting is missing.\n  The approach involves multi-round training and parameter searches, but no details on training time, computational cost, or resource consumption are provided. Such analysis would be helpful for assessing scalability and practical feasibility."}, "questions": {"value": "Please refer to the weaknesses above, particularly regarding:\n\n* How to mitigate the reliance of fidelity control on future ground truth;\n* Whether stronger generative baselines can be added;\n* How the theoretical section could be strengthened with formal analysis or guarantees;\n* And what the training cost and computational overhead are for multi-round RSIR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TNAwyi17ih", "forum": "Q8HRE2E5wp", "replyto": "Q8HRE2E5wp", "signatures": ["ICLR.cc/2026/Conference/Submission24977/Reviewer_Rqbe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24977/Reviewer_Rqbe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827039778, "cdate": 1761827039778, "tmdate": 1762943268821, "mdate": 1762943268821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce RSIR (Recursive Self-Improving Recommendation), a framework that enables recommender models to iteratively generate, filter, and retrain on their own synthetic data—improving performance without external supervision.\n\nEach iteration involves: (1) generating user–item sequences, (2) filtering them via a fidelity-based control mechanism to ensure alignment with real user behavior, and (3) retraining a new model on the enriched dataset. Theoretically, RSIR functions as a data-driven implicit regularizer, smoothing the loss landscape and enhancing generalization. Experiments across multiple datasets and model backbones show consistent, cumulative gains over heuristic data augmentation methods such as reordering and insertion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. RSIR is the first recursive self-improving framework for recommender systems that does not rely on external models or knowledge.\n2. The paper provides a theoretical interpretation of RSIR, showing it acts as an implicit regularizer.\n3. The experiments are extensive, covering multiple datasets and backbone models, with ablation studies on fidelity control, exploration balance, and hyperparameter sensitivity."}, "weaknesses": {"value": "1. The paper does not clearly explain why existing self-improving methods based on external models (e.g., LLMs) or knowledge are not included for comparison. Even if such exclusion is justified, it would still be valuable to provide comparative results to quantify the benefits (or trade-offs) of using external knowledge.\n2. A possible motivation for avoiding external models is computational efficiency, yet the proposed method appears computationally expensive. First, it involves tuning several sensitive hyperparameters via grid search (Line 303-304), which is resource-intensive. Second, the model is retrained \"from scratch\" on the updated dataset in each iteration, which is especially costly for large backbones. What about fine-tuning with incremental data rather than retraining? Repeated full retraining is often impractical in real-world settings. In fact, the total optimization time may exceed that of methods relying on external models or knowledge. However, the paper provides no analysis of runtime or computational complexity.\n3. The explanation of Figure 3 (model performance vs. τ) appears inaccurate or overstated. The trend does not clearly follow a U-shaped curve; for example, performance at τ=10 is substantially worse than at τ=5. As such, the claim is unconvincing and may require revision."}, "questions": {"value": "1. Please address weakness 1\n2. Please address weakness 2\n3. Please address weakness 3\n4. In Line 219, RSIR filters duplicates. However, doesn't exploitation naturally produce sequences that overlap with historical items? Please clarify how duplicates are defined and handled."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G06EQWDWfd", "forum": "Q8HRE2E5wp", "replyto": "Q8HRE2E5wp", "signatures": ["ICLR.cc/2026/Conference/Submission24977/Reviewer_hwWB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24977/Reviewer_hwWB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974353438, "cdate": 1761974353438, "tmdate": 1762943268529, "mdate": 1762943268529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Recursive Self-Improving Recommendation (RSIR) framework, which addresses the challenge of data scarcity in recommender systems by enabling models to bootstrap their performance through iterative self-improvement. In each iteration, the model generates plausible user interactions, filters them for consistency with true user preferences via a fidelity-based quality control mechanism, and retrains itself using the enriched dataset. The paper demonstrates the theoretical advantages of RSIR as an implicit regularizer and provides empirical evidence that the framework improves performance across various datasets and architectures. Notably, even weak models benefit from this self-improvement process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The introduction of a recursive self-improvement loop in the context of recommender systems is a novel idea and a creative approach to tackling data sparsity. The idea of bootstrapping performance through self-generated data without the need for external models or data is a significant innovation.\n\n- The experiments are thorough and cover multiple benchmark datasets. The use of multiple backbone models (SASRec, CL4SRec, HSTU) demonstrates the framework’s versatility, and the comparisons against heuristic-based augmentation methods provide strong evidence of RSIR’s effectiveness.\n\n- The theoretical analysis of RSIR as an implicit regularizer adds depth to the work and makes the proposed framework more compelling."}, "weaknesses": {"value": "- While the recursive loop is theoretically sound, the practical implementation of such a system might face challenges, especially regarding error amplification in early iterations. The paper discusses the importance of fidelity control, but there may be further considerations regarding the computational complexity and convergence behavior of the loop over many iterations.\n\n- While the fidelity-based quality control mechanism is crucial for preventing performance collapse, a more in-depth discussion of how the parameters (like the rank threshold) are chosen could improve understanding. The paper touches on this, but further details would be helpful for practitioners looking to implement the system.\n\n- While NDCG and Recall are appropriate, additional evaluation metrics such as precision or F1-score could provide a more comprehensive view of the performance, especially in diverse scenarios."}, "questions": {"value": "1. Could the authors elaborate on how the fidelity threshold is selected and whether there are any recommendations for choosing it based on dataset characteristics?\n\n2. How would RSIR behave in cases where there is a high degree of noise in the training data? Would the model still be able to self-correct, or could error propagation become more severe?\n\n3. Are there any limitations in the framework when applied to real-time recommendation systems with dynamic data (e.g., streaming data)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4XEMKFHcgq", "forum": "Q8HRE2E5wp", "replyto": "Q8HRE2E5wp", "signatures": ["ICLR.cc/2026/Conference/Submission24977/Reviewer_tgHh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24977/Reviewer_tgHh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994514579, "cdate": 1761994514579, "tmdate": 1762943268280, "mdate": 1762943268280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}