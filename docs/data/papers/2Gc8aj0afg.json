{"id": "2Gc8aj0afg", "number": 22402, "cdate": 1758330602215, "mdate": 1759896868345, "content": {"title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models", "abstract": "Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for *reasoning faithfulness*, defined by two testable conditions: *stance consistency* (a coherent stance linking reasoning to answer) and *causal influence* (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present **RFEval**, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7\\% of outputs, predominantly from post-intervention stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with training paradigms than scale: hybrid pipelines combining diverse supervised fine-tuning with reinforcement learning are more faithful, while size alone is not predictive. Crucially, *accuracy is neither necessary nor sufficient for faithfulness*: once controlling for model and task, the accuracy–faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process.", "tldr": "RFEval introduces a benchmark and evaluation framework that probes Large Reasoning Models with counterfactual reasoning interventions to measure reasoning faithfulness—via stance consistency and causal influence—separately from final-answer accuracy.", "keywords": ["Faithfulness", "Large Reasoning Models", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d32c550065b16409fd3ec89444bb4f8d64375932.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper defines reasoning faithfulness in terms of stance consistency and causal influence, independent of accuracy. It introduces RFEval, a 7,186-instance benchmark across seven reasoning tasks that uses counterfactuals to test model responses. Evaluation of 12 open-source models shows unfaithfulness in about half of the cases, especially due to stance inconsistency. Math and code tasks are most challenging, and training methods impact faithfulness more than model size. Accuracy alone is not a reliable indicator of faithfulness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. An interesting benchmark for evaluating the faithfulness of LRMs and a clear formalization of reasoning faithfulness (stance consistency and causal influence) are presented in the paper.\n2. This paper constructs a broad, heterogeneous benchmark (7 tasks, 7,186 items), enabling cross-family, cross-task patterns."}, "weaknesses": {"value": "1. The definition and computation of Stance Consistency seem misaligned with how large reasoning models (LRMs) behave on complex tasks. In a genuine reasoning process, each step should not be assumed to be consistently aligned with the last step. For example, as mentioned in DeepSeek-R1, LRMs often experience \"aha moments,\" where they explore different reasoning paths and self-correct after encountering errors. This is generally regarded as a normal behavior for LRMs on complex problems. However, the notion of Stance Consistency as used in this paper appears more suitable for low-difficulty tasks where reasoning is simple and linear, with little trial-and-error or backtracking. This is exemplified by the use of GSM8K in the Math portion, where the problems are relatively straightforward and the reasoning path is direct.\n2. Results emphasize that hybrid SFT+RL pipelines correlate with higher RF, but comparisons span different families, data mixtures, and training details. Without controlled ablations (same base model, held-out data), causal interpretations about the training recipe remain tentative and unreliable.\n3. The scope is limited to open-source LRMs. Although the authors argue that evaluating closed-source models is challenging, this limitation represents a notable weakness of the benchmark and appears to stem from its design choices."}, "questions": {"value": "1. Since the externally injected reasoning is not generated by the model itself, the evaluation reflects whether the model adheres to the external reasoning rather than to its own reasoning process. This mismatch raises concerns about the validity of the conclusions drawn. How significant is the impact of this mismatch? For instance, what would the results look like if we evaluate reasoning faithfulness directly on the original model outputs (without considering the injected reasoning r') rather than on the counterfactual contrast?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "inQd7KIItA", "forum": "2Gc8aj0afg", "replyto": "2Gc8aj0afg", "signatures": ["ICLR.cc/2026/Conference/Submission22402/Reviewer_WEsV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22402/Reviewer_WEsV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713767645, "cdate": 1761713767645, "tmdate": 1762942204296, "mdate": 1762942204296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RFEval, a benchmark designed to evaluate the reasoning faithfulness of Large Reasoning Models (LRMs). It defines reasoning faithfulness based on two criteria: (1) the model maintains a coherent stance throughout its output, and (2) the reasoning causally determines the final answer. The benchmark assesses faithfulness by inserting flawed steps into the models’ reasoning traces. Experiments across 7 tasks and 12 LRMs reveal that a large proportion of intervened outputs are unfaithful, and that accuracy is only weakly correlated with faithfulness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper addresses reasoning faithfulness, an important yet underexplored topic. The proposed approach, probing faithfulness by performing intervention to the reasoning traces, is original.\n2.\tThe authors conduct extensive experiments and analyze the results from multiple perspectives, providing a comprehensive evaluation."}, "weaknesses": {"value": "1.\tOverall, I am not convinced that interventions on reasoning traces truly measure model faithfulness. As the authors note, “a faithful explanation should reflect a model’s internal reasoning process.” However, modifying the output reasoning trace is not equivalent to altering the internal reasoning process and may instead confuse the model. If the model does not believe the inserted reasoning, it may fail to continue coherently, but this does not necessarily mean it would be unfaithful when reasoning autonomously.\n2.\tThe definition of the causal influence metric lacks clarity. In Equation (3), reasoning is said to have causal influence if either the stance of the reasoning or the answer changes. However, if the reasoning stance changes but the answer stance does not (or vice versa), it should not be considered causally influential or faithful.\n3.\tIf I understand correctly, the flaw identification in Section 4 corresponds to the measurement of stance consistency. This evaluation is conducted by LLMs, but their agreement with human judgments is not very high. This discrepancy may undermine the reliability of the reasoning faithfulness scores, especially since the analysis indicates that unfaithfulness is mainly driven by stance consistency failures."}, "questions": {"value": "1.\tWhat is the contrast coverage in Table 2 and what does it signify?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wfkXNpQfk7", "forum": "2Gc8aj0afg", "replyto": "2Gc8aj0afg", "signatures": ["ICLR.cc/2026/Conference/Submission22402/Reviewer_Zx74"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22402/Reviewer_Zx74"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799971534, "cdate": 1761799971534, "tmdate": 1762942203838, "mdate": 1762942203838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RFEval, a new benchmark and formal framework for evaluating reasoning faithfulness in Large Reasoning Models. Faithfulness is defined via two testable criteria: stance consistency and causal influence. The authors construct 7,186 instances across seven diverse tasks and evaluate 12 open-source LRMs using output-level counterfactual interventions by injecting flawed but plausible reasoning into the model’s own thought process and observing whether the model’s answer shifts coherently."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "a. The paper provides a clear, operational definition of reasoning faithfulness grounded in causal influence and logical coherence.\n\nb. RFEval is carefully built with human-reviewed, subtly flawed counterfactual reasoning across diverse domains (math, code, law, etc.), enabling fine-grained diagnostics.\n\nc. This study performs large-scale evaluation of 12 open-source LRMs across 7 tasks."}, "weaknesses": {"value": "a. I personally find Section 2 hard to follow. Maybe the authors can add some examples to explain the idea.\n\nb. Answer for Q3 in Section 5 is not convincing, as the models use different architectures and are trained on different data. Training method may not be the only factor influencing reasoning faithfulness.\n\nc. The reasoning traces for the evaluation of faithfulness are different between models since they are filtered beforehand. The comparison is potentially not fair since they are evaluated with different question in the dataset."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "h4d8cUZg99", "forum": "2Gc8aj0afg", "replyto": "2Gc8aj0afg", "signatures": ["ICLR.cc/2026/Conference/Submission22402/Reviewer_JhME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22402/Reviewer_JhME"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811723449, "cdate": 1761811723449, "tmdate": 1762942203480, "mdate": 1762942203480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a formalization of reasoning faithfulness, RF (stance consistency + output‑level causal influence) and introduces RFEval, a 7,186‑instance benchmark spanning seven tasks to test whether injected counterfactual reasoning r′ coherently changes a model’s reasoning and/or answer. Evaluating 12 open‑source LRMs, the authors find \n- ~50% unfaithfulness overall,\n- failures concentrated in post‑intervention stance inconsistency,\n- training recipe correlates with RF more than size, and\n- accuracy is neither necessary nor sufficient for RF once model and task are controlled."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Tackles a timely question of reasoning output quality and consistency\n- Clear, testable behavioral definition of faithfulness separate from accuracy; simple, interpretable metrics.\n- Broad multi‑task coverage and 12‑model comparison; informative diagnostics by transition location and causality type."}, "weaknesses": {"value": "A. Granularity gap: Despite formal step‑wise notation, implementation evaluates coarse components (r/e/a), not per‑step CoT causality; this undermines a key motivation/contribution. \n\nB. Right‑censoring: Heavy reliance on contrast‑conditional filtering (δ=1) and exclusion of truncated or malformed outputs creates informative censoring; cross‑model comparability is not fully addressed.\n \nC. Evaluator bias/Confoundness :  A single judge with low recall on flaw identification underpins major conclusions about stance consistency.  Differences in prompts/decoding and coverage confound the hybrid vs RL‑heavy analysis. (Major Issue)\n \nE. Weak Claim: “Accuracy is neither necessary nor sufficient for faithfulness.” Figure 5 reports weak, non‑significant association after controlling for model and task. However, the experimental setup is riddled with confounding variables, mainly LLM errors.\n \nF. Comparison with previous work: Please contrast and cite your work against:\n \na. DeYoung etal., 2020 (ERASER): end‑task evaluation with deletion/insertion tests for rationales. \nb. Wiegreffe & Marasović, 2021: comprehensive critique of faithfulness vs plausibility in NLP explanations. \nc. Hase & Bansal, 2020 and Pruthi etal., 2020: methodology for evaluating whether rationales cause predictions under perturbations.\nd. Xu et al 2025 (Re-Imagine: Symbolic Benchmark Synthesis for Reasoning Evaluation): counterfactual based reasoning benchmark.\ne. On the causal side, the related literature on causal tracing / representation‑level interventions (e.g., patching internal activations) is not discussed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PTbknwmDuC", "forum": "2Gc8aj0afg", "replyto": "2Gc8aj0afg", "signatures": ["ICLR.cc/2026/Conference/Submission22402/Reviewer_4J4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22402/Reviewer_4J4o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971439770, "cdate": 1761971439770, "tmdate": 1762942203017, "mdate": 1762942203017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}