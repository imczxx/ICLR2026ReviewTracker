{"id": "x7aLhLMVn1", "number": 6303, "cdate": 1757965960471, "mdate": 1759897923614, "content": {"title": "Multiplayer Nash Preference Optimization", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradleyâ€“Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. \nIn this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. \nOur framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences.", "tldr": "", "keywords": ["Preference Optimization", "RLHF"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36c1d892d45f55c7a45260f1af43226782ecfd56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends the field of reinforcement learning from human feedback and alignment of LLMs.\nTheir primary assumption is that reward models are limited in expressing the complexity and diversity of human preferences therefore more general preferences models should be considered. In particular, they model n-wise comparisons instead of pairwise comparisons when one completions out of a pool of n is chosen as the best response by the LLM for a given prompt.\nSince this is a non-transitive model, standard reward models can't be used and the authors formulate the problem as a game between n players and aim to approximate the Nash Equilibrium. However, they model generalises to settings when reward models are adequate choice for human preferences.\nThe proposed algorithm, MNPO, builds on online mirror descent that provides on average convergence guarantee and follow the previous line of work on Nash Learning from Human Feedback (Munos et al. 2023) and extends it to time-dependent opponent selection (TD-MNPO).\nTheir experimental results show consistently better performance than comparable algorithms and results on par with open-weight and closed-source models on standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the paper lies in its generality as highlighted in Table 1 furthermore in the time-dependent opponent selection that extends on the standard OMD approach.\nThe experimental results look promising and the comprehensive evaluation on a wide range of benchmarks is appreciated, however, I have some questions as discussed below."}, "weaknesses": {"value": "I found the following points unclear and potential weaknesses of the paper:\n1. On top of page 2, the authors motivate the multi-player formulation via diverse annotators and heterogeneous evaluation criteria that are parts of the feedback signal that evaluates multiple completions for a single prompt, however, the MNPO framework focuses on multiple models generating the responses. In my opinion, this motivation is more suitable to argue that preference modelling should be improved and not that alignment algorithms need to compare more than 2 responses. It would be great if the authors could clarify the connection between the motivation and the framework.\n2. While the authors claim that the iterative framework they build upon has an asymptotic convergence guarantee to the optimal policy on average (Line 215-216), it is not discussed whether this guarantee holds in their case after introducing modifications to the online mirror descent in Eq. 15 and 16 especially as it moves from an offline dataset $D_t$ in Eq. (15) to an online optimisation problem in Eq. (16). Furthermore, how it carries over to the TD-MNPO algorithm. The last paragraph in Section 3 discussed the benefits of TD-MNPO without theoretical or empirical validation. Discussion on this would be much appreciated.\n3. TD-MNPO requires a mixture of all previous policies of opponents in its loss function defined in Eq. 18. This can be limiting when either n or T are large. I suggest the authors to comment on how this can be scaled to large n or T."}, "questions": {"value": "I have the following questions to the authors that would be great to clarify beyond the points mentioned in the Weaknesses section\n1. In the definition of the DualGap for multiplayer games, Eq. (10), the first term takes both the expectation and the maximum over $\\pi_j$. Could the authors clarify this notation and definition?\n2. In the experiments, Gemma-2-9B-it was used as a base model which is an older version and has already been preference fine-tuned. Why did the authors choose this model and do the results hold when MNPO is initialised with an on SFT model? Due to this, the base model already achieves high scores on most benchmarks leaving small room for improvement and the differences between algorithms and models are marginal.\n3. How does the model compare against more recent open-source LLMs e.g. Tulu3 instead of Tulu2, Olmo2, or SmolLM3? Some models for comparison seems outdated and would be better to compare against more recent variants.\n4. Does the results carry over more general preference oracle? Current experiments use a reward model, ArmoRM-Llama3-8B-v0.1, which results in transitive preference signals while game-theoretic approaches are meant to optimise preference signals that are more diverse.\n5. Why did the authors choose to use GPT-5 as judge for the benchmarks? It is not standard for some of them, e.g., AlpacaEval 2.0 and its alignment with the human annotators is not evaluated, therefore, results with these models are less reliable. I would suggest to validate results with some of the standard annotators provided by the original codebase.\n6. How did the authors decide that 3 iterations for MNPO is sufficient? Results and comments on convergence and DualGap would be appreciated to judge whether the model really converged."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sOTiO4cIrI", "forum": "x7aLhLMVn1", "replyto": "x7aLhLMVn1", "signatures": ["ICLR.cc/2026/Conference/Submission6303/Reviewer_RaDU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6303/Reviewer_RaDU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760971002249, "cdate": 1760971002249, "tmdate": 1762918605424, "mdate": 1762918605424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multiplayer Nash preference optimization (MNPO) framework that extends two-player NLHF to an $n$-player setting. The authors derive an iterative multiplicative-weights update that leads to a practical surrogate loss minimizing squared log-probability ratios between a policy and a set of opponent policies. Experiments with Gemma-2-9B-it show modest improvements over strong baselines on standard alignment benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear and mathematically coherent derivation of multiplayer Nash preference optimization extending two-player Nash optimization. \n2. The reduction of several other optimization frameworks and approaches to the proposed time-dependent MNPO algorithm in Table 1 is nice."}, "weaknesses": {"value": "My primary concern is the lack of motivation. What is the actual problem with existing two-player Nash optimization that we are addressing here? Why should preference optimization be modeled as an $n$-player game at all? Even the standard two-player Nash formulation only arises incidentally, because we wish to optimize for intransitive pairwise preferences / against a general preference model $p$, which naturally induces a constant-sum (or zero-sum) game and the NE is the most intuitive solution concept. Extending this to multiple players is not obviously motivated. In the paper, I only see this motivating paragraph:  \n> However, real-world preference alignment often involves diverse\nannotators, heterogeneous evaluation criteria, or mixtures of historical model checkpoints - contexts\nthat are better modeled as multiplayer games (Freund & Schapire, 1999).\n\nBeyond the \"mixtures of historical model checkpoints\" (in case you want to use several), it don't see why we want a multiplayer formulation? This would be more plausible *if* you were to model heterogeneous or conflicting preferences explictly (as in pluralistic alignment) where each player represents a distinct preference profile or reward model, e.g., each corresponding to a distinct demographic group. However, the MNPO formulation assumes a symmetric, player-invariant universal preference function, which eliminates this justification entirely. Without better motivation the resulting formulation seems to generalize two-player NLHF without offering a clear conceptual benefit. In fact, optimizing in multi-player games is harder as the dynamics are less stable. What do we gain in turn for the additional complexity of optimizing a $n$-player game and more hyperparameters versus a $2$-player game (which is already hard to optimize)? Empirically, the results also fail to justify the additional complexity. \n\nAlso, the \"universal preference oracle\" can itself be problematic to obtain if it is not reward-based. If this oracle is implemented via an LLM-as-a-judge, prompting it with $n$ candidate responses becomes increasingly unreliable as $n$ grows. Generally, you'd expect the quality and consistency of this feedback to be worse when querying for multiway preferences compared to pairwise comparisons."}, "questions": {"value": "1. Please comment on the weaknesses above. \n2. Have you more qualitatively looked at the NE in your multi-player formulation compared to the two-player formulation? The preference function induces a symmetric game and the NE is symmetric as you write on page 4. Under the reward-based preference function the game then reduces again to the two-player game where the equilibrium condition is that a player plays the best response to the mixture of the other players (which are symmetric). Similarly for non-transitive preferences, for example, $n$-player rock-paper-scissors (or the preference function induced by it), we do not need to model more than two players to represent the equilibrium condition (I think) as it is enough for a player to best respond to the population-average / mixture. I think this is a quite standard perspective for symmetric games and in mean-field games where the population-average acts as the second player and provides us with a fixed point condition that defines the same equilibrium as the $n$-player game. What I want to say with this is that often adding more players does not necessarily generate new symmetric equilibria or notable differnt dynamics unless players differ (heterogeneous preferences, asymmetric roles, etc.). I'd be curious whether you have thought about this in some basic examples to understand the solution concept of your multi-player Nash approach and whether there is anything we can learn from this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7OWsGlvX2w", "forum": "x7aLhLMVn1", "replyto": "x7aLhLMVn1", "signatures": ["ICLR.cc/2026/Conference/Submission6303/Reviewer_iCKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6303/Reviewer_iCKV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668686050, "cdate": 1761668686050, "tmdate": 1762918604947, "mdate": 1762918604947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Multiplayer Nash Preference Optimization (MNPO), a new framework for aligning Large Language Models (LLMs). \n\nThe authors identify a key limitation in existing alignment methods:\n* Classical Reinforcement Learning from Human Feedback (RLHF) built on the Bradley-Terry model assumes preferences are transitive, which often fails in practice.\n* Recent game-theoretic approaches, known as Nash Learning from Human Feedback (NLHF), overcome the transitivity assumption by reframing alignment as a two-player Nash game (e.g., INPO, EGPO).\n* The authors argue that even these NLHF methods are limited, as their two-player \"single-opponent bias\" fails to model the full complexity of real-world preferences, which are often heterogeneous (e.g., from diverse annotators or model checkpoints).\n*To address this, MNPO generalizes NLHF from a two-player game to an n-player game. \n\nThe paper's main contributions are:\n1. Theoretical Framework: It formally defines an n-player alignment game, introducing a multiplayer objective function (Eq. 8), an n-player Nash Equilibrium (Eq. 9), and a generalized Duality Gap (Eq. 10) to measure alignment quality.\n2. Algorithmic Innovation: It derives a practical and tractable loss function from the complex multiplayer game dynamics. The final proposed algorithm, Time-Dependent MNPO (TD-MNPO), defines the $n$ players as a weighted mixture of historical policy checkpoints, inspired by methods like INPO and SPIN.\n3. Conceptual Unification: The paper demonstrates that this TD-MNPO framework is a generalization that can recover many existing preference optimization algorithms (e.g., DPO, INPO, SPPO) as special cases by simply varying the number of players ($n$), the choice of opponents, and the distance metric (Table 1).\n4. Empirical Validation: Through comprehensive experiments, the authors show that their 9B MNPO model consistently outperforms strong baselines (DPO, SimPO, SPPO, INPO) on instruction-following benchmarks (AlpacaEval 2, Arena-Hard, MT-Bench) and also shows superior performance and capability preservation on academic benchmarks for math, code, and reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* **High Significance and Novelty:** The paper provides a clear, logical, and significant next step in the alignment literature (i.e., moving from 2-player to n-player games). This is a much more realistic model for real-world alignment problems involving diverse and non-transitive preferences.\n\n* **Theoretical Soundness:** The work is theoretically well-grounded. The derivation of a practical loss (Eq. 16) from the intractable multiplicative weight update (Eq. 11)  using log-ratio techniques is elegant and builds directly on the foundations of DPO and IPO.\n\n* **Excellent Conceptual Unification:** The framing in Table 1, which shows how TD-MNPO unifies a family of recent algorithms, is a major conceptual strength. It provides a \"recipe\" for understanding and creating new alignment methods. Appendix D (Table 5)  is also an outstanding resource for the community.\n\n\n* **Strong Empirical Results:** The experimental results are impressive. MNPO shows consistent and sometimes significant gains over SOTA baselines across all benchmark categories. The 4.23-point improvement over INPO on Arena-Hard and the fact that it is the only method to score on AIME-24  are particularly noteworthy.\n\n* **Comprehensive Evaluation:** The authors correctly evaluate their model not just on alignment (Table 2) but also on capability preservation (Tables 3, 4), demonstrating that MNPO avoids the performance degradation on reasoning tasks that can plague other alignment methods.\n\n\n* **Clarity:** The paper is exceptionally well-written and clear. The authors do an excellent job of motivating the work and explaining the complex theoretical concepts."}, "weaknesses": {"value": "* **Disconnect Between Motivation and Experiment:** The primary motivation is to handle \"heterogeneous annotator conditions\". However, the experiment uses a single reward model (ArmoRM-Llama3-8B-v0.1) as the preference oracle. This setup does not actually test the core hypothesis. Instead of a true multiplayer game against diverse preference functions, the experiment is an n-player game where all players compete to align with the same static oracle. This is a significant gap between the problem statement and the empirical validation.\n\n* **Anomalous Result in Table 2:** The empirical results in Table 2 contain a suspicious data point. The GPT-5 model scores 41.42 on Arena-Hard, which is significantly lower than the 9B MNPO model (52.26), other baselines, and especially Claude-Sonnet-4 (77.58). This score seems anomalously low for GPT-5 and raises questions about the evaluation setup or a potential typo.\n\n* **Missing Ablation Study:** The core mechanism of TD-MNPO is the use of $n$ players (historical policies). The main paper lacks a crucial ablation study showing how performance is affected by the choice of $n$. The authors claim a win for the multiplayer framework, but it is not empirically proven that $n>2$ is better than $n=2$ (which would be an INPO-like baseline). The performance gain could be due to other implementation details rather than the multiplayer formulation itself."}, "questions": {"value": "1. Could you comment on the disconnect between the motivation (handling heterogeneous annotators) and the experimental setup (using a single RM)? How can you be sure the observed gains are due to the multiplayer formulation's strengths, rather than it just being a more stable version of an $n=2$ (INPO-like) algorithm? Would a more \"true\" test involve training $n$ distinct RMs (e.g., on different data slices) and having each player $\\pi_i$ optimize against a different oracle?\n\n2. Could you please verify the Arena-Hard scores in Table 2? The GPT-5 score of 41.42 seems exceptionally low, especially given your judge is \"GPT-5-mini\" and other models like Claude-Sonnet-4 scored 77.58. Is this a typo? If not, can you explain this result?\n\n3. The value of $n$ (number of players) and the weighting scheme $\\{\\lambda_j\\}$ seem critical.\n    * What value of $n$ was used for the main results in Tables 2-4?\n    * Could you provide an ablation study, even if brief, showing how performance on a key benchmark (e.g., Arena-Hard) changes as you vary $n$ from 2 (like INPO) to the value you used? This would be a smoking gun to prove that the multiplayer aspect is the key to the performance gain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k656U97vlh", "forum": "x7aLhLMVn1", "replyto": "x7aLhLMVn1", "signatures": ["ICLR.cc/2026/Conference/Submission6303/Reviewer_Thdj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6303/Reviewer_Thdj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871198953, "cdate": 1761871198953, "tmdate": 1762918604556, "mdate": 1762918604556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}