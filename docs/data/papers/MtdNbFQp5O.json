{"id": "MtdNbFQp5O", "number": 24924, "cdate": 1758362015268, "mdate": 1759896742251, "content": {"title": "Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias", "abstract": "Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. \nThis phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction.\nWe introduce \\emph{\\textbf{M}ixture \\textbf{o}f \\textbf{La}tent \\textbf{C}oncept \\textbf{E}xperts (\\textbf{MoLaCE})}, a framework that directly addresses confirmation bias through a mixture of hidden experts.\nOur method identifies a latent direction in the model internal representations that reflects confirmation bias, instantiates experts as different activation strengths along this direction, and employs a gating mechanism to adaptively mix their predictions. \nThis design enables a single LLM to emulate the benefits of debate internally while remaining lightweight and scalable. \nIt can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. \nWe empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.", "tldr": "", "keywords": ["LLM", "Question Answering", "Bias"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee5a1f085efa5995fe9dbcd8328b9c0452bc2514.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* This paper introduces a way to use activation steering to improve test-time compute methods for answering questions / improving truthfulness\n* The method works by using activation steering to choose different experts during the forward pass, and thereby produce an ensemble, with the aim of using that to improve the effectiveness of using test-time compute to elicit better answers from LLMs\n* The authors find very large gains in performance from the proposed technique over directly using LLMs to answer questions, or to answer questions using the test-time compute approach they build off of (Debate+)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The numbers look very strong; I’m very surprised by the level of improvement obtained\n* The method itself is interesting/creative, and would be interesting if it worked well. I'd imagine there are a number of extensions if something like this works well, and variations which could work even better\n* While there aren't many evaluations (3), they are pretty reasonable / cover some breadth of applications (especially MMLU)"}, "weaknesses": {"value": "* I’d love to see this work on more datasets — I’m not sure if this method is designed around truthfulQA, though the fact that it helps on MMLU is helpful. I’d be interested if it seems like it’s very broadly helping across say 10 datasets, including other high profile ones like GPQA. I’d also be interested to know what the improvements are on specific MMLU subsets, to know where the gains are coming from\n* It’s unclear to me how the method works, in particular how the earlier work on Debate+ works and how this method connects with that. I’m not sure if I just missed this in the text, but it would be helpful to have a simple explanation of these in the intro. For example, I’m unsure if this new method is using additional test-time compute over the Debate+ method. I’m also not that clear on how the activation steering vectors are produced (like with what data)\n* It would be helpful to have some simpler baselines which leverage the same underlying intuition (like rephrasing the prompt in a positive vs. negative way, then ensembling over those versions), and also compare to some alternative hypotheses for where the improvement could be coming from (like from using different experts at test-time, but where we don’t use activation steering to pick those)."}, "questions": {"value": "1. What is the improvement on various subsets of MMLU? This would help to get a sense of how broadly the method is working (vs. if it’s just causing large gains on a specific few subsets of the dataset)\n2. Is this method using more test-time compute than the earlier methods Debate+ etc., in the comparisons in the main table?\n3. Are there any other changes made to the Debate+ method, other than using the activation-steering to guide/vary expert selection? Did you use the same code and hyperparameters, and just make the MoLaCE modification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dUsvCl34tn", "forum": "MtdNbFQp5O", "replyto": "MtdNbFQp5O", "signatures": ["ICLR.cc/2026/Conference/Submission24924/Reviewer_sPa2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24924/Reviewer_sPa2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578585156, "cdate": 1761578585156, "tmdate": 1762943247419, "mdate": 1762943247419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates confirmation bias in LLMs, and proposes an inference-time method (MoLaCE, Mixture of Latent Concept Experts) to adress confirmation bias."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **[Important motivation]** The paper focuses on confirmation bias, which is a key hurtle in the effectiveness of multi-LLM inference (or even single LLM inference over multiple timesteps).\n\n- **[Training-free method]** MoLaCE operates entirely at inference time through activation steering; no additional fine-tuning or data are required. Im quite impressed with the way in which the authors construct the steering vector that they use for modifying the LLMs generation during inference time. I initially expected this to require some training, more like a linear probe, (even if it only required training a linear classifier), but the steering vector construction is even more lightweight (using only CAA), which simply requires averaging activations from positive and negative examples (which are synthetic and need not come from the domain being used for inference). While not 100% free, this seems pretty efficient at face value.\n\n- **[Clarity]** The text is mostly well-written, and the figures are easy to follow and understanding, (with some caveats discussed later).\n\n- **[Well motivated method]** The view of bias from the perspective of latent concepts provides a solid motivation for the construction of the authors method (although admittedly the reader needs to do some fairly heavy lifting to full get this motivation).\n\n- **[Improvements over baselines]** Although the authors experiments are quite limited in terms of benchmarks and datasets, they observe consistent improvement over the baselines.\n\n- **[Useful ablations and experiments]** Again, although the experiments cover limited models and benchmarks, the authors provide some useful additional experiments in terms of helping to understand the setting and the method (e.g., better understanding the linear probe effectiveness per-layer Fig-3, and the distribution of errors/bias under different types of prompts Fig-4)"}, "weaknesses": {"value": "Overall, this is a paper I wanted to like! The motivation is strong and the idea is very interesting, but the work feels incomplete. The evaluation is too narrow, the baselines omit the most relevant prior methods, and the theoretical component is a bit muddy. With a more fleshed out presentation of the theoretical foundation and with more complete experiments this could be a very nice paper.\n\n- **[Narrow and outdated evaluation]**  The study tests only on BoolQ, MMLU, and TruthfulQA (datasets that are simple factual QA). BoolQ, in particular, is rarely used in current works because of its simplicity. The evaluation omits modern reasoning or debate-style tasks where bias mitigation would be most relevant. I would have liked to see a more thorough investigation on \"harder\" or more \"complex\" benchmarks with the use of more modern models. For example, in my own tests I have observed that this type of bias is less impactful on reasoning models as they possess an innate ability to self correct or alter their trajectory, compared to \"traditional\" LLMs like the ones used in this paper. It would have been helpful to see the authors method/findings contextualized beyond simple QA with non-reasoning LLMs. \n\n- **[Minimal baseline coverage]:**  Beyond the issues mentioned above, the comparisons are baseline comparisons are limited to only two *Debate* and *Debate+*, with no inclusion of widely-used baselines such as Tree-of-Thoughts, CAMEL, or even multi-agent approach like that in \"Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\". These omissions make it difficult to judge actual improvement of the authors method.\n\n- **[No heterogeneous-model baseline]**  Of particular note is the fact that the authors do not provide any results for heterogeneous teams of models. A key observation of this work (and prior work) is that the echo-chamber bias arises when all agents share one base model, yet the authors never evaluates the most \"obvious\" mitigation to this, namely multi-family debate using models from different architectures. If confirmation bias is the core issue, this is the most direct baseline.\n\n- **[Vague theoretical contribution/foundation]**  The “latent concept” formulation is presented as a theoretical framework but functions almost exclusively as a conceptual analogy, rather than a rigorous foundation for the approach. Even within this analogy it's somewhat unclear what the reader should take away should be. I found myself needing to reread sections 2-3 quite a bit to full grasp what the setup/takeaway for the authors method (despite being quite familiar with the math behind the Bayesian interpretation of latent concepts). Maybe this is as simple as adding a numbered remark or theorem that outlines more exactly what the authors' intended key point. For example, on line 201, there is discussion about steering latent concepts to neutralize confirmation bias, and the authors propose their use of CAA here, however there is no direction connection made to the reduction of bias, just an assumption which seem state that this vector can modify bias (which seems fairly circular since we are assuming the main property of that we would expect the method to possess). Im not necessarily saying that the authors need to (or should) have theorems, but some type of more rigorous or concise statement would help the motivation and understanding be more clear.\n\n- **[Presentation of the latent concept setup]** This is more minor than the above point, but its a bit unclear in Sections 2-3 where prior work ends and where the authors' contributions start. The authors do a great job of presenting all the necessary ideas/background in these sections (this part should be in \"Strengths\"), but ultimately blur their contributions/proposals with those of existing works.  \n\n-  **[Missing some experimental details]** There is no discussion of how layer indices, steering magnitudes, or gates were chosen (maybe I missed this somewhere in the supplement)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "POEM9i1LjO", "forum": "MtdNbFQp5O", "replyto": "MtdNbFQp5O", "signatures": ["ICLR.cc/2026/Conference/Submission24924/Reviewer_7rAZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24924/Reviewer_7rAZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937354448, "cdate": 1761937354448, "tmdate": 1762943247230, "mdate": 1762943247230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of LLM confirmation bias. They introduce the MoLaCE frameworks that combines steering vector and MoE to reduce the stance-based prompt sensitivity of models. They further show that they can apply this to a single-agent setting to “mimic” the effect of multi-agent debate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated and provide a good operationalization of input confirmation bias to study the internal representation and propose interesting interventions based on the findings. The proposed method makes sense theoretically and seems to work well in practice."}, "weaknesses": {"value": "Weaknesses:\n\n1)\tDo you have any sense of which setting would MoLaCE be most effective? Consider a dataset where the ground-truth distribution is highly skewed (e.g., in a fact-checking task where 99% of statements are true). In such a scenario, a consistently \"pro-truth\" prompt framing would be a very effective strategy for maximizing accuracy. By steering the model away from this beneficial bias and toward neutrality, MoLaCE could paradoxically decrease performance on this subset of the data. Have you tried to stratify the results to analyze this trade-off, showing where MoLaCE provides the most benefit (likely on balanced or ambiguous cases) and where it might incur a cost?\n\n2)\tA very simple baseline that could add a lot of context to the result: direct ensemble over different prompt perturbations. One could simply generate answers from several manual or automated rephrasing of the prompt (e.g., positive, negative, and neutral framings) and aggregate the results via majority vote (or let the LLM aggregate). This would serve as a great point of comparison to gauge the added value of MoLaCE's more complex internal mechanism.\n\n3)\tUnclear generalizability: one concern I have is, the premise of the MoLaCE is that LLMs are quite sensitive towards stance-based prompt variations. Is this still true for a very large SOTA model (could still be an open-weight model), as larger models are typically more robust towards semantically similar prompt variations? Could the authors comment on the expected generalizability of their findings and method? Even showing a scaling trend on a wider range of small-to-medium models (e.g., 1B to 13B) would help readers contextualize how the problem and the solution might evolve with model scale."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J9azZa45M6", "forum": "MtdNbFQp5O", "replyto": "MtdNbFQp5O", "signatures": ["ICLR.cc/2026/Conference/Submission24924/Reviewer_GXnJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24924/Reviewer_GXnJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996212391, "cdate": 1761996212391, "tmdate": 1762943247024, "mdate": 1762943247024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses confirmation bias in large language models, proposing MoLaCE (Mixture of Latent Concept Experts), a framework that identifies latent directions associated with confirmation bias and uses a gating mechanism to mix experts with different activation strengths (\\alpha). The authors evaluate the robust performance on TruthfulQA, MMLU, and BoolQ across three bias types. They show improvements over base models and competitive performance with multi-agent debate reqauiring larger computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Confirmation bias in LLMs is a critical practical issue. Understanding the connection between single-agent bias and multi-agent echo chambers provides insight into addressing practical issues. Formulating the problem using latent concepts provides a principled framework.\n\n- Training-free intervention using CAA is practical for any LLM. The proposed method reqauires significantly lower computational costs than multi-agent debate and can therefore be integrated into existing systems without additional training or changes.\n\n- Three bias types were evaluated (correct-incorrect, positive-negative, and negation-based), providing comprehensive coverage for three benchmarks. Linear probing experiments effectively demonstrate that CB is linearly decodable."}, "weaknesses": {"value": "- The core components—Contrastive Activation Addition, mixture-of-experts architecture, and the debate framework—are all established techniques. While their combination for bias mitigation is new, the paper does not introduce fundamentally novel methods or theoretical insights beyond applying existing tools in a new context.\n\n- As stated in l.311 and l.315, \"For TruthfulQA, correctness is automatically judged by both Gemini 2.5 Pro and GPT-5\", and \"To systematically study confirmation bias, we construct paired prompts using Gemini 2.5 Pro.\", there is no human evaluation in some evaluations. There is also a simple concern that, since TruthfulQA and the prompts are both generated by LLMs, LLMs can mitigate cognitive bias.\n\n- They used relatively small LLMs in their experiments and not a larger LLM. This severely limits the generalization of the results. Can you employ at least one large model to demonstrate the performance? Hyperparameter choices, such as the range of alpha, selection of gating function, and the number of layers, are made without empirical or theoretical justification. Can you provide ablation study to support your claims?"}, "questions": {"value": "See weeknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X7Y7rVUD5W", "forum": "MtdNbFQp5O", "replyto": "MtdNbFQp5O", "signatures": ["ICLR.cc/2026/Conference/Submission24924/Reviewer_CXcQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24924/Reviewer_CXcQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059056201, "cdate": 1762059056201, "tmdate": 1762943245785, "mdate": 1762943245785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}