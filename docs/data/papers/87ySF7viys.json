{"id": "87ySF7viys", "number": 19755, "cdate": 1758299044447, "mdate": 1759897021318, "content": {"title": "RESTRAIN: From Spurious Votes to Signals — Self-Training RL with Self-Penalization", "abstract": "Reinforcement learning (RL) with human annotated data has boosted long chain-\nof-thought (CoT) reasoning in large language models (LLMs), but these gains\ncome at high costs in labeled data while still faltering on harder tasks. A nat-\nural next step is experience-driven learning, where models improve without cu-\nrated labels by adapting to unlabeled data. We introduce REinforcement learning\nwith Self-resTRAINt training (RESTRAIN), a self-penalizing RL framework that\ntransforms the absence of gold labels into a learning signal. Rather than amplify-\ning spurious majority votes, RESTRAIN leverages signals from the model’s entire\nanswer distribution, penalizing overconfident rollouts and low-consistent exam-\nples while preserving promising reasoning chains. This restraint mechanism inte-\ngrates seamlessly into policy optimization methods such as GRPO to self-improve\nwithout human supervisions. On challenging reasoning benchmarks, RESTRAIN\ndelivers large gains using only unlabeled data. On Qwen3-4B-Base and Octo-\nThinker Hybrid-8B-Base model, RESTRAIN boosts pass@1 by up to +140.7% on\nAIME25, +36.2% on MMLU STEM, and +19.6% on GPQA-Diamond. Re-\nmarkably, it comes within 0.4% of a fully supervised counterpart, nearly matching\ngold-label training while using no gold labels at all. These results demonstrate that\nRESTRAIN consistently boosts reasoning without supervision.", "tldr": "", "keywords": ["LLM", "self-training", "RL", "unsupervised learning", "self-penalization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ae512a8584bdcf6d57b7d46cd911bf8a2d200b89.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes RESTRAIN, an RLVR framework that does not need ground truth answers for questions. It tackles the problem that majority-voted answer may not be correct, especially for hard labels, and designed a weight-smoothing method to consider infrequent answers (pseudo-label weighting, negative rollout penalization and prompt-level weighting). The experiments show relatively large improvements over other unsupervised baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The motivation is very clear (majority voting != correct) and the designed soft weighting of infrequent answers seem reasonable and directly tackle the problem of majority-voting.\n- The negative rollout penalization is also convincing in down-weighting too infrequent/long-tailed answers (where the model is unsure about).\n- From the experiment results, it seems the proposed RESTRAIN can largely improve over existing unsupervised RLVR methods like TTRL. Besides, they test on both Qwen3-4B-Base model and Octothinker Hybrid 8B, which demonstrate that the method is applicable to recent SOTA base models and various model sizes.\n- They also provided very detailed ablation studies and useful details for reproduction."}, "weaknesses": {"value": "- Not sure if the threshold kappa will be sensitive to rollout numbers and sampling temperature. Is it fixed over different datasets (which have different difficulty)?\n- Typo in Table 1: w/ access to gold labe -> w/ access to gold label"}, "questions": {"value": "- How does the gap between pass@1 and majority voting change after RESTRAIN training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GRvfcCEiFy", "forum": "87ySF7viys", "replyto": "87ySF7viys", "signatures": ["ICLR.cc/2026/Conference/Submission19755/Reviewer_hpMx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19755/Reviewer_hpMx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760666195523, "cdate": 1760666195523, "tmdate": 1762931590949, "mdate": 1762931590949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to solve the training instability and collapse caused by unreliable majority-vote signals in unsupervised RL methods like TTRL. The paper proposes RESTRAIN, a self-penalizing framework that replaces TTRL's reward scheme. Its novel loss function stabilizes training by using soft, frequency-based weighting for all potential answers, actively penalizing low-consistency outputs, and down-weighting unreliable prompts identified by a frozen model. Experiments show RESTRAIN significantly outperforms TTRL, avoids training collapse, and nearly matches the performance of a fully supervised, gold-label baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work directly addresses the critical and well-documented stability problem of majority-vote-based unsupervised RL.\n\n2. The three components are well-motivated and logically designed to counteract the specific failure modes of TTRL, such as signal smoothing and noise penalization.\n\n3. The empirical results are a significant strength, demonstrating performance that not only avoids collapse but also approaches the upper bound of fully supervised gold-label training.\n\n4. The paper is supported by a thorough set of ablation studies that demonstrate the necessity of each component for stable training and final performance."}, "weaknesses": {"value": "1. High Hyperparameter Sensitivity: The method's performance appears highly sensitive to its key hyperparameters, including the weighting skewness, the penalty threshold, and the penalty offset. The ablation studies show that performance drops sharply outside a narrow range of these values, which may hinder its general applicability and reproducibility.\n\n2. Limited Novelty: The contribution appears to be more of a successful systems-level engineering effort than a fundamentally new paradigm. The core components (soft weighting, curriculum learning, and negative penalization) are all established concepts. Overall, this work is more like an incremental \"patch\" on TTRL."}, "questions": {"value": "1. The hyperparameters are clearly critical. How were the optimal values selected?\n\n2. Computational Cost: This method inherits the high computational cost of TTRL while introducing additional computation. Are these additional overheads significant compared to the TTRL baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5B9Lpq6Zor", "forum": "87ySF7viys", "replyto": "87ySF7viys", "signatures": ["ICLR.cc/2026/Conference/Submission19755/Reviewer_7W3F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19755/Reviewer_7W3F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951545886, "cdate": 1761951545886, "tmdate": 1762931590144, "mdate": 1762931590144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new GRPO-based loss for self-training. Specifically, it does not rely solely on majority-voted answers but instead treats all answers as potentially correct, assigning different weights and penalizing low-consistency examples. Through extensive experiments on math and science datasets and two different models, the authors demonstrate that their self-training method outperforms previous approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is intuitive, well-motivated, and clearly described. It is simple yet effective.\n2. The method achieves significant improvements over comparative self-training approaches across different models and two task domains."}, "weaknesses": {"value": "1. The proposed loss and experiments are too closely tied to GRPO. It remains uncertain whether the method is compatible with other RL algorithms, such as PPO and PRIME, as compared by TTRL.\n2. The tested datasets and models are relatively limited. Although the proposed method shows promising results in this paper, it is unclear whether it is biased toward inherently stronger models with certain specialized skills (both are base models), which may not generalize well in practice."}, "questions": {"value": "1. How can TTRL be adapted to other RL framework and what are the results?\n2. Could you provide results using Llama-3.1-8B-Instruct and Qwen2.5-Math-1.5B or 7B models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mkZfbOEu21", "forum": "87ySF7viys", "replyto": "87ySF7viys", "signatures": ["ICLR.cc/2026/Conference/Submission19755/Reviewer_Fb6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19755/Reviewer_Fb6c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961645043, "cdate": 1761961645043, "tmdate": 1762931589143, "mdate": 1762931589143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}