{"id": "sFjxg8cyJS", "number": 13510, "cdate": 1758218798480, "mdate": 1759897432012, "content": {"title": "When Shift Happens - Confounding Is to Blame", "abstract": "Distribution shifts introduce uncertainty that undermines the robustness and generalization capabilities of machine learning models. While conventional wisdom suggests that learning causal-invariant representations enhances robustness to such shifts, recent empirical studies present a counterintuitive finding: (i) empirical risk minimization (ERM) can rival or even outperform state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) OOD generalization performance improves when all available covariates, including non-causal ones, are utilized. We present theoretical and empirical explanations that attribute this phenomenon to hidden confounding. Shifts in hidden confounding induce changes in data distributions that violate assumptions commonly made by existing approaches. Under such conditions, we prove that generalization requires learning environment-specific relationships, rather than relying solely on invariant ones. Furthermore, we explain why models augmented with non-causal but informative covariates can mitigate the challenges posed by hidden confounding shifts. These findings offer new theoretical insights and practical guidance, serving as a roadmap for future research on OOD generalization and principled covariate-selection strategies.", "tldr": "Hidden confounding can make all-feature models outperform causal ones under distribution shifts. Learning environment-specific patterns and using confounder proxies improves OOD robustness beyond standard causal approaches.", "keywords": ["Explainability", "OOD Generalization", "Confounding shifts"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4f74f9f0891bd19bfbb0b3747c088192bcbf31a.pdf", "supplementary_material": "/attachment/294cb83dde88083057769cb0b60c375a9a31aa75.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates data generating processes in which confounding of $X$ and $Y$ takes place through an unobserved variable $U$ (‘hidden confounding’, Figure 3). \n\nTheory:\nUsing mutual information (MI) of $Y$ and its prediction $\\hat{Y}$ as objective (O) to be maximised, the authors provide results that show that in cases where either $X \\to Y$ or $Y \\to X$ (but not both), O is equal to the sum of the environment specific MI between $Y$ and learned representations $\\phi(X)$, plus a residual term (will refer to this sum as S below), showing that in such scenarios learning environment specific information is expected to be superior to invariant approaches. \n\nExperiments:\nThe empirical part of the paper focusses on the TableShift benchmark, which contains a range of tabular datasets with distribution shift. According to the authors, most of these datasets follow a data generating process in which either fully or predominantly $X \\to Y$ (but not $Y \\to X$)\nUsing measurements of MI, the authors find changes in conditional MI terms that would be expected under the data generating process the paper considers (Table 3). The authors further evaluate 5 different predictive methods (XGBoost, MLP, GDRO, IRM, VRex) on TableShift and find a positive spearman correlation between average test accuracy of a method and the value of S it achieves in both ID and OOD (Table 2)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The fundamental problem of distribution shift is relevant and recent findings by Nasal and Hardt (2024) on the effectiveness of ERM make the paper timely \n- The decomposition in Theorem 4.1 is innovative and a clever way to analyse different effects of hidden confounding shift.\n- The derived result in Theorem 4.2 is insightful and relevant\n- The experiments to corroborate the applicability of the assumptions in Propositions 4.1 and 4.2 are generally speaking well designed (but could be improved, see weaknesses)"}, "weaknesses": {"value": "- It would be good if the authors could show how their insights from Theorem 4.2 can be turned into practical algorithms. They mention ideas in the the paragraph “Remarks on conditional informativeness” on page 7, but do not pursue those to derive a method that maximises Eq (4) that is compared to existing methods in the experiment section. One idea could be for example to equip the standard MLP with environment variable(s) similar to Figure 2 (ii), e.g. the feature means or similar and see if this can improve results.\n- Along similar lines, the empirical results in Table that are meant to corroborate practical relevance of Theorem 4.2, are all only observational and based on correlations of measured quantities. Doing some intervention on conditional informativeness - residual values of a method (e.g. actively minimising or maximising the term) and observing changes in accuracy would make a stronger point, or alternatively add some more methods (e.g. logistic regression ERM to keep it simple, or other invariant or domain generalisation approaches) or the same methods with different hyper parameters (e.g as found in Figure C4) to the analysis in Table 2 would make the correlation estimate more reliable. \n- I think Eq (2) and (3) should be derived formally in a proposition, rather than justified in a few words of text as the derivation is not clear to me as such\n- Estimating MI of continuous, high-dimensional variables is a non-trivial problem. It would be good to summarise  in the main text how this was achieved and reference where to look to find details on it in the Appendix\n\n\nMinor:\n- Presentation of results in Table 2: as is, it does not clearly show the correlation between conditional informativeness - residual and accuracy as the main focus is on the test accuracies and the spearman correlations are just somewhat floating around. Why not add the values of conditional informativeness - residual for both ID and OOD in the table to make the picture more complete? Or otherwise you might as well just give the Spearman correlation coefficients in line in the text, and don’t need to use all the space for the table. \n- Proof of theorem 4.2 should be referred to in the main text. \n- An example graph similar to Figure 3 (or an extension thereof) would be helpful to understand definition 4.3 / the concept of informative covariates."}, "questions": {"value": "- Line 76: Why is learning an invariant representation sufficient? The paper seems to suggest the opposite in Eq 1?\n- Line 209: Why is the sum over $\\mathcal{R}^{\\mathcal{D}^e}$ and not simply over $\\mathcal{R}^{e}$? For the other methods in the same paragraph the objective is always stated using $\\mathcal{R}^{e}$. \n- Table 2: why are the authors excluding the results from Figure C4 in the analysis? Adding these would make the estimate of the correlation more robust (and also their value lower I think)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BRk6POXVWI", "forum": "sFjxg8cyJS", "replyto": "sFjxg8cyJS", "signatures": ["ICLR.cc/2026/Conference/Submission13510/Reviewer_ov4h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13510/Reviewer_ov4h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834281914, "cdate": 1761834281914, "tmdate": 1762924121550, "mdate": 1762924121550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a comprehensive information-theoretic framework to understand various components of distribution shift in out-of-domain generalization. Based on existing puzzles on why simple ERM outperforms robust/causal methods, the authors point out -- through both careful theoretical analysis and empirical benchmark -- that confounding bias is the key issue that affects generalization performance, and adding non-causal yet confounding-informative variables can improve generalization. The proposed theory centers around a decomposition of the predictive information $I(Y;\\hat{Y})$ into several terms related to environment change and variable-related shift terms.  This result is well explained through several results in representative settings. The proposed theory is corroborated via numerical experiments and provides insights on generalization performance in popular distribution shift benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Understanding out-of-domain generalization is a very important problem, and the paper targets at a key issue in this area, which is the performance gap between robustness-oriented methods and plain ERM. The results in the paper should benefit the community in both understanding and subsequent method development. \n2. The paper is well-written and well-structured, with rich discussion and inspiring insights. \n3. The theory is supported by solid experiments and empirical insights."}, "weaknesses": {"value": "1. Maybe I missed something but it would be helpful to clarify what datasets the results in Section 5 are from (are they from synthetic data or the TableShift benchmark?). Seems the results are from one single dataset? Or did you merge all the data?\n2. While the authors clarify that this paper aims to provide empirical insights instead of solutions, which is totally fair, it might be useful to suggest some solutions based on the observed results."}, "questions": {"value": "1. (same as weakness 1) How are the results in Section 5 obtained -- are they from a single dataset (synthetic or real) or merged everything?\n2. The sign consistency metric is interesting. It's natural to expect a high fraction when a decomposition term is related to the performance. However, I'm a bit confused as to how to interpret a metric as low as $0.2$ (e.g., FS). Even for random guess (sth totally unrelated to prediction performance) the metric should be like $0.5$. Does a low metric mean this term predicts the improvement accurately in an opposite direction? If so, does it mean the theory is not accurate?\n3. Given the important role of CI and CS in sign consistency metric, would you suggest optimizing them in training OOD generalization algorithms? \n4. A potential use of the results is to estimate these terms in the existing environments to guide model choice. Could the authors comment on such applications, e.g., could it work and how future method development may involve them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GRhbuXqt2C", "forum": "sFjxg8cyJS", "replyto": "sFjxg8cyJS", "signatures": ["ICLR.cc/2026/Conference/Submission13510/Reviewer_ZFUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13510/Reviewer_ZFUv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849679214, "cdate": 1761849679214, "tmdate": 1762924121166, "mdate": 1762924121166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a theoretical and empirical explanation for observed paradoxes in Out-of-Distribution (OOD) generalization, specifically why conventional Empirical Risk Minimization (ERM) models often match or surpass state-of-the-art OOD methods, and why including all available covariates, even non-causal ones, improves OOD accuracy. The core argument states that shifts in hidden confounding variables induce simultaneous distribution shifts that invalidate the assumptions of invariance-focused OOD methods. Under this causal structure, the authors prove that generalization success depends on learning environment-specific relationships. The benefit of non-causal but informative covariates is explained by their ability to act as proxies for the hidden confounders, thereby mitigating the negative effects of the shifts and increasing conditional informativeness. This conditional informativeness measure correlates strongly with both In-Distribution and OOD accuracy across eight real-world datasets, validating the hypothesis that under hidden confounding, exploiting environment-specific information is necessary for robustness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The work theoretically justifies the importance of non-causal, informative covariates (XI) by showing they help maximize generalization performance.  Proposition 4.1 demonstrates that adding these variables reduces concept shift and increases conditional informativeness, thereby mitigating the negative impact of unobserved confounders.\n\n\n\nExperiments using synthetic data with known causal structure and extensive testing on eight real-world tabular datasets (TableShift benchmark) consistently confirm the core hypotheses: hidden confounding is prevalent (inducing simultaneous shifts), and conditional informativeness is highly correlated with both ID and OOD accuracy."}, "weaknesses": {"value": "The core theoretical insights (Theorems 4.2 and related decompositions) are dependent on assuming a very specific, unobserved causal graph. This foundational assumption remains unverifiable in real-world data. This limits the ability of the derived guidance to be applied with certainty, as the true underlying causal structure is unknown.\n\nThe empirical validation is focused almost exclusively on tabular prediction tasks employing relatively simple models like XGBoost and MLP. The role of the feature extractor in complex domains is much more sophisticated. It is questionable whether the conclusion that invariance is insufficient and environment-specific information should be prioritized holds true when the extractor is a large, non-linear model and the data shifts are visual or semantic rather than structural/tabular.\n\nThe authors explicitly state that their primary goal is to explain existing phenomena rather than provide a novel solution to the problem of hidden confounding shift. While the theoretical explanation is robust, the paper primarily offers a lens through which to understand why simple methods already work well, rather than proposing a new method that reliably and optimally maximizes conditional informativeness across generalized shift settings.\n\nThe identification of \"informative covariates\" is critical to the proposed strategy. However, outside of carefully constructed synthetic settings, rigorously identifying these non-causal proxy variables remains a challenge. The reliance on LLM queries for semantic insight into potential confounders in real-world datasets underscores the difficulty of establishing these variables non-qualitatively."}, "questions": {"value": "Could the authors elaborate on the architectural properties of XGBoost that make it inherently proficient at capturing the environment-specific relationships required for maximizing conditional informativeness compared to IRM, which attempts to enforce invariance by minimizing penalties associated with environment dependency?\n\nGiven the complexity of feature extraction in computer vision, how would the predictive information decomposition terms (especially variation and feature shift behave in a system dealing with spurious correlations in image data, and what theoretical adjustments might be needed to apply this framework to deep OOD models outside the tabular setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jeTjxSiqvV", "forum": "sFjxg8cyJS", "replyto": "sFjxg8cyJS", "signatures": ["ICLR.cc/2026/Conference/Submission13510/Reviewer_cZis"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13510/Reviewer_cZis"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762301662782, "cdate": 1762301662782, "tmdate": 1762924120884, "mdate": 1762924120884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The established understanding of OOD generalization suggests that learning invariant and causal representations improves generalization. However, recent empirical work has shown two things: 1)ERM often generalizes better than methods designed for OOD generalization 2)Non-causal features can improve performance. This paper gives theoretical justification for these empirical findings. This work shows that under hidden-confounder shift, invariant causal representations could be suboptimal to ERM and using non-causal features. They also empirically verify that several real datasets do have hidden confounder shifts and their OOD performance using various approaches can be explained by this work’s theoretical framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides a neat information theory based framework to understand the OOD performance of predictors under hidden confounding shift\nThe theory explains two empirical phenomenon: 1) Learning invariant representations is not optimal for hidden confounder shift 2)Non-causal features can improve performance\nThe theory does indeed explain the empirical OOD performance of different training algorithms and the different sets of features (causal, arguably causal, all) used. However, in practice E is often hidden and calculating the terms would not be possible.\nThe paper is generally well-written"}, "weaknesses": {"value": "Regarding invariant representations: It is well known that under hidden confounder shift, they would underperform compared to methods that use information about the environment or its proxies. This intuitively makes sense since having information about the environment should help over methods that do not consider that. I believe the primary strength of invariant methods is in cases where the environment in test is out of the support of the train environments. Therefore, I do not see explaining the underperformance of invariant representations in hidden confounder shift setting a substantial contribution.\nRegarding using non-causal features improves performance: The way I intuitively understand this is that if there is a hidden confounder, then we do not observe all causal variables. Therefore, conditioning on the observed causal variables does not d-separate the non-causal ones. Hence, using non-causal features may help. It's unclear what additional insight the information-theoretic framework provides. Moreover, the d-separation argument applies to other hidden variable scenarios as well (e.g., measurement bias)."}, "questions": {"value": "Why is it surprising/insightful that environment information should give better models? If not, are there any quantitative predictions we can make in practice when E is hidden?\n\nWhy extra insight does the information theory framework provide that the d-separation argument does not?\nOpen to raising the rating if questions are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dT6ztfkbRR", "forum": "sFjxg8cyJS", "replyto": "sFjxg8cyJS", "signatures": ["ICLR.cc/2026/Conference/Submission13510/Reviewer_5zx4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13510/Reviewer_5zx4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762360494957, "cdate": 1762360494957, "tmdate": 1762924120576, "mdate": 1762924120576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}