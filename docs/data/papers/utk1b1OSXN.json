{"id": "utk1b1OSXN", "number": 7672, "cdate": 1758031372456, "mdate": 1759897840025, "content": {"title": "Towards Universal Neural Operators through Multiphysics Pretraining", "abstract": "Although neural operators found common use in contemporary data-driven physical systems simulation, their training procedure remains computationally expensive and time-consuming. Some advances have been made with the study of downstream problems, where the model is trained on a simpler problem and later fine-tuned on a more challenging one to achieve better quality and lower over-all time costs. In this research we examine capabilities of transformer-based neural operator architectures, which were previously used only for particular problems solutions, in more generalized transfer learning. We evaluate performance of the transformer- and state space model-based neural operators on wide range of downstream PDE simulation problems, including extension of models to the out-of pretraining sample parameter values, addition of new variables into the dynamics, and transfer of operator, trained on datasets, composed of solutions of several differential equations. The results indicate the ability of neural operators of advanced architectures to be used to transfer knowledge between problems, involving partial differential equations.", "tldr": "There could be a physical pretrain of neural operator that could be used to solve various problems", "keywords": ["Neural Operator", "Pretrain", "Multiphysics", "Adapters", "Foundational models"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d5adfb2c063c394f607c61676d96335d29cda0c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a way to make neural operators more universal by pretraining across multiple physics, and then fine tune on a new PDE. The suggestion is to keep a shared stack of kernel integral operator blocks and, at the fine tune time, freeze this backbone, while training only small input/output adapters. This cuts adaptation cost and tests, whether the backbone captures reusable physics. The authors build a multiphysics data pipeline that standardizes heterogeneous PDE datasets, enabling joint pretraining over advection, Burgers, Navier–Stokes, etc. The training scheme is: pretrain the full model on mixed physics, then fine tune only adapters for the target task. Fig. 2 illustrates the setup. The authors find that: (i) Pretraining a backbone and fine tuning only the small adapters improves error and shortens epochs versus training the whole model from scratch (table 1), (ii) When the fine tune task augments the equation with an extra term, the pretrained backbone plus adapters still wins (table 2), (iii) Pretrain on one set of PDEs , e.g. advection and Burgers, then fine tune on another, e.g. reaction–diffusion, yields lower error and faster epochs (table 3 and figure 3)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(i) Clear recipe: freeze backbone and train adapters. The pretrain to finetune scheme is explicit: fix the common integral operator stack and update only small input/output adapters, which highlights what transfers and cuts training cost. (ii) Well-designed evaluation: the authors test three distinct scenarios, new parameters, added physics/inputs, and cross PDE transfer, without changing the modeling recipe, (iii) Consistent empirical gains:  across tables, pretraining plus adapters improves accuracy and reduces average epoch time versus scratch, sometimes by large margins."}, "weaknesses": {"value": "(i) Scope limited to same dimensionality and curated grids: the method is explicitly evaluated when pretrain and fine tune tasks have the same problem dimensionality, and the pipeline resamples all data to a shared fixed grid. That leaves open transfer across 2D to 3D, irregular meshes, or complex geometries/boundaries, (ii) Speedups reported per epoch, not end to end:  tables report Avg. epoch time(s) but not the total wall clock including pretraining. This makes it hard to judge the true efficiency once pretraining cost is amortized, (iii) Missing controls/ablations on fine tuning strategy: fine-tuning always freezes the operator backbone and there is no baseline that unfreezes the backbone after pretraining or trains adapters with a randomly initialized frozen backbone to isolate how much of the gain comes from pretraining vs. just fewer trainable parameters, (iv) Evaluation metrics are generic and not physics aware: the core metric used is range normalized MAE (NMAE) and MSE, and  there is no explicit assessment of conservation laws, stability under long rollouts, or other physics diagnostics, so it is unclear how models behave far off the train distribution."}, "questions": {"value": "Suggestions: (i) Report end to end efficiency and not just per epoch time. The tables list Avg. epoch time(s), but one cannot judge the total \nwall clock once pretraining is amortized. Add: total pretrain hours, fine tune hours, and amortization break even curves, i.e  accuracy vs. total time/energy, and include FLOPs and trained parameter counts per setting, (ii) Ablate the fine tuning strategy by adding controls: (1) unfreeze the backbone (full fine tune), (2) train adapters atop a randomly initialized frozen backbone, (3) partial unfreezing, e.g. top k operator blocks.This isolates how much gain comes from pretraining vs. fewer trainable parameters, (iii) Go beyond shared regular grids and same dimensionality by adding tests on irregular meshes, varying domains/BCs, and 3D cases to demonstrate geometry robust transfer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XkYt0fZCDn", "forum": "utk1b1OSXN", "replyto": "utk1b1OSXN", "signatures": ["ICLR.cc/2026/Conference/Submission7672/Reviewer_xuEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7672/Reviewer_xuEJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578620845, "cdate": 1761578620845, "tmdate": 1762919736888, "mdate": 1762919736888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studied the possibility of pretrain on multi-physics data and finetune to downstream tasks, as the first stage of a foundation model for PDEs. The paper proposes two new architectures based on the Mamba and Perceiver, combined with FNO. The results show the new model has competitive performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies foundation model for PDE, which is a very interesting and important problem.\n- The paper systematically studied generalization across to new coefficients, forcing terms, and PDEs.\n- Experiment shows pretraining is generally helpful.\n- The new architectures seems to have better accuracy on extended equations and on new PDEs."}, "weaknesses": {"value": "- Similar Pretraining stage has been studied in Poseidon and [1].\n- One of the key question is not yet answered: is pretraining always helpful? Should we train on all the data (across PDEs, forcing terms, and coefficients)? or should we limit pretraining to a certain subset?\n- The new architectures of combining Mamba and Perceiver are not very significant. It is unclear if these modifications are helpful.\n\n[1] McCabe, Michael, et al. \"Multiple physics pretraining for spatiotemporal surrogate models.\" Advances in Neural Information Processing Systems 37 (2024): 119301-119335."}, "questions": {"value": "- In the paper, finetuning is only learning on the lifting layer and projection layer. It will be better to some ablation. Sometime it's better to only adapt the last several layers.\n- In the experiment is ths Swin-v2 model exactly the same as the SCOT model in Poseidon paper? If so, it would be better to name it as SCOT or Poseidon."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "khnp1TqBFz", "forum": "utk1b1OSXN", "replyto": "utk1b1OSXN", "signatures": ["ICLR.cc/2026/Conference/Submission7672/Reviewer_1i3M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7672/Reviewer_1i3M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841408152, "cdate": 1761841408152, "tmdate": 1762919736522, "mdate": 1762919736522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a pre-training approach for transformer neural operators based on adapter fine-tuning, and showcase high generalization capabilities, including cross-equation transfers. Two architectures, Mamba FNO and the Perceiver, are studied in detail. Experiments emphasized the models’ high performance in comparison to other neural operators, in tasks such as inference for PDEs with out of sample parameter values, addition of terms in the input function and knowledge transfer to unseen physical problems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tTackles the difficult problem of training foundation models applicable across a range of different PDEs and parameters.\n-\tIntroduces two novel architectures allowing pre-training and efficient fine-tuning.\n-\tStudies a diverse benchmark of PDEs and groups of PDEs, showcasing the robustness of the models."}, "weaknesses": {"value": "-\tNo ablation studies to showcase the importance of the adapters, or other layers such as the self-attention\n-\tGenerally speaking, it does not seem that the gains in terms of accuracy and training time are significant, vis-à-vis other neural operator architectures allowing pre-training and fine-tuning, but even in comparison with FNO."}, "questions": {"value": "-\tIn the tables, does the average per epoch include the pre-training for the fine-tuned cases ?\n-\tCould the authors provide some ablation studies to showcase the importance of the various architectural elements in their models ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TJwng3OcNv", "forum": "utk1b1OSXN", "replyto": "utk1b1OSXN", "signatures": ["ICLR.cc/2026/Conference/Submission7672/Reviewer_N1Vh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7672/Reviewer_N1Vh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994250295, "cdate": 1761994250295, "tmdate": 1762919736220, "mdate": 1762919736220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}