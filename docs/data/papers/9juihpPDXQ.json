{"id": "9juihpPDXQ", "number": 7337, "cdate": 1758016702240, "mdate": 1759897859016, "content": {"title": "Differential Gated Self-Attention", "abstract": "Transformers excel across a large variety of tasks but remain susceptible to corrupted inputs, since standard self‐attention treats all query-key interactions uniformly. \nInspired by lateral inhibition in biological neural circuits and building on the recent Differential Transformer’s use of two parallel softmax subtraction for noise cancellation, we propose Multihead Differential Gated Self-Attention (M-DGSA) that learns per‐head input-dependent gating to dynamically suppress attention noise. Each head splits into excitatory and inhibitory branches whose dual softmax maps are fused by a sigmoid gate predicted from the token embedding, yielding a context-aware contrast enhancement. M-DGSA integrates seamlessly into existing Transformer stacks with minimal computational overhead. We evaluate on both vision and language benchmarks, demonstrating consistent robustness gains over vanilla Transformer, Vision Transformer, and Differential Transformer baselines. Our contributions are (i) a novel input-dependent gating mechanism for self‐attention grounded in lateral inhibition, (ii) a principled synthesis of biological contrast‐enhancement and self‐attention theory, and (iii) comprehensive experiments demonstrating noise resilience and cross-domain applicability.", "tldr": "We propose Multihead Differential Gated Self-Attention (M-DGSA) that learns per‐head input-dependent gating to dynamically suppress attention noise.", "keywords": ["self-attention", "noise-cancelling", "differential gated attention", "lateral inhibition"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76347cdb1f056b1923272d35d749baaab8d6379c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new approach called Multihead Differential Gated Self-Attention (M-DGSA), designed to enhance the robustness of Transformers against noisy inputs. By introducing an input-dependent gating mechanism for each attention head, M-DGSA dynamically combines excitatory and inhibitory branches, allowing it to more effectively suppress noise. Experimental results demonstrate that M-DGSA consistently boosts accuracy and robustness, particularly under noisy conditions, across both vision and language tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces M-DGSA, a method that learns per-head, input-dependent gating to dynamically suppress attention noise. The results demonstrate consistent improvements in noisy environments, showcasing the method's effectiveness. Additionally, the paper is well-written and accessible."}, "weaknesses": {"value": "The paper presents a relatively straightforward idea and lacks significant originality. Compared to the approach in Ye et al. (2024), there are no major innovations or departures in the proposed method.\n\nThe experiments have some notable limitations. The CIFAR and MNIST datasets are relatively small and simple, which may not fully showcase the model's capabilities. Additionally, the reported ImageNet accuracy (Table 2) is low. Due to these factors, the claims regarding the algorithm's effectiveness remain somewhat unconvincing."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OfVMqQcAoM", "forum": "9juihpPDXQ", "replyto": "9juihpPDXQ", "signatures": ["ICLR.cc/2026/Conference/Submission7337/Reviewer_WULu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7337/Reviewer_WULu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624214947, "cdate": 1761624214947, "tmdate": 1762919457183, "mdate": 1762919457183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose multihead differential gated self-attention (M-DGSA), a modification of the differential transformer (DT). M-DGSA replaces the scalar \\lambda in DT with an input-dependent sigmoid gate. The method is evaluated in both vision and language tasks, showing modest accuracy gains over the DT baseline on (clean) datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Replacing DT's static $\\lambda$ with an input-dependent gate is a reasonable modification\n- Evaluations are averaged over 5-seeds with sd reported, which is excellent\n- Empirical evaluation is fairly sound, though shows relatively modest results (ImageNet is convincing, mod concerns about memory/compute being held equal)\n- Something only mentioned in the appendix: they got rid of DT's $\\lambda$ schedule, using a fixed value of 0.8. This seems like a potentially useful contribution, especially if it applies only to the new gated version. Tuning the schedule sounds like a hassle\n- Qualitative results for images seem convincing"}, "weaknesses": {"value": "- More discussion of compute/memory requirements would be appreciated; it's unclear if the relatively small gains on ImageNet are worth potential additional training/inference time. The appendix mentions it's roughly equal, but seems somewhat offhand. \n- Relatively small gains compared to DT itself, except the Newsgroup dataset where DT performs suspiciously badly.\n- Undertrained CIFAR-10 baselines -- 75% accuracy on CIFAR-10 is super low, makes it hard to trust the comparisons. You could use something like mimetic initialization to compensate for small datasets (I know it's hard to train ViTs on small datasets)"}, "questions": {"value": "- Are the comparisons to DT and vanilla ViT fair wrt the use of SwiGLU? Should there be additional ablations for this?\n- Do you have any more analyses of memory/compute requirements compared to baselines? \n- The method is motivated as improving noise robustness to some extent -- have you done experiments on this in particular?\n\nI'd increase my score pretty easily if some of these things were cleared up."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AWgKFZMpA4", "forum": "9juihpPDXQ", "replyto": "9juihpPDXQ", "signatures": ["ICLR.cc/2026/Conference/Submission7337/Reviewer_H1mv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7337/Reviewer_H1mv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878430075, "cdate": 1761878430075, "tmdate": 1762919456491, "mdate": 1762919456491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Multihead Differential Gated Self-Attention (M-DGSA), a novel self-attention mechanism for Transformers inspired by lateral inhibition in biological neural circuits. M-DGSA splits each attention head into excitatory and inhibitory branches, fusing their outputs via a learned, input-dependent gating mechanism to dynamically suppress attention noise. The approach is designed to enhance robustness to corrupted or noisy inputs and integrates seamlessly into existing Transformer and Vision Transformer (ViT) architectures with minimal computational overhead. Experiments on both vision (e.g., CIFAR-10/100, ImageNet) and language (e.g., IMDB, MNLI) benchmarks show that M-DGSA improves accuracy and noise resilience over standard and Differential Transformer baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The motivation of the propopsed method is resonable introducing an interpretable gating mechanism based on lateral inhibition.\n\n+  M-DGSA shows improved accuracy and noise resilience across several vision and language tasks, outperforming baselines. It also produces sharper, more focused attention maps.\n\n+ M-DGSA can be incorporated into existing Transformer architectures with negligible computational or memory cost."}, "weaknesses": {"value": "- The gating mechanism, while lightweight, adds more complexity to the attention computation and may require careful tuning. It is not clear how the proposed method can effectively and efficiently scale up: effects on training stability, convergence speed, or performance on very large-scale or long-sequence tasks.\n\n- The evaluations are limited to synthetic noise. Most robustness experiments use synthetic corruptions, while real-world noise and other modalities e.g., cross-attention, multimodal or diffusion tasks are not explored.\n\n- While gains are consistent obtained on several benchmarks, the margin over Differential Transformer and ViT baselines is marginal, especially on saturated or simple benchmarks."}, "questions": {"value": "Please refer to the detailed questions raised in Weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MBrPpgvo5y", "forum": "9juihpPDXQ", "replyto": "9juihpPDXQ", "signatures": ["ICLR.cc/2026/Conference/Submission7337/Reviewer_ZxeQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7337/Reviewer_ZxeQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985913586, "cdate": 1761985913586, "tmdate": 1762919455808, "mdate": 1762919455808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}