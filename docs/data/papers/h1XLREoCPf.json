{"id": "h1XLREoCPf", "number": 9628, "cdate": 1758131170780, "mdate": 1759897708031, "content": {"title": "Efficient Prediction of pass@$k$ Scaling\\\\in Large Language Models", "abstract": "Assessing the capabilities and risks of frontier AI systems is a critical area of research, and recent work has shown that repeated sampling from models can dramatically increase both.\nFor instance, repeated sampling has been shown to increase their capabilities, such as solving difficult math and coding problems, but it has also been shown to increase their potential for harm, such as being jailbroken.\nSuch results raise a crucial question for both capability and safety forecasting: \nhow can one accurately predict a model's behavior when scaled to a massive number of attempts, given a vastly smaller sampling budget?\nThis question is directly relevant to model providers, who serve hundreds of millions of users daily, and to governmental regulators, who seek to prevent harms.\nTo answer this questions, we make three contributions.\nFirst, we find that standard methods for fitting these laws suffer from statistical shortcomings that hinder predictive accuracy, especially in data-limited scenarios.\nSecond, we remedy these shortcomings by introducing a robust estimation framework, which uses a beta-binomial distribution to generate more accurate predictions from limited data.\nThird, we propose a dynamic sampling strategy that allocates a greater budget to harder problems.  Combined, these innovations enable more reliable prediction of rare risks and capabilities at a fraction of the computational cost.", "tldr": "", "keywords": ["Pass@k", "sampling", "inference compute scaling", "scaling laws"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59a4091dd721493a6d7ef0b72a8254f9990048ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to efficiently and accurately predict model performance across massive repeated queries using only a limited number of trials. The authors 1) identify the limitations of two existing approaches for pass@$k$ estimation, i.e., linear regression and distributional fitting; 2) propose an alternative fitting method and a dynamic sampling strategy; and 3) demonstrate that the proposal brings significant improvements across various hyperparameters and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation and the research problem are clearly introduced at the beginning of the paper. The problem is also significant for the practical deployment of LLMs at scale.\n2. The authors contribute to the problem in multiple ways, including a formal theoretical review and the proposal of targeted solutions.\n3. The experimental results are well presented and demonstrate that the proposed method outperforms the baselines by a clear margin."}, "weaknesses": {"value": "1. Given that $k<B$, i.e., dozens to thousands of attempts, I don't think the setup corresponds to what is mentioned in L40, i.e., billions of daily interactions. \n2. The authors list the $k$-dependency as a drawback of linear regression for pass@$k$ estimation. However, as pass@$k$ inherently depends on $k$, there should be a more detailed justification for this claim.\n3. In this paper, both linear regression and distributional fitting are criticized for being inapplicable to smaller $k$. While I agree that a robust scaling law should generalize across different values of $k$, the authors’ claim that these pass@$k$ predictions are most relevant to large-scale deployment may suggest a mismatch between the critique and their stated motivation.\n4. The discussion section feels somewhat brief. Since you have pointed out the limitations of existing estimation approaches and designed your algorithm accordingly, I recommend you to provide a more detailed analysis clarifying how your proposals contribute to the efficiency and accuracy, and whether your method avoids the aforementioned limitations."}, "questions": {"value": "1. For the mention of success rate in L36, I think the typical interpretation of this metric is that it's averaged across all attempts. You may refer to the definitions of empirical probability and expected maximum toxicity in the RealToxicityPrompts paper (Gehman et al., 2020) for a more rigorous statement.\n2. Could you provide examples of the larger $k$ values mentioned in L194?\n3. The proposed method seems to be a more flexible and efficient version of the distributional fitting approach. If my understanding is correct, I would like to see the individual contributions of these two improvements.\n4. Could you elaborate on why you chose these ranges for $k$ and $B$ and how the conclusions drawn from this setting would inform the model providers and regulators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t73rm9PXXc", "forum": "h1XLREoCPf", "replyto": "h1XLREoCPf", "signatures": ["ICLR.cc/2026/Conference/Submission9628/Reviewer_CBht"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9628/Reviewer_CBht"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760755557764, "cdate": 1760755557764, "tmdate": 1762921163329, "mdate": 1762921163329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackle how to accurately predict pass@k for large k when you only have a small sampling budget. They show why popular approaches (e.g., log–log power-law regression and a discretized beta fit) give biased, high-variance extrapolations, and then propose to replace them with a beta–binomial fit to per-problem success rates and a dynamic policy that concentrates trials on the hardest problems, then shows this combo matches ground truth far better across AdvBench, MATH, and Code Contests benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper: \n- Investigates a valuable, high-impact problem: how to predict pass@k under tight sampling budgets, which matters for safety (scaling of rare failures) and capabilities (planning compute for methods like RLVR). \n\n- Provides a clear critique of existing approaches: shows why log–log regression and discretized-beta fits are statistically flawed and yield poor high-k extrapolations. \n\n- Introduces a novel solution with strong empirical evidence: a beta–binomial estimator plus dynamic sampling that focuses trials on harder items; across AdvBench, MATH, and Code Contests, it tracks ground truth much better than baselines. \n\n- Stress tests & ablations clarify when it helps most: allocate more samples to hard problems, and in heavy-tailed cases, dynamic sampling usually beats (or at least matches) uniform across distributions."}, "weaknesses": {"value": "- Certain assumptions in the proposed solution, that is i.i.d. attempts with a fixed per-problem success rate, a single Beta prior over difficulty, and assuming task stationarity (no changes in guardrails, prompts, or caching) over time, may reduce robustness of the proposed method and introduce bias.\n- Evaluations cover a few benchmarks and ~hundreds of items. Although useful but limited for broad generalization.\n- The paper primarily validates the methods using MSE to ground-truth pass@k curves, which is limited. Incorporating calibration measures and decision-centric metrics (e.g., risk at a target pass@k) would give a better picture of the proposed method’s advantages.\n- Limited number of baselines considered for comparison. Adding richer-prior baselines, e.g., Beta mixtures or Dirichlet-process priors that better handle tail misspecification, would more clearly demonstrate the proposed method’s advantages."}, "questions": {"value": "- Could you clarify why the error bars differ so much across LLMs, especially for the Gemini models compared with the others?\n- What happens under non-stationarity (model updates, prompt/guardrail changes)? Do you have a rolling or re-weighting variant that stays calibrated?\n- How does your method handle non-i.i.d. retries on the same problem. For example, when later attempts reuse earlier chain-of-thought or tool outputs (adaptive changes in success probability) or when samples are correlated (e.g., beam/diverse decoding sharing prefixes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GXzwQvHz9m", "forum": "h1XLREoCPf", "replyto": "h1XLREoCPf", "signatures": ["ICLR.cc/2026/Conference/Submission9628/Reviewer_5fkD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9628/Reviewer_5fkD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761425601878, "cdate": 1761425601878, "tmdate": 1762921163048, "mdate": 1762921163048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current LLMs have a large number of users and their behavior in such conditions is not well studied. In particular, trying to understand performance for “pass @ k” where k is a large number is important. As such, this paper tries to predict the performance of models for “pass @ k” where k is large and experiments cannot be run to provide exact measurements. The authors model task success probabilities with a beta-binomial distribution, which allows accurate extrapolation from limited data. They also propose a dynamic sampling strategy that focuses more on hard tasks, improving prediction accuracy and efficiency. Overall, their method provides a practical, data-efficient way to estimate LLM performance or failure rates at scale."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A simple method for estimating the performance of models for pass @ k"}, "weaknesses": {"value": "Not clear how the method performs for tasks that are subjective."}, "questions": {"value": "Do you have an intuition how well the method performs for tasks that involve safety, which tend to be subjective in nature (as opposed to the math/reasoning tasks you’re covering in the paper)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xng0c4YJsp", "forum": "h1XLREoCPf", "replyto": "h1XLREoCPf", "signatures": ["ICLR.cc/2026/Conference/Submission9628/Reviewer_JNYc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9628/Reviewer_JNYc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932748603, "cdate": 1761932748603, "tmdate": 1762921162802, "mdate": 1762921162802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle the problem of predicting LLMs' pass@k for verifiable problems, for a specific regime where the budget of LLM calls (for gathering data to predict pass@k) is fixed and k exceeds the average LLM budget per problem in the data set, thereby necessitating extrapolation. The authors propose making variable numbers of attempts per query by spending more of the LLM budget on problems with the low success rates. They present a theorem stating that if the probability of success is known exactly for each problem, difficult problems should receive a greater share of the budget for LLM calls, validating their approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and interesting.\n- The dynamic sampling strategy is sound and makes perfect sense for the problem: this is a valuable contribution to the literature.\n- The theorem justifying over-sampling of difficult problems is mathematically clean."}, "weaknesses": {"value": "- The framing of the significance of the problem seems misleading. For example, the cited research on brute-force jailbreaking actually varies the prompt on each attempt, so it's not directly comparable to repeated sampling. Similarly, it is not clear that \"The relevance of [predicting pass@k] is only underscored by the massive scale at which these frontier AI systems are deployed, with some experiencing billions of daily\ninteractions.\" - does massive scale imply that customers are running the same query a million times? How does the rise of reasoning LLMs affect the significance of pass@k: given the increased latency of producing a thinking trace, does it still make sense to sample repeatedly from these models?\n- Pass@1 is a fraught quantity: either an LLM is capable of answering a question, in which case it will get the correct answer reasonably quickly after some repeated trials, or the problem is beyond the LLM's capability. In the latter case, the probability of answering correctly is near zero. However, correctly estimating pass@1 for difficult problems with very low but nonzero success rates is important for accurately predicting pass@k as k -> inf. It appears that estimating pass@1 values with greater precision is the main benefit of your dynamic sampling approach, NOT the reduced variance of the pass@k estimator as described in your theorem (although that is an added benefit). The theorem assumes that pass@1 is known precisely, whereas in reality, estimating pass@1 seems to be the core problem, diminishing the significance of your theorem.\n- Your evaluation estimates each problem's ground truth pass@1 with only 10,000 samples, effectively capping the resolution of detecting nonzero pass@1 at somewhere near 1e-4. Is this justified? I could easily see true pass@1 value to be smaller than 1e-4. This resolution limit affects all your posted results, so a discussion would be valuable. Do you expect many pass@1 values to be smaller than 1e-4 in practice?\n- Your beta model for estimating the distribution of pass@1 may not adequately capture the heavy concentration of probability mass near zero. Perhaps a mixed continuous-discrete model with a discrete lump of probability at pass@1 = 0 would be a sensible approach. For example, see the paper https://openreview.net/forum?id=YCBVcGSZeR.\n- This paper is not \"scaling law research,\" which seems to imply a connection to the famous scaling laws from the pre-training literature. This paper is about pass@k and repeated sampling from LLMs. Thus, the related work section should not cover \"scaling laws\" in the abstract but stick to the paper's core question of repeated sampling from LLMs. Specifically, it would be relevant to discuss the other side of the coin of repeated-sampling: non-verifiable problems, where majority voting etc. must be applied. For example, see the paper https://openreview.net/forum?id=m5106RRLgx. Similarly, statistics papers on estimating very small probabilities could be relevant. As it stands, the positioning of the paper within the literature appears misleading and the related work section should stick closer to the knitting of the paper: repeated LLM calls."}, "questions": {"value": "- Please see \"Weaknesses\". Overall, I respect your paper a great deal and consider your dynamic sampling methodology to be an important contribution to the literature. That said, I would appreciate more discussion or clarification on the issue of very small pass@1 values, and how to interpret your evaluation in light of such issues. I'm still unsure if the paper meets ICLR's bar in terms of substantial novelty and significance, and welcome your comments.\n- Upon introducing Algorithm 1, I'd recommend clarifying that the arrays of \"attempts\" and \"successes\" are mutable arrays that start out empty and will be gradually extended (during Algorithm 2). As written, Algorithm 1 seems to require the input arrays to be fully formed. It also seems that you could describe Algorithm 1 more simply: essentially, the idea is to sample among all problems that have not yet observed a success and within all such \"difficult\" problems, the least-attempted ones are prioritized. Correct? I have this impression because in practice, the pool of problems with zero successes will likely never shrink to zero, since there will be SOME pass@1 values low enough never to yield a success.\n- Some empirical illustration of the scale and magnitude of typically observed pass@1 values (specifically the ones near zero) would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TLAJqN4PqJ", "forum": "h1XLREoCPf", "replyto": "h1XLREoCPf", "signatures": ["ICLR.cc/2026/Conference/Submission9628/Reviewer_uMij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9628/Reviewer_uMij"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957998477, "cdate": 1761957998477, "tmdate": 1762921162537, "mdate": 1762921162537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}