{"id": "QwBvxAAKWJ", "number": 7923, "cdate": 1758043384458, "mdate": 1763098714918, "content": {"title": "Reliability Scheduling for Robust Domain Adaptation under Label Noise", "abstract": "Unsupervised Domain Adaptation (UDA) becomes especially challenging when source labels are noisy, a situation common in real-world pipelines involving crowdsourcing or automated annotation. Label noise and domain shift jointly cause reliability issues: corrupted labels mislead supervision at the sample level, while ambiguous predictions hinder class-level alignment. Existing methods often address these issues in isolation with static heuristics, leading to fragile adaptation under severe noise. We introduce the Reliability Scheduling Framework (RSF), which unifies noisy-label learning and domain adaptation through multi-scale reliability scheduling. At the sample level, Confidence-Modulated Adaptive Learning (CMAL) dynamically adjusts gradients using an entropy-guided exponent, suppressing noise memorization while retaining strong signals from reliable samples. At the class level, Entropy-Guided Confusion Alignment (EGCA) reweights alignment based on prediction entropy, reducing inter-class confusion and sharpening decision boundaries. Together, CMAL and EGCA coordinate how much to learn and what to align, yielding robust transfer even under heavy label corruption. Extensive experiments on Office-31, Office-Home, and VisDA demonstrate that RSF consistently outperforms prior state-of-the-art methods across symmetric and asymmetric noise settings. These results establish RSF as a principled and effective solution for robust UDA with noisy supervision.", "tldr": "", "keywords": ["Noisy Label", "Domain Adaptation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0430a2b8e751009de88d2c03161cdf3e6a07a1d9.pdf", "supplementary_material": "/attachment/6a84c108452cf5b2c09ff61b2920721d103c24ac.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework called the Reliability Scheduling Framework (RSF) to address unsupervised domain adaptation (UDA) when source labels are noisy. RSF unifies noisy-label learning and domain adaptation.\n\nThe poposed methodology introduces a curriculum-style framework which relies on two complementary  scheduling mechanisms:\n\n1. Confidence-Modulated Adaptive Learning (CMAL), which works at sample level by controlling the amount of supervision (in order to mitigate noise overfitting) through a reliability-aware loss in place of standard cross-entropy\n\n2. Entropy-Guided Confusion Alignment (EGCA) which works at class level by reweighing alignment between source and target domains using prediction entropy: high-confidence (low-entropy) target predictions get more weight.\n\nEvaluations are conducted on Office-31, Office-Home, and VisDA under both symmetric and asymmetric noise where RSF consistently outperforms all baselines (MCC, SHOT, SENTRY, ROAD, CC-Loss, DRANet-SWD). Gains are largest under high noise.\n\nThe method has a low (~10%) computational overhead. \n\nAblation show how the two components work in combination, alone, and combined with other methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The two proposed mechanisms — Confidence-Modulated Adaptive Learning (CMAL) and Entropy-Guided Confusion Alignment (EGCA), are both technically sound and intuitively clear.\n\nCMAL generalizes and improves upon existing robust losses (CE, MAE, Focal Loss) by adaptively modulating gradients based on confidence.\n\nEGCA introduces an entropy-weighted class alignment scheme grounded in information theory, explicitly reducing inter-class confusion while maintaining discriminability.\n\nI have appreciated the gradient analysis, which theoretically justified CMAL’s stability, and hthe information-theoretic interpretation of EGCA in terms of mutual information maximization."}, "weaknesses": {"value": "I think the paper could be improved with a more thorough experimental validation, although I find it sufficient.\n\nHere follow some points of discussion:\n\n1. The experimental results are strong but limited to visual domain adaptation benchmarks (Office-31, Office-Home, VisDA). The framework’s generality would be more convincing if tested on non-visual or cross-modal UDA tasks or real-world noisy datasets.\n\n2. The symmetric and asymmetric noise utilized are not very realistic and it has been shown (e.g. in [1]) that in those cases noisy samples can be easily separated (to be treated/weighted differently). One more realistic source of noise is domain shift itself.\n\n3. No comparison under different backbone architectures (e.g., Vision Transformers, ConvNeXt) — all results rely on ResNet-50,/101, which may not reflect modern architectures.\n\n4. There is a lot of literature addressing the problem of source-free adaptation, where the problem reduces to\n   - training on source\n   - run inference on the target (producing noisy pseudo-labels)\n   - de-noise the target labels / robust learning on the noisy pseudo-labelled target set\n   It would be interesting to see how CMAL alone would work in this settings\n\n5. In appendix A.2 the authors mention \"hyperparameter validation\". Validation is a long-standing issue in UDA. In principle one should not peek at target metrics for tuning the hyper-parameters, since this would mean validating on the test set. On the other hand, source metrics are not informative. One strategy is to use a toy dataset for validation of the hyper-parameters (e.g. SVHN to MNIST) and then use the same hyper-parameters for the other benchmarks. Alternatively, [2] proposes a criterion based on the estimation of the entropy on the predictions of the target. The paper is not discussing the validation issue in any respect.\n\n[1] Cleaning Noisy Labels by Negative Ensemble Learning for Source-Free Unsupervised Domain Adaptation - WACV 2022\n\n[2] Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation - ICLR 2018"}, "questions": {"value": "I think the paper only needs a slightly stronger experimental evaluation. In particular, see my point n. 1 and 3.\n\nPoint 2 is easy to address especially in multi-domain datasets such as Office: instead of manually injecting symmetric or asymmetric noise into source label, one could use a model trained on another domain (not the source, not the target) in order to infer noisy pseudo-labels for the source domain, to be used a noisy ground-truth. This would represent a more realistic and challenging noise.\n\nPoint 4 is more of a curiosity.\n\nPoint 5 is actually quite critical since many UDA works are indeed validating on the target set in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0oeBxPDFjF", "forum": "QwBvxAAKWJ", "replyto": "QwBvxAAKWJ", "signatures": ["ICLR.cc/2026/Conference/Submission7923/Reviewer_KJQS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7923/Reviewer_KJQS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761227950868, "cdate": 1761227950868, "tmdate": 1762919946123, "mdate": 1762919946123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "dI1xpcuXEt", "forum": "QwBvxAAKWJ", "replyto": "QwBvxAAKWJ", "signatures": ["ICLR.cc/2026/Conference/Submission7923/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7923/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763098714214, "cdate": 1763098714214, "tmdate": 1763098714214, "mdate": 1763098714214, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of Unsupervised Domain Adaptation (UDA) under label noise and proposes a Reliability Scheduling Framework (RSF), which decomposes the problem into two reliability scheduling mechanisms at the sample level and class level. The sample-level module, Confidence-Modulated Adaptive Learning (CMAL), suppresses the impact of noisy samples by modulating gradients based on confidence. The class-level module, Entropy-Guided Confusion Alignment (EGCA), adjusts the alignment strength using predicted entropy to reduce inter-class confusion. Experiments validate the framework’s robustness in high-noise environments on benchmark datasets such as Office-31, Office-Home, and VisDA. However, there are issues regarding the innovativeness of RSF and certain details of the paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper focuses on unsupervised domain adaptation under label noise and proposes a Reliability Scheduling Framework (RSF) that decomposes the problem into sample-level and class-level reliability mechanisms with a clear structure.\n- Gradient visualization (Fig. 3b) and ablation studies (Table 2) demonstrate that the proposed CMAL effectively stabilizes training and improves robustness under noisy supervision.\n- Extensive experiments on standard benchmarks show that RSF achieves superior robustness, outperforming SOTA methods under high noise levels (60% and 80%).\n- Moreover, RSF is a parameter-efficient approach."}, "weaknesses": {"value": "The strategy of CMAL to enhance learning for high-confidence samples and suppress learning for low-confidence samples is not novel in recent fields of UDA, SF-UDA  or even TTA. For instance: [1] applies weighting to pseudo-label losses; [2] performs consistency-based filtering of pseudo-labels; [3] introduces a \"confusion metric\" to distinguish sample reliability; [4] partitions data into high-confidence subsets and uncertain subsets, and strengthens learning on the former. Essentially, these methods all filter samples based on confidence, and their core idea is nearly consistent with that of CMAL in this paper.\n\n[1] Guiding Pseudo-Labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation, CVPR'23\n[2] ProtoCon: Pseudo-Label Refinement via Online Clustering and Prototypical Consistency, CVPR'23\n[3] De-Confusing Pseudo-Labels in Source-Free Domain Adaptation, ECCV'24\n[4] Selective Label Enhancement Learning for Test-Time Adaptation, ICLR'25"}, "questions": {"value": "There are several questions that need to be clarified, as they may affect the final rating:\n1. The core mechanism of CMAL appears conceptually similar to several existing methods such as confidence-based filtering or entropy-weighted pseudo-labeling.Could the authors elaborate on the specific advantages or unique design aspects of CMAL compared to these approaches? For instance, does it introduce any new theoretical justification or performance improvement in terms of gradient modulation, dynamic weighting, or stability?\n2. The paper proposes EGCA, which reduces inter-class confusion through an entropy-weighted alignment matrix at the class level.\nHowever, prior works [2] and [3] also address inter-class confusion via prototypical consistency constraints and confusion-matrix regularization, respectively. Could the authors further explain how EGCA differs from or improves upon these methods, e.g., in terms of adaptivity, information utilization, or computational efficiency?\n3. Since RSF integrates CMAL and EGCA—both of which build upon established ideas—the strong performance observed under high noise conditions is somewhat unexpected. It would strengthen the paper if the authors could provide additional visualizations, comparative experiments, or pseudocode to illustrate how the proposed framework achieves its advantages and to make its effectiveness more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w0KLctCyfP", "forum": "QwBvxAAKWJ", "replyto": "QwBvxAAKWJ", "signatures": ["ICLR.cc/2026/Conference/Submission7923/Reviewer_Qzwd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7923/Reviewer_Qzwd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539016474, "cdate": 1761539016474, "tmdate": 1762919945495, "mdate": 1762919945495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Unsupervised Domain Adaptation (UDA) becomes particularly challenging when source labels are noisy, as label noise and domain shift together cause reliability problems: corrupted labels mislead sample-level supervision, while ambiguous predictions hinder class-level alignment. Existing methods usually tackle these two issues in isolation with static heuristics, resulting in fragile adaptation when noise is severe. The proposed Reliability Scheduling Framework (RSF) unifies noisy-label learning and domain adaptation via multi-scale reliability scheduling: Confidence-Modulated Adaptive Learning (CMAL) adjusts gradients dynamically to suppress noise memorization at the sample level, and Entropy-Guided Confusion Alignment (EGCA) reweights alignment to sharpen decision boundaries at the class level. Experiments on Office-31, Office-Home, and VisDA demonstrate that RSF consistently outperforms state-of-the-art methods under both symmetric and asymmetric noise, proving it an effective solution for robust UDA with noisy supervision."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to follow.\n\n2. According to the provided experiments, the proposed technique is effective.\n\n3. It is good to see the comparision between Focal loss and the proposed CMAL.\n\n4. The author provide various experiments from multiple perspectives like ComplexityAnalysis and Visualization."}, "weaknesses": {"value": "1. This field has plenty of works like [1] [2] [3] which should be compared in experiments. The authors discuss these method in related work, and mention that \"they usually treat noise  robustness and domain alignment as separate modules without aprincipled mechanism to coordinate them. Incontrast,our work introduces the Reliability Scheduling Framework (RSF), which  integrates sample-level and class-level reliability into aunified curriculum-style paradigm.\" Tha authors should provide experimental evidence to support their claim.\n\n[1] Z. Han, X. Gui, C. Cui, and Y. Yin, “Towards accurate and robust domain adaptation under noisy environments,” in Proc. 29th Int. Joint Conf. Artif. Intell., C. Bessiere, Ed., 2020, pp. 2269–2276. \n[2] Y. Zuo, H. Yao, L. Zhuang, and C. Xu, “Seek common ground while reserving differences: A model-agnostic module for noisy domain adaptation,” IEEE Trans. Multimedia, vol. 24, pp. 1020–1030, 2022. \n[3] Junbao Zhuo, Shuhui Wang, Qingming Huang. Uncertainty modeling for robust domain adaptation under noisy environments. IEEE Transactions on Multimedia. pp. 6157-6170. 2023.\n\n2. Missleading compared methods, the authors compared \"noise-resistant baseline DRANet-SWD (Soletal.,2025)\" which is only a UDA method, not for noisy UDA. Such comparisons mis-lead readers to think that their method beat SOTA, which is not true. I think such comparision is not appropriate.\n\n\n3. Line 350: CMAL+EGCA without entropy normalization already achieves 71.5%, which is not consistent with data in Table 2."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3nbsHUvnSZ", "forum": "QwBvxAAKWJ", "replyto": "QwBvxAAKWJ", "signatures": ["ICLR.cc/2026/Conference/Submission7923/Reviewer_AdBf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7923/Reviewer_AdBf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881023084, "cdate": 1761881023084, "tmdate": 1762919944842, "mdate": 1762919944842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose an instance- and category-level Reliability Scheduling Framework (RSF) for the unsupervised domain adaptation (UDA) problem under label noise in the source domain. In particular, they introduce a confidence-modulated adaptive learning loss to regulate the participation of uncertain data in source model training, and an entropy-guided confusion alignment via class-wise entropy regularization. The proposed method is validated on several UDA benchmarks, including Office-Home, Office-31 and VisDA."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript is easy to follow.\n    \n- The problem studied is interesting and represents a promising research direction."}, "weaknesses": {"value": "- The presentation of the paper, especially the Introduction, could be further polished.\n    \n- There is insufficient evaluation on larger datasets or more challenging benchmarks (e.g., DomainBed [1]).\n    \n- Some relevant related work and discussion are missing (see Questions).\n    \n- The proposed method appears to be somewhat incremental.\n    \n\n[1] Moment matching for multi-source domain adaptation."}, "questions": {"value": "1. Writing issues in the Introduction\n    \n    - In lines 71–74, the authors mention that the CMAL (sample-level) module employs entropy-based confidence estimation, while the EGCA (class-level) module emphasizes low-uncertainty target samples. As both modules depend on uncertainty information, the distinction between them is unclear. This overlap makes the presentation somewhat confusing and weakens the logical flow of the paper.\n     - The content in lines 82–92 is overly repetitive and could be substantially condensed.\n        \n\n2. Methodological concerns\n\n    - The basic idea of the CMAL component is to use a specific loss format to reduce the participation of uncertain data in the neural-network update process. However, how do the authors ensure that the uncertain ones are actually the noisy ones? What is the difference between the proposed loss term and other noise-robust losses? See [1] and [2].\n    - The difference or similarity between the proposed EGCA component and Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation (SHOT) [3] should be explicitly discussed.\n    \n\n[1] Early-Learning Regularization Prevents Memorization of Noisy Labels.\n\n[2] Symmetric Cross Entropy for Robust Learning with Noisy Labels.\n\n[3] Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YZGOyOpCO0", "forum": "QwBvxAAKWJ", "replyto": "QwBvxAAKWJ", "signatures": ["ICLR.cc/2026/Conference/Submission7923/Reviewer_aZzg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7923/Reviewer_aZzg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183764544, "cdate": 1762183764544, "tmdate": 1762919944408, "mdate": 1762919944408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}