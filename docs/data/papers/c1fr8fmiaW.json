{"id": "c1fr8fmiaW", "number": 23304, "cdate": 1758341924788, "mdate": 1759896821915, "content": {"title": "Black-box Attack Robustness with Model Diversity and Randomization", "abstract": "Query-based black-box attack algorithms can compute imperceptible adversarial perturbations to misguide learned models, relying only on model outputs. The success of these attack algorithms poses a significant problem, especially for Machine Learning as a Service (MLaaS) providers. Our study explores a new approach to obfuscate information from an attacker. To craft an adversarial example, attacks\nexploit the relationship between successive responses to queries to optimize a perturbation. Our idea to attempt to obfuscate this relationship is to randomly select a model from a diverse set of models to respond to each query. Effectively, this randomization of models violates the attacker’s assumption of model parameters remaining unaltered between queries to extract information. What is unclear is, if model randomization leads to sufficient obfuscation to confuse attacks or how best to build such a method. This study seeks answers to these questions. Our theoretical analysis proves this approach consistently increases robustness. Extensive experiments across 7 state-of-the-art attacks and all major perturbation norms ($l_\\infty$, $l_2$, $l_0$), including adaptive variants, confirm its effectiveness. Importantly, our findings reveal a new avenue for investigating robust methods against black-box attacks, offering theoretical understandings and a practical implementation pathway.", "tldr": "", "keywords": ["Adversarial Robustness", "Query-based black-box attacks", "adversarial defense"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ce19b0b06a815a8e836417ce1c651580cf5a23c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel defense mechanism called Disco (Diversity Induced Stochastic Obfuscation) against query-based black-box adversarial attacks. Unlike existing defenses that rely on adding random noise to inputs or features, the authors investigate randomizing model parameters by training a diverse set of well-performing models and randomly selecting a subset to respond to each query.\n\nThe key insight is that query-based attacks depend on successive queries with fixed model parameters to estimate gradients or search directions. By randomizing which models respond to each query, the defense obfuscates the relationship between query-response pairs, making it significantly harder for attackers to extract useful information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper provides comprehensive experimental evaluation over 7 state-of-the-art attacks across all three perturbation norms.\n\n2. The paper provides formal analysis of the defense mechanism: Proposition 3.1 quantifies query complexity, Proposition 3.2 bounds the probability of misleading gradient-free attacks, with analysis extends to adaptive attacks with EOT.\n\n3. The paper addresses real-world deployment concerns: (1) LoRA Integration (2) Multiple Diversity Methods (3) Cost Analysis.\n\n4. The method achieves consistently superior robustness across various settings."}, "weaknesses": {"value": "1. **Fundamental Novelty Concerns Regarding PuriDefense [1]**\n\nBoth papers exploit the same fundamental principle: *randomizing which function processes each query disrupts the inter-query relationships that attacks rely on*. While the authors claim differences (Appendix Q), the conceptual framework is strikingly similar:\n - PuriDefense: Maintains K purification models, randomly selects N to process input patches, feeds result to fixed classifier\n - Disco: Maintains K full classification models, randomly selects N to generate response\n\nThey both everage the principle that random selection from a diverse pool obfuscates gradient estimation and search directions.\nThe distinction between \"input-space\" vs. \"model-space\" randomization appears to be an implementation detail rather than a conceptually novel defense paradigm.\nFrom an attacker's perspective, both methods present a randomly varying function $f_i$ sampled from a set ${f_1, ..., f_K}$.\n\n2. **Theoretical Contribution Overlap**\n\nThe theoretical analysis appears to formalize what PuriDefense [1] already demonstrated:\n\n - Proposition 3.1: More queries needed when models are diverse -- This is intuitive and was shown empirically/theoretically in [1]\n - Proposition 3.2: Higher probability of misleading gradient-free attacks with diversity -- Again, this follows directly from the randomization principle established in [1]\n\nThe mathematical formalization using Hoeffding's inequality (Eq. 7) and Markov/Jensen inequalities (Eq. 8) provides rigor but doesn't offer fundamentally new insights beyond \"diversity increases query complexity,\" which [1] already established.\n\n3. **Incremental Nature of Contribution**:\n\nGiven PuriDefense established the model randomization paradigm, this work appears incremental:\n- **Training Method (SVGD+)**: The sample loss addition is a reasonable engineering improvement but doesn't constitute a major conceptual advance\n- **Diversity Promotion**: Multiple prior works (cited: DivDis, DivReg, ADP) already explored diversity for robustness\n- **Application**: Applying existing diversity techniques within the PuriDefense framework seems evolutionary rather than revolutionary\n\n[1] Guo et al. (2024), \"Puridefense: Randomized local implicit adversarial purification for defending black-box query-based attacks\" (arXiv:2401.10586)."}, "questions": {"value": "Q1: From a defense mechanism perspective, both PuriDefense and Disco implement the same high-level strategy: maintain $K$ diverse functions, randomly sample $N$ for each query, and use their combined output. Can the authors articulate why moving the randomization from input-space purification (PuriDefense) to model-space selection (Disco) constitutes a fundamentally novel defense paradigm rather than an implementation variant?\n\nQ2: Proposition 3.1 essentially states: \"diverse models require more queries for gradient estimation.\" Proposition 3.2 states: \"diverse models increase probability of misleading gradient-free attacks.\"\nDoesn't PuriDefense already establish these principles (theoretically and empirically) for their randomized purifiers? What fundamentally new theoretical insight do these propositions provide beyond formalizing the query complexity of a randomization-based defense that PuriDefense already demonstrated?\nCould the authors identify specific theoretical results that would be:\n- (a) Impossible to derive from PuriDefense's framework?\n- (b) Provide actionable insights not available from PuriDefense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "06PL7zHw0n", "forum": "c1fr8fmiaW", "replyto": "c1fr8fmiaW", "signatures": ["ICLR.cc/2026/Conference/Submission23304/Reviewer_YRWy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23304/Reviewer_YRWy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992636056, "cdate": 1761992636056, "tmdate": 1762942597534, "mdate": 1762942597534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper's goal is to design is to design a defense against adversarial examples in the black-box setting. The idea behind the proposed defense is to sample from a distribution of models, and serve incoming queries with such a sampled model. The model distribution is constructed such that model parameters satisfy some diversity property, with the intuition that the resulting classifier is no longer smooth, making it harder to estimate a quality gradient with fewer points."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- It is commendable to tackle image-based adversarial examples, given progress in recent years.\n- The presentation of the paper is neat."}, "weaknesses": {"value": "The primary weakness of this work is that this style of defense has already been studied, and is known to not work.\n\nTo elaborate, the adversarial machine-learning literature has converged upon several lessons in the past few years, with one being that defenses based on randomness *typically* do not work. As an aside, they also tend to make evaluation more confusing, but that is not the concern here. In this case, the specific class of defenses based on random transformations/ensemble diversity are very well known to lack any kind of meaningful robustness properties when a perturbation is (properly) computed in expectation across the randomness. Yes, more queries are needed to estimate a gradient when the function is not smooth. The defense still breaks, and as a community we are already well aware that it will take a few more queries to do this. You can see this is your results on the EoT \"adaptive\" attack, where robustness goes down. I am quite confident that if I or someone else could further worsen this curve with stronger implementations of EoT, or with even more queries. Note that adaptive attacks are the more important evaluation criterion when concerned with adversarial examples, i.e., we are truly concerned with worst-case behavior here. In fact, zero-knowledge adversaries are practically unlikely for image classification, particularly in today's times with the vast body of work on how to break defenses.\n\nI would like to communicate some other points. Note that you are considering a gradient of a gaussian smoothed version of your classification function, not the function itself. That is the premise of evolutionary strategies for black-box optimization. Here, the analysis is somewhat superfluous, i.e., the takeaway from the first result is a somewhat known concept - the variance of your estimate of the search distribution gradient goes down with more query samples. This comes from basic probability, and there are tons of results on this already. For example, consider a silly adversary, that makes their own life harder by using a gaussian with a very large \\sigma - this is in a similar spirit, and you can find results on the quality of the estimated gradient in terms of \\sigma. Less concerningly, but still a bit odd - with the way it is presented, I am not sure why one should concern themself with the *difference* between gradient estimate and true gradient - how that would that affect the optimization procedure?\n\nI am sure it is possible to raise arguments about the computational difficulty of serving such a defense. Yes, it would be annoying to rotate models. But this is not *my* concern - my problems with the work begin and end with the already-known robustness properties. Also, I would recommend playing with more modern black-box attacks, like SurFree, QEBA, etc. There are so many newer ones, they are really quite efficient. \n\nOverall, it is clear that effort has been put into writing and presenting the paper, and I appreciate that and respect the work from the authors towards trying to build a defense. This is a hard problem, but unfortunately, this is definitely not the solution."}, "questions": {"value": "How is this fundamentally any different from a canonical randomization-based defense? One could randomize the input query, randomize the model served, randomize anything. This simply builds a distribution around the problem. What is special about this defense, such that one cannot optimize in expectation across the distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UQWW9QVHPV", "forum": "c1fr8fmiaW", "replyto": "c1fr8fmiaW", "signatures": ["ICLR.cc/2026/Conference/Submission23304/Reviewer_gxrF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23304/Reviewer_gxrF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052031831, "cdate": 1762052031831, "tmdate": 1762942597275, "mdate": 1762942597275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Disco, an adversarial defense that randomly samples a model from a diverse ensemble to answer each query, breaking the attacker’s assumption that the target model is fixed across queries. The authors perform theoretical and empirical analysis and demonstrate that this approach significantly improves robustness across multiple attack types and perturbation norms, while maintaining clean accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- clearly written paper\n- extremely thorough experiments, comparison to baselines, exploration of the method\n- theoretical analysis\n- I especially appreciated the discussion of cost/practicality (Sec 4.5) -- the fact that Disco works with LoRA is an important point"}, "weaknesses": {"value": "- impracticality of the proposed method\n\t- need to train multiple models (10-40 per dataset) and do multiple forward passes at inference time\n\t- incompatible with deterministic APIs, which one often wants in practice\n- weak threat model and security guarantees\n\t- method operates by preventing gradient-based attacks from converging through stochasticity in function space. However, prior work [1] has argued how methods like these do not fundamentally improve the security of the system\n\t- generally makes attacks more *difficult* without preventing them: e.g. only partial robustness to adaptive attacks like EOT. DISCO requires 10x query cost, but doesn't fundamentally change the robustness of the system.\n\t- easily detectable by an attacker (and perhaps more easily defeated by a corresponding attack method) by observing that outputs differ across calls\n\nGiven these weaknesses, I don't think this paper represents a fundamental advancement in adversarial defenses. However, I still believe this paper is a well-executed, strong contribution to the community's scientific understanding of the problem."}, "questions": {"value": "- how do we reconcile the effectiveness of this defense with the fact that adversarial examples transfer across models? Would this be robust if the attacker optimizes against an ensemble of models? (I may have missed this experiment somewhere, but would generally be curious for the authors' thoughts on this)\n- describe what \"single (undef)\" and \"ensemble (undef)\" are in Table 1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oEJlDGHGlj", "forum": "c1fr8fmiaW", "replyto": "c1fr8fmiaW", "signatures": ["ICLR.cc/2026/Conference/Submission23304/Reviewer_DwGH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23304/Reviewer_DwGH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762646468037, "cdate": 1762646468037, "tmdate": 1762942597105, "mdate": 1762942597105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a defense against query-based black box attacks. Unlike defenses that inject random noise into inputs or features, Disco randomizes model parameters by responding to each query using a random subset of models sampled from a diverse pool. Theoretical analysis is provided to show that increased model diversity degrades the attacker's gradient estimation. Extensive evaluation is performed on MNIST, CIFAR-10, STL-10, and ImageNet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- valuable theoretical results\n- extensive evaluation"}, "weaknesses": {"value": "- My main concern is regarding the notion of using obfuscated gradients as a defense. Past work has demonstrated again and again that any advantage offered by the defense is usually due to inefficiencies in the attacks (Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples, by athalye et al.). The authors should evaluate against stronger adaptive attacks such as AutoAttack and also analyze attack progression with increasing number of steps.\n\n- The authors should also compare with other work on diversity based defenses such as TRS: Transferability Reduced Ensemble via\nPromoting Gradient Diversity and Model Smoothness, by Yang et al."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "INcOIanWIG", "forum": "c1fr8fmiaW", "replyto": "c1fr8fmiaW", "signatures": ["ICLR.cc/2026/Conference/Submission23304/Reviewer_5i4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23304/Reviewer_5i4w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762796422903, "cdate": 1762796422903, "tmdate": 1762942596919, "mdate": 1762942596919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They propose a defense against black-box adversarial attacks. The main idea is to train multiple models, randomly select some of them, and output the mean logit."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) With theoretical analysis.\n2) The training loss and efficiency are improved.\n3) The experimental result shows superiority."}, "weaknesses": {"value": "1) I find a counter-example against Proposition 3.1. Let the probability mass function of $g(x)$ be\n$$\n\\begin{cases}\n\\epsilon,&g(x)=a\\\\\\\\\n1-2\\epsilon,&g(x)=0\\\\\\\\\n\\epsilon,&g(x)=b\n\\end{cases}\n$$\nwhere $a<0<b$ and $0<\\epsilon<0.5$. Then we have $a\\leq g(x)\\leq b$. Let $|a-b|\\to\\infty$ and then the bound in (7) can be arbitrarily large. Meanwhile, let $\\epsilon\\to 0$ and then $g(x)$ can be estimated with arbitrary precision, i.e. $n$ can be small. It is a contradiction.\n\n2) Proposition 3.2 merely indicates the vulnerability of low diversity, rather than the robustness of high diversity. You should provide the lower bound instead of upper bound.\n\n3) The proof employs Taylor approximation, which may lead to errors.\n\nMinor revisions: At Lines 771~777, $\\pmb{u}\\nabla F(\\pmb{x})$ should be $\\pmb{u}^T\\nabla F(\\pmb{x})$, and the last two $\\approx$ should be $=$. Similar mistakes are in other places."}, "questions": {"value": "What are the values of $\\mu_1,\\mu_2,\\ldots,\\mu_K$ in your experiment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s3dUkuDxbi", "forum": "c1fr8fmiaW", "replyto": "c1fr8fmiaW", "signatures": ["ICLR.cc/2026/Conference/Submission23304/Reviewer_sojJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23304/Reviewer_sojJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23304/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762970697436, "cdate": 1762970697436, "tmdate": 1763001855138, "mdate": 1763001855138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}