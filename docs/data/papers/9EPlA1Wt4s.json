{"id": "9EPlA1Wt4s", "number": 10929, "cdate": 1758184945985, "mdate": 1763723209314, "content": {"title": "REL-RAG: Relation-Aware Retrieval-Augmented Generation for Generalizable Knowledge Graph Question Answering", "abstract": "Large Language Models (LLMs) augmented with knowledge graphs (KGs) have been widely studied for knowledge graph question answering (KGQA). Graph-based retrievers exhibit strong empirical performance, but their generalization ability remains limited.  \nIn this work, we show that applying a *line graph transformation* to the KG provably enhances the generalizability of GNN-based retrievers. By elevating relations to first-class objects, line graphs encode relation transitions explicitly, and the resulting inductive bias aligns naturally with relational reasoning in KGs. This alignment makes multi-hop reasoning substantially easier to learn and improves generalizability across different types of distribution shifts.  Building upon this representation, we propose $\\texttt{REL-RAG}$, a framework that emphasizes relational reasoning for graph retrievers and is equipped with two complementary training objectives for flexible integration with LLMs. Path-based learning achieves higher precision with fewer tokens, making it especially suitable for smaller LLMs with limited context capacity. Triple-based learning encourages richer evidence diversity, which stronger LLMs can exploit more effectively with larger token budgets.  Empirically, $\\texttt{REL-RAG}$ establishes new state-of-the-art results on KGQA benchmarks, surpassing prior graph retrievers by up to $20.3\\\\%$ with Llama3.1-8B and $10.3\\\\%$ with GPT-4o-mini.", "tldr": "", "keywords": ["KGQA", "RAG", "graph neural networks", "graph retriever", "line graph"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebf3e7203b3e7ab6b4b880fabe757ad26dbf8ee6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes REL-RAG, a GNN-based RAG solution for knowledge graph question answering. \nThe main contribution of REL-RAG lies in follows. First, REL-RAG proposes to transform KG into a line graph, which each triple is a node. The REL-RAG then devises a message passing mechanism on this line graph. From the theoretical perspective, the authors prove that owing to line-graph, GNN retriever has better generalization. The experimental results also empirically show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed method shows to be effective, achieving better performance than baseline method Sub-graph RAG and GNN-RAG. \n\n2.\tThe authors theoretically proves the better generalization of the proposed method, making the main claims theoretically grounded. \n\n3.\tThe manuscript shows to have comprehensive experimental design, which empirically reconfirms the conclusion by testing OOD performance."}, "weaknesses": {"value": "1.\tThis paper contains several confusions need to clearly defined or explained.\n\n2.\tThe paper states how to perform message passing and path encoding in section 3, but does not introduce how to select out the best relation path that contributes to the question. To be specific, there exists a clear logic gap between Section 3 and Section 4. This reviewer cannot even make an educatable guess that how z_t^{(1)}  in equation (5) contributes in equation (10) and (11). \n\n3.\tIn Appendix E.1, the manuscript mentions “when multiple valid paths exist”. However, in Section 4 and Appendix B (Inference.), how to select valid path in path-based learning is not explicitly mentioned. This reviewer can only make an educatable guess that we can calculate the product of each triple within the path and sort them. In addition, the term “predicted probabilities” mentioned in line 1216/1217, lack clear definition. This reviewer can only make an educatable guess that the probability of a triple is the softmax term exp(z_q, z_q(i))/ \\sum … in equation (75).\n\n4.\tBased on the available information provided in this manuscript, it can be hard for an educatable researcher to construct the system and replicate the results. Due to the logical gaps and the absence of precise definitions, this reviewer believes that researchers outside the KBQA-related areas may be completely lost when reading this paper.\n\n5.\tThis paper lacks discussion and/or comparison with several baseline methods, namely DoG [1], FastToG [2], GoG [3], PoG [4], KARPA [5], SRP [6], RAR [7], and READS [8], where [1-5] are accepted to top-tier conferences. \n\n[1] Li et al., Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs through Generation of Well-Formed Chains (ACL2025)\n\n[2] Liang and Gu., Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large Language Model on Knowledge Graph (AAAI2025)\n\n[3] Xu et al., Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering (EMNLP2024)\n\n[4] Chen et al., Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs (NeurIPS 2024)\n\n[5] Fang et al., KARPA: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model’s Reasoning Path Aggregation (ACL 2025)\n\n[6] Zhu et al., Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering\n\n[7] Shen et al., Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA\n\n[8] Xu et al., LLM-based Discriminative Reasoning for Knowledge Graph Question Answering"}, "questions": {"value": "1.\tWhat does “First-class objects” means needs to be clearly defined in the manuscript. It appears 4 times in the manuscript but neither of its occurrences is associated with clear definitions / citation to other paper. \n\n2.\tAlthough theorems and proofs can lend rigor and credibility to the main claims, ***their inclusion should not come at the expense of the main sections’ overall logical coherence***. According to W2, this reviewer strongly suggests the authors to move equation (74)-(76) to the main sections and provide more detailed elaboration. \n\n3.\tWhich specific work does “prior work” in line 354 means? Are embeddings of entities and relations learnable? For a “representation learning” conference, these are highly important. This reviewer sincerely requests the authors to add relevant information, including explanations and citations. \n\n4.\tThis reviewer sincerely requests the authors to discuss related works [1]-[8] mentioned before, and compare [1]-[5] accordingly.\n\nIn view of the manuscript’s current organization, this reviewer recommends rejection. ***Nonetheless, given the paper’s findings and contributions, this reviewer would consider raising the score contingent on a comprehensive improvement of the overall organization***."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LOsAxVatBy", "forum": "9EPlA1Wt4s", "replyto": "9EPlA1Wt4s", "signatures": ["ICLR.cc/2026/Conference/Submission10929/Reviewer_Lrvk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10929/Reviewer_Lrvk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761096957180, "cdate": 1761096957180, "tmdate": 1762922129026, "mdate": 1762922129026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "REL-RAG proposes a line graph transformation that better adapts knowledge graphs for message passing in KGQA tasks. By treating triplets as nodes in this transformation, REL-RAG enables two learning strategies: sequential path-based learning and classic node classification (equivalent to triplet retrieval). Experimental results on WebQSP and CWQ benchmarks demonstrate that REL-RAG achieves competitive performance against other graph retrievers, with the line graph showing better generalization compared to the original graph structure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1) REL-RAG operates on a line graph that treats triplets as nodes, better suiting triplet retrieval with GNNs. By compacting triplets into graph nodes, the underlying GNN requires fewer layers to work effectively and the line graph transformation enables REL-RAG to employ GNNs for both path-based learning (sequential predictions) and triplet retrieval tasks (Section 4).\n\n- S2) Table 2 demonstrates that the line graph achieves better generalization compared to the raw graph. This improvement likely stems from triplets having richer semantics than individual nodes, making unseen triplets easier to handle than unseen nodes."}, "weaknesses": {"value": "- W1) REL-RAG's theoretical analysis (Section 3) assumes GCN-style MPNNs that aggregate information from all relations (Eq. 1). However, practical GNN/MPNN implementations use query-conditioned message passing based on semantic similarity between the query and relations/subgraphs (NSM, GNN-RAG), which the current theory doesn't capture. This limits the insights to a narrow case; the authors should also demonstrate that line graph transformation benefits other GNN architectures beyond GCN.\n\n- W2) REL-RAG requires storing embeddings for all triplets (edges), making it computationally impractical for billion-scale or real-world graphs. While REL-RAG's performance may benefit from triplets \ngeneralizing better than graph nodes (e.g., handling unseen entity names), its practical application is limited its storage requirements.\n\n\n-  W3) The paper makes some overclaims on performance results. Line 369 states \"REL-RAG achieves improvements of up to 20.3% on CWQ,\" but the actual results show GNN-RAG at 66.8% versus REL-RAG at 67.2%, which is only a 0.4 percentage point difference. Given that REL-RAG retrieves more triplets than GNN-RAG (Table 5), these improvements appear marginal."}, "questions": {"value": "- Q1) How is the question triplet $v_{q(0)}$ obtained in Section 4? This appears to be a crucial component but I could not find the explanation.\n\n- Q2) Can the line graph transformation benefit other KGQA GNNs beyond vanilla GCN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nPUGs7UQwB", "forum": "9EPlA1Wt4s", "replyto": "9EPlA1Wt4s", "signatures": ["ICLR.cc/2026/Conference/Submission10929/Reviewer_Sq8a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10929/Reviewer_Sq8a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435531556, "cdate": 1761435531556, "tmdate": 1762922128466, "mdate": 1762922128466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REL-RAG, a relation-aware retrieval-augmented generation framework for knowledge graph question answering (KGQA). The key innovation is to transfer KG into line graph, elevating relations to first-class objects and explicitly encoding relation transitions. This paper provides a thorough theoretical analysis showing that this transformation leads to tighter generalization bounds compared to standard entity-graph representations. The model offers two training regimes: path-based learning for token-efficient reasoning with smaller LLM and triple-based learning for evidence-diverse reasoning with larger LLMs, which makes the contribution even larger and model more flexible. Experiments on WebQSP, CWQ, and GraiLQA benchmarks demonstrate improvements over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written, with clear motivation and problem formulation, and provides a compelling intuition for why line graph transformation helps - by eliminating \"relational mixing\" at entity nodes and making relation transitions explicit in the graph structure. The visual illustrations effectively communicate this idea. The paper goes beyond empirical results to provide formal analysis, including proofs that line graph models admit tighter generalization bounds under various distribution shifts (ID, compositional, and OOD). This theoretical foundation strengthens the contribution.\n\n2. Transfer KG into line-graph sounds interesting and has been proven to have good results.\n\n3. The dual training regime (path-based vs. triple-based) shows consideration for real-world deployment with different LLM capacities and token budgets.\n\n4. The experiments cover multiple datasets and different types of generalization scenarios."}, "weaknesses": {"value": "1. As for theoretical analysis, the proofs rely on several strong assumptions (e.g., sub-Gaussian concentration, specific Lipschitz constants) that may not hold in practice. The connection between the theoretical guarantees and empirical performance is not clearly established.\n\n2. While the method achieves SOTA results, the improvements are often modest (e.g., 3.1% on WebQSP with 500 triples). Given the additional complexity of line graph transformation, the cost-benefit trade-off is questionable.\n\n3. The paper mentions $O(|E|d_{max})$ preprocessing time but doesn't provide wall-clock comparisons or memory usage analysis. For large-scale KGs, this could be a significant limitation. \n\n4. Some design decisions lack justification (e.g., why specifically 50 vs 500 triples? Why 2-layer GCN?). The reliance on GPT-4o for path annotation introduces a circular dependency that isn't adequately addressed."}, "questions": {"value": "1. Have you evaluated the scalability of line graph transformation on industrial-scale KGs (e.g., full Freebase or Wikidata)? What are the practical memory and time constraints?\n2. Can you provide examples where line graph transformation actually hurts performance? Are there specific types of questions or graph structures where the approach fails?\n3. The theoretical analysis assumes the number of relations R ≤ d (embedding dimension). How realistic is this assumption for real-world KGs, and what happens when it's violated? (In ultra-large KG, the number of relations may be larger than embedding dimension)\n4. Why 50 triples are used? And why 2-layer GCN used? Can you provide some sensitivity analysis to justify your choice?\n5. The improvement on GrailQA is relatively small compared to other datasets. However, GrailQA has the smallest value, suggesting significant room for improvement. Can you provide some analysis to explain why this is the case?\n6. How sensitive is the method to the quality of the initial shortest path extraction? Have you experimented with alternative path selection strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MKpH8XLkLv", "forum": "9EPlA1Wt4s", "replyto": "9EPlA1Wt4s", "signatures": ["ICLR.cc/2026/Conference/Submission10929/Reviewer_o1im"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10929/Reviewer_o1im"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637732680, "cdate": 1761637732680, "tmdate": 1762922128067, "mdate": 1762922128067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an RAG approach by applying a line graph transformation to knowledge graphs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Flexible Training Objectives: The introduction of two complementary training objectives (path-based learning and triple-based learning) is a solid contribution. It offers flexibility to accommodate different capacities of downstream LLMs."}, "weaknesses": {"value": "**Redundancy and Complex Notation**\n\n The paper uses several mathematical definitions and formulas that may seem redundant and overly complicated, such as the introduction of ID, OOD, message-passing updates as well as the. The theorems and proofs presented in the paper provide valuable theoretical insights and enhance the depth of the work. However, they largely abstract away from the uncertainties present in real-world graph retrieval and RAG systems. From a practical or system-oriented perspective, empirical results demonstrating actual performance are more critical than the theoretical analysis.\n\n**Handling of Ambiguous Queries** \n\nThe paper does not sufficiently address how the method handles ambiguous or vague queries that require more exhaustive reasoning, such as \"Who is the largest shareholder of Tencent?\" In such cases, the system must evaluate potentially hundreds of candidates, and path-based retrieval alone would likely be insufficient. The method might be prone to \"memorizing\" the correct answers when no further context is available, especially for questions with a large number of possible answers. A more robust mechanism for handling such cases would strengthen the proposed approach.\n\n**Limited Innovation** \n\nWhile the paper claims a novel approach with the line graph transformation, the overall reasoning and training methodologies are relatively standard. \n\nThe line graph proposed in the paper actually only adds an extra layer of abstraction on top of the triples.  The method essentially wraps the triples in a new \"package\" without introducing fundamentally new reasoning mechanisms. Therefore, the use of the line graph is a redefinition of existing concepts rather than a truly groundbreaking innovation. \n\n**Insufficient Experimental Validation of Generalization:**\n\nAlthough the paper claims enhanced generalization capabilities, the experimental validation is limited to only three datasets (WebQSP, CWQ, GrailQA), which are relatively simple and traditional. The claim of generalization across distribution shifts would be more convincing with experiments on more diverse and challenging datasets. Additionally, testing on more complex and large-scale problems would demonstrate the true strength of the proposed method."}, "questions": {"value": "The bidirectional GCN on the directed and reversed graphs is mathematically equivalent to using a single GCN on an undirected graph. The authors should clarify the motivation for this design choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5Uyjgdgm1W", "forum": "9EPlA1Wt4s", "replyto": "9EPlA1Wt4s", "signatures": ["ICLR.cc/2026/Conference/Submission10929/Reviewer_c9mw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10929/Reviewer_c9mw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804815533, "cdate": 1761804815533, "tmdate": 1762922127710, "mdate": 1762922127710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}