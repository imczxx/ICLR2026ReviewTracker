{"id": "qz7g2MC32U", "number": 14998, "cdate": 1758246615347, "mdate": 1763021972753, "content": {"title": "Global and Fine-Grained Framework for CLIP with Cross-Modal Mamba in Few-Shot Image Classification", "abstract": "CLIP is a highly efficient cross-modal text-image embedding model with remarkable generalization ability. However, the encoders in CLIP usually operate independently without dynamic cross-modal interaction, leading to suboptimal performance in few-shot classification. Therefore, we propose a Global and Fine-Grained Framework for CLIP with Cross-Modal Mamba in Few-Shot Image Classification (GF4FC). Specifically, the CLIP with Cross-Modal Mamba module (CLIMA) is conducted to leverage Transformer and Vision-Transformer to interdependently encode text and image. These cross-modal representations then serve as mutual prompts to refine the embedding space, while the proposed Cross-Modal Mamba module ensures efficient time complexity. Moreover, we design a Fine-Grained Capture module (FGC) to enhance CLIMA's image representations using a Vssm module to extract prior fine-grained information. Furthermore, the Local Feature Supplementation (LFS) module is conducted to supplement CLIP's logits with FGC-derived fine-grained representations through a residual structure. Finally, the Adaptive Logits Fusion module is constructed to dynamically fuses logits using learned adaptive weights. Experiments on seven datasets demonstrate that GF4FC achieves superior performance compared with state-of-the-art methods in few-show image classification.", "tldr": "", "keywords": ["CLIP", "Multimodality", "Few-Shot Learning", "Mamba"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/80ebe555fdd5e99bf78f296ed7496fee88100987.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes GF4FC, an approach to few-shot image classification that enhances CLIP by integrating dynamic cross-modal interaction and fine-grained visual feature supplementation. It introduces four key modules: (1) CLIMA, which uses a Cross-Modal Mamba (CMM) module to enable bidirectional text-image prompting with linear complexity; (2) FGC, which extracts multi-scale fine-grained features via a VSSM; (3) LFS, which supplements CLIP logits with local features through a residual structure; and (4) ALF, which adaptively fuses global and local logits. Experiments on seven benchmarks show improvements over SOTA CLIP-based FSL methods in higher-shot settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper propose GF4FC, which combines Mamba-based State Space Models (SSMs) with cross-modal prompting in CLIP.\n2) The paper is clearly structured and the method is easy to conduct."}, "weaknesses": {"value": "1) The motivation of this paper is not clear. In fact, the interaction among modalities is widely explored, such as CLIP-MSA: Incorporating Inter-Modal Dynamics and Common Knowledge to Multimodal Sentiment Analysis With Clip, CMFS: CLIP-Guided Modality Interaction for Mitigating Noise in Multi-Modal\nImage Fusion and Segmentation, and CLIP-BCA-Gated: A Dynamic Multimodal Framework for Real-Time Humanitarian Crisis Classification with Bi-Cross-Attention and Adaptive Gating\n\n2) In Table 1, GF4FC performs worse than Tip-Adapter-F in most cases (results are wrongly marked with bold), and the improvement over 16 shots is also limited.\n\n3) Fine-grained and global features are widely used for multimodel alignment. Also, logits-based fusion is a common strategy for multimodal fusion. All these show that the novelty of this paper is limited. The authors should compare these methods and clarify the differences. \n\n4) Some important references are missing, such as Li Y, Xing Y, Lan X, et al. AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 24774-24784."}, "questions": {"value": "1) Could the authors provide qualitative examples (e.g., misclassified images on FGVC-Aircraft) showing how cross-modal interaction harms performance? Is the issue due to misaligned text prompts or Mamba’s sequence modeling bias?\n2) Why does the CLIP model perform so badly in Table 2?\n3) How about changing the fusion and alignment modules with existing mechanisms (such as CFA [1], XKanFuse[2], CoolNet[3], AlignMamba [4], or [5-6])?\n\n[1] Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection\n[2] XKanFuse: A novel cross-modal fusion method based on Kolmogorov-Arnold Network for multi-modal medical image fusion\n[3] Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis\n[4] AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment\n[5] Wan Y, Wang W, Zou G, et al. Cross-modal feature alignment and fusion for composed image retrieval[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8384-8388.\n[6] Mingyong L, Yewen L, Mingyuan G, et al. CLIP-based fusion-modal reconstructing hashing for large-scale unsupervised cross-modal retrieval[J]. International Journal of Multimedia Information Retrieval, 2023, 12(1): 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3lA864e1mg", "forum": "qz7g2MC32U", "replyto": "qz7g2MC32U", "signatures": ["ICLR.cc/2026/Conference/Submission14998/Reviewer_LGEW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14998/Reviewer_LGEW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760934231816, "cdate": 1760934231816, "tmdate": 1762925326615, "mdate": 1762925326615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "iBuMBrjKzl", "forum": "qz7g2MC32U", "replyto": "qz7g2MC32U", "signatures": ["ICLR.cc/2026/Conference/Submission14998/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14998/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763021971106, "cdate": 1763021971106, "tmdate": 1763021971106, "mdate": 1763021971106, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that enhances CLIP's performance on FSL by performing interaction between image and text modalities and fusing global and local visual features to capture fine-grained information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to understand.\n\n2. The framework comprehensively considers text–image modality interactions and integrates global and local information effectively."}, "weaknesses": {"value": "1. The main contribution appears to be a combination of existing methods, including Mamba-based attention and global--local feature fusion.\n2. The experiments mainly compare against works before 2023, which is insufficient\n\n3. If I understand correctly, the performance of the proposed method is worse than Tip-Adapter-F in most settings (1--8 shots), and only slightly better in the 16-shot setting. I think such performance is unacceptable for few-shot scenarios.\n\n| Methods       | 0-Shot | 1-Shot | 2-Shot | 4-Shot | 8-Shot | 16-Shot |\n|----------------|:------:|:------:|:------:|:------:|:------:|:-------:|\n\n| Tip-Adapter-F|   -    | 71.99  | 73.52  | 76.71  | 79.57  | 81.86   |\n\n| GF4FC |               -    | 71.26 | 72.09 | 76.27 | 79.55 | 81.95 |"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OksqgNnX9y", "forum": "qz7g2MC32U", "replyto": "qz7g2MC32U", "signatures": ["ICLR.cc/2026/Conference/Submission14998/Reviewer_WHU7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14998/Reviewer_WHU7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940126160, "cdate": 1761940126160, "tmdate": 1762925326218, "mdate": 1762925326218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GF4FC, a method that integrates Mamba and several specially designed modules to enhance the final performance in few-shot image classification. The whole story lacks of strong motivation and the model have small impacts."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Experiments are conducted on 7 datasets, and ablation studies are made.\n- The proposed method surpasses other baselines, but they are not the latest ones."}, "weaknesses": {"value": "- The research problems (or the issues of related works) lack of in-depth analyses, and all the issues (classification boundaries cannot be optimized, underutilizing cross-modal interactions, neglecting dynamic interaction) are superficial.\n- Due to the superficial summarization of existing problems, the motivations are significantly weakened. The introduction of Mamba, VSSM, and corresponding modules do not make sense. It seems to be that the authors tried so and found these modules worked, so they made up a story.\n- The baselines are ancient (2021, 2022, and 2023) and I cannot tell if the final results are really SOTA performance.\n- This paper lacks of significant analysis.\n    - If granularity is a big contribution of your method, please design metrics to demonstrate its effectiveness.\n    - If Mamba really works on this, please explain why it works by specially designed experiments or analysis rather than just showing the final performance gaps."}, "questions": {"value": "- Vssm lacks of proper definition when it first appears."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eNbr3LuMap", "forum": "qz7g2MC32U", "replyto": "qz7g2MC32U", "signatures": ["ICLR.cc/2026/Conference/Submission14998/Reviewer_vPEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14998/Reviewer_vPEZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976799148, "cdate": 1761976799148, "tmdate": 1762925325441, "mdate": 1762925325441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "See Questions"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "See Questions"}, "weaknesses": {"value": "See Questions"}, "questions": {"value": "After reading the paper, I have the following comments and suggestions that I hope the authors could take seriously:\n\n- Q1. Writing issues. The writing quality of the paper requires significant improvement. For example:\n\n-- (a) In the abstract, Cross-modal Mamba is introduced abruptly without any prior context or motivation. It's unclear why Mamba is introduced at all, and its relevance is not properly established.\n\n-- (b) In the second paragraph of the introduction, the authors categorize few-shot learning methods into three types. However, in the second type, the focus shifts to adapters for preventing catastrophic forgetting, which deviates from the main topic of few-shot learning. The explanation should be aligned with the few-shot learning context. Moreover, the discussion seems to imply that adapters are only applicable to CLIP, which is inaccurate. Also, the example of “a dog eating” appears to be irrelevant to the paper's core content and adds to the confusion.\n\n- Q2. Lack of clear motivation for using Mamba. The motivation for introducing Mamba in this work is unclear. It appears that Mamba is used merely to \"jump on the bandwagon\", rather than being a well-justified design choice. The paper claims to adopt an SSM-based approach but does not provide sufficient explanation or reasoning for this decision.\n\n- Q3. Limited novelty. The proposed contributions, such as integrating global feature enhancement with local feature supplementation, or leveraging SSM-based methods like Mamba and SSM, are not particularly novel. Similar ideas have already been extensively explored in existing literature. The contributions of this paper do not stand out as innovative.\n\n- Q4. Insufficient related work review. CLIP was proposed in 2021, and since then, a large body of research has focused on few-shot learning based on CLIP. While this paper claims to propose a CLIP-based method, it lacks a comprehensive review of relevant prior work. It only briefly mentions some basic approaches such as CoOp and CoCoOp. Additionally, the reference list contains only around 28 citations, which is far too limited for a paper in the few-shot learning domain. This suggests the authors are a lack of deep understanding of the field.\n\n- Q5. Overuse of acronyms and jargon. The paper introduces numerous method names and acronyms (e.g., GF4FC, CLIMA, FGC, LFS, CMM, ALF), which makes the reading experience overwhelming and confusing. Many of these terms do not appear to be essential or meaningful.\n\n- Q6. Other issues. \n\n-- (a) The punctuation usage is inconsistent. For instance, proper quotation marks (“ ”) should be used instead of incorrect ones (\" \").\n\n-- (b) The mathematical expressions are poorly formatted and appear unprofessional, indicating inexperience in technical writing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wxiDwWD07m", "forum": "qz7g2MC32U", "replyto": "qz7g2MC32U", "signatures": ["ICLR.cc/2026/Conference/Submission14998/Reviewer_TEZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14998/Reviewer_TEZR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762421609860, "cdate": 1762421609860, "tmdate": 1762925325115, "mdate": 1762925325115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}