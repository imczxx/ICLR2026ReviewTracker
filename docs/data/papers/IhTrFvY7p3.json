{"id": "IhTrFvY7p3", "number": 10326, "cdate": 1758167193206, "mdate": 1763719781374, "content": {"title": "MeSH: Memory-as-State-Highways for Recursive Transformers", "abstract": "Recursive transformers reuse parameters and iterate over hidden states multiple times, decoupling compute depth from parameter depth.\nHowever, under matched compute, recursive models with fewer parameters often lag behind non-recursive counterparts.\nBy probing hidden states, we trace this performance gap to two primary bottlenecks: __undifferentiated computation__, where the core is forced to adopt a similar computational pattern at every iteration, and __information overload__, where long-lived and transient information must coexist in a single hidden state.\nTo address the issues, we introduce a **Me**mory-as-**S**tate-**H**ighways **(MeSH)** scheme, which externalizes state management into an explicit memory buffer and employs lightweight routers to dynamically diversify computation across iterations.\nProbing visualizations confirm that MeSH successfully resolves the pathologies by inducing functional specialization across iterations. On the Pythia suite (160Mâ€“1.4B), MeSH-enhanced recursive transformers consistently improve over recursive baselines and outperforms its larger non-recursive counterpart at the 1.4B scale, improving average downstream accuracy by +1.06\\% with 33\\% fewer non-embedding parameters. Our analysis establishes MeSH as a scalable and principled architecture for building stronger recursive models.", "tldr": "We diagnose why recursive transformers underperform and propose a targeted solution for building stronger recursive backbones.", "keywords": ["Recursive Transformer", "Language Model", "Parameter Sharing", "Parameter Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db6b135eb9c9b315f4e79a5b6b295da0fc028678.pdf", "supplementary_material": "/attachment/872360ecf7e7cffb8b028dc494b75c9e50925fc5.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates performance bottleneks in recursive transformers used in LLMs, and identifies two issues: \"undifferentiated computing\" where the recursive computational function cannot specialize its function to different iterations, and \"information overload\" where the single hidden state needs to perform various roles. To address these issues, the paper prooses a new network design, Memory-as-State-Highways (MeSH). MeSH improves over common recurrent schemes that re-include long-term context (e.g. residual connections, or re-adding the original input) by introducing a multi-slot memory module. For this a simple Read and Write routing blocks are introduced, simple linear layers+softmax that map each D-dimensional hidden state element to one of B memory slots, such that the network can differentiate different long/short term memory needs inside the hidden state. Experiments on four Pythia LLM benchmark models show that MeSH outperforms common recursive baseline schemes, and sometimes even non-recursive baselines with significantly more parameters. The paper also presents analysis of the state per iteration, which supports that MeSH is better able to distribute its information updates over the iterations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed MeSH architecture is simple and intuitive, both conceptually and in terms of implementation.\n* The proposed MeSH architecture is tested on four models from the Pythia LLM benchmark, and is shown to outperform non-memory baseline approaches, and sometimes even non-recursive vanilla networks of similar depth but therefore more parameters.\n* The paper's story is very well-structured. The paper first presents an analysis of using various metrics and visualizations of a problem in regular recursive transformers. This analysis is used to motivate the proposed novel MeSH design, and finally the experiments refer back to the analysis and show how the new design improves the analysed network behavior. This makes the storyline and core argument of the paper very clear (although I find some arguments still doubtful, see below)\n* There are various supporting experiments, such as performance as a function of # of parameters, and learning curves. Even more experiments can be found in the Appendix. Overall, experimental validation appears thorough."}, "weaknesses": {"value": "## [W1] Relevant related work missing\n\nThe introduction of explicit memory modules in recurrent networks is not novel per se, but the paper's related work review, comparison, and discussion does not seem to make any connections to prior work in this area. One obvious parallel older than transformers are LSTMs, which also provides a memory mechanism for recursive networks to selectively update the latent state and differentiate short-term and long-term information. Various more recent works have also proposed integrated memory banks into transformer architectures, e.g. [Bulatov,NeurIPS'22][Wang,NeurIPS'23], among others. Perhaps the review in [Omidi,25] could help. In any case, there is no mention of or comparison to such works, which makes it difficult to assess this paper's contributions.\n\nI see this as the main weakness of the submission.\n\n## [W2] The paper's arguments are not clear at various points (or perhaps I do understand and simply disagree):\n* line 108: \"The information overload on the hidden state forces the model to find a low-dimensional 'common ground' representation that can safely survive multiple transformations, which directly causes loop representational collapse.\"\n\t* -> while I do not doubt the representational collapse, based on the presented diagnostics in the paper, the paper presents as cause the \"information overload\", but I don't find this a logical explanation by itself. Intuitively a low-dimensional subspace would indicate that the feature distribution is simpler than what a model can express. So, if the network has to manage \"multiple, often conflicting, roles simultaneously\" (line 103), wouldn't that mean we should expect the network to require a higher-dimensional subspace, rather than less? I don't have hard evidence for this either, but my point is that it doesn't seem so self-evident to me what causes representational collapse as is posited in the paper.\n* line 252: \"Since each router (R(t)_write,R(t)_read) has its own unique set of learnable parameters for each iteration t, the model is no longer forced to apply a single, universal transformation.\"\n\t* But also in MeSH, the f_core function remains fixed. Yes, MeSH allows to swap input and output, but the core operation is still not aware of the processing step (the \"undifferentiated computation\" of line 092). The proposed memory block operations themselves don't appear to impute information that could inform the core compute unit of the iteration, and it is unexplained how adding memory enables the model to assign specalized roles to each iteration. Of course, the model could learn to encode some step counter in the input/output statespace to inform it of its processing stage, but this was already true for the memory-less baseline.\n\n## [W3] Missing analysis of how proposed memory banks operate in practice\n* The paper lacks experiments focused on inspecting the behavior of the proposed new module, which are critical to assess if the method works because of the claimed reasons, and to validate the various claims and hypotheses made throughout the paper:\n\t* I would have liked to see some investigation if the memory model works \"as advertised\": can we see that the different iterations indeed assign different weights in the linear layer to different memory banks, and that memory written in earlier iterations is used in later iterations? Some sort of analysis on how the memory banks are read/write in real-world scenarios could help.\n\t* The effect of the number of memory banks B also belongs in the main paper, in my view. I did find the ablation study on Pythia-410M in Appendix E.1, but I was somewhat surprised a general rule of B = N_loop + 3 was propose on this single result. Investigating the effect of B on all four datasets would have made the analysis more thorough and convincing.\n\n## Minor\n* Line 133, symbols $L$ and $D$ are not explained.\n* It is not completely clear from the text what the \"Vanilla\" baseline refers to in Section 4.2. Only later in the paper did I understand that it is a non-recursive transformer, which has different parameters for each iteration.\n\n\n## References\n[Bulatov,NeurIPS'22]: Bulatov, Aydar, Yury Kuratov, and Mikhail Burtsev. \"Recurrent memory transformer.\" Advances in Neural Information Processing Systems 35 (2022): 11079-11091.\n\n[Wang,NeurIPS'23]: Wang, Weizhi, et al. \"Augmenting language models with long-term memory.\" Advances in Neural Information Processing Systems 36 (2023): 74530-74543.\n\n[Omidi,25]: Omidi, Parsa, et al. \"Memory-augmented transformers: A systematic review from neuroscience principles to enhanced model architectures.\" arXiv preprint arXiv:2508.10824 (2025)."}, "questions": {"value": "* How does the work relate to other works that have integrated memory in recurrent networks, and transformer networks specifically? What are the main differences and similarities to the proposed approach, and what does that mean for the contributions?\n* See my issues on the argumentation in Weaknesses, can the authors comment on these?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nSCBHniNwc", "forum": "IhTrFvY7p3", "replyto": "IhTrFvY7p3", "signatures": ["ICLR.cc/2026/Conference/Submission10326/Reviewer_LF4x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10326/Reviewer_LF4x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738256152, "cdate": 1761738256152, "tmdate": 1762921665243, "mdate": 1762921665243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript describes MeSH (Memory as State Highway), a novel approach to enhancing the representational power of recursive transformers, i.e., models with repeated Transformer layer blocks for better parameter efficiency. MeSH is based on computing and updating a bank of memory buffers during the looping over the recursive blocks. The results show that augmenting the recursive transformer alleviates the undifferentiated computation seen in unaugmented (base) recursive transformers (as demonstrated by the increase in the changes in the magnitude of the state update and reduced CKA metric in non-initial iterations of the recursive block). Experiments on the Pythia suite show that the MeSH augmentation significantly improves the learning and the eval performance (lower perplexity and loss and higher task accuracy) compared to both the vanilla transformer baseline and the base recursive transformer baseline in the majority of the cases. Interestingly, at the larger model size tested (e.g., 1.4B), the MeSH architecture is shown to outperform the vanilla transformer in the Pythia benchmarks, despite having roughly 30%-50% fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. Well motivated study on improving the computational efficiency and representational richness of recursive transformers. Includes thorough literature review that sets the stage nicely and makes the paper easy to follow for readers.\nS2. Includes a set of baselines including the vanilla transformer, base recursive transformer, and recursive transformer with anchor and residual augmentation. This makes the MeSH results meaningful and strong.\nS3. Analyzes the model's internal representation and how it is altered by the MeSH augmentation, which forms a nice mechanistic interpretation of the cause of MeSH's benefits.\nS4. The finding of superior performance of the smaller, MeSH-augmented recursive transformer over that of the vanilla transformer indicates a future direction of investigating the performance of MeSH for larger model sizes."}, "weaknesses": {"value": "W1. As the authors pointed out, the experiment results are limited to relative small transformers (up to 1.4B). It is not entirely clear whether the MeSH augmentation of recursive transformers can extend to larger, SOTA LLMs. The Pythia suite actually supports up to the model size of 12B, the authors did not explain why they stopped at 1.4B."}, "questions": {"value": "Q1. The use of CKA (Centered Kernel Alignment) as a metric for showing the similarity of computation over iterations of the recursive transformer needs to be better motivated.\nQ2. Table 1 and/or Section E.3 should additionally report the computational overhead (FLOPs) caused by the augmentation types (residual, anchor, and MeSH). \nQ3. The paper mentions the open source code for the implementation of MeSH, but the files in the anonymized repo appear to be unavailable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mXcIOJq5TB", "forum": "IhTrFvY7p3", "replyto": "IhTrFvY7p3", "signatures": ["ICLR.cc/2026/Conference/Submission10326/Reviewer_hA3W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10326/Reviewer_hA3W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762275186042, "cdate": 1762275186042, "tmdate": 1762921664875, "mdate": 1762921664875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the present paper, the authors introduce MeSH, a recursive Transformer backbone which utilizes a dynamic routing mechanism with read-, and write-routers which manage an external memory buffer. The proposed backbone is deeply motivated through failure analysis on existing recursive backbones. Performance is evaluated across a wide scale of trained recurrent models for perplexity across 4 datasets, and average accuracy on 10 downstream tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper has an expansive number of strengths, beginning with its analysis of failure modes of current recurrent mechanisms as the foundation the paper is built from. This clarity persists throughout the paper. Most notable are the following strengths of the paper:\n* Clarity of the problem formulation, starting with the problem analysis and the subsequent presentation of the core MeSH architecture block\n* Highly expansive evaluation with a very wide set of models trained from scratch to enable the performance evaluation of the architecture at varying sizes. In addition, the authors perform evaluation on a good number of downstream evaluations. A vast effort not to be underestimated.\n* The provided codebase permits for reproducibility, but furthermore the easy ability to build on the results\n* The authors go to great lengths to embed this work into the existing literature with references going back to the RNN era, where the first memory augmentations appeared. \n\nWhile the paper leaves a number of questions open, see the weaknesses for example, its intellectual clarity, the expansive evaluation, reproducible experiments, and good embedding into existing literature make this a very good paper which should stay significant as time progresses."}, "weaknesses": {"value": "The paper as is suffers from two core weaknesses in the eye of the reviewer:\n* While the routing mechanism is briefly introduced, there exists no ablation or design exploration amongst routing mechanisms in the paper. Further design comparisons & ablations would aid greatly here to understand inherent tradeoffs better, especially considering the orthogonality of said routing mechanism to similar mechanisms in mixture-of-expert models.\n* While the reviewer appreciates the great cost of the many experiments and trainings run by the authors, the paper would yet benefit from comparisons to other contemporary recursive models. A simple fix here could potentially be to add evaluations of recurrent Gemma, and potentially other publicly available open-weight recurrent models, to the evaluation to provide the much needed ability for comparison.\n* Comparison to the historic memory-augmented neural network of Rae et al. [1] is missing. While these are RNNs, they nevertheless display many of the same traits of the models presented here.\n\nReferences:\n1. Rae, J.W., Hunt, J.J., Danihelka, I., Harley, T., Senior, A.W., Wayne, G., Graves, A., & Lillicrap, T.P. (2016). Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes. ArXiv, abs/1610.09027."}, "questions": {"value": "* What is the author's intuition on the behavior of the routing mechanism through training, but equally as important, at inference time. The routing mechanism evokes analogies to mixture-of-expert model routing mechanisms, which pose a number of intricate concerns when seeking to deploy these models. How do you intuit these deployment concerns to play out with a MeSH-based model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qCguX575bS", "forum": "IhTrFvY7p3", "replyto": "IhTrFvY7p3", "signatures": ["ICLR.cc/2026/Conference/Submission10326/Reviewer_nSZz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10326/Reviewer_nSZz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762607698046, "cdate": 1762607698046, "tmdate": 1762921664491, "mdate": 1762921664491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to improve upon recently introduced recursive transformer models.  Such networks replace a long sequence of layers in transformer models, with a shorter sequence.  The paper claims recursive networks fail due to not encoding where they are in the looping sequence and sets out to fix this problem.  The paper does some analysis and suggests that the failure is due to two factors it calls \"undifferentiated computation\" and \"information overload\", respectively.  The solution, MeSH, uses what it calls \"lightweight routers\" to specialize computations across iterations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Brings attention to recursive transformers, which look like a very promising direction for improving efficiency of LLMs."}, "weaknesses": {"value": "Very poorly written: For example, the paper frequently introduces terminology without defining it.  Similarly it employs metrics without clear explanations of why they are the right thing to measure or even what they are attempting to measure.   \n\nUnclear novelty:  The paper does not sufficiently differentiate the proposed solution from similar strategies in the works it cites.   For example, on lines 086-087 the paper says \"the block lacks any explicit information about its progress within the iterative sequence\" but the related work explicitly tackles this problem and the reader must actually read the related works to discover this discrepancy.\n\nThe abstract and introduction say that recursive models \"lag behind\" non-recursive models, but it is unclear (a) what \"lag behind\" means and (b) which specific recursive models (e.g., which papers) this assessment applies to."}, "questions": {"value": "The description of \"+residual\" on lines 153-156 does not seem to capture the LoRA technique described in \"Bae et al., Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA\" nor does the \"+anchor\" approach described on lines 157-160.  Conceptually, how does the specialization used in \"+mesh\" differ from the use of LoRA to specialize weights across iterations in that work?\n\nI'm confused about the premise of this paper.  Section 2 heading \"Why Naive Recursion Fails\" presumes that recursive transformers are doing poorly but a cursory review of related works on these networks show the opposite -- that they work very well indeed.\n\nWhat is meant by directed computation? \n\nWhat is representational stagnation and why is CKA similarity a good way to measure it?\n\nIt is unclear to me what Figure 1a is plotting.  What is meant by 'computational effort'?  What are prelude, 1st f_core, etc. of the x-axis labels referring to?   Why is the Frobenius norm of hidden states a meaningful measure of effort?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QZzI3mN4uC", "forum": "IhTrFvY7p3", "replyto": "IhTrFvY7p3", "signatures": ["ICLR.cc/2026/Conference/Submission10326/Reviewer_4cbP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10326/Reviewer_4cbP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762623105354, "cdate": 1762623105354, "tmdate": 1762921663887, "mdate": 1762921663887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}