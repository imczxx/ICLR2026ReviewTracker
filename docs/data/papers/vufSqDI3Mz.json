{"id": "vufSqDI3Mz", "number": 4359, "cdate": 1757666141712, "mdate": 1759898037365, "content": {"title": "Training-free Task Classification for Multi-Task Model Merging", "abstract": "The advent of foundation models, coupled with the pretraining-finetuning paradigm, has ignited a proliferation of various tasks and corresponding task-specific models. \nThis has, in turn, spurred research into finding a unified system that can handle any input coming from various tasks, by combining models via weight interpolation.\nWhile many works have focused on improving the model merging process, most methods assume the knowledge of which task distribution (or task ID) each input belongs to.\nWhile few recent works have attempted to design new merging methods that can handle scenarios where task ID is unknown (task-unknown scenarios), they require either additional training or multiple number of forward passes, undermining the efficiency of a unified framework.\nIn this work, we aim to empower existing merging methods with the capability of handling task-unknown scenarios, without additional training or multiple number of forward passes.\nTo this end, we reconceptualize the pursuit of model merging for task-unknown scenarios as a task-classification challenge: identifying the task distribution a given input data belong sto. \nLeveraging Gaussian discriminant analysis (GDA), we introduce our method, \\textsc{MaD}, which identifies the task identity of input data by comparing the \\textbf{Ma}halanobis \\textbf{D}istance between input features and each task-conditional Gaussian distribution. \nConsequently, \\textsc{MaD} can be applied to existing model merging methods in an off-the-shelf manner to empower them with the capability to handle task-unknown scenarios.\nExperimental results demonstrate the effectiveness and flexibility of \\textsc{MaD} for both computer vision and natural language processing domains, under task-unknown scenarios.", "tldr": "", "keywords": ["model merging", "task classification", "training-free"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b908c430a6a30e92655f9a5f12e08f7578c0be86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MAD, a training-free framework designed to empower existing subspace-based model merging methods to operate in task-unknown scenarios. It leverages Gaussian Discriminant Analysis by first creating a merged model via simple weight averaging. Using this model, it extracts feature representations for a small sample of training data from each task and models each task's feature distribution as a multivariate Gaussian. At inference time, MAD calculates the Mahalanobis distance between the features of a new input and the pre-computed mean and covariance of each task's distribution. The task corresponding to the minimum distance is selected, and its specific binary mask is used to configure the final merged model for prediction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This approach provides a clear and creative departure from existing methods that either require training a dedicated routing network (e.g., TWIN-Merging) or rely on computationally expensive multiple forward passes (e.g., DaWin). The \"plug-and-play\" nature of MAD, allowing it to be combined with various subspace merging methods.\n* The proposed method is well-motivated and clearly described. The empirical evaluation is thorough and convincing, spanning multiple domains (vision and NLP). The authors include comprehensive comparisons against relevant baselines. Furthermore, the ablation studies on the choice of distance metric (Table 6) and the number of samples needed for distribution estimation (Table 5) provide strong support for their design choices.\n* The requirement of knowing the task ID at inference is a major barrier to deploying these models in real-world systems."}, "weaknesses": {"value": "* The method computes the initial feature distributions using a model created by simple weight averaging (θ_merge). The paper's own results (Tables 2 and 3) confirm that this \"Weight Averaging\" baseline performs poorly on the end tasks. This creates a conceptual tension: the paper implicitly argues that this poorly performing model produces a feature space with enough class separation to enable highly accurate task classification.\n* The experiments are conducted on task sets that are largely distinct (e.g., MNIST digits, EuroSAT satellite images, Cars). The paper does not explore how MAD would perform when faced with a set of very similar, fine-grained tasks (e.g., classifying 20 different species of birds or 15 different models of cars). In such scenarios, the feature distributions would likely have much greater overlap, posing a more significant challenge to the Mahalanobis distance-based classifier."}, "questions": {"value": "Some symbols in the text are not used consistently. For example, in lines 304–305, the merged task vector is sometimes denoted as $\\tau_{merge}$ and sometimes as $\\Delta_{merge}$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "guE17Z3ZM5", "forum": "vufSqDI3Mz", "replyto": "vufSqDI3Mz", "signatures": ["ICLR.cc/2026/Conference/Submission4359/Reviewer_Jwgd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4359/Reviewer_Jwgd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760598466774, "cdate": 1760598466774, "tmdate": 1762917315353, "mdate": 1762917315353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of merging task-specific models into a unified multi-task model when the task identity at inference time is unknown. Their contributions include:\n. \n* They propose to treat identifying the task of a given input as a task classification problem, without requiring extra training or multiple forward-passes through multiple models. Specifically, they use Gaussian discriminant analysis (GDA) and Mahalanobis distance on features extracted from a pre-merged model to estimate which task distribution the input belongs to.\n\n\n* The authors demonstrate the method on both vision and NLP domains under multi-task merging with unknown task IDs. They show that MAD improves performance (compared to baselines that either rely on training a router or multiple forward-passes) while maintaining training‐free and single pass inference complexity.\n\n\n* Compared to other “task‐unknown” merging methods that require multiple forward passes or a trained router network, MAD claims to provide a plug‐and‐play, efficient alternative (single forward pass, no extra training) with lower memory overhead (due to binary masks rather than full expert models)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The problem of task unknown at inference is realistic (in deployment scenarios where an input might come from any of many tasks) and under‐addressed in merging literature. The framing is compelling.\n\n\n* The claim of requiring no additional training and only one forward pass per input (rather than multiple experts) is a strong operational advantage which enhances practicality.\n\n\n* The empirical demonstration in both vision and NLP domains strengthens the argument that the method is general rather than domain‐specific."}, "weaknesses": {"value": "The method assumes that the merged model’s feature distributions for each task can be approximated by a multivariate Gaussian (mean µₜ, covariance Σₜ). While the authors provide some empirical justification (Figure 3) showing clustering/separation of features, it remains a strong assumption, especially as tasks scale or become more heterogeneous.\n\n\n* It is unclear how well the method scales when there are many (e.g., dozens or hundreds) of tasks, or when tasks are very similar/broadly overlapping. The generality to large task sets needs more evidence.\n\n\n* Since the method uses extracted features and covariance estimates from the merged model, any drift in feature representation (e.g., as new tasks are added, or distribution shifts) may degrade task classification. How robust is the method to such changes?\n\n\n* The approach assumes that the pre‐merged model yields meaningful features for task separation. If the merging is poor or tasks are interfering heavily, then the feature separation may collapse and the Mahalanobis classification may fail. The method seems to build on top of “good” merging methods rather than solving merging itself.\n\n\n\n* While Table 1 and other sections compare to other task‐unknown merging methods, it could be helpful to include classical routing approaches (trained router), open‐set classification approaches, or evaluate cost/latency vs. inference performance."}, "questions": {"value": "* Have you evaluated the method when T is large (e.g., > 20) or when tasks cover very diverse domains? What is the decline in accuracy or task‐classification error when T increases?\n\n\n* You estimate the task‐conditional feature covariance matrix Σₜ using N = 64 samples by default . How sensitive is the method to the number of samples, and to the estimation quality (especially for high dimensional features)? How do you ensure Σₜ is well‐conditioned?\n\n\n* How does your method behave when the input comes from a task that was not in the training set of tasks (i.e., an entirely new task)? Does the Mahalanobis distance raise a flag (e.g., high distance for all tasks) or will it mis‐classify to the “closest” known task? Have you considered detection of unknown tasks?\n\n\n* Could you provide detailed latency and memory cost comparisons (single forward pass with mask vs. multiple passes or trained router) in your experiments? This would help quantify the practical advantage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j9pM5DhJS9", "forum": "vufSqDI3Mz", "replyto": "vufSqDI3Mz", "signatures": ["ICLR.cc/2026/Conference/Submission4359/Reviewer_5sgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4359/Reviewer_5sgK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761456675778, "cdate": 1761456675778, "tmdate": 1762917314935, "mdate": 1762917314935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a model-merging method for task-unknown scenarios that assembles the merged model by automatically identifying the task id to which each test sample belongs at inference time. However, the proposed approach incurs additional inference-time overhead, lacks validation on large-scale models, and contains numerous grammatical and spelling issues."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper improves the accuracy of the merged model by distinguishing task IDs through the extraction of the mean and covariance matrices of each task’s data.\n- The overall structure and organization of the paper are well-designed."}, "weaknesses": {"value": "- The proposed method is straightforward, offering limited technical contribution.\n- The approach relies on extracting the mean and covariance of the data. However, most model merging scenarios assume that no data is available.\n- The MAD method requires each sample to undergo a forward pass before actual inference, which reduces inference efficiency. In Table 4, the inference cost is not compared with static merging methods such as Task Arithmetic or Ties-Merging.\n- The definition of the task-unknown scenario in this paper is inaccurate. For instance, methods like Task Arithmetic and Ties-Merging do not require task IDs, while dynamic merging approaches seem more consistent with the scenario described in this paper.\n- The paper lacks comparison with dynamic merging approaches such as WEMoE [1] and MoW-Merging [2], which are conceptually similar to Twin-Merging.\n- There is no validation of the proposed method on larger-scale language models.\n- The paper contains numerous typographical and grammatical errors, including but not limited to:\n  - Line 23: “belong sto” -> “belong to”\n  - Line 48: Incomplete usage of “either”\n  - Line 66: Incomplete “not only …” structure\n  - Line 67: “multiple forward pass” -> “multiple forward passes”\n  - Line 78: The sentence “Consequently, the input is ultimately predicted by the finally merged model.” is inaccurate—why would the input be predicted?\n  - Line 450: Redundant phrase in “Gaussian distribution’s covariance (third row) (fourth row)” (the first parenthetical note should be removed).\n\n[1]Tang, Anke, et al. \"Merging multi-task models via weight-ensembling mixture of experts.\" Proceedings of the 41st International Conference on Machine Learning. 2024.\n\n[2]Ye, Peng, et al. \"Dynamic model merging with mixture of weights.\" IEEE Transactions on Circuits and Systems for Video Technology (2025)."}, "questions": {"value": "Refer to the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BJ1vWbX7m1", "forum": "vufSqDI3Mz", "replyto": "vufSqDI3Mz", "signatures": ["ICLR.cc/2026/Conference/Submission4359/Reviewer_75xv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4359/Reviewer_75xv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658265551, "cdate": 1761658265551, "tmdate": 1762917314714, "mdate": 1762917314714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MAD, a training-free method that enables existing subspace-based model merging techniques to handle \"task-unknown\" scenarios, where the task identity of an input is unknown during inference. The core idea is to treat the problem as a task classification challenge. MAD uses Gaussian Discriminant Analysis (GDA) to model the feature distribution of a pre-merged model for each task. For a new input, it calculates the Mahalanobis distance between the input's features and each task's distribution, classifying the input to the task with the smallest distance. The corresponding task-specific binary mask is then applied to the pre-merged model for inference. This approach requires no additional training, only a single forward pass, and is memory-efficient, as it stores only a single pre-merged model and binary masks instead of all task-specific models. Experiments on vision and NLP tasks show that MAD outperforms existing task-unknown merging methods while being faster and more memory-efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of the reframing model merging for task-unknown scenarios as a training-free task classification problem using GDA and Mahalanobis distance seems novel. It offers a simple yet effective plug-and-play solution to a key limitation of existing subspace-based merging methods.\n\n2. The paper is well-supported by extensive experiments across multiple model architectures, task scales, and domains. The results demonstrate clear performance improvements over strong baselines like TWIN-Merging and DaWin. Ablation studies effectively justify key design choices.\n\n3. The paper is generally well-written and logically structured. The problem definition, motivation, method description, and experiments are clearly presented. Figures 1 and 2 help visualize the concept and pipeline effectively."}, "weaknesses": {"value": "1. The core method relies on the assumption that features from the merged model for each task follow a Gaussian distribution. While Figure 3 provides some visual evidence and prior work is cited, a more rigorous quantitative analysis or discussion of the limitations of this assumption across different models and tasks would strengthen the foundation.\n\n2. For models with very high-dimensional feature spaces (large D), estimating and inverting the full covariance matrix can become computationally expensive and numerically unstable. The paper mentions adding a small epsilon to the diagonal but does not deeply discuss the computational overhead or potential limitations for extremely large feature dimensions."}, "questions": {"value": "The experiments are conducted on CLIP-ViT for vision and T5-large for NLP tasks. Is the proposed method generalizable to other model architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PIQbSkhK1T", "forum": "vufSqDI3Mz", "replyto": "vufSqDI3Mz", "signatures": ["ICLR.cc/2026/Conference/Submission4359/Reviewer_2yxj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4359/Reviewer_2yxj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806027495, "cdate": 1761806027495, "tmdate": 1762917314513, "mdate": 1762917314513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}