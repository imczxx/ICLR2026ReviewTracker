{"id": "2pMmCDYPQi", "number": 1480, "cdate": 1756886264135, "mdate": 1763636943200, "content": {"title": "Extending Foundation Models to Low-Resource Languages: Vocabulary Expansion and Policy Optimization", "abstract": "Multilingual machine translation for low-resource languages struggles with inefficient tokenization and unstable exploration--exploitation when optimized via reinforcement learning.\n    We propose Multilingual Translation Policy Optimization (MtPO), a comprehensive three-stage framework: (1) two-stage continued pretraining that expands low-resource vocabularies, boosting compression ratios and inference efficiency; (2) curriculum-based supervised fine-tuning that ramps up task complexity across three phases while preserving general and specialized translation skills; and (3) reinforcement learning optimization that tackles length bias and diversity collapse affecting methods such as GRPO, enhanced with Reinforcement Learning with Verifiable Rewards (RLVR).\n    The RL component supplements semantic rewards with fast deterministic constraints on length ratio, structural token retention (HTML/Markdown), target-language validity, and code-mixing to harden models against messy real-world prompts.\n    MtPO couples entropy-tempered advantages, temporal decay, asymmetric clipping, and token-wise reward normalization to sustain early exploration before settling, while RLVR enforces reliable outputs without harming translation quality.\n    Experiments confirm notable gains in tokenization efficiency, translation quality, and exploration--exploitation balance, marking a substantive step forward for multilingual models serving underrepresented languages and practical deployments.", "tldr": "We provide a general language model that supports multiple minority languages", "keywords": ["Natural Language Processing", "Large Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9bd510f5e54a783cf8d5e745eda058b8b156968.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work introduces multilingual translation policy optimization, which i) expands the vocabulary of tokenizer, ii) balances mixing ratio in training corpora and iii) reinforcement learning with verifiable rewards. i) directly improves inference efficiency for low-resource languages. ii) preserves model capability in English and iii) controls length, format, target language and language consistency in the output. The objective also includes an token-position based entropy regularizer to encourage more exploration in the early stages of decoding. Overall, the approach demonstrates token efficiency, reduces token entropy, improves length stability and token diversity. Translation performance also improves in terms of sacreBLEU."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper describes a comprehensive pipeline to adapt LLM for low-resource translation.\n2. The expanded VLRL with entropy regularized objective is interesting, and potentially inspires many questions to explore. \n3. The approach demonstrates clear advantage in terms of entropy increment, token diversity and length stability."}, "weaknesses": {"value": "1. Line 187: \"Production translation systems encounter frequent failure modes: overlong outputs, format corruption, language mixing, and off-target responses... \" is there a citation for this? \n2. While the results in Table 2 shows marginal improvement of MtPO over SFT baseline, there is no evidence that any of this is a direct result of more length stability, format and language accuracy. It could be possible that vocabulary expansion alone improves translation, or that superior format accuracy (Table 1) alone makes translation better.\n3. Increasing token entropy leads to less confident models, what is the evidence that this is beneficial specifically for translations? And why not start with highly confident tokens, then explore only towards the end of the sequence, as in https://arxiv.org/pdf/2507.01679?\n4. Again, the effect of length stability on improved translation performance is unclear. The right length ratio often depends on the source and target languages. The values would be vastly different, for instance, between En-Bn and Bn-En, given their vocab difference. How is this being accounted for in the current set up?\n5. Please also report translation performance in terms of chrF, which better captures morphological variability, especially in lower-resourced languages. \n6. The derivations in Section 5 don't seem to help in our understanding of why any of the properties make sense to improve translation.\n7. The overall presentation is poor. There are no labels and captions for figures."}, "questions": {"value": "1. Table 1 shows MtPO stands out in terms of format accuracy, relative to the other models. What if format accuracy alone account for all improvement of MtPO in Table 2? What if more meaningful tokens given vocabulary expansion alone drove most of the improvement instead? Please consider adding ablation studies.\n\n\nSee also points 3 and 4 above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HG4z7NEewc", "forum": "2pMmCDYPQi", "replyto": "2pMmCDYPQi", "signatures": ["ICLR.cc/2026/Conference/Submission1480/Reviewer_218C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1480/Reviewer_218C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552317899, "cdate": 1761552317899, "tmdate": 1762915779937, "mdate": 1762915779937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The presents method to extend large language models for low resource translation via (1) vocabulary expansion, (2) fine-tuning with distilled data, and (3) a novel reinforcement learning method. This mainly follows the current state-of-the-art, it is solid work, and the particular reinforcement learning method is novel."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Solid experimentation with 8 language pairs, many of the low resource. This is generally well executed work with covincing results.\n\nNovel variant of reinforcement learning (MtPO) that introduces a number of new elements. The method needs to be explained better.\n\nThe vocabulary expansion is executed well, but it is not a novel method. You may want to emphasize this less."}, "weaknesses": {"value": "The write-up of the paper needs to be improved substantially. Key elements are not explained or explained insuffienctly.\n\nSection 3.3.2: This section needs to be restructured, so that you first motivate the approach, introduce the terms, and then have the complete equations. It is currently difficult to understand. Since this is maybe the strongest contribution, you need to explain it.\n\nThe \"analysis\" method is more a motivation for the approach - it should be in the methods section.\n\nMinor issues:\nline 68: \"falls short in production settings, where reliance on fixed translation-style instructions results in overly narrow and inflexible use cases\" -> can you explain this more and add a citation?\n\nLine 191ff: connect the terms in the equation to the text\n\nLine 310: This is the first time you mention \"KL-control schemes (K2, K3)\" - I assume that this is KL-divergence regularization, but you need to explain it.\n\nFigure 3ff: None of these figures are labeled or have a caption.\n\nLine 326: Is \"entropy collapse\" overfitting to the training data?"}, "questions": {"value": "I do not understand the long discussion of length inflation. In machine translation, the length is fixed within some margin, so this should not happen."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dH3hXcz4Fh", "forum": "2pMmCDYPQi", "replyto": "2pMmCDYPQi", "signatures": ["ICLR.cc/2026/Conference/Submission1480/Reviewer_FtdW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1480/Reviewer_FtdW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949471471, "cdate": 1761949471471, "tmdate": 1762915779706, "mdate": 1762915779706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Multilingual Translation Policy Optimization (MtPO), a unified three-stage framework 1) integrating continued pre-training, 2) curriculum-based supervised fine-tuning (SFT), and 3) reinforcement learning (RL) optimization for multilingual MT, particularly focusing on low-resource languages. The approach aims to enhance vocabulary coverage, maintain balanced multilingual performance, and mitigate issues such as length bias and diversity collapse during RL training. Experimental results demonstrate improvements in tokenization efficiency and translation quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses the challenge in low-resource multilingual translation, such as subword segmentation inefficiency and limited data availability.\n- The paper is generally easy to follow."}, "weaknesses": {"value": "- Since the proposed approach MtPO consists of three major stages, I was wondering that an ablation study is crucial to understand the contribution of each component (continued pre-training, curriculum SFT, RL optimization) to the overall translation improvement.\n- The paper primarily reports surface-level metric, sacreBLEU. Using semantic-based evaluation metrics such as COMET (https://github.com/Unbabel/COMET) would provide deeper insights into translation quality.\n- helpful if you could report the impact of tokenizer expansion not only on compression and tokenization efficiency but also on training and inference speed"}, "questions": {"value": "- Section 3 includes a little too many multiple sub-sections that could be merged. This would improve readability and free up space for more important analyses.\n- The paper primarily compares the proposed approach against generic (multilingual/translation-focused) LLM baselines. Including comparisons with previous related multilingual optimization referred in Related Work sections at least would strengthen the empirical claims and positioning within existing literature.\n- Figures 3–2 and Tables 2–1 -> Figures 2–3 and Tables 1–2 for better readability\n- Can the authors conduct an ablation study to quantify the contribution of each of the three MtPO stages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i92eWWXjNL", "forum": "2pMmCDYPQi", "replyto": "2pMmCDYPQi", "signatures": ["ICLR.cc/2026/Conference/Submission1480/Reviewer_TiQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1480/Reviewer_TiQb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970665258, "cdate": 1761970665258, "tmdate": 1762915779549, "mdate": 1762915779549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We sincerely appreciate the time and effort you have devoted to reviewing our paper and providing valuable feedback. We are deeply grateful for the constructive comments and high-quality reviews from all reviewers, which have significantly improved our manuscript."}, "comment": {"value": "Dear PCs, SACs, ACs, and Reviewers,\n\nWe sincerely appreciate the time and effort you have devoted to reviewing our paper and providing valuable feedback. We are deeply grateful for the constructive comments and high-quality reviews from all reviewers, which have significantly improved our manuscript.\n\nWe would like to express our particular gratitude for the following:\n- **Reviewer TiQb and Reviewer FtdW**: Thank you for your valuable suggestions on the manuscript organization. We have restructured the paper by elevating the priority of the experimental section and moving it to the main text immediately after the method section for better readability.\n- **Reviewer TiQb**: Thank you for requesting COMET benchmark evaluations. We have conducted additional experiments and included comprehensive COMET results in Table 3.\n- **Reviewer 218C**: Thank you for requesting chrF benchmark evaluations. We have added complete chrF scores across all translation directions in Table 3.\n- **Reviewer FtdW**: Thank you for raising insightful questions about our alignment component. We have significantly expanded Section 3.x with detailed explanations of the design rationale and implementation.\n\nIn the following, we provide detailed responses to each reviewer's comments on a point-by-point basis. We hope our responses adequately address all your concerns. All revisions have been incorporated into the updated manuscript."}}, "id": "BtBLrhkJaI", "forum": "2pMmCDYPQi", "replyto": "2pMmCDYPQi", "signatures": ["ICLR.cc/2026/Conference/Submission1480/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1480/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission1480/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763691862262, "cdate": 1763691862262, "tmdate": 1763691862262, "mdate": 1763691862262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}