{"id": "IzdfeFH88G", "number": 11469, "cdate": 1758199862291, "mdate": 1759897573660, "content": {"title": "Subspace-Guided Continual Learning: Hessian Based Stable‚ÄìPlastic Decomposition for Exemplar-Free Class-Incremental Learning", "abstract": "Exemplar-Free Class-Incremental Learning (EFCIL) presents a significant challenge in continual learning, where a model must learn new classes sequentially without access to old data, making it susceptible to catastrophic forgetting. The core difficulty lies in balancing model stability (preserving old knowledge) and plasticity (acquiring new knowledge). We propose Subspace-Guided Continual Learning (SGCL), a novel method that tackles this dilemma from a geometric perspective. SGCL functionally decomposes the feature space into two orthogonal subspaces: a ''stable subspace'' containing feature directions critical for previous tasks, and a ''plastic subspace'' where new knowledge can be learned with minimal interference. We demonstrate that this decomposition can be efficiently identified by analyzing the feature-space Hessian, where its high-curvature eigendirections define the stable subspace. Building on this, SGCL introduces two synergistic components: 1) Subspace-Guided Regularization (SGR), which imposes strong, curvature-weighted penalties on feature drifts within the stable subspace, and 2) Subspace-Guided Prototype Alignment (SGPA), which adaptively corrects the shift of old-class prototypes to recalibrate the classifier. Extensive experiments on standard benchmarks, including CIFAR-100, Tiny-ImageNet and ImageNet-Subset, show that SGCL significantly outperforms existing state-of-the-art methods. Our work provides a principled and effective approach to EFCIL, offering a new perspective on mitigating forgetting by analyzing the loss landscape structure.", "tldr": "Our method leverages the feature-space Hessian to decompose features into stable and plastic subspaces, enabling selective regularization to achieve better performance.", "keywords": ["Continual Learning", "Exemplar-Free Class-Incremental Learning", "Computer Vision", "Feature Space Decomposition", "Stability-Plasticity Dilemma"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64c06c19cfc9902f02012703bd9196a023d72c44.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Subspace-Guided Continual Learning (SGCL), a novel method for exemplar-free class-incremental learning (EFCIL). The key idea is to decompose the feature space into two orthogonal subspaces‚Äîstable and plastic‚Äîbased on the eigenstructure of the feature-space Hessian of the cross-entropy loss. The stable subspace captures directions with high curvature that are critical for preserving previous knowledge, while the plastic subspace allows adaptation to new tasks. Building on this, the authors design two components: Subspace-Guided Regularization (SGR), which applies curvature-weighted penalties to prevent forgetting, and Subspace-Guided Prototype Alignment (SGPA), which corrects class-prototype drift and recalibrates the classifier. The method is theoretically grounded, computationally efficient, and empirically validated on several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method provides a clear geometric interpretation of continual learning by decomposing the feature space using the eigenstructure of the feature-space Hessian. This offers a mathematically grounded alternative to heuristic or empirically tuned regularization schemes.\n\n2. The curvature-weighted penalty mechanism (SGR) explicitly controls drift along critical directions, allowing an interpretable balance between stability and plasticity rather than a uniform penalty."}, "weaknesses": {"value": "1. Sensitivity to hyperparameters:\nThe method‚Äôs performance strongly depends on the choice of the stability and plasticity weights (Œª‚Çõ and Œª‚Çö). Since there‚Äôs no adaptive or self-tuning mechanism, the optimal balance must be found manually for each dataset or architecture, which limits its general applicability and robustness.\n\n2. Approximation of the Hessian:\nSGCL relies on an expected feature-space Hessian estimated through softmax probabilities. This assumes the loss landscape is locally smooth, which may not hold in highly non-linear or deeper models. As a result, the estimated curvature directions might not always represent the true ‚Äústable‚Äù directions, leading to imperfect subspace separation.\n\n2. Dependence on classifier conditioning:\nThe subspace extraction process assumes that the classifier weight matrix \nùëä\nùë°\n‚àí\n1\nW\nt‚àí1 is full-rank and well-conditioned. In practice, as the number of classes grows, some directions can become degenerate or highly correlated, making the QR decomposition unstable and potentially distorting the identified subspace.\n\n3. Simplified distributional assumption:\nThe prototype alignment module (SGPA) assumes that each class‚Äôs feature distribution is roughly Gaussian when generating synthetic samples for calibration. However, deep feature spaces are often multi-modal and non-Gaussian, which can lead to inaccurate prototype correction and weaker classifier adaptation.\n\n4. Rigid subspace separation:\nThe method treats the stable and plastic subspaces as strictly orthogonal, but in real feature manifolds, these directions are often correlated. This rigid projection-based split might discard useful shared information, causing minor interference or loss of representational richness over time."}, "questions": {"value": "1. The evaluation metrics needs more clarity. You introduced new metric, average anytime accuracy, without explaining. Normally, we Average Incremental Accuracy and Last accuracy is reported. Do they mean same?\n\n2. Equation 10. Computing and compensating the drift between the features has already been explored in SDC and LDC. Specifically, the LDC compensates the drift using learnable projector. How this orthogonal projection is helping the model not to forget? \n\n3. Retraining the for calibration is post-training process? What is the computational overhead for this process?\n\n\nReferences\n\nLDC: Gomez-Villa, Alex, et al. \"Exemplar-free continual representation learning via learnable drift compensation.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n\nSDC : Yu, Lu, et al. \"Semantic drift compensation for class-incremental learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "McsEdWjEy6", "forum": "IzdfeFH88G", "replyto": "IzdfeFH88G", "signatures": ["ICLR.cc/2026/Conference/Submission11469/Reviewer_HLnu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11469/Reviewer_HLnu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675695823, "cdate": 1761675695823, "tmdate": 1762922575010, "mdate": 1762922575010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method, SGCL, for the exemplar-free class-incremental learning (EFCIL). SGCL utilizes the Hessian martrix in the feature space to impose seperate regularization on the stable subspaces (with large loss curvature) and plastic subspaces (with smaller loss curvature) respectively via subspace projection process. The projection basis is obtained by an efficient QR factorization from the weight matrix. Subsequently, a prototype alignment process using the stability information is introduced to adrress the shift of old prototypes. Experiments are conducted to verify the performance of SGCL and its components."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, this paper is well written and easy to follow. \n2. The theorectical analysis in this paper is solid.\n3. Decomposing the stable and plastic subspaces in the perspective of curvature is noval and effective."}, "weaknesses": {"value": "1. The information of stable subspaces seems only workable for the latest task. The stable subspaces used in task $t$ relies on the Hessian matrix calculated with the data in task $t-1$, which seems that this information only works for the perservation of important knowledge of task $t-1$ and not for previous tasks. Can this regularization benefit the previous knowledge or can the subspace decomposion cover the previous tasks?\n2. Limitation in proposition 1. The proposition 1 requires a full row rank $ {\\mathbf{W}_{t-1}}$.  However, for learning task on a larger dataset like ImageNet-1k where $d \\gt c$ , it is likely that  it will not have full row rank.  As such, proposition 1 may not hold and what will happen on the subspace decomposition? Corresponding analysis and experiments should be included. The experiment may be conducted on ImageNet-1k since the ResNet-18 has $d = 512$ and $c = 1000$.\n\n\n3. The results of the plastic regularization seems not consistent. Intuitively, the regularization on plasticity can hinder learning on the new tasks. The corresponding experiment in Figure 5 also validates this, where accuracy of current tasks experiences a performance drop as the $\\lambda_{p}$ increases. However, without the plastic regularization, the overall performance of SGCL drops. Why this happen and could you please include further analysis on it?\n4. The methods selected in the comparative study are a bit old. Since this paper is summited to ICLR 2026, the comparative studies should contains methods proposed in 2025, for example, the DPCR [1] and etc. Also, there are several methods considering stability / plasticity feature space decoupling via gradient projection [2]. It is interesting to compare SGCL with them. \n\n[1] Run He, et al. Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning. In ICML 2025.\n\n[2] Zhen Zhao, et al. Rethinking Gradient Projection Continual Learning: Stability / Plasticity Feature Space Decoupling. In CVPR 2023."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NfAviFiHTB", "forum": "IzdfeFH88G", "replyto": "IzdfeFH88G", "signatures": ["ICLR.cc/2026/Conference/Submission11469/Reviewer_f2nm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11469/Reviewer_f2nm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829644181, "cdate": 1761829644181, "tmdate": 1762922574647, "mdate": 1762922574647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the cold start class-incremental learning (CIL). It presents SGCL, an exemplar-free prototype-based approach with two synergistic components: subspace-guided regularization (SGR) and subspace-guided prototype alignment (SPGA). SGCL is motivated by the assumption that the drift of the decision plane with higher curvature need to be restricted to keep stability, and the plasticity can be improved by learning on the subspace with low-curvature decision boundaries. Therefore, SGCL divides the feature space into two subspaces (a stable subspace and a plastic subspace) based on the Hessian of the loss function. These two spaces are identified by an efficient algorithm based on the low-rank structure of the feature-space Hessian. SGR decomposes features into the stable and the plastic subspaces, while SPGPA corrects the drift of the prototypes. The authors demonstrate state-of-the-art performance on several benchmarks and provide extensive ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper focuses on the cold-start EFCIL setting, which is a challenging reclaim in continual learning.\n2. The shift from parameter to feature space and the use of Hessian curvature for a geometrically grounded stability-plasticity decomposition is an insightful contribution."}, "weaknesses": {"value": "1. The optimum of the subspace division is neither experimentally nor theoretically verified. Although this division, based on feature space Hessian, seems to be intuitive, we still need to verify that it is optimal and better than other potential division schemes.\n\n2. The paper lacks key information. For example,\n    1. How do you select the hyper-parameters (e.g., $\\lambda_s$ and $\\lambda_p$) in this paper? What is the division of the training/validation/testing dataset? Is there a guideline for tuning these hyper-parameters when applying SGCL to a new dataset?\n    2. What is the ImageNet-Subset dataset? How it is sampled from the full ImageNet dataset is not justified nor referenced.\n    3. How we can obtain $\\mathbf{W}_t$ from $\\mathbf{W} _{t-1}$ is not shown in the pseudo-code (Algorithm 1). It seems that the previous classifier weight $\\mathbf{W} _{t-1}$ is never used in Algorithm 1.\n    4. In line 1 of Algorithm 1 (line 273), how $\\theta_{\\text{prev}}$ and $\\mathcal{S}$ are initialized is not justified in this paper.\n\n3. Many of the equations in this paper are confusing. For example,\n    1. What is the difference between $W_t$ (line 176, 280, 232, and Figure 1) and $\\mathbf{W}_t$ (line 151)?\n    2. What is $\\mathcal{L}$ in line 196 and beyond? Is it the same as $\\mathcal{L}_{ce}$ in Eq. (1)?\n    3. $\\operatorname{argmin}$ is a stand-alone operator, but is separated in Eq. (1).\n    4. The transpose operator is usually non-italic. However, it is italicized in this paper.\n    5. Eq. (4) is not obvious. Please supplement its derivation process to make this paper easier to follow.\n    6. What is the definition of $\\Sigma_i$ in line 304? How it is calculated is not shown in Algorithm 1, and its cost is not considered in this paper when analyzing the computational complexity.\n\n4. The paper confuses textual and parenthetical citations. The authors used textual citations in many places where parenthetical citations should be used."}, "questions": {"value": "My key concerns about this paper are listed in the Weaknesses section. Besides, I list some of my concerns that do not impact my rating:\n1. Can you provide visualizations of feature distributions and decision planes for different classes that change as the number of classes increases? This may help us visually observe the effect of SGCL in the feature space.\n2. Can you provide the standard deviation or confidence interval of the accuracy reported in the paper, randomly varying the class order? This will enhance the statistical significance of the conclusion.\n3. Using pre-trained models to solve cold-start CIL has become a popular solution in recent years [1]. Can you provide experimental results using pre-trained ViT as the backbone network?\n\n---\n\n[1] Zhou, Da-Wei, et al. \"Continual learning with pre-trained models: A survey.\" *Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence*. 2024. doi:[10.24963/ijcai.2024/924](https://doi.org/10.24963/ijcai.2024/924)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WkxuC615Th", "forum": "IzdfeFH88G", "replyto": "IzdfeFH88G", "signatures": ["ICLR.cc/2026/Conference/Submission11469/Reviewer_4taE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11469/Reviewer_4taE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829746475, "cdate": 1761829746475, "tmdate": 1762922574266, "mdate": 1762922574266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to tackle the stability-plasticity dilemma in exemplar-free class-incremental learning (EFCIL) from a geometric perspective. The authors propose to decompose the feature space into a stable subspace and a plastic subspace by interpreting the feature-space Hessian. This allows to add extra regularization to prevent feature drifts explicitly on the stable subspace. The proposed method also incorporates a prototype-alignment module to recalibrate the classifier for old classes. The proposed method outperforms existing methods across benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written with good motivation covering relevant works in EFCIL. The proposed method is novel and intuitive.\n2. Exploiting the feature-space hessian to decompose into subspaces is interesting.\n3. The extensive experiments and ablations are appreciated."}, "weaknesses": {"value": "1. The proposed method could be further validated on fine-grained classification tasks on datasets like Cars or CUB following ADC, LDC.\n\n2. Some analysis and comparison of the SGR regularization with full model feature distillation (without decomposing into stable and plastic directions) could highlight the significance of regularizing only the stable directions. Similarly, the impact of using the stability score $S_i$ for prototype alignment should be evaluated.\n\n3. Since the prototype alignment stage is after the feature extractor is updated, this could be directly compared with methods like SDC, ADC and LDC which are drift correction techniques. For instance, is the SGPA approach better than these methods? How would the combination of SGR+ADC or SGR+LDC perform in comparison to SGR+SGPA?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iipAy8D3Rx", "forum": "IzdfeFH88G", "replyto": "IzdfeFH88G", "signatures": ["ICLR.cc/2026/Conference/Submission11469/Reviewer_qRq9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11469/Reviewer_qRq9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210215832, "cdate": 1762210215832, "tmdate": 1762922573885, "mdate": 1762922573885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}