{"id": "msX2KKICRr", "number": 23121, "cdate": 1758339886064, "mdate": 1759896831523, "content": {"title": "Emergence of Exploration in Policy Gradient Reinforcement Learning via Retrying", "abstract": "In reinforcement learning (RL), agents benefit from exploration because they repeatedly encounter the same or similar states, where trying different actions can improve performance or reduce uncertainty; otherwise, a greedy policy would be optimal. We formalize this intuition with ReMax, an objective that evaluates a policy by the expected maximum return over $M$ samples ($M \\in \\mathbb{N}$), while accounting for return uncertainty. Optimizing this objective induces stochastic exploration as an emergent property, without explicit bonus terms. For efficient policy optimization, we derive a new policy-gradient formulation for ReMax and introduce ReMax PPO (RePPO), a PPO variant that optimizes ReMax while generalizing the discrete retry count $M$ to a continuous parameter $m > 0$, enabling fine-grained control of exploration. Empirically, RePPO promotes exploration without bonuses and outperforms entropy-regularized PPO on the MinAtar benchmark.", "tldr": "", "keywords": ["Exploration", "Policy gradient"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/315c7a56f126f561986818ec8d97cdbf897b7527.pdf", "supplementary_material": "/attachment/77d7444b6d93a84305fc32757015d1958ee87eb7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ReMax objective to train explorative reinforcement learning policy. The authors claimed this objective can serve an alternative approach to exhibit exploratory without adding curiosity bonus."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The objective proposed in this paper, ReMax, is new to the RL community.\n2. The authors provides proof to justfiy the objective of the proposed algorithm."}, "weaknesses": {"value": "1. The algorithm is not well-motivated, and the paper is not well-organized. \n* The authors should better address the question: \"Why not use curiosity bonus? Why should anyone consider using ReMax objective in their work?\"\n* It is good to have a warm-up section, however, without enough motivation or background, it looks more like a part of methodology section.\n\n2. The experiment result is not convincing, with the following flaws:\n* Only 4 environments were used to test RePPO on a small-scale domain like MinAtar. This is too limited.\n* The authors claim that ReMax is an alternative exploration method. The experiments are not conducted on the environments that require exploration. \n* The baseline should include curiosity-based exploration (explicit bonus). Entropy regularized exploration (used by original PPO paper) has been observed to be insufficient for exploration."}, "questions": {"value": "See weakness. Please try to address the problems mentioned there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hLhCPIb39s", "forum": "msX2KKICRr", "replyto": "msX2KKICRr", "signatures": ["ICLR.cc/2026/Conference/Submission23121/Reviewer_vuQR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23121/Reviewer_vuQR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768757264, "cdate": 1761768757264, "tmdate": 1762942518722, "mdate": 1762942518722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReMax, a novel RL objective that induces exploration as an emergent property of maximizing the expected maximum return over M retries under uncertainty, without explicit bonuses; it formalizes this in bandits where stochastic policies become optimal for Mâ‰¥2 and adapts to reward variance, then extends to MDPs via Q-function retries, deriving an estimation-friendly policy gradient using expected improvement and generalizing M to continuous m>0. The proposed RePPO, optimizes ReMax with implicit Q-uncertainty, achieving superior performance and higher entropy than entropy-regularized PPO on experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The article exhibits logical coherence by systematically framing exploration as reward maximization under retrying uncertainty, introducing the ReMax objective, demonstrating its efficacy in inducing adaptive stochastic policies in bandits, and extending it to RL via Q-function emulation and an on-policy actor-critic adaptation in RePPO, which delivers robust superior performance on MinAtar with higher scores and policy entropy than entropy-regularized PPO; this is underpinned by a rigorous theoretical foundation, including a closed-form inner expectation and an intriguing expected improvement-based policy gradient derivation that enables single-trajectory estimation and continuous retry parameterization for precise exploration control."}, "weaknesses": {"value": "The reliance on Q-function approximation to emulate retries in non-resettable simulators, a practical workaround for RL extensions that lacks dedicated empirical verification of its approximation quality or potential biases, potentially undermining the objective's fidelity; furthermore, experimental validation is confined solely to the lightweight MinAtar benchmark, limiting evidence of RePPO's scalability and robustness across more complex, diverse, or real-world RL environments, thus insufficiently substantiating the method's broader effectiveness."}, "questions": {"value": "1. Can you provide more validation for the Q-function approximation?For example, comparisons with ground-truth Q values and comparisons with other Q-estimation methods.\n2. Can you design more detailed experimental verification? Such as horizontal comparisons of more complex scenarios (Robomimic, D4RL) and different exploration methods (UCB, Thompson, Ornstein, Entropy-regularized...) under various RL algorithms (PPO, TD3...).ation methods.\n3. As a higher-order statistical measure, does the variance of EI need further verification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JUVQc1IDYM", "forum": "msX2KKICRr", "replyto": "msX2KKICRr", "signatures": ["ICLR.cc/2026/Conference/Submission23121/Reviewer_PEMp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23121/Reviewer_PEMp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829439151, "cdate": 1761829439151, "tmdate": 1762942518330, "mdate": 1762942518330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel RL objective, ReMax, which aims to emergently drive exploration by maximizing the expected maximum return over $M$ \"retries,\" without explicit bonuses. To make this practical for on-policy RL, the authors derive a new, estimable policy gradient based on \"Expected Improvement\" (EI) that works with single-trajectory returns. They also generalize the discrete retry count $M$ to a continuous parameter $m > 0$ for fine-grained control. Their resulting algorithm, RePPO, is shown to outperform PPO on MinAtar and maintain higher policy entropy, demonstrating its exploration capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea of framing exploration as an emergent property of a retry-based reward objective is elegant and new.\n\n- The derivation of the \"Expected Improvement\" (EI) policy gradient (Proposition 2) is a solid and clever technical contribution that makes the ReMax objective practical for policy gradient methods.\n\n- Generalizing the discrete retry count $M$ to a continuous parameter $m$ is a useful feature, allowing for fine-grained control over the exploration-exploitation trade-off, as validated in the experiments.\n\n- The empirical results on MinAtar are convincing. RePPO not only outperforms PPO baselines in terms of score but also maintains significantly higher policy entropy without an explicit bonus, strongly supporting the paper's main claim."}, "weaknesses": {"value": "- The paper's motivation relies on (epistemic) uncertainty, but the final algorithm uses the \"implicit\" non-stationarity of a single Q-network as a proxy. The justification for why this is a sufficient or valid source of uncertainty is weak.\n\n- The method requires sorting Q-values, incurring an $O(K \\log K)$ computational cost. This limits the algorithm to discrete action spaces and prevents scaling to continuous control or domains with very large $K$.\n\n- Experiments are only on MinAtar. It is unclear if the performance gains and exploration properties will generalize to more complex environments like the full Atari suite or Procgen.\n\n- As a paper on exploration, it lacks experimental comparisons to other modern exploration methods (e.g., RND, count-based methods), making it hard to gauge its effectiveness relative to the SOTA."}, "questions": {"value": "- If the Q-function converges (becomes stationary), will ReMax stop exploring? This seems to contradict the motivation of exploring under epistemic uncertainty, which should persist in unseen states.\n\n- The ablation shows Q-replacement is critical. Is this technique necessary because the critic is undertrained? Could this issue be solved with more critic updates or a target network, rather than \"replacing\" the Q-value?\n\n- What is the probabilistic or physical interpretation of a non-integer retry parameter, such as $m=1.2$? Or is it best understood simply as a new hyperparameter to tune exploration intensity?\n\n- Could you please quantify the $O(K \\log K)$ overhead more clearly? How much of a bottleneck does this sorting step become as $K$ (the number of actions) grows large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iWkDQPZVvU", "forum": "msX2KKICRr", "replyto": "msX2KKICRr", "signatures": ["ICLR.cc/2026/Conference/Submission23121/Reviewer_XGDA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23121/Reviewer_XGDA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903655489, "cdate": 1761903655489, "tmdate": 1762942518036, "mdate": 1762942518036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main idea of this paper is to propose an exploration strategy based on the idea to \"bias\" the classical RL objective by taking the best outcome over M trials (the ReMax objective).\nThe article discusses this approach in both a multi-armed bandit framework and RL framework.\nIn the RL framework, an algorithm RePPO is proposed: a combination of ReMax with PPO.\nThe paper proposes both several theoretical results regarding the elaboration of some closed-forms (proofs are given in appendix), as well as some empirical evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Exploration is an ever-lasting question in RL and bandits. The main advantage of the approach explored in this paper is its simplicity in terms of additional parameters: it only requires an integer M.\nEven if I did not have the time to perform an in depth reading of the paper, I enjoyed going through it. The presentation is somehow original, with some didactical elements such as questions encouraging the reader to think proactively while reading."}, "weaknesses": {"value": "Some notations are considered as implicitly defined, such as $\\Delta^{K-1}$. What are precisely per-action values in a bandit problem, expected values or random variables associated with arm pulling ?"}, "questions": {"value": "I may clearly have missed some information, but my main question would be related with the possibility that optimizing policies according to ReMax may induce some sort of bias ?\nThe article speaks much more about exploration than exploitation, whereas these two notion are usually intertwined. Is it because, in practice, the authors did not observed any over-exploitation behaviors (because of too-low values for M) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hOpIR9RLYw", "forum": "msX2KKICRr", "replyto": "msX2KKICRr", "signatures": ["ICLR.cc/2026/Conference/Submission23121/Reviewer_ERuW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23121/Reviewer_ERuW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929087612, "cdate": 1761929087612, "tmdate": 1762942517654, "mdate": 1762942517654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}