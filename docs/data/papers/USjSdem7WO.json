{"id": "USjSdem7WO", "number": 24818, "cdate": 1758360648504, "mdate": 1763153028760, "content": {"title": "Just a Simple Transformation is Enough for Data Protection in Split Learning", "abstract": "Split Learning (SL) aims to enable collaborative training of deep learning models while maintaining privacy protection. However, the SL procedure still has components that are vulnerable to attacks by malicious parties. In our work, we consider feature reconstruction attacks --- a common risk targeting input data compromise. We theoretically claim that feature reconstruction attacks cannot succeed without knowledge of the prior distribution on data. Consequently, we demonstrate that even simple model architecture transformations can significantly impact the protection of input data during SL. Confirming these findings with experimental results, we show that MLP-based models are resistant to state-of-the-art feature reconstruction attacks.", "tldr": "", "keywords": ["Privacy", "Split Learning", "Feature Reconstruction attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6303b6199c065612139eccba36edd1c93e9cf75c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes the differences in data reconstruction attacks between CNN and MLP models in split learning systems and concludes that MLP models are more resistant to such attacks than CNN models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The study topic is good, but I can no longer identify any notable strengths."}, "weaknesses": {"value": "1.The paper confuses the two scenarios, VFL and Two-Party SL. In previous works [1,2], VFL typically adopts a two-party setting where the client holds part of the model (the bottom model), and the server also holds partial data and a portion of the bottom model. The intermediate representations from both sides are aggregated and passed through the server’s top model. In contrast, Two-Party SL [3,4,5,6,7] is fundamentally different: the server only holds the latter part of the model and has no access to private data—it merely receives intermediate representations and returns the corresponding gradients to the client. The scenario studied in this paper is SL, not VFL, as clearly shown in Section 3 (Setup).\n\n2.The paper does not evaluate state-of-the-art attacks. It only considers UnSplit [3] and FSHA [4], which are outdated methods developed before 2022. More recent and more powerful attacks, such as PCAT [5], FORA [6], and SDAR [7], are not included in the evaluation. These newer attacks have demonstrated superior performance compared to UnSplit and FSHA and may not experience the same poor results on MLP models. Moreover, as shown in [8], attackers can also achieve effective data reconstruction using a ViT architecture.\n\n3.The remarks in Section 3 lack proper justification. In Remarks 1 and 3, the paper directly claims that the attacker “cannot reconstruct the initial data X” but this conclusion is neither theoretically proven nor experimentally verified. Moreover, the attacker is unaffected by the client’s transformations on the model or input—as long as the activations ( H ) remain the same, nothing changes from the attacker’s perspective. Additionally, many attack methods do not rely on matching data distributions; they can still perform effectively even when using auxiliary data with different distributions (as shown in FSHA [4], FORA [6], and SDAR [7]). \n\n4.FID is already a widely used metric in model inversion attacks. While the paper claims that FID is superior to MSE, MSE is not specifically designed for image-level evaluation. What about other commonly used metrics such as SSIM, PSNR, or LPIPS? In Table 1, the MSE of the MLP-based model is better than that of the CNN on F-MNIST, while the FID of the MLP-based model is better than CNN on CIFAR-10. These results are inconsistent with the paper’s conclusions.\n\n5.The cut layer is set too shallow in the experiments, with only layer 1 evaluated, as UnSplit performs poorly on deeper layers. To convincingly demonstrate that attacks on MLP models are less effective than on CNNs, a range of cut layers should be tested for a comprehensive comparison. Additionally, to support the claim that MLP models are harder to attack than CNNs, experiments should include diverse MLP architectures, and the client could also add several MLP layers to existing CNN models for evaluation.\n\n6.Questions arise regarding the results. In Figure 5, the attack results on the MLP-based model do not match the original images; for example, an original “0” or shoe is reconstructed as “1” or a T-shirt. If the attacker were less effective, the output should appear blurry rather than completely mismatched. Similarly, the results in Figure 7 suggest inappropriate hyperparameter settings rather than a fundamental weakness of the attack—the hyperparameters may need adjustment for different models, as the attack does not appear fully optimized.\n\n7.Minor Issues: Many references lack conference or journal names and are therefore incomplete. The paper does not disclose significant LLM usage, as required by the author guidelines.\n\n[1]Fu, Chong, et al. \"Label inference attacks against vertical federated learning.\" 31st USENIX security symposium (USENIX Security 22). 2022.\n\n[2]Yao, Duanyi, et al. \"URVFL: Undetectable Data Reconstruction Attack on Vertical Federated Learning.\". NDSS 2025.\n\n[3]Erdoğan, Ege, Alptekin Küpçü, and A. Ercüment Çiçek. \"Unsplit: Data-oblivious model inversion, model stealing, and label inference attacks against split learning.\" Proceedings of the 21st Workshop on Privacy in the Electronic Society. 2022.\n\n[4]Pasquini, Dario, Giuseppe Ateniese, and Massimo Bernaschi. \"Unleashing the tiger: Inference attacks on split learning.\" Proceedings of the 2021 ACM SIGSAC conference on computer and communications security. 2021.\n\n[5]Gao, Xinben, and Lan Zhang. \"{PCAT}: Functionality and data stealing from split learning by {Pseudo-Client} attack.\" 32nd USENIX Security Symposium (USENIX Security 23). 2023.\n\n[6]Xu, Xiaoyang, et al. \"A stealthy wrongdoer: Feature-oriented reconstruction attack against split learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024.\n\n[7]Zhu, Xiaochen, et al. \"Passive inference attacks on split learning via adversarial regularization.\" NDSS 2025.\n\n[8]Wang, Lixu, et al. \"Split adaptation for pre-trained vision transformers.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "Please see the above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "02j2h5v0Wy", "forum": "USjSdem7WO", "replyto": "USjSdem7WO", "signatures": ["ICLR.cc/2026/Conference/Submission24818/Reviewer_ic93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24818/Reviewer_ic93"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760869208337, "cdate": 1760869208337, "tmdate": 1762943207465, "mdate": 1762943207465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the Reviewers for time, and their feedback. I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "cCRtKk5KFu", "forum": "USjSdem7WO", "replyto": "USjSdem7WO", "signatures": ["ICLR.cc/2026/Conference/Submission24818/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24818/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763153027802, "cdate": 1763153027802, "tmdate": 1763153027802, "mdate": 1763153027802, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper found that in split learning, the MLP-mixer is enough to defend against inversion attacks in split learning frameworks as more than 1 layer is placed on the client side."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper empirically shows that for visual tasks such as image classification, leveraging MLP and transformers are sufficient to defend against inversion attacks in split learning"}, "weaknesses": {"value": "1. I think authors have a misunderstanding of vertical federated learning and split learning. But it is fine to focus on split learning.\n2. I am confused about the motivation. For split learning, a portion of the application is image classification and it is common to use CNNs. It is also verified that a vision transformer will leak privacy [1]. Why do authors focus on MLP? What is the motivation for using MLP for visual tasks? If the paper is going to discuss MLP, related work on transformers should be discussed and compared [2], [4].\n3. The three main contribution. 1: Without prior data knowledge, split learning is privacy-preserving enough. 2: MLP based model are sufficient to defense against inversion attacks. 3: using human-feeling central metrics, are already discussed previously in the NLP fields [2]. The difference is that this paper focuses on vision. But again, what is the motivation for using MLP in vision? The reason why contribution 1 stands out in NLP is because it is basically impossible to find similar prior knowledge of data distribution in the real world. But for image, it is common. There is a lot of medical image application and human face identification systems in real-world using split learning. You can easily get public datasets of medical images and human faces. The assumption that attackers have no chance to know prior data distribution does not hold. See examples in [1] and [3].\n4. The paper's main contribution: Linear layer and random dropout can preserve privacy is already well-discussed in previous literature [1], [2],\n5. Experiments are weak. More practical datasets such as CelebA, Coco should be evaluated.\n\n[1] Xu, Hengyuan, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li. \"Permutation equivariance of transformers and its applications.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5987-5996. 2024.\n[2] Yao, Dixi, and Baochun Li. \"Is Split Learning Privacy-Preserving for Fine-Tuning Large Language Models?.\" IEEE Transactions on Big Data (2024).\n[3] Ghosh, Bishwamittra, Yuan Wang, Huazhu Fu, Qingsong Wei, Yong Liu, and Rick Siow Mong Goh. \"Split learning of multi-modal medical image classification.\" In 2024 IEEE Conference on Artificial Intelligence (CAI), pp. 1326-1331. IEEE, 2024.\n[4] Oh, Seungeun, Sihun Baek, Jihong Park, Hyelin Nam, Praneeth Vepakomma, Ramesh Raskar, Mehdi Bennis, and Seong-Lyun Kim. \"Privacy-preserving split learning with vision transformers using patch-wise random and noisy cutmix.\" arXiv preprint arXiv:2408.01040 (2024)."}, "questions": {"value": "1. Why is the accuracy for MNIST and CIFAR10 so low? ViT should be able to reach 94% on CIFAR10 and 99% on MNIST without any pre-training.\n2. Why is FID for CNN higher than that for MLP?\n3. What models do you use and at which layers do you cut the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no ethics issues regarding privacy and other related problems."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KKxLGywKTz", "forum": "USjSdem7WO", "replyto": "USjSdem7WO", "signatures": ["ICLR.cc/2026/Conference/Submission24818/Reviewer_L3NX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24818/Reviewer_L3NX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705735968, "cdate": 1761705735968, "tmdate": 1762943207240, "mdate": 1762943207240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies privacy protection in Split Learning (SL), focusing on feature reconstruction attacks such as Model Inversion (MI) and Feature-space Hijacking (FSHA). The authors propose a simple yet powerful idea: Merely changing the client-side model architecture (replacing CNNs with MLP-based models) can inherently improve privacy protection — even without any additional defense mechanisms.\nExtensive experiments demonstrate that MLP-based models can effectively defend against state-of-the-art feature reconstruction attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written, logically organized, and supported by carefully reasoned formal proofs that effectively illustrate both the motivation and the method.\nThe paper introduces an elegant idea — structural transformation itself can serve as privacy protection and shows that Hijacking and Model Inversion attacks fail on MLP-based models \nwithout any additional changes.\nThe authors point out the limitations of MSE for assessing image privacy and introduce FID as a more perceptually faithful measure."}, "weaknesses": {"value": "The claim of “the findings can be combined with any of the existing defense frameworks” is appealing but lacks experiments comparing with known defenses.\nThe experiment only focuses on image datasets and lacks text datasets."}, "questions": {"value": "Could you please provide experimental results comparing with or combining with the existing defense frameworks?\nPlease provide small-scale experiments with text or tabular data to strengthen the generalizability of your argument."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u98GzOUJ5g", "forum": "USjSdem7WO", "replyto": "USjSdem7WO", "signatures": ["ICLR.cc/2026/Conference/Submission24818/Reviewer_4ZxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24818/Reviewer_4ZxJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715880775, "cdate": 1761715880775, "tmdate": 1762943206674, "mdate": 1762943206674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is set in the context of malicious attacks in a Split Learning setup.\nIn particular, the authors discuss the feature reconstruction attacks like Model\nInversion (MI) and Feature Space Hijacking Attack (FSHA). The authors provided\na detailed background study and claimed that these attacks cannot be\nsuccessful without prior knowledge of the distribution of the data. They also\nshow that theoretically, for a single dense layer at the client side, it is possible\nto prevent such attacks using a simple translation using an orthonormal matrix.\nThe authors also argue that models depending on dense layers prevent\nreconstruction to a much greater extent compared to convolutional networks.\nOverall, the paper is well-written and the arguments are clear and precise. The\nauthors provided some experiments showcasing the same attacks on different\nmodels to establish their claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper establishes that an orthogonal transformation to the inputs and\nweights of the last layer makes it impossible to reconstruct features from\nthe final activation at the cut layer.\n(2) The paper provides a detailed background study and establishes the claims\nand arguments precisely. The paper is well-written.\n(3) The paper argues and introduces FID as a better metric for the success\nof reconstruction attacks.\n(4) The paper also establishes that it is impossible for the attacker to reconstruct\nthe inputs without the knowledge of prior data distribution."}, "weaknesses": {"value": "(1) The paper makes a compelling case for the simple transformation using the\northogonal matrix. However, it is unclear how the transformation affects\nthe underlying distribution of the activations and weights. If the distributions\ndo not change with the transformations, it may still be possible to\nrecover some information at the attacker’s end.\n(2) The authors theoretically establish the efficacy of the transformations for\na single layer. They also argue that in the case of an MLP, the layers\nbefore the last layer would change the distribution of data, making the\nfeature reconstruction impossible. It would be nice to see this happening\nempirically.\n(3) The authors claim that the FID is a better metric than MSE for reconstruction.\nHowever, it is not clear why. While FID is often used to measure\nperceptual quality and fidelity in generative models, it does not guarantee\nany measurement of the information content. And for the task at hand,\ninformation content seems to be more important than fidelity or perceptual\nquality. To elaborate, FID may measure pixel-level distribution of\noriginal and reconstructed images. However, it is not guaranteed that\nthe poorly constructed dissimilar images will pass enough information to\nunderstand the latent distribution.\n(4) The authors showed empirical results on CNNs and MLPs using MSE\nand FID, and claimed that MLPs are better in the defense of feature\nreconstruction attacks. However, from the results in Table 1, it is very\nunclear for two of the three datasets authors have showcased. For FMNIST\nand CIFAR-10, the MSE and FID metrics, respectively, show\nthat CNN has a better defence than MLP.\n(5) Although the authors claim that the MLPs are better than CNNs in such\ndefence, there is no discussion of why CNNs fail.\n2"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xrwL72QFXo", "forum": "USjSdem7WO", "replyto": "USjSdem7WO", "signatures": ["ICLR.cc/2026/Conference/Submission24818/Reviewer_Cxar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24818/Reviewer_Cxar"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24818/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762961977358, "cdate": 1762961977358, "tmdate": 1762961977358, "mdate": 1762961977358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}