{"id": "GsLvRPAfoU", "number": 2093, "cdate": 1756987076680, "mdate": 1763087607027, "content": {"title": "Improving Fine-Grained Control via Aggregation of Multiple Diffusion Models", "abstract": "While many diffusion models perform well when controlling particular aspects such as style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper introduces a novel training-free algorithm for fine-grained generation, called Aggregation of Multiple Diffusion Models (AMDM). The algorithm integrates features in the latent data space from multiple diffusion models within the same ecosystem into a specified model, thereby activating particular features and enabling fine-grained control. Experimental results demonstrate that AMDM significantly improves fine-grained control without training, validating its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional generation in diffusion models. Specifically, it allows us to fully utilize existing or develop new conditional diffusion models that control specific aspects, and then aggregate them using the AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs.", "tldr": "", "keywords": ["diffusion models", "generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c8daa0cc8a7440dc6e9d590b809d96b5ea36bde.pdf", "supplementary_material": "/attachment/6fc72af89acccdcc67c0e9ea9e22507f6399f481.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a two-step process for combining multiple diffusion models. In the first step, the authors use Spherical Linear Interpolation (SLERP) to combine the models, and in the second step, they perform a descent procedure to move the combined output closer to the mean ( increasing probability of the sample )."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Propose novel method of Spherical Linear Interpolation for combining multiple diffusion models."}, "weaknesses": {"value": "### Need Proofs for Claims / Additional Assumptions\n\n**Lines 130–146:**  \nThe assumption \\(p_{\\theta_1}, p_{\\theta_2} \\subset \\mathcal{M}_0 \\) implies that \\( p_{\\theta_1} \\) and \\( p_{\\theta_2} \\) are trained on the same data, or equivalently, that they are generative models of the same underlying data distribution.  \nHowever, this assumption holds **only if** both models are trained on data drawn from the same distribution. For example, if model A is trained on real images while model B is trained on paintings, the underlying data distributions differ, and hence the assumption \\( p_{\\theta_1}, p_{\\theta_2} \\subset \\mathcal{M}_0 \\) becomes invalid.\n\n---\n\nCan you provide a proof supporting the claim that additive or modified architectures only enhance control features?  \nThis assertion appears **baseless** and **lacks supporting theoretical or empirical justification**. Consequently, the argument for a “same diffusion-model ecosystem” becomes **ill-defined and conceptually inconsistent**.\n\n---\n\n**Line 164:**  \nWhat do you mean by *“same task”*?\n\n---\n\n### Clarifications Needed\n\nThe motivation behind using *spherical interpolation* and *deviation optimization* is unclear.  \nIt is also not evident how the parameters \\( q_1, w_2, n_\\theta \\) are chosen.\n\nThere exist several approaches to aggregate diffusion models, many of which are derived from a probabilistic perspective under certain assumptions of independence (broadly referred to as *compositionality*).\n\n### Comparison to Relevant Works\n\n1. **Reduce, Reuse, Recycle:** [arXiv:2302.11552](https://arxiv.org/pdf/2302.11552)  \n2. **Compositionally:** [arXiv:2206.01714](https://arxiv.org/pdf/2206.01714)  \n3. **Conditional Independence Assumed:** [arXiv:2503.01145](https://arxiv.org/abs/2503.01145) — enforcing conditional independence  \n4. **Without Conditional Independence (Controllability):** [arXiv:2302.14368](https://arxiv.org/pdf/2302.14368)  \n5. [arXiv:2505.13213](https://arxiv.org/pdf/2505.13213)  \n6. **Algebraic Perspective:** [arXiv:2502.04549](https://arxiv.org/abs/2502.04549)\n\nAll these works rely on certain **underlying assumptions**, particularly regarding independence.  \nWhen such independence does not hold, an additional **weighting or hyperparameter term** is typically introduced to account for the violation.\n\n---\n\n### Example: Independence vs. Dependence\n\nIf conditional independence is assumed:\n\n\\[\np_{\\theta}(x \\mid y_1, y_2)\n  = p_{\\theta}(x \\mid y_1)\n  + p_{\\theta}(x \\mid y_2)\n  - p_{\\theta}(x \\mid \\varnothing)\n\\]\n\nIf independence **is not** assumed:\n\n\\[\np_{\\theta}(x \\mid y_1, y_2)\n  = p_{\\theta}(x \\mid y_1)\n  + w_1\\, p_{\\theta}(x \\mid y_2)\n  - w_1\\, p_{\\theta}(x \\mid \\varnothing),\n\\]\n\nwhere \\( w_1 \\) controls the degree of independence between \\( y_1 \\) and \\( y_2 \\).\n\n---\n\n### Connecting to the Current Work\n\nIn line with the assumptions of the current work, suppose \\( z_t \\) is drawn from the distribution \\( p(z_t) = p(z_t \\mid \\varnothing) \\), and assume that \\( z_t \\subset \\mathcal{M}_t \\) for all \\( t \\in [0, T] \\).  \nThe objective can then be viewed as **linearly combining distributions** to maximize the probability of lying on the data manifold:\n\n\\[\n\\max_{a,b,c}\\; p(a z^{(1)}_{t-1} + b z^{(2)}_{t-1} + c \\mid \\varnothing, z_t),\n\\]\n\nsuch that the combination lies on the manifold of \\( p(z_t) \\).\n\nSince the distribution is Gaussian, the maximum corresponds to its **mean**, reducing the sampling process to:\n\n\\[\na\\, p_{\\theta}(x \\mid y_1, y_2)\n  = p_{\\theta}(x \\mid y_1)\n  + b\\, p_{\\theta}(x \\mid y_2)\n  + (1 - a - b)\\, p_{\\theta}(x \\mid \\varnothing).\n\\]\n\n---\n\n### Interpretation\n\nMethods such as **ADAM** or **spherical interpolation** represent alternative interpolation strategies—implicitly introducing weighting factors rather than explicitly modeling independence.  \nIn the current work, this combination is obtained via **spherical interpolation**, without direct access to \\( p(z_t \\mid \\varnothing) \\) (as in the classifier-free guidance formulation).  \nHowever, it remains unclear **why any \\( s < T \\)** should necessarily lie on a sphere.\n\n---\n\n### Broader Concern\n\nFor methods that do not assume any structural property of the data distribution, the introduction of a **hyperparameter** becomes unavoidable.  \nIt is also unclear **how these hyperparameters should be selected**.\n\n---\n\n### Results and Evaluation\n\nThe reported results appear potentially misleading.  \nAs the authors themselves claim, combining any two methods tends to improve performance over both individual methods.  \nSince the hyperparameters are tuned by the authors, the worst possible case corresponds to \\( w_2 = 0 \\).  \nTherefore, unless a **principled approach** for hyperparameter selection or a **comparison with existing aggregation methods** is provided, the results section offers **limited insight**."}, "questions": {"value": "Addressing all the weaknesses will answer all my questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "s"}}, "id": "VXAXRVTYnW", "forum": "GsLvRPAfoU", "replyto": "GsLvRPAfoU", "signatures": ["ICLR.cc/2026/Conference/Submission2093/Reviewer_qUaq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2093/Reviewer_qUaq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528959018, "cdate": 1761528959018, "tmdate": 1762936833040, "mdate": 1762936833040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of fine-grained conditional control in diffusion models. The proposed method, AMDM, aggregates multiple diffusion model scores via spherical interpolation, and employs a deviation optimization step to stabilize the aggregated score function. The framework enables users to combine the strengths of multiple fine-tuned diffusion models without requiring complex multi-capability datasets or multi-stage finetuning. Experiments on several conditional generation tasks demonstrate that AMDM can effectively integrate diverse capabilities from different models at inference time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is grounded in solid theoretical analysis, particularly regarding the confidence and reliability of aggregated diffusion scores.\n- The empirical evaluation is thorough, and the comparisons clearly demonstrate how AMDM improves over baselines in multiple tasks.\n- The motivation is practical and relevant: enabling reuse and integration of existing specialized diffusion models without retraining."}, "weaknesses": {"value": "- Although the theoretical analysis is detailed, the paper may benefit from a simple controlled or toy example to help intuitively illustrate the effect of score aggregation and deviation optimization.\n- The evaluation primarily focuses on three types of models (MIGC, InteractDiffusion, and IP-Adapter). While the results are encouraging, a broader range of conditional diffusion methods or application settings would better support the generality of the method.\n- One of the key claims — that diffusion models initially prioritize feature generation before later refining image quality and consistency — is mainly supported by a single ablation (Table 3). Additional analysis or diagnostic visualization would help strengthen this conclusion."}, "questions": {"value": "1. Regarding spherical interpolation and Equation (6):\n\n    Could the authors clarify the geometric intuition or assumption behind applying spherical aggregation to score vectors? Is the assumption that these score fields locally reside on a shared spherical manifold, or is the spherical constraint introduced primarily for normalization and stability?\n    \n2. Computation overhead:\n\n    What is the computational cost of AMDM compared to running a single diffusion model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZB1lG1xJwp", "forum": "GsLvRPAfoU", "replyto": "GsLvRPAfoU", "signatures": ["ICLR.cc/2026/Conference/Submission2093/Reviewer_WAAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2093/Reviewer_WAAK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680537406, "cdate": 1761680537406, "tmdate": 1762916018169, "mdate": 1762916018169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Aggregation of Multiple Diffusion Models (AMDM) algorithm, a training-free method for improving fine-grained conditional control in image generation. AMDM works by aggregating features from multiple diffusion models within the same ecosystem to integrate their respective strengths. The key components are 1) Spherical aggregation 2)Deviation optimization. The authors demonstrate that AMDM significantly improves fine-grained control capabilities by integrating different models' strengths."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors identify a genuine limitation in current models that they excel in specific aspects but struggle with others, and provide a solution without requiring retraining.\n- The approach is backed by mathematical analysis of why aggregation works for models in the same diffusion ecosystem.\n- Extensive experiments demonstrate clear improvements in both qualitative results and quantitative metrics.\n- Unlike many compositional methods that introduce significant computational overhead, AMDM has minimal additional cost."}, "weaknesses": {"value": "- While the authors provide theoretical justification, the assumptions (functional proximity, conditional proximity) are somewhat heuristic and don't offer global guarantees.\n- AMDM only works for models within the same diffusion ecosystem, limiting its general applicability.\n- While some comparisons with compositional methods are provided, a more comprehensive comparison with other training-free approaches would strengthen the paper.\n- When aggregating models, there might be unintended interactions between different aspects that aren't fully explored.\n- The optimization step size is selected empirically, and the authors acknowledge this as a limitation requiring future work."}, "questions": {"value": "- How sensitive is AMDM to the choice of models within an ecosystem? Are there certain model combinations that work particularly well or poorly?\n- (minor) What happens to the result if the positional information is not given? Would it still struggle to generate as specified from the text prompt or could this fine-grained control be coming from additional information, poisition.\n- (minor) This is more like a question and just curiosity on my side. Could this fine-grained problem only exist within the open-source models? In other words, would the problem still exist in the models such as Sora?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ox6ANmnlD7", "forum": "GsLvRPAfoU", "replyto": "GsLvRPAfoU", "signatures": ["ICLR.cc/2026/Conference/Submission2093/Reviewer_E8Pz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2093/Reviewer_E8Pz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967754594, "cdate": 1761967754594, "tmdate": 1762916017977, "mdate": 1762916017977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of fine-grained control in diffusion-based generative models that fail at nuanced, multi-conditional control ,e.g., spatial arrangement and style preservation. The authors propose a training-free algorithm called AMDM that aggregates latent representations from multiple diffusion models.  During inference, AMDM merge latent variables geometrically using spherical aggregation and a deviation optimization step. The authors conducted several empirical experiments to show improvements in attribute, style, and interaction controllability with AMDM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The perspective of leveraging existing diffusion models to address fine-grained, controllable generation in a training-free manner is an interesting direction.\n- The paper is well-written and easy to follow; empirical performance and results demonstrate the promise of the proposed method."}, "weaknesses": {"value": "- \"Diffusion ecosystem\" is mentioned across the paper as a prerequisite of AMDM but somewhat loosely defined, for example, how to quantitatively verify whether any two models belong to the same ecosystem is unclear. \n- Evaluation scope is limited to certain derivatives of Stable Diffusion models. Throughout the experiments, the authors only picked a few classic SD1.4/1.5 and SDXL models, but did not examine whether the findings generalize to more recent model architectures based on DiT instead of U-Net, such as as SD3 or Flux. \n- Comparing to other model composition methods in the literature, the proposed method only applies to high-dimensional Gaussian and fails to general distributions (as shown in Table 7)"}, "questions": {"value": "- Besides the comparisions between SD1.5 and SDXL, how sensitive is AMDM to broader model heterogeneity (schedulers or attention designs)? For example, you may conduct some experiments on models based on DiT to show the generality of your method?\n- The weighting factor $w$ in Slerp is treated as hyperparameter, but lacks a principled selection method. Empirical experiments are conducted based on two/three model aggregation where $w$ is determined with ablations. Can you discuss more on how to systematically select these parameters, and especially with a set of models (beyond three typical models you selected), how your propose method work empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1pcJhOLeyP", "forum": "GsLvRPAfoU", "replyto": "GsLvRPAfoU", "signatures": ["ICLR.cc/2026/Conference/Submission2093/Reviewer_MHdz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2093/Reviewer_MHdz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979246852, "cdate": 1761979246852, "tmdate": 1762916017762, "mdate": 1762916017762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}