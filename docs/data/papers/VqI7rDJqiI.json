{"id": "VqI7rDJqiI", "number": 22519, "cdate": 1758332222542, "mdate": 1759896861813, "content": {"title": "C2Rust-Bench: A Minimized, Representative Benchmark for C-to-Rust Transpilation", "abstract": "Despite significant effort in vulnerability detection over the last two decades, memory safety vulnerabilities continue to be a systemic problem that affects most mainstream software. Recent reports have concluded that the key to solving this issue once and for all is to migrate legacy C code to memory-safe languages. To this end, C-to-Rust \"transpilation\" has become a popular research topic. Recent work has proposed various approaches; however, what the community lacks is a comprehensive evaluation dataset. Currently, researchers rely on completeness through sheer sample volume, but this bloats the time required to run experiments and makes verification, which is currently done manually, laborious. In this work, we propose a method for selecting functions from a large set to construct a minimized yet representative dataset to evaluate C-to-Rust transpilation systems. We propose C2Rust-Bench, a dataset of only 2,905 functions that are nevertheless an objectively representative benchmark for C-to-Rust transpilation. This dataset was distilled from 15,503 real-world functions encompassing previous work.", "tldr": "C-to-Rust transpilation is key to addressing memory safety issues in C code, but current evaluation relies on large, unwieldy datasets. We propose C2Rust-Bench, a dataset of only 2,905 functions that offers an objectively representative benchmark.", "keywords": ["LLM Transpiler", "Transpilation Evaluation", "C", "Rust", "Memory Safety", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4db9ed7c2a7940148d8e0a881aa8944f8491346c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces C2Rust-Bench, a dataset aimed at providing a minimized and representative benchmark for evaluating C-to-Rust transpilation tools. The authors propose a method to select 2,905 functions from 15,503 C functions from existing literature. The method is based on four code complexity metrics, partitioning the metric space, and selecting functions via systematic sampling guided by Principal Component Analysis. The authors claim that this benchmark retains the essential characteristics of the original set while drastically reducing evaluation time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Clear Problem Identification: The paper correctly identifies a significant pain point in the emerging field of C-to-Rust transpilation: the lack of a standard, manageable benchmark. The motivation, supported by recent reports, is strong and relevant to the community.\n2.Practical Utility: The achieved reduction in dataset size (81.3%) and the corresponding decrease in evaluation time (≈80%) are substantial and directly address the problem of lengthy experiments. Releasing the dataset and code is a positive practice that facilitates adoption and reproducibility.\n3.Cross-LLM Validation: The experiment in Appendix B.2, which tests the representativeness of the selected set across different LLMs, is a thoughtful addition. It helps mitigate the concern that the benchmark is overly tailored to the specific LLM (qwen2.5-coder:32b) used in its creation."}, "weaknesses": {"value": "1.Fundamental Circularity in Methodology: A core weakness lies in the methodology's reliance on a specific LLM's transpilation output. The Rust-side metrics (MI-Rust, Unsafe Complexity, Data Type Complexity) are derived from code generated by qwen2.5-coder:32b. If this LLM has systematic biases or errors in how it handles certain C constructs (e.g., pointers, complex types), these errors are baked into the complexity metrics used for selection. This creates a potential feedback loop where the benchmark is \"representative\" of what a specific LLM finds challenging, not necessarily of the intrinsic challenges of C-to-Rust transpilation. While the cross-LLM experiment provides some reassurance, it does not fully resolve this conceptual issue.\n2.Validation via Proxy is Incomplete: The primary validation uses the distribution of compilation-error fixing attempts as a proxy for representativeness. While a useful signal, this is a narrow measure of transpilation quality. It does not account for:\n*Semantic Equivalence: A compilable Rust function can be semantically incorrect.\n*Idiomaticity: The benchmark does not measure how \"Rust-like\" the output is, a key concern in prior work.\n*Runtime Behavior: Correct compilation does not guarantee correct execution. A more robust validation would involve a sample-based check for semantic correctness, perhaps using existing test suites from the original C programs.\n3.Lack of a Strong Baseline for Comparison: The paper does not compare its selection method against a strong baseline, such as random sampling with the same reduction ratio. It is plausible that a carefully stratified random sample could achieve a similar level of \"representativeness\" in terms of code construct coverage and compilation attempt distribution, with a much simpler methodology. The added value of the complex PCA-and-binning approach over simpler methods is not conclusively demonstrated.\n4.Limited Scope of \"Representativeness\": The benchmark is built at the function level. While this is justified by LLM context limits, it ignores critical challenges in transpiling programs, such as inter-procedural analysis, global state management, and module/system architecture translation. The benchmark is thus primarily useful for evaluating function-level translation engines (like LLMs) and may be less applicable for evaluating tools that operate on a whole-program level."}, "questions": {"value": "1.Given the circularity concern, did the authors consider using the output of a rule-based transpiler (like c2rust) to calculate the Rust-side metrics, as it would provide a more deterministic and predictable baseline?\n2.How does the performance of a simple random sampling baseline compare to the proposed method on the same representativeness metric?\n3.The hyperparameter tuning uses the same proxy metric (compilation attempts) for optimization. Could this lead to overfitting to this specific, imperfect measure of quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z8VlWSEgNb", "forum": "VqI7rDJqiI", "replyto": "VqI7rDJqiI", "signatures": ["ICLR.cc/2026/Conference/Submission22519/Reviewer_xyiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22519/Reviewer_xyiR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451228979, "cdate": 1761451228979, "tmdate": 1762942255817, "mdate": 1762942255817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "C2RUST-BENCH is a compact, function-level benchmark designed to evaluate C2Rust transpilation with high coverage and low redundancy. Starting from a large corpus of real-world C code, the authors preprocess and split files into individual functions, then select 2.9k representative items from ~15k candidates. Selection is driven by multiple complexity signals: Maintainability Index in C and in transpiled Rust, the presence and extent of unsafe features, and data-type/operation complexity (e.g., pointers, casts, arrays/structs, control flow). They stratify these metrics into bins, compute an aggregate complexity score (via PCA), and systematically sample across easy-to-hard regions to avoid a mid-difficulty bias. Difficulty is further grounded by empirical transpilation feedback: functions are passed through an LLM-based pipeline to observe compile success and the number of fix attempts, and both successes and failures remain in the set. The resulting benchmark stresses known pain points—pointer arithmetic, type casts, function pointers, goto/switch—while remaining small enough for rapid iteration, ablation studies, and leaderboard-style comparisons. Positioned as complementary to repository-scale suites (e.g., CRUST-Bench), C2RUST-BENCH prioritizes controlled, repeatable micro-evaluations over end-to-end realism. It’s most suitable for measuring incremental improvements in transpiler reliability, error modes, and code-quality trade-offs, and for probing how specific C constructs affect Rust compilation and safety outcomes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Principled, balanced curation: Uses multi-metric stratified sampling (complexity, unsafe usage, types) to cover a broad easy to hard spectrum rather than skewing to median cases.\n\n* Empirical difficulty grounding: Calibrates “hardness” with real transpilation/compile outcomes (incl. number of fixes), not just static code metrics—improves relevance for evaluation."}, "weaknesses": {"value": "1. Limited ecosystem realism: Function-level snippets miss repo-level issues (build systems, headers/linking, cross-file types, macros), so results may not transfer to end-to-end C to Rust.\n\n2, Potential measurement bias: Difficulty signals depend on one/few transpilers and chosen metrics/PCA; different tools or metrics could shift what’s labeled “hard,” affecting generality."}, "questions": {"value": "What is the fundamental challenge to translate C to Rust? How your approach is designed towards this challenge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "On3fKeUC2w", "forum": "VqI7rDJqiI", "replyto": "VqI7rDJqiI", "signatures": ["ICLR.cc/2026/Conference/Submission22519/Reviewer_n7AF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22519/Reviewer_n7AF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758492513, "cdate": 1761758492513, "tmdate": 1762942255149, "mdate": 1762942255149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces C2Rust-Bench, a minimized but representative benchmark dataset for evaluating C-to-Rust transpilation systems. The authors first argue that existing evaluations rely on large or manually curated datasets that hinder reproducibility and efficiency. Then they observe that there is an upper bound beyond which increasing program size no longer contributes to the complexity or representativeness of transpilation. To address these issues, the authors propose a principled reduction method that extracts C functions from prior datasets using quantitative complexity metrics, including the Maintainability Index (for both C and Rust), unsafe operation density, and data type diversity. A PCA-based scoring and sampling algorithm is used to preserve representativeness and diversity. The benchmark is validated through cross-LLM transpilation experiments on nine different models. Both the dataset and source code are publicly released."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Standardizing evaluation in the growing field of AI-assisted code transpilation from C to Rust is timely and well-motivated.\n2. The dataset construction uses several types of complexity metrics with PCA-based sampling, which is technically sound and interpretable.\n3. The empirical validation across nine LLMs demonstrates generality and robustness.\n4. Public release of both the dataset and code significantly enhances reproducibility and potential community impact."}, "weaknesses": {"value": "1. Several metrics (e.g., Maintainability Index and unsafe complexity) are computed on transpiled Rust code, not on ground-truth Rust implementations, which may introduce model-dependent bias.\n2. The correctness validation focuses only on compilation success, which does not ensure semantic equivalence between the C and Rust code. More rigorous behavioral or semantic checks would strengthen the claims.\n3. The representativeness is guaranteed by minimizing the relative difference score, which is somewhat heuristic. It does not directly show that the selected subset of functions preserves semantic or structural diversity."}, "questions": {"value": "1. Have you verified whether the selected functions in C2RUST-BENCH cover similar functional semantics or API categories as those in the full dataset?\n2. Since both MI and unsafe complexity depend on the LLM-generated Rust code, how do you ensure these metrics reflect the intrinsic complexity of the original C functions rather than translation artifacts introduced by the LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WfC10XfPbi", "forum": "VqI7rDJqiI", "replyto": "VqI7rDJqiI", "signatures": ["ICLR.cc/2026/Conference/Submission22519/Reviewer_LffA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22519/Reviewer_LffA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830672242, "cdate": 1761830672242, "tmdate": 1762942254665, "mdate": 1762942254665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a funtion level benchmark towards measuring C to Rust transpilation.  The paper proposes curation of a minimalized set of tasks that are selected using 3 metrics, the maintainability index, unsafe code complexity, and data type complexity. The authors propose a pipeline towards generating a benchmark that measures transpilation of language models at the function level. The final benchmark contains various constructs like deal with memory management, control flow constructs, etc."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes a benchmark, with an LLVM tool that can automatically create large amounts of function-level data. The final dataset covers various aspects like pointer types, type casting, memory management, control flow constructs, etc. The authors use pre-existing work to curate a large set of functions using an automated technique"}, "weaknesses": {"value": "- The benchmark measures success using compilation success but not execution-based correctness (unit test/integration tests). Not having validation correctness is a __severe limitation__ of the current work, given that a trivial implementation could also pass the current success measure. \n- Limited evaluation on models, why are closed-source models not accounted in the evaluation? Is it too trivial for LLMs like gpt-5, claude-4.5-sonnet to transpile function-level C code, given that qwen-2.5-coder gets ~97% (after 3 rounds of repair)? \n- The authors claim that the benchmark is a representative set, but do not perform an analysis of various kinds of errors made by language models (unsafe, borrowing, type mismatch, etc.) when transpiling from C to Rust. \n- The paper makes a strong claim on line 244 that generating compilable and correct output by an LLM is nearly impossible for multiple functions, while this is not the case, as shown in [CRUST-bench (Khatry et. al.)](https://arxiv.org/abs/2504.15254) for closed-source models like o3, gpt-4o, etc."}, "questions": {"value": "Please refer to weaknesses.\n\nOther questions:\n- What are the average lines of code in each instance of C2RustBench\n- Avg number of arguments, and argument types?\n\nMinor errors in presentation : \n- Line 211 states 4 metrics instead of 3"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RsgLEl5RHk", "forum": "VqI7rDJqiI", "replyto": "VqI7rDJqiI", "signatures": ["ICLR.cc/2026/Conference/Submission22519/Reviewer_fNEx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22519/Reviewer_fNEx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964559493, "cdate": 1761964559493, "tmdate": 1762942253989, "mdate": 1762942253989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}