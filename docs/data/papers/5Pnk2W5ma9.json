{"id": "5Pnk2W5ma9", "number": 11316, "cdate": 1758196130195, "mdate": 1759897593449, "content": {"title": "ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures", "abstract": "Neural architecture search (NAS) automates the design process towards high-performing architectures, but remains bottlenecked by expensive performance evaluation. Most existing studies that achieve faster evaluation are mostly tied to cell-based search spaces and graph encodings tailored to those individual search spaces, limiting their flexibility and scalability when applied to more expressive search spaces. In this work, we aim to close the gap of individual search space restrictions and search space dependent network representations. We present ONNX-Bench, a benchmark consisting of a collection of neural networks in a unified format based on ONNX files. ONNX-Bench includes all open-source NAS-bench-based neural networks, resulting in a total size of more than $600$k {$architecture, accuracy$\\}  pairs. This benchmark allows creating a shared neural network representation, ONNX-Net, able to represent any neural architecture using natural language descriptions acting as an input to a performance predictor. This text-based encoding can accommodate arbitrary layer types, operation parameters, and heterogeneous topologies, enabling a single surrogate to generalise across all neural architectures rather than being confined to cell-based search spaces. Experiments show strong zero-shot performance across disparate search spaces using only a small amount of pretraining samples, enabling the unprecedented ability to evaluate any neural network architecture instantly.", "tldr": "We present ONNX-Bench, a unified benchmark across diverse NAS search spaces, and ONNX-Net, a universal text-based representation and surrogate that enable fast performance prediction of arbitrary neural architectures.", "keywords": ["Neural Architecture", "Neural Architecture Representation", "Task Performance Evaluation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0abe8f537770f5fe03df72da64afd7ee41ba349.pdf", "supplementary_material": "/attachment/3c6ac5bf9e06a79dffe2832bf175e4f1acdd7ae6.zip"}, "replies": [{"content": {"summary": {"value": "Traditional NAS surrogate models predict architecture performance within a fixed search space, which ties them to a specific architecture representation with constrained topologies. To enable research that spans multiple NAS search spaces and to improve generalization across them, the authors unify architectures from different NAS benchmarks in ONNX format and build ONNX-Bench. To learn from these representations, the authors design ONNX-Net, an LLM-based predictor that treats ONNX files as text. The experiments demonstrate the generalization ability of ONNX-Net, and the ablation studies further support the effectiveness of the surrogate’s design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper unifies search spaces from different NAS works using ONNX. This allows surrogate models to predict performance across search spaces and provides the community with a valuable dataset.\n\n2. The paper is well structured and easy to follow.  \n   - It explains how ONNX-Bench is built, shows the similarities and differences between the search spaces, and displays the text form of the ONNX files for easy understanding.  \n   - It provides ablation experiments for the text encoding and shows how each component contributes to training and prediction.\n\n3. Generalization is a key problem for surrogate models. The authors validate ONNX-Net in three ways:  \n   - Cross search space in subsection 5.1.  \n   - Zero-shot transfer in subsection 5.2.  \n   - Cross dataset in subsection 5.3.  \n   These experiments collectively demonstrate that ONNX-Net achieves well-generalized performance across different search spaces and datasets."}, "weaknesses": {"value": "1. Subsection 5.1 does not compare with other baselines. It is hard to judge how well the proposed surrogate model is in that setting.\n\n2. Table 3 compares zero-shot results only for models trained on 50k samples from NAS-Bench-101 and evaluated on NAS-Bench-201. Readers may want to see zero-shot comparisons for more search spaces ( such as hNAS-Bench-201, NAS-Bench-301 ), since ONNX-Bench collects many spaces from NAS Benchmarks.\n\n3. While the paper demonstrates strong cross-space and zero-shot results, several potential causes behind the OOD behaviors remain under-analyzed:\n   - In the all-but-one search space setting, why do some target spaces show weaker OOD performance? Could this be caused by differences in operator op_type across spaces?\n  \n   - The ablation shows that Input information and Parameter information clearly contribute to performance. Does this imply that the model relies mainly on information that is independent of operator names in unseen spaces, such as input shapes or operator parameters?\n  \n   - Since different NAS benchmarks have different node number distributions, could OOD prediction performance also be affected by such node scales?"}, "questions": {"value": "1. ONNX-Net is evaluated with an ONNX-based representation, while other baselines are evaluated with their own representations. Since both the input representation and the surrogate model architecture differ, can this comparison be regarded as fair and meaningful?\n\n2. In subsection 5.2, the correlation after leaving out NATS-Bench is 0.390, while training on all is 0.788. Does this large gap indicate poor generalization?\n\n3. Are the issues listed in Weaknesses 3 reasonable and important? If they are indeed important, could the authors provide more explanations or discussions on these points?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TGWqPo4Z6f", "forum": "5Pnk2W5ma9", "replyto": "5Pnk2W5ma9", "signatures": ["ICLR.cc/2026/Conference/Submission11316/Reviewer_a8D2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11316/Reviewer_a8D2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626593444, "cdate": 1761626593444, "tmdate": 1762922455989, "mdate": 1762922455989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is about designing a generalizable predictor for Neural Architecture Search (NAS) using the Open Neural Network eXchange (ONNX) representation standard, and then using a Large Language Model (LLM) to perform the predictions. The name for this framework is ONNX-Net - consisting of ONNX-Bench, the neural networks in (representation, accuracy) pairs, on CIFAR-10, and ONNX-Net, the LLM-based predictor. ONNX-Net is evaluated on some unseen NAS tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strongest contribution of this paper is representing neural network architectures using the ONNX standard. \nThis is probably the best method to do so as ONNX is a platform for saving a neural architecture on one device, then deploying on another, e.g., for mobile deployment applications.\n\nFurther, the reviewer appreciates the operation distribution calculations shown in section 3, e.g., the JSD calculation and Fig. 3. This provides some necessary insights on the distributions of different search spaces. \n\nExtensive experiments are performed measuring the Kendall's Tau and Spearman Rho across different benchmarks, in the transfer context, and on unseen tasks."}, "weaknesses": {"value": "The first weakness of this work is that it is primarily on CIFAR-10 which is an incredibly worn-out benchmark at this stage and unlikely to be a good representative of how an architecture would perform on a higher-resolution task. For instance, NAS-Bench-201 [1] also consider CIFAR-100 and downsampled ImageNet; TransNAS-Bench [2] consider other tasks besides image classification and also provide macro search space architectures; AIO-P [3] only consider macro search space architectures for high-resolution tasks as well. While this work considers hierarchical search spaces as well as cell-based, it fails to substantially reach beyond CIFAR-10 and low-resolution tasks.\n\nSecond, the characterization of GENNAPE [4] is not accurate, since GENNAPE is not limited to cell-based architectures, but uses the same representation as [3] which covers macro-search space architectures for cross-task prediction. The description the authors use in the paper is better suited to CDP [5], which is one of the earliest iterations of a generalizable predictor but also confined to cell-based architectures.\n\nThird, the use of an LLM in this paper to predict performance seems like a large leap but doesn't provide sufficient pay-off, given the results, which while not lackluster, are mostly incremental. The reviewer would note that there have been several advances in low-cost predictor design to take advantage of the graph structure [6, 7] that this paper either does not seem to be aware of or simply discards."}, "questions": {"value": "Two questions:\n- Can the authors provide further comparison with flow-based [6] predictor models as well as causal predictor models [7]? This would help to better justify the use of an LLM.\n- L038: \"Recently, researchers have begun to focus on more expressive search spaces that enable the discovery of more diverse and innovative architectures\". There is more work in this field than the authors lead on. Are you able to provide some revised/further commentary/dialogue/work on these efforts?\n\nReferences:\n\n[1] Dong, Xuanyi, and Yi Yang. \"Nas-bench-201: Extending the scope of reproducible neural architecture search.\" arXiv preprint arXiv:2001.00326 (2020).\n\n[2] Duan, Yawen, et al. \"Transnas-bench-101: Improving transferability and generalizability of cross-task neural architecture search.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[3] Mills, Keith G., et al. \"Aio-p: Expanding neural performance predictors beyond image classification.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 8. 2023.\n\n[4] Mills, Keith G., et al. \"Gennape: Towards generalized neural architecture performance estimators.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 8. 2023.\n\n[5] Liu, Yuqiao, et al. \"Bridge the gap between architecture spaces via a cross-domain predictor.\" Advances in Neural Information Processing Systems 35 (2022): 13355-13366.\n\n[6] Hwang, Dongyeong, et al. \"Flowerformer: Empowering neural architecture encoding using a flow-aware graph transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[7] Ji, Han, et al. \"CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor.\" arXiv preprint arXiv:2506.04001 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqEQB7jXVd", "forum": "5Pnk2W5ma9", "replyto": "5Pnk2W5ma9", "signatures": ["ICLR.cc/2026/Conference/Submission11316/Reviewer_zxk1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11316/Reviewer_zxk1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692489193, "cdate": 1761692489193, "tmdate": 1762922455473, "mdate": 1762922455473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ONNX-Net, a universal surrogate model for neural architecture performance prediction that operates across diverse NAS search spaces. It builds ONNX-Bench, a unified benchmark of 600k architectures in ONNX format, and converts each network into text for LLM-based prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces ONNX-Bench, which collects architectures from multiple search spaces into a unified ONNX format. The dataset may benefit further research.\nThe paper explored the ONNX-to-text encoding method that applies to arbitrary architectures."}, "weaknesses": {"value": "The motivation for using the text encoding method is unclear. The authors should further clarify the differences and advantages of introducing text encoding compared to other possible approaches.\nThe authors argue that using Python code as an architectural representation could produce nonsensical or syntactically incorrect results. I believe the proposed ONNX approach in this paper faces a similar issue, and the authors may need to provide further clarification on the key difference.\nI noticed that the authors report Kendall’s τ for some results (Table 2) but Spearman’s ρ for others (Table 3). They should either include both metrics for completeness or explain why different correlation measures are used.\nThe zero-shot transfer experiments are only conducted on NAS-Bench-101 and NAS-Bench-201, both of which are cell-based search spaces. The authors should also demonstrate the model’s generalization ability across different types of search spaces."}, "questions": {"value": "In addition to the above, I also have a question: how do the authors view the relationship between the ONNX format and the encoded text? It seems that the ONNX-to-text process is essentially a simplification of the ONNX representation to fit the model’s input length. Therefore, can we consider ONNX merely as an intermediate format, and in fact, directly establish a search-space-to-text representation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "crpDoW7LQn", "forum": "5Pnk2W5ma9", "replyto": "5Pnk2W5ma9", "signatures": ["ICLR.cc/2026/Conference/Submission11316/Reviewer_9D7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11316/Reviewer_9D7F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907603017, "cdate": 1761907603017, "tmdate": 1762922455080, "mdate": 1762922455080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work mainly accomplished the representation of mainstream neural network architectures using the ONNX file format, which can be used for NAS research. It has more engineering value and lacks research innovation. In this standardisation process, the technical work also lacks sufficient validation. I suggest the authors to focus just one point, ONNX format or LLM refining on ONNX, with more in-depth research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This work explores the possibility of using ONNX for the unified conversion of network architectures, which provides some inspiration for subsequent research."}, "weaknesses": {"value": "1. The focus of this work is not clear enough. Specifically, is the theme of this paper the unified handling of network representations in the ONNX file format, or is it verifying LLM performance based on this? In either case, the research content is insufficient. \n\n2. For work involving the design and release of a unified representation format, the key point should be that the unified format does not alter the performance of existing models. This is essential to verify the effectiveness and reliability of a compromise unified representation format. However, this paper indicates that the performance of the proxy model changes at this point, which seems abnormal. It is recommended that the authors consider comparing the performance of the same proxy model prediction method under the proposed ONNX format and the original format, and then further demonstrate it."}, "questions": {"value": "I have one big concern. The author claims that 'a surrogate model using the novel text-based encoding trained on ONNX-Bench achieves competitive performance, especially for zero-shot transferability with minimal pretraining.' The question here is, compared with existing work, ONNX-NET only differs in file format or network representation, so why does it lead to model performance improvement? If asked further, is it a general performance improvement or mainly targeted at zero-shot? In fact, I doubt this conclusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lEMfD5Rlrm", "forum": "5Pnk2W5ma9", "replyto": "5Pnk2W5ma9", "signatures": ["ICLR.cc/2026/Conference/Submission11316/Reviewer_JBju"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11316/Reviewer_JBju"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154982634, "cdate": 1762154982634, "tmdate": 1762922454590, "mdate": 1762922454590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}