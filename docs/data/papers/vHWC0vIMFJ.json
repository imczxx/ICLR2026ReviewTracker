{"id": "vHWC0vIMFJ", "number": 6957, "cdate": 1758003354999, "mdate": 1762930791935, "content": {"title": "Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration", "abstract": "Many existing studies have achieved significant improvements in the reasoning capabilities of large language models (LLMs) through reinforcement learning with verifiable rewards (RLVR), while the enhancement of reasoning abilities in small language models (SLMs) has not yet been sufficiently explored. Combining distilled data from larger models with RLVR on small models themselves is a natural approach, but it still faces various challenges and issues. Therefore, we propose \\textit{\\underline{R}}ecall-\\textit{\\underline{E}}xtend \\textit{\\underline{D}}ynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration. In this paper, we explore the perspective of varying exploration spaces, balancing offline distillation with online reinforcement learning. Simultaneously, we specifically design and optimize for the insertion problem within offline data. By monitoring the ratio of entropy changes in the model concerning offline and online data, we regulate the weight of offline-SFT, thereby addressing the issues of insufficient exploration space in small models and the redundancy and complexity during the distillation process. Furthermore, to tackle the distribution discrepancies between offline data and the current policy, we design a sample-accuracy-based policy shift mechanism that dynamically chooses between imitating offline distilled data and learning from its own policy.", "tldr": "RED enhances small model reasoning by balancing distillation and reinforcement learning via entropy-based weighting and a dynamic policy switch, reducing redundancy and distribution mismatch.", "keywords": ["LLM", "reasoning", "SLM", "distill"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b7e8e50a99de0789b43da00ac4534f9a894916c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission presents Recall-Extend Dynamics (RED), a training paradigm designed to enhance the reasoning abilities of small language models (SLMs) by effectively combining reinforcement learning with verifiable rewards (RLVR) and supervised fine-tuning (SFT). The authors conceptualize RLVR as a “Recall” mechanism that optimizes reasoning paths within the model’s existing knowledge space, while they conceptualize SFT as acting as an “Extend” mechanism that broadens exploration by distilling reasoning patterns from larger teacher models. \n\nRED dynamically regulates the interaction between these phases through entropy-based exploration control, measuring changes in model entropy to adjust the relative influence of RL and SFT during training. When exploration entropy is small, SFT’s contribution is amplified to inject new reasoning diversity. In the opposite case, RL is weighted more heavily to refine learned reasoning strategies. This approach avoids the static or staged scheduling used in prior methods and provides a continuous, self-regulating balance between exploration and exploitation.\n\nThe authors also introduce an accuracy-aware policy shift mechanism that adaptively adjusts how the model integrates offline distillation data. Specifically, the authors estimate the off-policy probability of the current trajectory for a larger teacher model based on sample correctness: high-accuracy samples are used to reinforce the model’s own policy, while low-accuracy ones encourage imitation of the teacher’s policy. This approach yields theoretical guarantees of gradient stability, unbiased learning, and asymptotic consistency while mitigating entropy collapse. \n\nFinally, the authors also run experiments on mathematical reasoning benchmarks (MATH500, AIME24/25, AMC, Minerva, and Olympiad), and compare their performance with that of prior SFT, RL, and hybrid frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem of reasoning in small language models is important and well-motivated. Moreover, the authors propose a practical framework which comes with some nice built-in properties."}, "weaknesses": {"value": "With that being said, the authors’ contribution is somewhat incremental when compared to previous work in this area. Additionally, the empirical results are also somewhat incremental/inconclusive as to whether (and by how much) this method improves over baselines. \n\nThe theoretical results could also be proven more rigorously. For example, the “proof” of Proposition B.3 reads more like a proof sketch than an actual proof. Finally, the paper was hard to follow at times (e.g. many long equations are presented without much intuition)."}, "questions": {"value": "Could you provide a more formal proof of Proposition B.3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u8MPWriXPo", "forum": "vHWC0vIMFJ", "replyto": "vHWC0vIMFJ", "signatures": ["ICLR.cc/2026/Conference/Submission6957/Reviewer_1zfK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6957/Reviewer_1zfK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687007892, "cdate": 1761687007892, "tmdate": 1762919184237, "mdate": 1762919184237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "NLUtZIKPfR", "forum": "vHWC0vIMFJ", "replyto": "vHWC0vIMFJ", "signatures": ["ICLR.cc/2026/Conference/Submission6957/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6957/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762929397531, "cdate": 1762929397531, "tmdate": 1762929397531, "mdate": 1762929397531, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Recall-Extend Dynamics (RED), a framework designed to enhance the reasoning capabilities of small language models (SLMs) by unifying supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). The core idea is to balance Recall (refining reasoning within the model’s existing competence through RL) and Extend (expanding reasoning capacity through SFT). This balance is achieved via Dynamic Entropy Regulation and an Accuracy-Aware Policy Shift Mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Recall-Extend dichotomy is conceptually elegant and provides an intuitive lens for understanding SFT–RL synergy in small models.\n\n2. The proposed entropy-based regulation and accuracy-aware policy offset are both theoretically analyzed with proofs of stability.\n\n3. RED achieves consistent gains in reasoning accuracy and efficiency across multiple datasets."}, "weaknesses": {"value": "1. The paper’s core contributions, Dynamic Entropy Regulation and Accuracy-Aware Policy Shift, represent well-motivated but incremental refinements to existing unified SFT–RL frameworks, rather than a fundamentally new paradigm.\n\n2. Figure 1, and Figure 5 report accuracy or entropy trends, but the paper does not specify on which dataset or benchmark these results were obtained.\n\n3. The paper claims that RED improves efficiency but provides no quantitative comparison (e.g., GPU hours or rollout time) with other methods.\n\n4. Experiments are limited to mathematical reasoning; validation on general or multimodal reasoning tasks would strengthen the empirical evidence."}, "questions": {"value": "1-4. See weaknesses above.\n\n5. Have the authors evaluated RED beyond mathematical and scientific reasoning, for example, on commonsense or multimodal datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q9vZJyw6G6", "forum": "vHWC0vIMFJ", "replyto": "vHWC0vIMFJ", "signatures": ["ICLR.cc/2026/Conference/Submission6957/Reviewer_ZeYh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6957/Reviewer_ZeYh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824681781, "cdate": 1761824681781, "tmdate": 1762919183908, "mdate": 1762919183908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a new workflow in RLVR called Recall-Extend Dynamics (RED), which combines the GRPO loss and the SFT loss with modifications. An advantage term and an importance-sampling term are added into the SFT loss. This paper proposes a novel way to design the importance-sampling term. The experimental results are evaluated on several benchmarks, though only on one base model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The design of $\\pi^{offline}$ is novel to the reviewer and the figure 6 looks good.\n- The results are evaluated on several benchmarks, though only on one base model."}, "weaknesses": {"value": "The general algorithm is not novel, which is a combination of GRPO objective and SFT objective with minimal modifications. However, the intuition of the modifications is not well explained. Please see the \"Questions\" section.\n\nThe main experiments are only conducted on Qwen2.5-math-1.5B model, making the empirical improvement less convincing.\n\nThe theoretical part is highly informal, and the reasons are listed as follows:\n1. The proof of \"Entropy Preservation\" property in Proposition 1 is not rigorous. Please formally state the implicit assumptions and provide  rigorous proof. Otherwise it's not a proposition. For example, 1) the first-order approximation of $\\Delta H(\\pi_\\theta)$ is wrong: $\\Delta H(\\pi_\\theta)$ is a scalar while the formula given by the authors is a vector, and the expectation over states is missed; 2) $\\nabla_\\theta \\pi_\\theta(a'\\vert s_t)$ is a vector, and the authors cannot claim that vector to be negative; 3) the authors need to assume the existence of a action $a$, s.t. $\\log\\pi_\\theta(a)\\rightarrow -\\infty$, which is non-trivial;\n2. Eq.9 is trivial when $\\beta=0$. While for $\\beta\\rightarrow 0$ (which is covered in Proposition 2), to use Dominated convergence theorem, you need assumption on the boundness of $\\nabla_\\theta\\log \\pi_\\theta(a_t|s_t)$. Please state it formally.\n3. The proof of Proposition 3 is informal and not self-contained.\n\nAdditionally, the theoretical results can only provide weak results, such that the \"policy\" will converge to optimal policy a.s., which can be guaranteed by most RL algorithms. More important properties such as convergence rates and sample complexity are not covered. \n\nTherefore, the theoretical part is of minimal contribution and should be further revised.\n\nAs a conclusion, the reviewer thinks that this paper cannot be accepted to a top-tier venue like ICLR."}, "questions": {"value": "- Why should there be a length normalization term for the SFT component? The Eq 4 is not maximizing $\\pi(o_{sft}\\vert q)$ but maximizing $(\\pi(o_{sft}\\vert q))^{1/|o_{sft}|}$.\n - Why \"By adding an advantage term, RL with SFT loss can be easily transformed into on-policy format\"? What's the intuition of this choice? And what's $\\hat A_{i,t}$ in the second line of Eq 7? There seems to be no $i$ in the SFT component.\n- What's the definition of $r_{group}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HsTZOuv8uj", "forum": "vHWC0vIMFJ", "replyto": "vHWC0vIMFJ", "signatures": ["ICLR.cc/2026/Conference/Submission6957/Reviewer_B8QP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6957/Reviewer_B8QP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959400727, "cdate": 1761959400727, "tmdate": 1762919183270, "mdate": 1762919183270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets reasoning improvements for small language models by concurrently using offline distillation (SFT) and online RL with verifiable rewards. The model highlights two designs that balance different objectives. Dynamic entropy regularization balances SFT and RL objectives. An accuracy-aware mechanism balances sample weights for distilled trajectories and the model’s own collected trajectories."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The idea of using entropy to balance different objectives is intuitive and easy to implement. \n\n2. The model is compared with competitive baselines, including unified and stage-wise frameworks."}, "weaknesses": {"value": "1. Optimizing SFT and RL objectives concurrently can induce conflicting gradient signals and non-stationary targets, especially when the SFT teacher distribution and the RL policy are far from each other. This can exacerbate training instability that RL-based approaches face. \n\n2. The method is framed as targeted at small language models, but the design and analysis appear model-size agnostic. The paper does not isolate challenges unique to SLM fine-tuning (e.g., weaker in-context learning, shorter context windows, tighter compute/memory budgets, instability under long CoT) nor demonstrate behaviors that fundamentally differ from larger models.\n\n3. Lines 195-196 directly remove the KL divergence in GRPO and make the assumption that the ratio of current and reference policy is always within clipping range. This assumption needs to be further justified."}, "questions": {"value": "Please see weaknesses. \n\nFurther questions: \n\n1. What is $ r_{group}$ in equation 8 and how is it related to $\\beta$? Is it the high accuracy or high error rate indicator of a group of samples? How is it obtained in practice? \n\n2. What is proposition 1 (Stability and Adaptivity); 2. Robustness trying to prove? Why robustness can be inferred given the current policy has a zero probability of sampling action a given state s."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "augjDgAEYT", "forum": "vHWC0vIMFJ", "replyto": "vHWC0vIMFJ", "signatures": ["ICLR.cc/2026/Conference/Submission6957/Reviewer_6Qd9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6957/Reviewer_6Qd9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980665847, "cdate": 1761980665847, "tmdate": 1762919182797, "mdate": 1762919182797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}