{"id": "ACn1rbjlFB", "number": 23086, "cdate": 1758339385141, "mdate": 1759896832796, "content": {"title": "Adversarial Attack on Tensor Ring Decomposition", "abstract": "Tensor ring (TR) decomposition, a powerful tool for handling high-dimensional data, has been widely applied in various fields such as computer vision and recommender systems. However, the vulnerability of TR decomposition to adversarial perturbations has not been systematically studied, and it remains unclear how adversarial perturbations affect its low-rank approximation performance. To tackle this problem, we introduce a novel adversarial attack approach on tensor ring decomposition (AdaTR), formulated as an asymmetric max–min objective. Specifically, we aim to find the optimal perturbation that maximizes the reconstruction error of the low-TR-rank approximation. Furthermore, to alleviate the memory and computational overhead caused by iterative dependency during attacks, we propose a novel faster approximate gradient attack model (FAG-AdaTR) that avoids step-by-step perturbation tensor tracking while maintaining high attack effectiveness. Subsequently, we develop a gradient descent algorithm with numerical convergence guarantees. Numerical experiments on tensor decomposition, completion, and recommender systems using color images and videos validate the attack effectiveness of the proposed methods.", "tldr": "", "keywords": ["High-dimensional Data", "Tensor Ring Decomposition", "Adversarial Attack", "Tensor Completion"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b516c719b1c0f4a4d2b7546e9c6fe492202ad06.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AdaTR, an adversarial attack algorithm for Tensor Ring(TR) decomposition. It also proposes a faster variant to reduce iterative dependency called FAG-AdaTR. It first shows that directly extending the max-min formulation from ATNMF to form a baseline ATTR is not an effective attack. It shows this both empirically and theoretically. Moreover, it shows that ATR can improve performance for small perturbations. It then proposes the asymmetic max-min optimization whose objective is to directly maximize the reconstruction error of the TR decomposition. To reduce the gradient complexity and compurational cost, it proposes FAG-AdaTR. At last, it performs extensive experiments on various workloads comparing the proposesd attack with the baseline method with different defending algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a clear theoretical analysis revealing that existing adversarial training formulations (ATTR, which is directly applied from ATNMF) can paradoxically improve tensor decomposition performance, thereby motivating the need for a stronger attack formulation.\n\nIt proposes a conceptually sound asymmetric adversarial objective that better aligns with the notion of maximizing reconstruction error and demonstrates this design through empirical results. The experiments cover diverse tasks and clearly show that the proposes attacks are substantially more destructive than exisiting baselines."}, "weaknesses": {"value": "The paper’s main limitation is that it provides no theoretical guarantee or analysis for the proposed AdaTR and FAG-AdaTR algorithms. The only formal result concerns ATTR’s weakness, while the new methods are presented as heuristic formulations without convergence or optimality proofs. As a result, the contribution feels unbalanced.\n\nConceptually, AdaTR is a natural extension rather than a fundamentally new idea, and the “fast” variant is mainly an engineering improvement. The link between the theoretical critique of ATTR and the proposed algorithm is intuitive but not rigorously established.\n\nIn terms of presentation, the definition of “vulnerability” in the introduction is vague, and the distinction between adversarial training (ATTR) and attack (AdaTR) is not made clear until later. The experiments, while broad, rely only on reconstruction metrics and lack comparisons with general adversarial baselines or analysis of statistical robustness.\n\nTypo: in line 45, it misses a citation for the ANMF paper."}, "questions": {"value": "NA, see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pzv3d4r2LD", "forum": "ACn1rbjlFB", "replyto": "ACn1rbjlFB", "signatures": ["ICLR.cc/2026/Conference/Submission23086/Reviewer_9zHS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23086/Reviewer_9zHS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761427186890, "cdate": 1761427186890, "tmdate": 1762942505431, "mdate": 1762942505431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the adversarial attacks on tensor ring decomposition. It is first observed that the classical symmetric min-max method (ATTR) may even improve the performance of the model under certain conditions. Therefore, it motivates to development of the asymmetric method based on the bilevel optimization. Since the proposed method requires complicated gradient computation with backpropagation, a simplified version of the algorithm is proposed by deactivating some variables’ gradients (FAG-AdaTR). Experiments on color image decomposition attacks, video decomposition attacks, tensor completion, and recommendation systems are presented to show the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The writing is clear, and readers can easily follow.\n\n(2) The experiments are comprehensive. It covers various tensor data, including color images, videos, recommendation systems, and general tensor data."}, "weaknesses": {"value": "(1) I do not quite agree that the max-min problem (5) is symmetric and the bilevel form (10) is asymmetric. In minimax optimization, if we do not assume the Nash equilibrium (or some other conditions such as a strongly convex and strongly concave objective), the order of min and max cannot be changed. Therefore, (5) is asymmetric. I mean (5) is exactly equivalent to (10) if the order of min and max cannot be changed. In (5), given E, G is selected to minimize the objective. Therefore, the main motivation and claims in this paper are not correct from a minimax optimization perspective.\n\n(2) In your algorithm, you deactivate many variables’ gradients (w.r.t. E) for simplifying the computation. However, does it still guarantee the convergence of the algorithm? Is the simplified gradient still a descent direction? Is it possible that after you mask some gradients, the simplified gradient is not valid for the problem? I am suspecting the effectiveness of the simplified algorithm, at least theoretically. \n\n(3) The paper lacks a theoretical analysis of the algorithm (FAG-AdaTR). Is your algorithm convergent? If yes, what kind of point does it converge to?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EbtBaEZxxQ", "forum": "ACn1rbjlFB", "replyto": "ACn1rbjlFB", "signatures": ["ICLR.cc/2026/Conference/Submission23086/Reviewer_dx5X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23086/Reviewer_dx5X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791740803, "cdate": 1761791740803, "tmdate": 1762942505090, "mdate": 1762942505090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies adversarial attacks on TR decomposition. The authors argue that the conventional minmax ATTR objective (maximize w.r.t. perturbation, then minimize w.r.t. TR factors) can unintentionally improve low-rank reconstruction under small budgets and therefore isn't a true \"attack\" on TR (Thm 1). To address this, the authors propose an asymmetric bilevel formulation (AdaTR): minimize TR factors on the perturbed tensor but maximize the reconstruction error on the original tensor in Eq. (10). The perturbation is obtained through the ALS updates. The paper also introduces a faster version in which a closed-form approximate gradient for each mode is derived. Experiments on images, videos, and a recommender show larger degradation vs. ATTR across several TR-based defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper identifies a failure mode of ATTR for small budgets.\n+ The bilevel objective is an interesting formulation aligning with the attack goal. \n+ Fast approximate variant (FAG-AdaTR) with explicit gradients.\n+ Good experimental evaluation (images, videos, completion, recommender)."}, "weaknesses": {"value": "- The problem lacks clear motivation and lacks clarity on the threat model. The attack norm is Frobenius on the full tensor (global energy budget). For vision tasks this may not be aligned with perceptual threat models. For recommendation it's nontrivial what perturbing all entries means. The paper would benefit from a precise threat model per application domain (who controls what, where noise is injected, etc).\n- While Theorem 1 shows ATTR's potential to help at small $\\varepsilon$, the intuition behind how the proposed formulation in Eq. (10) fixes this is lacking. It is more procedural than structural. A theorem or a lemma explaining why the AdaTR objective directly targets the final error (and can't collapse as with ATTR) would strengthen the story. You can contrast the two objectives' gradients w.r.t E to make the fix more convincing. Right now the argument is mostly empirical (Fig. 1). \n- If I understood correctly, the FAG-AdaTR efficiency comes from decoupling E from some iterates trading bias for speed. However, the approximation error (how far from the true gradient ascent) is not quantified, and there is no theoretical attack optimality.\n- Beyond the approximation shortcut, there is no complexity analysis unless I missed that. \n- The paper mainly compares to ATTR and Gaussian noise, plus defense methods designed for completion/denoising, not attack methods on TR. A comparison to projected gradient attacks on the low-rank objective or to adversarial subspace attacks adapted from matrices would be important and show the gains. \n- For recommendation, it is unclear whether attacks respect the typical sparsity. Perturbing the dense rating tensor can be unrealistic. The setup should align with feasible manipulations (such as limited user/item edits).\n\nThe paper makes a worthwhile point (ATTR can help rather than harm for small $\\varepsilon$) and proposes a nice bilevel objective with a fast approximation. However, there are many weaknesses that need to be addressed to make this a solid paper. For ICLR, I'd want to see a crisper formal contrast between ATTR and AdaTR, compute/scaling and approximation-error analysis for FAG, models aligned per application. If strengthened along these lines, the paper could be compelling."}, "questions": {"value": "1) What is the adversary's capability per domain (images/videos vs recommender, etc)? Why Frobenius norm on the entire tensor, and how would results change with other realistic constraints?\n2) Can you provide a theoretical statement showing that the AdaTR gradient aligns with maximizing the final reconstruction error, whereas ATTR can degenerate to maximizing $\\Delta E$ as claimed? \n3) Can you bound or study the bias introduced by the FAG? When does it deviate most from AdaTR?\n4) What are the peak memory and time vs. tensor size, rank, and inner ALS iterations for AdaTR and what speedup does FAG deliver in practice? \n5) Comparisons to other attack methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bcf9mqwKYV", "forum": "ACn1rbjlFB", "replyto": "ACn1rbjlFB", "signatures": ["ICLR.cc/2026/Conference/Submission23086/Reviewer_HDzR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23086/Reviewer_HDzR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008171985, "cdate": 1762008171985, "tmdate": 1762942504490, "mdate": 1762942504490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates a previously unexplored question: are tensor decompositions vulnerable to adversarial perturbations? Focusing on Tensor-Ring (TR) decomposition, the authors formulate a dedicated adversarial attack on TR and derive a convergent gradient-based solver for the attacker."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work proposes the first adversarial attack tailored to tensor decomposition; prior work (ATNMF, LaFa) targets matrix factorisation or poisons data, not the decomposition operator itself. \n2. Additionally, it proposes a novel asymmetric bilevel formulation that flips the usual min-max adversarial-training order, directly optimising the attacker’s goal (max reconstruction error).\n3. The experiments are conducted in various applications and show the universality of the method."}, "weaknesses": {"value": "1. This paper claims “tensor decomposition” vulnerability, but only TR-ALS is attacked; it is unclear whether fragility extends to CP, Tucker, TT, or SVD-based methods. The authors may run the same asymmetric objective on Tucker-ALS and TT-SVD (only requires swapping the composition operator). \n2. This paper should compare more baselines. ATTR is a natural extension of ATNMF; however, data-poisoning attacks or subspace-rotation attacks are also relevant but omitted. The authors should include a subspace-rotation attack baseline that maximises principal-angle deviation; this checks whether TR fragility is simply due to low-rank bias rather than the proposed bilevel formulation.\n3. The experiments don't contain simple defenses. This work does not investigate whether adversarial training or input denoising can mitigate perturbations, leaving practitioners without effective countermeasures. The authors should add a defensive experiment: wrap TR-ALS with adversarial training using the proposed attack."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mbCoocd2mZ", "forum": "ACn1rbjlFB", "replyto": "ACn1rbjlFB", "signatures": ["ICLR.cc/2026/Conference/Submission23086/Reviewer_CGTS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23086/Reviewer_CGTS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101793022, "cdate": 1762101793022, "tmdate": 1762942503887, "mdate": 1762942503887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}