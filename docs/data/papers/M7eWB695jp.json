{"id": "M7eWB695jp", "number": 451, "cdate": 1756740265765, "mdate": 1763333971456, "content": {"title": "Purifying Generative LLMs from Backdoors  without Prior Knowledge or Clean Reference", "abstract": "Backdoor attacks pose severe security threats to large language models (LLMs), where a model behaves normally under benign inputs but produces malicious outputs when a hidden trigger appears. Existing backdoor removal methods typically assume prior knowledge of triggers, access to a clean reference model, or rely on aggressive finetuning configurations, and are often limited to classification tasks. However, such assumptions fall apart in real-world generative LLM settings. In this work, we propose a new framework for purifying **generative LLM** without any prior trigger knowledge or clean references. Through systematic sanity checks, we find that backdoor associations are redundantly encoded across MLP layers, while attention modules primarily amplify trigger signals without establishing the behavior. Leveraging this insight, we shift the focus from isolating specific backdoor triggers to cutting off the trigger–behavior associations, and design an immunization-inspired elimination approach: by constructing multiple synthetic backdoored variants of the given suspicious model, each trained with different malicious trigger–behavior pairs, and contrasting them with their clean counterparts. The recurring modifications across variants reveal a shared **\"backdoor signature\"**—analogous to antigens in a virus. Guided by this signature, we neutralize highly suspicious components in LLM and apply lightweight finetuning to restore its fluency, producing purified models that withstand diverse backdoor attacks and threat models while preserving generative capability.", "tldr": "", "keywords": ["LLM; Backdoor attack; Backdoor Elimination."], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8fb5649560df7b4cfc8ae90d1a863af8fa75bd88.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors propose a method for eliminating backdoors in large language models. The idea is to conduct multiple backdoor attacks on the same model, and identify those MLP parameters that are often updated as targets for finetuning and backdoor mitigation. The proposed approach has been evaluated using 3 tasks, 5 attacking methods, and compared with a number of baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "First, the empirical study discussed in Section 3.2 is fairly interesting, although some of the results are known through studies on model editing, still it is great to see that they are confirmed in the backdoor attacking as well (a special form of finetuning I suppose).\n\nSecond, the proposed method for identifying guilty parameters is a reasonable one. Although one can imagine certain adaptive attacks which avoid using commonly attacked parameters, it is good to see such an approach for five different kinds of backdoor attacks. \n\nLastly, the paper is fairly well-written, i.e., easy to follow, with well-designed evaluation session and discussion on the experimental results."}, "weaknesses": {"value": "On the other hand, the draft can be perhaps improved from the following aspects.\n\nFirst, the method can be further improved through counter-factual analysis, that is, you can improve the magnitude-and-consistency score by filtering those that are not causally related to the backdoor (e.g., if disabling the update on some parameters does not disable the backdoor, those parameters are deemed not causally related). \n\nSecond, the experimental evaluation can be improved by considering adaptive attacks (which, for instance, aim to update different parameters, e.g., by LoRA finetuning focusing on different parameters or layers each time).\n\nThe following are a list of detailed comments.\n\nAblation study on using different attacking methods should be done to show the robustness of the backdoor signature.\n\nPage 1: “... which can be deliberately obfuscated by adaptive attackers during injection.”\n\nComment: Can you provide some references to support your claim?\n\nPage 2: “... MLPs encode the malicious association: removing poisoned MLP updates reliably eliminates backdoor behavior, suggesting that trigger–response associations are established in MLP layers.”\n\nComment: Isn’t this what was found by those works on model editing, such as ROME through causal tracing?\n\nPage 2: “Intuitively, if very different trigger-behavior pairs all induce consistent parameter\nshifts, these shared neurons or channels must encode the abstract association machinery rather than any specific trigger.” \n\nComment: This may not be true if a different backdoor attack method is adopted. It would be helpful to comment on that here.\n\nPage 5: “We then define a define a magnitude-and-consistency score, sj , for each channel as …”\n\nComment: Typo. \n\nPage 6: “we intervene on the neurons in the gate_proj and up_proj matrices,\ntogether with the input channels in down_proj.”\n\nComment: What are gate_proj and up_proj and down_proj?"}, "questions": {"value": "How do you defend against an adaptive backdoor attack that randomly chooses some layers or parameter for backdoor injection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uX1ApZjVem", "forum": "M7eWB695jp", "replyto": "M7eWB695jp", "signatures": ["ICLR.cc/2026/Conference/Submission451/Reviewer_dxCf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission451/Reviewer_dxCf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760618313742, "cdate": 1760618313742, "tmdate": 1762915523347, "mdate": 1762915523347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of removing backdoors from generative large language models (LLMs) without relying on prior trigger knowledge or clean reference models. The authors conduct a detailed analysis revealing that backdoor associations are redundantly encoded in MLP layers, while attention modules primarily amplify trigger signals. Based on these insights, they propose an immunization-inspired framework that extracts backdoor signatures, followed by targeted neuron suppression and lightweight fine-tuning. The proposed method aims to eliminate diverse backdoor behaviors while preserving generative utility across different models, tasks, and attack types."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The topic of backdoor defense for generative LLMs is both important and timely, given the growing deployment of large models in safety-critical applications.\n- The authors conduct comprehensive experiments across multiple attacks and defense settings, including the BackdoorLLM benchmark, which provides strong empirical evidence for the method’s effectiveness."}, "weaknesses": {"value": "1.\tClarification on “without clean reference model” claim:\nAlthough the paper claims to remove backdoors without clean reference models, Section 3.3 shows that the computation of the differential delta (Δ) between backdoored and clean parameters is used to derive the backdoor signature. This implicitly relies on clean references, contradicting the stated assumption. Please clarify this inconsistency or reformulate the claim.\n2.\tReliability of conclusions in Table 1:\nThe observation that backdoors mainly reside in MLP layers may not be fully reliable. Different LoRA fine-tuning configurations can alter where triggers are embedded. For instance, backdoors can also be injected effectively by fine-tuning only attention layers. It would be more convincing if the authors fixed the fine-tuned layers and then re-examined the trigger localization patterns.\n3.\tLayer-wise backdoor analysis granularity:\nThe current analysis of backdoor behavior lacks fine-grained evaluation. The authors are encouraged to conduct layer-wise pruning to observe trigger activation rates. This would yield stronger interpretability and empirical insights.\n4.\tDirect mitigation from localization:\nIf backdoor behaviors can indeed be precisely localized, could pruning or fine-tuning those specific layers directly mitigate the attack? This connection should be discussed, as it might offer a simpler and complementary defense approach.\n5.\tGeneralization of the backdoor signature:\nThe proposed backdoor signature is derived from a set of pre-trained backdoored models. How well does this generalize to unseen attacks or datasets?"}, "questions": {"value": "Overall, this paper presents an interesting and valuable contribution to understanding and mitigating backdoors in generative LLMs. The empirical findings regarding layer-wise backdoor distributions are insightful, and the immunization-inspired framework is novel. However, the paper would benefit from more rigorous layer-wise empirical studies, clarified claims regarding reference-free assumptions to solidify its conclusions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TeEQbrYcyH", "forum": "M7eWB695jp", "replyto": "M7eWB695jp", "signatures": ["ICLR.cc/2026/Conference/Submission451/Reviewer_1Jtm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission451/Reviewer_1Jtm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761299497988, "cdate": 1761299497988, "tmdate": 1762915523218, "mdate": 1762915523218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To all Reviewers and AC: Global clarification on contribution and novelty"}, "comment": {"value": "Dear All:\n\nThank you for your time and valuable insights on this work. Several reviewers (rkEg and Khsx) expressed concern that our second insight (lines 60–62: “MLPs encode the malicious association …”) is not entirely new compared to [1], and therefore the overall contribution should be considered low to 1. We would like to clarify our position.\n\n**First**, our work was conducted **independently**, and our methods, model scale, and the details of the conclusions are fundamentally **different**. \n\n| Aspect                      | Lamparth et al. [1]                                   | This paper                                                |\n|----------------------------|--------------------------------------------------------|-----------------------------------------------------------|\n| **Model scale & setting**  | GPT-2 toy / medium (≤355M), text completion           | 7B–13B *chat* LLMs (LLaMA-2, Mistral, Code-LLaMA), instruction–response |\n| **Methodology**            | Activation-based： needs internal activations on trigger-containing data | Parameter-space only: no activations required |\n| **MLP-related finding**    | Backdoor mainly in **early MLP layers** plus **trigger embedding** changes | Backdoor associations redundantly encoded across **almost all MLP blocks**; remain active under block shuffling; also studied with **fixed trigger embeddings** |\n| **Role of attention**      | Primarily maintains language coherence in GPT-2       | Amplifies and routes trigger signals (esp. in Mistral), while MLPs encode the association itself |\n| **Goal & threat model**    | Mechanistic **editing** (insert/scale backdoors); assumes data **containing the true trigger** | Practical **purification**: no attacker trigger in any data; works under **full-model** and **LoRA-only** access; large ASR reductions with preserved utility |\n\n**Second**, similar observations that MLPs play a central role in storing associations have already appeared in knowledge editing work [2]. If we speak of the direct “influence”, then it would be the ``ICLR 2025 best paper [3]`` on hallucination and factual correction in MLP blocks: they operate in a setting where the target knowledge and inputs are known and explicitly edited; **in contrast**, we study backdoors under completely unknown triggers, where the defender does not know key–behavior pairs at all. We will clearly position our results as extending this intuition from knowledge-editing and hallucination correction to the backdoor setting in large generative LLMs.\n\n**Most importantly**, this MLP insight is only the **starting point that motivates our technique**, not the main contribution by itself. The core of our paper is a backdoor elimination framework, not a localization study: an immunization-style, reference-free signature extraction method plus a two-stage purification pipeline, evaluated on modern 7B–13B chat LLMs under both full-model and LoRA-only threat models and across multiple realistic attack families. Even if one considers the high-level insight that “MLPs matter more than attention” as shared with prior work, ``this does not diminish our contribution to “1”``: the main novelty lies in transforming this understanding into a practical, scalable defense for generative LLMs.\n\n**Finally**, we have acknowledged (``Introduction (lines 060-063) and Appendix A``) in the revised version that [1] has earlier pointed out, in smaller GPT-2–style models, that early MLPs are more important than attention for backdoor mechanisms. Our paper builds on an independently obtained, consistent observation and moves the focus from localization to actual purification in large-scale chat LLMs.\n\n**References**\n\n[1] Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. Lamparth et al.\n\n[2] Locating and Editing Factual Associations in GPT. Kevin Meng et al. [NeurIPS 2022]\n\n[3] AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models, Junfeng Fang et al. [ICLR 2025 Outstanding Paper]\n\nAuthors"}}, "id": "TtURqjsSbJ", "forum": "M7eWB695jp", "replyto": "M7eWB695jp", "signatures": ["ICLR.cc/2026/Conference/Submission451/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission451/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission451/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763333929588, "cdate": 1763333929588, "tmdate": 1763337887864, "mdate": 1763337887864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method to remove backdoors from LLMs in settings where the triggers aren't known and a clean reference model is not available by identifying trigger–behavior associations in MLP layers. It introduces an immunization-inspired approach that extracts shared backdoor signatures across poisoned variants and suppresses them to neutralize backdoors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper clearly motivates the problem of backdoor removal in LLMs without access to trigger information or a clean reference model, which is a realistic and practically relevant scenario.\n- The proposed immunization-inspired signature extraction framework is conceptually clear and intuitive \n- The method’s ability to operate effectively under both full-model access and adapter-only settings increases practical relevance, because many deployed LLMs expose only adapter-level modification capabilities"}, "weaknesses": {"value": "- Multiple claims throughout the paper (e.g., backdoors are “easy to inject” and “extremely difficult to detect”, Sec. 1) are not sufficiently supported by citations or empirical justification, and would benefit from references.\n- Some terminology remains underspecified, particularly the contrast implied by the term “generative LLM” (Sec. 1): it is unclear what the authors consider a “non-generative LLM” in this context. Additionally, the phrase “safe conditions” (Sec. 1, line 50) lacks a precise definition or operational criteria.\n- The comparison to prior work on backdoor localization is incomplete. For example, the paper identifies MLP layers as the central locus of trigger–behavior associations, but does not discuss how this finding relates to prior mechanistic localization analyses (e.g., https://arxiv.org/abs/2302.12461 and others). It's unclear to me how their contributions are novel compared to prior literature. I'm willing to update my score once this gets clarified.\n- The procedure for constructing poisoned vs. clean variants used in immunization-style signature extraction is not described in enough detail to reproduce: the paper does not specify sampling strategy for D_clean, how triggers and behaviors are selected or diversified, or whether dataset overlap across variants influences extracted signatures.\n- The experimental evaluation omits coding-specific utility measurements in the code injection setting: while Code-LLaMA models are included, the paper does not report any post-purification coding performance metrics.\n- The reported reductions in ASR are not consistently below 5% as claimed in the text, particularly for targeted refusal attacks (per Table 2).\n- The structure of the paper is confusing (e.g., why are key findings listed as part of the methodology section?)."}, "questions": {"value": "- How would one suspect a model is backdoored in the first place under your assumed setting?\n- How are poisoned and clean variant datasets constructed, and how is variant diversity ensured across trigger and behavior choices?\n- Do backdoor signatures transfer across models or architectures, or must extraction be repeated per model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5i4ZTJL6wo", "forum": "M7eWB695jp", "replyto": "M7eWB695jp", "signatures": ["ICLR.cc/2026/Conference/Submission451/Reviewer_Khsx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission451/Reviewer_Khsx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863805306, "cdate": 1761863805306, "tmdate": 1762915523101, "mdate": 1762915523101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global update on the revised PDF: all changes are highlighted in blue."}, "comment": {"value": "We have uploaded a revised version of the paper, with minor changes in **blue**. The changes are deliberately minimal and do not affect the method or results:\n\n- (1) We updated lines (060-063) and added a short subsection in Appendix A discussing prior mechanistic and knowledge-editing papers (e.g., Lamparth et al. on GPT-2).\n- (2) Added more citations in the introduction and related work (**please check more details to address weakness 1 posted by reviewer, rkEg, and several from the other reviewers**).\n- (3) Added a footnote on the first page to clarify generative LLM. \n- (4) Deleted a confusing part in section 3.1 (lines 155-156 in the old version).\n\nWe addressed **all other points** raised by the reviewers in detail in this rebuttal and will incorporate them in the camera-ready version if this paper is accepted."}}, "id": "L5z9y6d5zA", "forum": "M7eWB695jp", "replyto": "M7eWB695jp", "signatures": ["ICLR.cc/2026/Conference/Submission451/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission451/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission451/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763334156181, "cdate": 1763334156181, "tmdate": 1763337731558, "mdate": 1763337731558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission proposes an “immunization-inspired” purification method to remove backdoors from LLMs without knowing the true trigger or having a clean reference model by creating multiple synthetic poisoned and clean fine-tuned variants of the same base model, each with different key–behavior pairs.\n\nThe authors compute parameter update differences between (LoRa and SFT) poisoned and clean variants and identifies shared, consistently aligned channels as the backdoor signature in their experiments and introduce a two-stage purification pipeline: Suppress/reinitialize high-scoring channels in MLPs or LoRA adapters and lightly fine-tune on clean data to recover fluency.\n\nThe authors claim to provide novel insights on MLPs encoding backdoor association while attention modules are not the key driver of the mechanism, that the activation is distributed across the model and different parts of the model can learn the backdoor, even when shuffled. They demonstrate the effectiveness if their method across multiple LLMs (LLaMA-2, Mistral) and attack types (e.g., BadNets, CTBA, Sleeper), outperforming their chosen baselines like pruning and fine-pruning and report results using attack success rate (ASR) and general benchmark utility. They also observes that backdoor activation is redundant and order-invariant across many MLP layers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The submission \n* proposes a novel, reference-free purification framework that extracts a shared backdoor signature across synthetic poisoned variants via magnitude + alignment scoring.\n* introduces a two-stage purification pipeline (channel suppression + light clean fine-tuning) effective for both full-model and LoRA-only access.\n* demonstrates good empirical results across multiple large models (LLaMA-2, Mistral) and diverse attack types, outperforming established baselines such as pruning and fine-pruning.\n* provides clear experimental methodology and presentation, including ablation studies on model components and purification stages."}, "weaknesses": {"value": "## Weakness 1 [Significance/Originality] \n\nThe paper's related work omits several critical contributions that shape today’s LLM backdoor landscape, including attacks via instruction tuning, attacks in other training steps, attacks in PEFT/LoRA settings, and, crucially, recent mechanistic analyses of how and where backdoors are encoded that come to similar findings as this submission, raising significant concerns regarding novelty and quality of the contributions made to the field. \n\nFor example, the authors did not cite influential papers in the field of backdoored LMs like\n\nUniversal Jailbreak Backdoors from Poisoned Human Feedback. Rando et al.\nTrojaning Plugins of Large Language Models. Dong et al.\nAttention-Enhancing Backdoor Attacks Against BERT-based Models. Lyu et al.\nPoisoning Language Models During Instruction Tuning. Wan et al.\nPPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning. Du et al.\nAnti-Backdoor Learning (ABL): Training Clean Models on Poisoned Data. Li et al.\nBlind Backdoors in Deep Learning Models. Bagdasaryan et al.\nSpinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures. Bagdasaryan et al.\n\nlimiting the rigor and completeness of the paper's threat model and contextual grounding. \n\nFurther, the paper \n\nAnalyzing And Editing Inner Mechanisms Of Backdoored Language Models. Lamparth et al. (Arxiv 2023, published 2024)\n\n(which is also not cited) makes several key contributions that significantly overlap with the claimed novel insights made by this submission. In particular, \nit\n* identifies that early-layer MLPs and embedding projections encode backdoor behavior; attention modules are not triggers.\n* introduces a method to localize, remove, or reinsert backdoor mechanisms (in clean and backdoored LLMs).\n* shows backdoor activation distributed across early layers and scalable by parameters edits.\n* studies the effect of keeping MLPs and attention modules fixed during fine-tuning to reduce backdoor eprformance without harming utility.\nin a trigger-agnostic way (manipulation of backdoors without needing to know the trigger, only a large dataset containing it). Meaning that both show MLPs encode the malicious association while attention mainly amplifies or maintains coherence, confirm activation is distributed across layers (strongest in early MLPs), attacks are trigger-agnostic in approach (although with different methods; dataset activations vs synthetic variant contrasts), and enable backdoor removal without external clean reference models.\n\nBesides using newer models and attack methods that since came out compared to the old paper,  this seemingly reduces the novel contributions of the submission to their method to collect the backdoor signature extraction and for the purification pipeline, studying PEFT settings, and applications like Coding. \n\nA more rigorous literature review and better positioning of the submitted paper could strengthen it terms of significance/originality.\n\n## Weakness 2 [Quality]\n\nThe submission sutdies attack success rate and a general utility score, but omits the metric of accidental trigger rate (ATR), which can lead to underestimating false positives or collateral damage from purification not captured in the utility score. It is also unclear how potential over-purification could be a problem, asfull-channel reinitialization may remove benign semantics or have other unmeasured side effects.\n\nAdditional experiments and clarifications could strengthen the paper in terms of quality.\n\n## Weakness 3 [Quality]\n\nIt is unclear how real attacker backdoors may behave differently to the author self-generated poisoned variants. Also, there seems to be a dependence on the behavior knowledge of the backdoor, as “trigger-agnostic” still assumes ability to define synthetic behaviors and fine-tune models.\n\nAdditional experiments or clarifications could strengthen the paper in terms of quality."}, "questions": {"value": "* Is there a reason ATR was not studied and can over-purification be a problem?\n\n* How realistic are the generated attacks compared to real-world attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vAWWZyAmIC", "forum": "M7eWB695jp", "replyto": "M7eWB695jp", "signatures": ["ICLR.cc/2026/Conference/Submission451/Reviewer_rkEg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission451/Reviewer_rkEg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043584129, "cdate": 1762043584129, "tmdate": 1762915522980, "mdate": 1762915522980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}