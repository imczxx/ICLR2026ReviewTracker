{"id": "LW2BrESOMQ", "number": 11371, "cdate": 1758197598749, "mdate": 1759897579359, "content": {"title": "Probabilistic Forecasting via Autoregressive Flow Matching", "abstract": "In this work, we introduce autoregressive flow matching (AFM) for probabilistic forecasting of multivariate timeseries data. Given historical measurements and optional future covariates, we formulate forecasting as sampling from a learned conditional distribution over future trajectories. Specifically, we decompose the joint distribution of future observations into a sequence of conditional densities, each modeled via a shared flow that transforms a simple base distribution into the next observation distribution, conditioned on observed covariates. To achieve this, we leverage the flow matching framework, enabling scalable and simulation-free learning of these transformations.\nBy combining this factorization with the flow matching objective, AFM retains the benefits of classical autoregressive models—including strong extrapolation performance, compact model size, and well-calibrated uncertainty estimates—while also capturing complex multi-modal conditional distributions, as seen in modern transport-based generative models. We demonstrate the effectiveness of AFM on multiple stochastic dynamical systems and real-world forecasting tasks.", "tldr": "We propose a method for sample-effcient forecasting of multivariate timeseries data via autoregressive flow matching.", "keywords": ["Forecasting", "Dynamical systems", "Generative Modeling", "Flow Matching"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07cfeb379a8f1255a77c93d870854cba45f408a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose to forecast time series with autoregressive flow matching.\nThey achieve good forecasting results on 5 datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is straightforward to implement and understand"}, "weaknesses": {"value": "1. Line 91-92 claim that the model \"provides well-calibrated uncertainty estimates\" but the uncertainty estimates and their calibration are not evaluated in any experiments.\n1. Table 2 lists TSFlow and AFM as best on Exchange when actually ETS is better."}, "questions": {"value": "1. How many steps did you forecast in your experiments?\n1. How does your model compare in runtime performance to other models?\n1. Are your baselines univariate or multivariate models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "W3u1L7O5ag", "forum": "LW2BrESOMQ", "replyto": "LW2BrESOMQ", "signatures": ["ICLR.cc/2026/Conference/Submission11371/Reviewer_NhUs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11371/Reviewer_NhUs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761051432161, "cdate": 1761051432161, "tmdate": 1762922498148, "mdate": 1762922498148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address a limitation of existing flow matching approaches for time-series to be either restricted to univariate forecasting or require direct learning of the complex conditional distribution of a fixed forecasting window. To this end, they propose Autoregressive Flow Matching (AFM), which factorizes the joint distribution of future observations into a sequence of conditional densities. They model each of these densities via parameter sharing and a shared flow. The authors show that this approach enables mutlivariate forecasting and better generalization abilities compared to previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is mostly easy to follow\n- Clear motivation for addressing the limitations of non-autoregressive flow matching methods in time-series forecasting, regarding (not needing priors, being constrained to univariate forecasting, separating the problem into easier subproblems)\n- Autoregressive component is ablated and the advantages are made clear\n- Method scales to multivariate forecasting \n- Improvements in \"out-of-distribution\" settings (with caveats)"}, "weaknesses": {"value": "- The autoregressive factorization maks the problem sequential, which should increase inference time compared to fully parallel methods; no quantitative analysis of this trade-off is provided as far as I could see (the limitation is briefly discussed in the conclussion) \n- No efficiency comparison between different methods in general (e.g., Table 2)\n- Font size in figures ins considerably to small\n- The Markov assumption with fixed window size may limit long-range temporal dependencies. Based on the experiments, it is unclear how sensitive the results are to the choice of window length\n- In general, there are not many ablation studies regarding the hyperparameter of the proposed method (apart from evaluating a non-autoregressive variant)\n- More details regarding the non-autoregressive baselines could be moved to the main paper (there is space available, and this baseline is quite important in my opinion)\n- OOD claims rely mainly on synthetic “extrapolation window” splits. If claims about OOD generalizations are made, I would expect shifts in the data distribution. This claim may be adjusted accordingly / made more precise"}, "questions": {"value": "- How does AFM’s inference speed compare quantitatively to TSFlow and diffusion-based baselines, especially for long horizons, where there are a lot of forecasting steps required?\n- How sensitive is AFM’s performance to the history window size w? E.g., would increasing w improve long-range forecasting?\n- Does sharing the same flow network across all time steps limit performance? Did you evaluate other ways to parameterize the forecasting (I understand that this would increase complexity quite a bit and dont view it as a limitation) just curious\n- Are there scenarios where the non-autoregressive methods may be preferable? Is there a clear limitation to your approach beyond efficiency?\n- There are a lot of comparisons to (Kollovieh et al., 2024). However, the authors do not conduct any experiments on unconditional forecasting. Would this be possible with the proposed approach? I assume the autoregressive formulation makes this difficult?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4qoIkoJMmu", "forum": "LW2BrESOMQ", "replyto": "LW2BrESOMQ", "signatures": ["ICLR.cc/2026/Conference/Submission11371/Reviewer_DHD9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11371/Reviewer_DHD9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385264541, "cdate": 1761385264541, "tmdate": 1762922497873, "mdate": 1762922497873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an auto-regressive flow matching model, which allows for flexible and expressive modelling of time series."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Good and clear presentation and writing.\n- AR-Flow matching is a very reasonable model for time-series."}, "weaknesses": {"value": "My main concern about this paper is the main motivation for autoregressive modelling. To me this approach is a step backwards, since autoregressive modelling is costly and can lead to divergence/accumulation of error. This is the exact motivation for more recent generative models to directly model the joint distribution non-autoregressively.\n\nNovelty and Contribution concerns:\n- The paper claims, that modelling the joint distribution is more complex, than just the next step autoregressively, which is true. However, this does not necessarily imply that sequentially sampling is also simpler or less costly or correct for that matter.\n- To me it seems like the motivation for the paper is that extending existing approaches to multivariate cases, such as the referred to Kollovieh et al, is non-trivial/hard. Which however does not necessarily imply that autoregressive approaches are the right solution.\n- Furthermore, even though it argues about multivariate cases its shown for univariate time series.\n- The methodological contribution is limited, since its applying standard Flow Matching and defining it within the specific application.\n- Claims cheaper training than joint modelling, which is not immediately evident to me. Furthermore, reports shorter training times for non-autoregressive baselines in the appendix.\n\nExperiments:\n- Real-world results are non-significant, sometimes worse.\n- Efficiency is not evaluated even though this is likely the main limitation of the method. This is especially problematic for the AR vs non-AR comparison, since one has to evaluate the AR per step. Thus, the model complexity of the non-AR can be greater while still allowing fast inference.\n- Limited evaluation solely for forecasting. How does the method compare to baseline for unconditional generation?"}, "questions": {"value": "- Can you confirm the architectures and model complexity of AR vs Non-AR variants. To me it seems like both leverage the same complexity in terms of model size, which for above mentioned reasons might be unfair for the joint models.\n- Can you provide us with an analysis of different hyperparameter configurations for the AR and Non-AR models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4du8QNnMN7", "forum": "LW2BrESOMQ", "replyto": "LW2BrESOMQ", "signatures": ["ICLR.cc/2026/Conference/Submission11371/Reviewer_YYJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11371/Reviewer_YYJT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663890746, "cdate": 1761663890746, "tmdate": 1762922497389, "mdate": 1762922497389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose autoregressive flow matching for forecasting. Instead of generating the whole future trajectory at once, the model factorizes the distribution autoregressively and generates each step via a CFM model. The framework is compared to a non-autoregressive baseline on dynamical systems and other baselines on real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written, the methodology is clearly described and motivated. While the idea of autoregressive forecasting via generative models is not new, the paper provides interesting insights and applies this concept to flow matching models.\n- The autoregressive framework demonstrates strong results on the dynamical systems and clearly shows its advantages over non-autoregressive models."}, "weaknesses": {"value": "- The novelty of the autoregressive flow matching is limited. A similar approach was explored for diffusion models [1].\n- The setup for the real-world experiments is unclear to me. The paper mentions *univariate* and *multivariate* datasets (L320), but it is not explained how these are treated. As most baselines are evaluated in a univariate setting and do not allow information exchange between channels, I expect AFM to be evaluated analogously. In any case, the model should be compared to [1].\n\n[1] **Rasul, K., Seward, C., Schuster, I., & Vollgraf, R.** (2021, July). Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International conference on machine learning (pp. 8857-8868). PMLR."}, "questions": {"value": "- How does the runtime compare to the non-autoregressive flow matching baseline?\n\nAnd see weaknesses.\n\nI am willing to increase my score if a comparison with [1] is included."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s6LJRiuNs1", "forum": "LW2BrESOMQ", "replyto": "LW2BrESOMQ", "signatures": ["ICLR.cc/2026/Conference/Submission11371/Reviewer_gHtv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11371/Reviewer_gHtv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854630631, "cdate": 1761854630631, "tmdate": 1762922496850, "mdate": 1762922496850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}