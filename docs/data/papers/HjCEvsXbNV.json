{"id": "HjCEvsXbNV", "number": 18124, "cdate": 1758284151020, "mdate": 1759897131491, "content": {"title": "FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations", "abstract": "We introduce FloorplanQA, a diagnostic benchmark for evaluating spatial reasoning in large-language models (LLMs). FloorplanQA is grounded in structured representations of indoor scenes, such as (e.g., kitchens, living rooms, bedrooms, bathrooms, and others), encoded symbolically in JSON or XML layouts. The benchmark covers core spatial tasks, including distance measurement, visibility, path finding, and object placement within constrained spaces. Our results across a variety of frontier open-source and commercial LLMs reveal that while models may succeed in shallow queries, they often fail to respect physical constraints, preserve spatial coherence, though they remain mostly robust to small spatial perturbations. FloorplanQA uncovers a  blind spot in today’s LLMs: inconsistent reasoning about indoor layouts. We hope this benchmark inspires new work on language models that can accurately infer and manipulate spatial and geometric properties in practical settings.", "tldr": "FloorplanQA evaluates large language models’ spatial and geometric reasoning on structured indoor layouts, featuring questions on topological logic and design constraints, revealing gaps in models’ ability to reason about spatial environments.", "keywords": ["Spatial Reasoning", "Layout Reasoning", "Scene Understanding", "Structured Scene Representations", "Benchmark", "Large Language Models (LLMs)"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd43bc434cd535ca579fd9a37415a506cd4e9fee.pdf", "supplementary_material": "/attachment/46b176f598901c186dea1430a1fe7e5248f2ae83.zip"}, "replies": [{"content": {"summary": {"value": "FloorplanQA proposes a benchmark for evaluating spatial and geometric reasoning in large language models (LLMs) using structured 2D indoor layouts encoded in JSON/XML, focusing on tasks such as distances, visibility, placement feasibility, free space computation, and path planning with clearance. The dataset comprises 2,000 layouts—1,800 synthetically generated via Gemini 2.5 Pro prompts and 200 layouts extracted from HSSD—paired with 16,000 questions (eight per layout), and the evaluation protocol compares outputs to deterministic ground truth with tolerance thresholds. The study assesses 15 models (7 “reasoning” and 8 “general”) under zero-shot prompts, reports accuracy by model and question, and provides an encoding ablation (JSON vs. XML) that shows limited sensitivity.\nOverall, the benchmark highlights that current LLMs can answer simple metric queries but struggle with overlap-heavy layouts, free-space unions, and shortest-path planning, with HSSD layouts exposing more errors than synthetic axis-aligned ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark isolates symbolic spatial reasoning using structured floorplans rather than images, offering a complementary diagnostic perspective to vision-language tasks\n2. A clear taxonomy spans metric, topological, and action-like tasks; answer formats and scoring rules are specified with tolerances for numeric and geometric checks\n3. Encoding ablation (JSON vs. XML) suggests limited sensitivity to layout serialization, at least for selected tasks and models"}, "weaknesses": {"value": "1. 1800 layouts are generated by an LLM with rule-based filters, and synthetic objects are axis-aligned boxes; only 200 HSSD layouts introduce non–axis-aligned geometry. This raises concerns about realism, diversity, and potential generator biases\n2. Questions are posed on single-room layouts; multi-room reasoning and dynamic layout changes (e.g., moving objects and re-evaluating visibility across rooms) are not covered. Room types are mainly kitchens, living rooms, and bedrooms\n3. Numeric tolerances (2% for scalars, 5% for complex areas) are asserted but not justified via sensitivity analysis. Path validation thresholds and clearance buffering are not stress-tested across diverse geometries\n4. The protocol depends on regex-based parsing with strict “Final answer” markers; while invalid formats are reportedly rare, this design can bias task solvability and error attribution"}, "questions": {"value": "How will you mitigate comparability confounds from token budgets and truncation? Can u report per-model truncation rates with error-mode analyses\n\nCould you add sensitivity studies for numeric tolerances (2%/5%) and path validation thresholds (Fréchet, clearance), including stress tests across polygon concavity and overlap density"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oGwCy0P4FL", "forum": "HjCEvsXbNV", "replyto": "HjCEvsXbNV", "signatures": ["ICLR.cc/2026/Conference/Submission18124/Reviewer_FFLW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18124/Reviewer_FFLW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760584152628, "cdate": 1760584152628, "tmdate": 1762927888363, "mdate": 1762927888363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new benchmark, FloorPlanQA, designed to evaluate the ability of large language models (LLMs) to infer spatial awareness and perform reasoning based solely on structured inputs. The benchmark consists of two main components. The first and largest portion is synthetically generated using Gemini 2.5 Pro. The second portion projects 3D scenes from HSSD-200 into corresponding 2D layouts, enriching the dataset with realistic spatial configurations.\n\nTo generate questions, the authors employ a template-based approach to form eight question types spanning three categories: Action, Topology, and Metric. This design enables comprehensive evaluationmfrom low-level geometric calculations to higher-level reasoning tasks that require detailed spatial planning.\n\nThe experiments involve 15 LLMs, including both naive and reasoning-oriented LLMs, evaluated on the proposed dataset. Results show that current SOTA LLMs still struggle to fully comprehend geometric representations, particularly when layouts involve overlapping spatial elements. The category-wise findings further indicate that low-level tasks are generally easier for models, whereas high-level reasoning tasks remain challenging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a publicly available dataset for geometric representations of room layouts, enabling the evaluation of LLMs’ spatial and layout understanding.\n\n- An automatic pipeline is introduced to generate synthetic layouts using LLMs, demonstrating strong potential for building scalable benchmarks.\n\n- The proposed benchmark consists of both synthetic and realisitc layouts, providing rich information to be evaluate.\n\n- The proposed benchmark reveals the limitations of current LLMs in comprehending scenes solely from geometric representations and reasoning over them.\n\n- The results show that models struggle with complex layouts but achieve reasonable performance on simpler configurations.\n\n- The paper provides a detailed quantitative analysis of the results, revealing the specific behavior of each LLM across different layout types and task categories.\n\n- The paper also illustrates the consistenl results acorss different types of layout representations (JSON vs XML)\n\n- Paper is well-written and easy to follow. Significant details are provided for reproducibility."}, "weaknesses": {"value": "- The discussion of experimental results appears somewhat shallow (only based on numerical number), with limited analysis of why the models fail on specific tasks. Some failures qualitative analysis would be insightful on model behavior in this task.\n\n- No methods are proposed to improve model performance; even preliminary ideas or directions for enhancement would strengthen the contribution.\n\n- A discussion regarding models with visual training could be valuable, as it may reveal whether such training influences performance on spatial reasoning tasks. \n\n- Although the paper includes examples of actual layouts in the main text, the layouts generated by Gemini are never discussed. Including at least one example would improve understanding and reproducibility.\n\n- The paper mentions a few case studies in the appendix that highlight model mistakes, but these are not referenced in the main content. Briefly mentioning them in the main paper would help readers recognize where to find detailed examples of model failures."}, "questions": {"value": "There are no additional questions, except respond to each weakness raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rg6xmmRTRO", "forum": "HjCEvsXbNV", "replyto": "HjCEvsXbNV", "signatures": ["ICLR.cc/2026/Conference/Submission18124/Reviewer_2emM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18124/Reviewer_2emM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942145185, "cdate": 1761942145185, "tmdate": 1762927888028, "mdate": 1762927888028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FloorplanQA is a benchmark for testing LLMs’ spatial reasoning on indoor floor plans using structured, symbolic layouts (JSON/polygons with rooms, doors/windows, objects, and sizes) rather than images or external tools. It targets three capability groups—metric (e.g., distances, areas), topological (e.g., visibility, occupancy, placeability), and action/path reasoning (e.g., relocation, shortest paths with safety margins). The dataset contains 2,000 layouts (1,800 synthetically generated and 200 derived from HSSD), with 8 questions per layout for a total of 16,000 QA pairs. A unified automatic scoring protocol (numeric tolerances, set matching, geometric validity, Fréchet thresholds for paths) is provided, along with explicit accounting for invalid outputs caused by formatting or truncation, enabling fair comparisons and detailed error diagnosis across models."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Symbolic input, tool-free setup that isolates pure geometric/topological reasoning without visual noise or help from external solvers.\n\n2. Comprehensive coverage across metric, topological, and action/path tasks, including Free Space, Max Box, Placement, Visibility, and Shortest Path.\n\n3. Strong comparability via automation: strict output formats and tolerance thresholds; tailored scoring rules for numbers, sets, and sequences (e.g., 2–5% tolerances, set matching, Fréchet threshold with collision constraints).\n\n4. Realistic geometric diversity by combining controllable synthetic layouts with irregular polygons from HSSD under a unified representation."}, "weaknesses": {"value": "1. Gap to real-world perception/interaction: purely symbolic floor plans omit imagery, noise, and perception errors, limiting ecological validity for embodied/vision tasks.\n\n\n2. Planar-geometry focus: limited coverage of richer functional metrics (e.g., door flow, dynamic crowds, reachability and behavior constraints).\n\n\n3. Sensitive to long-context/token budgets: truncation and formatting issues materially affect outcomes."}, "questions": {"value": "With only symbolic floor plans, this isn’t directly usable. Could you also provide corresponding image-based modeling (e.g., via image-generation models) and even try to generate 3D models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WiCabY1Sto", "forum": "HjCEvsXbNV", "replyto": "HjCEvsXbNV", "signatures": ["ICLR.cc/2026/Conference/Submission18124/Reviewer_cFtC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18124/Reviewer_cFtC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969599712, "cdate": 1761969599712, "tmdate": 1762927887611, "mdate": 1762927887611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark to evaluate the spatial understanding and quantitative reasoning capabilities of llms in a indoor floorplan setting. The benchmark describes the geometrical structure and state of a room including the location of doors, windows, structures and obstacles and asks an llm to a set of different questions such as distance between two objects, area of free space, etc. This evaluates how well the llm can internally visualize and reasoning about the spatial context from only the description.\n\nThe contribution of the paper is the new benchmark which allows the community to  evaluate llm capabilities in spatial reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A large dataset containing both synthetic and hand designed samples of structural design schematics in a descriptive json / xml format\n- The dataset is well designed  to cover a lot of different cases such as room shapes, object placement, collision, etc.\n- Robust evaluation which shows current capabilities of llms for quantitative reasoning in spatial questions.\n- Well organized paper describing the data generation and evaluation process"}, "weaknesses": {"value": "- The benchmark only evaluates the internal quantitative capabilities and doesn't consider tool use or agentic workflows. LLMs are bad at generating high precision quantitative answers such as \"distance between two points\". It would be better to see how well the model can plan to reach the objective, but perform the mathematical calculations either using code or external tools like a calculator for better precision.\n- The authors haven't evaluated any VLLMs with their benchmark. Since this is a spatial reasoning task, instead of only providing the textual coordinate based description of the room and letting the model decipher the plan, maybe providing a rendered image of the room to a VLLM might improve accuracy. This is an interesting case to see how much performance difference can be gained by providing the rendering of the design."}, "questions": {"value": "- Please provide evidence or results for the items mentioned in weakness.\n- In Fig 2 and 3, the right plot containing accuracy by question - which model's accuracy is shown in this plot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YmT16IsYW2", "forum": "HjCEvsXbNV", "replyto": "HjCEvsXbNV", "signatures": ["ICLR.cc/2026/Conference/Submission18124/Reviewer_1riV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18124/Reviewer_1riV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762093590848, "cdate": 1762093590848, "tmdate": 1762927887175, "mdate": 1762927887175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}