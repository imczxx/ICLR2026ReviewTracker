{"id": "1W3r4CVivv", "number": 8679, "cdate": 1758094766696, "mdate": 1763107096593, "content": {"title": "ThinkAfford: Enhancing VLM Reasoning for Affordance Grounding in 3D Scenes", "abstract": "Task-driven affordance grounding in 3D scenes is crucial for embodied AI agents to identify and operate functional interactive elements (e.g., switches, hinges, handles) and thereby accomplish their objectives. However, current approaches have the following limitations: purely 3D point cloud pipelines struggle to generalize across scenes and categories, while 2D-driven methods guided by generic vision–language models often miss small, functionally distinct parts and produce view-dependent, inconsistent results. We introduce ThinkAfford, a coarse-to-fine RGB-D framework for grounding natural-language instructions to fine-grained 3D affordances in cluttered scenes. The coarse stage uses vision-language reasoning to efficiently prune thousands of frames to a compact set of relevant candidate views, leveraging context and relational cues to avoid exhaustive search. The fine stage then focuses on functional parts: it produces affordance-centric proposals that remain stable across viewpoints, and employs an instruction-guided selector fine-tuned with Group Relative Policy Optimization (GRPO) to enhance fine-grained spatial reasoning, by explicitly rewarding choices that satisfy attribute, relational, and geometric constraints. Experiments on SceneFun3D demonstrate state-of-the-art performance, achieving 14.97% AP25 on the test split—a 70.1% relative improvement over the previous SOTA method. Our results show that this structured decomposition, combined with fine-grained spatial reasoning, effectively bridges the gap between high-level language understanding and precise 3D affordance localization. The code will be made available for future exploration.", "tldr": "", "keywords": ["3d Affordance", "VLM", "GRPO", "robotic"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f13163e3d0507d36efb776a91031ea9c2df2359f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a framework ThinkAfford that infers object affordances in complex 3D environments. ThinkAfford is corse-to-fine framework. In the coarse stage, a vision-language model is used to efficiently reduce thousands of frames to a compact set of relevant candidate views by leveraging contextual and relational cues. In the fine stage, the model focuses on functional parts by generating affordance-centric proposals that stay consistent across viewpoints. An instruction-guided selector, fine-tuned with Group Relative Policy Optimization (GRPO), enhances fine-grained spatial reasoning by explicitly rewarding selections that meet attribute, relational, and geometric constraints. Experiments on SceneFun3D show promising results over the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Lifting 2D masks to 3D affordance is interesting, where 2D affordance can be obtained from pretrained VLMs.\n2. Overall, this paper is well organized though some small parts seem unclear.\n3. Experimental results show that ThinkAfford outperforms the baselines significantly in 3D affordance detection tasks."}, "weaknesses": {"value": "1. The assumption that the visibility-aware pixel→point index map and per-affordance 2D labels are available may not hold in real-world, which hinders the applicability of the proposed framework.\n2. The two-stage pipeline, involving view pruning and affordance proposal generation, introduces additional complexity.\n3. The reliance on RGB-D data restricts the model applicability in real-world scenarios where the depth data are not available.\n4. The final performance of 3D strongly relies on 2D mask generation on the first stage. When the candidates are not good, the final performance cannot be promised."}, "questions": {"value": "1.  The final 3D performance appears to heavily rely on the quality of 2D mask generation. Could the authors clarify how accurate and robust the mask candidates are in the first (corse) stage?\n2. It would be helpful to see ablation studies using different vision-language models in the coarse stage, to better assess the contribution and importance of the first stage.\n3. How does the computational overhead of the proposed two-stage framework compare with one-stage methods?\n4. The comparison in Table 1 looks limited. Could the authors provide more comprehensive comparisons with recent affordance grounding approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NrXv2uCF6s", "forum": "1W3r4CVivv", "replyto": "1W3r4CVivv", "signatures": ["ICLR.cc/2026/Conference/Submission8679/Reviewer_owXp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8679/Reviewer_owXp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656283732, "cdate": 1761656283732, "tmdate": 1762920493785, "mdate": 1762920493785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "JE9ynzPEy0", "forum": "1W3r4CVivv", "replyto": "1W3r4CVivv", "signatures": ["ICLR.cc/2026/Conference/Submission8679/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8679/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763107095826, "cdate": 1763107095826, "tmdate": 1763107095826, "mdate": 1763107095826, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a coarse-to-fine framework for grounding natural language instructions to affordances in 3D scenes. The framework first prompts VLM to select relevant views from the original large number of frames. It then generates affordance proposals using a light-weight APG module. Afterwards, it uses a reasoning VLM fine-tuned with GRPO to select the correct affordance for each view. Finally, the same reasoning VLM is used to fuse multiple views. The framework is experimented on SceneFun3D dataset and surpasses the performance of baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of a reasoning VLM for grounding compositional language instructions to fine-grained affordances is an interesting idea, and empirically contributes a lot to the performance.\n\n2. The proposed fine-grained pipeline achieves higher AP25 scores than baselines with a clear margin."}, "weaknesses": {"value": "1. The modular pipeline is a little complicated with many non-trainable hyperparameters. The modular design would accumulate errors from the prior modules to later ones, and cannot be easily corrected from the final results. From ablation studies, we can see that the overall performance is quite sensitive to some hyperparameters, such as voting threshold.\n\n2. The experiments are conducted on only one dataset, which is not thorough enough. It remains unclear whether and to what extent the proposed framework can generalize to other scenes. I think demonstrating the application of the proposed framework to some downstream embodied tasks would largely strengthen the work."}, "questions": {"value": "What is the inference time of the proposed framework? Can you discuss how to apply this method to practical scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lEjdYPQhKJ", "forum": "1W3r4CVivv", "replyto": "1W3r4CVivv", "signatures": ["ICLR.cc/2026/Conference/Submission8679/Reviewer_Zi8Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8679/Reviewer_Zi8Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832906376, "cdate": 1761832906376, "tmdate": 1762920493252, "mdate": 1762920493252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ThinkAfford, a two-stage framework designed to enhance 3D affordance grounding using Vision-Language Models (VLMs) and RGB-D observations. The approach aims to identify actionable object parts in complex 3D environments based on natural language instructions (e.g., “turn on the lamp switch”).\n\nThinkAfford consists of:\n1/ A coarse stage that selects contextually relevant views from multi-view RGB-D data using a VLM-driven relevance scoring mechanism;\n2/ A fine stage where an Affordance Proposal Generator (APG) proposes candidate regions, followed by a Visual-Prompted Affordance Reasoning (VPAR) module that leverages reinforcement learning (Group Relative Policy Optimization, GRPO) to select the most semantically consistent region;\n3/ A 3D reconstruction step that fuses multi-view 2D predictions into a unified 3D affordance mask.\n\nExperiments on SceneFun3D demonstrate improved AP25 and AP50 scores compared to prior works (OpenMask3D-F, Fun3DU), suggesting better alignment between linguistic cues and 3D visual reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1/ The paper is clearly written, well structured, and the proposed pipeline is logically presented with informative figures. The motivation to integrate VLM reasoning with 3D affordance grounding is sound and relevant to embodied AI.\n2/ The use of GRPO for optimizing affordance selection through reward signals is an interesting direction, aligning with recent attempts to train VLMs beyond static instruction tuning."}, "weaknesses": {"value": "1/ While the framework performs well, it largely repackages existing ideas — multi-view selection, mask proposal, and VLM-based scoring — into a sequential pipeline. Each module (e.g., DINOv2 feature extraction, CLIP-based region-text matching, GRPO fine-tuning) relies on well-established techniques. The overall contribution is incremental, not conceptually new.\n2/ The approach primarily projects 2D affordance masks back into 3D rather than reasoning directly within the 3D space. This weakens the claim of “3D affordance reasoning,” as the model’s spatial understanding still stems from 2D feature correlations.\n3/ Much of the performance gain seems to come from the strong underlying VLM (Qwen2.5-VL-7B). The paper does not convincingly show what ThinkAfford adds beyond leveraging existing large models.\n4/ All experiments are confined to a synthetic dataset. There is no demonstration of real-world generalization or deployment in physical scenes, making it unclear whether the system can handle domain shifts or noisy sensory inputs.\n5/ The reward design in GRPO and the role of the “visual prompts” in VPAR are insufficiently studied."}, "questions": {"value": "Please see my weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nEqxmEzK5l", "forum": "1W3r4CVivv", "replyto": "1W3r4CVivv", "signatures": ["ICLR.cc/2026/Conference/Submission8679/Reviewer_29uV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8679/Reviewer_29uV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904234158, "cdate": 1761904234158, "tmdate": 1762920492806, "mdate": 1762920492806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}