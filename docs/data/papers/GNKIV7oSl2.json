{"id": "GNKIV7oSl2", "number": 11495, "cdate": 1758200390828, "mdate": 1759897571902, "content": {"title": "KVCompose: Efficient Structured KV Cache Compression with Composite Tokens", "abstract": "Large language models (LLMs) rely on key-value (KV) caches for efficient autoregressive decoding; however, cache size grows linearly with context length and model depth, becoming a major bottleneck in long-context inference. Prior KV cache compression methods either enforce rigid heuristics, disrupt tensor layouts with per-attention-head variability, or require specialized compute kernels.\n        \nWe propose a simple, yet effective, KV cache compression framework based on attention-guided, layer-adaptive composite tokens. Our method aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the uniform cache structure required by existing inference engines. A global allocation mechanism further adapts retention budgets across layers, assigning more capacity to layers with informative tokens. This approach achieves significant memory reduction while preserving accuracy, consistently outperforming prior structured and semi-structured methods. Crucially, our approach remains fully compatible with standard inference pipelines, offering a practical and scalable solution for efficient long-context LLM deployment.", "tldr": "", "keywords": ["KV-cache", "compression", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a20b70b8aaa84caf9ea5fabd37348ccf51df0c87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents KVCompose, a structured KV-cache compression method for LLM inference. The method first aggregates attention across tasks or contexts to score token importance and then creates composite tokens by allowing each KV head to select its own most important original positions, aligning those selections into a shared per-layer sequence dimension, which preserves the standard dense tensor format. A layer-adaptive global budget distributes retention across layers by ranking composite-token scores. Experiments on RULER-4096 with LLaMA-3.1-8B, Qwen2.5-7B-Instruct, and Qwen3-14B show higher AUC than other baselines such as TOVA, SnapKV, and PyramidKV."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* KVCompose is evaluated across various model families and model sizes.\n* Ablation studies examine key design choices, such as operator selections for aggregation, Aggtask, Agggroup, and Agghead. These analyses help clarify the contribution of each component."}, "weaknesses": {"value": "* No runtime latency and memory usage comparison. Reporting compression ratios alone may not fully reflect efficiency improvements. Latency and memory statistics would provide a clearer picture of the practical benefits of KVCompose.\n\n* The range of benchmarks is somewhat limited. KVCompose has been evaluated only on RULER-4096. Evaluation with longer input lengths on RULER and on other widely used benchmarks, such as LongBench, would provide a more comprehensive assessment of KVCompose's performance.\n\n* What is the computational cost of the token selection process? For example, it would be helpful to discuss whether KVCompose remains compatible with Flash-Attention, given that attention computation is explicitly involved. It may be useful to compare or discuss KVCompose in relation to recent structured eviction approaches, such as “LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models” (ICML'25), particularly under accuracy-latency trade-offs."}, "questions": {"value": "Please refer to the points raised in the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UeFdLDtlvb", "forum": "GNKIV7oSl2", "replyto": "GNKIV7oSl2", "signatures": ["ICLR.cc/2026/Conference/Submission11495/Reviewer_82QU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11495/Reviewer_82QU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855726354, "cdate": 1761855726354, "tmdate": 1762922597508, "mdate": 1762922597508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces KVCompose. The core innovation is the concept of \"composite tokens.\" Recognizing that while the number of tokens must be uniform across heads in a layer for tensor compatibility, the identity of those tokens need not be, KVCompose allows each head to independently select its most important tokens based on aggregated attention scores.  This aims to combine the flexibility of per-head eviction (unstructured) with the hardware efficiency of dense tensor layouts (structured)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The concept of \"composite tokens\" is an interesting attempt to bridge the gap between unstructured eviction flexibility and the hardware efficiency required by standard inference engines."}, "weaknesses": {"value": "**Q1** The central claim is \"fully compatible with standard inference pipelines\" and \"Works without engine changes\". However, . Modern LLMs, including Llama and Qwen (used in the experiments), rely heavily on positional embeddings, often Rotary Positional Embeddings (RoPE). RoPE applies rotations based on the absolute position of tokens. KVCompose's core mechanism constructs composite tokens by aligning K/V entries from different original positions across different heads. For example, the k-th composite token might contain the K/V from position 50 for Head 1 and position 1000 for Head 2. Standard inference engines assume that all K/V entries at tensor index k share the same original position, applying a uniform positional embedding for that index. KVCompose does not follow this assumption. \n\n**Q2** The claim that KVCompose is \"Streaming ready\" (Table 1) is misleading. The proposed scoring mechanism (Section 3.2) and global budget allocation (Eq. 22, 23) require processing the full context N to calculate attention statistics and determine budgets. This makes the approach suitable only for fixed-context compression (e.g., prompt compression during prefill) but not for true unbounded streaming applications (like StreamingLLM), which must make online eviction decisions as tokens arrive.\n\n**Q3** I think the reorganization of the cache into composite tokens breaks the semantic and positional continuity of the cache structure. This fundamentally complicates or prevents the application of standard system optimizations crucial for efficient serving, such as prefix caching, cache sharing across requests, and speculative decoding. \n\n\n**Q4** The evaluation is severely limited in context length, failing to validate the method's effectiveness for long-context inference. Experiments are solely conducted on Ruler-4096. This is insufficient, as KV bottlenecks become critical at much longer sequences. The method must be evaluated on significantly longer contexts (e.g., 32k, 128k+) with limited token budgets 256-512.\n\n**Q5** The evaluation lacks essential metrics and task diversity. Performance on tasks like long-document summarization or complex QA (e.g., LongBench2). The impact of eviction on coherence is critical in multi-turn scenarios, requiring evaluation on benchmarks such as SCBench.\n\n**Q6.** The experimental evaluation is weak as they are ignoring recent work such as Lcache, RocketKV, ShadowKV, and seerAttention.\nAt least you should compare with RocketKV and ShadowKV in different token budgets and show how far you are from them.\n\n**Q7.** The paper claims efficiency but provides no empirical analysis of the computational overhead. KVCompose introduces significant computation during the prefill phase: attention aggregation, sorting scores for every head (O(L * H_kv * N log N)), and indexed gathers to reorganize the cache. The latency and throughput impact of these operations must be measured and reported. \nQ8 The authors incorrectly state that the \"memory required to store KV pairs\" grows quadratically with context size. The KV cache memory footprint grows linearly (O(N)), as correctly noted later. The computational cost of prefill attention is quadratic (O(N^2)).\nQ9. The memory calculation assumes FP32 (4 bytes). Modern deployments predominantly use FP16/BF16 (2 bytes); the calculation should reflect modern precision standards.\n\n**Q10.** Minor: Conclusion and abstract should be in one paragraph."}, "questions": {"value": "Please look at the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JSXI2DGXY7", "forum": "GNKIV7oSl2", "replyto": "GNKIV7oSl2", "signatures": ["ICLR.cc/2026/Conference/Submission11495/Reviewer_ZsMN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11495/Reviewer_ZsMN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950851606, "cdate": 1761950851606, "tmdate": 1762922596938, "mdate": 1762922596938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KVCompose, a structured KV cache compression method. It uses attention-guided scoring to identify important tokens , allows each head to select different tokens while maintaining a uniform tensor structure (termed \"composite tokens\") , and adaptively allocates retention budgets across layers. The method is designed for compatibility with standard inference engines and demonstrates strong performance on the Ruler-4096 benchmark against other structured methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method intelligently allocates a global retention budget across layers based on token importance scores, rather than using fixed heuristics.\n\n- Consistent performance gain is observed across the board."}, "weaknesses": {"value": "- The method requires to collect attention scores, adding complexity. \n\n- Experiments mostly conducted on relatively small models. Would suggest the author conduct experiments on larger models and more challenging problems, like Qwen3-thinking and AIME."}, "questions": {"value": "1. Can you clarify on the concept of \"composite tokens\"? What's its difference from conventional kv cache? What benefits it brings by viewing kv cache from different layer as a token? \n\n2. The idea for consturcting adaptive kv cache sounds very similar to \"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "slr7s9iteY", "forum": "GNKIV7oSl2", "replyto": "GNKIV7oSl2", "signatures": ["ICLR.cc/2026/Conference/Submission11495/Reviewer_Gk8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11495/Reviewer_Gk8h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970349157, "cdate": 1761970349157, "tmdate": 1762922596532, "mdate": 1762922596532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes KVCompose, a structured KV cache compression method for long-context LLM inference, by pre-collecting attention distributions, calculating token importance, constructing \"composite tokens,\" and using global sorting to achieve layer-adaptive retention budget allocation, while maintaining each layer's KV cache still as a uniform length, dense tensor. Token selection is fine-grained at the head dimension, yet implemented as structured eviction.\n\nOn long-context benchmarks such as Ruler-4096 and kvpress, compared with methods like TOVA, StreamingLLM, SnapKV, PyramidKV, and DuoAttention, KVCompose can achieve higher compression rates at the same accuracy tolerance and significantly outperforms in the AUC metric."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Structured and engineering-friendly\n\nThe design of composite tokens is natural and inspiring\n\nGlobal layer-adaptive budget allocation\n\nThe method is conceptually clear and formally complete"}, "weaknesses": {"value": "The evaluation benchmarks are narrow and lack real-world tasks and diverse model settings. Currently, the main evaluations focus on Ruler-4096 / kvpress synthetic long-context tasks. These tasks are sensitive to patterns like needle in a haystack / aggregation, but they are insufficient to demonstrate generalizability to real multi-turn dialogues, code, long-form summarization, and RAG scenarios and robustness across more architectures.\n\nAttention = Importance assumption lacks deeper analysis. The method is entirely based on aggregated attention scores, but related literature has pointed out that attention does not always reliably characterize causal contributions. The paper performs an ablation study on pooling methods, but it lacks comparison with gradient/importance estimation, also lacks discussion of failure cases or scenarios where attention is unreliable. Currently, the impression is that it works empirically on this benchmark, but the theoretical and empirical analysis is somewhat shallow.\n\nSemantic and Interpretability Issues of Composite Tokens. Composite tokens combine different original positions from different heads into the same new position, essentially altering the alignment of each head. Although this does not mathematically violate the attention formula, it can make position-wise distribution interpretation difficult. Also, the behavior under positional encoding or certain structured prompt patterns opaque.\n\nThe evidence for being practically deployable and user-friendly is insufficient.  \nThe article repeatedly emphasizes that there is no need to customize the kernel and that it can directly integrate with vLLM / HF, which is a selling point; however, it lacks actual end-to-end throughput and latency data;  Memory vs QPS curves under real serving settings;  \nSystem-level comparison with existing simple sliding window.\n\nThe method relies on a preconstructed task set T to collect attention. If the downstream task distribution changes, do we need to recollect attention and regenerate the mapping? For closed-source or commercial models, how can we efficiently collect enough attention to create a static configuration?  \n\nCurrently, it is more of a conceptual implementation compatible claim. We would like to see solid metrics supporting the conclusion of being practical & scalable."}, "questions": {"value": "It is recommended to briefly explain in the main text the offline overhead involved in generating compressed configurations.\n\nYou can add a diagram for comparison traditional structured eviction, this allows readers to immediately see the structural differences. \n\nIn the comparison methods, some, such as SnapKV / KVzip, are reconstruction-based / unstructured, which differs slightly from the goal of this work, which is to only perform structured eviction. \n\nIt is recommended to release the implementation to lower the barrier for reproduction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7bLa08IQs7", "forum": "GNKIV7oSl2", "replyto": "GNKIV7oSl2", "signatures": ["ICLR.cc/2026/Conference/Submission11495/Reviewer_A8M8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11495/Reviewer_A8M8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762836591396, "cdate": 1762836591396, "tmdate": 1762922596085, "mdate": 1762922596085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}