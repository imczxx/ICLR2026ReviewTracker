{"id": "zjaV5zmlkl", "number": 11530, "cdate": 1758200978970, "mdate": 1759897569845, "content": {"title": "Towards True Speech-to-Speech Models Without Text Guidance", "abstract": "Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction. We will release our code and models to support further research in true speech-to-speech foundation models.", "tldr": "We present a true speech-to-speech LLM that understands and generates speech directly, without text intermediates, achieving state-of-the-art spoken QA.", "keywords": ["multimodal large language model", "large language model", "speech language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0f9cdbf1ad6f3835cc38b5f5593462f39eb592c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a speech-to-speech large language model that directly processes and generates speech without relying on intermediate text representations. The key contributions are: (1) a modality-based layer-splitting architecture that separates text and speech generation paths in the final transformer layers, (2) a frozen pre-training strategy that preserves text capabilities while adding speech modality, and (3) demonstration of competitive performance on speech-to-speech benchmarks while maintaining text performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Architecture Design: The modality-based layer splitting is well-motivated by the empirical analysis in Figure 2, showing that speech-text representations fuse in lower/middle layers but diverge in final layers. This observation drives a principled architectural choice rather than an arbitrary design decision.\n2. Comprehensive Evaluation: The paper evaluates multiple dimensions - speech modeling, text modeling preservation, speech quality (UTMOS), and spoken QA across multiple languages.\n3. Thoughtful Training Strategy: The two-stage frozen pre-training approach with progressive unfreezing is well-designed and the ablation study (Table 6) validates its effectiveness.\n4. Open sourcing code and models"}, "weaknesses": {"value": "1.  The streaming-capable tokenizer with ASR-based encoder training shows good performance, though at slightly higher WER than non-streaming baselines. However, there is no discussion of how fast the streaming codec is. Some discussion on RTF of different models is definitely needed.\n2. Incomplete Comparison: Include stronger speech-in speech-out baselines like Qwen2.5-OMNI and Kimi Audio. Also, more discussion about comparison with closed API models like GEMINI or GPT-4o audio preview to understand performance gap.\n3. Text Performance Degradation: Compared to Qwen3-8B used as this model's LLM backbone, MMLU drops from 76.60% → 67.19% (12% relative decline), CMMLU drops from 77.35% → 69.53% (10% relative decline), this is still substantial degradation. The paper doesn't adequately discuss whether this trade-off is acceptable or how to further mitigate it.\n4. Incomplete Comparison with Text-Guided Generation: Table 5 shows mixed results: the model underperforms GLM-4-Voice* (text-guided) on TriviaQA and WebQA S→S tasks. Missing comparison: What if you added text guidance to YOUR model? This would isolate whether architectural improvements matter beyond the guidance mechanism.\n5. Incomplete Evaluation of All Claims: While the evaluation covers reasoning and QA well, some claims about expressivity and paralinguistic capabilities are not deeply evaluated. Some examples or analysis showing the model can generate non-verbal vocalizations (laughter, hesitation) that text cannot represent would strengthen this work.\n6. Limited Scope of Evaluation: No evaluation with real human speech. No evaluation on multi-turn dialogue scenarios, or interactive human in the loop conversations\n7. Ablation Study Limitations: It would be interesting to ablate the layer split position (currently at layer 32 of 36)?"}, "questions": {"value": "check weakness, particularly if you can provide some discussion on Weakness 1, 2, 3 and 4."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "Could you clarify the collection process of the ~4 million hours of speech data used for pre-training? Was any of it proprietary or sensitive content, and how did you ensure compliance with privacy or copyright considerations? A discussion on how the dataset was collected and filtered in terms of ethics would be helpful."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ntrVYDc3II", "forum": "zjaV5zmlkl", "replyto": "zjaV5zmlkl", "signatures": ["ICLR.cc/2026/Conference/Submission11530/Reviewer_XJ6K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11530/Reviewer_XJ6K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450306546, "cdate": 1761450306546, "tmdate": 1762922625092, "mdate": 1762922625092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a speech to speech model that can directly understands and generates speech without using text. The proposed approach use modality-based layer-splitting architecture and freeze some of the parameters which preserve the reasoning and the world knowledge of the LLM. \n\nExperiments on the proposed model show that it gets state of the art results on spoken question answering with the comperable performance to text-guided algorithms."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed architecture is elegant with shared lower layers and speech and text specialized upper layers, which balancing between the speech generation and the text capabilities of the LLM\n\n2. The proposed ides of frozen pertaining and unfreezing upper layers help with the problem of catastrophic forgetting\n\n3. The proposed architecture which use streaming encoder and a flow-matching decoder give good latency for interactive usage \n\n4. The evaluation of the proposed model is well designed - all IO directions S2S, S2T, T2S, T2T"}, "weaknesses": {"value": "1. The training of the proposed model use TTS generated speech and ASR data with can give compound error to the model\n\n2. The metrics of UTMOS and WER is not enough to evaluate prosody such as hesitations or laughter.\n\n3. Using single codebook may limit the results of the proposed model using richer multi codebook may improve the results"}, "questions": {"value": "1. How the results change if you use real audio for training ? Or can you tell something about the results from using the TTS/ASR generated data?\n\n2. What happen if you use LoRA instead of the proposed approach? \n\n3. How the model behave with background noise and non-verbal sounds ?\n\n4. Regarding the latency - How does the quality degradate when using more compute/layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FqQxdk0CZJ", "forum": "zjaV5zmlkl", "replyto": "zjaV5zmlkl", "signatures": ["ICLR.cc/2026/Conference/Submission11530/Reviewer_24Lx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11530/Reviewer_24Lx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744883041, "cdate": 1761744883041, "tmdate": 1762922624587, "mdate": 1762922624587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study presents a high-quality speech–text multimodal large language model (LLM) developed through techniques such as layer-wise analysis and unfreezing strategies aimed at improving speech–text interleaving and information sharing. The paper demonstrates the model’s performance across various benchmarks, including continuation and conversation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Basing the approach on branching to differentiate the model’s behavior and architecture is novel.\n- The effectiveness of the proposed method is demonstrated through multiple experiments."}, "weaknesses": {"value": "Rather than listing separate weaknesses, I noted a few brief questions in the questions section."}, "questions": {"value": "- It would be helpful to analyze whether the layer-based behavioral patterns are commonly observed in other SLMs as well. \n\n- As for CosyVoice 2, if my memory is correct: (1) the decoder’s chunk size is quite large; (2) there is lookahead; and (3) it requires reference speech at inference. For MOSS-Speech-Codec, I’m curious about the flow-matching chunk size, whether lookahead is used, whether reference speech is required, and the vocoder’s chunk size (or whether it is fully causal).\n\n- The layers were split 32/4, but this analysis would likely vary depending on the criteria, the size of the model examined, and other variables. I’m also curious about what changes are observed (or expected) when adjusting that split point and experimenting with different branch locations.\n\n- You mentioned that the encoder supports full-streaming, but as far as I remember, both glm-4-voice and WhisperFeatureExtractor used a lookahead mechanism and also processed a full 30-second segment for computation. How did you handle this part?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G1dgHB2JJ1", "forum": "zjaV5zmlkl", "replyto": "zjaV5zmlkl", "signatures": ["ICLR.cc/2026/Conference/Submission11530/Reviewer_eUke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11530/Reviewer_eUke"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799360413, "cdate": 1761799360413, "tmdate": 1762922624180, "mdate": 1762922624180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of building a speech-to-speech model (speechLM). \nLike most previous works, it uses a single-codebook speech tokenizer (finetunes the GLM tokenizer), and finetunes a causal language model with an expanded vocabulary for speech tokens on text and audio data (paired, interleaved and unsupervised).\nTheir main contribution claims two: (1) layer splitting - \"duplicating\" the K deepest layers of the LLM that would specialize on producing speech tokens. (2) two stage training: first only trains randomly initilized parameters, where the second traines all parameters.\n\nIm my view, both ideas have already been proposed and investigated. Having speech/text prediction heads for example has been done in SpeechT5 (speech/text post net), lately with HiggsAudio (Audio adapter).\nWhat they denote as frozen pretraining is also a common practice, e.g. projector training in speech/vision-aware LLMs. It ensures your backbone won't get noisy gradients from the randomly initilized modules.\nAn interesting direction this paper explored is how to select the layer to start layer splitting, they do so by investigating an existing speechLM, and visulizing the similarity between the text embeddings, and the audio embeddings of the matching audio. They see the highest similarity within the ~80% of the depth, which they use to perform splitting in that point."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "comprehensive model training, with large-scale data collection. \nInteresting experimental results ablation on model pretraining (no new conclusions, but it validates the known approach of \"frozen pretraining\")\nInteresting analysis on speech/text embedding similarity in speechLMs across layers (but only done on one model)"}, "weaknesses": {"value": "- Most claimed novelties are not new ideas. A solid tech report, but not a major contribution.\n\n- Layer splitting also increases the parameter count, hard to say if the improvements comes from the increase in model size of not.\n- We're missing an ablation on what happens when you split in different locations. e.g. split on K first/middle/last layers - same parameter count but different locations. \n- The layer similarity experiment is best done on multiple speech-text LLMs (e.g. GLM?)"}, "questions": {"value": "You claim that there’s a text “bottleneck” in text-guided speechLMs. But the audio is still within the context in those models, and the non-verbal cues can be utilized implicitly. Do you have an indication that the input audio tokens are not used in text-guided speechLMs?\nLooks like there are high-similarity results on Figure 4 (appendix) layer 26. Any explanation on why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lH0IWKUDyg", "forum": "zjaV5zmlkl", "replyto": "zjaV5zmlkl", "signatures": ["ICLR.cc/2026/Conference/Submission11530/Reviewer_bYkG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11530/Reviewer_bYkG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071725663, "cdate": 1762071725663, "tmdate": 1762922623727, "mdate": 1762922623727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}