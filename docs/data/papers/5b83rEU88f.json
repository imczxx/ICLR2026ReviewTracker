{"id": "5b83rEU88f", "number": 19657, "cdate": 1758298035049, "mdate": 1759897027696, "content": {"title": "Optimal Policy Minimum Bayesian Risk", "abstract": "Inference scaling helps LLMs solve complex reasoning problems through extended runtime computation. On top of long chain-of-thought (long-CoT) models, purely inference-time techniques such as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes risk decoding (MBRD), can further improve LLM accuracy by generating multiple candidate solutions and aggregating over them. These methods typically leverage additional signals in the form of reward models and risk/similarity functions that compare generated samples, e.g., exact match in some normalized space or standard similarity metrics such as Rouge. Here we present a novel method for incorporating reward and risk/similarity signals into MBRD. Based on the concept of optimal policy in KL-controlled reinforcement learning, our framework provides a simple and well-defined mechanism for leveraging such signals, offering several advantages over traditional inference-time methods: higher robustness, improved accuracy, and well-understood asymptotic behavior. In addition, it allows for the development of a sample-efficient variant of MBRD that can adjust the number of samples to generate according to the difficulty of the problem, without relying on majority vote counts. We empirically demonstrate the advantages of our approach on math (MATH-$500$) and coding (HumanEval) tasks using recent open-source models. We also present a comprehensive analysis of its accuracy-compute trade-offs.", "tldr": "A new method to integrate a reward model, a risk/similarity function and a generator for inference scaling. The method is based on the optimal policy and admits an efficient version.", "keywords": ["Inference Scaling", "Best-of-N", "Minimum Bayesian Risk", "Optimal Policy"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3665ed5be531eeb9b4b3150588ae4cdee468c53d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Optimal Policy Minimum Bayesian Risk Decoding , a new inference-time decoding algorithm for large language models. Building on Minimum Bayes Risk Decoding (MBRD), OP-MBRD integrate reward models and similarity measures through a KL-controlled reinforcement learning framework. This method aim to enhance robustness, accuracy, and efficiency in reasoning and coding tasks. Experiments on MATH-500 and HumanEval datasets demonstrate that OP-MBRD often matches or outperforms existing methods like Best-of-N sampling and MBRD with rewards while maintaining computational efficiency and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The integration of MBRD with a KL-regularized optimal policy formulation is elegant and well-grounded in reinforcement learning theory, providing clear asymptotic guarantees.\n- OP-MBRD consistently achieve strong or superior performance across various LLM–PRM pairs, particularly with sample-efficient variants that adapt sampling to task difficulty.\n- The method remains simple to implement, requiring minimal additional parameters, and is compatible with existing reward models, making it widely applicable to different LLMs and domains."}, "weaknesses": {"value": "- Overall, this work is good in theoretical parts. Yet, the study primarily tests small to mid-sized open-source models and focuses on math/coding tasks. This limits the generalizability to larger or multimodal ones like benchmark on Geometry3k.\n- The effectiveness of OPE-MBRD depends heavily on the calibration quality between the generator and the reward model. Poorly calibrated pairs, such as Phi-4 with Phi-4-PRM, yield weaker gains."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RENIQsl2RW", "forum": "5b83rEU88f", "replyto": "5b83rEU88f", "signatures": ["ICLR.cc/2026/Conference/Submission19657/Reviewer_QbyP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19657/Reviewer_QbyP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667087061, "cdate": 1761667087061, "tmdate": 1762931508030, "mdate": 1762931508030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new test-time decoding strategy based on minimum Bayesian risk decoding (MBRD) and evaluates it on MATH500 and HumanEval."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "The contributions and empirical results are underwhelming. Conceptually, the proposed OP-MBRD applies the weight $\\frac{p_R}{p}\\exp(R/\\beta)$ (close to the form of the KL-regularized optimal policy) multiplied by a comparison function M. In the MATH500 experiments, M is a delta function and $p_R=p$, and Eq. 9–11 seems to imply that the strategy is simply maximizing $R(x,y)/\\beta + \\log(\\text{empirical frequency of the answer part of} y)$. It is unclear how this meaningfully differs from BoN. It is also possible that Eq. 9-11 are not presented properly (the presentation of Section 4 is not very clear in my opinion).\n\nThe experimental comparison among BoN, MBRD-EM (majority voting), MBRD-EM*R (majority voting with PRM), and OP-MBRD-EM (proposed) is weak, and it is only performed on MATH500 and HumanEval. There is no analysis of why OP-MBRD-EM can outperform BoN; any gains may reflect the adaptivity of OP-MBRD or errors in the reward model R."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xZkJ3CcXAn", "forum": "5b83rEU88f", "replyto": "5b83rEU88f", "signatures": ["ICLR.cc/2026/Conference/Submission19657/Reviewer_ys5d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19657/Reviewer_ys5d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767809394, "cdate": 1761767809394, "tmdate": 1762931507638, "mdate": 1762931507638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method called Optimal Policy Minimum Bayesian Risk Decoding (OP-MBRD), which builds upon minimum Bayes risk decoding (MBRD) to enhance the inference scaling of large language models (LLMs). Unlike MBRD, which relies on majority vote counting, OP-MBRD incorporates reward and risk/similarity signals into the MBRD framework. It retains much of the simplicity of both the Best of N (BoN) and MBRD methods while introducing only a single new parameter and remaining compatible with general MBRD. Furthermore, the authors demonstrate the efficiency of their Rao-Blackwellized rejection sampling method and provide formal guarantees for it. Empirically, they compare their proposed algorithm with two baselines: BoN and MBRD, using the Math500 dataset and show that their proposed algorithm performs better."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem the authors are attempting to address is very important.  \n- The idea of improving MBRD with Rao-Blackwellized rejection sampling is very interesting.  \n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- Although incorporating the closed form of the optimal policy into MBRD is novel, there has been other research that has utilized the tilted distribution as a way to improve policy performance during test time. The authors did not compare their work to these existing studies.  \n- The paper does not cite relevant work regarding the derivations in certain sections. For example, when discussing Maximum Entropy (MaxEnt), it would be appropriate to cite Ziebart, B. D. for \"Maximum Entropy Inverse Reinforcement Learning\" or Grünwald, P. D. for \"Maximum Entropy, Minimum Discrepancy, and Robust Bayesian Decision Theory.\"  \n- Basic naive baselines, such as simple importance sampling, are missing as comparisons to the proposed method when attempting to tilt the distribution. \n- As the average number of samples per input increases, the difference between baselines, such as BoN and the proposed methods, decreases. However, we would expect the opposite effect."}, "questions": {"value": "- How does basic importance sampling using the closed form of the optimal compare with the proposed method empirically? Essentially, performing importance sampling as \\(\\pi_t \\exp(Q) / \\pi_t = \\exp(Q)\\) allows for the application of Self-Normalized Importance Sampling (SNIS) or Clipped Importance Sampling (IS).\n- The experiments do not compare against a weight-majority vote, which is a strong baseline for inference time scaling.\n- The empirical results show pass@1, but pass@k—where k is greater than 1—is extremely important to demonstrate in order to understand the performance of the policy. The plots in the paper may be showing this, but it is unclear what 'pass@1' on the y-axis means when the average samples per input increase. \n- The paper is missing important citations, such as:\n  - \"Controlled Decoding from Language Models\" by Mudgal et al.\n  - \"Value-Guided Search for Efficient Chain-of-Thought Reasoning\" by Wang et al.\n  - \"Value-Augmented Sampling for Language Model Alignment and Personalization\" by Han et al.\n  - \"Inference-Time Language Model Alignment via Integrated Value Guidance\" by Qiao et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DYleZGB9tP", "forum": "5b83rEU88f", "replyto": "5b83rEU88f", "signatures": ["ICLR.cc/2026/Conference/Submission19657/Reviewer_cnDM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19657/Reviewer_cnDM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762324293949, "cdate": 1762324293949, "tmdate": 1762931507195, "mdate": 1762931507195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to integrate reward models into the sampling process of an LLM. The authors assume a reference function as well as a reward function, and use a commonly used KL-regularized objective to arrive at an algorithm for picking the best response from the LLM. The proposed method uses a self-normalized importance sampling to find the best candidate. The authors show favorable properties of this method such as consistency. Furthermore, they show competitive experimental performance with voting verifiers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The background section up to page 4 is well written.\n* The method has theoretical derivation.\n* The experimental results show competitive performance with voting verifiers."}, "weaknesses": {"value": "* First, the reference model is discussed in all the sections, but it is set to the original model in the experiments. This makes most of the fractions in the reward model equal to 0, and simplifies most of the equations. So, the existence of a reference model is not really justified in the derivations. Also, in practice, it is not really common-practice to set it to a stronger teacher model, so in practice we also do not have a different reference model.\n* The paper is only not self-contained, and references to many papers for derivations. For instance, it is not clear from the cited reference how Eq (9) is derived, given the notation difference, and extra factors (e.g,. should y' in the summation have constraints?)\n* The writing in section 4.3 and its subsequent experimental results are not easy to follow, this section needs more elaboration. \n* The method depends on a hyperparameter $\\beta$, and rarely outperforms the voting verifiers (which does not require any hyperparameter)."}, "questions": {"value": "* The paper seems to be generalizable to any reward model, why is process reward model chosen?\n* In line 248, is the maximum over each step, or over each \"sequence\"?)\n* I would suggest the authors give explicit examples of what x, y, y' is in a given context (such as math examples and final answers), so it would be more accessible to the general audience.\n* How sensitive is the overall algorithm to the hyperparameter $\\beta$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JsxzlpHElY", "forum": "5b83rEU88f", "replyto": "5b83rEU88f", "signatures": ["ICLR.cc/2026/Conference/Submission19657/Reviewer_Tk67"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19657/Reviewer_Tk67"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762559143536, "cdate": 1762559143536, "tmdate": 1762931506896, "mdate": 1762931506896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}