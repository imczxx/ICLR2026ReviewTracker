{"id": "7wjFjOzCtB", "number": 9802, "cdate": 1758141334273, "mdate": 1759897694775, "content": {"title": "No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks", "abstract": "The memorization of training data by neural networks raises pressing concerns for privacy and security. Recent work has shown that, under certain conditions, portions of the training set can be reconstructed directly from model parameters. Some of these methods exploit implicit bias toward margin maximization, suggesting that properties often regarded as beneficial for generalization may actually compromise privacy. Yet despite striking empirical demonstrations, the reliability of these attacks remains poorly understood and lacks a solid theoretical foundation. In this work, we take a complementary perspective: rather than designing stronger attacks, we analyze the inherent weaknesses and limitations of existing reconstruction methods and identify conditions under which they fail. We rigorously prove that, without incorporating prior knowledge about the data, there exist infinitely many alternative solutions that may lie arbitrarily far from the true training set, rendering reconstruction fundamentally unreliable. Empirically, we further demonstrate that exact duplication of training examples occurs only by chance. Our results refine the theoretical understanding of when training set leakage is possible and offer new insights into mitigating reconstruction attacks. Remarkably, we demonstrate that networks trained more extensively, and therefore satisfying implicit bias conditions more strongly -- are, in fact, less susceptible to reconstruction attacks, reconciling privacy with the need for strong generalization in this setting.", "tldr": "", "keywords": ["Deep learning theory", "Implicit bias", "Training reconstruction attack", "Data privacy", "Data protection", "Deep learning security"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb25e761e48df156ed0580a163024e579ffe83d2.pdf", "supplementary_material": "/attachment/95f67b737534803cbb4ecb2f46a065863062f283.zip"}, "replies": [{"content": {"summary": {"value": "This paper revisits reconstruction attacks based on implicit bias (e.g., KKT-condition–based attacks) and provides a rigorous theoretical analysis of their limitations. The authors show that without prior knowledge about the data distribution, such reconstruction objectives admit infinitely many global optima that are arbitrarily far from the true training samples. The paper also introduces the notion of a “secret bias,” illustrating that hidden transformations or priors can effectively protect data from being reconstructed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Solid theoretical foundations.**  \n  The mathematical analysis is thorough and clearly presented. The authors formalize the implicit assumptions in prior KKT-based attacks and prove that, without priors, the reconstruction loss function has infinitely many degenerate global minima. This provides a precise theoretical characterization of why such attacks cannot succeed “from scratch.”\n\n- **Novel perspective on priors and implicit bias.**  \n  Instead of proposing yet another reconstruction algorithm, the paper takes a principled view, showing that the presence (or absence) of prior knowledge fundamentally determines whether data can be reconstructed. This shifts the discussion of privacy leakage from algorithmic ingenuity to epistemic limitations."}, "weaknesses": {"value": "1. **Limited empirical validation.**  \n   The experimental results are somewhat narrow, being limited to a shallow 3-layer network on the CIFAR dataset. To strengthen the empirical evidence, it would be helpful to include additional datasets (e.g., MNIST, CelebA, ImageNet) and deeper or modern architectures (e.g., ResNet, ViT) to confirm that the theoretical claims hold in more realistic settings.\n\n2. **Lack of quantitative evaluation of prior strength.**  \n   The paper emphasizes that both the “secret bias” and the attacker’s prior knowledge are key to reconstruction success, but it does not measure how the degree or accuracy of prior information affects attack outcomes.  \n   A systematic experiment varying the strength of prior knowledge (e.g., partial mean shift known, partial pixel-range constraint, etc.) and plotting reconstruction quality or KKT loss would make the argument more convincing."}, "questions": {"value": "1. Can the theoretical framework extend beyond classification to regression or generative models (e.g., diffusion models)?  \n2. Could the “secret bias” defense be generalized or implemented in practical training pipelines without harming model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0O9MZdWCBe", "forum": "7wjFjOzCtB", "replyto": "7wjFjOzCtB", "signatures": ["ICLR.cc/2026/Conference/Submission9802/Reviewer_1FR8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9802/Reviewer_1FR8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726481608, "cdate": 1761726481608, "tmdate": 1762921289612, "mdate": 1762921289612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper studies the attack presented in [1], specifically focused on the importance of the prior knowledge loss term in the reconstruction loss\n-This loss encodes information about the pixel min/max values, and constraints the reconstructed images to be within this valid set\n- Theoretically, they show that through two operations: splitting and merging, it is possible to construct sets which satisfy the reconstruction loss exactly (excluding the prior term), but are not the original dataset\n- They extend this analysis to cases where the reconstruction loss is only fit within a error term\n- Empirically they show that when the image initialization is mismatched with the statistics of the actual images, reconstruction quality suffers dramatically\n\n\n\n\n\n[1] Reconstructing training data from trained neural networks"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Presentation is solid, idea is well motivated\n- Provides some theoretical understanding into these reconstruction losses, which generally speaking, lags far behind the empirical studies of them"}, "weaknesses": {"value": "- Lemma 2 and 3 require that the split/merged datapoints match the activation patterns of the original datapoints. It is unclear how much of a restriction this is. Can the authors empirically demonstrate that such operations are possible (on slightly less toy data than the 2d example presented). While it seems interesting, I am unconvinced that it is possible, for example, to split a datapoint with any reasonably large \\alpha, \\beta splitting coefficients, so that the split datapoints are actually \"far\" from the original. While the authors claim that reconstructed can be \"arbitrarily far\" from the original dataset, it seems this activation pattern constraint could actually restrict it substantially.\n\n- Empirical evidence is somewhat unconvincing. From experience, the attack presented in [1] generally is very brittle and hard to get working. Indeed, this paper aims to study why this might be the case. The authors present empirical evidence on CIFAR-10, where the attack already doesn't work very well. It would be additionally be useful if the authors present results on MNIST (or a simpler dataset), where the attack works better, so that there can be a greater difference between the version of the attack without prior knowledge/with prior knowledge.\n\n\n[1] Reconstructing training data from trained neural networks"}, "questions": {"value": "- The authors report that all runs achieve a similar KKT loss of 330-332. If I remember correctly, the KKT loss of this attack does not actually decrease significantly over the course of training, so this gap of 330/332 may actual be significant. I would suggest setting up this experiment differently. Instead, would it be possible to terminate all runs when they achieve the same KKT loss exactly? I.e. do early stopping on all runs when they hit loss = 333.0 exactly, for example, then compare quality. This would prevent the issue where the remaining 0-2 loss points carry the majority of image quality.\n\n- Empirically, what KKT loss does the original dataset achieve? I.e. if we initialize the reconstruction images with the original dataset, and compute the loss, what is the result? Is it below 330-332?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tyOSzztTOm", "forum": "7wjFjOzCtB", "replyto": "7wjFjOzCtB", "signatures": ["ICLR.cc/2026/Conference/Submission9802/Reviewer_2UcW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9802/Reviewer_2UcW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872333462, "cdate": 1761872333462, "tmdate": 1762969170223, "mdate": 1762969170223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides theoretical analysis of the KKT-based data reconstruction objective in the seminal work Haim et. al., and showed that perhaps surprisingly, if one throw away the regularization term based on prior information in the reconstruction objective, then one can construct new data records that are global optima for the reconstruction objective, yet are infinitely far away from the training dataset in Euclidean distance. Their construction is simple yet intuitive, linear combinations of records on the margin that have the same label and same activation pattern would still lie in the margin, and moving training data records along the margin would not break the KKT condition. They then prove that when the training data does not span the full space, one can construct data records with the same label and same activation pattern of a training data that lie arbitrarily far in Euclidean distance. The assumption could be further relaxed to training data that only approximately falls into a low-dimensional space, under which the maximum feasible perturbation scale (without breaking KKT) is inverse proportional to the smallest singular value of the data matrix S. They then extend their analysis to the approximate training case, where the trained model may be $\\delta$ away from the KKT condition, and prove that interestingly, if the adversary does not know $\\delta$, similar splitting constructions still hold; but if the adversary knows $\\delta$, then the feasible perturbation scale of the splitting construction significantly decreases. Experiments for ReLU networks on synthetically constructed classification dataset, as well as shifted CIFAR10 dataset, confirm that the reconstructed image's similarity to true training data degrades with increasing shift (corresponding to weaker prior knowledge)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, and performs thorough theoretical analysis of the reconstruction objective.\n\nThe construction of \"non-training data\" that satisfy KKT condition is intuitive and interesting. The further extended analysis for models that only satisfy approximate KKT condition is non-trivial and aligns well with experiments."}, "weaknesses": {"value": "The only limitation I see is that the analysis and experiments seem restricted to homogeneous networks, while most recent NN models are non-homogeneous. I understand this limitation is inherent for KKT-based reconstruction objectives, but it would be nice if the authors could discuss or use experiments to understand whether the insights generalize to typical other NN models, such as CNN."}, "questions": {"value": "Besides weakness, I have a followup question on prior knowledge. The construction relies on splitting, or merging training data records, while in practice the adversary may be stronger prior knowledge on the number of training data records. In such case, it appears that possibly, a large fraction of the reconstructed data could still be the true training data. This could be another layer of prior knowledge worth discussing, besides the initialization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YSUQtWyokY", "forum": "7wjFjOzCtB", "replyto": "7wjFjOzCtB", "signatures": ["ICLR.cc/2026/Conference/Submission9802/Reviewer_3CEM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9802/Reviewer_3CEM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893845523, "cdate": 1761893845523, "tmdate": 1762921288744, "mdate": 1762921288744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers parameter-only reconstruction attacks against shallow homogeneous ReLU neural networks performing binary classification, specifically analyzing the importance of the prior in the reconstruction attack developed in (Haim et al. 2022). The main contributions are the following:\n\nIn the event where a model converges to a KKT point as described by (Lyu and Li (2020), Ji and Telgarsky (2020)), the paper describes a family of candidate training sets indistinguishable to an adversary using the (Haim et al. 2022) attack without prior information through techniques of merging and splitting training data examples. The paper describes how, under an assumption that the training data does not span the whole input space, the adversary can find training sets arbitrarily far from the original training set. The paper also analyzes how far candidate training sets can be from the original training set under a relaxed assumption of closeness of the training data to some linear subspace.\n\nIn the event where a model converges to a $(\\varepsilon, \\delta)$-KKT point (an approximate KKT point), the paper proves the effectiveness of similar merging and splitting techniques, with splitting at the cost of a worse $\\delta$ value. The paper explores how large the family of split training sets can be depending on the adversary's knowledge of $\\delta$, suggesting that models that have converged more completely may be more difficult to attack.\n\nThe paper supports its theoretical arguments with empirical results on 1) synthetic data generated along a hypersphere and 2) CIFAR, showing how a lack of prior on training data magnitude can lead to poor reconstructions, suggesting a possible defense method against such attacks (manually shifting all samples by some magnitude)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-organized and easy to follow.\n\nThe paper adds clear theoretical analysis to the rapidly expanding discussion on the importance of training data priors in reconstruction attacks (see e.g. [Chaudhuri et al. 2025]).\n\nThe paper expands the collective understanding of the conditions necessary for the (Haim et al. 2022) attack to be useful.\n\nThe empirical results clearly support the theoretical analysis, particularly in the 'interpolated' training examples shown in Figure 2b."}, "weaknesses": {"value": "In the empirical results, the discussion of prior information available to the adversary is limited to data magnitude; while the plausibility of other forms of prior information, such as knowledge of the training data geometry or knowledge of the KKT $\\delta$, are discussed in the theoretical analysis, these are not mentioned in the empirical analysis.\n\nThe discussion could benefit from an expanded set of suggestions for future work in this field, including the potential to expand these results to the larger, higher-capacity models that are often used in empirical analyses of data reconstruction (see e.g. Struppek et al. 2022).\n\nThe empirical results and/or discussion could benefit from an analysis of potential adaptive attacks, in which a defender is aware that the magnitudes of the training data may have been altered.\n\nSpecific areas for clarification / improvement:\n\n- In Equation (6), why are the arguments of $L_{stationary}$ only $(x_1, ..., x_l)$, as opposed to $(x_1, ..., x_l, \\lambda_1, ..., \\lambda_l)?$\n\n- In the synthetic data experiment, why are no radii less than 1 tested? Would such a test be plausible, as a version of the adversary enforcing an overly strong prior on the data?"}, "questions": {"value": "Do we have any guarantees on the strength of the (Haim et al. 2022) attack when we know a sufficient amount of prior information about the data? If not, does your paper's result move us closer to gaining such guarantees?\n\nReferences in my review above: \n(Haim et al. 2022) - cited in the authors' paper\n(Lyu and Li 2020) - cited in the authors' paper\n(Ji and Telgarsky 2020) - cited in the authors' paper\nChaudhuri, Kamalika, et al. \"Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds.\" arXiv preprint arXiv:2404.02866 (2024).\nStruppek, Lukas, et al. \"Plug & play attacks: Towards robust and flexible model inversion attacks.\" arXiv preprint arXiv:2201.12179 (2022)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FqMtL6j63a", "forum": "7wjFjOzCtB", "replyto": "7wjFjOzCtB", "signatures": ["ICLR.cc/2026/Conference/Submission9802/Reviewer_e1G7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9802/Reviewer_e1G7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957167629, "cdate": 1761957167629, "tmdate": 1762921288226, "mdate": 1762921288226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes why implicit-bias–based reconstruction attacks on trained neural networks can fail, arguing that without explicit priors about the data domain, the KKT-driven objective used by prior attacks admits many alternative optima far from the true training set. The paper formalizes constructive merge/split operations that generate indistinguishable KKT sets, extends the analysis to approximate KKT points, and presents experiments on synthetic data and CIFAR, indicating that weaker priors degrade reconstructions while better-optimized networks are less susceptible, thereby reframing when leakage is feasible."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper offers a clear formalization of the attack objective and isolates the role of priors by analyzing the KKT-only loss.\n- This paper offers practical guidance to related fields by indicating when reconstruction is unlikely without priors, which helps motivate practical countermeasures.\n- The writing is clear."}, "weaknesses": {"value": "- The theory is developed for binary classification with homogeneous ReLU models, and it's not clear which proof steps would still hold in multiclass or with other losses. A brief *\"what carries over vs. what doesn't\"* discussion, or a tiny multiclass sanity check, would help readers judge how far the results travel.\n\n- Even if the Discussion later clarifies that results pertain to KKT-based reconstruction without priors, the abstract/Section 1 do not foreground this scope where readers first encounter the contributions; briefly surfacing the scope at the outset would align expectations and reduce the risk that practice-facing takeaways are read as general privacy claims, especially given prior work that studies broader regimes (multiclass, other losses) [1, 2].\n\n- The proposed shift/bias trick is intuitive but lacks robustness guarantees and could be learned by an adaptive adversary; scoping the claim (e.g., \"effective for KKT-only objectives under unknown shifts\") would avoid overgeneralization.\n\n- The discussion suggests implicit bias may hinder leakage along the KKT route, but memorization and extraction remain documented in overparameterized and generative settings [3, 4]. The authors should make it clearer: which leakage channels your analysis does and does not cover, so readers don’t overgeneralize the safety implications.\n\n## References\n\n- [1] N. Haim et al. Reconstructing Training Data from Trained Neural Networks. NeurIPS 2022.\n- [2] G. Buzaglo et al. Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses. NeurIPS 2023.\n- [3] V. Feldman and C. Zhang. What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation. NeurIPS 2020.\n- [4] N. Carlini et al. Extracting Training Data from Large Language Models. USENIX Security 2021."}, "questions": {"value": "- Beyond explicitly removing domain priors in the objective, which elements of the training pipeline might still act as priors (data normalization, augmentations, early stopping, weight decay), and how did you control or record them so readers can interpret the reconstruction outcomes?\n- Did you try residual connections to see if the phenomena qualitatively persist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A7R3679ULT", "forum": "7wjFjOzCtB", "replyto": "7wjFjOzCtB", "signatures": ["ICLR.cc/2026/Conference/Submission9802/Reviewer_73hu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9802/Reviewer_73hu"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975556971, "cdate": 1761975556971, "tmdate": 1762921287780, "mdate": 1762921287780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}