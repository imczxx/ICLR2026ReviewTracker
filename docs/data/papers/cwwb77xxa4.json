{"id": "cwwb77xxa4", "number": 21582, "cdate": 1758319283601, "mdate": 1759896913837, "content": {"title": "TRI-ANSLATE: LLM-Based Code Translation Of High-Performance Software", "abstract": "We present an LLM-based code translation and repair framework called TRI-anslate, which translates existing code written in an arbitrary source language to an arbitrary target language and validates that the output code adheres to desired properties via testing.\nExisting work has shown that LLMs are remarkable at code translation and repair tasks. Furthermore, specialized fine-tuned or distilled LLMs can extend these capabilities to handle niche languages, perform syntax repair with relatively small cost, or perform semantic repair taking into account common errors.\nHowever, the most robust currently available tools that leverage these LLMs assign all these distinct subtasks to a single LLM with a feedback loop from a validation tool. Further, they rely on a rigid set of possible errors as part of the corrective feedback from the validator or verifier. By contrast, TRI-anslate allows for a user-specified error set and leverages 3 separate LLM feedback loops to fully utilize the capability of LLMs specialized for generation, syntactic repair, and semantic repair. This also avoids wasting context of later LLMs on the correction conversation of previous LLMs.\nWe conduct an extensive evaluation, showcasing the advantage of TRI-anslate over the existing work using the same setup ($\\approx8$% increase comparing the base model, $\\approx 45$% for the fine-tuned model in CUDA to OpenMP Target Offloading Translation). We also demonstrate how being able to choose different models per subtask allows TRI-anslate to outperform LASSI using any of the individual models, and highlight the extensibility of TRI-anslate by documenting the effort required to add a new translation task (CUDA to SYCL).", "tldr": "We create an easily extensible tool that leverages three independent LLM and validator feedback loops to outperform the SOTA in source-to-source code translation.", "keywords": ["Code Translation", "Large Language Models (LLMs)", "High-Performance Computing (HPC)", "Code Modernization", "Code Portability"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/926d03746c8a2d3d106df953c0f58d2fdc170600.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TRI-anslate, an end-to-end automated code translation framework that leverages three independent large language models (LLMs) in separate feedback reprompting loops for generation, syntax repair, and semantic repair. TRI-anslate achieves success rates of approximately 8% for the base model and 45% for the fine-tuned model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Decomposing translation into three validated loops (Generation, Syntax, Semantic) is a strength.\n\n- The results are impressive, particularly the ~45% improvement in CUDA to OpenMP Target Offloading Translation.\n\n- TRI-anslate is flexible, with plug-and-play components."}, "weaknesses": {"value": "- Using three LLMs in iterative loops likely incurs high computational cost and latency, which are not discussed.\n\n- Evaluation is limited to HeCBench, restricting assessment of generalization and scalability.\n\n- The paper uses multiple LLMs but does not justify why a single LLM would be insufficient. An ablation comparing single- versus multi-LLM setups is needed.\n\n- It should be validated on a broader set of programming languages to demonstrate its generality."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XTjFmZg9ef", "forum": "cwwb77xxa4", "replyto": "cwwb77xxa4", "signatures": ["ICLR.cc/2026/Conference/Submission21582/Reviewer_wSfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21582/Reviewer_wSfw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393275446, "cdate": 1761393275446, "tmdate": 1762941844109, "mdate": 1762941844109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces TRI-anslate, a framework that performs automatic code translation and repair using three specialized LLMs connected through validation feedback loops. Each LLM handles a distinct subtask, namely, generation, syntax repair, and semantic repair, guided by automated validators and user-defined error sets. This modular design improves translation accuracy, scalability, and flexibility compared to existing systems like LASSI, which rely on a single feedback loop. Experiments demonstrate that TRI-anslate outperforms LASSI by about 8% with base models and 45% with fine-tuned ones in CUDA-to-OpenMP translation, and it easily adapts to new translation tasks such as CUDA-to-SYCL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Modular multi-step approach\n- Effectiveness\n- Extensible framework"}, "weaknesses": {"value": "- Considered benchmarks are rather small\n- No qualitative comparison with LASSI\n- Comparing only with LASSI"}, "questions": {"value": "- The evaluation is pretty small. What other benchmarks are available for evaluation?\n- Is it possible to see some qualitative comparison between LASSI and your technique?\n- What other tools are available beside LASSI that you can compare?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "i5f80VLqnY", "forum": "cwwb77xxa4", "replyto": "cwwb77xxa4", "signatures": ["ICLR.cc/2026/Conference/Submission21582/Reviewer_2987"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21582/Reviewer_2987"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439808520, "cdate": 1761439808520, "tmdate": 1762941843831, "mdate": 1762941843831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TRI-anslate, an LLM-based framework for automatic code translation and repair targeting high-performance computing (HPC) applications. The proposed system decomposes translation into three modular loops—generation, syntactic repair, and semantic repair—and allows the use of different LLMs for each stage. The authors evaluate TRI-anslate against LASSI on two translation tasks (CUDA→OpenMP and CUDA→SYCL) using the HeCBench benchmark suite."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This work aims to address an important and tough problem: HPC code translation.\n2. The idea of three-loop modular architecture makes sense and could be important in translation performance.\n3. The comparison with LASSI is well designed with modified prompts and settings for fair comparison."}, "weaknesses": {"value": "1. The experiments cover only two translation tasks and a single translation direction (CUDA→OpenMP and CUDA→SYCL), which limits the generalizability of the proposed framework.\n2. While the three-loop modular design is conceptually interesting, the paper does not provide convincing evidence of its effectiveness. For agentic or multi-stage pipelines, a thorough ablation study—especially on how information and context are transferred or reset across modules—would be necessary to demonstrate tangible benefits and boundaries of each component.\n3. The differences from previous systems such as LASSI are not well articulated. The proposed three-loop architecture is presented at an abstract level, and many claimed advantages (e.g., using different prompts or models for subtasks) could be implemented relatively easily in existing frameworks without substantial innovation.\n4. The paper omits relevant prior pipelines such as Chen, Le, et al. “Fortran2CPP: Automating Fortran-to-C++ Migration Using LLMs via Multi-turn Dialogue and Dual-agent Integration” (arXiv:2412), which also employ multi-agent, dialogue-based, and verification-guided translation strategies. Including such comparisons would help position TRI-anslate more clearly within the landscape of recent LLM-based translation research.\n5. Writing needs significant polish.\na. Contribution 2 is not really a contribution;\nb. need to check your \\citet and \\citep;\nc. the references are with mixed formats;\nd. Figure 1 needs more details and a clearer presentation. In your text, Preprocessor is not a part of Generation; but it is included in the Generation box in Figure 1."}, "questions": {"value": "1. I appreciate the work of the CUDA→SYCL translation task, which is indeed challenging and relevant. However, translating OpenMP→CUDA could be even more impactful and representative of practical HPC needs. Could you comment on why this direction was not explored?\n2. The current evaluation focuses only on one-way translation. Why did you not include bi-directional translation (e.g., CUDA↔OpenMP or CUDA↔SYCL)? Such results could better demonstrate the generality of the proposed framework.\n3. The paper briefly mentions improvements over prior work such as LASSI, but the distinctions remain vague. Could you elaborate more clearly on how TRI-anslate differs in methodology and implementation from existing multi-agent or feedback-based code translation frameworks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "84x6XdcOK7", "forum": "cwwb77xxa4", "replyto": "cwwb77xxa4", "signatures": ["ICLR.cc/2026/Conference/Submission21582/Reviewer_fzT7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21582/Reviewer_fzT7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965883587, "cdate": 1761965883587, "tmdate": 1762941843547, "mdate": 1762941843547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents TRI-anslate, an approach for performing automatic code translation using a triple feedback loop system relying on separate LLM calls for generation, syntactic and semantic repair. The method is tested on the HecBench benchmark and compared to the performance of LASSI, a state of the art suite for the task. The authors show that their novel method outperforms LASSI and can be extended to new translation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper deals with a complex and pressing challenge in software engineering, namely automatic code translation of legacy and machine-specific code to more modern forms, especially in HPC settings. Given the ubiquitous presence of LLMs and their ability at translation tasks in general, the paper examines their potential for this challenge by leveraging a combination of LLM calls to assess various types of possible mistakes."}, "weaknesses": {"value": "I think this paper is a good starting point for a relevant piece of research at the intersection of agentic AI and code translation / repairment in HPC settings, however in its current form is still in early days. I have listed here some of the major aspects that I believe the authors should address before resubmitting this piece of work, but it is important to note that some of them are already recognised by the authors in their limitations section. The fact that the paper does not benefit from the 9th available page makes me think this work was submitted a bit early on.\n\n* The paper should conduct a more in-depth analysis of the role of the coding LLM used (e.g. studying what are the issues that a reasoning model like gpt-oss:20B brings) and why a combination of different sizes of the same LLM (see B3) leads to better results. This is necessary to understand the tradeoffs between the different settings.\n\n* The work needs a proper error analysis to understand which types of mistakes the tool makes, especially in comparison with LASSI (does it make fewer mistakes but of the same type or different mistakes?). This is necessary to understand whether the two tools approach things differently or not.\n\n* It would be important to test such approach on a second benchmark and examining whether the same findings emerge when not addressing high performance code.\n\n* [Optional] To reach a wider audience, this work could be set more specifically in the literature focusing on agentic AI and multiple LLMs interoperating with each other. Instead of a single step-by-step pipeline, the authors should at least consider (given its relevance in the current literature) a more holistic approach, with a general LLM agent deciding which available tool to use, in which order and why.\n\nAll in all, I think this paper is on the right track and with more error analysis and testing on a wider range of translation tasks it could become a solid piece of work.\n\nSide note: The paper seems to be written a bit in a rush and has a few typos (e.g. lables in page 5)."}, "questions": {"value": "* Could you clarify the conceptual motivation for separating syntax and semantics repair? Are the boundary conditions between the two well defined?\n\n* The experiments rely entirely on HeCBench, which is also used for fine-tuning and validation. Did you investigate generalisation to unseen codebases or domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZpE1ts6K0k", "forum": "cwwb77xxa4", "replyto": "cwwb77xxa4", "signatures": ["ICLR.cc/2026/Conference/Submission21582/Reviewer_pKEb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21582/Reviewer_pKEb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186708655, "cdate": 1762186708655, "tmdate": 1762941843229, "mdate": 1762941843229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}