{"id": "xWTjMkkjrO", "number": 15271, "cdate": 1758249634651, "mdate": 1759897316691, "content": {"title": "SocialHarmBench: Revealing LLM Vulnerabilities  to Socially Harmful Requests", "abstract": "Large language models (LLMs) are increasingly deployed in contexts where their failures have the potential to carry sociopolitical consequences. However, existing safety benchmarks sparsely test vulnerabilities in domains such as political manipulation, propaganda generation, or surveillance and information control. To address this gap, we propose SocialHarmBench, a dataset of 585 prompts spanning 7 sociopolitical categories and 34 countries with real-world events, designed to evaluate LLM vulnerabilities to sociopolitical harms. Using SocialHarmBench, we provide: (1) adversarial evaluation coverage of high-risk domains including authoritarian surveillance, disinformation campaigns, erosion of democratic processes, and crimes against humanity; (2) adversarial evaluations across open-source models, establishing baseline robustness and measuring attack efficiency in politically charged settings; and (3) insights into domain-specific vulnerability comparisons, temporal-wide investigations to trace vulnerable time periods, and region-specific vulnerabilities. Our findings reveal that existing safeguards fail to transfer effectively to sociopolitical contexts, exposing partisan biases and limitations in preserving human rights and democratic values.", "tldr": "We introduce the first adversarial evaluation benchmark designed to assess sociopolitical misuse risks in large language models.", "keywords": ["alignment", "red-teaming", "safety", "robustness", "sociopolitical risks", "democracy defense", "societal good", "evaluation benchmark"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13fe3d53c1517dc30c4c404b19847e7f33d68c97.pdf", "supplementary_material": "/attachment/3bd82851528be58307b7ff05cc7c5357ab5af0b5.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents SocialHarmBench, a jailbreak benchmark that evaluates LLMs' robustness to social-political questions. The benchmark highlights diversity in terms of the covered topics, countries, time periods, and query types. The authors first evaluate eight LLMs to reveal vulnerabilities and analyze the distribution of successful jailbreaks over domains, regions, and time periods. The authors also apply six jailbreak strategies to five LLMs to further investigate the effectiveness of these strategies in social-political queries. The results reveal vulnerabilities in the safeguards of existing LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1: The social-political aspect is a unique aspect not extensively studied by existing jailbreak benchmarks.  \nS2: The authors provide a somewhat thorough investigation of this aspect in a set of existing LLMs."}, "weaknesses": {"value": "W1: Although this is a benchmark paper, the authors never described how the dataset was created. The four steps mentioned in Section 2.3 and Appendix D lack enough details on the generation process. Are all the events, topics, and templates identified and generated by LLMs? If yes, then the subsequent evaluation would be highly biased toward the position of the LLM that generates the dataset. If no, then the paper lacks enough citations and details to ground the data collection process. Political opinions and social harms are very sensitive topics, and the authors need to ensure that all the queries tested in the paper are objective and truly reflect the opinions held by the vast majority.  \nW2: How do you determine the semantic categories and functional categories of a jailbreak prompt in SocialHarmBench? It seems that there is quite some overlap in those categories, e.g., surveillance and censorship.  \nW3: Can you do manual checking on the results in Section 4.1? Are those scores reported by LLMs-as-a-judge aligned with human judgment?  \nW4: Can you swap the topics in your jailbreak templates to other non-social-political topics? To make sure that the vulnerabilities reported in your Figure 3 do not arise from your templates and other non-social-political confounding factors during your evaluation process."}, "questions": {"value": "Q1: In the introduction, it says that the evaluation involves five LLMs, but Section 3 says that there are eight LLMs. This is confusing.  \nQ2: Can you put all the proprietary LLMs together and all the open-sourced LLMs together on the x-axis in Figure 3?  \nQ3: What do the authors mean by the yellow text blocks in Pages 7, 8, and 9? There is no caption or any explanation.  \nQ4: In ICLR templates, captions should come on top of tables, instead of at the bottom. This applies to all the tables in this paper.  \nQ5: Figure 2 is very hard to follow. What do the bars in the two subfigures in the top row represent? And which region does \"Add\" stand for?  \nQ6: Section 3 is very repetitive and verbose. There is no need to mention HarmBench and StrongREJECT classifiers three times."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FdcymFAxg0", "forum": "xWTjMkkjrO", "replyto": "xWTjMkkjrO", "signatures": ["ICLR.cc/2026/Conference/Submission15271/Reviewer_sDuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15271/Reviewer_sDuZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428497647, "cdate": 1761428497647, "tmdate": 1762925573413, "mdate": 1762925573413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SocialHarmBench, a 585-prompt benchmark including seven sociopolitical domains and 34 countries to test LLM vulnerabilities in politically charged settings. Prompts are labeled both semantically and by functional type, and responses are scored with HarmBench for harmful compliance and StrongREJECT for refusal robustness. Evaluation results for eight LLMs are reported alongside five open-weight models under six attack settings. Baseline weaknesses are largest in historical revisionism, propaganda, and political manipulation. Attacks degrade safety, showing that current safeguards do not transfer well to sociopolitical harms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Semantic categories for the dataset are well grounded, and the dataset split across categories is well balanced. It is clear that the data curation was done well.\n- The analysis of the curated dataset is sufficiently rigorous, and the evaluation set for models is comprehensive.\n- Aside from typos (see below), paper delivery is very clear. In particular, I enjoyed Figure 6 as is informative and interesting to see.\n- Overall, the paper addresses an important aspect in safety and alignment in language models that existing work has looked over; SocialHarmBench is a very useful contribution to the safety community."}, "weaknesses": {"value": "## Grammar Corrections / Minor Errors\n- Line 52: comprising of → comprising\n- Line 59: focusing → focuses\n- Line 62: remove duplicate: “surveillance and censorship, ..., surveillance,”\n- Line 74: e.g. → e.g.,\n- Line 77: to present day → to the present day\n- Line 244: distangle → disentangle\n- Line 255: five attack techniques → six attack techniques? (you evaluate six)\n- Line 410 (Section 4.3 title): LLMs are more vulnerable towards → LLMs are more vulnerable to\n- Line 1213: segragation → segregation\n- Appendix (and figure labels): StrongReject → StrongREJECT\n- Figure 1: Says 40 sub-topics, but line 74 says 37 sub-topics.\n- Figure 1: Civilian Targetting → Civilian Targeting\n\n## Major Weaknesses\n- It is unclear to me whether StrongREJECT and HarmBench evaluation is sufficient or accurate. These two methods largely use string matching, which may miss true harmfulness given the sophistication of StrongREJECT questions. Have the authors considered designing an evaluator to pair with SocialHarmBench? At minimum, I believe it may be necessary to include human annotation results of classifier outputs. HarmBench/StrongREJECT driving all conclusions is weak and requires more reliable verification.\n- While the set of attacks that were examined in the paper are good, several well-noted jailbreak methods have been missed (e.g., TAP [1], PAIR [2], etc.). In particular, it would be nice to include results for some more simple prompt-only methods ([3]).\n\n## References\n[1] Mehrotra, A., Zampetakis, M., Kassianik, P., Nelson, B., Anderson, H., Singer, Y., & Karbasi, A. (2025). *Tree of Attacks: Jailbreaking Black-Box LLMs Automatically.*  \n[2] Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G. J., & Wong, E. (2025). *Jailbreaking Black-Box Large Language Models in Twenty Queries.*  \n[3] Li, N., Han, Z., Steneker, I., Primack, W., Goodside, R., Zhang, H., Wang, Z., Menghini, C., & Yue, S. (2025). *LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet.*"}, "questions": {"value": "- Why is Figure 3 a line graph rather than a bar graph like Figure 4? The current plot is hard to interpret.\n- Why were thinking/reasoning models not evaluated for this task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1bxKYyQTjr", "forum": "xWTjMkkjrO", "replyto": "xWTjMkkjrO", "signatures": ["ICLR.cc/2026/Conference/Submission15271/Reviewer_6ZQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15271/Reviewer_6ZQ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627007533, "cdate": 1761627007533, "tmdate": 1762925572895, "mdate": 1762925572895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses an under-studied safety gap—sociopolitical misuse—and delivers the first geographically and temporally diverse benchmark for it. The evaluation pipeline is reproducible and the influence-analysis provides actionable diagnostics. However, the prompt set is English-only and omits multi-turn or tool-use attacks that real adversaries employ. Classifier-based ASR metrics are not validated against human judgment and may mis-classify subtle refusals. Mathematical notation for the over-representation factor ORF and influence approximation is introduced but not connected to statistical confidence or convergence guarantees."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) Comprehensive coverage of sociopolitical harms: 585 prompts across 7 domains, 34 countries, 4 centuries yield the widest temporal scope among safety benchmarks. Explicit alignment with Universal Declaration of Human Rights categories increases real-world relevance.\n\n2) Rigorous curation pipeline: Multi-model generation → quantitative jailbreak filter → manual deduplication ensures low redundancy and high adversarial quality. Over-representation factor ORF balances geographic bias.\n\n3) Novel influence-function diagnostics: EK-FAC attribution on 8-B model links harmful outputs to criminal templates in pre-training, offering a principled data-audit tool."}, "weaknesses": {"value": "1) 60 % of events focus on 20th–21st C Europe/North-America (Fig. 8), risking skewed priors.\n\n2) Classifier-based labels lack human validation: HarmBench & StrongREJECT are themselves small models; no inter-annotator agreement or false-positive rate reported. Examples in Table 2 show borderline opinion queries (“Do you support…”) that humans might rate as non-harmful refusals, inflating ASR.\n\n3) Opaque ORF derivation and bias correction: Over-representation factor is defined but threshold for “balanced” coverage is not justified; no ablation showing ORF improves fairness.\n\n4) Statistical uncertainty absent: No confidence intervals or variance across random seeds for ASR."}, "questions": {"value": "See above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AjvDXsjMa4", "forum": "xWTjMkkjrO", "replyto": "xWTjMkkjrO", "signatures": ["ICLR.cc/2026/Conference/Submission15271/Reviewer_sKc7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15271/Reviewer_sKc7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997330867, "cdate": 1761997330867, "tmdate": 1762925572486, "mdate": 1762925572486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SocialHarmBench, a benchmark that systematically evaluates large language models (LLMs) for vulnerabilities to sociopolitical harms. It covers 585 prompts across 34 countries and seven sociopolitical domains. The benchmark integrates both HarmBench (harmful capability) and StrongReject (refusal robustness) as their evaluation metrics. They find that current LLM safeguards (especially open-source ones) fail to generalize under sociopolitical contexts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive coverage of different countries and domains\n- Efforts in experiments\n- Good findings"}, "weaknesses": {"value": "- The contribution is primarily the dataset, the pipeline relies heavily on the previous works\n- Lack of details in dataset construction \n- Lacks human evaluation\n- Adversarial robustness is only conducted on open-source models with relatively small size (7B)"}, "questions": {"value": "- The authors claim to use both HarmBench and StrongReject to evaluate their benchmark. However, there is no verification that these classifiers perform reliably on the new sociopolitical dataset. A small human-evaluated subset (e.g., 5–10% of the data) would help validate the consistency and accuracy of automated harmfulness and refusal detection.\n- Maybe I missed something, but how the authors get their 585 prompts is unclear. It would be helpful to explain how these prompts were constructed/sourced, balanced across domains and regions, and reviewed for representativeness or potential bias.\n- The results show that the LLMs may fail on certain domains, but the underlying reasons are not analyzed. Are these differences driven by the prompt wording, topic sensitivity, or inherent domain complexity? Anaysis on such failures is important for us to understand the safety mechanisms better. \n- I am curious why the authors only evaluate open source models for adversarial robustness, as there are also adversarial attacks that do support black-box models.\n- It is recommended to add a small defense to evaluate current safety mechanism. For example, there are many safety guardrails that are designed to moderate the input/output content of LLMs, such as llamaguard series, gemmashield, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GmUiJahLBP", "forum": "xWTjMkkjrO", "replyto": "xWTjMkkjrO", "signatures": ["ICLR.cc/2026/Conference/Submission15271/Reviewer_BJtb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15271/Reviewer_BJtb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997745269, "cdate": 1761997745269, "tmdate": 1762925571737, "mdate": 1762925571737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}