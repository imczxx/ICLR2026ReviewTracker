{"id": "89j1hUxOiF", "number": 18708, "cdate": 1758290310850, "mdate": 1759897086240, "content": {"title": "Cross-ControlNet: Training-Free Fusion of Multiple Conditions for Text-to-Image Generation", "abstract": "Text-to-image diffusion models achieve impressive performance, but reconciling multiple spatial conditions usually requires costly retraining or labor intensive weight tuning.\nWe introduce Cross-ControlNet, a training-free framework for text-to-image generation with multiple conditions.\nIt exploits two observations: intermediate features from different ControlNet branches are spatially aligned, and their condition strength can be measured by spatial and channel level variance.\nCross-ControlNet contains three modules: PixFusion, which fuses features pixelwise under the guidance of standard deviation maps smoothed by a Gaussian to suppress early-stage noise; ChannelFusion, which applies per channel hybrid fusion via a consistency ratio gate, reducing threshold degradation in high dimensions; and KV-Injection, which injects foreground- and background-specific key/value pairs under text-derived attention masks to disentangle conflicting cues and enforce each condition faithfully.\nExtensive experiments demonstrate that Cross-ControlNet consistently improves controllable generation under both conflicting and complementary conditions, and further generalizes to the DiT-based FLUX model without additional training.", "tldr": "Cross-ControlNet is a training-free framework that fuses multiple spatial conditions for text-to-image generation via three novel modules: PixFusion, ChannelFusion, and KV-Injection.", "keywords": ["Training free", "Multi-Condition", "Controllable Image Synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84c418bc35a2de98bf69ccaa7fa676229c789db0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* The paper proposes Cross-ControlNet, a training-free framework for fusing multiple conditional branches in T2I generation.\n* It is built upon two key observations: the spatial alignment across ControlNet branches and the variance-based condition strength.\n* The framework introduces three modules:\n  * PixFusion for pixel-level feature fusion guided by Gaussian-smoothed variance maps,\n  * ChannelFusion for adaptive hard, soft fusion based on channel-wise consistency ratios,\n  * KV-Injection for foreground-background disentanglement using text-derived attention masks.\n* Without any additional training, Cross-ControlNet achieves robust controllable generation under both complementary and conflicting conditions.\n* It outperforms existing training-free methods such as MaxFusion, AnyControl, and Uni-ControlNet, and generalizes to DiT-based models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The method is training-free, making it practical and computationally efficient for multi-condition control.\n* The variance-guided fusion offers an intuitive yet mathematically grounded mechanism for balancing control strength and spatial coherence.\n* The KV-Injection module elegantly leverages textual attention maps to isolate and refine foreground and background regions.\n* The paper provides comprehensive ablations that demonstrate the contribution of each module.\n* Quantitative results show clear improvements over baselines."}, "weaknesses": {"value": "* The framework is sensitive to several hyperparameters, but their selection rationale is largely empirical.\n* The approach inherently depends on pre-trained single-condition ControlNets, making it less flexible when new modalities are introduced.\n* Using multiple ControlNet branches may increase inference latency and GPU consumption."}, "questions": {"value": "* How stable is the variance-based fusion under time-dependent noise variations during the diffusion process?\n* While generalization to DiT-based models is promising, how does cross-branch fusion behave in transformer-based architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "x7o7K0KzBQ", "forum": "89j1hUxOiF", "replyto": "89j1hUxOiF", "signatures": ["ICLR.cc/2026/Conference/Submission18708/Reviewer_y6kF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18708/Reviewer_y6kF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937803800, "cdate": 1761937803800, "tmdate": 1762928413745, "mdate": 1762928413745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces a novel method for combining conditioning signal from several ControlNets in a single diffusion model. The proposed approach uses two main observations: that different ControlNets are spatially aligned and that the strengths of individual conditions can be quantified via feature variances, that combined with KV-injection allows for accurate fusion of spatial conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- There is a clear introduction to the main idea behind the submission in section 3.1. Presented observations are sound and provide great motivation for the proposed solution."}, "weaknesses": {"value": "- The usability of the proposed technique is very limited to the specific task of fusing multiple spatial conditions for T2I models\n- The proposed technique is a combination of several “add ons” that build on top of the standard controlnet, this slightly limits the novelty of the proposed method\n- Main experiments are performed with quite old SD 1.5. While the qualitative results with FLUX are impressive, the submission lacks proper comparison with other approaches using this model.\n- The experimental section focus mostly on a setup with clear separation between foreground and background which highly utilizes the KV-injection technique. It would be great to see some results with more complex compositions\n\n\nSmal issue:\n(Presentation) - Lines 60-64 in introduction are copies from the abstract. As I didn’t fully understand what authors meant in the abstract, it was not easier when presented with exactly the same sentence for the second time."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sSPVuPvBCF", "forum": "89j1hUxOiF", "replyto": "89j1hUxOiF", "signatures": ["ICLR.cc/2026/Conference/Submission18708/Reviewer_Tyso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18708/Reviewer_Tyso"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950912475, "cdate": 1761950912475, "tmdate": 1762928411557, "mdate": 1762928411557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Cross-ControlNet, a training-free framework for fusing multiple spatial conditions in text-to-image diffusion models. It introduces three modules—PixFusion (pixel-wise fusion guided by smoothed spatial variance), ChannelFusion (channel-wise adaptive fusion using a consistency ratio), and KV-Injection (foreground–background disentanglement via attention masks), to handle both conflicting and complementary conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. PixFusion uses Gaussian-smoothed spatial standard deviation to guide pixel-wise selection, suppressing early-stage noise as validated by Fig. 3b showing cleaner fused variance maps. ChannelFusion and KV-Injection are also resonable modules.\n\n2. Qualitative results in Fig. 4 show faithful preservation of both teddy bear pose and train window structure, where baselines fail to reconcile conflicting cues. Other qualitative results also performs better than other baslines.\n\n3. The framework can transfer to DiT-based FLUX without modification, producing sharp, controllable images, demonstrating architecture agnosticism."}, "weaknesses": {"value": "1. No inference time or memory usage is reported despite combining multiple ControlNet branches\n\n2. The claim that ChannelFusion is applied only in the final layer (Sec. 4.1) is not justified; no ablation tests layer-wise placement (e.g., middle vs. final layer)."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sAcpqTCl5G", "forum": "89j1hUxOiF", "replyto": "89j1hUxOiF", "signatures": ["ICLR.cc/2026/Conference/Submission18708/Reviewer_ajmX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18708/Reviewer_ajmX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967633417, "cdate": 1761967633417, "tmdate": 1762928409699, "mdate": 1762928409699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Cross-ControlNet, a training-free framework designed for multi-condition text-to-image generation. The framework integrates multiple spatial conditions without retraining, using three main components: PixFusion, ChannelFusion, and KV-Injection. These modules allow for robust feature fusion while handling both conflicting and complementary conditions, with demonstrated improvements in controllable generation. The method improves performance over existing models, notably in generating high-quality images under complex, multi-condition prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an innovative solution to the challenge of multimodal conditional image generation by introducing Cross-ControlNet, which uses a combination of PixFusion, ChannelFusion, and KV-Injection to enhance feature fusion and address the issues of noise sensitivity and high-dimensional fusion degradation. \n2. The framework is compatible with existing models (e.g., DiT-based architectures), demonstrating its generalization potential and ability to adapt to different underlying network backbones.\n3. The experiments are comprehensive, covering both quantitative metrics (e.g., mIoU, MSE) and qualitative results. The method consistently outperforms state-of-the-art methods, such as MaxFusion, Multi-ControlNet, and AnyControl, particularly under conflicting conditions."}, "weaknesses": {"value": "1. The model’s architecture, which combines multiple ControlNet branches, leads to significant increases in memory consumption and inference time. This makes it challenging to deploy Cross-ControlNet for high-resolution image generation or real-time applications where computational efficiency is critical.\n\n2. While the method works well under conflicting and complementary conditions, extreme conflicting signals may still cause artifacts, such as foreground-background misalignment or color bleeding. The paper could further explore ways to mitigate these extreme cases."}, "questions": {"value": "A more concise explanation of why these components are crucial for multimodal consistency could strengthen the paper's readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bshVTtpYiA", "forum": "89j1hUxOiF", "replyto": "89j1hUxOiF", "signatures": ["ICLR.cc/2026/Conference/Submission18708/Reviewer_jzWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18708/Reviewer_jzWz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991571146, "cdate": 1761991571146, "tmdate": 1762928408554, "mdate": 1762928408554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores training-free methods to combine multiple ControlNet conditions via fusing ControlNet outputs of multiple branches. This is done through either PixFusion or ChannelFusion. They also use KV-Injection, to separate the foreground/background more clearly to avoid conflicting requirements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Training-free method, so quick to test/implement for use\n- Works on any architecture that ControlNet works upon\n- Results show improvement over baselines on SD 1.5"}, "weaknesses": {"value": "- Quantitative results for only SD 1.5, though this may be due to ControlNet restrictions\n- Combining the end product of multiple branches will increase latency, though no values are given as to the cost\n- Could be good to see results without the use of the Gaussian Kernels - how much does this use contribute to the results?\n- The paper assumes knowledge of ControlNet implementation - perhaps some time could be spent on introducing it since the paper relies on it entirely\n  - I found the paper hard to follow in places and so was unsure of the exact methodology used - see questions"}, "questions": {"value": "Clarifications:\n- How are the spatial-level variance maps calculated for PixFusion?\n- For KV-Injection, the terms \"background and foreground ControlNets\" are introduced. Does this mean that KV-Injection only works for combinations of ControlNets that work on foreground/background, and not for two ControlNets that both only focus on the foreground?\n- In equation (9), Q1 is not used. Is this correct?\n- In Figure 3a, the x axis is referred to as the \"depth of the feature layer\", which I am taking to mean the depth of the block in the model, but the text referring to it talks about the dimensionality of the feature space, which does not seem to be the same thing?\n\n\nSuggestions:\n- My understanding for ChannelFusion is that each channel is over all the tokens. Could it not be that the channel would be more useful in some spatial areas than others for each ControlNet, especially if the nets do not overlap?\n- For both PixFusion and ChannelFusion, the threshold is a binary choice between taking only one ControlNet, vs an averaging approach. Could a less hard boundary be used instead, such as an interpolation or sliding scale? Perhaps the lack of a hyperparameter in this way would avoid the threshold degredation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gVQQSQdpq1", "forum": "89j1hUxOiF", "replyto": "89j1hUxOiF", "signatures": ["ICLR.cc/2026/Conference/Submission18708/Reviewer_MNfM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18708/Reviewer_MNfM"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18708/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763399484308, "cdate": 1763399484308, "tmdate": 1763399484308, "mdate": 1763399484308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}