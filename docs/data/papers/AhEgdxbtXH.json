{"id": "AhEgdxbtXH", "number": 468, "cdate": 1756741319142, "mdate": 1763717469613, "content": {"title": "Productive LLM Hallucinations: Conditions, Mechanisms, and Benefits", "abstract": "Hallucinations in large language models (LLMs) are typically regarded as harmful errors to be suppressed. We revisit this assumption and ask whether, and under what conditions, hallucinations can instead be beneficial. To address this question, we introduce $\\textbf{HIVE}$ ($\\textbf{H}$allucination $\\textbf{I}$nference and $\\textbf{V}$erification $\\textbf{E}$ngine), a task-agnostic framework that systematically evaluates the impact of hallucinated semantics across diverse tasks and models. By unifying generation, discrimination, and downstream evaluation, HIVE enables controlled comparative assessments of how hallucinations alter overall model performance. Extensive experiments on nine datasets and ten models show that hallucinations can yield substantial improvements up to $\\textbf{+17.2}$ \\% in accuracy especially in open-ended domains such as reasoning, biomedical, and vision language tasks. Stronger models consistently harness hallucinations, while weaker ones are more volatile. Mechanistic analyses show that hallucinations broaden semantic coverage, stabilize reasoning trajectories, and follow an inverted-U profile where moderate strength maximizes benefits across diverse tasks. These findings reframe hallucination from a defect to a controllable cognitive resource, suggesting opportunities for evaluating and training LLMs not merely to avoid hallucinations, but to exploit them constructively.", "tldr": "We introduce HIVE, a framework showing that moderate hallucinations in LLMs can broaden semantics, stabilize reasoning, and improve accuracy across diverse tasks, reframing them as a controllable cognitive resource.", "keywords": ["Large Language Models; Hallucination; Productive Hallucinations; Reasoning Dynamics"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8593e98813daf1da15cdf91ab11e3c0927e47e0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the counterintuitive phenomenon that hallucinated inputs can, under certain conditions, enhance model performance across a range of tasks and architectures. The authors introduce HIVE, a systematic framework designed to evaluate the impact of hallucinations on downstream task performance. Using this framework, they generate both hallucinated and faithful captions for the same input and quantify the performance gap between two settings. The analysis spans both textual and multimodal tasks and involves multiple large language models. Empirical results demonstrate that hallucinated captions consistently improve accuracy on multimodal and perception-driven tasks. These findings provide novel insights into the productive role of hallucinations and open promising directions for leveraging controlled hallucination to enhance LLM reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed evaluation framework is conceptionally simple and task-agnostic\n- The authors conduct comprehensive evaluation across diverse datasets, with significance testing and robustness checks, to validate the effect of hallucinations on model performance\n- The paper is well structured and easy to follow. The implementation detail of HIVE framework and analysis is well documented."}, "weaknesses": {"value": "The conclusion that hallucinations promote intra-chain convergence is not well supported by Figure 4. Examining the step-wise cosine similarity of reasoning chains with raw input and faithful input would provide more evidence."}, "questions": {"value": "Some experiment settings and discussions in the analysis section require further clarity: \n\n- In HIVE workflow, how do you construct the contrastive pairs based on multiple candidate captions?\n- In the reasoning convergence analysis (Section 4.3), what is the step-wise cosine similiarity of reasoning chains when the input contains faithful captions? Is the pattern different from that in Figure 4?\n- In Section 4.5, how do you control the “level of hallucinations”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HVPb6cCgPT", "forum": "AhEgdxbtXH", "replyto": "AhEgdxbtXH", "signatures": ["ICLR.cc/2026/Conference/Submission468/Reviewer_fxuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission468/Reviewer_fxuZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761091992760, "cdate": 1761091992760, "tmdate": 1762915526731, "mdate": 1762915526731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the conventional view that hallucinations in large language models (LLMs) are always undesirable. It introduces the concept of productive hallucinations—outputs that deviate from ground truth but enhance reasoning, creativity, or generalization. The authors propose HIVE (Hallucination Inference and Verification Engine), a unified framework that systematically compares faithful versus hallucinatory augmentations across multiple tasks and modalities. Experiments on nine benchmarks (including reasoning, perception, and multimodal tasks) across nine models demonstrate that moderate hallucinations can sometimes improve downstream accuracy by up to 17.2%, particularly in open-ended reasoning settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper presents a novel and thought-provoking perspective, reframing hallucinations as potentially beneficial under controlled conditions, which challenges a dominant assumption in LLM research.\n\n+ The proposed HIVE framework is technically sound and broadly applicable, offering a structured way to quantify and evaluate the effects of hallucinations across diverse tasks and models.\n\n+ The experimental validation is extensive and convincing, covering multiple benchmarks, models, and modalities, and demonstrating clear empirical evidence for the concept of productive hallucinations."}, "weaknesses": {"value": "- The theoretical grounding for why certain hallucinations are productive remains underdeveloped, as the paper largely relies on empirical observations without a deeper cognitive or information-theoretic explanation.\n\n- The scope of evaluation is limited to short-term performance metrics, leaving questions about long-term reliability, factual consistency, and safety implications of encouraging controlled hallucinations.\n\n- The paper has limited algorithmic novelty. Despite its solid analysis and empirical breadth, the paper’s core contribution lies primarily in the evaluation framework and observations. It does not propose a new model or training approach beyond HIVE’s evaluation setup, which may limit its technical novelty."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yk9SBq1SQn", "forum": "AhEgdxbtXH", "replyto": "AhEgdxbtXH", "signatures": ["ICLR.cc/2026/Conference/Submission468/Reviewer_EHqo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission468/Reviewer_EHqo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981466711, "cdate": 1761981466711, "tmdate": 1762915526629, "mdate": 1762915526629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their careful reading and constructive feedback. With **all four** reviews awarding an initial **score of 6**, we are encouraged that reviewers found the problem formulation interesting, the analysis comprehensive, and the HIVE framework broadly applicable.\n\nDuring the rebuttal, we focused on clarifying key methodological details and adding the analyses requested by the reviewers:\n\n- 1. **Corrected Table 1** and moved it to *Appendix Table A1* for clarity.\n\n- 2. **Added the requested step-wise convergence comparison** across Raw / Faithful / Hallucinatory inputs (*Appendix A Fig. A1* ).\n\n- 3. **Clarified the caption-pairing procedure** and related methodological details.\n\nWe hope these updates make the contribution and empirical findings easier to follow. We sincerely appreciate the reviewers’ time and constructive suggestions."}}, "id": "w8gQRWFdVf", "forum": "AhEgdxbtXH", "replyto": "AhEgdxbtXH", "signatures": ["ICLR.cc/2026/Conference/Submission468/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission468/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission468/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763704211035, "cdate": 1763704211035, "tmdate": 1763704211035, "mdate": 1763704211035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that some LLM hallucinations can be productive under the right conditions and introduces HIVE, a framework that generates, filters, and evaluates hallucinated semantics to test their impact. It shows that hallucination-augmented inputs can boost performance in open-ended, perception-like tasks, while effects are mixed or negative in strict rule-driven domains. The authors explain the gains by showing that hallucinations broaden semantic coverage and raise semantic entropy, diversifying reasoning without disrupting convergence. They advise using a moderate strength, reporting an inverted-U response where balanced doses work best across settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel contribution: The paper reframes hallucination as a controllable resource and introduces a unifying, general-purpose framework (HIVE) to study when and why it helps across modalities. \n\n2. Methodological soundness & breadth: The design enables apples-to-apples, controlled comparisons (raw vs. faithful vs. hallucinatory) and uses an ensemble discriminator validated on benchmarks; the setup is task-agnostic and scalable across models and tasks. I appreciate the authors' effort in this.\n\n3. Mechanistic insight with stability assurances: The authors smartly tie gains to broadened semantic coverage and higher semantic entropy while showing intra- and inter-chain convergence is preserved. \n\n4. Actionable guidance & good presentation. The paper offers practical knobs and presents the work clearly with an intuitive case study and well-organized structure. It was very easy to read and follow."}, "weaknesses": {"value": "Labeling reliability & narrow metrics: HIVE’s conclusions hinge on an ensemble detector to label captions as faithful vs. hallucinatory -- even the authors note hallucination detection is inherently imperfect -- while downstream evaluation is instantiated mainly as accuracy.\n\n\nSuggestion: I recommend moving the Experimental Setup (Appendix S7) to the main text, as it contains essential information."}, "questions": {"value": "I don't have any questions so far."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eiDLb9Hl7d", "forum": "AhEgdxbtXH", "replyto": "AhEgdxbtXH", "signatures": ["ICLR.cc/2026/Conference/Submission468/Reviewer_GeAh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission468/Reviewer_GeAh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989030800, "cdate": 1761989030800, "tmdate": 1762915526515, "mdate": 1762915526515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework called HIVE, to systematically evaluate the impact of hallucinations on a particular task and/or model. The aim is to recognize settings where hallucinations can be beneficial. The paper provides analyses over several datasets and models, both language and multi-modal, showing the benefits of hallucinations in some settings while causing harm in others. The paper also does a study of reasoning chains and their relationship with hallucinations, highlighting how hallucinations can help with reasoning chain stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper attacks an important problem in LLMs. Hallucinations have a predominantly negative reputation in the field, and the paper aims to show how they can, in fact, be useful in some cases.\n2. The paper does an extensive analysis of several different models as well as datasets, both language and multi-modal models, which is a great way to study overarching trends in when hallucinations can be helpful.\n3. The paper is well written and was easy to follow."}, "weaknesses": {"value": "1. While I enjoyed the overall analysis of several tasks and models and whether hallucinations benefit them or not, the framework itself feels restrictive to me. It explicitly focuses on adding a 'caption', which is a hallucination (or not a hallucination), to see the impact of hallucinations on the task. This is clearly only one way to see how hallucinations in model generation can help with eventual performance. For example, another framework could be about studying the reasoning trace of models, identifying hallucinations, and studying how their presence impacts downstream performance.\n2. I'm not entirely convinced the variations are not due to just prompt sensitivity. Lack of study of the variance makes me doubt the conclusions. In fact, I disagree with the claim in the experiment setup that 'identical prompts, temperature, and token budget' ensures fair comparison. Just the choice of the playground for comparison, even though identical for everyone, can implicitly favor one behavior over others (https://proceedings.mlr.press/v279/ganesh25a.html). It is important to vary the prompts, the temperature, and the token budget, and see whether the trends of certain tasks or models benefiting from hallucinations actually persist."}, "questions": {"value": "1. Despite the discussions in (4) (line 292), it's still unclear to me why the 'benefits' of hallucination are dependent on the model so much. I would expect that if a task benefits from 'creative thinking', it should benefit most models. What exactly do authors mean by 'models with stronger hallucination–handling ability' and which models are these?\n2. Is 'hallucinations' really the correct term for the phenomenon discussed here? The paper uses the following definition of hallucinations in the introduction: 'information inconsistent with the given input'. But it seems to me that the motivation isn't to allow inconsistent or wrong information, but just new information that might not be verifiable, given the input. I understand the choice to use the term 'hallucinations' in the title, since that is the term accepted more widely in the community and thus is important for the paper's visibility. But I'm curious to hear if the authors think it is still the right choice for the rest of the paper, or maybe they would have preferred a different term or definition (there is a lot of work on trying to define 'hallucinations' and discussion of other similar terms, for example - https://aclanthology.org/2024.emnlp-main.375/)? \n2. Comment: Table 1 markers for how much performance has increased or decreased are incorrect (GPT-3.5 AntiCP2, Claude-3 Sonnet multiple datasets)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8UhBVxQwOm", "forum": "AhEgdxbtXH", "replyto": "AhEgdxbtXH", "signatures": ["ICLR.cc/2026/Conference/Submission468/Reviewer_hgFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission468/Reviewer_hgFn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091204069, "cdate": 1762091204069, "tmdate": 1762915526371, "mdate": 1762915526371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}