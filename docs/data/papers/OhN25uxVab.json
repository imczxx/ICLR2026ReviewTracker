{"id": "OhN25uxVab", "number": 12790, "cdate": 1758210330068, "mdate": 1759897486295, "content": {"title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes", "abstract": "Do large language models (LLMs) anticipate when they will answer correctly?  To study this, we extract activations after a question is read but before any tokens are generated, and train linear probes to predict whether the model’s forthcoming answer will be correct. Across three open‐source model families ranging from 7 to 70 billion parameters, projections on this “in-advance correctness direction’’ trained on generic trivia questions predict success in distribution and on diverse out‐of‐distribution knowledge datasets, indicating a deeper signal than dataset-specific spurious features, and outperforming black‐box baselines and verbalised predicted confidence. Predictive power saturates in intermediate layers and, notably, generalisation falters on questions requiring mathematical reasoning. Moreover, for models responding “I don’t know’’, doing so strongly correlates with the probe score, indicating that the same direction also captures confidence. By complementing previous results on truthfulness and other behaviours obtained with probes and sparse auto-encoders, our work contributes essential findings to elucidate LLM internals.", "tldr": "By reading only question-time activations, we show that a tiny linear probe predicts if the next answer will be correct for certain questions on a wide range of families and model sizes.", "keywords": ["LLMs", "linear probes", "activations", "correctness", "confidence", "uncertainty", "mechanistic interpretability", "probing", "generalization", "direction"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3804f38460f217a899ec6beddbc4c4b035bd035.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a simple method which identifies a ``correctness'' vector as the difference between the average representations of incorrect and correct answers, projecting new problems' representations onto the computed vector to extract the LLMs learned evaluation of its own answer's correctness on the problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper includes sufficient detail to understand the complete experimental and methodological design\n2. The text is in general clearly written."}, "weaknesses": {"value": "1. Experimental setting: datasets used are mostly synthetic and private. Weak justification was provided for the exclusion of the typically included datasets.\n2. Lack of clarity in the description of experiments and results. Figures 3 and 4, in particular, are lacking. Figure 3 is difficult to interpret without an idea of how the baselines perform. Figure 4 is difficult to understand given the hazy decision rule. Table 3 also has some strange values which go unaddressed. Table 3 would benefit greatly from a baseline LLM performance on the immediate task (or a numerically equivalent methodological baseline which naively predicts ``Correct'' at all times). \n3. Lack of explicit motivation or downstream design: the text does not go far to motivate the work. In the same vein, the experimental design is such that the method's downstream utility is hard to gauge: a decision rule, or a process for finding a decision rule, is not described. Instead, results are aggregated for all possible thresholding rules. Correctness prediction is a 2nd-degree task, in that it is a prediction on a prediction. However, its utility in improving its 1st-degree task (Question answering) is not discussed."}, "questions": {"value": "1. Figure 4 demonstrates the distributions of your three classes on their correctness scores. We see that, particularly in the 7B case, \"Wrong\" is entirely overlapped by the other two classes. This means that, given this observation, for any correctness score, one of the other classes will have a higher probability. This means that the optimal maximum likelihood estimation-based prediction would never predict \"wrong\". In practice, I understand that you are in fact picking a naive score threshold (I suppose two thresholds since you've defined a third ``IDK'' class), and are measuring AUROC over all thresholds. This makes Figure 4 seem strange, however, since score overlap across classes should not be possible with a thresholding decision. Can you comment on the class-distribution interpretation of Figure 4, and clarify how the data in this Figure was compiled?\n2. Some of the datasets you use have very high direct task accuracy on modern LLMs. GSM8K, for example, is typically considered too easy a dataset at this point. I would also expect Cities' accuracy to be extremely high on the immediate task, assuming you only used major cities to construct questions. For these datasets, always predicting ``True'' would appear as having very high accuracy. In fact, such a prediction scheme would allow us to always match downstream performance in the correctness classification task. How do your own reported statistics compare with the direct tasks'? For GSM8K, the only public dataset you use, it seems likely to be much weaker. In your main table, you should indicate vanilla-prompted direct-task performance (or the naive classifier performance we get by always predicting ``True'').  \n3. In truth, semantically, all single-concept free-form generation problems are effectively multiple choice. This is because in practice, when temperature-sampled (i.e. self-consistency), the LLM will only generate a handful of potential answers. Most of the time, with the exception of very hard datasets, one of the LLMs potential answers will be the correct one. This commonly observed behavior makes your dataset-selection justification hard to buy. In addition, even if there were strong random-selection bias, such a bias would be present in each method's observed performance and so the comparison would remain fair. In fact, this would probably be a good test for training robustness. If 5% of the collected ``True'' datapoints were lucky guesses, would this really impact training substantially? Besides, you can always take public datasets and convert them from multiple-choice to free-form generation by simply not providing the multiple choices in your prompt. Can you either provide experimental results on a few new datasets or provide stronger justification of your experimental design?\n4. You present in Figure 3 your method's performance when trained on datasets other than TriviaQA. How does this compare to the assessor?\n5. You have a massive volume of training data, which is all high-dimensional. Why not use a standard MLP classifier as an assessor instead of logistic regression? This seems the perfect setting for a softmaxxed MLP. \n6. While the work is not expressly motivated in the text, motivation for such a correctness prediction seems clear: we can estimate answer-verification a priori in order to develop schemes to improve post-training performance. While I understand that this verification scheme is not in scope, it would be trivial to test your method's downstream utility on a binary task such as True-or-False questions, where if the classification is categorized by your method as incorrect then the true answer must be the single alternative class. Effectively, you could procedurally generate the ``True'' class for each question and test whether this answer is correct, making your method immediately applicable to the task. I'd like to see this on something like FOLIO, which is True/Unknown/False (you can use your \"IDK\" for maybe, or just cut out all the \"Unknown\"-labelled problems from the dataset). \n7. In Table 3, you often report sub-random performances (less than 0.5). What is happening in the models to produce such class inversions? \n8. I don't understand why you don't commit to a single decision boundary. In training, the identification of an optimal boundary should be simple to find. In truth, this method is unusable without committing to a boundary, and your experimental results are all weakened and made less interpretable by requiring an AUROC measurement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DxCLTySZ5Q", "forum": "OhN25uxVab", "replyto": "OhN25uxVab", "signatures": ["ICLR.cc/2026/Conference/Submission12790/Reviewer_vgab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12790/Reviewer_vgab"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576078087, "cdate": 1761576078087, "tmdate": 1762923597672, "mdate": 1762923597672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tries to answer the question \"Do LLMs know if they're gonna answer it right, before generating any response and after a question is read?\" They extract residual stream activations after processing a question and train linear probes to predict answer correctness. They first identify the best layer (which is usually a middle layer) and then use train sets to identify the correctness vector in that layer. They show that training on TriviaQA generalizes to 4 other custom data. Their work mostly contributes to mechanistic interpretability by revealing that LLMs embed a latent correctness signal."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The empirical results on the evaluated benchmarks show that their method is substantially better than baselines and especially the confidence scores generated by the model itself. \n2. They evaluate on a set of LLMs with varying sizes and architectures  (6 models from 3 families). The results are consistent across all models. \n3. A qualitative analysis that provides more insights into error modes and why model's own confidence is not robust (e.g., A: 1961, Correct: 1962). The observation that responses containing \"i don't know\" are captured by their correctness direction further confirms the validity of the correctness direction."}, "weaknesses": {"value": "Although I find the conclusions interesting and potentially impactful, I think several methodological and conceptual aspects could be improved or clarified\n\n1. The paper fixes a single “most discriminative” layer (found using TriviaQA) for all datasets and analyses, which is not justified. While this is stated in limitations, adding an analysis on using layers identified from each dataset will be interesting. Given that computational costs are stated being few minutes on CPU, it seems feasible to repeat Figure 3 experiments using dataset-specific optimal layers (or even Figure 2 with more datasets instead of TriviaQA).\nLine 247: \n> \"which mitigates the risk of discovering an activation direction tied to features merely correlated with model success rather than model's internal correctness prediction. \n* 1.1 This is neither theoretically nor empirically justified. Why should there exist a layer that is responsible for model's internal correctness prediction? Are there any theoretical explanations or previous work (other than the empirical results) justifying this?\n\n2. The proposed method is barely better than random chance on GSM8k. While the authors acknowledge this, there is no reasonable explanation. Is this a problem with all reasoning tasks or only mathematical reasoning? Adding additional reasoning tasks to the experiments might help answer this. \n\n3. The \"correctness direction\" may inadvertently capture memorization rather than self-correctness. All datasets with high performance (TriviaQA and custom datasets) require factual recall that likely overlaps with training data, where as GSM8K (which has a low performance) requires reasoning and likely contains fewer memorized items. The performance drop on GSM8K could therefore be because of absence of memorized associations rather than the absence of correctness awareness. Evaluating and discussing memorization (and potential contamination) effects would strengthen the paper and the claim that such layer exists."}, "questions": {"value": "Re weakness #2 on low performance on GSK8K: One interpretation that I can think about is trivia-style questions like TriviaQA involve retrieving facts, while reasoning tasks like GSM8K require intermediate computation before the model realizes if it can get it correct or not. What do authors think about this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qaimLaqbfJ", "forum": "OhN25uxVab", "replyto": "OhN25uxVab", "signatures": ["ICLR.cc/2026/Conference/Submission12790/Reviewer_xEzz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12790/Reviewer_xEzz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771793384, "cdate": 1761771793384, "tmdate": 1762923597287, "mdate": 1762923597287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether large language models (LLMs) already \"know\" the correctness of their answers within their internal activations before responding. The study finds that by analyzing the model's activation state after reading a question using a simple linear probe, it is possible to predict the accuracy of its answer in advance for knowledge-based questions. Furthermore, this method demonstrates superior generalization performance compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Approach: The research focuses on predicting answer correctness solely from internal model activations before any tokens are generated. This differs from most studies that detect hallucinations or errors after generation, offering a new perspective on understanding the model's internal cognitive processes.\n2. Simple and Effective Method: The study achieves results using only a simple linear probe (calculating the mean difference between correct/incorrect activations). This demonstrates that the \"correctness\" signal is linearly separable within the model, representing a clear and fundamental representation that doesn't necessarily require a complex classifier.\n3. Strong Generalization: A probe trained on a general knowledge base (TriviaQA) can effectively generalize to other unseen knowledge domains. Its performance significantly surpasses black-box evaluators and model self-consistency methods, proving that internal activations contain unique and valuable signals."}, "weaknesses": {"value": "1. Narrow Scope of Application: The probe's core capability is limited to knowledge-retrieval questions. The paper also explicitly states the method is almost ineffective for mathematical reasoning tasks (GSM8K) that require multi-step computation, indicating this \"pre-cognition\" ability is highly limited.\n2. Inconsistent Signal Across Layers: The appendix (e.g., Figure 9) shows that the signal peak for different task types appears in different model layers. This suggests the model may lack a unified \"self-awareness\" center, making the practical deployment of a single, simple probe unstable.\n3. Oversimplified Definition of Correctness: The study treats correctness as a binary label and operates in a deterministic (zero-temperature) setting. This ignores the stochasticity of answers and the ambiguity of correctness in real-world scenarios."}, "questions": {"value": "1. Why does it fail on reasoning? This is the most critical question. Is it because the correctness signal for reasoning only emerges during the generation process, or is this type of signal inherently non-linear? This points to an important direction for future research.\n2. Practical value of \"white-box\" access? This method requires access to internal model activations and necessitates training a probe for a specific model and layer. In practical deployment, what are its main advantages in terms of computational overhead and maintenance costs compared to post-generation detection methods? Furthermore, since the main experiment only proves its generalization is significantly better than the baseline, but poorer performance on the corresponding datasets, does this also imply the method might just be a mediocre choice rather than the optimal solution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QGDczHHmcc", "forum": "OhN25uxVab", "replyto": "OhN25uxVab", "signatures": ["ICLR.cc/2026/Conference/Submission12790/Reviewer_BTu1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12790/Reviewer_BTu1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888996477, "cdate": 1761888996477, "tmdate": 1762923596903, "mdate": 1762923596903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies LLM probing (linear probing) on the first generation token, i.e., before the answer is generated. The experiments are done for various LLMs. The authors study generalization and obtain decent performance for TriviaQA and some simplistic synthetic factual QA datasets, but esp. transfer seems to fail for the mathematical reasoning data. The authors also show some qualitative examples (e.g., questions where the LLM answers IDK in the extreme of the correctness direction)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- LLM probing is an interesting topic \n- The paper is well-written\n- The missing transfer to reasoning data is an interesting observation, but no more analysis is provided. It could also be more generally that it's data different from factual QA."}, "weaknesses": {"value": "- Given that verbalized confidence is known to be uncalibrated it's a very weak baseline. \n- In terms of factual data, where the answers are short, the question-only setting does not seem to have considerable efficiency advantage, compared to the one with answer generation.\n- The paper completely misses critically-related work. There are other papers considering the same kind of probing: [1-5]. Those papers show some very similar results.\n- I consider the provided, very simplistic experiments not very novel/innovative, esp. in the light of the related work. Overall, the insights are in my opinion not enough to justify presentation at ICLR.\n\n----------\n\n[1] Snyder et al. \"On early detection of hallucinations in factual question answering.\" Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024.\n\n[2] Gottesman et al. \"Estimating knowledge in large language models without generating a single token.\" arXiv preprint arXiv:2406.12673 (2024).\n\n[3] Wang et al. \"Hidden question representations tell non-factuality within and across large language models.\" arXiv e-prints (2024): arXiv-2406\n\n[4] Ji et al. \"Llm internal states reveal hallucination risk faced with a query.\" arXiv preprint arXiv:2407.03282 (2024)\n\n[5] Slobodkin et al. The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models, EMNLP'23"}, "questions": {"value": "-----"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kvFqEHJhUI", "forum": "OhN25uxVab", "replyto": "OhN25uxVab", "signatures": ["ICLR.cc/2026/Conference/Submission12790/Reviewer_DabN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12790/Reviewer_DabN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917442299, "cdate": 1761917442299, "tmdate": 1762923596077, "mdate": 1762923596077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}