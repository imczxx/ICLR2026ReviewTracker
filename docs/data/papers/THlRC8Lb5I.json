{"id": "THlRC8Lb5I", "number": 10989, "cdate": 1758186287692, "mdate": 1759897616081, "content": {"title": "An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs", "abstract": "With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.  Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to $\\textit{implicitly}$ prolong output, and fail to directly maximize the output token length as an $\\textit{explicit}$ optimization objective, lacking stability and controllability. To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images. Specifically, we first perform $\\textit{adversarial prompt search}$, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct $\\textit{vision-aligned perturbation optimization}$ to craft adversarial examples on input images, maximizing the similarity between the perturbed image’s visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.", "tldr": "", "keywords": ["Vision Language Model", "Adversarial Attack", "Verbose Outputs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48f4421e5fc5a1d36f4b2b2a13d04a30b814e2bd.pdf", "supplementary_material": "/attachment/70be2398e67b31139b9f50210d2440f9242126fa.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel Verbose-Text Induction Attack (VTIA) targeting Vision-Language Models (VLMs). The attack aims to inject imperceptible perturbations into images so that the target VLMs generate excessively long (verbose) outputs, thereby increasing inference time, energy use, and token cost. Experiments on four representative VLMs (BLIP2, InstructBLIP, LLaVA, and Qwen2-VL) show that VTIA dramatically increases output lengths (up to 1000+ tokens) while maintaining high perceptual similarity (low LPIPS)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel attack objective.  The paper directly maximizes the number of output tokens as the attack goal, providing a more stable and controllable objective compared to prior methods.\n2. Effective two-stage decoupled attack framework. The separation of prompt-level RL optimization and image-level perturbation optimization is technically sound.\n3. State-of-the-art results.  The attack achieves state-of-the-art performance, significantly outperforming baseline methods across four VLMs."}, "weaknesses": {"value": "1.  Evaluation is insufficient. The method is evaluated on only 100 images randomly selected from MS-COCO. This sample size is too small; at least 1,000 images should be used for a more reliable assessment. Additionally, evaluation across multiple datasets is necessary to demonstrate the method’s generalizability.\n2. Ablation studies lack consistency. Some experiments are conducted on four VLMs, while others are limited to only two, making it difficult to fairly assess the contributions of each component.\n3. Insufficient comparisons. The paper presents comparison results only with verbose images (Ref. 1) and does not include comparisons using verbose samples (Ref. 2).\nRef 1: Inducing high energy-latency of large vision-language models with verbose images. arXiv preprint arXiv:2401.11170, 2024a.\nRef 2: Energy-latency manipulation of multi-modal large language models via verbose samples. arXiv preprint arXiv:2404.16557, 2024b.\n4. Limited discussion of real-world applicability. The attack is demonstrated in a controlled setting, but the paper lacks discussion of its feasibility under more restrictive black-box conditions or against deployed VLM services with additional safeguards, such as input filtering.\n5. Minor issue: The equation in line 161 (the last line of page 3) appears to be incorrect and should be revised."}, "questions": {"value": "1. Visual interpretation and attribution analysis. To better reveal the mechanisms behind the verbose-text induction attack, the authors should provide visual interpretations and token–region attribution analyses that highlight which image regions most strongly drive generation of long/verbose sequences. Such analyses will (1) offer mechanistic insight into why certain perturbations lengthen outputs, (2) help verify that the attack targets semantic content rather than spurious features, and (3) increase reproducibility and trust in the results.\n2. Expanded performance metrics. Please report more evaluation metrics, such as inference latency and captioning quality, to provide a more comprehensive assessment of the method’s effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JVuVsRgWFf", "forum": "THlRC8Lb5I", "replyto": "THlRC8Lb5I", "signatures": ["ICLR.cc/2026/Conference/Submission10989/Reviewer_vrSH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10989/Reviewer_vrSH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381770473, "cdate": 1761381770473, "tmdate": 1762922179054, "mdate": 1762922179054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel attack method named VTIA, Verbose-Text Induction Attack. The goal is to craft an adversarial image that appears benign but, when fed into a VLM, induces the model to generate an extremely long and low-information text response to increase the cost."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The two-stage (search-then-align) methodological framework is clear and easy to understand.\n\n- The method successfully induced the target open-source models to generate maximum-length tokens, demonstrating technical feasibility in the controlled setting."}, "weaknesses": {"value": "- I cannot understand the fundamental attack scenario.\nThe paper claims the attack increases \"user costs\", which makes no sense if the user is the attacker, as they would just be increasing their own costs. This is completely different from general adversarial attacks (general adversarial attacks assue the user is the attacker).\n If the paper suggests a DoS attack against the service provider, this is an inefficient vector: an attacker could simply spam the API with normal requests to achieve the same goal much more easily.\n\n- The work completely ignores simpler and more direct text-based attack vectors, such as using prompt engineering or **jailbreaking** to ask the LLM component, as the authors claim that \"recent VLM relys on LLM\". So the paper fails to compare its complex image-based attack against this obvious text-based baseline, undermining the necessity of the proposed method.\n\n- The method has extremely low practical relevance because it relies on a \"gray-box\" threat model.\nThe attack requires access to the internal parameters of the visual encoder and intermediate modules, which is impossible for the commercial, black-box models (like GPT-4V, Gemini) that actually operate on a \"per-token\" cost basis.\nThe paper does not evaluate the transferability of the attack, further limiting its application to real-world scenarios."}, "questions": {"value": "- Can you clarify a realistic and compelling attack scenario where an attacker must use a complex adversarial image, rather than simpler alternatives like spamming normal requests or using malicious text prompts?\n\n- Why did the paper not include a comparison against the most obvious baseline: using text-based prompt engineering to induce verbose outputs from the LLM component?\n\n- Given that this attack requires gray-box access, how can it be applied to the real-world, black-box commercial VLMs that are the primary examples of token-based billing? What is the practical security implication if it cannot be applied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1e9bpd0d6V", "forum": "THlRC8Lb5I", "replyto": "THlRC8Lb5I", "signatures": ["ICLR.cc/2026/Conference/Submission10989/Reviewer_1hH7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10989/Reviewer_1hH7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551225142, "cdate": 1761551225142, "tmdate": 1762922178718, "mdate": 1762922178718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Verbose-Text Induction Attack (VTIA) that injects adversarial perturbations into images to induce vision-language models (VLMs) to generate excessively long textual outputs. The method explicitly optimizes for output token length rather than indirectly delaying the EOS token. VTIA uses a two-stage process: (1) adversarial prompt search via reinforcement learning to identify prompts that trigger verbosity, and (2) vision-aligned perturbation optimization to align image embeddings with adversarial prompt embeddings. Experiments on several popular VLMs show that VTIA can effectively increase output verbosity and demonstrate strong transferability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. For originality, this paper proposes a novel attack objective, which focuses on generation length and verbosity as a vulnerability metric. This is original and highlights an overlooked efficiency issue in VLM deployment.\n\n2. For clarity, the proposed two-stage pipeline combining prompt search and image-space optimization is well-formulated and intuitive.\n\n3. For quality, the evaluations across multiple VLMs provide a convincing demonstration of the generality of the method."}, "weaknesses": {"value": "1. There is a lack of comparison with representative baselines. The paper does not compare against existing multimodal attack frameworks such as VLAttack, which limits understanding of its relative effectiveness.\n\n2. The focused problem setting is incremental. Adversarial attacks on VLMs have been extensively studied, and the contribution mainly reorients the objective toward verbosity, which may be viewed as a narrow extension rather than a fundamentally new attack paradigm.\n\n3. The quality of the generated text is missing. While the method maximizes output length, there is no quantitative analysis of the informational or semantic quality of the generated verbose text, which is important for assessing real-world impact.\n\n4. There is only limited discussion on practicality.1 It remains unclear how such verbose induction affects model usability or downstream performance in realistic scenarios."}, "questions": {"value": "1. Could the authors include a comparison with VLAttack or other vision-language adversarial methods to contextualize their improvement?\n\n2. How is “verbose” output quality measured—could metrics like perplexity, redundancy, or semantic coherence be reported?\n\n3. What is the computational overhead of the reinforcement learning–based prompt search stage?\n\n4. Would combining verbosity objectives with traditional adversarial goals (e.g., misclassification or hallucination) yield stronger or more general attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J9jUJa4TBB", "forum": "THlRC8Lb5I", "replyto": "THlRC8Lb5I", "signatures": ["ICLR.cc/2026/Conference/Submission10989/Reviewer_KcUd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10989/Reviewer_KcUd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963895889, "cdate": 1761963895889, "tmdate": 1762922178365, "mdate": 1762922178365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Verbose-Text Induction Attack (VTIA), a novel framework that combines reinforcement learning with vision-aligned perturbation optimization. It first generates an adversarial prompt embedding using reinforcement learning and then optimizes visual perturbations to align the image embeddings with this adversarial prompt, effectively inducing the Vision-Language Model (VLM) to generate excessively long responses. The authors evaluate VTIA on several models, including Blip2, InstructBlip, LLaVA, and Qwen2-VL. The results demonstrate VTIA's significant advantage in terms of the number of tokens generated and the rate of extra-long responses. Ablation studies further analyze the impact of individual components and hyperparameters on the attack's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  The paper is well-organized and clearly structured.\n- The paper proposes the VTIA attack framework, a novel methodology that integrates reinforcement learning with vision-aligned perturbation optimization. Its two-stage decoupled design ensures that the second phase operates completely independently of the target VLM’s textual module, thereby circumventing the significant computational overhead associated with repeatedly invoking large LLMs during iterative optimization.\n- Experiments are conducted on multiple models such as Blip2, InstructBlip, LLaVA, and Qwen2-VL, demonstrating the superiority of the VTIA framework. Additionally, extensive ablation studies are designed to validate and analyze the impact of each component and associated hyperparameters on the method's performance."}, "weaknesses": {"value": "- **Limited Evaluation Scope**: Although experiments are conducted on four models, the evaluation could be further strengthened by including a broader and more diverse set of modern VLMs (e.g., LLaVA-NEXT, Qwen3-VL) to better demonstrate generalizability. Moreover, the use of only 100 randomly selected images from the MSCOCO dataset constitutes a relatively small sample size, raising concerns about the robustness and real-world applicability of the proposed method. It is also suggested to evaluate the attack on additional tasks or benchmarks (e.g., visual question answering) to verify its generalizability across modalities and task settings.\n    \n- **Ambiguous Notation**: In Equation (8), the term $L_{cos}$ is used without definition. It remains unclear whether this corresponds to $L_{sim}$ from Equation (6).\n    \n- **Lack of Robustness Analysis**: The paper does not assess the robustness of the proposed method against common defense strategies. It is recommended to analyze its effectiveness under mainstream defenses such as smoothing-based methods (e.g., Gaussian, average, and median smoothing), GAN-based defenses (e.g., NRP [1]), and diffusion-based approaches (e.g., DiffPure [2]).\n\n\n- **Max Token Limitation**: The maximum token length is set to 1024, and most samples reach this limit. The authors should consider increasing the maximum token length to better showcase the true effectiveness of the adversarial attack and avoid potential truncation effects.\n\n[1] M Naseer et al. A self-supervised approach for adversarial robustness. In CVPR 2020\n\n[2]W Nie et al. Diffusion models for adversarial purification. In ICML 2022"}, "questions": {"value": "- **Transferability**: How transferable is the attack? Specifically, does the adversarial perturbation remain effective when applied to different images or prompts?\n- **Semantic Interpretation**: Does the adversarial prompt convey meaningful semantic information? It would be helpful to include a visualization of the generated prompt to illustrate its interpretability and semantic coherence.\n- **Text-Image Consistency**:\nWhat is the embedding similarity between the attacked image and the generated text? Does the generated text deviate significantly from the image semantics? Additionally, it is suggested to provide quantitative metrics of text quality (e.g., BLEU, perplexity, or GPT score) to evaluate robustness against similarity-based or text-quality-based detection methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t6qltydQ4w", "forum": "THlRC8Lb5I", "replyto": "THlRC8Lb5I", "signatures": ["ICLR.cc/2026/Conference/Submission10989/Reviewer_mpXp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10989/Reviewer_mpXp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762534988223, "cdate": 1762534988223, "tmdate": 1762922177958, "mdate": 1762922177958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}