{"id": "pz3WCd3HtG", "number": 18595, "cdate": 1758289348716, "mdate": 1759897093403, "content": {"title": "SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated Refusal Steering", "abstract": "Large Audio–Language Models (LALMs) are becoming essential as a powerful multimodal backbone for real-world applications. However, recent studies show that audio inputs can more easily elicit harmful responses than text, exposing new risks toward deployment. While safety alignment has made initial advances in LLMs and Large Vision–Language Models (LVLMs), we find that vanilla adaptation of these approaches to LALMs faces two key limitations: 1) LLM-based steering fails under audio input due to the large distributional gap between activations, and 2) prompt-based defenses induce over-refusals on benign-speech queries. To address these challenges, we propose \\textbf{S}afe-\\textbf{A}blated \\textbf{R}efusal \\textbf{Steer}ing (SARSteer), the first inference-time defense framework for LALMs. Specifically, SARSteer leverages text-derived refusal steering to enforce rejection without manipulating audio inputs and introduces decomposed safe-space ablation to mitigate over-refusal. Extensive experiments demonstrate that SARSteer significantly improves harmful-query refusal while preserving benign responses, establishing a principled step toward safety alignment in LALMs.", "tldr": "", "keywords": ["AI Safety", "Safety Alignment", "Large Audio Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/329a320013bf381afe94c0a2be441223d038f6d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors identify that existing safety alignment techniques developed for text-based LLMs and vision-language models (LVLMs) fail to generalize effectively to the audio modality. They highlight two key challenges: (1) a large distributional gap in model activations between text and audio inputs, which causes traditional activation steering to break down, and (2) prompt-based defenses that often lead to excessive refusals of benign queries. To address these issues, the paper proposes SARSteer, the first inference-time defense framework designed specifically for large audio-language models (LALMs). The approach integrates text-derived refusal steering with a decomposed safe-space ablation mechanism to enhance harmful-query rejection while preserving benign responses. Extensive experiments show that SARSteer substantially improves refusal of harmful audio queries without sacrificing general utility or increasing over-refusals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized and clearly written, presenting a coherent progression from problem identification to methodology and evaluation.\n2. It introduces the first inference-time safety alignment framework for Large Audio-Language Models (LALMs), offering a practical and training-free approach to enhance model safety.\n3. The empirical evaluation is comprehensive, covering multiple benchmarks for both harmful and general-use scenarios, and demonstrates strong validation of the proposed method."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "1. The experiments primarily evaluate on Qwen2-Audio and Kimi-Audio. It remains unclear whether the proposed approach generalizes to other architectures or proprietary models. Including results on additional LALMs would better support claims of general applicability.\n2. While the paper emphasizes inference-time efficiency, including at least one fine-tuning-based baseline would provide a stronger comparison point to highlight the trade-off between efficiency and effectiveness.\n3. The evaluation employs a matching-based method for computing refusal rates but an LLM-based method for attack success rates. Could the authors clarify the rationale behind using different evaluation paradigms for these two metrics?\n4. The results on AdvBench-Audio (Table 2) show unusually low harmfulness scores. It would be helpful to discuss possible causes.\n5. The method samples 100 harmful–safe paired queries for alignment. Is the method robust to different random seeds or sample selections? Reporting variance across runs could help assess the stability and reproducibility of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "akot2hnwjT", "forum": "pz3WCd3HtG", "replyto": "pz3WCd3HtG", "signatures": ["ICLR.cc/2026/Conference/Submission18595/Reviewer_mRwg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18595/Reviewer_mRwg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672445385, "cdate": 1761672445385, "tmdate": 1762928313133, "mdate": 1762928313133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of achieving safety alignment in large-scale audio-language models (LALMs), which often fail to block harmful queries or over-refuse harmless queries when extending text-based defence techniques to audio inputs. To resolve this, this paper proposes SARSteer (Safe-Ablated Refusal Steering), an inference-time alignment framework. This approach combines text-based refusal steering (capturing safe semantics from text refusals) with safe region ablation (removing harmless semantic subspaces via PCA). The paper emphasizes that this approach operates entirely at inference time, avoiding costly fine-tuning while maintaining cross-modal generalisation.\nThe paper conducts comprehensive experiments on benchmarks such as AdvBench-Audio, Figstep, and SORRY-Bench, utilizing LALM backbones like Qwen2-Audio and Kimi-Audio. This paper reports that SARSteer achieves a superior safety-utility balance compared to existing techniques by improving the accuracy of rejecting harmful queries while maintaining or enhancing the usefulness of harmless responses. Furthermore, quantitative metrics and removal experiments demonstrate that SARSteer provides a robust solution for inference-time safety alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates strengths in its contributions to the field of LALM safety alignment. It introduces SARSteer, a novel inference-time defense mechanism tailored for Large Audio Language Models (LALMs), effectively adapting text-derived refusal steering and safe-ablated refusal techniques to tackle audio-specific safety challenges, as detailed in Sections 4.1 and 4.2. The development of a harmful-safe paired audio dataset (Section 3.2) and the balanced refusal rate (BRR) metric (Section 3.4) provide practical tools for evaluating and improving safety, addressing a gap in existing multimodal benchmarks. Additionally, the empirical analysis, including t-SNE visualizations (Figure 2, Section 3.3) and ablation studies (Section 5.3), offers clear insights into the effectiveness of the proposed approach, supported by measurable improvements in ASR and BRR across Qwen2-Audio and Kimi-Audio models."}, "weaknesses": {"value": "This paper proposes SARSteer, a framework for protecting large-scale audio language models (LALMs) through text-based adversarial induction and decomposed safe region removal. However, the proposed text-based adversarial induction and safe region removal are highly similar to variations of existing concepts. For instance, activation space steering has been addressed in prior work such as SafeSteer (Ghosh et al., 2025), and PCA-based semantic decomposition was utilized in Representation Surgery (Yang et al., 2023). The novelty of SARSteer lies in its application to the audio modality; however, the theoretical rationale for this application—for instance, why the distributional gap between text and speech makes steering transitions challenging—is not sufficiently presented, thereby weakening its originality.\n\nTherefore, the authors should quantitatively demonstrate how the latent space of the audio encoder structurally differs from that of text, utilizing methods such as centred kernel alignment or mutual information metrics. Currently, this is explained solely through ASR performance on two models and two datasets, and through embedding t-SNE visualizations on the Qwen2-Audio + Figstep dataset case. An evaluation report for additional benchmarking datasets such as Gao et al.(Gao et al., 2024) is required.\n\nFurthermore, there is insufficient explanation of the encoder used to extract these embeddings. This would also lend theoretical support to why text-based vectors fail during steering. Also the insufficient detail provided regarding the encoder, projector, and Perf necessitates additional explanation to enhance clarity.\n\nThis paper demonstrates the potential of text-based steering by achieving LALM protection through steering and a safety-region PCA-based removal method. However, this raises the question of whether steering is possible by simply performing audio-to-text conversion before the input audio enters the audio-only encoder, then feeding the text and prompt into the text encoder. There is a need to clarify instances where text conversion proves challenging.\n\nConsidering the lateness of the research, the speech dataset was constructed using OpenAI TTS, however, quality validation of this newly constructed dataset is essential. Additionally, analysis across multiple languages is required.\n\nThe evaluation method RR(Refusal Rate) was newly devised by emulating the matching-based approach of LVLM research; however, relying solely on identified keywords does not appear reasonable for assessing whether a response constitutes a genuine refusal. Judgement is required regarding the full context of the response whether it is a safe refusal or an avoidance answer.\n\nReferences\n\n1. Ghosh, Shaona, et al. \"SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs.\" arXiv preprint arXiv:2506.04250 (2025).\n2. Yang, Enneng, et al. \"Representation surgery for multi-task model merging.\" arXiv preprint arXiv:2402.02705 (2024).\n3. Gao, Kuofeng, et al. \"Benchmarking open-ended audio dialogue understanding for large audio-language models.\" arXiv preprint arXiv:2412.05167 (2024)."}, "questions": {"value": "Major Comments\n- Why not consider a simpler speech-to-text conversion followed by text-based steering instead of text-derived vectors, and clarify specific cases where text conversion is challenging.\n- Section 3 lacks detailed explanations of the encoder and projector components—provide specifics on their architecture and role in the pipeline.\n- Is the text encoder a transformer decoder, or does it use a GPT-style tokenizer? Clarify it.\n- Is the projector specifically designed for audio-to-text alignment? If so, elaborate on its mechanism.\n- The evidence for text-audio embedding space differences (e.g., t-SNE in Figure 2) could be strengthened by specifying which encoder and projector were used, as this would better support the authors' arguments.\n- A case study demonstrating the proposed Refusal Rate (RR) and Balanced Refusal Rate (BRR) metrics would improve the manuscript. The current matching-based evaluation relying on keywords struggles to assess refusal context—distinguishing true safety refusals from evasive responses remains a risk.\n- Beyond PCA, how would alternative methods like kernel PCA, autoencoders (AE), or canonical correlation analysis (CCA) model the safe space, and what impact would they have on performance, over-refusal, and safety?\n- How was the quality of the OpenAI TTS-generated speech dataset validated? What differences would arise if real human speech were used instead?\n- FSD and AdaShield show competitive ASR performance in Table 1. Can the authors provide Harm_RR and Safe_RR for AdvBenchAudio? Is a Receiver Operating Characteristic (ROC) plot feasible to assess Safe retention rate versus refusal rate?\n\nMinor Comments\n- Add a detailed explanation of ‘Perf’ to clarify its role in the evaluation.\n- Was AdvBench-audio also randomly sampled into 100 pairs like Figsteps for alignment? Provide additional details on the alignment process and summarize evaluation statistics for each dataset.\n- For Table 1, were statistical tests conducted? Clarify this to support the reported results.\n- Is it possible to provide standard deviaion for all results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "69c7yXCvxm", "forum": "pz3WCd3HtG", "replyto": "pz3WCd3HtG", "signatures": ["ICLR.cc/2026/Conference/Submission18595/Reviewer_75A4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18595/Reviewer_75A4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811012048, "cdate": 1761811012048, "tmdate": 1762928312725, "mdate": 1762928312725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SARSteer, an inference-time defense for Large Audio Language Models (LALMs) that aims to refuse harmful speech inputs while keeping helpful behavior on benign queries. The authors show that direct transfers of text-based activation steering fail on audio because harmful and safe speech occupy very different hidden spaces, and that prompt defenses over-refuse borderline safe inputs. SARSteer instead builds a refusal vector from textual refusal prompts and removes components aligned with a PCA-estimated safe subspace. The method improves refusal on harmful audio while maintaining utility, tested on Qwen2-Audio and Kimi-Audio with lower attack success rates and higher balanced refusal than adapted baselines. The paper also constructs paired harmful-safe audio datasets and uses a balanced refusal metric to better capture the safety-helpfulness trade-off."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It addresses an important issue: attacks on speech inputs are often easier than those on text, so LALMs need defenses that account for the audio modality.\n- The proposed method is efficient because it does not require fine-tuning.\n- The PCA-based decomposition of the steering vector is clear and interpretable, as it explicitly removes components aligned with safe activations before inference.\n- The evaluations comprehensively assess harmfulness, helpfulness, and general utility across multiple audio safety datasets and a general benchmark, with ablations, comparisons to adapted baselines, and sensitivity analyses for key hyperparameters."}, "weaknesses": {"value": "- Harmful prompts are generated using TTS, and their safe counterparts are created through LLM purification and then synthesized. However, this process may fail to capture natural speech patterns such as disfluency, accents, speaker overlaps, or background noise.\n- The paper does not clarify how the LLM-as-a-judge performs. It does not specify which model was used, nor does it provide evidence that its judgments align well with human evaluations.\n- Although several open-source LALMs exist, only two are tested in all experiments, making it difficult to assess whether the proposed method is truly general and model-agnostic."}, "questions": {"value": "- What decoding strategies are used for Qwen2-Audio and Kimi-Audio? Are there any random or sampling components that might affect the experimental results?\n- There is a typo in Section 4: \"correpsonding\" should be \"corresponding.\"\n- The citation format appears slightly different from other ICLR papers. It may be worth checking if it meets the conference style requirements.\n- Some equations are missing numbers. They should be numbered consistently throughout the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sieOjpK4G6", "forum": "pz3WCd3HtG", "replyto": "pz3WCd3HtG", "signatures": ["ICLR.cc/2026/Conference/Submission18595/Reviewer_thXK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18595/Reviewer_thXK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965256392, "cdate": 1761965256392, "tmdate": 1762928312210, "mdate": 1762928312210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the underexplored problem of safety alignment in LALMs. The authors identify the ineffectiveness of vanilla activation steering for audio inputs and the over-refusal problem in prompt-based methods. To address these issues, they propose SARSteer, an inference-time defense framework combining text-derived refusal steering and decomposed safe-space ablation to achieve robust safety alignment without compromising model utility. Experimental results demonstrate that SARSteer effectively balances harmful-query refusal and benign-query performance for LALMs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed SARSteer framework presents an interesting inference-time defense that integrates text-derived refusal steering with decomposed safe-space ablation using PCA. The experimental results show that it outperforms the baselines."}, "weaknesses": {"value": "* The presentation lacks clarity, making it difficult to follow the technical details of the proposed method. For instance, in Section 4.1, it is unclear why a refusal text prompt (e.g., “I cannot assist with that.”) contributes effectively to the model’s behavior, particularly when the harmfulness of the input is not predetermined. Similarly, the rationale for choosing PCA, a linear dimensionality reduction technique, over alternatives such as SVD or non-linear factorization methods is not adequately justified. It is also unclear how PCA can reliably disentangle “safe” representations. Without a clearer explanation of the problem formulation and experimental setup, it is challenging to provide a meaningful evaluation of the reported results.\n\n* It would be great to compare the proposed method with the following works:\n\nJin, W., Cao, Y., Su, J., Xue, M., Hao, J., Xu, K., Dong, J. S., & Wang, D. (2025). ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models. arXiv preprint arXiv:2510.26096.\n\nYang, H., Qu, L., Shareghi, E., & Haffari, G. (2025). Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models. arXiv preprint arXiv:2505.19670."}, "questions": {"value": "* It is unclear what does the V2 mean in the ablation study in Sec. 5.3.\n\n* The manuscript would benefit from a major revision to improve clarity and coherence. In particular, the argument structure should be made more explicit, with clear connections between the problem definition, research gaps, methodological choices, and experimental results. Each design decision should be better justified with theoretical reasoning or empirical evidence.\n\n* The citation formatting is frequently incorrect and does not conform to ICLR style. For example, in the first sentence, Tang et al. (2023); should be written as (Tang et al., 2023). In general, when citing multiple works, all references should be grouped within a single set of parentheses and separated by semicolons, e.g., (Tang et al., 2023; Ding et al., 2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oorJMoRWPJ", "forum": "pz3WCd3HtG", "replyto": "pz3WCd3HtG", "signatures": ["ICLR.cc/2026/Conference/Submission18595/Reviewer_dWD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18595/Reviewer_dWD1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002126500, "cdate": 1762002126500, "tmdate": 1762928311778, "mdate": 1762928311778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}