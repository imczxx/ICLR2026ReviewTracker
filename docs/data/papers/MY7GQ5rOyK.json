{"id": "MY7GQ5rOyK", "number": 11481, "cdate": 1758200177354, "mdate": 1759897572820, "content": {"title": "Sim2Act: Robust Simulation-to-Decision Learning via Adversarial Calibration and Group-relative Perturbation", "abstract": "In digital twins, simulation-to-decision has become a cornerstone in mission-critical domains like supply chains and power systems, because it enables safe decision learning in digital worlds without risking real-world deployments. \nHowever, the complex and noisy nature of real-world data calls for robust solutions against distribution shifts and uncertainty. \nExisting methods often fall short: 1) surrogate simulation models tend to be biased in decision-critical regions;  2) policies derived from surrogate models are highly sensitive to perturbations, thus, result in brittle performance. \nTo address these challenges, we propose a novel two-step framework that advances 1) simulation fidelity by adversarial calibration and 2) policy robustness with group-relative perturbations. Our solution enables non-disruptive robustness that stable under perturbation while preserving decision performance.\nWe present intensive experiments on both synthetic and real-world domain datasets, including DataCo, GlobalStore, and OAS, to demonstrate the simulation and decision robustness of our method even in noisy and biased settings.", "tldr": "We propose a two-stage framework that calibrates predictive simulators and learns robust policies via group-relative perturbations, enabling reliable decision-making under distribution shift in offline supply chain scenarios.", "keywords": ["Offline Policy Learning", "Simulator Calibration", "Robust Decision-Making", "Supply Chain Optimization", "Group Relative Preturbation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0236c298926f337c800051eab140754174fa7799.pdf", "supplementary_material": "/attachment/f1429831aa2e31d0b638d11749c89566bfc45b59.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the disparity between simulation and real datasets (i.e. sim2real) where shifts in state distributions create uncertainty and performance degradation using adversarial simulator calibration and a group-relative perturbation mechanism for robust policies. The learned simulator perturbs states where prediction errors are most harmful to the agent, and the agent is optimized under the simulator using a group-relative regret. The experiments show that the proposed method is robust to distribution shifts in logistics data as compared to baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper provides an interesting insight into group-relative advantage and its role in robustness and stabilizing policy gradient updates. \n- The latent-space perturbations used in the adversarial simulator and group-relative perturbations are an interesting idea. It seems reasonable that this way of viewing perturbations would produce more semantically meaningful perturbations with respect to the task."}, "weaknesses": {"value": "#### Notation\n- The paper appears to use reinforcement learning (RL) terminology and ideas (i.e., advantage, state/action distributions), but does not define or discuss them explicitly. Doing so (for instance, by discussing the underlying MDP of the simulator and dataset) would likely yield more thorough theoretical results and greatly aid in mapping this work to prior work.\n\n#### Experiments\n- The table captions should be more descriptive. Particularly, the metrics in Table 1 are not explained.\n- There are no robust baseline methods present, which are important in assessing the effectiveness of robustness work. GPT 3.5 is shown, but it is not clear how or why it is used to solve the tasks.\n- The proposed method underperforms the preceding method (S2D) in the \"Status\" metric in Table 1, but it is not discussed why or what the metric means.\n\n#### Methodology\n- The adversarial calibrator is stated to be inspired by or taken from preceding work [1]. It should be discussed if this is a direct adaptation or a novel iteration, and what challenges present in the original method are addressed in this work.\n- While interesting, the latent-space perturbations do not have a stated advantage or motivation as compared to state perturbations such as PGD [2], which are a well-defined setting in adversarial literature [3]. \n- Equation 8 appears to be a regret term similar to that in adversarial RL [4] that is either counterintuitive or incorrect. Regret terms are minimized, since an effort to maximize regret would result in the minimization of the target reward $\\bar{r}$. When minimizing the loss in Equation 9, under a negative $\\mathcal{S}(s,a)$, Equation 8 would be maximized unless the regret is negated as well.\n\n\n\n[1] Haoyue Bai, Haoyu Wang, Nanxu Gong, Xinyuan Wang, Wangyang Ying, Haifeng Chen, Yanjie Fu: Supply Chain Optimization via Generative Simulation and Iterative Decision Policies. CoRR abs/2507.07355 (2025)\n\n[2] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu: Towards Deep Learning Models Resistant to Adversarial Attacks. ICLR (Poster) 2018\n.\n[3] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane S. Boning, Cho-Jui Hsieh: Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations. NeurIPS 2020\n\n[4] Roman Belaire, Arunesh Sinha, Pradeep Varakantham: On Minimizing Adversarial Counterfactual Error in Adversarial Reinforcement Learning. ICLR 2025"}, "questions": {"value": "- Is the adversarial calibrator different from the cited S2D? \n- Why is GPT 3.5 used as a baseline solver for the task? \n- What is the motivation for using latent perturbations and an encoder-decoder setup?\n- Can the method be adapted to work under state perturbations, i.e. PGD?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3g5WuBw0U6", "forum": "MY7GQ5rOyK", "replyto": "MY7GQ5rOyK", "signatures": ["ICLR.cc/2026/Conference/Submission11481/Reviewer_1iRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11481/Reviewer_1iRH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842002795, "cdate": 1761842002795, "tmdate": 1762922587814, "mdate": 1762922587814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Sim2Act (S2A), a two-stage framework for achieving robust simulation-to-decision (Sim2Dec) learning in digital twins, particularly suited to mission-critical domains such as supply chains and power systems. Sim2Act enhances both components of the pipeline: (1) it improves simulation fidelity through adversarial calibration, and (2) strengthens policy robustness via group-relative perturbations. The overarching goal is to realize non-disruptive robustness, ensuring stable performance under perturbations while preserving decision quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper grounds the Sim2Act framework in practical, high-stakes domains such as supply chains, power grids, and robotics, where inherent noise, uncertainty, and the cost/risk of real-world interaction pose significant challenges. This clear and realistic context enhances the work's credibility and applied relevance.\n\n2. The experimental validation is robust and well-structured, utilizing three distinct real-world supply chain datasets: DataCo, GlobalStore, and OAS. The results consistently demonstrate quantitative gains supporting the paper's claims."}, "weaknesses": {"value": "1. Limited Experimental Scope: The paper claims broad applicability to high-stakes domains like robotics and power grids, but all experiments are confined to discrete-action, logistics-focused supply chain datasets. The generalizability of the method to complex, high-dimensional continuous control problems or systems with non-stationary dynamics remains unproven.\n\n2. Lack of Qualitative Policy Analysis: While the quantitative results are strong, the evaluation lacks depth in analyzing policy behavior. The paper doesn't explore how the calibrated simulator fundamentally changes specific decision trajectories, which actions are avoided, or whether the policy's failure modes are genuinely safer or more interpretable.\n\n3. Notation Issues : (1) The main problem statement (Equation 2) is **ill-typed** because the simulator $\\mathcal{S}(s_t, a_t)$ is formally defined to output a **tuple** of the next state and reward $(\\hat{s}_{t+1}, \\hat{r}_t)$ in line 246. However, the objective function sums $\\mathcal{S}(\\cdot)$ over time, implying it must be a scalar reward. (2) The parameter $\\mathbf{w}$ is introduced in the Method Overview (Section 3.1) in the calibrator notation $\\overline{b}(s,a,w)$ in line 162, but it is not formally defined at that point."}, "questions": {"value": "1. How reliable is the learned covariance estimator $\\mathbf{\\Sigma(s,a)}$ in practice? For instance, does its estimated variance remain stable across different random seeds? Additionally, evaluating only three seeds might be too limited to assess robustness in reinforcement learning settings.\n\n2. The Group-Relative Perturbation technique, by aggregating results over multiple perturbed states, shares a functional similarity with ensemble methods. Would adopting an explicit ensemble approach for the simulator (e.g., training multiple $\\mathcal{S}$ models) yield a superior or more robust estimate of the latent uncertainty $\\mathbf{\\Sigma(s, a)}$ compared to relying on a single simulator's variance output?\n\n3. How does Sim2Act scale to continuous control domains (e.g., MuJoCo or D4RL tasks)? Does the adversarial calibration bottleneck model performance for large state spaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3HF7WJBPsg", "forum": "MY7GQ5rOyK", "replyto": "MY7GQ5rOyK", "signatures": ["ICLR.cc/2026/Conference/Submission11481/Reviewer_91xm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11481/Reviewer_91xm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845847454, "cdate": 1761845847454, "tmdate": 1762922587443, "mdate": 1762922587443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Sim2Act, a two-step framework designed to improve robustness in simulation-to-decision (Sim2Dec) learning. The authors aim to mitigate two central issues: biased surrogate simulators and fragile decision policies under distributional shifts. Their approach combines (1) adversarial calibration to enhance simulator fidelity in decision-critical regions, and (2) group-relative perturbation to improve policy robustness while maintaining performance stability. Experiments are conducted on both synthetic and real-world datasets (DataCo, GlobalStore, and OAS), demonstrating the proposed method’s ability to handle noisy and biased data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important problem of robustness in Sim2Dec learning, which is crucial for digital twin applications and model-based RL under noisy or biased environments.\n\nThe idea of emphasizing simulator errors in decision-critical regions is intuitively meaningful and could inspire future work in coupling model fidelity and policy robustness.\n\nThe paper includes both synthetic and real-world experiments (DataCo, GlobalStore, and OAS), which help demonstrate practical applicability."}, "weaknesses": {"value": "Bridging the gap between simulation accuracy and decision robustness is important for digital twin applications. However, the novelty and empirical strength of the proposed approach appear limited. The adversarial calibration component, while conceptually sound, resembles the mechanism used in Sim2Dec, and the distinction between the two frameworks is not clearly articulated. Section 3.2 in particular reads as a close variant of Sim2Dec’s adversarial training procedure. From the experimental results, Sim2Act shows only marginal improvement over Sim2Dec, suggesting that the contribution is incremental rather than substantially novel.\n\nThe simulator should focus on regions where prediction errors most affect policy actions, which is intuitive and potentially useful. However, the current design, which trains the simulator and policy in separate stages, weakens this argument. The authors assume that prediction errors naturally concentrate in decision-critical areas, yet without joint training, there is no guarantee that simulator inaccuracies indeed lead to different actions. In fact, prediction errors may or may not cause the policy to output distinct decisions, depending on the policy’s sensitivity in specific state regions. Therefore, the crucial factor is whether the policy is sensitive to simulator errors in those states, not merely whether the simulator is inaccurate in those states. The cited reference at line 179 mentions that inaccuracies can compound and degrade policy performance, but it does not support the claim that policy sensitivity aligns with the distribution of simulator prediction errors.\n\nEquation (5) is difficult to interpret. The paper states that the method increases weights for inaccurate $(s,a)$ pairs, yet the weighting term \n$\\bar{b}(s, a;w)$ is defined in an indirect way. It is unclear why this formulation is preferred over more straightforward functions such as squared or exponential error. Moreover, since $\\bar{b}$ depends primarily on the inner product $<s,w_a>$, where $w_a$ is a learnable vector for each action, the relationship between this design and actual prediction error remains ambiguous.\n\nAnother issue arises from the claim that the method works well on noisy datasets. Typically, noisy samples correspond to larger prediction errors, and increasing their weights could amplify noise instead of improving robustness. The paper would benefit from a more thorough analysis or ablation study to demonstrate that the model does not overfit to noise-dominant regions.\n\nSection 3.2.2 is also somewhat confusing. The authors introduce stochasticity on the policy side rather than modeling uncertainty in the environment, but it is not clear why this design captures robustness more effectively. Additionally, in Equation (9), when the reference reward is absent, the authors set $r^∗=0$, which effectively drives $S(s, a)$ toward zero. However, “no reference reward” does not necessarily imply a zero-reward scenario, and this simplification may bias the optimization process. Finally, the description of how ChatGPT-3.5 is used as a policy is vague and needs more concrete explanation."}, "questions": {"value": "- How is Sim2Act fundamentally different from Sim2Dec in terms of both formulation and training procedure?\n\n- Without joint training of the simulator and policy, how can the method ensure that prediction errors truly affect policy actions, rather than occurring in regions irrelevant to decision-making?\n\n- In Equation (5), why not directly use a squared or exponential loss instead of the indirect weighting function $\\bar{b}(s, a;w)$?\n\n- Noisy samples usually have larger prediction errors. Wouldn’t increasing their weights risk amplifying noise and harming generalization?\n\n- How exactly is ChatGPT-3.5 used as a policy in your framework? What role does it play in the training or evaluation process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2o1N6N1gjx", "forum": "MY7GQ5rOyK", "replyto": "MY7GQ5rOyK", "signatures": ["ICLR.cc/2026/Conference/Submission11481/Reviewer_poEg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11481/Reviewer_poEg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899791066, "cdate": 1761899791066, "tmdate": 1762922586975, "mdate": 1762922586975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SIM2ACT, a two-stage sim-to-decision pipeline: (1) adversarial simulator calibration that reweights errors in decision-critical regions and applies a closed-form correction; (2) group-relative decision training that perturbs the simulator’s latent state, then updates the policy with a GRPO-style group-mean–centered objective to favor actions that remain good across the sampled neighborhood. Experiments on supply-chain datasets claim improved robustness metrics and comparable nominal performance, with ablations for each stage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Decision-focused calibration is well motivated: it aims at reducing errors that matter for downstream decisions rather than global fit.\n- The group-relative objective is simple and critic-free, echoing GRPO ideas where a group average serves as the baseline."}, "weaknesses": {"value": "- Positioning vs robust RL is loose. The method is not a two-player min–max robust MDP or RARL; it optimizes relative performance over a sampled ensemble. Direct comparisons to robust MDPs and RARL/EPOpt are missing.\n- Uncertainty modeling is narrow: latent Gaussian noise from the encoder. There is no analysis of distributional misspecification (heavy tails, multimodality) or sensitivity to the number/scale of perturbations\n- Robustness baselines and metrics under true worst-case or tail risk (CVaR/DRO) are abscent"}, "questions": {"value": "- Could your latent-perturbation neighborhood be replaced or augmented with multimodal samplers such as diffusion planners or flow-matching paths to better capture diverse futures? Any preliminary results with Diffuser-style trajectory denoising or Flow Matching-driven latent paths would be valuable.\n- Can you report min-over-group or CVaR@α returns and compare against robust MDP baselines to clarify the robustness claim?\n- How sensitive are results to the latent covariance source and to the number of samples M? Any failures with heavy-tailed noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ayHCVcTgL7", "forum": "MY7GQ5rOyK", "replyto": "MY7GQ5rOyK", "signatures": ["ICLR.cc/2026/Conference/Submission11481/Reviewer_wZnc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11481/Reviewer_wZnc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950491760, "cdate": 1761950491760, "tmdate": 1762922586529, "mdate": 1762922586529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}