{"id": "NbHuzXQT8U", "number": 24599, "cdate": 1758358349646, "mdate": 1763730182692, "content": {"title": "Inverse Linear Bandits via Linear Programs", "abstract": "Inverse reinforcement learning (IRL) is a well-established paradigm for circumventing the need for explicit reward.\n In this paper, we study the problem of estimating the reward function from a single sequence of actions (i.e., a demonstration) of a stochastic linear bandit algorithm. Our main result is a unified approach for inverse linear bandits, based on the idea of formulating a linear program by tightly characterizing the confidence intervals of pulled actions. We show that the estimation error of our algorithms matches the information-theoretic lower bound, up to polynomial factors in $d$ and $\\log T$, where $d$ is the dimensionality of the feature space and $T$ is the length of the demonstration. Compared to prior approaches, our approach (i) gives a unified reward estimator that works when the demonstrator employs LinUCB or Phased Elimination, two popular algorithms for stochastic linear bandits, while existing estimator only works for Phased Elimination; (ii) does not require access to hyperparameters or internal states of the demonstrator algorithm as required by prior work; and (iii) works for general action sets, while existing estimator requires assumptions on the density and geometry of the action set. We further demonstrate the practicality of our new approach by validating our new algorithms on synthetic data and demonstrations constructed from real-world datasets, where our estimators significantly outperform existing ones.", "tldr": "", "keywords": ["Inverse Reinforcement Learning", "Linear Bandits"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3fd3a2ba1086a0d85a8ee073e70fc98573111dd0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies inverse bandit problems. Instead of learning a reward function from expert demonstrations, it aims to learn a reward function from the actions in the learning process, basically the process that the actions improve. The authors argue that this action slection process can help reveal information about the reward function. The paper formulates a (general) linear program to solve this problem and provide theoretical guarantee that the method achieves information-theoretically optimal reward recovery (up to polynomial factors)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of learning a reward from process instead of learning from demonstrations is interesting. The authors rigorously formulate this problem as a linear program and theoretically guarantee the information-theoretically optimal reward recovery (up to polynomial factors). In general, the paper is well written and technically solid."}, "weaknesses": {"value": "1. The paper assumes the access to an approximation of optimal reward value.\n\n2. The paper assumes to know the algorithm that the demonstrator is using."}, "questions": {"value": "Can the authors discuss how to solve the two weaknesses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bhuME5OJ5N", "forum": "NbHuzXQT8U", "replyto": "NbHuzXQT8U", "signatures": ["ICLR.cc/2026/Conference/Submission24599/Reviewer_asqs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24599/Reviewer_asqs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760986065118, "cdate": 1760986065118, "tmdate": 1762943133066, "mdate": 1762943133066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of this paper is to provide provably efficient algorithms for the recovery of the unknown reward function used by a linear bandit algorithm during its execution. The paper specifically assumes to observe a single demonstration from either linUCB of phase elimination, and building upon Guha et al. it derives various provably efficient algorithms for both settings, as well as a single unifying algorithm. All the algorithms are based on the same idea that each observed action provides a linear constraint on the true reward, so all algorithms simply consist in solving an LP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a strong theoretical analysis of all the proposed algorithms, along with a lower bound.\n- The idea of presenting a single unifying algorithm for both LinUCB and phased elimination is nice.\n- All algorithms are also computationally efficient."}, "weaknesses": {"value": "- The main limitation of this work is the scope. It is not clear in which realistic settings we can apply the modelling assumptions of this paper, i.e., that we want to recover the *linear* reward from someone that is using exactly an algorithm between phased elimination and linUCB. Also, for what should we use this linear parameter? Moreover, this paper requires knowledge of $\\mu^*$ (or, at least, of an interval containing it).\n- The bounds provided in the paper are very large: e.g., $d^8$. Moreover, it is not clear why the error due to $k$ reduces with more data, and I hope authors can clarify this point.\n- The presentation of the paper is quite poor. Although the paper extends the work of Guha et al., the formulation of the problem and the presentation of the results is often quite imprecise. To make an example, Theorem 1 is written so badly (as well as its proof and also the proof of Theorem 2, where $\\Delta$ is undefined. I did not check the others).\n\nSee also my questions below."}, "questions": {"value": "- lines 42-43: why optimal expert leads to high sample complexity?\n- lines 49-53: I would not say that assuming the expert is learning provides a practical advantage against assuming an optimal expert. Of course this holds in case the expert is actually learning, but in case the expert is not, this modelling assumption might introduce non-neglectable misspecification error, that cannot be dropped with more samples (while it can be reduced with more samples as long as we assume the expert to behave optimally).\n- I do not get Theorem 1. What is parameter $\\theta'$? The fact that the lower bound holds for the maximum error also with $\\theta'$ seems very weird. If $\\theta'$ refers to another problem instance, please, rewrite completely Theorem 1 (and also its proof, which is written bad) to allow a reader understand it. Moreover, can you clarify the difference between your Theorem 1 and Theorem 5.1 of Guha et al.? \n- I do not get how you obtain the expression in line 285, because I would expect an additional $d$. Can you please show how you upper bound $\\|a\\|_{\\hat{V}^{-1}}\\le d \\|_{\\overline{V}^{-1}}$ knowing the relation in Theorem 3?\n- Why all the efforts for assuming $\\mu\\in[\\mu^*,\\mu^*+k]$ in the paper? This requirement does not seem to improve much the generality of the method w.r.t. assuming to know $\\mu^*$ directly as in Guha et al.; indeed, the extension of your algorithms to this assumption $\\mu\\in[\\mu^*,\\mu^*+k]$ instead of directly knowing $\\mu^*$ is quite trivial, except for the theoretical guarantees. About this, it seems very weird to me that, e.g., in Algorithm 3, the error due to $k$ disappears as we collect more data from the expert. I would expect it to provide a fixed independent approximation error term. Can you please clarify better this point?\n\ntypos:\n- line 194: I guess when $\\mu^*$ is unknown\n- line 253 misses the term in $k$\n- $\\Delta$ is never defined neither in the paper nor in the appendix, but used a lot\n\nIf you will address all my concerns (clarify the importance of the work, clarify that improving the bounds is not trivial, improve the writing, show that the error due to $k$ indeed reduces with more data), then I will increase the score to 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4lVDe5ZXIM", "forum": "NbHuzXQT8U", "replyto": "NbHuzXQT8U", "signatures": ["ICLR.cc/2026/Conference/Submission24599/Reviewer_abJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24599/Reviewer_abJb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761030978044, "cdate": 1761030978044, "tmdate": 1762943132879, "mdate": 1762943132879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the inverse linear bandit problem, where the goal is to estimate the underlying reward function (i.e., the linear parameter vector) from a sequence of actions taken by a demonstrator. Unlike traditional inverse reinforcement learning (IRL) settings, where the demonstrator is assumed to be optimal, this work considers demonstrators that follow no-regret learning algorithms. Specifically, the authors analyze two well-known stochastic linear bandit algorithms: LinUCB and Phased Elimination. They provide consistent estimators for the true reward parameters under both algorithms.\nBuilding on this, the paper introduces a unified reward estimation approach that does not require prior knowledge of which demonstrator algorithm (LinUCB or Phased Elimination) generated the actions. Finally, the authors empirically evaluate their method against the benchmark from Guha et al., demonstrating improved reward estimation performance on both simulated and semi-synthetic datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper presents a strong and original contribution for inverse linear bandits, offering new theoretical and algorithmic insights into reward estimation from no-regret demonstrators. The proposed linear programs for reward estimation under both Phased Elimination and LinUCB are novel and address several open problems in Guha et al:\n\n1. General action sets: The paper removes prior restrictions on the density and geometry of the action set, demonstrating that consistent reward estimation is achievable under general conditions.\n\n2. Inverse estimation for LinUCB: The construction of an estimator for LinUCB is technically sophisticated requiring a detailed round-by-round analysis rather unlike the phase by phase LP for Phased Elimination.\n\n3. Unified estimator: The authors further propose a unified reward estimator that works across both demonstrators and without needing access to internal hyperparameters."}, "weaknesses": {"value": "The paper is technically solid and very clearly written, with no methodological flaws. I raise a few minor points for completeness and clarification, in the Questions section."}, "questions": {"value": "Q1. Could the authors elaborate on the binary search version of Algorithms 3 and 4?\n\nQ2. What are the main challenges in extending the approach to Thompson Sampling? Is the difficulty primarily due to the stochastic nature of action selection (making it hard to define a deterministic LP constraint per timestep), whereas LinUCB and Phased elimination are deterministic given the current mean and confidence estimate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yzfnX05PCe", "forum": "NbHuzXQT8U", "replyto": "NbHuzXQT8U", "signatures": ["ICLR.cc/2026/Conference/Submission24599/Reviewer_cMoR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24599/Reviewer_cMoR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550112363, "cdate": 1761550112363, "tmdate": 1762943132665, "mdate": 1762943132665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper deals with the inverse linear bandits problem. In this setting, the goal is to estimate the unknown reward function from a single action trajectory coming from a linear bandit algorithms, like Phased elimination or LinUCB. The paper extends previous works by providing a unified estimator for both Phased elimination and LinUCB that is based on solving a linear program with appropriate constraints. The paper first provides a version of the approach for phased elimination and LinUCB separately, together with their analysis, and finally combine them both in a unified linear program. The estimation error of the algorithms is compared to a theoretical lower bound, previously derived in the paper. The paper includes a brief empirical evaluation of the approach against prior methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes an approach for inverse reward estimation in linear bandits that seems to advance over prior works on various aspect:\n- It provides a unified estimator for phased elimination and LinUCB, which is quite critical in a setting where it is not obvious to assume that the learning algorithm is fully known;\n- The estimator only needs (approximate) knowledge of the maximum reward and no access to hyperparameters of the algorithm.\n\nOther strengths include:\n- The theoretical analysis looks rigorous and clear (although the proofs were not checked for this review), including a lower bound that helps to weigh the factors appearing in the estimation error.\n- The empirical analysis gives at least some empirical support to a theoretically grounded approach.\n- The paper is well written and easy to follow."}, "weaknesses": {"value": "I do not see any clear weakness for this paper, which looks like a relevant and sound research effort. A few minor weaknesses are:\n- While the paper improves over prior works, the contribution is mostly incremental at an higher level of abstraction;\n- The motivation for the problem setting does not appear to be very strong, although there are previous publications tackling a similar problem;\n- The unified approach still looks like a combination of the specialized algorithms, rather than a general procedure that can cover many other algorithms with similar premises."}, "questions": {"value": "While my evaluation of the paper is positive, I report a few questions the authors may address in their response. It is worth mentioning that none of the points below will have a significant impact on my evaluation.\n\n- Motivation: Estimating the reward from a trajectory of a learning algorithm is motivated by the fact that the algorithms are already deployed in real-world systems, so data can be collected easily. However, if such algorithms are deployed, it is natural to believe that they are optimizing a known reward function, which makes the benefit of solving the inverse problem less clear. Instead, if a human is collecting data, it is highly unlikely they are following an algorithm like LinUCB or Phased elimination. Some applications that come to mind are the following. Perhaps a reward is known by the company deploying the algorithm, but not by the one solving the inverse estimation problem, which may be a competitor or a player in another market. Perhaps the reward the algorithm is maximizing is coming from a very complex and unknown function, so that the goal becomes to distill the reward in a simpler model. If that is the case, it would be nice to consider the misspecification in the analysis and a setting where both the actions and the rewards realizations are available.\n\n- Lower bound: Can the authors clarify how their lower bounds relate to previous works, especially the alternative lower bound in Sec. 5 of Guha et al.?\n\n- Can the authors discuss which factors of the estimation errors of their algorithm they believe are unavoidable (e.g., the term $O(\\kappa)$ seems to come directly from the optimal reward assumption), may be overcome with a more sophisticated analysis/algorithm?\n\n- Do the authors believe that the unification can be pushed even further to more linear bandits algorithms? TS sampling is mentioned in the conclusion. What about any \"no-regret algorithm\"?\n\n- The performance of the algorithms is evaluated in term of estimation error on the worst-case action. However, an estimation error in some actions (e.g., very suboptimal actions) may be more acceptable than an estimation error in some other (e.g., close to optimal actions). I am wondering whether the evaluation metric for the estimator can be refined in this sense. One idea that comes to mind is to provide an estimate that minimizes the probability of producing an action sequence that is different from the one given by the ground truth rewards, especially for deterministic algorithms..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mZGQkS4B7v", "forum": "NbHuzXQT8U", "replyto": "NbHuzXQT8U", "signatures": ["ICLR.cc/2026/Conference/Submission24599/Reviewer_j7mb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24599/Reviewer_j7mb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581439009, "cdate": 1761581439009, "tmdate": 1762943132404, "mdate": 1762943132404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision"}, "comment": {"value": "We thank the reviewers for recognizing our contribution and for their thoughtful and detailed feedback. We have updated a revised version of the paper, with changes highlighted in red. Key changes include the followings.\n\n* Wording changes in the introduction part regarding problem settings and motivations. \n\n* **Tighter Bounds for LinUCB:** The analysis for LinUCB has been improved to reduce the dependence on $d$, decreasing the bound from $d^8$ to $d^7$.\n\n* **Refined $\\kappa$ Error Term:** The original bound $\\sqrt{T}\\kappa \\lVert a\\rVert_{V^{-1}}$ has been replaced with a tighter bound $\\sqrt{d}\\kappa$ for both phased elimination and LinUCB. This modification ensures the $\\kappa$ term is a fixed error rather than one growing with $\\sqrt{T}$. \n\n* **Theorem 1 Rewritten:** Theorem 1 has been completely rewritten to improve clarity.\n\n* **Proof Revisions:** The proofs for both Theorem 1 and Theorem 2 have been rewritten and carefully checked for correctness.\n\n* **Binary Search Detail:** A paragraph has been added to the \"Analysis of Algorithm 4\" to describe the implementation of binary search.\n* **Parameter Definitions:** All previously undefined parameters and symbols are now introduced.\n\n* **Typo Corrections:** Various typos have been corrected."}}, "id": "UJwSXPixbe", "forum": "NbHuzXQT8U", "replyto": "NbHuzXQT8U", "signatures": ["ICLR.cc/2026/Conference/Submission24599/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24599/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24599/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744790386, "cdate": 1763744790386, "tmdate": 1763744849432, "mdate": 1763744849432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}