{"id": "mJ5jxzRqVF", "number": 23244, "cdate": 1758341207884, "mdate": 1759896824561, "content": {"title": "Reward Inflation Paradigm Through the Lens of Monetary Economics", "abstract": "Reward is fundamental to reinforcement learning (RL), where the agent treats it as an incentive to maximize.\nThis appearance is akin to a rational human who maximizes their income.\nHowever, in the real economy, money expands to stimulate economic growth.\nInspired by this principle of monetary economics, we introduce a novel RL paradigm, reward inflation, which gradually increases the reward scale during training.\nAnalogous to inflationary policies used by central banks to stimulate economic growth, reward inflation acts as an incentive stimulus for agents to accelerate policy learning.\nReward inflation can be applied in two ways: fixed or adaptive.\nMotivated by the Fed's monetary policy, we propose FedeRL, a dynamic controller for adaptive inflation.\nTheoretical analysis suggests that the effect of reward inflation is threefold: (1) induces recency bias in temporal-difference learning, (2) amplifies policy gradients, and (3) enhances neural activation.\nEmpirical results corroborate these insights, showing that moderate inflation improves performance on continuous control tasks.\nMoreover, FedeRL performed even better than fixed inflation and outperformed comparable baselines.\nBy translating economic growth principles into RL, our approach offers a novel perspective that strengthens policy optimization and addresses fundamental RL objectives.\nThe implementation code will be made publicly available.", "tldr": "", "keywords": ["reinforcement learning", "reward inflation", "money supply", "monetary economics", "economic growth", "monetary policy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c38646831dd05f72e08e215f459a7321367acc1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FedeRL, a method inspired by monetary economics that dynamically scales reward functions.\nSpecifically, the authors first motivate why adjusting the scales of the rewards can be beneficial during training, using concepts from macroeconomics.\nThis is followed by the FedeRL algorithm, which, inspired by how central banks work, adjusts the inflation rates based on gradient norms of the function approximator.\nFinally, the authors show empirically that FedeRL can reduce dead neurons and improve performance in MuJoCo when used with the soft actor-critic algorithm.\nOverall, the paper proposes an RL algorithm that achieves improved empirical results, but its theoretical framing is underdeveloped and, in its current form, does not add meaningful insight."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple to implement and appears to perform well on MuJoCo tasks.\nThe idea of modifying the rewards to stabilize training is an interesting one, which may provide new insights into algorithm designs."}, "weaknesses": {"value": "The connection to economic theory appears largely superficial.\nWhile the paper draws some analogies to economic concepts, these do not seem to inform the design of the proposed algorithm (FedeRL) in a meaningful way.\nThe “economic inspiration” is mentioned primarily at a narrative level rather than as a source of substantive methodological insight.\nAs a result, the framing may give the impression of novelty without contributing actual conceptual or theoretical value to the RL problem.\n\nThe proposed method, while effective, also introduces eight new hyperparameters ($Y$, $\\beta_s$, $\\beta_l$, $K$, $\\rho_Y^{*}$, $\\lambda_1$, $\\lambda_2$, $\\tau_\\rho$), and a study of these parameters is lacking."}, "questions": {"value": "1. Can the authors clarify precisely how the economic analogy contributes to the algorithm’s design?\n2. How sensitive is the algorithm regarding the hyperparameters?\n3. In the implementation of SAC, are the rewards scaled first before being stored into the replay buffer? Or is it scaled on-the-fly during training depending on the global step? If I understand correctly, Assumption 1 is based on the setting where all rewards are scaled by the same factor. However, if the replay buffer contains rewards that are scaled differently, it is not clear to me what Q-learning would converge to, if it converges at all."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nz16vZlCO4", "forum": "mJ5jxzRqVF", "replyto": "mJ5jxzRqVF", "signatures": ["ICLR.cc/2026/Conference/Submission23244/Reviewer_2CBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23244/Reviewer_2CBW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744521873, "cdate": 1761744521873, "tmdate": 1762942572707, "mdate": 1762942572707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a reward inflation method that increase the reward scale during training.  Theoretical analysis suggests three main effects: (1) recency bias, (2) gradient norm amplification, and (3) enhanced neural activation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This idea is interesting and novel\n2. the result is promising"}, "weaknesses": {"value": "1. The proposed approach assumes a noise-free environment when adjusting the reward scale. However, if consider stochasticity—such as observation noise, reward corruption, or environmental uncertainty, the change of reward scale also affect these noise and may also have negative affect on the learning.\n\n2. The hyperparameter study on soft update coefficent will strength the paper."}, "questions": {"value": "1. The reward inflation process is happened before saved into replay buffer or after sample from replay buffer? if happened before replay buffer, then this will cause the issue that reward sclae uneven in the replay buffer which may limit the performance.\n\n2.Is there a upper bound for the inflation which cause learning collapse (idea also from economics)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mvKxQIu4kr", "forum": "mJ5jxzRqVF", "replyto": "mJ5jxzRqVF", "signatures": ["ICLR.cc/2026/Conference/Submission23244/Reviewer_Qmnv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23244/Reviewer_Qmnv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843625293, "cdate": 1761843625293, "tmdate": 1762942572390, "mdate": 1762942572390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"reward inflation,\" a novel setup for reward rescaling during RL inspired by monetary economics, where a 'nominal' reward scale is gradually incremented by some percent during training (inflation if a positive percent, deflation if a negative percent). The authors propose that when acting as an inflation, this reward modulation acts as an \"incentive stimulus\" that amplifies policy gradients, induces a recency bias in learning, and increases network activation (i.e. reducing the percent of dead neurons). They also propose FedeRL, an adaptive mechanism for setting the reward inflation percent with an analogy to a central bank which attempts to dynamically adjust inflation based upon an economic environment's speed of change. In this RL application, FedeRL dynamically adjusts the inflation rate based on two exponential moving averages of the policy gradient norms to stabilize training. Empirically, both fixed inflation and the FedeRL adaptive inflation method are shown to outperform the standard SAC baseline and other related methods on continuous control tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- This work presents a highly original and creative paradigm by drawing a compelling analogy from monetary economics to RL. I genuinely found this interesting to read and consider.\n- The adaptive FedeRL inflation rate determination is quite practical and shows clear, intuitive, and interpretable behavior.\n- Consistent empirical performance gains are shown over the standard (0%) baseline and other comparable methods (like LR scheduling and PER)."}, "weaknesses": {"value": "- The primary theoretical effect (gradient norm amplification) is described as functionally very similar to learning rate scheduling, and yet the paper doesn't fully disentangle why \"reward inflation\" is a superior mechanism. When comparing against learning rate scheduling, this is only done to a limited degree (+ vs - 2%) when a more complete range of searching is completed for FedeRL and static inflation rates.\n- The new FedeRL controller introduces its own set of hyperparameters (natural rate, short and long range moving norm averages, etc.), which may be difficult to tune and it is clear from Table 1 that these values might require tuning per task. These limitations are not explored.\n- The inflationary rate is motivated heavily from a monetary perspective, though it is in the end a rather simple change to reward rescaling and any negative side effects (or limitations) of such are not much explored. E.g. catastrophic forgetting could be made much worse by any positive inflationary value. Furthermore, it is unclear whether the training time for models with inflationary rates which are non-zero has to be limited to avoid the eventual exponential rise in training impact. How the model training times must be limited is not discussed herein.\n- This method is only applied with the SAC RL algorithm. Ensuring that these benefits transfer to alternative RL algorithms would be ideal and important for demonstrating generality."}, "questions": {"value": "A response to the above weaknesses would be much appreciated.\n\nThough the inflationary process is described as similar to learning rate scheduling, it might be more accurate to say that it could be similar to (perhaps even equivalent to) a particular scheduler for the learning rate combined with a change to the discounting term (gamma). Is this correct?\n\nIdeally to address the above, the nominal return (sum of discounted future rewards) could be unpacked and given a form with respect to the regular reward. This should allow an identification of the degree to which these inflationary measures are simply equivalent to a modification of the discounting factor along with a learning rate change. Doing so could significantly demystify this method and also ensure that claims are not overblown. Note that this would also indicate that the baselines for comparison should also be extended with a sweep over learning rate scheduler rates and discounting factors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gZLpCRxxCH", "forum": "mJ5jxzRqVF", "replyto": "mJ5jxzRqVF", "signatures": ["ICLR.cc/2026/Conference/Submission23244/Reviewer_Fq7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23244/Reviewer_Fq7A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863929081, "cdate": 1761863929081, "tmdate": 1762942572018, "mdate": 1762942572018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a \"reward inflation\" paradigm inspired by monetary economics where the \"nominal reward\" scale is gradually increased during training, and introduces FedeRL, an adaptive controller mimicking the Federal Reserve's monetary policy. The authors analyze three effects of this approach: inducing recency bias, amplifying gradient norms, and enhancing neural activation. Empirical results on MuJoCo tasks are shown."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "No"}, "weaknesses": {"value": "The paper's entire premise hinges on an analogy between RL rewards and money. This analogy is incorrect and unreasonable. It mistakes a simple heuristic (non-stationary reward scaling) for a economic principle."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X9kcN8tZbC", "forum": "mJ5jxzRqVF", "replyto": "mJ5jxzRqVF", "signatures": ["ICLR.cc/2026/Conference/Submission23244/Reviewer_t5eG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23244/Reviewer_t5eG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991977680, "cdate": 1761991977680, "tmdate": 1762942571795, "mdate": 1762942571795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}