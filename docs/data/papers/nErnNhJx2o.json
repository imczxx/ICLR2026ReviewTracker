{"id": "nErnNhJx2o", "number": 12176, "cdate": 1758206182409, "mdate": 1762943826017, "content": {"title": "REORIENTING THE FROZEN SPACE: TRAINING-FREE TEST-TIME ADAPTATION BY GEOMETRIC TRANSFORMATION", "abstract": "With the widespread application of Vision-Language Models (VLMs) in downstream tasks, test-time adaptation methods based on VLMs, particularly the training-free paradigm, have been gaining increasing attention due to their advantages in handling distribution shifts during testing. Yet, existing training-free methods remain constrained by the fixed geometry of pretrained feature spaces, which limits class separability. We propose SOBA, a training-free TTA method that edits decision geometry by re-expressing class prototypes in a test-induced orthogonal basis. SOBA maintains a lightweight dynamic queue of high-confidence test samples, derives an orthogonal basis via singular value decomposition, and aligns prototypes to the most discriminative directions of the test distribution. This simple adjustment enlarges inter-class margins, sharpens decision boundaries, and improves the recognition of semantically similar categories—without modifying features, prompts, or model parameters. Extensive experiments on multiple benchmarks demonstrate that SOBA achieves state-of-the-art accuracy and superior efficiency compared to both training-free and backprop-based TTA methods.", "tldr": "", "keywords": ["Training-free test-time adaptation", "vision-language model", "clip"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8bb1053f34b07d23c30bd355c5d31130d1a7fcb7.pdf", "supplementary_material": "/attachment/6cb9198651b3a26efee370b0675e2bb04de9f1da.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes to use orthogonal basis transformation on the embeddings in visual-language models (VLMs), such as CLIP, for better out-of-distribution (OOD) detection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies VLMs, the frontier combining vision and language."}, "weaknesses": {"value": "The method is poorly motivated, i.e., why did the authors select the orthogonal basis transformation? There is a comparison with other transformation methods; however, there is no evidence that orthogonal basis transformation must be the optimal transformation. For example, there are many more distances and transformations beyond those listed in Fig. 3, such as Minkowski distance, etc.\n\nImportantly, it is pointed out that \"standard basis is **suboptimal**\" on page 4. However, can you prove that the orthogonal basis transformation used here is optimal? If the orthogonal basis transformation cannot be proved to be optimal, how do you know that other basis transformations are suboptimal?"}, "questions": {"value": "There is no theoretical proof showing the soundness of SOBA. For example, in Fig. 3(a), orthogonal basis transformation > Mahalanobis > NCM > Euclidean > Manhattan. But why? Does this hold true under every circumstance (settings, datasets, models, etc.)? If not (e.g., on some dataset in Table 2), why?\n\nThe application of the proposed method, SOBA, is too limited. The experiment is only about OOD. And SOBA cannot consistently achieve the best among the competitors. VLMs have a broad range of applications, not limited to OOD.\n\nIs it possible to apply the method to some more recent, state-of-the-art VLMs, such as Qwen2.5-VL? If not, why?\n\nCrucial: It is important to know where the improvement comes from. A simple number (higher/lower than SOTA by what percentage) cannot tell you much. What images are misclassified before using SOBA but corrected by SOBA? What images are well classified before using SOBA, but SOBA fails? Why? Are the corrected images truly because of the orthogonal basis transformation? I am not confident about the method without seeing this information about the in-depth analysis.\n\nMoreover, is Figure 1 created using a real-world dataset, or simply a toy data distribution? If it is a toy distribution, please switch to the real data. You can show more (if not all) real-world distributions in the appendix. It is important to visually illustrate what this method does, geometrically, to the features."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5YD6jjYNoA", "forum": "nErnNhJx2o", "replyto": "nErnNhJx2o", "signatures": ["ICLR.cc/2026/Conference/Submission12176/Reviewer_dkvg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12176/Reviewer_dkvg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100307680, "cdate": 1761100307680, "tmdate": 1762923127150, "mdate": 1762923127150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "0jcfCJoa01", "forum": "nErnNhJx2o", "replyto": "nErnNhJx2o", "signatures": ["ICLR.cc/2026/Conference/Submission12176/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12176/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762943825205, "cdate": 1762943825205, "tmdate": 1762943825205, "mdate": 1762943825205, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free test-time adaptation (TTA) method for vision-language models, termed Space Reorienting with Basis Transformation (SOBA). The key idea is to reorient class prototypes into the subspace defined by the feature covariance matrix of test samples, thereby improving class separability in the feature space.\nSpecifically, SOBA maintains a dynamic queue of high-confidence test samples to estimate class prototypes. At test time, it performs eigenvalue decomposition of the feature covariance matrix and transforms the prototypes into the eigenspace of the input features. The final prediction combines the zero-shot prediction with that of the transformed prototype-based classifier.\nSOBA is computationally efficient (requiring no backpropagation) and demonstrates strong performance under out-of-distribution (OOD) and cross-dataset benchmarks using CLIP models with transformer-based text encoders and either ResNet or ViT visual encoders."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed eigenspace-based prototype transformation is conceptually simple, computationally lightweight, and empirically effective across multiple OOD and cross-dataset benchmarks. \n* The method fits within the training-free TTA category, requiring only forward-pass computations (specifically, eigenvalue decomposition), which makes it appealing for large-scale or latency-sensitive applications."}, "weaknesses": {"value": "* Limited analysis of class separability effects: The core motivation of SOBA is to enhance inter-class separability—especially for hard classes with overlapping features—yet no ablation or quantitative analysis is provided to verify this. It remains unclear whether the transformation actually improves accuracy for previously hard-to-classify classes, or whether it introduces trade-offs that harm already well-separated classes. A deeper empirical study is needed to substantiate this motivation. \n* Lack of justification for transforming only text embeddings: The transformation is applied solely to class prototypes derived from text embeddings, while image embeddings remain unchanged. Under distribution shift, image features may also vary non-uniformly across classes, potentially misaligning the transformed text space. The rationale for limiting the transformation to text embeddings should be clarified, along with possible benefits or drawbacks of extending it to image embeddings. \n* Missing related work: The paper should cite Forward-Only Adaptation (FOA) (Niu et al., ICML 2024, Oral), which also achieves efficient TTA without backpropagation using forward-only adaptation. A conceptual or empirical comparison with FOA would strengthen the paper’s positioning within the training-free TTA literature."}, "questions": {"value": "1. Can the authors analyze the effect of SOBA on class separability, e.g., by showing per-class accuracy improvements or changes in inter-class distances before and after transformation? \n2. What is the reasoning behind transforming only the text embeddings rather than both modalities? \n3. How does SOBA compare conceptually and empirically with FOA (Niu et al., 2024)? \n4. Can the authors quantify the memory overhead of the dynamic queue? Since it scales linearly with the number of classes, this could be a concern for large-scale datasets like ImageNet."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9cnbmwAIzf", "forum": "nErnNhJx2o", "replyto": "nErnNhJx2o", "signatures": ["ICLR.cc/2026/Conference/Submission12176/Reviewer_QiLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12176/Reviewer_QiLa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645373889, "cdate": 1761645373889, "tmdate": 1762923126804, "mdate": 1762923126804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SOBA, a training-free test-time adaptation method for CLIP. Instead of modifying the feature extractor or updating parameters, the method estimates a global covariance structure from high-confidence test samples and uses principal components to define a new orthogonal basis. Class prototypes are then re-expressed in this basis, yielding re-oriented decision boundaries. The final prediction fuses the transformed similarity scores with the original CLIP logits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method adapts to test-time distribution shift through basis transformation of class prototypes, offering a clean and original idea.\n2. The proposed method requires no fine-tuning or gradient updates, making it computationally efficient and deployment-friendly.\n3. The algorithm is conceptually simple yet effective.\n4. Evaluated across multiple datasets and distribution shift scenarios, showing consistent gains over zero-shot CLIP and several TTA baselines.\n5. The use of shared covariance and principal components has a solid geometric motivation and is well explained."}, "weaknesses": {"value": "1. Despite claims of general applicability, experiments are restricted to CLIP models (ViT-B/16, ResNet). The method’s effectiveness on other architectures (e.g., SigLIP series, other VLMs, CLIP with other visual backbones) remains untested.\n2. The approach assumes that reorienting class prototypes suffices to adapt, but ignores potential distortions in the feature space itself under distribution shift.\n3. The fusion weight $\\alpha$ is tuned on ImageNet and fixed elsewhere. It is unclear if this choice generalizes across datasets or domains. It is also not fair as ImageNet is one of the test dataset.\n4. Potential failure cases are not discussed.\n5. Related work like TCA [1] on training-free TTA is notably absent in discussion and comparison.\n\n[1] Is Less More? Exploring Token Condensation as Training-free Test-time Adaptation"}, "questions": {"value": "1. Did you observe cases where the feature extractor (e.g., CLIP encoder) itself outputs distorted features under shift, making prototype reorientation insufficient? Would adapting the encoder (e.g., via BN stats) help in such cases?\n2. Your method updates the prototypes and basis every fixed percentage of test data. Since the test data is sequentially processed with random order (due to random seed), it is unclear why update frequency has such a strong effect, as shown in the FGVC dataset.\n\nSee weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4eVf0vCBYR", "forum": "nErnNhJx2o", "replyto": "nErnNhJx2o", "signatures": ["ICLR.cc/2026/Conference/Submission12176/Reviewer_QEvC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12176/Reviewer_QEvC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717051816, "cdate": 1761717051816, "tmdate": 1762923126465, "mdate": 1762923126465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a training-free TTA method, SOBA, which addresses the limitation of class-boundary ambiguity commonly observed in static feature spaces under the training-free TTA paradigm. The core contribution lies in a simple visual prototype alignment strategy that sharpens decision boundaries by deriving orthogonal basis via singular value decomposition. Extensive experiments across 15 datasets demonstrate competitive performance and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The evaluation is comprehensive, including two experimental settings across 15 diverse datasets.\n- The idea of updating queue prototypes through orthogonal bases is interesting."}, "weaknesses": {"value": "- The paper lacks detailed justification and quantitative evidence regarding the improved decision boundaries. Specifically, the explicit relationship between the class matrix and the resulting decision boundaries needs to be clarified. Moreover, after transforming the feature space, how can one quantitatively verify that the decision boundaries are indeed better aligned or more discriminative?\n- Another weakness is the lack of discussion regarding the most recent works, which may reduce the impact of this paper. For instance, works such as MPE [1] (which shares the same motivation of addressing fuzzy class-boundaries), TT-RAA [2] (a similar method involving covariance‐matrix estimation), BCA [3] (a faster approach that does not require maintaining a queue), and the very recent GS-Bias [4] and TCA [5] (2025 publications) are not sufficiently acknowledged or compared against.\n\n[1] Multi-Cache enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models. ICCV 2025\n\n[2] Test-Time Retrieval-Augmented Adaptation for Vision-Language Models. ICCV 2025\n\n[3] Bayesian Test-Time Adaptation for Vision-Language Models. CVPR 2025\n\n[4] GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models. ICML 2025\n\n[5] Is Less More? Exploring Token Condensation as Training-free Test-time Adaptation. ICCV 2025"}, "questions": {"value": "- It would be helpful to quantify the computational and memory overhead introduced by SVD, especially when scaling to larger models or higher-dimensional features.\n- High-confidence erroneous samples in the queue may distort prototype updates and adversely impact the NCM-based classification performance. How do the authors consider addressing this over-confidence issue?\n- Discussing failure cases of SOBA would be valuable for better understanding its limitations and the scenarios where the method may struggle."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RSsjnHizPO", "forum": "nErnNhJx2o", "replyto": "nErnNhJx2o", "signatures": ["ICLR.cc/2026/Conference/Submission12176/Reviewer_Cfgx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12176/Reviewer_Cfgx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731848785, "cdate": 1761731848785, "tmdate": 1762923126065, "mdate": 1762923126065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SOBA, a training-free, test-time adaptation (TTA) method for Vision-Language Models. The authors identify that existing training-free methods are limited by the fixed geometry of the pre-trained feature space, which can lead to poor separability for semantically similar classes. SOBA's approach is to (1) maintain a dynamic queue of high-confidence, pseudo-labeled test samples, (2) compute a shared covariance matrix from these samples and use SVD (PCA) to find a new orthogonal basis, and (3) re-express the class prototypes (class means) in this new, test-aligned basis. The final prediction is an ensemble of the original zero-shot classifier and a new classifier (NCM) that compares test features to these reoriented prototypes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a valid and important problem in training-free TTA: the fixed, pre-trained feature space can be suboptimal for a specific test distribution, especially when classes overlap. The core idea of finding a training-free way to adjust the decision boundaries to better fit the test data is a well-motivated and practical research direction. The method itself is simple, intuitive, and computationally lightweight, as it primarily relies on SVD and vector-matrix multiplication rather than expensive backpropagation."}, "weaknesses": {"value": "1. The paper frames the method as a novel \"geometric transformation\" that \"reorients\" and \"reshapes\" the feature space. However, the core methodology (Sec. 3.3) is a standard, well-known statistical technique. The method computes a shared covariance matrix $C$ from test-set features and finds its principal components $Q_c$ via SVD. It then transforms the class prototypes $\\mu$ using this matrix ($\\hat{\\mu} = \\mu Q_c$). This is not a new form of geometric transformation; it is a standard implementation of a Nearest Centroid (NCM) classifier in a space transformed by Principal Component Analysis (PCA). The paper's own baseline comparisons to \"PCA*\" and \"LDA*\" (variants constructed by the authors) confirm that the proposed method is, at best, a very specific and incremental implementation of these classical techniques, not a new paradigm. The justification in Appendix D (\"WHY SOBA IS NOT SIMPLE PCA?\") is weak, arguing that SOBA only transforms the prototypes, not the features, which is an implementational choice, not a fundamental methodological innovation.\n\n2. The method's efficiency and simplicity rely on a strong \"GDA assumption\" (Line 248) of a shared covariance matrix $C$ for all classes. This is a very restrictive assumption that is unlikely to hold true in complex, real-world OOD distributions. The paper brushes this aside in Table 4 by claiming it has \"little impact on... performance\" but \"significantly reduces inference time.\" This trade-off is not properly analyzed and feels more like a post-hoc justification for a simplifying assumption.\n\n3. For a paper claiming state-of-the-art in training-free TTA, the experimental comparison is incomplete. It omits a significant number of recent and directly relevant training-free and transductive methods that also operate on VLM feature spaces. Key missing baselines include (but are not limited to) [1-4]. A convincing claim of SOTA would require comparison against these contemporary works.\n\n4. The entire method hinges on building a dynamic queue of high-confidence samples to estimate the covariance matrix. This creates a potential failure case: in scenarios with significant domain shifts or high-entropy datasets (like Aircraft), the model's initial predictions may be systematically wrong, yet still \"high-confidence.\" This would lead to a queue populated with incorrect samples, resulting in a corrupted basis transformation that degrades performance rather than improving it. The paper's robustness analysis (Appendix C.1.4) only injects random noise and does not adequately address this more realistic failure mode.\n\n\n\n[1] Ra-tta: Retrieval-augmented test-time adaptation for vision-language models, ICLR2025\n\n[2] Bayesian test-time adaptation for vision-language models, CVPR2025\n\n[3] Awt: Transferring vision-language models via augmentation, weighting, and transportation, NeurIPS2024\n\n[4] Efficient and context-aware label propagation for zero-/few-shot training-free adaptation of vision-language model, ICLR2025"}, "questions": {"value": "1. Can the authors clearly differentiate the proposed method from a standard NCM classifier applied in a PCA-transformed space? The justification in Appendix D seems to be the only difference, and it's an implementational choice, not a fundamental methodological leap.\n\n2. How does the method perform if the GDA assumption is violated, for example, on a benchmark known to have classes with highly distinct covariance structures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MW18RSKF3n", "forum": "nErnNhJx2o", "replyto": "nErnNhJx2o", "signatures": ["ICLR.cc/2026/Conference/Submission12176/Reviewer_T9Er"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12176/Reviewer_T9Er"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903962884, "cdate": 1761903962884, "tmdate": 1762923125604, "mdate": 1762923125604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}