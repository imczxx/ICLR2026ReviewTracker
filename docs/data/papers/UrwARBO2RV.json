{"id": "UrwARBO2RV", "number": 4172, "cdate": 1757622062440, "mdate": 1759898049111, "content": {"title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "abstract": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language models to answer knowledge-intensive queries with citation-supported responses across languages. While such systems have been proposed, an open questions is whether the mixture of different document languages impacts generation and citation in unintended ways. To investigate, we introduce a controlled methodology using model internals to measure language preference while holding other factors such as document relevance constant. Across eight languages and six open-weight models, we find that models preferentially cite English sources when queries are in English, with this bias amplified for lower-resource languages and for documents positioned mid-context. Crucially, we find that models sometimes trade-off document relevance for language preference, indicating that citation choices are not always driven by informativeness alone. Our findings shed light on how language models leverage multilingual context and influence citation behavior.", "tldr": "We propose a controlled methodology showing that in multilingual RAG, models often favor citing English evidence documents, even at the cost of relevance.", "keywords": ["multilingual", "language preference", "retrieval-augmented generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d47cff470d6b4275f1e01ab6f46bc6fbd5cc976.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper authors introduce a methodology to check whether multilingual LLMs show language bias during fact citation. The authors evaluate six models across eight languages on ELI5 and MIRACL (English-only subset). They find that models trade off document relevance for language preference, indicating that citation choices are not always driven by informativeness or accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow through. They evaluate six open models on 8 diverse languages. The problem that the paper studies (bias towards document citations from a particular language) is an interesting and relevant topic to the audience."}, "weaknesses": {"value": "### 1. **Findings about multilingual LLMs are derived using a translated English-only dataset: ELI5**\n\nThe biggest drawback of the work is the ELI5 (English only) evaluation dataset they used for analysis and evaluation. The paper translates each evidence document into target languages and uses that as a basis for evaluation. \n\n- ELI5 has issues as described earlier in publications dated back in 2021, with a significant train/test overlap, and often the system's generated answers are not grounded in retrieved documents [1]. I would recommend that authors mention the issues present in ELI5 and provide a reason why it has been used for evaluation.\n\n- Citation accuracy results (sections 5, 6, and 7) reported in the paper are unreliable without considering the effect of first-stage translation errors. E.g., a low-resource language is more likely to have a translation error. Now, because of its low-quality translation, does the multilingual LLM avoid citing the translated document, instead of the original document in English? Can they evaluate the model on real documents from a target language except English, e.g. Swahili subset in MIRACL? Adding these experiments would be crucial to understanding the role of translation error in your experiments.\n\n### **2. The two main contributions (evidence of strong English preference, language preference towards query language) are weak.**\n\nIn a RAG scenario, when a user asks queries in a particular language, let's say X, the system should likely provide back citations in the same language X, as the user can read/write documents in language X. Therefore, the language preference seen in multilingual LLMs in citing documents in preference towards the query language is a trivial choice, as they should exhibit a higher similarity between query and document in the same language. Therefore, two of the main contributions listed are weak.\n\n### **3. The evaluation setup is weak and not robust.**\nThe claims made from a benchmark translating English documents into different languages for analysis are not a robust setup in ELI5. Even in MIRACL, queries only from the English split are considered. If possible, I would like to see the same experiments being done robustly on a multilingual long-form dataset like MIRACL, similar to other works mentioned in [2, 3], where authors can use queries from a target language in MIRACL and translate only the relevant document to English (keeping the rest documents in non-English). \n\n**Relevant Citations**:\n- [1] Hurdles to Progress in Long-form Question Answering. Krishna et al. NAACL 2021.\n- [2] MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation. Blandón et al. ACL 2025.\n- [3] MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems. Thakur et al. NAACL 2025."}, "questions": {"value": "In Section 7, why is only one irrelevant document considered for the evaluation setup? Would not all the irrelevant documents be a more interesting setup?\n\nSmall typo ( and open bracket [ ) in L1040 in Figure 8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Oni2aaNsVL", "forum": "UrwARBO2RV", "replyto": "UrwARBO2RV", "signatures": ["ICLR.cc/2026/Conference/Submission4172/Reviewer_NyKx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4172/Reviewer_NyKx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950197592, "cdate": 1761950197592, "tmdate": 1762917213441, "mdate": 1762917213441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores lingual biases in retrieval-augmented generation settings, with a focus if language models prefers a specific language when citing documents. To check for this bias, the authors construct a set of statements with their supporting documents, and check how the performance changes when the cited document is in different languages. The original English document was translated into other language through Google Translate. \nThe experiments include eight different languages (e.g., English, Arabic, Bengali, Chinese, etc.) and six different models (Llama, Qwen, Gemma, etc.). The paper claims that existing models all show strong preference for English documents due to higher citation accuracy when the original document is in English. There is also a strong positional bias towards the start of end of the context that lead to higher accuracy. The paper also includes additional analysis into layers’ inner representations and using different languages for the query."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an original problem on the multilingual biases of language models when they are citing documents in retrieval-augmented generation settings. The experiments are thorough and covers several different languages and models. The analyses also expands across different settings, such as inspecting different layers’ representations and checking how the accuracy changes with different layers.\nFurthermore, the paper is mostly well-written and presented well that could use some minor improvements."}, "weaknesses": {"value": "The main claims that “models preferentially cite English sources” is confounded with other variables. Specifically, the model statements are generated in English, and so it seems natural that the model would prefer citing an English document with an English statement.\nIt could also be the case that there exists another document in the context that contains the same information as the statement, which would also make for a valid citation and in the same language as the statement, so it wouldn’t necessarily be incorrect that the model chooses to cite that document\nFurthermore, in sec 6, the authors show that the highest accuracy tend to be when the query language is the same as the cited document language (orange dots).\n\nAnother problem is the reliance on Google Translate for all experiments. Despite the COMET scores, Google Translate is known to be imperfect and can lead to unnatural translations. \nSince the original ELI5 dataset was in English, it’s also possible that the question is just more common in the English-speaking world that is not as commonly seen in other languages, leading to such bias.\nThe experiments would be more convincing if the main dataset used naturally occurring query and documents in other languages. \nThe authors could also use more human validation to ensure the fluency and quality of the translated queries/documents.\n\nIn terms of the presentation, the paper could use more clarity to make the setting easier to understand. For example, the differences between the settings shown in sec 6 could be visualized clearer in terms of the query language, d_c language, and d_\\neg_c language. Including the main results in Figure 4 would also allow for better comparison (i.e., query in English but d_c in English vs. tgt language)."}, "questions": {"value": "In step 3, why is the LLM judge needed in additional to the NLI entailment model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AllpYHLNFh", "forum": "UrwARBO2RV", "replyto": "UrwARBO2RV", "signatures": ["ICLR.cc/2026/Conference/Submission4172/Reviewer_oc7N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4172/Reviewer_oc7N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953321971, "cdate": 1761953321971, "tmdate": 1762917213212, "mdate": 1762917213212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies “language preference” in multilingual RAG, the tendency of LLMs to cite sources in certain languages irrespective of relevance. The authors construct a controlled evaluation by translating each English evidence document into eight target languages, generate reference, citation-supported English reports using a strong LLM, and made verification via an LLM-as-judge step plus NLI. They then probe models using a next-token citation-prediction task to quantify language bias while holding content constant. Main empirical findings include strong English preference (especially for lower-resource languages), position effects that amplify the bias (“lost in the middle”), query-language (own-language) preference when the query is non-English, and cases where language preference outweighs document relevance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting and important problem. I think the motivation and importance of this study should be recognized, which highlights the lingual bias exisiting widely in current LLMs.\n\n2.  Good writing. The authors use lots of figures, tables and case studies to illustrate the phenomenon. They also use logit lens to inspect the mechanism deep into LLM architecture, which, although not definitive, is still inspiring.\n\n3.  Experimental contribution. The authors originally constructed a dataset and have done comprehensive controlled experiments, across different languages of different popularity, multiple mainstream LLMs."}, "weaknesses": {"value": "Have got several concerns regarding the experiment part:\n\n1. About using an entailment model to filter the statement dataset. While I agree the NLI model would be a good choice for checking whether information is in the documents, it bears some weaknesses in accuracy and robustness for long text[1][2]. The author reports a high retain rate, but not the accuracy of the NLI filter. The authors should compare the consistency between NLI filter and human judge at least in a small subset.\n\n2. About the sensitivity of the lingual bias measure. In step 4 from section 3.2, the author uses argmax + exact match to compute the correctness of the citation. However, this measure may be coarse to capture fine-grained probability change. For example, a change from 0.1 to 0.4 might be significant but not captured by the argmax sampling. Besides, the exact match only produces 1 or 0 score. While the authors control the prompt template carefully, there might still be diverse generations which is equally correct but not captured (for example, a citation number in different languages or styles, maybe).  I recommend that the authors refer to a great work in [3], which has similar and compatible settings with this paper, to test the lingual bias in a more fine-grained and robust manner. \n\n3. About the lingual bias experiment setting. According to my understanding,  the authors use all contrastive context in English and vary the language of the target statement. Would it be possible that the tested bias is due to the language of the background context? Maybe a controlled experiment of using a different background context language is necessary. \n\nWould improve the score if the above concerns can be addressed.\n\nReferences:\n\n[1] Evaluating Open-QA Evaluation. NeurIPS 2023. \n\n[2] A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. ACL 2018.\n\n[3] Seper: Measure retrieval utility through the lens of semantic perplexity reduction. ICLR 2025."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7c2RvcKxSP", "forum": "UrwARBO2RV", "replyto": "UrwARBO2RV", "signatures": ["ICLR.cc/2026/Conference/Submission4172/Reviewer_pZYv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4172/Reviewer_pZYv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987138095, "cdate": 1761987138095, "tmdate": 1762917213003, "mdate": 1762917213003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds a controlled long-form mRAG evaluation that keeps content and relevance fixed by translating each relevant evidence document into multiple languages, then verifies sentence-level statements with an LLM-as-judge stage and a multilingual NLI entailment check. The study reports consistent English preference with amplification from position effects in the middle of context, a shift toward the query’s language when the query is non-English, and a trade-off that sometimes favors language over relevance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The pipeline isolates language as the variable of interest by constructing parallel evidence sets and holding content constant, which avoids confounds from cross-document topical drift.\n\n2. The measurement targets the exact modeling decision by predicting the citation-ID token, rather than relying on coarse citation counts or overlap heuristics.\n\n3. The layer-wise logit-lens analysis reveals a concrete decision point where the model settles on which document to cite and shows that the choice rarely flips afterward."}, "weaknesses": {"value": "1. The multilingual setting is synthesized via machine translation of English documents, which risks injecting translation artifacts that correlate with language preference and may skew the measured gaps.\n\n2. Context manipulation changes the cited document’s language while leaving all other documents in English, which creates an unnatural contrast that may amplify English salience.\n\n3. The relevance-versus-language stress test uses a minimal distractor configuration and does not examine richer retrieval noise or heterogeneous non-English distractors drawn from realistic corpora."}, "questions": {"value": "see weak"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v5gnp8aL2m", "forum": "UrwARBO2RV", "replyto": "UrwARBO2RV", "signatures": ["ICLR.cc/2026/Conference/Submission4172/Reviewer_iLH6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4172/Reviewer_iLH6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990670302, "cdate": 1761990670302, "tmdate": 1762917212773, "mdate": 1762917212773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}