{"id": "tCFYwPdmT4", "number": 11014, "cdate": 1758186923625, "mdate": 1759897614521, "content": {"title": "CatalystBench: A Comprehensive Multi-Task Benchmark for Advancing Language Models in Catalysis Science", "abstract": "The discovery of novel catalytic materials is a cornerstone of chemical engineering and sustainable energy, yet it remains a complex, knowledge-intensive process. While Large Language Models (LLMs) have demonstrated remarkable potential in various scientific domains, their application to catalysis is hindered by the lack of specialized, multi-dimensional benchmarks to guide their development and evaluation. To bridge the critical gap, we introduce CatalystBench, a comprehensive and challenging benchmark meticulously constructed from scientific literature and public datasets, specifically designed to assess the capabilities of LLMs in the nuanced domain of catalyst design. The tasks covered by this benchmark dataset encompass the entire closed-loop process of catalyst development, including reading comprehension, experimental analysis, and scheme reasoning. Based on this benchmark, we propose a Multi-head Full-task (MFT) domain-specific fine-tuning method that employs coupling task-specific output heads. We systematically compare with other three distinct fine-tuning strategies: Single-Task (ST), Full-Task (FT) and Multi-head Single-Task (MST). The extensive experiments demonstrate that the MFT strategy consistently achieves the most substantial performance improvements across all tasks, underscoring the effectiveness of explicit multi-task architectures in complex scientific reasoning. The resulting CatalystLLM significantly outperforms a wide array of state-of-the-art open-source and closed-source models on CatalystBench. We will publicly release both the CatalystBench benchmark and the CatalystLLM model, providing the community with a robust evaluation framework and a powerful new tool to accelerate AI-driven research in catalytic materials science.", "tldr": "", "keywords": ["Scientific Benchmark", "AI for Science", "Catalyst Design", "Large Language Models", "Multi-task Learning", "Domain Adaptation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35980ec1a4743a679cdac8dd55ac70bda530aae8.pdf", "supplementary_material": "/attachment/27d059273dd5e505103305ecae636bd7ff72ceec.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CatalystBench, a multi-task benchmark designed to evaluate LLMs in catalysis science, comprising 8 tasks (26,911 samples total) spanning the entire catalyst development workflow: material extraction (ME), synthesis extraction (SE), regulation method classification (RMC), Faradaic efficiency prediction (FEP), adsorption prediction (AP), d-band center prediction (d-CP), formation energy prediction (FP), and regulation scheme comprehension (RSC). The authors propose a Multi-head Full-Task (MFT) fine-tuning strategy that decouples output heads for classification, regression, and text generation tasks,"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper targets catalysis science, a cornerstone of sustainable energy and chemical engineering with direct industrial relevance (CO₂ reduction, hydrogen evolution, ammonia synthesis). Unlike generic molecular benchmarks, CatalystBench explicitly mirrors the real-world catalyst R&D workflow.\nThe MFT strategy is a thoughtful strategy to the heterogeneous task composition inherent in catalyst design.\nCatalystLLM shows great improvements, which achieves best performance across 19/20 metrics, with particularly dramatic improvements in regression tasks: +18% R² (AP: 0.81 vs. 0.63 for ChemLLM), +16% R² (d-CP: 0.73 vs. 0.54), +38% MAE reduction (FEP: 1.72 vs. 2.80). For classification, improvements are more modest but consistent (+29% accuracy for RMC: 0.81 vs. 0.52)."}, "weaknesses": {"value": "CatalystBench focuses on CO₂ reduction and surface catalysis, with tasks heavily biased toward electrochemical systems (FEP: Faradaic efficiency for CO₂RR) and metal/alloy catalysts (AP: transition metal surfaces). Some other important catalysis systems can be included such as: (1) Organocatalysis non-metallic catalysts like proline, BINOL which is important for asymmetric synthesis. (2) Enzymatic catalysis (biocatalysts), critical for pharmaceutical manufacturing. (3) Photocatalysis. The current benchmark is relatively narrow.\n\nThe authors provide no theoretical analysis of why MFT works e.g., gradient conflict mitigation, task relatedness measures, relying solely on empirical validation."}, "questions": {"value": "Despite claiming \"interpretable reasoning,\" the paper provides minimal qualitative analysis of where and why CatalystLLM fails. e.g. Table 1 shows CatalystLLM achieves 0.81 R² on AP prediction—what about the remaining 19% variance?\n\nFor regression tasks, the paper compares only against 3 traditional ML baselines (CatBERTa, GAP-CatBERTa, GPR in Appendix B.5), missing state-of-the-art graph neural networks (GNNs) for materials: (1) SchNet, DimeNet++, GemNet-OC (message-passing NNs explicitly encoding 3D geometry)—these are standard baselines for OC20/OMAT24 adsorption energy prediction. (2) Equivariant transformers \n\n[1] Schütt, Kristof T., et al. \"Schnet–a deep learning architecture for molecules and materials.\" The Journal of chemical physics 148.24 (2018).\n[2] Gasteiger, Johannes, et al. \"Fast and uncertainty-aware directional message passing for non-equilibrium molecules.\" arXiv preprint arXiv:2011.14115 (2020).\n[3] Gasteiger, Johannes, et al. \"GemNet-OC: developing graph neural networks for large and diverse molecular simulation datasets.\" arXiv preprint arXiv:2204.02782 (2022)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0hJalHMoVh", "forum": "tCFYwPdmT4", "replyto": "tCFYwPdmT4", "signatures": ["ICLR.cc/2026/Conference/Submission11014/Reviewer_2hjb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11014/Reviewer_2hjb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886650209, "cdate": 1761886650209, "tmdate": 1762922194515, "mdate": 1762922194515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CatalystBench, the first comprehensive multi-task benchmark tailored for LLMs in catalysis science. It covers the entire catalyst research and development (R&D) workflow, integrating high-fidelity theoretical simulation and curated experimental data.\nTo address the inherent heterogeneity of tasks (including regression, classification, and language generation), the authors propose the Multi-head Full-task (MFT) fine-tuning strategy. MFT achieves this by decoupling output heads, which successfully alleviates interference between loss landscapes and results in the most substantial performance improvements across all tasks.\nThe resulting specialized model, CatalystLLM, achieves state-of-the-art performance and exhibits superior domain expertise compared to general LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unified, workflow-aligned benchmark\n\nCatalystBench unifies theoretical data and experimental literature, covering the entire catalyst development process (reading comprehension, analysis, reasoning).\n\n2. Effective MFT fine-uuning\n\nThe MFT strategy decouples output spaces for diverse tasks while sharing a backbone, achieving a 12.44% performance improvement over single-task baselines.\n\n3. SOTA domain performance\n\nCatalystLLM significantly outperforms leading LLMs on the benchmark, demonstrating superior domain expertise and fewer scientific hallucinations."}, "weaknesses": {"value": "1. Limited knowledge scope and bias\n\nThe predictive accuracy of CatalystLLM is constrained by the coverage of the CatalystBench dataset, potentially limiting utility for materials outside this range, and introducing potential bias from the selection of scientific literature sources.\n\n2. Performance comparison can be enhanced further\n\nWhile versatile, CatalystLLM is sometimes outperformed by highly optimized traditional ML baselines in specific single-task numerical predictions. It also could be better if more traditional machine learning baselines could be included to further demonstrate the effectiveness of the proposed method."}, "questions": {"value": "1. is it possible to add the non-LLM based SOTA of each task for a more comprehensive comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vpGCP7zrDH", "forum": "tCFYwPdmT4", "replyto": "tCFYwPdmT4", "signatures": ["ICLR.cc/2026/Conference/Submission11014/Reviewer_dAYs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11014/Reviewer_dAYs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977728939, "cdate": 1761977728939, "tmdate": 1762922194075, "mdate": 1762922194075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CatalystBench, a catalysis-focused benchmark that tries to cover the full catalyst R&D workflow in one place by assembling eight tasks: extracting catalyst materials and synthesis steps from literature, classifying the regulation strategy used (e.g., alloying, defect, interface modulation), predicting key catalytic properties from public datasets via natural-language instructions (adsorption energy, d-band center, formation/efficiency values), and understanding long “regulation-scheme” texts that explain why a given catalyst modification improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper correctly observes that catalysis papers mix narrative experimental text, semi-structured synthesis details, and simulation/numerical descriptors — and real catalyst design jumps between them. Existing benchmarks like ChemBench, LLM4Mat, SciBench are more fragmented and not tied to the full catalyst R&D loop. Their comparison table makes this point clearly.\n\n2. The 8 tasks do cover the four capability buckets the authors call Understanding → Reasoning → Explaining (their Fig. 1 / Sec. 3.1) and they really do span text, structured catalyst descriptors, and numbers.\n\n3.  Many “LLM-for-science” benchmarks just dump all tasks into SFT and hope; here they at least propose an architectural separation (MFT) and run ablations against other fine-tuning strategies (claimed in Sec. 3.2 / 3.3 and Appendix). That’s a reasonable, domain-motivated engineering idea."}, "weaknesses": {"value": "1. A lot of the “hard” data is LLM-assisted before human filtering. For the regulation-scheme comprehension, the pipeline is: literature → GPT-4o (with SciQAG) → Q&A → human filtering. That is not the same as “fully human authored”, and it raises the usual questions: how much of the style/format is baked in from GPT-4o, how diverse the questions really are, and whether future GPT-4.x will get an unfair advantage because it’s closer to the generation distribution. The paper says experts “perform annotation and filtering,” but does not give inter-annotator agreement or rejection rates in the main text. I would ask to quantify that.  \n\n2. The biggest task, SE, has 6,612 instances; several key tasks (FEP 2,148; RMC 2,364; RSC 4,307) are in the low-thousands. The authors do mention that this “reflects a common constraint in catalytic research” and that they “prioritize task breadth”, but it does mean that (i) leaderboard variance could be high, and (ii) large multi-task LLMs may overfit the linguistic shell rather than the chemistry. This should be made more explicit.  \n\n3. MFT novelty is incremental. Multi-head, task-decoupled training for mixed classification/regression/generation is not new in ML; here it is justified by domain heterogeneity, but methodologically it’s a small step. For ICLR, I would like to see either (i) a clearer theoretical/optimization story (e.g., loss interference curves between numerical vs generative heads) or (ii) a stronger empirical claim like “without MFT, regression collapses by X%, with MFT, text tasks do not degrade.” Right now the paper mostly states superiority."}, "questions": {"value": "How much is “real” experimental/literature text vs LLM-generated Q&A? \n\nAre the 8 public catalytic datasets actually licensed / stable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HiOnE2uUWX", "forum": "tCFYwPdmT4", "replyto": "tCFYwPdmT4", "signatures": ["ICLR.cc/2026/Conference/Submission11014/Reviewer_SpDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11014/Reviewer_SpDV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983428695, "cdate": 1761983428695, "tmdate": 1762922193580, "mdate": 1762922193580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CatalystBench, a large-scale multi-task benchmark for evaluating language models in catalysis science. It covers seven task categories, from catalyst knowledge to mechanism reasoning and molecular design, built through automated data extraction and expert validation. Experiments across general and chemistry-specific LLMs reveal that while models handle basic factual tasks reasonably well, they still perform poorly on mechanism-level reasoning and condition prediction, highlighting major gaps in domain understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Evaluating LLMs in catalysis is both scientifically important and practically valuable. The benchmark spans multiple levels of reasoning, from basic recognition to scientific inference.\n2. The combination of automated extraction and expert validation ensures data reliability. Figures and examples make the complex tasks easy to follow."}, "weaknesses": {"value": "1. Although the results highlight model weaknesses, the paper does not analyze why models fail (e.g., due to missing domain knowledge, poor reasoning ability, or lack of grounding). Adding this discussion would make the findings more actionable.\n2. The paper could include more analysis of data quality and diversity, such as the balance of catalyst types, reaction categories, and distribution of task difficulties.\n3. While the benchmark is comprehensive, the work is mainly a dataset contribution, with relatively limited technical novelty in terms of modeling or learning methodology."}, "questions": {"value": "Please refer to the cons"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mfmO6ACfVx", "forum": "tCFYwPdmT4", "replyto": "tCFYwPdmT4", "signatures": ["ICLR.cc/2026/Conference/Submission11014/Reviewer_UAyg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11014/Reviewer_UAyg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139869796, "cdate": 1762139869796, "tmdate": 1762922193129, "mdate": 1762922193129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}