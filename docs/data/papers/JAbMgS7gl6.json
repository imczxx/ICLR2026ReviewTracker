{"id": "JAbMgS7gl6", "number": 447, "cdate": 1756740092395, "mdate": 1759898260658, "content": {"title": "HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space", "abstract": "Mixture-of-Experts (MoE) architectures in large language models (LLMs) deliver exceptional performance and reduced inference costs compared to dense LLMs. However, their large parameter counts result in prohibitive memory requirements, limiting practical deployment. While existing pruning methods primarily focus on expert-level pruning, this coarse granularity often leads to substantial accuracy degradation. In this work, we introduce HEAPr, a novel pruning algorithm that decomposes experts into smaller, indivisible atomic experts, enabling more precise and flexible atomic expert pruning. To measure the importance of each atomic expert, we leverage second-order information based on principles similar to Optimal Brain Surgeon (OBS) theory. To address the computational and storage challenges posed by second-order information, HEAPr exploits the inherent properties of atomic experts to transform the second-order information from expert parameters into that of atomic expert parameters, and further simplifies it to the second-order information of atomic expert outputs. This approach reduces the space complexity from $O(d^4)$, where $d$ is the model’s dimensionality, to $O(d^2)$. HEAPr requires only two forward passes and one backward pass on a small calibration set to compute the importance of atomic experts. Extensive experiments on MoE models, including DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing expert-level pruning methods across a wide range of compression ratios and benchmarks. Specifically, HEAPr achieves nearly lossless compression at compression ratios of $20\\% \\sim 25\\%$ in most models, while also reducing FLOPs nearly by $20\\%$. The code can be found at \\href{https://anonymous.4open.science/r/anonymous-code-B927/}{anonymous-code-B927}.", "tldr": "", "keywords": ["Pruning; MoE"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/907a19045b4f952d594429c1481cd0635126974e.pdf", "supplementary_material": "/attachment/6a66f8a6a4f27a4feab2478a5f6956c98f597b24.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces HEAPr, a novel pruning method for Mixture-of-Experts (MoE) models that decomposes experts into smaller atomic experts and leverages second-order information based on Optimal Brain Surgeon theory to assess their importance. By transforming the analysis from expert parameter space to atomic expert output space, the method reduces computational complexity and enables more precise fine-grained pruning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is well-grounded in Optimal Brain Surgeon theory and provides clear mathematical derivations for adapting second-order information to MoE architectures. The theoretical framework is comprehensive and enhances the method's credibility.\n-  The paper effectively addresses the fundamental challenge of applying Optimal Brain Surgeon theory to modern large-scale models by cleverly shifting the pruning analysis from high-dimensional parameter space to a more tractable output space, thereby overcoming the computational difficulties in MoE"}, "weaknesses": {"value": "- The experimental evaluation of the proposed HEAPr method is conducted solely on models from the DeepSeekMoE and Qwen families. This narrow scope raises concerns about the method's generalizability to other model families (such as Mixtral or GPT-OSS), and it remains unclear whether the method would maintain its effectiveness across diverse MoE implementations and architectures.\n- The evaluation is limited to commonsense reasoning and QA benchmarks, notably omitting crucial domains like code generation, which prevents a comprehensive assessment of the pruned model's full capabilities.\n-  The paper emphasizes the efficiency of HEAPr, yet the experiments provide no quantitative data to support this claim. The evaluation is missing a crucial comparison of the computational cost of the pruning process itself when compared to baseline methods.\n- The paper presents the \"atomic expert\" as a key contribution, but acknowledges and compares against concurrent work CAMERA[1], which introduces an identically defined structure called \"Micro-Expert.\" This suggests that the atomic expert pruning granularity may not be original to this work, but rather represents an emerging common approach in the field. Moreover, this pruning approach is conceptually similar to structured pruning within experts, as both involve removing corresponding columns/rows across multiple matrices.\n\n[1] CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis"}, "questions": {"value": "1. How does HEAPr perform on other major MoE model families beyond DeepSeekMoE and Qwen?\n\n2. How does HEAPr perform on other tasks?\n\n3. What are the quantitative comparisons of computational costs between HEAPr and baseline methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VFtLQYBRhk", "forum": "JAbMgS7gl6", "replyto": "JAbMgS7gl6", "signatures": ["ICLR.cc/2026/Conference/Submission447/Reviewer_UZ1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission447/Reviewer_UZ1E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760787311887, "cdate": 1760787311887, "tmdate": 1762915522473, "mdate": 1762915522473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HEAPr, a new method for expert pruning in mixture-of-experts (MoE) models. HEAPr first decomposes each expert into smaller atomic experts and then selects those whose removal results in the smallest increase in loss. Experiments across multiple model families, including DeepSeek and Qwen, demonstrate that HEAPr achieves a better compression–quality trade-off than baseline approaches in most cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is generally well written.\n\n* The proposed method is highly efficient.\n\n* The idea of decomposing each expert into smaller atomic experts is interesting and may have broader applications."}, "weaknesses": {"value": "* The notations in the paper require further clarification to improve readability. See the detailed comments below."}, "questions": {"value": "* How sensitive is HEAPr to randomness in selecting samples from a given calibration dataset? Please quantify this by adding standard deviation bars in Fig. 3, computed over multiple calibration subsets drawn with different random seeds.\n\n* About the notations and theoretical claims:\n    * Line 176: The notation E (the number of experts) is not defined. \n    * Line 215: Should the complexity term \"O((3d_{model}d_{inner})^2)\" actually be \"O((3d_{model})^2)d_{inner})\"? My reasoning is that each H^{(i)} has shape (3d_{model})^2, and there are d_{inner} such matrices. Please correct me if my understanding is incorrect.\n    * Line 220: \"the constraint in equation 2 can be equivalently enforced in the atomic expert output domain\" is not accurate. If Equation 2 is satisfied (i.e., the parameters of the atomic expert are zero), then the output of the atomic expert is indeed zero for any input. However, the reverse does not necessarily hold — even if the atomic expert’s output is zero for all inputs, its parameters are not guaranteed to be zero. Therefore, enforcing the constraint in the output domain is not strictly equivalent to enforcing it in the parameter domain. \n    * (Contining the above point) Furthermore, the algorithm does not actually ensure that \"the output of the atomic expert with respect to any input is always zero,\" since it evaluates this condition only on a limited set of samples. To be clear, I am not suggesting that the authors modify the algorithm — such simplifications and approximations are understandable and acceptable. However, the claims should be stated more precisely, and these approximations should be explicitly acknowledged and clarified in the paper.\n    *  Earlier in the paper, the input to the expert functions is the tokens (e.g., Eq. 3 and Eq. 7). However, in Line 222, the input to the expert function (e_p) becomes the parameters, and the input tokens are omitted. This inconsistency in notation may cause confusion for readers and should be clarified for consistency. \n    * Line 223: \"... denotes the parameters of the pruned atomic expert.\" Should it be the parameter of the atomic expert before pruning?\n\n* Table 1 does not consistently highlight the best results in bold. In some cases, the proposed method is shown in bold even when a baseline achieves a better result. This inconsistency could mislead readers and should be corrected to ensure fair and transparent comparison. For example,\n    * When baselines achieve the same (best) score as HEAPr, only HEAPr’s numbers are highlighted in bold: 0.32 for \"NAEE, 20% ratio, Openb. dataset\", 0.76 for \"HC-SMoE, 25% ratio, PIQA dataset\", 0.63 for \"Sub-MoE, 50% ratio, WinoG. dataset\", 0.75 for (D^2-MoE, 40% ratio, ARC_e dataset)\n    * Baselines have better results than HEAPr, yet HEAPr’s numbers are still highlighted in bold while the baselines’ numbers are not: 0.75 for \"D^2-MoE, 40% ratio, WinoG. dataset\", 0.71 for \"MoE-I^2, 40% ratio, HellaS. dataset\".\n\n* Line 367: it is mentioned that \"Table 1 are primarily drawn from the respective original papers, ensuring consistency.\" Given the use of \"primarily,\" could the authors clarify whether all baseline numbers are taken from the original papers? If not, please specify which results are directly from the original papers and which ones were reproduced or obtained otherwise.\n\n* Why are different compression ratios used for different datasets? Please clarify the rationale behind this choice.\n\n* Line 382: \"In contrast, competing new methods such as Sub-MoE, D 2-MoE, and NAEE show significant accuracy degradation under the same conditions\" appears to be an overclaim. For example, D^2-MoE performs only 0.01 lower than HEAPr, which does not constitute a *significant* degradation..\n\n* The proposed pruning strategy could also be applied directly to the original experts (rather than the atomic experts). How does this variant perform? It would be helpful to include an ablation study comparing the two settings. If pruning the original experts yields worse results, that would effectively highlight the importance of the atomic expert decomposition described in Section 3.1.\n\n* It would be helpful to discuss EEP (https://arxiv.org/abs/2407.00945), an expert pruning and merging method that appeared online over a year ago.\n\n* Other minor questions/suggestions:\n    * Line 44: lacks a conjunction in the sentence \"MoE layers typically account for over 97% of total model parameters, they represent the dominant storage bottleneck.\"\n    * Line 138: \"algorithmic\" should be \"algorithm\"\n    * Line 320: there might be a typo w.r.t. \"experiences\"\n    * Line 391: what is \\Phi?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ibh0S0zMuM", "forum": "JAbMgS7gl6", "replyto": "JAbMgS7gl6", "signatures": ["ICLR.cc/2026/Conference/Submission447/Reviewer_Y8vd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission447/Reviewer_Y8vd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639499319, "cdate": 1761639499319, "tmdate": 1762915522361, "mdate": 1762915522361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a principled framework for fine-grained MoE pruning with strong theoretical motivation from OBS theory and an efficient practical implementation. The atomic expert decomposition $E_i(x) = \\sum_j e_i^{(j)}(x)$ where $e_i^{(j)}(x) = W_{\\text{down}, i, j} [ \\text{SiLU}(W_{\\text{gate}, i, j} x) \\cdot (W_{\\text{up}, i, j} x) ]$ provides flexible pruning granularity (Eq. 5-6; Sec. 3.1; p.4). A key theoretical insight is that the cross-Hessians between atomic experts are zero ($\\frac{\\partial^2 E}{\\partial \\Theta^{(i)} \\partial \\Theta^{(j)}} = 0$ for $i \\neq j$), which enables a dramatic reduction in complexity (Eq. 7; Sec. 3.2; p.4). Experimental results show that 20% pruning on DeepSeekMoE-16B-Base maintains a 60.68 average accuracy vs. 60.03 for the layer-wise method, and 25% pruning on Qwen3-30B-A3B drops only 0.03 in average accuracy (Table 2; Sec. 4.3; p.8). However, approximations from the Taylor expansion (first-order dominance assumption) and the equivalence of Fisher information to the expected Hessian require stronger empirical validation (Eq. 9, 11; Sec. 3.2; p.5)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Principled theoretical framework with efficient complexity reduction**\n  - The OBS-based formulation $\\Delta \\ell = \\frac{1}{2} \\delta\\theta^T H \\delta\\theta$ provides a rigorous pruning criterion that minimizes the increase in loss (Eq. 1-2; Sec. 2; p.3), grounding the approach in established theory rather than heuristics.\n  - The atomic expert decomposition reveals parameter decoupling $\\frac{\\partial^2 E}{\\partial \\Theta^{(i)} \\partial \\Theta^{(j)}} = 0$, reducing the Hessian from $\\mathcal{O}((3d_{\\text{model}} \\cdot d_{\\text{inter}})^2)$ to $\\mathcal{O}((3d_{\\text{model}})^2 \\cdot d_{\\text{inter}})$ per expert (Eq. 7-8; Sec. 3.2; p.4), which makes the computation tractable.\n  - A further shift to the output space leverages the Fisher information $F = \\mathbb{E}[(\\nabla_\\Theta \\ell)(\\nabla_\\Theta \\ell)^T] = \\mathbb{E}[H]$, reducing complexity to $\\mathcal{O}(d_{\\text{model}}^2)$ with an importance score $s = \\frac{1}{2} e_P^T \\mathbb{E}[g_P g_P^T] e_P$ (Eq. 11-13; Sec. 3.2; p.4-5), achieving a dramatic improvement in efficiency that is critical for scaling.\n- **Elegant algorithmic design with minimal computational overhead**\n  - The shared gradient property $\\frac{\\partial \\ell}{\\partial e^{(i)}} = \\frac{\\partial \\ell}{\\partial E}$ enables the computation of a single gradient covariance matrix Ḡi per expert rather than per atomic expert (Eq. 14-15; Sec. 3.3; Algorithm 1; p.5-6), eliminating redundant computation.\n  - A two-stage process: (1) a backward pass for $\\bar{G}\\_i = \\frac{1}{|T\\_i|} \\sum\\_{x \\in T\\_i} g\\_{E\\_i}(x) g\\_{E\\_i}(x)^T$, and (2) a forward pass for $\\bar{s}\\_k = \\frac{1}{|T\\_i|} \\sum\\_{x \\in T\\_i} \\frac{1}{2} e\\_k(x)^T \\bar{G}\\_i e\\_k(x)$ efficiently computes all importance scores (Eq. 15-16; Algorithm 1; p.5-6), demonstrating practical elegance.\n  - Global ranking across layers based on a loss contribution metric enables consistent pruning without layer-specific thresholds (Sec. 3.2; p.5), improving coherence compared to layer-wise methods.\n- **Comprehensive experimental validation with strong empirical results**\n  - Near-lossless compression: 20% pruning on DeepSeekMoE-16B-Base achieves 60.68 avg accuracy vs. original (Table 2); 25% pruning on Qwen1.5-MoE-A2.7B-Chat reaches 53.59; 40% pruning on Qwen2-57B-A14B shows minimal degradation (Tables 1-2; Sec. 4.2-4.3; p.7-8), demonstrating effectiveness across model scales.\n  - Outperforms CAMERA-P by 1.2 accuracy points at 20% pruning with a principled global importance metric vs. a heuristic local energy (Table 2; Sec. 4.3; p.8), validating the theoretical approach.\n  - Ablations confirm the superiority of global pruning over layer-wise (HEAPr-G 60.68 vs. HEAPr-L 60.03 at 20%), robustness to calibration data sources (C4 vs. WikiText-2), and performance scaling with calibration set size (Fig. 3; Tables 2-3; Sec. 4.3; p.8-9), supporting the design choices."}, "weaknesses": {"value": "- **Approximation validity and empirical validation gaps**\n  - The Taylor expansion $e_P(\\Theta_P + \\delta\\Theta_P) \\approx e_P(\\Theta_P) + J_P \\delta\\Theta_P$ assumes first-order dominance but neglects higher-order terms $\\mathcal{O}(\\|\\delta\\Theta_P\\|^2)$(Eq. 9; Sec. 3.2; p.5). For large pruning perturbations (20-40% ratios), this approximation's accuracy is not empirically verified—no comparison of predicted vs. actual loss increases is provided.\n  - The Fisher information equivalence F=E[H] holds for well-converged networks but relies on a citation (Singh & Alistarh, 2020) without model-specific validation (Eq. 11; Sec. 3.2; p.5). Convergence analysis with varying calibration sizes is incomplete.\n  - The Taylor expansion for the atomic expert function claims that \"functions are not optimized with respect to parameters through gradient descent,\" so the first-order term dominates (Sec. 3.2; p.4), but this reasoning lacks rigorous justification or empirical loss landscape analysis.\n- **Limited scope of baseline comparisons and ablations**\n  - Comparisons focus on expert-level methods (NAEE, D2-MoE, Sub-MoE) and one concurrent atomic-level work (CAMERA-P); comparisons with recent decomposition methods (MoE-SVD, MoE-I2 at atomic granularity) or hybrid pruning+quantization approaches are missing (Sec. 4.2; Tables 1-2; p.7-8).\n  - No ablation isolates the contributions of: (1) atomic decomposition alone, (2) the OBS framework without the output space transformation, or (3) global vs. per-expert importance normalization—making it difficult to attribute gains to specific innovations.\n  - Efficiency analysis (Fig. 2; Sec. 4.3; p.8) reports FLOPs savings but omits memory footprint reduction, actual inference latency measurements, or GPU utilization metrics, which are critical for deployment assessment.\n- **Mathematical derivation completeness and notation clarity**\n  - The derivation from the constrained optimization (Eq. 10) to the importance metric (Eq. 13) is sketched, with intermediate steps \"detailed in Appendix A\" (Sec. 3.2; p.4-5). The main text would benefit from showing the Lagrangian formulation and optimality conditions.\n  - Notation switches between the atomic expert e(j)i(x)∈R^dmodel (Eq. 5) and the pruned atomic expert eP without clearly defining the relationship or indexing scheme (Eq. 9-13; Sec. 3.2; p.4-5). An explicit mapping from (i,j) to P would improve clarity.\n  - The pseudocode in Algorithm 1 uses Tk (tokens for atomic expert k) vs. Ti (tokens for expert i) inconsistently (Line 2 vs. Line 7; p.6). The relationship between expert-level and atomic-level token routing should be clarified.\n- **Reproducibility and implementation details**\n  - Calibration uses 128-256 samples, but the exact sampling strategy, sequence length handling, and token-level vs. sample-level statistics aggregation are not specified (Tables 2-3 mention a \"128-sample subset\"; Sec. 4.1 & 4.3; p.7,9). This is critical for replication.\n  - The computational complexity of the global ranking procedure is not analyzed—sorting dinter×nlayers×nexperts atomic experts could be expensive for large models (Sec. 3.2; Algorithm 1; p.5-6). Practical implementation considerations are absent.\n  - There is no discussion of numerical stability: the gradient covariance G̅i may become ill-conditioned with a small |Ti| (routing imbalance); how degenerate cases are handled is not specified (Eq. 15; Sec. 3.3; p.5-6)."}, "questions": {"value": "- **Validate approximations empirically**\n  - Compute the predicted loss increase ∆ℓpred using Eq. 12-13 for pruned models and compare it with the actual measured loss ∆ℓactual across varying pruning ratios (10-50%). Report the correlation coefficient, mean absolute error, and provide scatter plots to quantify the approximation quality.\n  - Analyze the Fisher-Hessian alignment: compute both $F = \\mathbb{E}[(\\nabla\\_\\Theta \\ell)(\\nabla\\_\\Theta \\ell)^T]$ and the empirical Hessian H (possibly via finite differences for a subset) and measure $\\frac{\\|F-H\\|\\_F}{\\|H\\|\\_F}$ Vary the calibration size $N \\in \\{32, 64, 128, 256, 512\\}$ and show convergence.\n  - Provide Taylor expansion validation: for selected atomic experts, compute the actual function change $e\\_P(\\Theta\\_P + \\delta\\Theta\\_P) - e\\_P(\\Theta\\_P)$ vs. the linear approximation $J\\_P \\delta\\Theta\\_P$ and report the approximation error distribution. Show empirically when first-order dominance holds.\n- **Expand baseline comparisons and ablations**\n  - Include comprehensive comparisons: adapt MoE-SVD/MoE-I2 to atomic granularity by applying low-rank decomposition/hybrid approaches to atomic weight groups. Compare accuracy, compression ratio, and FLOPs under matched conditions.\n  - Execute component ablations: (1) Atomic decomposition with random pruning (no OBS), (2) Expert-level OBS without atomic decomposition, (3) Atomic OBS in the parameter space (without the output space transformation), and (4) Per-expert normalized importance vs. global. Report the isolated impact of each contribution.\n  - Provide a comprehensive efficiency analysis: measure and report the memory footprint (GPU memory usage during pruning and inference), actual inference latency (ms/token), throughput (tokens/sec), and hardware utilization across models and pruning ratios in the main text.\n- **Complete mathematical derivations and clarify notation**\n  - Show the Lagrangian derivation in the main text: write $\\mathcal{L}(\\delta\\Theta_P, \\lambda) = \\frac{1}{2} \\sum (\\delta\\Theta^{(i)})^T H^{(i)} \\delta\\Theta^{(i)} + \\lambda^T (J_P \\delta\\Theta_P + e_P)$, derive the optimality conditions ∇L=0, substitute into the objective, and obtain Eq. 13. Move detailed algebra to the appendix but keep the outline in Sec. 3.2.\n  - Unify notation: explicitly define a pruning index map P:(expert_idx, atomic_idx)→global_atomic_idx and use it consistently. Clarify whether the $\\Theta^{(i)}$ indexing refers to atomic experts within one expert or globally across all experts.\n  - Revise Algorithm 1: make the $T\\_i$ vs. $T\\_k$ distinction explicit ($T\\_i$ are tokens routed to expert $i$; $T\\_k = T\\_i$ for atomic experts $k \\in \\text{expert } i$). Add complexity annotations for each line. Specify parallelization opportunities.\n- **Enhance reproducibility documentation**\n  - Detail the calibration procedure: specify the random seed for sampling, the exact sequence handling (padding/truncation strategy), batch vs. online covariance computation (streaming updates or full batch), and the token-level activation/gradient collection methodology.\n  - Analyze and report the global ranking complexity: provide the time complexity $O(K \\cdot \\log(K))$ where $K = \\text{total atomic experts}$, measure the actual sorting time vs. total pruning time, and discuss efficient implementations (e.g., approximate top-k selection for very large $K$).\n  - Address numerical stability: add regularization $\\epsilon$ to the gradient covariance $\\bar{G}\\_i \\leftarrow \\bar{G}\\_i + \\epsilon I$, specify the $\\epsilon$ value and selection rationale, and discuss routing imbalance handling when $|T\\_i| < \\text{threshold}$ (e.g., uniform fallback or expert merging). Report failure cases, if any."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9LxfrxG7gs", "forum": "JAbMgS7gl6", "replyto": "JAbMgS7gl6", "signatures": ["ICLR.cc/2026/Conference/Submission447/Reviewer_7buB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission447/Reviewer_7buB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870538235, "cdate": 1761870538235, "tmdate": 1762915522163, "mdate": 1762915522163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HEAPr, a pruning algorithm for MoE LLMs that decomposes each expert into smaller ''atomic'' sub-experts and scores them using Hessian information. By showing that cross-derivatives between atomic units are zero, the authors reduce the Hessian complexity drastically and then shift the analysis to the output space using a Fisher-Taylor approximation. In practice, HEAPr computes atomic importances with just two forward passes and one backward pass on a small calibration set, without introducing extra hyperparameters. Experiments show that HEAPr can prune 20–25% of parameters (refuce ~20% in FLOPs) with nearly no loss in accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-organized and easy to follow.\n- The method is lightweight. Importance scores are computed with only two forward and one backward pass over a small calibration set. By storing just one gradient-covariance matrix per expert, HEAPr can achieve a $O(d^2)$ storage complexity per expert.\n- HEAPr consistently outperforms existing expert-level pruning methods on multiple MoE architectures."}, "weaknesses": {"value": "Thank you for submitting your work to ICLR’26. While the paper introduces a promising method for fine-grained MoE pruning, my concerns are as follows:\n\n- (Approximation assumptions) The method assumes that atomic experts within the same expert share identical output gradients, which may not always hold in practice. Could the authors provide empirical evidence, such as measuring how well the predicted loss changes align with actual loss increases, to support the validity of this?\n\n- (Evaluation scope) The experiments focus only on several zero-shot reasoning tasks using the LM-Eval-Harness. \n\n- (Mixed results) Table 2 somehow mixes results from prior papers it is unclear if baselines were run under identical conditions. Could the authors clarify which baselines were rerun and whether all methods used consistent evaluation settings?\n\n- (Sample detail) The method relies on a calibration set (128 WikiText-2 sequences), but the sample selection process isn't described. Given that `random choices can cause 'slight variations'`, could the authors clarify how calibration samples are chosen and whether results are sensitive to this choice?\n\n- (Ablation)  Have the authors evaluated how pruning full experts compares to pruning at the atomic level? An ablation would help isolate the benefit of the decomposition strategy.\n\n- (minor) The bolding in Table 1 is sometimes inconsistent (e.g. under the Qwen2-57B-A14B model at a 40% ratio)."}, "questions": {"value": "Kindly refer to the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RHM1Sz76LN", "forum": "JAbMgS7gl6", "replyto": "JAbMgS7gl6", "signatures": ["ICLR.cc/2026/Conference/Submission447/Reviewer_Xbwv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission447/Reviewer_Xbwv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission447/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763020810361, "cdate": 1763020810361, "tmdate": 1763020810361, "mdate": 1763020810361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}