{"id": "LdcquFw3kf", "number": 17074, "cdate": 1758271853622, "mdate": 1759897199936, "content": {"title": "PeFoo-L: A General Framework for Preconditioned Enhanced Forward-Only Optimizer in LLM Fine-tuning on the Edge", "abstract": "Fine-tuning Large Language Models (LLMs) on resource-constrained edge devices is a critical but challenging task, primarily due to the prohibitive memory and computational costs of backpropagation. While forward-only optimizers like MeZO mitigate these costs by eliminating the backward pass, they often suffer from slow and unstable convergence, particularly on loss landscapes with heterogeneous curvature. To address this limitation, we introduce PeFoo, a general framework for preconditioner enhanced forward only optimizer. PeFoo integrates a carefully designed preconditioning strategy into the forward-only paradigm, corrects a fundamental source of bias and instability present in prior work HiZOO. Furthermore, to counteract the memory overhead introduced by the preconditioner itself, we propose PeFoo-L, which employs a layer-wise update strategy. This approach constrains preconditioner storage and weight updates to a single layer per iteration, reducing the overall memory footprint and data traffic.\nExperimental results validate the effectiveness of our framework. On the OPT-1.3B model, PeFoo surpasses the accuracy of leading zeroth-order methods MeZO and HiZOO by 2.7\\% and 2.1\\%, respectively. Furthermore, PeFoo-L achieves a memory footprint reduction of over 2.73$\\times$ and 1.75$\\times$ compared to Adam and HiZOO, while delivering faster convergence speed compared to MeZO and HiZOO.", "tldr": "PeFoo is a Preconditioner-Enhanced Forward-Only Optimizer that integrates a preconditioning strategy to accelerate convergence for fine-tuning Large Language Models specifically on resource-constrained edge devices.", "keywords": ["Transformer", "zeroth-order optimizer", "fine-tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/842be4073cd0fe20f814ebc0402a429642715b1a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes PeFoo and PeFoo-L, a preconditioned forward-only optimization framework for fine-tuning large language models under constrained memory settings. The method introduces a diagonal preconditioner within a zeroth-order optimization scheme to better adapt to curvature information, and further employs a layer-wise update strategy to reduce memory and bandwidth overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed PeFoo-L variant is practical, as it significantly reduces memory and storage costs while maintaining competitive performance.\n\n- Theoretical analysis is provided to support convergence and stability, adding rigor to the proposed approach."}, "weaknesses": {"value": "- The degree of novelty seems limited. The method is essentially an extension of HiZOO with refinements and engineering choices. \n\n- Experiments are primarily conducted on relatively older models (e.g., RoBERTa, OPT-1.3B). It would strengthen the paper to evaluate on more recent LLMs to demonstrate broader applicability.\n\n- HiZOO also introduced HiZOO-L version. How does your method differ in principle or in practice, and what concrete advantages does it bring over HiZOO-L? A direct comparison and deeper discussion would clarify the contribution.\n\n- It would be very useful to include more loss curves for different models and datasets to demonstrate training dynamics more clearly."}, "questions": {"value": "Seen in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uSiKrDolfP", "forum": "LdcquFw3kf", "replyto": "LdcquFw3kf", "signatures": ["ICLR.cc/2026/Conference/Submission17074/Reviewer_pp3x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17074/Reviewer_pp3x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760769659262, "cdate": 1760769659262, "tmdate": 1762927083494, "mdate": 1762927083494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PeFoo a preconditioned forward‚Äëonly ZO optimizer for on‚Äëdevice fine‚Äëtuning of LLMs, and PeFoo‚ÄëL, a memory‚Äë and bandwidth‚Äëaware layer‚Äëwise variant. In PeFoo, the standard MeZO/SPSA estimator is reparameterized with a diagonal preconditioner D, so perturbations become anisotropic $\\lambda D^{-1/2}p$. A key technical element is a moment‚Äëbased, trace‚Äëcorrected Hessian estimator that uses the same three forward passes per step to form\n$\n\\hat H ;=; \\tfrac12,\\frac{\\Delta L}{\\lambda^2},\\big(D^{1/2}pp^\\top D^{1/2}-D\\big),\\qquad\n\\Delta L=L(\\theta+\\lambda D^{-1/2}p)+L(\\theta-\\lambda D^{-1/2}p)-2L(\\theta),\n$\nand then builds a diagonal preconditioner via $D=\\mathrm{clip}(|\\mathrm{EMA}(\\hat H)|, D_{\\min}, D_{\\max})$. The paper argues this fixes two issues claimed for HiZOO (biased magnitude accumulation and an update inconsistency), and gives a convergence bound under smoothness and bounded‚Äë$D$. To reduce memory traffic and state, PeFoo‚ÄëL maintains and applies the preconditioner for only one layer per step. Experiments on OPT‚Äë1.3B and RoBERTa‚Äëlarge report accuracy with near‚ÄëMeZO memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It introduces a trace‚Äëcorrected Hessian estimator tailored to forward‚Äëonly training and a layer‚Äëwise preconditioner that targets weight‚Äëupdate bandwidth.\n2. Theoretical assumptions and proofs are standard and mostly sound; using $D_{t-1}$ consistently avoids the look‚Äëahead inconsistency flagged for HiZOO. \n3. Algorithms and figures are easy to follow."}, "weaknesses": {"value": "1. App.‚ÄØB mixes central‚Äëdifference with an $O(\\lambda^3)$ remainder. Central schemes eliminate odd terms, so the leading error is $O(\\lambda^4)$. ‚ÄúUnbiased‚Äù should be qualified as ‚Äúfor the smoothed loss, up to small $\\lambda$‚Äù. \n2. The proof hides ZO variance in a generic $\\sigma^2$ and introduces constants like $C_1=2\\beta_\\ell^{-1}+\\mathrm{tr}(D^{-1})$, implying explicit $d$‚Äëdependence in stepsize conditions. This needs discussion or a lemma bounding $\\mathbb{E}|\\nabla L_p|^2$ under the chosen $D$. \n3. Comparisons vs. Adam are not compute‚Äë or wall‚Äëclock‚Äëmatched (e.g., 20k ZO steps vs. 5 epochs on 1k examples). It makes PeFoo vs. FT conclusions hard to interpret. Similar step‚Äëbudget mismatches appear in Table‚ÄØ4. \n4. The paper clamps HiZOO‚Äôs Hessian at $10^4$ to prevent overflow, which may change its behavior; FP32/AMP results would isolate numerical causes from algorithmic differences. \n5. Alg.‚ÄØ2 reinitializes $D$ when switching layers, discarding history; no ablation quantifies whether caching last‚Äëseen preconditioners helps or hurts memory/speed. \n6. The claim that PeFoo ‚Äúpreserves negative curvature information‚Äù is true within EMA, but $|\\cdot|$ is applied at the end; please nuance the claim. Also clarify RNG reuse between perturb and update in Alg.‚ÄØ1. \n7. Other related works that might be worth comparing regarding differences including - HELENE (EMNLP 2025), ReLIZO (NeurIPS‚ÄØ2024), LOZO ICLR‚ÄØ2025."}, "questions": {"value": "1. For the central two‚Äësided difference in Eq.‚ÄØ(2), shouldn‚Äôt $\\Delta L=\\lambda^2 p^\\top D^{-1/2}HD^{-1/2}p + O(\\lambda^4)$ (not $O(\\lambda^3))$? If so, your bias term in Sec.‚ÄØ3.2 becomes $O(\\lambda^4)$. \n2. Can you bound $\\mathbb{E}|\\nabla L_p|^2$ for the preconditioned estimator in terms of $D$, $d$, and $|\\nabla L|$? This would clarify step‚Äësize choices and the role of $\\mathrm{tr}(D^{-1})$. \n3. Please add accuracy vs. forward‚Äëevaluations (plots for Adam vs. MeZO/HiZOO/PeFoo/PeFoo‚ÄëL on SST‚Äë2 and SQuAD to address the step/epoch mismatches in Tables‚ÄØ3‚Äì4. \n4. Do FP32/AMP runs of HiZOO (even at higher memory) show similar gaps? How sensitive is PeFoo to (D_{\\max}) in FP16? \n5. What happens if you cache the preconditioner for the $k$ most‚Äërecent layers instead of reinitializing to $I$? Is there a sweet spot between memory and speed? \n6.  Which Nsight metrics underpin Table‚ÄØ2 and Fig.‚ÄØ4? Were GPU clocks fixed? Please include batch size and whether numbers include cache line write‚Äëbacks. \n7. A small sweep of perturbation scale $\\lambda$ would inform the $O(\\lambda^2)$/variance trade‚Äëoff assumed in Sec.‚ÄØ3.2. \n8. Did you try block‚Äëdiagonal $D$ (e.g., per‚Äëmatrix blocks)? Even one experiment could show whether extra structure is worth the added traffic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wEKh0yjI9Q", "forum": "LdcquFw3kf", "replyto": "LdcquFw3kf", "signatures": ["ICLR.cc/2026/Conference/Submission17074/Reviewer_UY7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17074/Reviewer_UY7c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873028425, "cdate": 1761873028425, "tmdate": 1762927083197, "mdate": 1762927083197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm PeFoo for the preconditioned zeroth-order (ZO) optimization in LLM training. Previous work HiZOO has adopted the estimated Hessian information into the ZO gradient to accelerate the convergence. However, some fundamental issues lead HiZOO to converge in suboptimal paths. In contrast, PeFoo designs an unbiased estimation of the Hessian matrix to perform the EMA update coupled with clipping. Additionally, it utilizes the Hessian matrix in the last iteration instead of the fresh version in the current iteration to calculate the projected gradient. \n\nThis work then provides the convergence guarantee for PeFoo. In the experiments of fine-tuning two LLMs, an extension version PeFoo-L that adopts the layerwise update rule is proposed to further reduce the memory cost. PeFoo outperforms baselines in most downstream tasks, and PeFoo-L significantly reduces the GPU usage. Further studies show that PeFoo-L indeed reduces the memory cost of weight update, and the techniques in preconditioner clamping are effective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach has two main innovations: one is an unbiased Hessian estimator with EMA and clipping, and another is the utilization of the past preconditioner. The experimental results reveal that PeFoo could enhance the performances of MeZO and HiZOO. The ablation studies are also detailed. Thus, these evidences could support the claims of the paper. The presentation of this work is also clear and easy to understand."}, "weaknesses": {"value": "Some statements lack further explanation or support from references. For instance, it is said in \"Introduction\" that ‚Äúour analysis reveals that HiZOO‚Äôs Hessian estimator is inherently biased due to the premature application of an absolute value function‚Äù, but I do not find the related analysis in the following chapters. \n\nThe idea of designing the algorithm is also not clear. It is said that ‚ÄúHiZOO‚Äôs estimation tends to be larger than PeFoo‚Äôs unbiased estimation and does not account for the possibility of negative elements in the Hessian matrix, leading to saddle point attraction artifacts‚Äù. Firstly, how to understand that ‚Äúestimation tends to be larger‚Äù? Secondly, this assertion is not analyzed or explained. Then for the ‚Äúweight update step‚Äù, this work also does not make detailed explanation for ‚ÄúThis inconsistency is the reason for the large variance in HiZOO‚Äôs Hessian matrix estimation‚Äù. Thus, the motivation of proposing such an algorithm and the reason it performs well are not clear. In addition, the colors of the curves in Figures 2 and 3 are too similar to distinguish."}, "questions": {"value": "In Figure 1, it shows that MeZO performs two perturbations in one iteration. Is this right? I think MeZO seem to also perform three perturbations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "swvisA5IdA", "forum": "LdcquFw3kf", "replyto": "LdcquFw3kf", "signatures": ["ICLR.cc/2026/Conference/Submission17074/Reviewer_8XKo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17074/Reviewer_8XKo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966863583, "cdate": 1761966863583, "tmdate": 1762927082901, "mdate": 1762927082901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PeFoo, a preconditioner‚Äëenhanced forward‚Äëonly (ZO/SPSA) optimizer for fine‚Äëtuning LLMs under tight memory budgets, and PeFoo‚ÄëL, a layer‚Äëwise variant that keeps only one layer‚Äôs preconditioner active per step to cut storage and DRAM traffic. Methodologically, PeFoo injects a diagonal preconditioner D into both perturbation and projected‚Äëgradient formulasÔºå and claims a corrected, ‚Äúunbiased‚Äù Hessian estimator plus a weight‚Äëupdate rule consistent with the estimator, addressing instability of HiZOO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Preconditioner D shapes both perturbations and projected gradients, aligning exploration with local curvature.\n\n2. The paper corrects HiZOO‚Äôs estimator/weight‚Äëupdate mismatch and preserves Hessian sign before taking absolute value post‚ÄëEMA, which the ablation shows improves stability and accuracy.\n\n3. Meaningful empirical gains under FP16, and shows concrete peak‚Äëmemory savings vs Adam/HiZOO"}, "weaknesses": {"value": "1. The derivation hinges on second‚Äëorder central differences with an O(\\lambda^3) remainderÔºå but Sec.‚ÄØ3.1 repeatedly calls the estimator ‚Äúunbiased‚Äù and builds claims on it. In practice the estimate is for L(\\theta;B), not the population objective, and the bias/variance from minibatching is never analyzed. \n\n2. The paper advertises preserving negative curvature (p.‚ÄØ2), but the preconditioner actually used in updates is PSD by construction. Signs exist only inside EMA and are discarded before use; no theory is offered for why ‚Äúabs after EMA‚Äù should beat ‚Äúabs before EMA‚Äù beyond a single ablation.\n\n3. H^t depends on ùê∑_(t-1}, Alg.‚ÄØ1 line‚ÄØ16); Dt is then built from H^t . This feedback could amplify noise and cause oscillations. The paper relies on hand‚Äëpicked clamps ùê∑min=0.1 and ùê∑max=10^4,  and a single‚Äëtask curve, with no stability analysis.\n\n4. the paper claims O(d) memory by using a diagonal D, but according to algorithm 1, implementation is O(d^2).\n\n5. Alg.‚ÄØ2 resets the preconditioner to ùêº, when switching layers (line‚ÄØ5), repeatedly erasing accumulated geometry; the ‚Äúlast‚Äëto‚Äëfirst‚Äù schedule and ‚Äúswitch every 40 steps‚Äù are empirical only, with no principled rationale or ablations.\n\n6. PeFoo‚ÄëL is essentially preconditioned block‚Äëcoordinate forward‚Äëonly. The paper does not compare against peer block/layer methods under the same scheduling and budget, making it unclear whether gains stem from the preconditioner itself or from the scheduling"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PHf315L6MD", "forum": "LdcquFw3kf", "replyto": "LdcquFw3kf", "signatures": ["ICLR.cc/2026/Conference/Submission17074/Reviewer_iag3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17074/Reviewer_iag3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979658730, "cdate": 1761979658730, "tmdate": 1762927082585, "mdate": 1762927082585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}