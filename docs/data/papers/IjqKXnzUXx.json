{"id": "IjqKXnzUXx", "number": 5200, "cdate": 1757864714877, "mdate": 1759897988865, "content": {"title": "Phantom-Data:  Towards a General Subject-Consistent Video Generation Dataset", "abstract": "Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce \\textbf{Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset}, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines.", "tldr": "", "keywords": ["Video Generation", "Multimodal Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5df2baa7dc7e4e987f8214c857fdccdf1d71d19f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles a central pain point in subject-consistent video generation—the “copy–paste” failure mode that emerges from in-pair training, where a reference frame and target video come from the same clip and the model inadvertently binds identity to background/pose. The authors introduce **Phantom-Data**, a large-scale **cross-pair** dataset (1M identity-consistent pairs across humans, animals, products, IP characters), and a three-stage pipeline to build it: (1) S2V detection via open-vocabulary grounding with quality checks (completeness/specificity/text match), (2) large-scale retrieval from **~53M videos + 3B images** with category-specialized encoders, and (3) prior-guided identity verification (e.g., logo presence for products; same long video for living beings) plus a VLM check to ensure both identity match and context diversity (Fig. 4, p.5). \n\nUsing Phantom-Data to train an open-source subject-consistent model (Phantom-wan), the paper reports clear gains in **prompt following** and **video quality** while keeping **identity consistency** comparable to in-pair baselines (Table 2, p.7). Qualitative examples (Fig. 5, p.8), ablations on subject diversity/scale (Tables 3–4, p.9), and a user study (Table 5, p.15) corroborate the improvements and the importance of cross-pair diversity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* **Addresses a key failure mode.** Clearly identifies and targets the copy–paste issue caused by in-pair supervision, and proposes a principled data solution rather than ad-hoc augmentations (Fig. 2, p.3; Sec. 3). \n* **Well-designed, scalable pipeline.** The three-stage pipeline (detection → cross-context retrieval → prior-guided verification) is systematic, with concrete checks (completeness/specificity; upper/lower similarity thresholds; VLM verification) that materially reduce false positives (Fig. 4–6, pp.5–9). \n* **General beyond faces.** Moves cross-pair training from face-only domains to **general subjects** (humans, animals, products, multi-subject scenes), aligning with real user inputs (Fig. 3(d–e), p.4; Table 1, p.3). \n* **Fair, controlled comparisons.** Same base model, objective, resolution, and inference settings across training regimes; metrics cover text alignment (Reward-TA), identity (DINO/GPT-4o scores), and video quality (VBench), with both quant and qual results (Sec. 4, Table 2, Fig. 5). \n* **Clear, strong empirical gains.** Cross-pair training with Phantom-Data improves prompt alignment and overall video quality, with identity consistency on par with in-pair training (Table 2); ablations show benefits from subject diversity and scaling from 100k → 1M pairs (Tables 3–4)."}, "weaknesses": {"value": "* **Ambiguity around the “Face Cross-pair” baseline (Table 2).** The paper states this baseline “utilizes face-level identity matching across videos,” but the exact construction is unclear relative to **Ours**. Did “Face Cross-pair” (i) rely solely on ArcFace-based retrieval without the **prior-guided verification** step, (ii) omit **clothing/body** features for people, and/or (iii) exclude **non-face** subjects entirely? Clarifying the settings would explain the large Reward-TA gap (3.022 vs. 3.827) and the differences in DINO/GPT-4o scores (Table 2, p.7)."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uyNn1GTjtl", "forum": "IjqKXnzUXx", "replyto": "IjqKXnzUXx", "signatures": ["ICLR.cc/2026/Conference/Submission5200/Reviewer_czCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5200/Reviewer_czCa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760513540401, "cdate": 1760513540401, "tmdate": 1762917941383, "mdate": 1762917941383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Phantom-Data is a large-scale, general-purpose dataset designed to address the \"copy-paste\" problem in subject-to-video (S2V) generation. Existing models often fail to follow textual prompts because they are trained on in-pair data (reference and target frames from the same video), leading them to copy irrelevant background details from the reference image. Phantom-Data mitigates this by providing \"cross-pair\" data: identity-consistent reference and target frames from different videos or contexts.\nExperiments using the Phantom-wan model show that training with Phantom-Data significantly improves text-prompt alignment and video quality compared to in-pair training baselines, while maintaining high identity consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The \"copy-paste\" problem is a major, well-known limitation in current open-source video generation models. Tackling this via better data is a highly practical and valuable approach.\n\n2 Unlike previous cross-pair datasets that mostly focused on faces, this dataset covers general objects (products, animals, etc.) and is very large (1 million pairs)\n\n3. The data construction pipeline is well-designed. Specifically, combining large-scale retrieval with strict VLM-based verification  is a smart way to automate generating high-quality pairs without human annotation."}, "weaknesses": {"value": "1. While the dataset is a great contribution, the pipeline requires an enormous pool of data (53M videos, 3B images) to work effectively. This scale is likely out of reach for most academic labs to replicate or extend on their own.\n\n2. The experimental results primarily compare different custom data setups (in-pair, in-pair + aug, face cross-pair) against their proposed method. There is no comparison against models trained on currently existing public datasets to provide a true external baseline.\n\n3. While quantitative metrics are provided for the different data ablation setups (Table 3), visual examples comparing the outputs of these specific ablations (e.g., \"face only\" vs. \"+ product\" vs. \"+ multi-subject\") would strengthen the argument for the necessity of each data component."}, "questions": {"value": "1. Table 1 states the dataset is \"Publicly Available\". Does this mean the full 1 million pairs (images and corresponding video clips) will be directly downloadable, or will it be a list of URLs/IDs that users need to scrape themselves?\n\n2. Regarding the \"same long-form video\" (L339) constraint for living beings: Did you experiment with trying to find the same human/animal across completely different videos?\n\n3. What is the estimated error rate of the final VLM verification step? Are there many valid cross-pairs that get thrown out because the VLM is too conservative?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rI1CY391CJ", "forum": "IjqKXnzUXx", "replyto": "IjqKXnzUXx", "signatures": ["ICLR.cc/2026/Conference/Submission5200/Reviewer_zkPr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5200/Reviewer_zkPr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811014602, "cdate": 1761811014602, "tmdate": 1762917940741, "mdate": 1762917940741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Phantom-Data, a cross-pair subject-to-video consistency dataset for video personalization, comprising one billion identity-consistent pairs across diverse categories. Experiments indicate the high quality and potential impact of the dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a well-designed pipeline for curating subject-video paired datasets, leveraging Visual Language Models (VLMs) to achieve robust subject-attribute pairing.\n- The dataset is large-scale, providing valuable resources for advancing research in video personalization.\n- The curated data effectively addresses the prevalent copy-paste issue encountered in video personalization tasks."}, "weaknesses": {"value": "- It remains unclear how much the VLM-based pipeline improves over simpler approaches, such as using GPT-4o for generating image variations. As shown in Figure 7, though having drawbacks, generative models might sometimes even offer advantages in achieving controllable variations. Additionally, potential artifacts from generative approaches could be mitigated through VLM-driven verification and filtering, which the paper does not explore.\n- The evaluation is somewhat limited, as the dataset is tested exclusively with Wan2.1. Given that many established methods in video personalization predate the Wan series, broader evaluation across multiple models would strengthen the paper’s claims and demonstrate the wider utility of the dataset."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Z3DZKRv2u", "forum": "IjqKXnzUXx", "replyto": "IjqKXnzUXx", "signatures": ["ICLR.cc/2026/Conference/Submission5200/Reviewer_EDtH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5200/Reviewer_EDtH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934872489, "cdate": 1761934872489, "tmdate": 1762917940415, "mdate": 1762917940415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Phantom-Data, a large-scale cross-pair dataset for subject-consistent video generation, aiming to mitigate the copy-paste problem caused by in-pair training. The dataset is built via a three-stage pipeline combining open-vocabulary detection, cross-context retrieval, and prior-based identity verification. Experiments show improved prompt alignment and video quality while maintaining subject consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and addresses a real issue (“copy-paste”) in subject-consistent video generation.\n2. The dataset construction pipeline is well-designed, integrating VLM-based detection and verification modules.\n3. The ablation and user study provide some evidence of effectiveness."}, "weaknesses": {"value": "1. The prompts used in the dataset are overly simple and sparse, failing to accurately describe video semantics or capture complex spatiotemporal relations. This limits the dataset’s potential to improve text-video alignment in realistic scenarios.\n2. The paper evaluates only on the Phantom-Wan model. Without experiments on other open-source frameworks (e.g., CogVideoX, VACE, HunyuanVideo), it’s hard to judge whether the dataset quality generalizes.\n3. The paper compares mainly with older datasets, while recent large-scale subject-consistent datasets are not included. It is unclear how Phantom-Data performs relative to the latest baselines, such as OpenS2V."}, "questions": {"value": "As seen in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nU41AX1YfL", "forum": "IjqKXnzUXx", "replyto": "IjqKXnzUXx", "signatures": ["ICLR.cc/2026/Conference/Submission5200/Reviewer_Qtiu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5200/Reviewer_Qtiu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977216315, "cdate": 1761977216315, "tmdate": 1762917940140, "mdate": 1762917940140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}