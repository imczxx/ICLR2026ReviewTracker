{"id": "YTgJA2m5B0", "number": 11843, "cdate": 1758204212196, "mdate": 1759897551243, "content": {"title": "PAMA: Dual-Memory Augmentation Assisted Pseudo-Anomaly Contrastive Learning for Multivariate Time Series Anomaly Detection", "abstract": "For multivariate time series anomaly detection, most methods assume that training data are clean and ignore the characteristics of anomalous data. They often suffer from the overgeneralization problem during the reconstruction process. To address these problems, dual-memory augmentation assisted pseudo-anomaly contrastive learning for multivariate time series anomaly detection (shorted as PAMA) is proposed. First, the prior knowledge of anomalies is utilized to generate pseudo anomalies from the original time series. The normal and pseudo-anomalous feature representations that contain global and local information are respectively achieved by the global and local encoders. Two independent memory modules are constructed to further memorize normal and pseudo-anomalous prototypes. Second, a dual-memory augmentation mechanism is proposed to conduct data augmentation upon the normal and pseudo-anomalous feature representations and then obtain the memory-augmented feature representations. Third, the pseudo-anomaly contrastive learning is proposed to perform temporal contrastive learning and instance contrastive learning on the obtained memory-augmented representations. Compared with the thirteen baseline methods, the experimental results demonstrate that PAMA achieves the optimal detection performance.", "tldr": "Dual-Memory Augmentation Assisted Pseudo-Anomaly Contrastive Learning for Multivariate Time Series Anomaly Detection", "keywords": ["Multivariate time series anomaly detection", "Contrastive learning", "Dual-memory augmentation", "Pseudo anomaly", "Uncertainty learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/488bb71e1ef5c4a62fc743e85ad806e646976cd3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the issue that time-series anomaly detection models tend to overgeneralize and become insensitive to anomalies. It proposes a method that artificially generates anomalies based on prior knowledge and employs contrastive learning to distinguish between normal and abnormal representations, thereby enhancing the model’s sensitivity to anomalies."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-organized, logically structured, and easy to follow.\n\n2. The authors conduct comparative experiments with baselines on multiple datasets, demonstrating some improvements in F1 score achieved by the proposed method."}, "weaknesses": {"value": "1. Experimentally, the improvement in F1 score achieved by the proposed method is quite limited — on average, it does not exceed 0.5% compared to the best baseline.\n\n2. In terms of methodological soundness, the paper lacks sufficient justification regarding whether the prior knowledge used for artificially generating anomalies is reasonable. Moreover, it is highly questionable whether adding random perturbations and scaling the maximum amplitude component in the spectrum can effectively cover all possible types of anomalies.\n\n3. This approach, which relies on artificially generated anomalies and learning their representations, may ironically weaken the anomaly detection model’s ability to generalize — that is, to correctly identify unseen anomalies that were not observable during training.\n\n4. The core problem the paper aims to address is the model’s overgeneralization and its inability to effectively capture anomalies. However, according to the experimental results, the proposed method does not show an improvement in recall under the threshold selection strategy adopted by the authors, which undermines the claim that the method can mitigate the overgeneralization issue."}, "questions": {"value": "1. Could the authors provide a concrete justification or analysis demonstrating how the prior knowledge used to generate anomalies aligns with the actual distribution of anomalies in real-world applications?\n\n2. Could the authors conduct additional experiments to verify that the proposed method indeed enhances anomaly sensitivity and mitigates model overgeneralization? For example, under the condition of achieving the best F1 score, could they compare the recall rates across different methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hSmOqLI6ug", "forum": "YTgJA2m5B0", "replyto": "YTgJA2m5B0", "signatures": ["ICLR.cc/2026/Conference/Submission11843/Reviewer_Za4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11843/Reviewer_Za4k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624026519, "cdate": 1761624026519, "tmdate": 1762922860627, "mdate": 1762922860627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PAMA, a multivariate time-series anomaly detection (MTSAD) framework that (i) synthesizes pseudo anomalies informed by prior knowledge, (ii) learns separate prototype memories for normal and anomalous patterns, and (iii) trains with a tailored contrastive objective to mitigate over-generalization when training data contain noise/anomalies. Pseudo-Anomaly Generation (PAG) perturbs trends (random linear drifts) and seasonality (scaling the dominant FFT frequency per channel) within randomly sampled windows to produce pseudo anomalies. Dual-Memory Augmentation (DMA) store prototypes; similarities between representations and prototypes (sigmoid of scaled dot-product) are used to produce memory-augmented features. For Pseudo-Anomaly Contrastive Learning (PACL), temporal (adjacent positives for normal, pseudo-anomaly negatives) and instance-level (cross-memory positives/negatives) losses are combined. Uncertainty Learning (UL) adds a KL-regularized variational head and a distance loss to stabilize normal representations; the decoder reconstructs from concatenated (UL-refined + original) features. Anomaly score multiplies latent distance to nearest normal prototype with reconstruction error."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The division into PAG → DMA → PACL → UL is coherent; figures/equations formalize the pipeline and losses precisely.\n- Separating normal vs. pseudo-anomalous prototype stores is a sensible mechanism to avoid “all-normal” memories that over-reconstruct anomalies.\n- The temporal loss uses adjacency as positives and pseudo-anomalies as negatives; the instance loss leverages cross-memory augmentation—both are appropriate for time dependence and class asymmetry."}, "weaknesses": {"value": "- The paper claims to “combine/utilize prior knowledge of anomalies” to generate pseudo anomalies, but in practice this “prior” is instantiated as generic, hand-crafted perturbations—adding random linear trends per channel and scaling the dominant FFT frequency in short windows—rather than knowledge derived from domain experts, causal structure, or external metadata. A more precise framing—and an empirical audit of how realistic these perturbations are for multivariate, cross-channel failure modes—would improve conceptual clarity.\n\n- PAG perturbs the per-channel dominant frequency and injects linear trends. This may not capture multivariate cross-channel anomalies (e.g., lagged inter-sensor faults, topology-induced correlations) and could bias PACL toward single-channel spectral artifacts.\n\n- Although the paper claims improved performance broadly, it remains unclear how PAMA handles anomalies with weak spectral signatures or non-stationary multi-sensor couplings (e.g., actuator-sensor loops) given the PAG design and memory size (10 prototypes/module) reported.\n\n- The paper does not benchmark against CAROTS (ICML 2025)—which uses causality-preserving/disturbing augmentations and a similarity-filtered one-class contrastive loss—nor against CARLA (Pattern Recognition 2025)—which leverages contrastive learning with anomaly injection to sharpen the normal boundary. Because PAMA’s core claims also hinge on pseudo anomalies and contrastive objectives, comparison with these baselines are necessary to position its contribution relative to these state-of-the-art approaches.\n\n- Please include F1 without point adjustment (and, ideally, the corresponding precision/recall) under the same thresholds and preprocessing per dataset. This will let readers assess robustness to the choice of point adjustment and quantify any inflation gap between adjusted vs. raw F1.\n\n- Table 5 shows U-Transformer achieving strikingly high Aff-p on all datasets, and on SWaT, the performance of PAMA is notably lower than some baselines. Please provide a targeted discussion diagnosing these patterns.\n\n- Figure 1’s caption should succinctly narrate the pipeline and define symbols. For Figures 3–4, reducing whitespace and margins and adding richer yet compact results would improve readability and interpretability."}, "questions": {"value": "- PAG scales the dominant per-channel frequency and adds linear trends. How do you ensure these perturbations faithfully approximate multivariate anomalies where the signature is cross-channel phase/lag structure rather than single-channel spectra?\n\n- What is the trade-off curve between prototype count and performance, and how does prototype collapse or redundancy get avoided during training?\n\n- In the main text, the temporal/instance contrastive losses (Eqs. (7)–(9)) are written as raw dot products without a temperature or feature normalization term, whereas the appendix reports sensitivity to “temperature parameters. Does \\tau refer to a contrastive temperature (as in InfoNCE), and if so, where is it inserted in Eqs. (7)–(9)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CHiD33vG1L", "forum": "YTgJA2m5B0", "replyto": "YTgJA2m5B0", "signatures": ["ICLR.cc/2026/Conference/Submission11843/Reviewer_FXzj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11843/Reviewer_FXzj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722057232, "cdate": 1761722057232, "tmdate": 1762922860230, "mdate": 1762922860230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method that (i) synthesizes pseudo-anomalies, (ii) stores normal and pseudo-anomalous patterns in separate memories, (iii) drives their representations apart, and (iv) leverages both normal and anomalous patterns during training. Across multiple baselines, the average F1 score is notably strong."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors revisit the common assumption that training data are nearly anomaly-free and address its limitations by introducing a dual-memory design, one for normal patterns and one for pseudo-anomalies, for detection. This represents a novel direction relative to prior work.\n- As the ablation studies show, combining PAMA's components yields clear, quantitative performance gains."}, "weaknesses": {"value": "- The Abstract states that *\"Most methods assume that training data are clean and ignore the characteristics of anomalous data,\"* yet it remains unclear how PAMA concretely resolves this in practice. Although the numerical experiments use standard time‑series anomaly‑detection benchmarks, direct evidence that the stated challenge is specifically addressed would be helpful. \n- While reexamining the *\"few anomalies in training data assumption\"* is novel, the dual-memory module appears structurally similar to prior work, leaving the degree of methodological novelty somewhat unclear. Please clarify the precise differences between CutAddPaste (with PAG) and MEMTO (with a dual-memory module).\n- It is difficult to assess the statistical significance of the results in Tables 1 and 2. Because the proposed method introduces several hyperparameters, the robustness of the reported gains to these choices is not yet evident.\n- Relative to MEMTO, training time and memory consumption are substantially higher."}, "questions": {"value": "- Please state clearly how CutAddPaste + PAG differs from MEMTO + dual memory, both conceptually and operationally.\n- Is the approach restricted to pseudo-anomalies based on trend and seasonality, or does it generalize to other anomaly types?\n- How sensitive is performance to the entropy loss, i.e., how much does it change when this term is removed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rWs8n18bds", "forum": "YTgJA2m5B0", "replyto": "YTgJA2m5B0", "signatures": ["ICLR.cc/2026/Conference/Submission11843/Reviewer_dJLQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11843/Reviewer_dJLQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832206846, "cdate": 1761832206846, "tmdate": 1762922859849, "mdate": 1762922859849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PAMA (Dual-Memory Augmentation Assisted Pseudo-Anomaly Contrastive Learning), a novel method for multivariate time series anomaly detection (MTSAD). The main contributions of the paper are:\n\n(1) A modified Pseudo-Anomaly Generation (PAG) module that creates realistic pseudo-anomalies by introducing both trend and seasonal perturbations to the original time series.\n\n(2) A Dual-Memory Module (DMM) that uses two separate memory banks to store prototypes for normal and pseudo-anomalous patterns, respectively.\n\n(3) A Dual-Memory Augmentation (DMA) mechanism that leverages these distinct memory modules to perform data augmentation on the feature representations.\n\n(4) A Pseudo-Anomaly Contrastive Learning (PACL) framework that applies both temporal and instance-level contrastive objectives to the memory-augmented representations.\n\nExperimental results show that PAMA outperforms 13 relevant baselines on 5 benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) The core contribution, the dual-memory architecture for explicitly storing and utilizing both normal and pseudo-anomalous prototypes, is novel and well-motivated. It provides a principled way to leverage generated anomalies beyond just being negative samples in a contrastive loss.\n\n(2) The paper presents extensive experiments on 7 datasets against several baselines. Moreover, the ablation studies and parameter analyses provide strong empirical support for the proposed architecture.\n\n(3) Overall, the paper is clearly written. The motivation is compelling and the experimental findings are presented effectively."}, "weaknesses": {"value": "(1) PAMA involves a large number of hyperparameters that require tuning. The sensitivity analysis shows that performance can fluctuate significantly with changes in these values, particularly on the MSL dataset. This suggests that the model may require careful and extensive tuning for each new dataset, which could be a practical limitation.\n\n(2) Figures 3 and 4 are inconsistent with the main experimental setup, which is reported on 5 datasets. Figure 3 excludes SMD and Figure 4 excludes both SMD and SWaT, without any explanation. It is recommended to include all 5 datasets in both figures for completeness, as the space does not appear to be a limiting factor.\n\n(3) There are some minor issues with the writing and presentation quality of the paper.\n- The presentation of related work in the introduction contains a confusing chronological error. After discussing methods from 2023 and 2024, the paper incorrectly frames a 2018 paper as a \"subsequent study\", which weakens the logical flow.\n- In the last sentence of Related Work (Lines 121-122), the authors claim that pseudo-anomalies from prior work are \"significantly different from the real anomalies\" without providing any supporting evidence. This part would be stronger if the authors briefly explained why."}, "questions": {"value": "(1) The strongest baseline, H-PAD, is missing in Table 2. Can you provide the AUC-ROC / AUC-PR results for H-PAD as well?\n\n(2) In Lines 331-333, it says PAMA significantly improves over CAE-AD on MSL, SMAP, and SMD. However, the results of CAE-AD for SMD are not reported in Table 1. Is \"SMD\" a typo of \"SWaT\"?\n\n(3) The submission currently lacks an anonymous repository link and does not include any supplementary material. Do the authors plan to make the implementation code public to ensure the reproducibility of this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MVh1tZCfIz", "forum": "YTgJA2m5B0", "replyto": "YTgJA2m5B0", "signatures": ["ICLR.cc/2026/Conference/Submission11843/Reviewer_WtDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11843/Reviewer_WtDd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913503440, "cdate": 1761913503440, "tmdate": 1762922859340, "mdate": 1762922859340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}