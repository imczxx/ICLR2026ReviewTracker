{"id": "2GhjkigNTx", "number": 10428, "cdate": 1758171032367, "mdate": 1759897651117, "content": {"title": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs", "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks. However, Chinese LLMs face unique challenges, primarily due to the dominance of unstructured free text and the lack of structured representations in Chinese corpora. While existing benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly English-centric and fail to address the unique linguistic characteristics of Chinese, lacking structured datasets essential for robust evaluation. To address these challenges, we present a \\underline{\\textbf{C}}omprehensive \\underline{\\textbf{B}}enchmark for \\underline{\\textbf{E}}valuating \\underline{\\textbf{C}}hinese \\underline{\\textbf{L}}arge \\underline{\\textbf{L}}anguage \\underline{\\textbf{M}}odels (CB-ECLLM) based on the newly constructed Chinese Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million aligned text pairs, each consisting of unstructured text coupled with one or more corresponding triples, alongside a total of 15 million triples spanning four critical domains. The core contributions of CDTP are threefold: (i) enriching Chinese corpora with high-quality structured information; (ii) enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii) supporting multi-task fine-tuning to assess generalization and robustness across scenarios, including Knowledge Graph Completion, Triple-to-Text generation, and Question Answering. Furthermore, we conduct rigorous evaluations through extensive experiments and ablation studies to assess the effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark. To support reproducible research, we offer an open-source codebase\\footnote{Code and data are available at: \\href{https://anonymous.4open.science/r/CDTP-2D04}{https://github.com/CDTP.git}} and outline potential directions for future investigations based on our insights.", "tldr": "", "keywords": ["Chinese Data-Text Pair Dataset", "Large Language Model", "Chinese Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee9dfd5987795b45cf64fb063e9839d6d550ba8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CDTP, a dataset of aligned Chinese text–triple pairs. It also porpose a CB-ECLLM benchmark to evaluate three representative tasks: Knowledge Graph Completion (KGC), Triple-to-Text Generation (T2T), and Question Answering (QA). The experiment evaluates 8 LLMs to assess effectiveness, SFT gains, and OOD robustness."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The dataset is large, it might be useful for Chinese NLP community.\n\n2. The paper provides a detailed data construction process, and promises to open source code and data to facilitate reproducibility."}, "weaknesses": {"value": "1. The paper claims that Chinese language poses a challenge to LLM (L46), so this dataset was constructed. However, there is no discussion on why this dataset can help Chinese LLM training. The example in Tab. 1 could be reproduced in alternative languages.\n\n2. L245 `we selected eight state-of-the-art models'. These models are not representative because they only include 7-9b open source models and lack other scale and closed source commercial models as baseline. Also, these models are not state-of-the-art, some of them are published in 2023.\n\n3. The effectiveness of the evaluation tasks. The paper proposes three tasks: KGC, T2T, and QA, but does not discuss the justification for these task designs. For example, why should LLMs be evaluated using triples instead of natural language? These tasks also fail to represent the challenges LLM faces when constructing knowledge graphs. For example, in the example L805, only a person's name is provided and the model is asked to complete the birth date, while the options include different years. This question does not clarify the person being referred to and cannot constitute a valid evaluation. If this question provides more context in the form of natural language, it seems to be more effective."}, "questions": {"value": "See `Weakness' section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vISvgh57B7", "forum": "2GhjkigNTx", "replyto": "2GhjkigNTx", "signatures": ["ICLR.cc/2026/Conference/Submission10428/Reviewer_YJ4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10428/Reviewer_YJ4r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396036767, "cdate": 1761396036767, "tmdate": 1762921734916, "mdate": 1762921734916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **CDTP**, a large-scale and meticulously aligned **Chinese Data-to-Text Pair dataset** with over **7 million pairs and 15 million triples**, accompanied by a **Comprehensive Benchmark for Evaluating Chinese LLMs (CB-ECLLM)**. It evaluates multiple representative LLMs across **KGC, T2T, and QA tasks**, including both base and SFT settings, and provides extensive analysis on **effectiveness, generalization, and robustness**. The work fills a critical gap in Chinese LLM evaluation and provides valuable infrastructure for the community."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **High impact and originality.**\n\n   The paper addresses an urgent need for structured Chinese evaluation resources, complementing existing English-centric benchmarks such as WebNLG and KELM. The integration of triple-text alignment at such scale and quality is genuinely novel.\n\n2. **Rigorous dataset construction.**\n\n   The authors clearly describe multi-stage collection, cleaning, and validation pipelines, with redundancy filtering and external verification. The inclusion of four balanced domains ensures representativeness and cross-domain generalization.\n\n3. **Comprehensive benchmark design.**\n\n   The benchmark spans three core tasks (KGC, QA, T2T), enabling both structured-to-text and text-to-structure reasoning evaluation. The metrics and task formulation are well-motivated and comparable to international standards.\n\n4. **Extensive experimental analysis.**\n\n   Evaluation over eight diverse LLMs and detailed post-SFT/OOD robustness studies show impressive thoroughness. The visualizations (Figures 2–5) and interpretation of observations (1-7) demonstrate strong empirical insight."}, "weaknesses": {"value": "1. **Limited modality coverage.**\n   The dataset focuses solely on textual and structural information; integrating visual or multimodal signals (e.g., image-caption-triple) could further enhance generalization.\n\n2. **Cross-benchmark comparison missing.**\nIt would be informative to include a cross-evaluation against existing Chinese benchmarks (C-Eval, CMMLU, SuperCLUE, GraphEval) to highlight complementary strengths and illustrate where CDTP provides unique coverage.\n\n3. **Error and bias analysis.**\n   A more detailed qualitative study of typical failure cases (e.g., entity ambiguity, idiomatic errors) would enrich the interpretability of the results."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "amvr1u0sk1", "forum": "2GhjkigNTx", "replyto": "2GhjkigNTx", "signatures": ["ICLR.cc/2026/Conference/Submission10428/Reviewer_SJxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10428/Reviewer_SJxD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708012655, "cdate": 1761708012655, "tmdate": 1762921734428, "mdate": 1762921734428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents CDTP, a large-scale Chinese Data-to-Text Pair dataset consisting of over 7 million aligned text–triple pairs (15 million triples) across four domains, together with the CB-ECLLM benchmark evaluating Chinese LLMs on Knowledge Graph Completion (KGC), Triple-to-Text (T2T), and Question Answering (QA). The authors document dataset construction, cleaning, and validation, and conduct extensive multi-model experiments, including Supervised Fine-Tuning (SFT) and Out-of-Distribution (OOD) robustness analysis."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**1**  Scale and alignment quality: The combination of large scale (7M pairs, 15M triples) and careful alignment via human checks and retrieval-based validation creates a rare and valuable resource for the Chinese community.\n\n**2**  Well-scoped task design: The unified benchmark spans KGC/QA/T2T and explicitly accounts for Chinese ambiguity and polysemy, leading to targeted and discriminative evaluations.\n\n**3**  Thorough empirical study: Experiments cover eight mainstream Chinese/general LLMs, include SFT and OOD setups, and demonstrate consistent gains in effectiveness and robustness."}, "weaknesses": {"value": "**1**  Evaluation Fairness and Prompt Design: The benchmark employs eight LLMs, but their instruction formats, decoding strategies, and prompt templates are not clearly standardized. Without unified prompt calibration, the performance comparison might reflect prompt sensitivity rather than intrinsic model ability. Clarifying or releasing prompt templates would improve reproducibility.\n\n**2**  Missing Error Analysis and Case Studies – The paper reports large tables but lacks discussion of common failure patterns or qualitative examples illustrating where Chinese-specific ambiguities challenge models.\n\n**3**  Minor Writing and Presentation Issues: Some figures and tables (e.g., Table 4 and 5) are dense and difficult to read; summarizing relative gains or including visualization of error types could improve readability."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "37EoNH9sjz", "forum": "2GhjkigNTx", "replyto": "2GhjkigNTx", "signatures": ["ICLR.cc/2026/Conference/Submission10428/Reviewer_183W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10428/Reviewer_183W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723018547, "cdate": 1761723018547, "tmdate": 1762921734008, "mdate": 1762921734008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a Chinese dataset comprising text pairs and triples across four domains. It supports several tasks, including question answering, triple-to-text, and knowledge graph completion, which are useful for supervised fine-tuning of LLMs with structured knowledge data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset offers a large amount of structured knowledge, 7M text pairs, and 15M triples; which would be useful for many down stream tasks in NLP and others.\n- It highlights the need for structural knowledge alignment, potentially linguistic richness and other modalities. It is interesting to see how this translates into higher performance in KGC rather than QA.\n- The paper comes with a comprehensive evaluation using GLM, Yi, QiWen, ...."}, "weaknesses": {"value": "- No comparison with other Chinese datasets and benchmarks. It is also good to provide a decision tree, indicating when, why and how this dataset can be used.\n- Limited domains (History, Politics, Humanities, Society) and task coverages (KGC, QA, T2T) - These domains seem to be restrictive, especially for English-centric LLMs\n- Limited information on human validations"}, "questions": {"value": "- How is CDTP compared with other benchmarking datasets?\n- How can the paper address errors and biases in this dataset? Especially, the four domains (history/politics/humanities/society) are relatively restrictive in Chinese; thus, evaluation results might not be generalisable.\n- The evaluated models are relatively small (<30B). Given a large dataset, would this be a limiting factor? It would be interesting to see how larger models deal with the dataset. Also, how about GPT? Mistral? Claude? as they may perform well on multilingual tasks.\n- Four domains are insufficient - will there be a concrete plan to extend to more domains? Also, can the authors analyse the impacts of linguistic and structural features of the dataset/LLMs\n- Can the dataset be rigorously validated by humans?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bKzffBxn3J", "forum": "2GhjkigNTx", "replyto": "2GhjkigNTx", "signatures": ["ICLR.cc/2026/Conference/Submission10428/Reviewer_KL2W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10428/Reviewer_KL2W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998776197, "cdate": 1761998776197, "tmdate": 1762921733698, "mdate": 1762921733698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}