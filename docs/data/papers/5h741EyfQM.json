{"id": "5h741EyfQM", "number": 11467, "cdate": 1758199829796, "mdate": 1759897573715, "content": {"title": "A Brain-Inspired Gating Mechanism Unlocks Robust Computation in Spiking Neural Networks", "abstract": "While spiking neural networks (SNNs) provide a biologically inspired and energy-efficient computational framework, their robustness and the dynamic advantages inherent to biological neurons remain significantly underutilized owing to oversimplified neuron models. In particular, conventional leaky integrate-and-fire (LIF) neurons often omit the dynamic conductance mechanisms inherent in biological neurons, thereby limiting their capacity to cope with noise and temporal variability. In this work, we revisit dynamic conductance from a functional perspective and uncover its intrinsic role as a bio-inspired gating mechanism that modulates information flow. Building on this insight, we introduce the Dynamic Gated Neuron~(DGN), a novel spiking unit in which membrane conductance evolves in response to neuronal activity, enabling selective input filtering and adaptive noise suppression. We provide a theoretical analysis showing that DGN possess enhanced stochastic stability compared to standard LIF models, with dynamic conductance intriguingly acting as a disturbance rejection mechanism. DGN-based SNNs demonstrate superior performance across extensive evaluations on anti-noise tasks and temporal-related benchmarks such as TIDIGITS and SHD, consistently exhibiting excellent robustness. To the best of our knowledge, for the first time, our results establish bio-inspired dynamic gating as a key mechanism for robust spike-based computation, providing not only theoretical guarantees but also strong empirical validations. This work thus paves the way for more resilient, efficient, and biologically inspired spiking neural networks.", "tldr": "We propose the Dynamic Gated Neuron, a spiking model with adaptive conductance that serves as a biologically inspired gating mechanism to enhance noise robustness. Our work achieves superior performance across noisy tasks and speech recognition.", "keywords": ["Spiking Neural Networks (SNNs)", "Dynamic Gated Neurons", "Noise Robustness", "Brain-Inspired Computing"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56ee538ccfaac0244bf875adfb6f807fe1068239.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Dynamic Gated Neuron (DGN), a novel spiking neuron model that incorporates dynamic membrane conductance as a biologically-motivated gating mechanism. DGN modulates membrane decay via activity-dependent conductance, enabling selective information retention and improved robustness to noise. The authors connect DGN behavior to gating in LSTMs, analyze stochastic stability, and present empirical results on many temporal datasets, showing improved accuracy and robustness under noise and adversarial attacks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The work is significant, as it draws clear inspiration from neuroscience, building on established literature on activity-dependent ion-channel plasticity, to introduce an input-dependent gating mechanism into spiking neuron models. Different from LSTM in conventional deep learning, the proposed DGN model is biologically grounded and novel. It may have a significant multi-disciplinary impact.\n\n2. This paper is well-written and easy to follow. It presents a well-structured bridge among conductance-based neurons in computational neurosience, LSTM in deep learning, and LIF neurons in SNNs, and clearly articulates the differences and connections among DGN, LSTM, and LIF, offering valuable multi-disciplinary insight.\n\n3. The work is supported by rigorous theoretical analysis demonstrating how dynamic conductance mechanisms enable adaptive leakage and noise suppression. \n\n4. Experiments are extensive in this work, with strong results supporting the claims and demonstrating the strong temporal processing capability and robustness of DGN. \n\n5. The proposed model is also advantageous in parameter efficiency, which is particularly valuable for SNNs operating in edge applications."}, "weaknesses": {"value": "1. The authors seem not to discuss the feasibility of deploying the DGN model on neuromorphic chips. Since the computational advantages of SNNs are primarily achieved on such hardware, it would be valuable to discuss this aspect.\n\n2. The DGN model includes a numerical truncation function. However, it is unclear which specific type of truncation function is used in the experiments. What influence does the choice of truncation function have on performance? Are these functions hardware-friendly?"}, "questions": {"value": "It is not very clear how the DGN model is used to build both feedforward and recurrent networks. From Table 1, recurrent DGNs appear to outperform their feedforward counterparts. Could the authors clarify how the recurrent version is constructed and trained? Additionally, is the recurrent DGN more stable or easier to train compared to the feedforward architecture, and if so, why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dc43rIgmeB", "forum": "5h741EyfQM", "replyto": "5h741EyfQM", "signatures": ["ICLR.cc/2026/Conference/Submission11467/Reviewer_1AVk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11467/Reviewer_1AVk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836300597, "cdate": 1761836300597, "tmdate": 1762922573910, "mdate": 1762922573910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Dynamic Gated Neuron (DGN), a novel spiking neuron model that incorporates dynamic conductance mechanisms inspired by biological neurons. The authors propose that this mechanism acts as an intrinsic gating function, similar to forget gates in LSTMs, enabling adaptive control of information flow and memory retention. The model is rigorously evaluated on multiple speech and neuromorphic benchmarks, demonstrating state-of-the-art performance and superior robustness against noise and adversarial attacks. Theoretical analysis using stochastic differential equations further supports the noise resilience of DGN. The work is well-motivated, methodologically sound, and empirically thorough."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using dynamic conductance as a gating mechanism in spiking neurons is novel and well-grounded in neurophysiology. The connection to LSTM-like gating is insightful and bridges bio-inspired models with artificial neural networks.\n\n2. The paper provides a solid theoretical analysis of the model’s stability under noise, using stochastic differential equations and variance comparisons with LIF neurons.\n\n3. Extensive experiments on multiple datasets (TIDIGITS, SHD, SSC, etc.) across both feedforward and recurrent architectures demonstrate the model’s effectiveness in accuracy and robustness.\n\n4. The paper includes a wide range of noise and adversarial attack scenarios, showing consistent superiority over existing SNN models and even LSTMs in some cases.\n\n5. The appendix includes detailed derivations, training procedures, hyperparameters, and noise generation algorithms, which facilitate reproducibility."}, "weaknesses": {"value": "1. Although the model is compared with several recent SNN variants, it would be beneficial to include more baselines  (e.g., spiking-based Transformers).\n\n2. The DGN model introduces additional parameters and computational overhead compared to LIF. A more detailed analysis of energy efficiency or inference speed on neuromorphic hardware would strengthen the practical contribution.\n\n3. Lack of pseudocode for model computation.\n\n4. Why does DGN perform relatively poorly under the recurrent architecture on the SHD dataset?"}, "questions": {"value": "1. Could the dynamic conductance mechanism be integrated with other advanced SNN architectures (e.g., attention-based or transformer-like SNNs)?\n\n2. Have you considered evaluating the model on neuromorphic hardware to assess its real-world efficiency and latency?\n\n3. The paper mentions a \"simplified DGN\" (s-DGN) in the appendix. Could this be discussed more in the main text to highlight trade-offs between performance and complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DdgGMgaWAk", "forum": "5h741EyfQM", "replyto": "5h741EyfQM", "signatures": ["ICLR.cc/2026/Conference/Submission11467/Reviewer_UpB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11467/Reviewer_UpB6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990107168, "cdate": 1761990107168, "tmdate": 1762922573562, "mdate": 1762922573562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides the Dynamic Gated Neuron (DGN), a novel spiking unit in which membrane conductance evolves in response to neuronal activity. DGN appears enhanced stochastic stability compared to standard LIF models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes an improved model. Intuitively, this model introduces more computation but does not increase the number of parameters, and experiments show improved performance. These findings suggest that the research presented in this paper represents a worthwhile improvement."}, "weaknesses": {"value": "1. This paper is hard to follow.\n\n2. Theoretical analysis is limited. There is a great deal of analysis work on SNNs that combats randomness and noise.\n\n3. This paper proposes only one improved model. To my understanding, this model does not appear to have a special training mechanism; it simply uses the traditional global training method for SNNs. Based on past experience, this approach makes it difficult to achieve new functionalities. The authors' claimed intrinsic adjustment mechanism is merely a claim and cannot be verified experimentally or proven theoretically.\n\n4. The experiments in this paper are insufficient; comparative experiments with the same architecture or parameter set are needed. Furthermore, I don't understand why it's necessary to compare it with artificial neural network models like LSTM that take real-number sequences as input."}, "questions": {"value": "1. Does the improved model have a special or proprietary training mechanism that can find a solution with specific functions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5xTJSOni4w", "forum": "5h741EyfQM", "replyto": "5h741EyfQM", "signatures": ["ICLR.cc/2026/Conference/Submission11467/Reviewer_mPha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11467/Reviewer_mPha"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225932370, "cdate": 1762225932370, "tmdate": 1762922573192, "mdate": 1762922573192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Dynamic Gated Neuron (DGN) model that integrates a bio-inspired dynamic conductance mechanism to enhance the robustness and performance of spiking neural networks (SNNs), outperforming traditional LIF models and other baselines in anti-noise tasks and temporal benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  This paper introduces a biologically grounded Dynamic Gated Neuron (DGN) model with a dynamic conductance mechanism, which enables adaptive input filtering and noise suppression\n\n2. The DGN-based spiking neural networks (SNNs) demonstrate superior performance and robustness across multiple benchmarks (e.g., TIDIGITS, SHD) under various noise types and adversarial attacks.\n\n3. The comparison in the figures is very intuitive, allowing the innovative points of the authors' architecture to be clearly identified."}, "weaknesses": {"value": "1.  The authors claim that the DGN is a generalized spiking neuron model, yet there is no evaluation on relevant computer vision datasets (NMNIST, CIFAR, N-CALTECH).\n\n2.  This paper lacks an evaluation of energy consumption, and it remains unaddressed whether the newly introduced unit (DGN) will significantly increase power consumption, as reflected in Equations 5-8."}, "questions": {"value": "The low power consumption of SNNs lies in simplifying multiplication to addition; however, in my view, D (as denoted in the DGN model) behaves like a floating-point number, which causes SNNs to lose this characteristic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D88k7FqumO", "forum": "5h741EyfQM", "replyto": "5h741EyfQM", "signatures": ["ICLR.cc/2026/Conference/Submission11467/Reviewer_wod3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11467/Reviewer_wod3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762247437361, "cdate": 1762247437361, "tmdate": 1762922572759, "mdate": 1762922572759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}