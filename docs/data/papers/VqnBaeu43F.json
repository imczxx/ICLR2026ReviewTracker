{"id": "VqnBaeu43F", "number": 24961, "cdate": 1758362501397, "mdate": 1759896740176, "content": {"title": "From Parameters to Behaviors: Unsupervised Compression of the Policy Space", "abstract": "Despite its recent successes, Deep Reinforcement Learning (DRL) is notoriously sample-inefficient. We argue that this inefficiency stems from the standard practice of optimizing policies directly in the high-dimensional and highly redundant parameter space $\\\\Theta$. This challenge is greatly compounded in multi-task settings. In this work, we develop a novel, unsupervised approach that compresses the policy parameter space $\\\\Theta$ into a low-dimensional latent space $\\\\mathcal Z$. We train a generative model $g:\\\\mathcal Z\\\\to\\\\Theta$ by optimizing a behavioral reconstruction loss, which ensures that the latent space is organized by functional similarity rather than proximity in parameterization. We conjecture that the inherent dimensionality of this manifold is a function of the environment's complexity, rather than the size of the policy network. We validate our approach in continuous control domains, showing that the parameterization of standard policy networks can be compressed up to five orders of magnitude while retaining most of its expressivity. As a byproduct, we show that the learned manifold enables task-specific adaptation via Policy Gradient operating in the latent space $\\\\mathcal{Z}$.", "tldr": "", "keywords": ["reinforcement learning", "unsupervised reinforcement learning", "unsupervised representation learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3870585c897ac82ab25224802a2d18e5449c09f4.pdf", "supplementary_material": "/attachment/12481342ecb228e3021fba6a496cff2e5b535643.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles the issue of sample inefficiency in deep reinforcement learning caused by the high-dimensional parametrization of policies. The authors propose a two-stage approach: first, they compress the policy parameter space into a low-dimensional behavioral latent space using autoencoders; then, they leverage this learned latent representation to efficiently adapt and learn new tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear motivation, grounded in the Manifold Hypothesis, for learning and operating within a low-dimensional behavioral manifold.\n2. The work addresses the sample inefficiency of DRL. At the pre-training stage, it uses unsupervised RL, which does not require one to solve actual RL tasks\n3. The experiments show that the policy networks can be compressed up to five orders of magnitude\n4. The experiments show that fine-tuning the latent space allows faster convergence and low sample complexity"}, "weaknesses": {"value": "My main concern is that although the work is well motivated and supported by good intuition, the empirical evidence provided in the experiments is not sufficiently strong to support some of the authors’ claims. In particular:\n\n1. Complexity of the latent space: The authors evaluated latent space complexity through visualizations of the return of the encoded policies. However, the interpretation of these figures, especially for the 3D latent space, is somewhat ad hoc and lacks quantitative rigor. It would strengthen the paper to include concrete metrics that quantify latent space complexity rather than relying solely on visual inspection.\n\n2. Performance recovery metric: To assess performance recovery, the authors compare the upper and lower performance bounds of policies found in the latent space versus those from the original datasets. In my view, this comparison is insufficient to convincingly demonstrate recovery, as it only reflects the spread of the distribution and neglects the distribution shape or central tendency. A more informative metric (such as divergence measures) could provide a clearer picture of how well the latent policies capture the original performance characteristics.\n\n3. Convergence behavior: While Figures 4 and 5 show that Latent PGPE converges faster than other baselines, it does not consistently reach the optimal policy—approximately half of the time (e.g., Figures 4a, 4c, 4d, 5b, and 5c), convergence is suboptimal. In 5b, Parameter PGPE is significantly better than latent PGPE. This suggests that the chosen latent space may not be sufficiently expressive to fully recover the original policy space. This observation also reinforces my earlier point that spread-based metrics may fail to adequately capture true performance recovery."}, "questions": {"value": "1. Could you elaborate on the advantages and disadvantages of reducing cardinality versus dimensionality?\n\n2. Could you provide more details on how the policy network is updated without computing the Jacobian of the decoder? In Appendix B, what does $\\phi$ represent in the expression $z \\sim v_\\phi$?\n\n3. Could you offer some intuition regarding the trade-off among the size of the original policy network, the dimension of the latent space, and the resulting policy performance? For instance, does a larger policy network generally lead to better performance, but also require a larger encoded latent space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5k7lj0X65I", "forum": "VqnBaeu43F", "replyto": "VqnBaeu43F", "signatures": ["ICLR.cc/2026/Conference/Submission24961/Reviewer_uJZc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24961/Reviewer_uJZc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678622275, "cdate": 1761678622275, "tmdate": 1762943263076, "mdate": 1762943263076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method of optimizing a policy not directly in parameter space, but in a lower-dimensional learned latent space. This space is learned by an autoencoder trained on a dataset of policy parameters. The training set for the autoencoder is generated using policies discovered in an 'unsupervised' (i.e., reward-free) setting via novelty search, with the L2 distance between actions on some subset of the state space serving as the distance measure. Given a reward function, a concrete policy can then be optimized directly in the learned latent space using policy gradient algorithms. The paper shows that in mountain-car-continuous and reacher environments, this latent optimization often learns faster than methods like DDPG, PPO, or SAC, which learn in parameter space."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed framework has clear intuitive benefits and is well-motivated by the manifold hypothesis.\n- The experiments are clearly described and thoroughly documented.\n- In the tested environments, some advantages of the method can be seen (mainly faster convergence of the policy's performance, although not always)."}, "weaknesses": {"value": "- The method is empirically tested in only two very simple continuous control settings: Mountain Car Continuous and Reacher. It is difficult to judge the promise of the method without evaluation in much more challenging and open environments.\n- The method's core contribution (latent optimization) is significantly undermined if, in the best-case scenario, it simply recovers high-performing policies that were _already present_ in the training dataset generated by novelty search. The main benefit of the learned latent representation would be that it _generalizes_, i.e., that it allows for the efficient discovery of policies that are substantially different (in capability or performance) from those in the training set. It is not clearly shown in the paper that this happens in practice. While the paper mentions (lines 375-394) that some configurations recover performances _higher_ than in the training set, it is unclear if this result is statistically significant or within the bounds of evaluation noise. This point is critical to evaluating the method's true contribution.\n- The paper does not mention the significant challenges and problems arising when learning latent representations of neural network parameters (e.g., scalability to large networks, or weight space symmetries). The literature on weight space learning is ignored by the paper, even though it addresses exactly this topic and might lead to significant improvements. See for example \"Hyper-Representations as Generative Models: Sampling Unseen Neural Network Weights\" (Schürholt et al., 2022), \"Goal-conditioned generators of deep policies\" (Faccio et al., 2023), or \"Learning useful representations of recurrent neural network weight matrices\" (Herrmann et al., 2024). Going beyond the toy domains used in the paper would likely require insights from these works.\n\nWhile I find the proposed method interesting, I vote for a weak reject. I believe that a more thorough evaluation is necessary, specifically one that demonstrates statistically significant generalization beyond the training set policies and tests the method in more challenging environments."}, "questions": {"value": "- What are the best-performing policies for the various tasks in the training datasets? A direct comparison (with confidence intervals) between the policies found by Latent PGPE and the best policies already in the training set is needed to assess the true value of the latent optimization procedure.\n- Lines 359-360: \"We speculate that this is due to the increased range of behaviors expressed by larger policies\". Can the authors provide evidence (e.g., a quantitative measure of behavioral diversity) for an increased range of behaviors for larger policies in the training datasets?\n- Lines 361-362: \"...certain behavioral areas at high performance are able to grow larger, creating more nuanced decoded policies\". Do larger areas in the latent space really correspond to more _behaviorally_ nuanced policies, as opposed to e.g., simply multiple different parameters encoding the same behavior (a known redundancy)? Can this be tested?\n- Line 464: \"bitter lesson\" is obviously a loaded term in the context of RL. However, I can't see how the obvious dependency of the performance on the quality of the learned latent space relates to Sutton's essay, or why it is a \"bitter lesson\" in general. This seems like a straightforward and expected trade-off.\n- How would the subset of the state space (used to measure policy diversity for the training set) be chosen if the state space is too large or not well-defined enough for uniform sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XzAnAzS0ql", "forum": "VqnBaeu43F", "replyto": "VqnBaeu43F", "signatures": ["ICLR.cc/2026/Conference/Submission24961/Reviewer_Rt5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24961/Reviewer_Rt5R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926000262, "cdate": 1761926000262, "tmdate": 1762943262846, "mdate": 1762943262846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised method to address the sample inefficiency of deep reinforcement learning. The approach learns a compact latent representation of the policy parameter space by optimizing *behavioral* similarity, allowing policies to be optimized in a low-dimensional space. Experiments on classic continuous control benchmarks demonstrate that this representation not only accelerates convergence and allows significant policy compression but also produces semantically meaningful latent representations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* This paper studies a core problem in deep reinforcement learning: instead of optimizing the expected return over the entire space of states and actions, can we focus on visiting sttes and actions that matter for the task in hand?\n\n* The presentation and writing quality of the paper are strong. The problem is well motivated, and the research questions are explicitly stated, making it easy to understand the focus of the study. I particularly enjoyed the background and problem formulation sections—they are cohesive, detailed, and accessible, making the paper enjoyable and understandable even for readers who may not be very familiar with the field, like myself.\n\n* Experiments are also descent quality and help answer the main questions authors presented early in the paper."}, "weaknesses": {"value": "* The quality of Figure 1 is quite low; the grey text is difficult to read, and zooming in causes significant pixelation, which detracts from the overall presentation of the paper.\n\n* Figures 4 and 5 should be larger, as the curves are currently hard to distinguish. Additionally, the color choices make it difficult to differentiate the plots—for example, in Figure 4, DDPG, PPO, SAC, and TD3 all use similar green/blue colors with solid lines. Using more distinct colors and line styles would greatly improve readability."}, "questions": {"value": "* In line 047, the term *behavior* is introduced as \"a latent representation of the possible behaviors.\" Since its precise meaning is not clear at this point and is formally defined later in line 099, it would be helpful to emphasize it with italics to signal its special usage.\n\n* In the experiments, it would be useful to specify the architectures used for the policies. I am particularly interested in how the depth and width vary across different policy sizes (small, medium, large), as prior work has shown that network depth can significantly affect the expressivity of deep RL policies.\n\n* I could not identify the algorithm used for training the policies. If it is not described in the main text, this information should be added for completeness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zR1dlsx7bf", "forum": "VqnBaeu43F", "replyto": "VqnBaeu43F", "signatures": ["ICLR.cc/2026/Conference/Submission24961/Reviewer_RZJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24961/Reviewer_RZJt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973474983, "cdate": 1761973474983, "tmdate": 1762943262558, "mdate": 1762943262558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}