{"id": "UPHunx3Tah", "number": 8462, "cdate": 1758084903689, "mdate": 1759897782531, "content": {"title": "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models", "abstract": "AutoRegressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often seen as less competitive than diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training and inference efficiency as well as image resolution. To address this, we present Token-Shuffle, a novel yet simple method that reduces the visual token number in causal masked Transformer architectures. Our motivation is the dimensional redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs). Leveraging this, our method employs two key operations: token-shuffle, which merges spatially local tokens along channel dimension to decrease the token number, and token-unshuffle, which untangles the inferred tokens after Transformer blocks to restore the spatial arrangement for output. Jointly training with textual prompts, our strategy requires no additional pretrained text-encoder, and enables MLLMs to support extremely high-resolution image synthesis (over 1k resolution) in a unified next-token prediction way while maintaining efficient training and inference. For the first time, we push the boundary of AR text-to-image generation to resolution of 2048 × 2048, and set new baselins for AR-model image generation. In GenAI-benchmark, our 2.7B model achieves 0.77 overall score on hard prompts, outperforming the previous autore-gressive models LlamaGen by 0.18 and diffusion models LDM by 0.15. Based on large-scale human evaluation, we demonstrate that a pure AR-model can provide comparative or even better image generation quality as Diffusion-model and generate high-resolution images simultaneously.", "tldr": "", "keywords": ["Token-Shuffle", "Auto-Regressive model", "Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/789a321f9892330fcd4b422a045d7845b3ff3266.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work focuses on reduce number of image tokens in training Multimodal Large Language Models (MLLMs). To this end, the authors propose token-shuffle and token-unshuffle which merges and untangles spatially local image tokens before and after main boy Transformer  blocks, respectively. Also, some investigations are introduced for improved CFG on AR models. Empirical study on text-to-image generation benchmarks shows the proposed method achieves comparable performance to baselines while being computational efficient."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. \n2. The proposed method is easy to implement and reduces compute for images tokens.\n3. empirical study shows comparable quantitative performance on t2i generation."}, "weaknesses": {"value": "1. I found it a bit confusing about the naming: token-shuffle/unshuffle. The operation doesn't change the order the tokens but rather aggregating the locally close tokens. \n2. As shown in Figure 9, proposed method still leads to significant degradation as the larger shuffle window is, the higher training loss is."}, "questions": {"value": "1. Do the authors have the model that is trained without token-shuffle/unshuffle and how does it compare to Token-shuffle on benchmarks on GenAI-Bench?\n2. It would also help illustrate the advantage of proposed method to include the FLOPs of token-shuffle and baselines in Table 1.\n3. To me, the token-shuffle/unshuffle is a lot like the patchify/unpatchify operations in DiT. Can the authors elaborate more about the connection and difference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vjpwg73M3m", "forum": "UPHunx3Tah", "replyto": "UPHunx3Tah", "signatures": ["ICLR.cc/2026/Conference/Submission8462/Reviewer_udWS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8462/Reviewer_udWS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761001583420, "cdate": 1761001583420, "tmdate": 1762920344444, "mdate": 1762920344444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors identify \"dimensional redundancy\" in the visual vocabularies of Multimodal Large Language Models (MLLMs) as a key inefficiency. They propose \"Token-Shuffle,\" a simple and effective plug-and-play method that leverages this redundancy. The method comprises a \"token-shuffle\" operation, which merges a group of spatially local tokens into a single fused token to reduce the sequence length by a factor of $s^2$, and a corresponding \"token-unshuffle\" operation to restore the spatial arrangement after the Transformer computation. This allows AR model to efficiently predict fused tokens, enabling text-to-image generation at 2K resolution. The method achieves state-of-the-art results for AR models, outperforming LlamaGen and even diffusion models like LDM on key benchmarks ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method achieves, for the first time, $2048 \\times 2048$ image generation with an AR model, a significant and practical leap in resolution for this model class. The paper provides a strong quantitative and qualitative evaluation. The model significantly outperforms existing AR models like LlamaGen on the GenAI-Bench and even surpasses strong diffusion models (LDM) in human evaluations for text-alignment and visual appearance.\n2. The core insight of \"dimensional redundancy\", which is introduced and explored in early works likes DC-AE (Chen et al., 2024), is validated in the auto-regressive image generation task to accelerate the generation process. The paper compellingly shows that MLLM embedding dimensions are unnecessarily large for visual tokens by demonstrating that compressing the visual embedding rank by a factor of 8 has minimal impact on loss.\n3. The introduction of the \"half-linear\" CFG-scheduler is a valuable secondary contribution that successfully addresses a specific artifact-generation problem inherent to AR models ."}, "weaknesses": {"value": "1. The model requires a complex, three-stage pre-training process, scaling from $512 \\times 512$ to $1024 \\times 1024$ and finally $2048 \\times 2048$. This is computationally intensive and requires specific stabilization techniques (like z-loss) to function at high resolutions. Moreover, the large training cost seems conflict to the 'plug-and-play' statement in the paper.\n2. The central mechanism, \"Token-Shuffle,\" is a direct adaptation of the well-known \"pixel shuffle\" (space-to-depth) operation, which the paper itself cites as inspiration (Shi et al., 2016). This technique is used in super-resolution (ESPCN, Shi et al., 2016) and VAE design (DC-AE, Chen et al., 2024). While the application to AR image generation is new and yields strong results, the core technical contribution of the paper is limited to applying an existing method to a new domain."}, "questions": {"value": "1. Given that the core shuffle mechanism is functionally identical to the \"pixel shuffle\" used in previous super-resolution and VAEs works, could the authors clarify what they view as the paper's primary technical novelty, beyond the application of this method to AR generation?\n2. Please clarify the \"plug-and-play\" claim. Can the shuffle operators be removed from the $s=2$ model at inference and what will the performance of model be? If not, what are the quantitative performance scores (VQAScore, Human Eval on GenAI-Bench) for the $s=1$ (no shuffle) baseline model? Figure 9 suggests it has the lowest loss, so this comparison is essential for evaluating the trade-off, especially for $512 \\times 512$ generation, since users might want to use $s=1$ for a better low-resolution generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1fbBwH3Qcm", "forum": "UPHunx3Tah", "replyto": "UPHunx3Tah", "signatures": ["ICLR.cc/2026/Conference/Submission8462/Reviewer_1y8i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8462/Reviewer_1y8i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994544814, "cdate": 1761994544814, "tmdate": 1762920343944, "mdate": 1762920343944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Token-Shuffle, a method for efficient high-resolution image generation using autoregressive (AR) models. The key innovation addresses the computational burden of processing large numbers of visual tokens by leveraging dimensional redundancy in visual vocabularies. Token-Shuffle merges spatially local tokens along the channel dimension before feeding them into Transformers, then unshuffles them for output. This reduces token count by up to 75% (with shuffle window size 2), enabling the first AR text-to-image generation at 2048×2048 resolution. The 2.7B parameter model achieves competitive results against diffusion models on GenAI-bench, demonstrating that AR models can efficiently generate high-quality, high-resolution images while maintaining the standard next-token prediction paradigm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is simple but effective. It is an elegant plug-and-play solution that reduces visual token count without modifying the core Transformer architecture or causal masking, making it easily applicable to existing MLLMs.\n\n2. Strong empirical results. The 2.7b model outperforms larger AR models (like LlamaGen) and competitive diffusion models.\n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The core technical contribution, token shuffle, is incremental and lacks deep novelty. Spatial-to-channel transformation is a well-established practice in different domains, for example Diffusion Transformer and heirarchical vision transformers (e.g. Swin Transformer).\n\n2. Unlike some recent MLLMs (e.g., EMU3), the paper doesn't demonstrate support for flexible aspect ratios or arbitrary resolutions, limiting practical applicability."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H7afruYPKv", "forum": "UPHunx3Tah", "replyto": "UPHunx3Tah", "signatures": ["ICLR.cc/2026/Conference/Submission8462/Reviewer_LdD5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8462/Reviewer_LdD5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096662070, "cdate": 1762096662070, "tmdate": 1762920343568, "mdate": 1762920343568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}