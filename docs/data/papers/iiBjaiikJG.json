{"id": "iiBjaiikJG", "number": 12911, "cdate": 1758211551700, "mdate": 1763727307761, "content": {"title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization", "abstract": "Despite advances in pretraining with extended context sizes, large language models (LLMs) still face challenges in effectively utilizing real-world long-context information, primarily due to insufficient long-context alignment caused by data quality issues, training inefficiencies, and the lack of well-designed optimization objectives. To address these limitations, we propose a framework named **S**h**o**rt-to-**Lo**ng **P**reference **O**ptimization (**SoLoPO**), decoupling long-context preference optimization (PO) into two components: short-context PO and short-to-long reward alignment (SoLo-RA), supported by both theoretical and empirical evidence. Specifically, short-context PO leverages preference pairs sampled from short contexts to enhance the model's contextual knowledge utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score consistency for the responses when conditioned on both short and long contexts that contain identical task-relevant information. This facilitates transferring the model's ability to handle short contexts into long-context scenarios. SoLoPO is compatible with mainstream preference optimization algorithms, while substantially improving the efficiency of data construction and training processes. Experimental results show that SoLoPO enhances all these algorithms with respect to stronger length and domain generalization abilities across various long-context benchmarks, while achieving notable improvements in both computational and memory efficiency.", "tldr": "A well-designed long-context preference optimization (PO) framework that improves both the effectiveness of original PO algorithms and the efficiency of data construction and training procedure.", "keywords": ["Long Context Alignment", "Large Language Models", "Preference Optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d79c3d2bdcd122fa31e7750e9acb7654eb7a7fa1.pdf", "supplementary_material": "/attachment/b121ec61159716b1d40f6ad4eacc3cdf15f6211d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SoLoPO (Short-to-Long Preference Optimization), a framework designed to improve the long-context capabilities of large language models (LLMs) by decoupling long-context preference optimization into two components:\n1. Short-context PO: Enhances reasoning on compressed, task-relevant contexts.\n2. Short-to-Long Reward Alignment (SoLo-RA): Encourages consistency in reward scores between short and long contexts containing the same task-relevant information.\n\nThe authors provide theoretical justification for this decoupling and empirically validate SoLoPO across multiple preference optimization algorithms (DPO, SimPO, ORPO) and models (Qwen2.5, Llama3.1). Key results show that SoLoPO improves performance on long-context benchmarks (e.g., LongBench, RULER, NIAH-Plus) while maintaining short-context capabilities and significantly boosting training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A theoretical decomposition of long-context PO into short-context PO and SoLo-RA.\n2. The idea of decoupling long-context alignment into short-context reasoning and cross-context reward alignment is novel and well-motivated.\n3. Offers a practical and scalable solution with clear efficiency gains (e.g., 2.1× longer trainable sequences, 52% runtime reduction)."}, "weaknesses": {"value": "1. The theory relies on the redundancy hypothesis and Assumption 1, which, while empirically supported, may not hold for all long-context tasks (e.g., when all context is relevant).\n2. The synthetic dataset construction (mixing relevant and irrelevant documents) is simple but may not reflect real-world long-context complexity."}, "questions": {"value": "1. How does SoLoPO perform on non-QA long-context tasks such as summarization, multi-turn dialogue, or document-level translation?\n2. Can the framework be extended to generation-heavy tasks where both input and output are long? The current focus is on long-input scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QgicArLrwJ", "forum": "iiBjaiikJG", "replyto": "iiBjaiikJG", "signatures": ["ICLR.cc/2026/Conference/Submission12911/Reviewer_QMaK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12911/Reviewer_QMaK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916963891, "cdate": 1761916963891, "tmdate": 1762923685346, "mdate": 1762923685346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of long-context alignment and aims to improve context utilization in large language models. The authors propose a theoretically grounded framework that leverages short-context preference optimization to enhance performance in long-context settings. The key idea is to decouple long-context preference optimization into two components: short-context preference optimization and short-to-long reward alignment. This framework, termed SoLoPO, can be applied to existing preference optimization methods such as DPO, SimPO, and ORPO. Experimental results across multiple benchmarks demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. **Novel framework.** Proposes the SoLoPO framework to transfer short-context preference optimization capabilities to long-context alignment.  \n2. **Theoretical foundation.** The method is supported by solid theoretical results that justify the proposed decoupling. \n3. **General applicability.** The framework can be integrated with multiple preference alignment methods, showing consistent improvements across them.  \n4. **Strong long-context performance.** The chosen-only SoLoPO variant consistently outperforms standard PO baselines on long-context benchmarks."}, "weaknesses": {"value": "1. **Degraded short-context performance.** The method shows reduced performance on the short-context Open LLM Leaderboard. Lines 103 and 377 claim that SoLoPO maintains short-context performance, yet Table 4 indicates otherwise. SoLoPO underperforms the PO baseline in 16 out of 24 datasets.  \n2. **Limited intuition for theoretical results.** Although the theory is sound, the paper should do a better job providing intuition on *why* the decoupling leads to improved long-context performance (see Question 2).  \n3. **Lack of detail in key arguments.** Some claims and statements would be clearer with additional justification or elaboration (see Question 3)."}, "questions": {"value": "1. **Missing baseline.** Why does Table 4 not report the performance of the LongPO baseline?  \n2. **Clarification on Figure 4.** What exactly do “efficiency” and “runtime” refer to? It is unclear how SoLoPO improves computational efficiency, as it requires handling both $ x_{\\text{short}} $ and $ x_{\\text{long}} $, which requires additional time for dataset creation as well as an additional forward pass for every iteration. Could you clarify where the runtime or efficiency advantage arises?  \n3. **Clarification on Theorem 1 and the function $ s(\\cdot) $.**  \n   1. Theorem 1 introduces $ s(|x|) $. It would be helpful to provide intuition for the role of $ s(\\cdot) $ and its relationship with $ f $, beyond the theoretical requirement for this assumption.  \n   2. Since $ s(\\cdot) $ must satisfy the inequality, multiple valid functions may exist. Would a tighter $ s $ lead to a stronger upper bound, and how would this affect empirical behavior?  \n   3. It would improve readability to include some intuition, after Theorem 1, on how $ s(\\cdot) $ can be derived for a given alignment method. The appendix (I3, I4) provides examples, but referencing them directly in the main text would help readers.  \n4. **On the equivalence with standard PO.** Line 201 states that for $ \\rho = 100\\% $, SoLoPO is equivalent to the original PO. In this case, we have  \n   $$\n   L_{\\text{PO}}(x_{\\text{long}}) \\le \\tfrac{1}{3} L_{\\text{SoLoPO}}(x_{\\text{short}}),\n   $$\n   which is an upper bound. Is there theoretical evidence that this bound becomes an equality? If not, have you empirically tested whether optimizing PO or its SoLoPO upper bound leads to equivalent results when $ x_{\\text{short}} = x_{\\text{long}} $?  \n\n**Minor comments that do not affect rating**\n1. In Table 4 (Open LLM Leaderboard), best and second-best values are not bolded or underlined, unlike other tables. Any reason for this inconsistency?  \n2. While Assumption 1 appears reasonable, it would strengthen the work to provide empirical evidence that it holds in practice.  \n4. What is the base model used in the LongPO reimplementation?  \n5. In Figure 1a, the second box should read “short-context preference optimization.”  \n6. Line 159 defines $ x_{\\text{short}} $ as concatenating $ c_{\\text{rel}} $, but the equation shows $ c_{\\text{irr}} $. This appears to be a typo.  \n7. Section 4.1 would be stronger if it explicitly stated how many scenarios SoLoPO outperforms the corresponding PO baseline, further reinforcing its general applicability.  \n8. In Table 4, the result for $ M_{\\text{short}}^{\\text{SimPO}} $ in the Math column is shown in red, even though it improves over the Instruct model. Please verify the formatting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LjhQesmkYF", "forum": "iiBjaiikJG", "replyto": "iiBjaiikJG", "signatures": ["ICLR.cc/2026/Conference/Submission12911/Reviewer_mA21"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12911/Reviewer_mA21"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947857847, "cdate": 1761947857847, "tmdate": 1762923685122, "mdate": 1762923685122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SoLoPO (Short-to-Long Preference Optimization), a general framework for efficiently enhancing long-context reasoning in large language models. The key idea is to decouple long-context preference optimization (PO) into two stages: (1) short-context PO, which focuses on optimizing reasoning and alignment on short, information-dense contexts; and (2) short-to-long reward alignment (SoLo-RA), which aligns reward scores between paired short and long contexts that share essential task-relevant information.\nThe method can be plugged into existing PO algorithms such as DPO, SimPO, and ORPO. The authors derive a theoretical upper bound showing that the long-context PO objective can be approximated through this two-part decomposition, and propose a “chosen-only” SoLo-RA variant that further improves efficiency. Experiments on reasoning (LongBenchV1/V2, NIAH-Plus) and instruction-following benchmarks (MMLU-Pro, GPQA, BBH) demonstrate that SoLoPO achieves higher long-context performance, better reward–KL efficiency, and shorter training time without harming short-context capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed decoupling framework (short-context PO + reward alignment) is elegant, theoretically grounded, and easy to integrate into existing RLHF/PO pipelines.\n\n2.The theoretical formulation clearly explains how SoLoPO approximates the long-context objective through an upper bound, providing a solid foundation for the method.\n\n3.The “chosen-only” SoLo-RA variant is an insightful practical contribution that reduces instability and significantly cuts training cost while maintaining effectiveness.\n\n4.Extensive experiments cover multiple backbones (Qwen2.5-7B, Llama3.1-8B) and benchmarks, showing consistent gains in both long-context reasoning and efficiency.\n\n5.The method generalizes well across DPO, SimPO, and ORPO, confirming its broad applicability.\n\n6.The paper is well written, conceptually coherent, and supported by detailed ablations and efficiency analyses."}, "weaknesses": {"value": "1.The paper could provide more qualitative analysis or visualization to show how SoLoPO improves long-context reasoning (e.g., attention heatmaps or retrieved key information patterns).\n\n2.The framework assumes that short contexts can fully preserve essential information; performance may degrade if the summarization or compression is imperfect, which is not deeply discussed."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1mXvEzaQqu", "forum": "iiBjaiikJG", "replyto": "iiBjaiikJG", "signatures": ["ICLR.cc/2026/Conference/Submission12911/Reviewer_N1ah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12911/Reviewer_N1ah"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972001245, "cdate": 1761972001245, "tmdate": 1762923684798, "mdate": 1762923684798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Updates in the manuscript"}, "comment": {"value": "We are very grateful for the valuable feedback from all reviewers. Based on these comments, we have revised our paper. All modified sections are highlighted in blue for easy comparison. The revisions can be summarized as follows:\n> **Main Text:**\n- **Clarifications on Theoretical Derivations**:\n    - In Assumption 1, we added a reference to Appendix I.7 about the experimental evidence.\n    - In Theorem 1, we have added the motivation for using the $s(\\cdot)$ function and discussed the impact of its tightness. \n    - In the section introducing SoLoPO's optimization objective, we have cited Appendix I3 and I4 regarding the derivations for $f(x)=x^2$ and $f(x)=\\log \\sigma(x)$ scenarios, along with Table 16, to enhance readability. Additionally, we clarified that SoLoPO and vanilla PO are formally equivalent when $\\rho$=100\\%.\n- **Intuition on Decoupling for Long-Context Performance**: We have expanded on why decoupling improves long-context performance in the \"What does SoLoPO learn?\" (previously detailed in Appendix I9).\n- **How SoLoPO  improves the Efficiency  of Long-context Alignment**: Section 2.3 now includes \"How does SoLoPO improve data sampling and training efficiency?\" to explain SoLoPO's benefits in data sampling and model training efficiency.\n- **Experimental Section:**\n    - Table 4 now includes the performance of LongPO, accompanied by a brief analysis.\n    - We clarified that SoLoPO's stability in short-context capabilities is relative to instruct models.\n    - We added a reference to Figure 9 in Appendix G, which visualizes different ORPO variants on the NIAH-Plus benchmark.\n- **Discussion**: We added a reference to the discussion of limitations and future work in Appendix C, located after the conclusion.\n- Revisions addressing Reviewer mA21's feedback on typos and readability improvements.\n> **Appendix:**\n- **Appendix C (Limitations & Future Work)**: Expanded more details on SoLoPO's potential in long-text generation tasks.\n- **Appendix I.7**: Added experimental verification for Assumption 1.\n- **Appendix I.8**: Added experimental verification for the tightness of $s(\\cdot)$."}}, "id": "t3Af6joC1g", "forum": "iiBjaiikJG", "replyto": "iiBjaiikJG", "signatures": ["ICLR.cc/2026/Conference/Submission12911/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12911/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission12911/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728638025, "cdate": 1763728638025, "tmdate": 1763728638025, "mdate": 1763728638025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}