{"id": "MDE6S92PJR", "number": 24094, "cdate": 1758352652731, "mdate": 1759896781925, "content": {"title": "SELECT2REASON: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning", "abstract": "A practical approach to activate long chain-of-thoughts reasoning ability in pre-trained large language models is to perform supervised fine-tuning on instruction datasets synthesized by strong Large Reasoning Models such as DeepSeek-R1, offering a cost-effective alternative to reinforcement learning. However, large-scale instruction sets with more than 100k samples incur significant training overhead, while effective strategies for automatic long-CoT instruction selection still remain unexplored. In this work, we propose \\textsc{Select2Reason}, a novel and efficient instruction-tuning data selection framework for long-CoT reasoning. From the perspective of emergence of rethinking behaviors like self-correction and backtracking, we investigate common metrics that may determine the quality of long-CoT reasoning instructions. \\textsc{Select2Reason} leverages a quantifier to estimate difficulty of question and jointly incorporates a reasoning trace length-based heuristic through a weighted scheme for ranking to prioritize high-utility examples. Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only 10\\% of the data selected by \\textsc{Select2Reason} achieves performance competitive with or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across three competition-level and six comprehensive mathematical benchmarks. Further experiments highlight the scalability in varying data size, efficiency during inference, and its adaptability to other instruction pools with minimal cost.", "tldr": "We propose Select2Reason, a novel and efficient instruction-tuning data selection framework for Long-CoT reasoning.", "keywords": ["Data Selection", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11b891d398cab8f1e332f7479632f913a4c0bdca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a heuristic for data selection for optimal training on long-CoT data. It combines selection based on response length and difficulty, using LLM-as-a-Judge for the latter. The method ranks data samples based on these two metrics and selects the final subset via weighred rank aggregation.\n\nThe authors show that the method yields better performance when taking a 10% subset of OpenR1-Math-220k dataset, compared to data selection based solely on problem difficulty or response lengths, and random selection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Presented method yields stronger performance than tested simpler heuristics, which might have practical applications for training on CoT data with limited compute budget.\nThe paper is well written, and method clearly presented and easy to understand."}, "weaknesses": {"value": "The main issue of the paper might be the limited novelty. As mentioned by the authors (Introduction & Preliminary Exploration sections), data selection based on length and difficulty was already explored by multiple authors, and this work combines both metrics by weighted ranking.\n\nThe method is complex, and provides only marginal gains over much simpler heuristic based on generation length. Moreover, the LLM-as-a-Judge used for assesing difficulty of the problem relies heavily on the capability of the teacher-model to determine difficulty based on the problem definition only.\n\nAs for more actionable problems:\n- The main results are shown for already capable Qwen2.5-Math-7B-Instruct model. Additional results for other models are missing baselines -- based on length and difficulty. How does the method compare to these baselines on different models?\n- The authors showed that the number of \"rethinking\" tokens correlate with both, the problem difficulty and response length. That seems like a good baseline heuristic for data selection, as it would be proxy for both metrics. How does the method compare to this simple heuristic?\n- Figures are hardly readable. It would be beneficial to align font size of figures with the one in the main text.\n- The authors claim achieving state-of-the-art performance on multiple benchmarks. This is a false claim, as there are models with better performance, eg. Qwen3 model family. It should be corrected/clarified."}, "questions": {"value": "As mentioned in weaknesses, here are the actionable problems:\n- The main results are shown for already capable Qwen2.5-Math-7B-Instruct model. Additional results for other models are missing baselines -- based on length and difficulty. How does the method compare to these baselines on different models?\n- The authors showed that the number of \"rethinking\" tokens correlate with both, the problem difficulty and response length. That seems like a good baseline heuristic for data selection, as it would be proxy for both metrics. How does the method compare to this simple heuristic?\n- Figures are hardly readable. It would be beneficial to align font size of figures with the one in the main text.\n- The authors claim achieving state-of-the-art performance on multiple benchmarks. This is a false claim, as there are models with better performance, eg. Qwen3 model family. It should be corrected/clarified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ujdL5GS1BJ", "forum": "MDE6S92PJR", "replyto": "MDE6S92PJR", "signatures": ["ICLR.cc/2026/Conference/Submission24094/Reviewer_eZ83"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24094/Reviewer_eZ83"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918456034, "cdate": 1761918456034, "tmdate": 1762942936293, "mdate": 1762942936293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SELECT2REASON, a data selection framework for long chain-of-thought (CoT) instruction tuning. The method combines reasoning trace length and question difficulty through a weighted ranking scheme to efficiently choose high-quality examples for fine-tuning. Experiments on multiple math reasoning benchmarks show that training on only 10% of the selected data achieves comparable or superior results to full-data fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This work tackles data efficiency for long-CoT reasoning, which is a key bottleneck in current LLM research. It combines reasoning trace length and difficulty in a joint ranker is intuitive."}, "weaknesses": {"value": "1. The paper motivates the heuristics empirically, but does not provide deeper theoretical analysis on why the combination works, lacking theoretical grounding\n\n2. Both “trace length” and “difficulty” metrics are individually known heuristics; their combination, while practical, may be viewed as incremental.\n\n3. Using LLM-as-a-Judge to estimate difficulty may introduce selection bias; this aspect is not systematically analyzed"}, "questions": {"value": "1. Could the method be extended to non-mathematical reasoning tasks without redesigning metrics?\n\n2. Does the efficiency advantage hold when scaling beyond 7B models or multi-turn reasoning datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CYh1qNi7O5", "forum": "MDE6S92PJR", "replyto": "MDE6S92PJR", "signatures": ["ICLR.cc/2026/Conference/Submission24094/Reviewer_RqGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24094/Reviewer_RqGQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150695558, "cdate": 1762150695558, "tmdate": 1762942936030, "mdate": 1762942936030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the SELECT2REASON framework, which efficiently selects high-quality long-chain-of-thought (long-CoT) reasoning data from large-scale instruction pools by quantifying question difficulty and reasoning trace length. Experiments show that fine-tuning models with only 10% of the carefully selected data achieves performance comparable to or surpassing that of models trained on the full dataset and open-source baselines across multiple mathematical reasoning benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Methodological Rigor: The framework is simple yet effective, leveraging quantifiable metrics (difficulty scores, trace length) without complex computations.\nComprehensive Experiments: Evaluations span 9 mathematical benchmarks, multiple data scales (2%–10%), and diverse models (Qwen, LLaMA), ensuring reliability.\nReproducibility: Experiments are based on public datasets."}, "weaknesses": {"value": "Metric Interplay Under-explored: The combination of difficulty and trace length relies on a simple weighted sum. A deeper analysis of their correlation or conflict scenarios (e.g., long-easy vs. short-hard traces) would strengthen the method's rationale.\nPotential Bias in Difficulty Scoring: The \"LLM-as-a-Judge\" approach may inherit biases from the specific judge model (Qwen2.5-Math-7B). Using a judge model to filter data might systematically favor a certain type of \"difficult problem,\" while overlooking other equally valuable types of \"difficult problems\" that are not recognized by the model.\nGeneralization Scope is Narrow: Strong empirical validation is primarily within mathematical reasoning. Claims about general \"long-chain-of-thought reasoning\" would be more solid if supported by tests on diverse domains."}, "questions": {"value": "1.The optimal weight (w=0.25) greatly favors trace length over difficulty. What is the underlying reason? Is this ratio consistent across different model scales or data pools?\n2.The ablation study shows that diversity-aware selection (Diverse) does not yield significant gains. Given that diversity is often crucial in other data selection paradigms, what is your hypothesis for why it is less critical for long-CoT reasoning instruction tuning specifically? Does this imply that the quality of individual reasoning traces outweighs the benefit of covering a wider range of problem types in this context?\n3.When combining rankings, has consideration been given to more complex fusion methods (such as multi-objective optimization) instead of simply using linear weighting? Why do you believe that linear weighting is sufficient to capture the interaction between difficulty and length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MqTxb6WDNh", "forum": "MDE6S92PJR", "replyto": "MDE6S92PJR", "signatures": ["ICLR.cc/2026/Conference/Submission24094/Reviewer_tmCD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24094/Reviewer_tmCD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158301399, "cdate": 1762158301399, "tmdate": 1762942935554, "mdate": 1762942935554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SELECT2REASON, a data selection framework designed to reduce the high computational cost of supervised fine-tuning for long CoT reasoning. The authors hypothesize that instruction quality can be effectively proxied by two simple heuristics: the length of the reasoning trace and the difficulty of the question. The proposed method involves training a small \"judge\" model to assign a difficulty score, and then creating a joint rank that combines this score with the trace length. The core claim is that fine-tuning a model on just the top 10% of data selected by this method can achieve comparable or even superior performance to training on the entire dataset, as demonstrated on the OpenR1-Math-220k dataset with a Qwen2.5-Math-7B model."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The method relies on two intuitive and low-cost heuristics (trace length and difficulty). The finding that trace length, in particular, serves as a proxy for data quality is a useful, simple baseline.\n2. The central finding—that a 10% subset can outperform the 100% full dataset—is compelling and demonstrates a path toward more efficient tuning for complex reasoning tasks."}, "weaknesses": {"value": "1. Validation is limited. The primary claim that 10% selected data surpasses 100% full-pool data is demonstrated on only one model and dataset (Qwen2.5-Math-7B on OpenR1-Math). While generalization is explored, this core efficiency-performance tradeoff is not shown to hold for other model families (e.g., Llama, Mistral). The paper would be stronger if this main result (Table 1) were replicated on at least one other distinct model architecture.\n\n2. Key baselines are not included. The paper discusses alternative data selection methods from prior work, such as using model-specific loss or perplexity (lines 294-295), but fails to include them as baselines in the main experiments. Without this comparison, it's impossible to know if SELECT2REASON is genuinely superior to other established methods for identifying \"difficult\" or \"informative\" samples.\n\n3. Questionable difficulty measures. The paper's claim that \"Existing work has not established quantitative criteria for identifying the difficulty of questions\" (line 211-212) appears to overlook a large body of work that uses metrics like training loss, perplexity, or pass@k as proxies for difficulty. The \"LLM-as-a-Judge\" method proposed here is not well-justified over these simpler alternatives, and its own reliability and calibration are not thoroughly validated.\n\n4. Confounding variables. The method treats \"reasoning trace length\" and \"question difficulty\" as two distinct signals to be combined. However, these two variables are almost certainly highly correlated; a harder question is very likely to require a longer reasoning trace. The paper fails to disentangle these factors, making it unclear if \"difficulty\" adds any real signal beyond what \"length\" already provides. The strong performance of the \"Longest\" baseline (w=0) supports this; the joint ranker's optimal w=0.25 only provides a modest boost, suggesting length is doing most of the work.\n\n5. Unfair comparisons. The comparisons in Table 1 to external models like R1-DISTILL-QWEN and OPENR1-QWEN are not well-controlled, as these models use different data sizes and (as the user notes) likely different pre-training and post-training mixtures. This distracts from the paper's main, valid comparison, which is between the SELECT2REASON subset and the FULL-POOL on the same base model."}, "questions": {"value": "1. The paper presents a counter-intuitive finding: training on long reasoning traces (from the selected data) results in a model that uses fewer thinking tokens (i.e., is more concise) during inference. What is the proposed mechanism for this? Does the model learn to be concise from verbose examples, or does it learn a more robust reasoning process that allows it to find the correct answer more directly, thus requiring less backtracking at inference time?\n\n2. Following on the weakness of confounding variables: Can the authors provide an analysis to disentangle the effects of trace length and difficulty? For example, what is the performance of subsets selected from \"long-and-easy\" vs. \"long-and-hard\" vs. \"short-and-hard\" instructions? This would clarify whether length is merely a proxy for difficulty or if it provides an independent quality signal.\n\n3. Why was the \"LLM-as-a-Judge\" approach for difficulty chosen over more standard, lower-cost metrics like perplexity or loss from the base model or the pass@k of the base model? Was this novel judge found to be a better predictor of difficulty than these established metrics?\n\n4. Given that many data selection works use training loss or perplexity, how would SELECT2REASON compare directly against a baseline that simply selects the top 10% of data with the highest training loss (or perplexity) when fine-tuned on the full dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bCf47smcWq", "forum": "MDE6S92PJR", "replyto": "MDE6S92PJR", "signatures": ["ICLR.cc/2026/Conference/Submission24094/Reviewer_SA5m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24094/Reviewer_SA5m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178285407, "cdate": 1762178285407, "tmdate": 1762942934739, "mdate": 1762942934739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "## Summary  \nThis paper presents SELECT2REASON, an efficient instruction-tuning data selection framework designed to address the high training overhead associated with activating long chain-of-thought (long-CoT) reasoning capabilities in pre-trained large language models (LLMs) via supervised fine-tuning (SFT). While SFT on instruction datasets synthesized by powerful large reasoning models (LRMs)—such as DeepSeek-R1—serves as a cost-effective alternative to reinforcement learning, large-scale instruction sets (e.g., those with over 100k samples) impose substantial computational costs, and automated strategies for long-CoT instruction selection remain underexplored. To fill this gap, SELECT2REASON focuses on two key quantifiable heuristics for identifying high-quality instructions: question difficulty (estimated using a fine-tuned LLM-as-a-Judge) and reasoning trace length (linked to higher frequencies of rethinking behaviors like self-correction and backtracking, which are taken as indicators of strong reasoning quality). These two metrics are integrated through a weighted joint ranking scheme to prioritize high-utility examples.  \n\nEmpirical evaluations on the OpenR1-Math-220k dataset (with 196k instructions retaining correct answers) show that fine-tuning the Qwen2.5-Math-7B-Instruct model on just 10% of the data selected by SELECT2REASON achieves performance comparable to or superior to full-pool SFT and open-source baselines (e.g., OpenR1-Qwen-7B with 94k samples and DeepSeek-R1-Distill-Qwen-7B with 800k samples) across nine mathematical benchmarks. These benchmarks include three competition-level ones (AIME 2024/2025, AMC 2023) and six comprehensive ones (MATH-500, OlympiadBench, Chinese GAOKAO 2023/2024, GAOKAO Math, KAOYAN). Additional experiments validate the framework’s scalability across different data sizes, its efficiency in reducing inference tokens while preserving performance, and its generalizability to other long-CoT instruction pools (e.g., the 110k-sample Chinese-DeepSeek-R1-Distill dataset) with minimal adaptation costs. The paper also confirms that diversity—a common metric in general instruction selection—does not meaningfully enhance long-CoT reasoning, further highlighting the value of its proposed heuristics.  \n\n\n## Strengths  \n1. **Clear Problem Definition & Novelty**: The paper addresses a critical, underexplored gap in long-CoT reasoning—namely, efficient instruction selection for SFT to reduce training overhead. Its core insight (prioritizing reasoning trace length and question difficulty as quantifiable heuristics) is well-motivated and novel, filling the void of automated selection strategies for long-CoT data.  \n2. **Rigorous Preliminary Exploration**: Pre-experiments linking trace length/difficulty to rethinking token frequency (a proxy for reasoning quality) provide a solid empirical foundation for the proposed method. This “motivation → validation” workflow aligns with the standards of top machine learning conferences.  \n3. **Comprehensive Experimental Design**: The study validates SELECT2REASON across 9 mathematical benchmarks (3 competition-level, 6 comprehensive), multiple model scales (Qwen2.5-3B/7B, LLaMA-3.1-8B), and cross-dataset generalization (Chinese-DeepSeek-R1-Distill). Ablation studies (on hyperparameter *w* and subset size) and cost-benefit analysis further enhance the work’s rigor.  \n4. **Practical Impact**: Achieving competitive or superior performance with only 10% of the data (compared to full-pool SFT and baselines using 800k samples) delivers tangible value—reducing computational costs and energy consumption, which aligns with the broader trend of sustainable AI development.  \n\n\n## Weaknesses  \n1. **Insufficient Analysis of Diversity’s Irrelevance**: The paper concludes that diversity does not contribute to long-CoT instruction selection, but its reasoning is underdeveloped. Is this result dataset-specific (e.g., because OpenR1-Math is already sufficiently diverse in mathematical concepts) or an inherent property of long-CoT reasoning tasks? Could combining diversity with trace length/difficulty yield additional performance gains for less curated datasets? No ablation studies (e.g., comparing the joint ranker with/without diversity signals) or qualitative analyses are provided to explain *why* diversity fails to add value in long-CoT selection.  \n2. **Rethinking Tokens as a Proxy for Reasoning Quality**: The paper links trace length/difficulty to the frequency of rethinking tokens (e.g., “Wait,” “Alternatively”) as a sign of high-quality reasoning. However, it fails to address edge cases: Are there instances where long traces or difficult questions contain few rethinking tokens but still drive significant SFT gains in long-CoT reasoning? Conversely, do low-quality traces (e.g., those with redundant “maybe” statements) artificially inflate rethinking token counts without providing useful supervision? A qualitative comparison of such cases would strengthen the validity of this proxy.  \n3. **Generalization to Non-Mathematical Long-CoT Tasks**: While the paper includes a small set of experiments on broader tasks (e.g., ZebraLogic, GPQA), its core evaluations focus exclusively on mathematics. Long-CoT reasoning is critical for non-mathematical domains, such as coding (where reasoning involves syntax validation and algorithm design), complex logical inference, and scientific problem-solving. Does SELECT2REASON generalize to these domains? For example, would trace length and difficulty be equally effective for selecting coding instructions (where reasoning quality depends on syntax correctness and semantic alignment rather than mathematical steps)? No systematic cross-domain validation is provided beyond mathematics and limited logical QA tasks.  \n\n\n## Questions  \nQ1: Could the authors supplement the ablation analysis on diversity for long-CoT instruction selection? (Related to Weakness 1)  \nQ2: Do edge cases exist in the dataset (e.g., long traces/difficult questions with few rethinking tokens that still boost SFT gains, or low-quality traces with inflated rethinking token counts)? If so, could the authors provide a brief explanation for these cases? (Related to Weakness 2)  \nQ3: Have the authors conducted any experiments on non-mathematical long-CoT reasoning tasks (e.g., coding, scientific problem-solving)? If yes, could they share the relevant results; if not, could they discuss the potential generalizability of SELECT2REASON to such tasks? (Related to Weakness 3)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "See Summary"}, "weaknesses": {"value": "See Summary"}, "questions": {"value": "See Summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zfI1kccUaS", "forum": "MDE6S92PJR", "replyto": "MDE6S92PJR", "signatures": ["ICLR.cc/2026/Conference/Submission24094/Reviewer_UD9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24094/Reviewer_UD9B"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762908517766, "cdate": 1762908517766, "tmdate": 1762942933890, "mdate": 1762942933890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}