{"id": "kdpeJNbFyf", "number": 4005, "cdate": 1757583304840, "mdate": 1763697446949, "content": {"title": "Latent Diffusion Model without Variational Autoencoder", "abstract": "Recent progress in diffusion-based visual generation has largely relied on latent diffusion models with Variational Autoencoders (VAEs). While effective for high-fidelity synthesis, this VAE+Diffusion paradigm suffers from limited training and inference efficiency, along with poor transferability to broader vision tasks. These issues stem from a key limitation of VAE latent spaces: the lack of clear semantic separation and strong discriminative structure. Our analysis confirms that these properties are not only crucial for perception and understanding tasks, but also equally essential for the stable and efficient training of latent diffusion models. Motivated by this insight, we introduce **SVG**—a novel latent diffusion model without variational autoencoders, which unleashes **S**elf-supervised representations for **V**isual **G**eneration. SVG constructs a feature space with clear semantic discriminability by leveraging frozen DINO features, while a lightweight residual branch captures fine-grained details for high-fidelity reconstruction. Diffusion models are trained directly on this semantically structured latent space to facilitate more efficient learning. As a result, SVG enables accelerated diffusion training, supports few-step sampling, and improves generative quality. Experimental results further show that SVG preserves the semantic and discriminative capabilities of the underlying self-supervised representations, providing a principled pathway toward task-general, high-quality visual representations.", "tldr": "", "keywords": ["generative model", "deep learning", "self-supervised learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/93f0f75ed06656d78f6386a2a0903b015ec91e65.pdf", "supplementary_material": "/attachment/45de3c10b3747527b0fb59cb4cb81578f439dc0a.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce SVG - a novel latent diffusion model without variational autoencoders. The method leverages features from a DINO model while a residual branch captures features useful for high fidelity generation. The authors claim that the method supports accelerated training, few step sampling and improved generation quality which is supported by extensive empirical comparisons with prior baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Simplicity of the approach**: The proposed pipeline is simple: Training diffusion models in a more discriminative feature space while augmenting it with residual generative feature spaces.\n\n2. **Empirical results**: I like the ablation experiments in Section 4.3 which clearly highlight the contributions of the core components of the method. Similarly the main results presented in Section 4.2 demonstrate the proposed method has better training efficiency than competing baselines. I also like Fig. 4b which presents a nice intuitive explanation of the impact of mode separation on diffusion models training. If the modes are mixed, any diffusion or flow model will have trouble learning the score directions while a clear mode separation enhances learning."}, "weaknesses": {"value": "1. In the abstract (line 014): The authors mention slow inference as one of the drawbacks of Latent diffusion models (LDMs). This claim is a bit shaky as the latent space dimension is often much less than the original image (compare 3x512x512 original image size to 64x32x32 in StableDiffusion 1.5). Therefore inherently LDMs support fast sample generation than a diffusion model trained solely in the pixel space. Moreover, in my personal experience, with fast samplers like DPM solver, good quality sample generation takes around 20-50 diffusion steps and is quite fast. Therefore I’m not sure how valid this claim is in practice.\n\n2. **Presentation**: In current Fig.1, the top row in the figure showing the block diagram for the architecture is a bit uninformative in terms of different color coding schemes. Also the figure caption provides no meaningful information and should be revised accordingly to make it more descriptive.\n\n3. **Related Work**: In line 67 the authors claim that VAE latent spaces inherently lack semantic separability. Can the authors provide a few citations to support this claim here? There has been a lot of work in disentangling the VAE representations starting from Beta-VAEs [Higgins et al]. Similarly a citation for DINOv3 is missing in Line 73 where it is first introduced. There is some related work on combining VAEs with Diffusion models which is missing from the main text to which these advances can be potentially applied too and thus worth discussing in the related work section. For instance:\n\na. Score-based Generative Modeling in Latent Space, Vahdat et al.\n\nb. DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents, Pandey et al.\n\nc. Diffusion Autoencoders: Toward a Meaningful and Decodable Representation, Preechakul et al.\n\nSimilarly there is some recent work on inspecting the spectral properties of the latent space of latent diffusion models which needs discussion in the main text:\n\na. Improving the Diffusability of Autoencoders, Skorokhodov et al.\n\nb. EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling, Kouzelis et al.\n\n4. **Re. Empirical Results**:\n\na. The analysis in Section 3.2 is quite interesting. While the t-SNE visualizations are interesting, I think the findings related to the discriminative ability of different feature extractors in Fig. 4a can be made more concrete by demonstrating the top-k accuracy using a classifier head on top of these features extractors.\n\nb. Can the authors provide the loss training curves (somewhere in the appendix maybe) to demonstrate the stability of the SVG training pipeline?\n\nc. Did the authors experiment with fitting diffusion models on the feature space of intermediate layers in the encoder? I understand this could be quite compute intensive for an ablation but could provide valuable insights into what feature spaces are good for working with diffusion models.\n\nd. From Table 4, the advantages of distribution alignment do not seem to statistically significant and therefore unclear. I’m curious why the authors decided to keep this component as a part of their core pipeline?\n\n5. **Minor**: Is there a typo in the heading of Section 3.3 (Visual Feature Generation vs Feature Visual Generation)?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VHTsailel7", "forum": "kdpeJNbFyf", "replyto": "kdpeJNbFyf", "signatures": ["ICLR.cc/2026/Conference/Submission4005/Reviewer_go1M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4005/Reviewer_go1M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683418029, "cdate": 1761683418029, "tmdate": 1762917133426, "mdate": 1762917133426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors claim that the latent space of VAE is not generation-friendly.  Instead of VAE, they propose a self-supervised representation for Visual Generative (SVG).\n- SVG autoencoder maintains the dimension of the DINO feature and augments the residual network to enhance reconstruction.\n- While training diffusion models in high-dimensional spaces is generally challenging, the well-dispersed semantic structure of SVG features makes training stable and efficient."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- They first introduce to directly employ the dimension of DINO v3 feature and show that the well-dispersed semantic feature enables training in a high-dimensional space\n- The feature visualization in Fig. 4 and toy experiments support the author's hypothesis that the latent space of VAE is not generation-friendly."}, "weaknesses": {"value": "- Lack of explanation of architectural choice (Residual encoder)"}, "questions": {"value": "- Have you tried other architectures except for the Residual encoder, such as adapting LoRA to DINO, or employing other foundational vision encoders? The explanation will be helpful to strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HmGKSPGhbt", "forum": "kdpeJNbFyf", "replyto": "kdpeJNbFyf", "signatures": ["ICLR.cc/2026/Conference/Submission4005/Reviewer_ANZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4005/Reviewer_ANZt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974879213, "cdate": 1761974879213, "tmdate": 1762917133231, "mdate": 1762917133231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes replacing the VAE-based latent space commonly used in latent diffusion models with a latent space derived directly from a pretrained self-supervised vision model, specifically DINOv3. The authors argue that the DINOv3 feature space already encodes strong semantic structure, making it a more suitable domain for diffusion than the low-level reconstruction-oriented latent space produced by VAEs. To enable high-fidelity image reconstruction from this feature space, the method introduces a lightweight residual decoder that is trained to recover fine-grained visual details. A diffusion model is then trained directly in this semantic latent space.\n\nThe paper provides extensive experiments demonstrating improved generation quality and training efficiency compared to standard VAE-based latent diffusion baselines. The authors also show that the resulting latent space preserves semantic separability and can benefit downstream recognition or manipulation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and intuitive idea. While prior work has attempted to align VAE latent spaces with features from vision foundation models to improve semantic consistency, this paper takes a more direct and conceptually clean approach by completely replacing the VAE encoder with pretrained DINOv3 features. This eliminates the need to engineer semantic structure into the latent space and leverages an already well-organized representation.\n2. Comprehensive empirical validation. The authors present an extensive set of experiments, covering comparisons with multiple baselines, ablation studies, visual quality assessments, and evaluation across different datasets. The breadth of experiments strengthens the credibility of the proposed approach.\n3. Strong performance gains. The method achieves consistently improved generation quality while also demonstrating benefits in efficiency and semantic controllability."}, "weaknesses": {"value": "1. Limited analysis of the residual decoder design. While the proposed residual module plays a critical role in reconstructing high-frequency details, the paper does not provide sufficient ablations on its architecture or training strategy. In particular, it is unclear how sensitive the overall performance is to this component, and the paper does not report results using only the DINOv3 features without the residual correction branch. This makes it difficult to assess how much of the performance gain comes from the semantic latent space itself versus the added decoder capacity."}, "questions": {"value": "1. Classifier-free guidance behavior. The paper mentions that classifier-free guidance has a reduced effect in this semantic latent space, but no dedicated experiment or quantitative analysis is provided. Could the authors clarify why guidance becomes less influential, and provide empirical evidence illustrating how guidance scales differ compared to standard VAE-based latent diffusion?\n2. Interpolation behavior in the semantic latent space. Although the latent representation is said to be semantically well-structured, the interpolation results sometimes show abrupt semantic changes rather than smooth transitions. How do the authors interpret this phenomenon? Does it indicate that the latent space is organized more in a clustered manner rather than forming smooth semantic manifolds?\n3. Extension to video generative models. Do the authors believe that this architecture can be extended to video (i.e., replacing video VAEs with video or image foundation model features)? If so, what modifications would be necessary to handle temporal consistency and motion priors? Any discussion on temporal latent alignment would be valuable.\n4. Model size and decoding efficiency. Compared to a standard VAE encoder–decoder pipeline, how does using a vision foundation model affect computational overhead and inference latency? In particular:\nWhat is the relative model size change?\nDoes the residual decoder introduce additional decoding cost?\nIs end-to-end sampling faster or slower in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2SQY7MMdoP", "forum": "kdpeJNbFyf", "replyto": "kdpeJNbFyf", "signatures": ["ICLR.cc/2026/Conference/Submission4005/Reviewer_w1GX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4005/Reviewer_w1GX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762228073217, "cdate": 1762228073217, "tmdate": 1762917132786, "mdate": 1762917132786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SVG, a latent diffusion framework that removes the VAE encoder and instead uses frozen DINOv3 features as the semantic backbone, augmented with a lightweight residual encoder for fine details. A decoder maps the concatenated features back to pixels; diffusion is trained directly in this high-dimensional feature space using a flow-matching setup. The authors argue this yields a latent space with better semantic separability, enabling faster convergence and competitive few-step sampling on ImageNet-256. Key components include (i) the DINO-based encoder + residual branch, (ii) a distribution alignment step for the residual features, and (iii) a standard DiT/SiT-style diffusion trained over the new latent space. Reported results show improved few-step FID/IS and faster training vs. VAE-based baselines, with additional qualitative analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Replaces VAE latents with a semantically structured feature space (frozen DINOv3) and adds a Residual Encoder to recover high-frequency detail for reconstruction. The training pipeline is simple and reuses standard diffusion tooling. \n\n- At 80 epochs / 25 steps, SVG-XL reports good generation performance and improves further with more training, indicating strong few-step behavior relative to baselines. \n\n- The velocity toy study ties semantic dispersion to optimization ease and step efficiency; paired with t-SNE, it provides an intuitive explanation for why VLM feature space diffusion may converge faster."}, "weaknesses": {"value": "- Missing REPA-XL 80-epoch (CFG) numbers.\n\nTable 1 lists REPA-XL (80 epochs, 250 steps) only without CFG and with missing IS, whereas the 800-epoch row provides both CFG and no-CFG. Since the central claim is superior data/compute efficiency, the 80-epoch CFG result for REPA-XL is essential for a fair comparison. \n\n- Tokenizer model-size comparison not shown in Table 1.\n\nThe system table includes #params for the generation model but omits tokenizer sizes. Given SVG uses frozen DINOv3 + Residual Encoder, it'd be helpful to have a side-by-side tokenizer capacity comparison (params and MACs) against SD-VAE / VA-VAE to contextualize efficiency and scaling claims.\n\n- Unclear scaling to higher resolution.\n\nThe Limitations section explicitly states that higher resolutions and larger datasets remain “underexplored.” Given VLM feature space diffusion relies heavily on the base frozen VLM, the method may inherit DINOv3’s resolution constraints. Please clarify whether scaling requires a different backbone or architectural changes. Empirical evidence beyond 256×256 and ideally 1024+ resolution is very helpful to justify the potential for real-world applications such as T2I. \n\n\n- Overclaims in the Introduction.\n\nThe paper claims to “fully retain DINOv3’s strengths beyond generation”, but the addition of a Residual Encoder and decoder training could alter the representational geometry. This needs quantitative evidence (e.g., linear probing / retrieval / segmentation probes done in the combined space, not just DINO). \n\nIt also states prior alignment approaches are “ad hoc fixes” that “do not fundamentally alter … latent space structure.” This contradicts the improved results of the baselines mentioned here. In my opinion, these methods absolutely alter the latent space structure.  \n\n\n- Insufficient detail on residual-distribution alignment.\n\nSection 3.2 notes: “we align the Residual Encoder outputs with the DINO feature distribution” but does not specify how. Sensitivity to alignment strength and its effect on semantic separability vs. reconstruction is crucial; please add technical details and an ablation sweep. \n\n- Semantic analysis of the new latent space is limited.\nt-SNE and the velocity toy example are helpful, but they don’t isolate the effect of the residual branch on semantics. Provide representation probes on the final concatenated space (not just DINO alone): e.g., linear probing, can be helpful."}, "questions": {"value": "- REPA-XL (80-epoch, CFG): Can you report CFG FID/IS for REPA-XL at 80 epochs in Table 1 to support efficiency claims under equalized compute? \n\n- Tokenizer size & cost: Please provide tokenizer parameter counts and FLOPs/MACs (encoder+decoder) for SD-VAE / VA-VAE / SVG in a single table to make system-level comparisons fair. \n\n- High-resolution scaling: What changes (if any) are needed to scale SVG to 512/1024? Any preliminary 512×512 results? \n\n- Alignment details: How exactly is the residual aligned to DINO (loss form, feature layers tapped, λ values)? Sensitivity plots would help. \n\n- Semantic probes on the final space: Can you add linear-probe / t-SNE on the concatenated SVG features (with and without alignment) to substantiate the “retains DINO strengths” claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "srSLNKF7Br", "forum": "kdpeJNbFyf", "replyto": "kdpeJNbFyf", "signatures": ["ICLR.cc/2026/Conference/Submission4005/Reviewer_j2Au"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4005/Reviewer_j2Au"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762592892870, "cdate": 1762592892870, "tmdate": 1762917132568, "mdate": 1762917132568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}