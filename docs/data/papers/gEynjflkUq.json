{"id": "gEynjflkUq", "number": 20704, "cdate": 1758309195284, "mdate": 1763721574583, "content": {"title": "Reasoning as Meta-Learning: An Optimization Perspective to Decipher Long CoT Reasoning in LLMs", "abstract": "We propose a novel framework RaML for interpreting the reasoning capabilities of large language models (LLMs) through the perspective of meta-learning. By conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLM's parameters, we identify parallels between LLM reasoning and various meta-learning paradigms. We formalize the training process for reasoning tasks as a meta-learning setup, with each question treated as an individual task, and reasoning trajectories serving as the inner loop optimization for adapting model parameters. Once trained on a diverse set of questions, the LLM develops fundamental reasoning capabilities that can generalize to previously unseen questions. Extensive empirical evaluations substantiate the strong connection between LLM reasoning and meta-learning. We further explore the potential of the proposed RaML framework to advance LLM reasoning and provide valuable insights. Our work deepens the understanding of LLM reasoning processes and provides actionable insights for enhancing these models through established meta-learning techniques.", "tldr": "We propose a novel framework for interpreting the reasoning capabilities of LLMs through the perspective of meta-learning.", "keywords": ["large language model reasoning", "interpretability", "optimization", "meta learning", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62e80e4c25cc9a6fb516ea605a17d85a56e25a7a.pdf", "supplementary_material": "/attachment/c282b1b693dce8d837bf704c1f79202590f97fec.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Reasoning as Meta-Learning as a theoretical and empirical framework that interprets\n\nLLM reasoning as a meta-learning process. It conceptualizes a model’s reasoning trajectory (chain-of-thought) as a pseudo-gradient descent path that updates the model’s hidden parameters while reasoning, analogous to how inner-loop optimization operates in Model-Agnostic Meta-Learning (MAML). The authors then utilize the conceptual framework to explain LLM reasoning."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The work proposes a novel conceptual framework. The association between meta-learning and ICL is interesting."}, "weaknesses": {"value": "The conceptual framework is quite disconnected to experiments. The experimental results are mostly past works’ finding, while the authors explain them in their own words using the proposed framework. However, it is poorly explained. Many places are very unclear, especially inner-loop, outer-loop in MAML and how is the optimization working here for LLMs. \n\n The framework is over-simplified, while actual LLMs are more complex. No sufficient discussion or theory to fill this gap in this paper.\n\nThe authors assume longer CoTs lead to better reasoning results and the model optimizes its result after each step (the authors assume the answer is always improving). However, this assumption is too strong, not well established empirically or no theoretical guarantee.  Many past works are showing the opposite. The models can give correct answers without CoT or in the middle of CoT. The models may even give right early-exit answers despite wrong reasoning steps.\n\nfor example, Lanham, Tamera, et al. \"Measuring faithfulness in chain-of-thought reasoning.\" *arXiv preprint arXiv:2307.13702* (2023). Yang, Chenxu, et al. \"Dynamic Early Exit in Reasoning Models.\" *arXiv preprint arXiv:2504.15895* (2025)…\n\nNotations are not very clearly defined. \n\n\n**Suggested references to add:**\n\nTransformers Learn In-Context by Gradient Descent\n\nThis work also introduce the idea of pseudo gradient. the reasoning steps in CoT can be viewed as context as well before the answer. The authors should discuss this work in literature review."}, "questions": {"value": "what is W1, W2? There is also no formal definition of L"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mSzbNhD1L2", "forum": "gEynjflkUq", "replyto": "gEynjflkUq", "signatures": ["ICLR.cc/2026/Conference/Submission20704/Reviewer_zmaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20704/Reviewer_zmaQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831484321, "cdate": 1761831484321, "tmdate": 1762934085446, "mdate": 1762934085446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAML (Reasoning As Meta-Learning), a novel framework for interpreting LLM reasoning capabilities through meta-learning principles. The key insight is conceptualizing reasoning trajectories (chain-of-thought steps) as pseudo-gradient descent updates to the LLM's parameters. The authors formalize reasoning task training as meta-learning, where each question is a task, reasoning trajectories serve as inner-loop optimization for parameter adaptation, and answers constitute the query set. The paper provides theoretical analysis (proving existence of parameter updates corresponding to trajectory tokens), extensive empirical validation on mathematical reasoning benchmarks, and practical insights for improving LLM reasoning through meta-learning techniques."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The mapping between training approaches (SFT vs. RL) and meta-learning concepts (off-policy vs. on-policy optimization, support set size) provides potentially new theoretical insights into recent advances in long-CoT reasoning models.\n2. The experimental design is sound—training models from scratch to avoid confounds from prior training. Multiple benchmarks are employed, and the visualization of loss landscapes and pseudo-gradient updates provides compelling, intuitive evidence. The ablation studies systematically examine key factors, and practical methods are proposed and validated.\n3.  The framework opens connections to the rich meta-learning literature, suggesting numerous research directions. The finding about optimal shorter trajectories maintaining performance has important practical implications for inference efficiency."}, "weaknesses": {"value": "1. The paper analyzes reasoning models through a meta-learning lens. However, it lacks a clear articulation of how this framework fundamentally differs from existing mechanistic analyses of transformers' learning capabilities via attention mechanisms. Specifically, the relationship to prior work on mesa-optimization [1], implicit dynamics of in-context learning [2], and higher-order optimization in ICL [3] remains unclear. The paper must establish stronger motivation for why this particular analytical framework is essential and demonstrate concrete advantages over existing frameworks for understanding transformers' implicit learning properties.\n2. Several critical aspects of the proposed framework are underspecified or unsound:\n- The paper borrows terminology from meta-learning and optimization theory without establishing rigorous connections. Specifically, how does a classical learned optimizer (e.g., meta-Adam, meta-SGD) formally correspond to a reasoning model that generates reasoning trajectories? This equivalence requires explicit justification.\n- Ambiguities in Algorithm 2  (Appendix C): For each question i, multiple trajectories t_j are generated. However, the granularity is unclear. Does a trajectory represent a single token or a token sequence? Section 3.3 states that each token corresponds to one optimization step, which would imply N×T inner-loop optimization steps for T trajectories of N tokens each. Additionally, is optimization performed in a mini-batch or a sequential format? These details are crucial for reproducibility.\n- Since the pseudo-gradient update lacks explicit evaluation and backward passes, and no explicit parameter updates occur, the second-order gradient must be approximated rather than computed via automatic differentiation. The approximation method and its implications require explicit discussion.\n3. The paper lacks an essential theoretical analysis of the proposed meta-learning framework. Specifically, there is no proof of convergence for the inner-loop optimization over reasoning trajectories. There is no analysis of generalization or smoothness properties. The pseudo-gradient approximation of second-order gradients (versus full backpropagation) introduces bias and computational differences that demand rigorous characterization.\n4. The framework exclusively relies on pseudo inner-loop objectives based on model log-likelihood. This raises concerns about error propagation when initial model likelihoods are inaccurate. The framework should consider a broader range of objectives and optimization methods within the meta-learning paradigm, such as energy function minimization [4,5] and meta-reinforcement learning approaches like Meta-CoT [6].\n5. Several key takeaways, e.g., \"SFT leads to stable reasoning rollouts\" and \"Longer reasoning trajectories improve performance\", have been extensively validated in prior work. While confirmation may have some value, these findings provide minimal marginal contribution to the field.\n\n### References\n[1] Von Oswald, J., Schlegel, M., Meulemans, A., Kobayashi, S., Niklasson, E., Zucchet, N., ... & Sacramento, J. (2023). Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858.\n[2] Dherin, B., Munn, M., Mazzawi, H., Wunder, M., & Gonzalvo, J. (2025). Learning without training: The implicit dynamics of in-context learning. arXiv preprint arXiv:2507.16003.\n[3] Fu, D., Chen, T. Q., Jia, R., & Sharan, V. (2024). Transformers learn to achieve second-order convergence rates for in-context linear regression. Advances in Neural Information Processing Systems, 37, 98675-98716.\n[4] Gladstone, A., Nanduru, G., Islam, M. M., Han, P., Ha, H., Chadha, A., ... & Iqbal, T. (2025). Energy-Based Transformers are Scalable Learners and Thinkers. arXiv preprint arXiv:2507.02092.\n[5] Oarga, A., & Du, Y. (2025). Generalizable Reasoning through Compositional Energy Minimization. arXiv preprint arXiv:2510.20607.\n[6] Xiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A., Blagden, C., ... & Finn, C. (2025). Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought. arXiv preprint arXiv:2501.04682."}, "questions": {"value": "See my questions in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dEOWA4g2Du", "forum": "gEynjflkUq", "replyto": "gEynjflkUq", "signatures": ["ICLR.cc/2026/Conference/Submission20704/Reviewer_3CaD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20704/Reviewer_3CaD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985050622, "cdate": 1761985050622, "tmdate": 1762934084052, "mdate": 1762934084052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors in thia paper reinterpret long chain-of-thought (CoT) reasoning in large language models (LLMs) as a form of meta-learning. Specifically, they treat reasoning trajectories as optimization: Each reasoning step is treated as an implicit parameter update, akin to gradient descent, guiding the model toward the correct answer. And provide both theoretical analysis and empirical results for this claim. Then, they conceptualize that each question is framed as a separate task. The reasoning trajectory acts as the inner-loop optimization during meta learning, and the final answer is the outer-loop objective.\n\nThey observed the following findings:\n- Supervised fine-tuning (SFT) provides stable inner-loop optimization, while reinforcement learning (RL) offers exploration benefits. Combining both yields better results.\n- Longer reasoning trajectories improve performance, similar to more optimization steps in meta-learning.\n- Reflection tokens (e.g., “Wait”, “Therefore”) act like optimization triggers, helping models escape local minima.\n- Generalization: Models trained with trajectories generalize better across domains (math, science, code), showing transferability of reasoning skills."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a meta-learning perspective on chain-of-thought reasoning, viewing each reasoning trajectory as a pseudo-gradient descent inner-loop update. This conceptual reframing is novel compared to [other work](https://aclanthology.org/2023.findings-acl.247.pdf) that view in-context learning as meta learning. \n\nThe paper is easy to follow. Authors provide takeaways and discuss implications to reasoning training in section 4."}, "weaknesses": {"value": "Training uses Open Reasoner Zero dataset with synthetic trajectories from large teachers; evaluation is said to be orthogonal to training data. Did the author check for contamination between ORZ/teachers and AIME24, MATH500, LiveMathBench, GPQA, LiveCodeBench?\n\nAuthors present this paper with core theoretical premise: each reasoning token induces a pseudo-gradient update to model parameters/ However such a premise is only partially formalized. While Proposition 2.1 provides a mapping, the paper is short of: analyzing approximation error, quantifying the degree of equivalence, specifying conditions under which the pseudo-update interpretation holds.\n\nMissing error bars/CI tests across main tables/figures. Please add appropriate information about statistical significance (error bars, confidence intervals, statistical significance tests) and details about how this was computed."}, "questions": {"value": "Most experiments use math-focused datasets (AIME, MATH500, LiveMathBench). While the authors test GPQA and code reasoning, the coverage is shallow. This narrows the generality claim, especially since reasoning difficulty distribution differs across domains. How does the method apply to reasoning in other domains, such as [medical](https://aclanthology.org/2025.acl-long.896/) and [law](https://arb.duckai.org/).\n\nThe pseudo-gradient interpretation would be more convincing if supported by probing internal activations. Would be great if authors could include representational similarity analysis or [causal tracing](https://www.anthropic.com/research/tracing-thoughts-language-model).\n\nCan gradient conflict resolution in meta learning inspire weighted trajectory blending in post training LLM? Meta-learning has the gradient interference problem. Conflicting tasks lead to brittle meta-initialization, with algorithms like PCGrad, GradVac, Fishr address this. Can this leads to some hidden implication that reasoning trajectories include conflict: deductive vs heuristic, long reflection vs direct derivation.\n\nAuthors mentioned that special ENDOFTHINKING tokens regulating the length of reasoning facilitate fast-converging optimization steps. But when should we insert ENDOFTHINKING tokens (early stop of reasoning) in the reasoning trajectory to make efficient reason: early stops can reach the same answer as continue thinking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UhoWYaiPMk", "forum": "gEynjflkUq", "replyto": "gEynjflkUq", "signatures": ["ICLR.cc/2026/Conference/Submission20704/Reviewer_PWxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20704/Reviewer_PWxJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022561534, "cdate": 1762022561534, "tmdate": 1762934081683, "mdate": 1762934081683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAML, a meta-learning perspective on LLM reasoning. The core idea is to treat chain-of-thought trajectories as pseudo–gradient descent steps on an implicit loss surface, so that answering each question becomes an inner-loop adaptation process. The authors give a theoretical formulation using a simplified transformer block, argue that attention over reasoning tokens can be reparameterized as parameter updates, and visualize the resulting “loss landscapes”. On the empirical side, they study several factors through this lens: SFT vs. RL training, on-policy vs. off-policy trajectories, trajectory length and reflection tokens, and within-domain vs. cross-domain generalization. Finally, they propose two simple strategies inspired by meta-learning (more trajectories per question and summarized trajectories) and show that these can improve or preserve reasoning performance under different budget settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper offers a clean and coherent framework that connects two active areas: long-CoT LLM reasoning and gradient-based meta-learning. The mapping between reasoning trajectories and inner-loop optimization steps is clear.\n\n* Theoretical, empirical, and conceptual parts are relatively consistent with each other. The simplified transformer analysis, loss-landscape visualizations, and behavioral experiments all support the same narrative.\n\n* The experimental section is fairly extensive: multiple benchmarks (AIME24, MATH500-L5, LiveMathBench-Hard, GPQA, LiveCodeBench), several training recipes (SFT, Zero-GRPO, SFT+GRPO), and analyses on trajectory length, token types, and generalization.\n\n* The writing and presentation are clear. Figures and tables are well organized and help the reader see the connection between the meta-learning view and concrete LLM behaviors."}, "weaknesses": {"value": "Overall, this paper proposes an interesting angle to try to unify several recent lines of work in reasoning, and it argues for its story and research questions in a fairly consistent way from the theoretical, empirical, and expository perspectives. The paper reads as well-reasoned, and the logic is relatively self-contained and closed; these aspects are very nice.\n\nHowever, my main concern is that the authors seem to build a new conceptual system to reinterpret existing problems from a different angle, and in the end almost everything is devoted to repeatedly emphasizing that this new system aligns very well with how people already think about today's LLM-reasoning tasks. The new framework, after all this effort, does not appear to bring many genuinely new directions or critiques that emerge from “re-thinking” the space. Most of the conclusions and proposed future directions look quite similar to what one would obtain from the original, non-meta-learning framing of reasoning.\n\nOf course, this is also a positive sign in one sense: the authors do convincingly show that the two perspectives are compatible, and I am convinced on that point. But if, under both systems, most things neither conflict nor lead to any critical tension, then it is not yet clear what concrete difference it makes to adopt this new perspective. This gap between “nice unifying story” and “new actionable or disruptive insight” is my main reservation about the contribution."}, "questions": {"value": "* If most findings, intuitions, and future directions remain essentially the same under both today's LLM-reasoning perspective (without the meta-learning concept) and the proposed meta-learning perspective, what is the practical benefit of switching to or adopting this new framework? How should a practitioner or researcher reason differently about model design or training because of RAML?\n\n* During the rebuttal, I am especially curious to see whether the authors can use the meta-learning angle to surface **different** critical viewpoints or guidance for several of the current major directions in reasoning, rather than future guidance that is almost completely aligned with existing narratives. If RAML can highlight specific tensions, trade-offs, or failure modes that the standard view misses, that would help the paper go beyond the feeling of merely being “self-consistent” and would make its contribution more instructive and potentially disruptive for the community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zmSxmNYyrC", "forum": "gEynjflkUq", "replyto": "gEynjflkUq", "signatures": ["ICLR.cc/2026/Conference/Submission20704/Reviewer_dnUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20704/Reviewer_dnUv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762284827653, "cdate": 1762284827653, "tmdate": 1762934081101, "mdate": 1762934081101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their thorough and thoughtful reviews of our paper. We are grateful for the positive feedback on RAML’s clean framework, extensive experiments, and clear presentation, as well as the constructive suggestions provided. These have helped us significantly improve the clarity and rigor of our work. We provide a detailed response to each comment. Here we highlight our major clarifications regarding the theoretical framework, practical utility, and experimental methodology. We hope our responses can properly address your concerns.\n\n- **Theoretical Grounding and Conceptual Feasibility (Reviewers dnUv, PWxJ, 3CaD, zmaQ):** We want to clarify that RAML serves primarily as a descriptive framework to interpret LLM behaviors rather than a prescriptive mathematical proof of convergence. Our goal is to establish a conceptual bridge between forward computation and pseudo-gradient updates to motivate the framework.\n    - **Feasibility:** **Proposition 2.1** demonstrates the structural feasibility of this mapping within standard Transformer layers.\n    - **Validation:** While quantifying exact approximation error is non-trivial due to the abstract objective , we provide strong empirical evidence (e.g., **Figures 2 and 13**) showing consistent alignment between our framework and observed model behaviors. We emphasize that explicit calculation of second-order gradients is unnecessary for this analogy to hold, as standard backpropagation in SFT/RL implicitly optimizes the \"optimizer\" (the LLM).\n- **Practical Utility and Novel Data Strategies (Reviewers dnUv, 3CaD):** We highlight that RAML offers critical, divergent guidance compared to standard data scaling narratives.\n    - **New Insight:** While current consensus emphasizes increasing question diversity ($|\\mathcal{Q}|$), our meta-learning perspective suggests that increasing **trajectory density per question** (Support Set Size) is equally critical for stabilizing inner-loop optimization.\n    - **Actionable Strategy:** Our experiments (**Figure 7 and Table 3**) validate that \"few questions, multi-trajectory\" datasets can yield greater gains. This provides a structured methodology to derive new strategies for improving LLM reasoning beyond simple interpretation.\n- **Validity of Proxy Objectives and Metrics (Reviewers PWxJ, 3CaD, zmaQ):** We clarify our choice of metrics and address concerns regarding objective functions.\n    - **Logits as Proxy:** Since reasoning is a discrete process with an intractable true objective, using the log-likelihood of the correct answer is a standard, principled proxy. We deliberately chose logits over internal state probes (like causal tracing) to ensure our analysis remains generalizable to both open-weights and closed-box models.\n    - **Robustness:** To ensure statistical significance, we have incorporated explicit error bars and confidence intervals (via bootstrap resampling) in our revised results, averaging performance over multiple runs (e.g., Pass@8)."}}, "id": "MeAtrcOp5O", "forum": "gEynjflkUq", "replyto": "gEynjflkUq", "signatures": ["ICLR.cc/2026/Conference/Submission20704/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20704/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20704/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763720324799, "cdate": 1763720324799, "tmdate": 1763720324799, "mdate": 1763720324799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}