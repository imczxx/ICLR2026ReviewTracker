{"id": "cFhcd4WGjO", "number": 5930, "cdate": 1757947233992, "mdate": 1759897944165, "content": {"title": "Beyond Instance-Level Alignment: Dual-Level Optimal Transport for Audio-Text Retrieval", "abstract": "Cross-modal matching tasks have achieved significant progress, yet remain limited by mini-batch subsampling and scarce labelled data. Existing objectives, such as contrastive losses, focus solely on instance-level alignment and implicitly assume that all feature dimensions contribute equally. Under small batches, this assumption amplifies noise, making alignment signals unstable and biased. We propose DART (Dual-level Alignment via Robust Transport), a framework that augments instance-level alignment with feature-level regularization based on the Unbalanced Wasserstein Distance (UWD). DART constructs reliability-weighted marginals that adaptively reweight channels according to their cross-modal consistency and variance statistics, highlighting stable and informative dimensions while down-weighting noisy or modality-specific ones. From a theoretical perspective, we establish concentration bounds showing that instance-level objectives scale with the maximum distance across presumed aligned pairs, while feature-level objectives are governed by the Frobenius norm of the transport plan. By suppressing unmatched mass and sparsifying the transport plan, DART reduces the effective transport diameter and tightens the bound, yielding greater robustness under small batches. Empirically, DART achieves state-of-the-art retrieval performance on three audio-text benchmarks, with particularly strong gains under scarce labels and small batch sizes.", "tldr": "", "keywords": ["Audio-Text Retrieval", "Cross-Modal Matching"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/404abf9bd0e9fd109a390e6ef33ec9681eb2c260.pdf", "supplementary_material": "/attachment/6ac469a117eeb99aa18e579f98a7926eb735d066.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes DART, a framework for cross-modal retrieval that addresses the limitations of purely instance-level alignment under small batch sizes and scarce labels. DART combines the conventional instance-level IOT objective with a novel feature-level regularization based on the UWD. It employs RAM to reweight feature channels based on statistical cues (correlation, variance, kurtosis), guiding the transport plan towards stable semantic dimensions and suppressing noisy ones. The framework demonstrates state-of-the-art results on audio-text benchmarks, and generalizing to image-text retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduced feature-level alignment using UWD is new. It tries to address the known weakness of traditional contrastive losses (and instance-level IOT) that treat all feature dimensions equally.\n2. DART achieves better performance compared to previous works.\n3. Experiments on image-text retrieval shows the generalization ability of the proposed DART."}, "weaknesses": {"value": "1. The writing of this paper is below ICLR's standard. For example, in Line 137 and 142, ``??'' apprear in the main text instead of the reference to equations, figures, or tables. In Eq. (6), x and y lack explanations.\n2. Sec. 2.3, limitations of instance-level IOT lacks of theoretical or experimental evidence to support the claims.\n3. Novelty is limited: The idea of learning feature importance (reweighting dimensions) is not new in cross-modal retrieval, as noted by the authors themselves when discussing Luong et al. (2024) . While DART's method uses cross-channel transport and richer statistics, the incremental novelty over existing per-channel weighting schemes is largely due to the expensive UWD machinery, which leads to a major concern about complexity.\n4. RAM is based on simple, first-order statistics (cross-modal correlation, variance, and kurt). These static, hand-designed proxies for ``semantic stability'' may be insufficient for highly complex, evolving feature spaces. The reliance on simple statistics makes the mechanism feel more like a heuristic than a deep, learned principle. The ablation also shows that RAM improves performance (Tab 1, DART w/ RAM vs w/o RAM) with limited improvements.\n5. Also, it would be interesting to see which part of RAM is most important.\n6. Lack of running time, memory comparison."}, "questions": {"value": "1. See weakness\n2. In eq. 10, how to calculate kurt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lt6ZCPjIrn", "forum": "cFhcd4WGjO", "replyto": "cFhcd4WGjO", "signatures": ["ICLR.cc/2026/Conference/Submission5930/Reviewer_BWQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5930/Reviewer_BWQA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904077049, "cdate": 1761904077049, "tmdate": 1762918359354, "mdate": 1762918359354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a dual-level alignment via robust transport (DART) method for audio-text retrieval. The proposed method has been explained in detail and experiments have been condudcted for evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This proposed method combines both instance-level alignment and feature-level regularization for cross-modal retrieval. Plenty of experiments have been condcuted to answer four key questions."}, "weaknesses": {"value": "The novelty of the proposed method needs more clarification. I don't think the introduction section gives a clear explanation on the connection between the proposed method and existing ones. Accroding to the Related Work section, channel-level considerations have been made in previous methods. Thus, the contribution of this paper would be not so significant if it only combines both instance and feature-level alignments. Besides, the paper writing still needs improvement. For example, there are ?? at line137/142."}, "questions": {"value": "According to Table 1, using Beats as audio encoder achieves better performance. Please explain why this configuration was not adopted in following experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FPDFyV6Lzw", "forum": "cFhcd4WGjO", "replyto": "cFhcd4WGjO", "signatures": ["ICLR.cc/2026/Conference/Submission5930/Reviewer_kiaK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5930/Reviewer_kiaK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914983618, "cdate": 1761914983618, "tmdate": 1762918358950, "mdate": 1762918358950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a dual-level optimal transport framework that integrates instance-level alignment with feature-level regularization via Unbalanced Wasserstein Distance and Reliability-Aware Marginals, achieving more robust and stable cross-modal (audio–text) retrieval under small-batch and noisy-label conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a novel and reasonable solution that incorporates instance-level inverse optimal transport and feature-level unbalanced Wasserstein regularization within a dual-level optimal transport framework.\n2. Introducing reliability-aware marginals (RAM) to reweight feature channels based on cross-modal statistics is intuitive and effective."}, "weaknesses": {"value": "1. Lack of Deeper Ablation for RAM Components: The existing ablation experiment table (1) only compares \"DART w/ RAM\" with \"DART w/o RAM\". I think more fine-grained ablation experiments should be provided. For example: (1) Use only corr, (2) Use only corr-var, (3) use other combinations (such as weighted sum). Without these experiments, we cannot determine if components like kurtosis are necessary or if the current combination is optimal.\n2. Computational scalability concerns: The feature-level OT module introduces a cost matrix of size *d×d*, leading to quadratic complexity in feature dimensionality.   The paper does not provide sufficient discussion on scalability to high-dimensional encoders such as CLIP or BEATs."}, "questions": {"value": "1. As mentioned in Weakness 1, the reliability score formula corr - var - kurt seems heuristic. Can the authors provide more theoretical or empirical evidence for choosing this specific combination?\n2. As noted in Weakness 2, the O(d^2) complexity is a potential threat. Have the authors considered methods to mitigate this issue in high-dimensional settings (e.g., d > 2048)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IuE2d14PoA", "forum": "cFhcd4WGjO", "replyto": "cFhcd4WGjO", "signatures": ["ICLR.cc/2026/Conference/Submission5930/Reviewer_LD5Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5930/Reviewer_LD5Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980351128, "cdate": 1761980351128, "tmdate": 1762918358686, "mdate": 1762918358686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DART, a framework that enhances cross-modal retrieval by combining instance-level alignment with feature-level regularization based on the Unbalanced Wasserstein Distance. By reweighting embedding channels according to cross-modal consistency, DART suppresses noisy dimensions and stabilizes learning under small batches and scarce labels."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The paper goes beyond conventional instance-level contrastive alignment by introducing a feature-level regularization mechanism based on UOT and IOT. This dual-level design elegantly captures both instance-wise and dimension-wise consistency, addressing the long-standing assumption that all embedding dimensions are equally informative.\n\n* The authors provide clear theoretical analysis showing how instance-level alignment objectives scale with the maximum distance among aligned pairs, while feature-level regularization scales with the Frobenius norm of the transport plan. This theoretical distinction explains DART’s robustness to noise and its improved generalization under small batch regimes."}, "weaknesses": {"value": "* The ablation study is poorly written so that it is not clear how each critical design choice is justified.\n\n* In the Introduction, the authors claim that ‘noisy channels tend to incur large transport costs.’ However, no empirical evidence is provided to support this claim, and it remains unclear whether the proposed feature alignment method effectively addresses this issue."}, "questions": {"value": "The ablation study needs a major revision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8lz0ZINB7d", "forum": "cFhcd4WGjO", "replyto": "cFhcd4WGjO", "signatures": ["ICLR.cc/2026/Conference/Submission5930/Reviewer_P5Cb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5930/Reviewer_P5Cb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062514343, "cdate": 1762062514343, "tmdate": 1762918358305, "mdate": 1762918358305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}