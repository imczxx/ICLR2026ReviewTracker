{"id": "ckAQ31T4Qv", "number": 2194, "cdate": 1757020326136, "mdate": 1759898163469, "content": {"title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", "abstract": "Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach.  As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each prompt. \n  By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-$n$ and majority voting. When applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-$n$, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux,  and TruthfulQA).\n  A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.", "tldr": "QAlign is a new test-time alignment approach that improves language model performance by using Markov chain Monte Carlo methods.", "keywords": ["Test-time alignment", "Quest", "Test-time compute", "Language models", "MCMC", "QAlign"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1de914c04fdbbdf11d12a82820cd82f77c638cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces QALIGN, a test-time alignment method for language models that leverages Markov Chain Monte Carlo (MCMC) sampling to approximate the “optimal aligned distribution” without finetuning model parameters. The method adapts the QUEST algorithm to use a reward model (RM) for preference-based acceptance–rejection sampling, allowing aligned text generation with closed or open-weight models. Experiments on mathematical reasoning (GSM8K, GSM-Symbolic) and general instruction-following (MMLU-Redux, TruthfulQA, IFEval, MATH500) demonstrate consistent gains over best-of-n (BoN), majority voting (MV), weighted MV (WMV), and even Direct Preference Optimization (DPO) under equivalent compute budgets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies an important limitation in current test-time search methods, over-optimization of imperfect RMs when scaling compute, and proposes a principled sampling-based alternative.\n\n- Results are shown across both task-specific and general alignment settings, demonstrating robustness to RM imperfections and outperforming multiple baselines."}, "weaknesses": {"value": "While the paper is clearly written and technically sound, several aspects could be clarified or strengthened to better support its claims:\n\n- The method can be viewed as a length-normalized, RM-guided extension of BoN using MCMC proposals from QUEST. While the MH-based acceptance criterion is well-formulated, it is not entirely clear that this constitutes a fundamentally new paradigm rather than a reparameterization of BoN/MBR with additional tuning. In addition, since QALIGN generates samples sequentially, it forgoes the parallel efficiency of BoN, which raises questions about its overall computational practicality.\n\n- The evaluation focuses on a single policy model (LLAMA-3.1-8B-Instruct) and one reward model (TÜLU3-8B-RM), leaving it uncertain whether the observed improvements would generalize across different architectures or RM designs.\n\n- The main comparison in Table 1 uses N = 1024 samples for both BoN and QALIGN, which is considerably higher than typical inference budgets (e.g., N = 16–32). Results under more practical compute settings would strengthen the study. Furthermore, the FLOPs-adjusted comparison with DPO is not fully equitable, as training FLOPs are a one-time cost whereas inference FLOPs recur for each query."}, "questions": {"value": "- A natural baseline, sampling multiple random indices and running BoN over them, is not tested, making it unclear whether MCMC brings tangible benefit.\n\n- Can the authors offer more details regarding FLOPs computation for inference-time and training-time methods and how do they align?\n\n- The related work section is relatively comprehensive but misses several works on inference-time alignment, such as:\n\n1. Fast Best-of-N Decoding via Speculative Rejection.\n\n2. Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback.\n\n3. TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling. \n\n4. Args: Alignment as reward-guided search.\n\n5. Inference-time language model alignment via integrated value guidance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "caqta4lZ96", "forum": "ckAQ31T4Qv", "replyto": "ckAQ31T4Qv", "signatures": ["ICLR.cc/2026/Conference/Submission2194/Reviewer_J4U9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2194/Reviewer_J4U9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761274158339, "cdate": 1761274158339, "tmdate": 1762916133182, "mdate": 1762916133182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a test-time alignment method for LLMs that improves performance without additional training or access to model logits. It does so by leveraging Markov chain Monte Carlo sampling to better explore and sample from the optimal aligned output distribution as computation increases, avoiding over-optimization issues seen in prior reward-model-based methods. The result is more accurate and well-aligned responses across reasoning and preference benchmarks, outperforming existing test-time compute and alignment baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper analyzes the language model sampling from a MCMC prospective, and the algorithm is novel.\n2. The algorithm is surprisingly simple and does not require accessing the models' logits, making it suitable for any LLM (open-sourced or commercial).\n3. The evaluation seems comprehensive."}, "weaknesses": {"value": "1. There seems error in derivation. First, in equation (7), when computing the ratio, the right hand side's numerator does not seem to be consistent with the definition of the proposal distribution. Also, it is not very clear how equation (8) is obtained, especially how everything is reduced to the length ratio. \n\n2. Unclear experiment setting: It is unclear how finetuning based method such as DPO and SFT is scaled during inference time. Also, can the author provide the number n for best-of-n? The Inference FLOPs is helpful, but number n is also commonly used. \n\n3. Lack of experimenting closed-source model. One claimed advantage of the paper is that it does not require accessing the logic. It would be very helpful if some results on closed-source model can be provided. \n\n4. This method requires selecting an index and complete the full response, for multiple times, which can be very expensive. Recently there are token-level reward model [1] that have shown to be efficient at guide a frozen LLM to generate aligned outputs. It would be helpful to also compare with such inference time alignment approach, which also seems to give the optimal distribution under the RL problem. \n\n[1] GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-Time Alignment, ICLR 2025."}, "questions": {"value": "1. Is there a reason to choose the specific proposal distribution by QUEST? Is this choice optimal in any sense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L7WQfTXSMX", "forum": "ckAQ31T4Qv", "replyto": "ckAQ31T4Qv", "signatures": ["ICLR.cc/2026/Conference/Submission2194/Reviewer_YAd4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2194/Reviewer_YAd4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636863444, "cdate": 1761636863444, "tmdate": 1762916133005, "mdate": 1762916133005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper *“SAMPLE, DON’T SEARCH: Rethinking Test-Time Alignment for Language Models”* proposes **QAlign**, a new test-time alignment method that leverages Markov chain Monte Carlo methods for text generation, improving efficiency–alignment trade-offs at high compute budgets, while maintaining the flexibility of inference-time alignment without modifying model parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Strong empirical performance in high-compute regimes:** QAlign consistently outperforms other alignment methods when sufficient inference-time compute is available.   \n- **Solid experimental validation:** The experiments include diverse prompts and datasets, providing a fair comparison across multiple baselines and compute settings."}, "weaknesses": {"value": "- **Compute intensity:** QAlign requires substantial inference-time compute to surpass baselines, limiting its practicality in typical deployment settings.  \n- **Limited theoretical justification:** Although the paper claims convergence to the “optimal aligned distribution,” this claim is not clearly seen to be proven. The paper would benefit from a clearer mathematical explanation or proof sketch illustrating how sampling asymptotically approximates the optimal aligned distribution.  \n- **Narrow advantage region:** In most compute-constrained regimes, traditional search-based or weighted-logit methods still outperform QAlign."}, "questions": {"value": "1. QAlign appears to require high compute budgets to outperform existing baselines. In practice, are such inference-time budgets realistic or allowed?  \n2. Under what specific conditions (model size, compute budget, or sampling strategy) would you recommend using QAlign over search-based alternatives?  \n3. Can you provide a formal or empirical justification for the claimed convergence of QAlign to the optimal aligned distribution for each prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QlQm09B5Wq", "forum": "ckAQ31T4Qv", "replyto": "ckAQ31T4Qv", "signatures": ["ICLR.cc/2026/Conference/Submission2194/Reviewer_PW39"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2194/Reviewer_PW39"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852591473, "cdate": 1761852591473, "tmdate": 1762916132664, "mdate": 1762916132664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper *“SAMPLE, DON’T SEARCH: Rethinking Test-Time Alignment for Language Models”* proposes **QAlign**, a new test-time alignment method that leverages Markov chain Monte Carlo methods for text generation, improving efficiency–alignment trade-offs at high compute budgets, while maintaining the flexibility of inference-time alignment without modifying model parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Strong empirical performance in high-compute regimes:** QAlign consistently outperforms other alignment methods when sufficient inference-time compute is available.   \n- **Solid experimental validation:** The experiments include diverse prompts and datasets, providing a fair comparison across multiple baselines and compute settings."}, "weaknesses": {"value": "- **Compute intensity:** QAlign requires substantial inference-time compute to surpass baselines, limiting its practicality in typical deployment settings.  \n- **Limited theoretical justification:** Although the paper claims convergence to the “optimal aligned distribution,” this claim is not clearly seen to be proven. The paper would benefit from a clearer mathematical explanation or proof sketch illustrating how sampling asymptotically approximates the optimal aligned distribution.  \n- **Narrow advantage region:** In most compute-constrained regimes, traditional search-based or weighted-logit methods still outperform QAlign."}, "questions": {"value": "1. QAlign appears to require high compute budgets to outperform existing baselines. In practice, are such inference-time budgets realistic or allowed?  \n2. Under what specific conditions (model size, compute budget, or sampling strategy) would you recommend using QAlign over search-based alternatives?  \n3. Can you provide a formal or empirical justification for the claimed convergence of QAlign to the optimal aligned distribution for each prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QlQm09B5Wq", "forum": "ckAQ31T4Qv", "replyto": "ckAQ31T4Qv", "signatures": ["ICLR.cc/2026/Conference/Submission2194/Reviewer_PW39"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2194/Reviewer_PW39"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852591473, "cdate": 1761852591473, "tmdate": 1763697732126, "mdate": 1763697732126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QALIGN, a novel test-time alignment method designed to address the over-optimization problem where existing methods, like Best-of-n (BoN), degrade as compute scales due to imperfect reward models (RMs). Instead of maximizing the RM, QALIGN uses a Markov chain Monte Carlo (MCMC) sampling approach, adapted from QUEST, to draw samples from the optimal aligned posterior distribution for a given prompt. This method notably requires no model finetuning or logit access, only the ability to sample from the base LM and query an RM. The final answer is selected from the resulting samples using Minimum Bayes Risk (MBR), which amounts to majority voting for tasks with discrete answers. The authors empirically demonstrate that QALIGN consistently improves with increased computation, outperforming BoN, majority voting (MV), and weighted majority voting (WMV) , and even surpasses the performance of the finetuned DPO model on a diverse suite of benchmarks when given a comparable inference budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength lies in its proposal of QALIGN, a novel and practical test-time alignment method that directly addresses the critical over-optimization problem where methods like Best-of-n (BoN) see performance degrade with increased compute. The method is well-motivated, and its technical approach—using MCMC sampling to approximate the optimal aligned posterior distribution—is elegant. The key advantage, which is well-supported by experiments, is that QALIGN's performance consistently improves as the compute budget scales, allowing it to avoid the performance degradation that plagues other methods. The empirical evaluation is strong, demonstrating that QALIGN not only outperforms other test-time methods (BoN, MV, WMV) but also surpasses the performance of a fully finetuned DPO model across a diverse suite of benchmarks when given a comparable inference budget."}, "weaknesses": {"value": "**Scope of Empirical Evaluation is Limited to \"Matched\" Model Pairs**: The paper does not test the robustness of QALIGN when the base Language Model (LM) and the Reward Model (RM) are \"mismatched.\" In the Task-Specific Tuning experiments (Sec 4.1), the base LM is LLAMA-3.1-8B-INSTRUCT, and the RM used is a custom model finetuned from that same LLAMA-3.1-8B-INSTRUCT model. In the General Alignment experiments (Sec 4.2), the base LM is TÜLU3-8B-SFT, which is paired with the TÜLU3-8B-RM. These models are from the same family and were explicitly chosen for their close relationship to allow for a fair comparison with the TÜLU3-8B-DPO model."}, "questions": {"value": "1. **Robustness to Mismatch**: Could the authors please comment on the robustness of QALIGN to \"mismatched\" model pairs? For instance, how would the method perform if the TÜLU3-8B-RM were used to align a different base model, such as a Llama 3.1 or Qwen 2.5 model?\n\n2. **RM Dependence**: How dependent is QALIGN's success on the RM being trained (or finetuned) on the specific output distribution of the base LM? Is it possible that the MCMC sampling process becomes inefficient or unstable if the RM's preferred distribution is too far from the base LM's proposals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QH78dxlPBJ", "forum": "ckAQ31T4Qv", "replyto": "ckAQ31T4Qv", "signatures": ["ICLR.cc/2026/Conference/Submission2194/Reviewer_WTu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2194/Reviewer_WTu9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993873785, "cdate": 1761993873785, "tmdate": 1762916132226, "mdate": 1762916132226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all the reviewers for their constructive and helpful feedback. \n    \nWe are glad that the reviewers found our approach to be **novel, principled, and practical test-time alignment method** that directly addresses over-optimization under imperfect reward models (WTu9, J4U9). They noted that it provides strong and scalable empirical performance (WTu9, PW39, J4U9). They stressed that the **approach is simple, elegant, and broadly applicable** (J4U9, YAd4) and, they agreed that the **experimental evaluation is thorough and robust**, spanning diverse prompts, datasets, and both task-specific and general-alignment settings (WTu9, PW39, YAd4, J4U9).\n\nWe believe that the straightforward revisions we will make in response to your feedback will significantly improve the quality and clarity of the paper. We will release the code to facilitate the reproducibility of our results upon acceptance. If we have succeeded in responding to your comments, kindly consider raising the scores. We are happy to address any more questions you might have.\n\nThank you,\n\nAuthors."}}, "id": "SzTk55ERP9", "forum": "ckAQ31T4Qv", "replyto": "ckAQ31T4Qv", "signatures": ["ICLR.cc/2026/Conference/Submission2194/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2194/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission2194/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763420524574, "cdate": 1763420524574, "tmdate": 1763420524574, "mdate": 1763420524574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}