{"id": "IdW0d0mRnG", "number": 7768, "cdate": 1758035445219, "mdate": 1763749479348, "content": {"title": "Asymptotic analysis of shallow and deep forgetting in replay with neural collapse", "abstract": "A persistent paradox in Continual Learning is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep (feature-space) and shallow (classifier-level) forgetting. We demonstrate that experience replay affects these two levels asymmetrically: while even minimal buffers anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting requires substantially larger buffers. To explain this, we extend the Neural Collapse framework to sequential training. We theoretically model deep forgetting as a geometric drift toward out-of-distribution subspaces, proving that replay guarantees asymptotic separability. In contrast, we show that shallow forgetting stems from an under-determined classifier optimization: the strong collapse of buffer data leads to rank-deficient covariances and inflated means, blinding the classifier to the true population boundaries. Our work unifies continual learning with OOD detection and challenges the reliance on large buffers, suggesting that explicitly correcting the statistical artifacts of Neural Collapse could unlock robust performance with minimal replay.", "tldr": "We analyze continual learning in the long-training limit, showing via Neural Collapse that replay preserves feature separability but causes head–feature misalignment, explaining why deep forgetting is mitigated while shallow forgetting persists.", "keywords": ["continual larning", "neural collapse", "deep learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/402160be52cdbe650e85409a688db9e2d13287c0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author developed an asymptotic framework to analyze feature geometry with and without replay buffers. They demonstrate that replay can reliably alleviate deep forgetting (loss of feature separability), but does not alleviate shallow forgetting (misalignment between classifier weights and features). This work extends NC theory to multi head CL settings, characterizes the effects of buffer size and weight decay, and establishes a theoretical connection between CL and OOD. The empirical results of CIFAR100, Tiny ImageNet, and CUB-200 validate the theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Extends Neural Collapse analysis to continual learning and multi-head architectures—an unexplored direction. Uses asymptotic analysis and connects NC with OOD theory in a rigorous manner. Establishes a bridge between NC, CL, and OOD detection, enriching all three research domains."}, "weaknesses": {"value": "While conceptually strong, it provides limited actionable guidance for improving CL performance."}, "questions": {"value": "Why do we focus on discussing Multi Head Models? Is this model commonly used in modern continuous learning and multi task learning? Do you analyze whether the purpose of this model is to increase workload or has practical significance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UuQqJb7DAj", "forum": "IdW0d0mRnG", "replyto": "IdW0d0mRnG", "signatures": ["ICLR.cc/2026/Conference/Submission7768/Reviewer_hg1i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7768/Reviewer_hg1i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893550057, "cdate": 1761893550057, "tmdate": 1762919809759, "mdate": 1762919809759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an asymptotic analysis of replay-based continual learning under the neural collapse phenomenon. The authors study how replay buffers affect shallow and deep forgetting. Empirically, they find that replay-based continual learning effectively mitigates deep forgetting but still suffers from shallow forgetting even when the replay buffers are large. They then use Neural Collapse theory to analyze the limiting geometry of features and heads in three continual learning setups. They also identify a connection between continual learning and OOD detection, showing that under weight decay, the distribution of OOD inputs converges to a degenerate null distribution. They also show the effect of replay in their framework, demonstrating that deep forgetting is not mitigated by the model with small replay buffer because of the approximation error when using the buffer distribution to approximate the true class distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work proposes a novel framework for replay-based continual learning. The results and implications are meaningful and helpful to the community. \n2. The authors provide a sufficient and comprehensive theoretical analysis for replay-based continual learning, considering three different setups and showing the effect of replay.\n3. The empirical study is consistent with the theoretical findings, across both real-world and simulated datasets.\n4. The work is well structured and easy to follow for the readers."}, "weaknesses": {"value": "1. In Theorems 1, 2, and 3, it seems that $\\nu = 1 - \\eta \\lambda$ is required to be non-negative or greater than $-1$, but I do not find any explicit condition on $\\nu$.\n\n2. The explanation of why replay cannot strongly mitigate shallow forgetting is not convincing to me. The authors argue that the approximation error is the key reason, but there is no formal result to support this claim, which limits the contribution of this paper.\n \n3. In the experimental results, the authors only vary the buffer size from $0\\\\%$ to $10\\\\%$. This seems insufficient to support their theoretical findings.\n\n4. There are some typos and inconsistencies:\n   1. Lines 139, 141. Two citations are missing.\n   2. Line 315, \"Theorem 6\" should be \"Theorem 2.\"\n   3. Line 987, \"class-il\", \"domain-il\", and \"task-il\" should be written as \"CIL,\" \"DIL,\" and \"TIL,\" consistent with other figures. Moreover, the task indices in Figure 12 should be positive integers.\n\n5. The term “balanced replay” is unclear. I think “balanced replay” refers to the replay buffer being sampled in a balanced manner from the training set, rather than the buffer size being equal to the size of the training data."}, "questions": {"value": "1. How to understand the theoretical results when $\\eta \\lambda \\geq 2$?\n2. Why are experiments conducted only for buffer sizes varying from $0\\\\%$ to $10\\\\%$?\n3. What is the precise meaning of \"balanced replay\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fy5cZsyJo4", "forum": "IdW0d0mRnG", "replyto": "IdW0d0mRnG", "signatures": ["ICLR.cc/2026/Conference/Submission7768/Reviewer_wGRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7768/Reviewer_wGRs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920857812, "cdate": 1761920857812, "tmdate": 1762919809377, "mdate": 1762919809377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes how the replay buffer in continual learning scenarios influences model forgetting, distinguishing between shallow and deep forgetting. It further investigates these phenomena within the Neural Collapse framework, examining the geometric structure of the feature space and supporting the analysis with empirical results."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and clearly organized, with a pleasant and coherent flow of discussion. The topic addressed is novel and engaging. Moreover, the theoretical analysis is insightful and clearly explained."}, "weaknesses": {"value": "While the theoretical discussion is sound and convincing, I have some concerns regarding the empirical analysis. First, it is unclear why the authors chose ResNet and ViT as reference models. It seems that the selected architectures could significantly influence the observed behaviors and results. If this is the case, the authors should explicitly discuss this aspect. Otherwise, a justification of why the chosen architectures do not affect the outcomes should be provided.  Along the same lines, the rationale behind considering both pretrained and from-scratch models is not entirely clear. In the case of pretrained models, it would be important to explain how the initialization was adapted to the continual learning setting, as mentioned in Section 1.1. Additionally, the discussion in Section 3.3.2 highlights the effect of weight decay, but the influence of other hyperparameters and architectural choices remains unexplored. Given their potential impact, especially in the context of deep forgetting, this omission seems non-negligible. The authors should include a discussion addressing this point to provide a more comprehensive understanding of the empirical results. Another aspect that would benefit from clarification is the adoption of the Neural Collapse (NC) framework. The authors should briefly discuss possible alternative frameworks and justify the choice of NC in this context.\nFinally, the discussion on the distinction between multi-head and single-head settings could be improved by adding a short introductory explanation earlier in the paper to help readers unfamiliar with these concepts. In addition, Section 3 contains some citation issues, where “?” symbols appear instead of proper references, and these should be corrected."}, "questions": {"value": "- Could the authors clarify the rationale behind choosing ResNet and ViT as reference architectures, and discuss how this choice might influence the observed behaviors and results?\n\n- How were pretrained models adapted to the continual learning setting, and what motivated the comparison between pretrained and from-scratch training approaches?\n\n- Beyond weight decay, have the authors examined the influence of other hyperparameters or architectural choices on the empirical results, particularly in relation to deep forgetting?\n\n- What motivated the adoption of the Neural Collapse framework, and could the authors discuss potential alternative frameworks or justify why NC is particularly suitable for this analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fHPE64IK8v", "forum": "IdW0d0mRnG", "replyto": "IdW0d0mRnG", "signatures": ["ICLR.cc/2026/Conference/Submission7768/Reviewer_5K6E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7768/Reviewer_5K6E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941230558, "cdate": 1761941230558, "tmdate": 1762919808652, "mdate": 1762919808652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We thank the reviewers for their constructive feedback. We have revised the manuscript to address the specific points raised in the reviews, alongside additional refinements developed since the initial submission. We believe these revisions have substantially improved the paper, and we invite you to examine the updated text. To assist in this process, we have outlined the major changes below, which should also respond to the main questions raised in the reviews: \n\n\n* **Formalized Mechanism for the Deep–Shallow Gap:** We have updated the theoretical explanation for the persistence of shallow forgetting. The revised analysis grounds this phenomenon in the *rank deficiency* of the buffer covariance matrix, which renders the decision boundary *under-determined*. This mechanism is now illustrated in Figure 1, with explicit measurements of covariance rank deficiency provided in Figure 6.\n* **Structural & Notational Refinements:** The manuscript structure has been reorganized for improved flow. Key changes include:\n    * **Formalized Hypotheses:** The two central hypotheses underpinning the theoretical model are now explicitly isolated within the text.\n    * **Expanded Preliminaries:** Background sections now include a formal definition of \"replay,\" clear notation for NC quantities, and a distinction between *population statistics* (full data) and *observed statistics* (buffer data).\n    * **Clarifications:** Added concise explanations for \"balanced sampling\" and \"multi-head\" architectures.\n* **Full Replay Baseline:** All relevant plots now include a baseline corresponding to *100% buffer size* (full training set) to visualize the asymptotic convergence point of the replay curves.\n* **Expanded Experimental Scope:** We have added several new analyses and ablations:\n    * **Pre-training vs. Scratch:** (Section A.3.1) We analyze convergence to NC in pre-trained models, observing significantly faster collapse compared to models trained from scratch.\n    * **Alternative Replay Algorithms:**  (Section A.4)  We demonstrate the evolution of deep–shallow forgetting curves under other prominent replay strategies.\n    * **Class Mean Norms:**  (Section A.4)  now details the evolution of class mean norms as a function of weight decay and buffer size.\n    * **Head Initialization:**  (Section A.3.3) We find that while head initialization affects the temporal growth of feature norms, it does not substantially alter forgetting outcomes.\n    * **Low Feature-Dimension Regime ($d < K$):**  (Section A.3.2)  We investigated the scenario where the feature dimension is smaller than the number of classes. We observe that while the rigid Simplex ETF structure ($\\mathcal{NC}2$) degrades in this regime—consistent with an expected geometric shift—the **deep–shallow forgetting gap persists**. This confirms the gap is not contingent on a specific ETF geometry.\n* **Visual Enhancements:** All figures have been updated for better readability, and theoretical results are now highlighted for easier navigation.\n\nWe remain fully available during the discussion period and are eager to engage with any further feedback. We welcome any additional ideas or suggestions for experiments that could further strengthen the paper and are happy to discuss them."}}, "id": "907jti7dXc", "forum": "IdW0d0mRnG", "replyto": "IdW0d0mRnG", "signatures": ["ICLR.cc/2026/Conference/Submission7768/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7768/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7768/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763749400471, "cdate": 1763749400471, "tmdate": 1763749400471, "mdate": 1763749400471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}