{"id": "0xmvlDKDTL", "number": 9762, "cdate": 1758138892339, "mdate": 1759897699918, "content": {"title": "Learning Composable Chains-of-Thought", "abstract": "A common approach for teaching large language models (LLMs) to reason is to train on chains-of-thought (CoTs) of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest. We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: they should combine atomic reasoning skills to solve harder unseen tasks. In this paper, we introduce a method to enable generalization to a target compositional task that has no labeled CoT data. We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable leads to improvement. Specifically, we augment our data by adding prefixes to CoTs, making sequences of CoTs in-distribution for the trained model. We train individual models on the atomic tasks with composable CoT data and combine them with multitask learning or model merging to address the target compositional task zero-shot. This model can be further trained on a small amount of compositional data using rejection sampling fine-tuning (RFT). Results on three domains of compositional tasks, natural language skills, string manipulation, and arithmetic, show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget.", "tldr": "", "keywords": ["Large Language Models", "Compositional Reasoning", "Generalization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19f5daffe89d264f82f19881aba7e8e6a1578f4c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a method for enabling LLMs to generalize to compositional reasoning tasks without using compositional chain-of-thought (CoT) data. By augmenting CoTs of atomic tasks with composable prefixes, the model learns to combine reasoning steps across tasks. Experiments across natural language, string manipulation, and arithmetic show that training on Composable CoT data outperforms multitask and fine-tuning baselines under the same data budget."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This study conducts an interesting and important research topic: generalization to compositional reasoning tasks by using only atomic reasoning data at training.\n- The experiments demonstrate the effectiveness of the proposed method though they are toy experiments.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- I think we need an additional ablation study about why the simple trick (adding just random tags) works. For example, which is more important, tag or random text? what if we remove or change the tag? what if we change the style of random text?"}, "questions": {"value": "- Table1: Does the result not apply RFT(rejection sampling fine-tuning)? I think there is no such description, but based on the context I feel so."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dzRgprZgvg", "forum": "0xmvlDKDTL", "replyto": "0xmvlDKDTL", "signatures": ["ICLR.cc/2026/Conference/Submission9762/Reviewer_ttWX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9762/Reviewer_ttWX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608625000, "cdate": 1761608625000, "tmdate": 1762921254013, "mdate": 1762921254013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data augmentation method that adds random prefixes to atomic CoT steps to improve compositional generalization. The idea is evaluated on synthetic reasoning tasks such as string manipulation, arithmetic, and Skill-Mix."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple and shows significant improvements on synthetic tasks."}, "weaknesses": {"value": "1. The paper only conducts experiments on synthetic tasks. It is unclear how the proposed method can be applied to real-world scenarios such as math reasoning or code generation. In particular, identifying the atomic tasks in these domains is non-trivial. I also doubt whether simply data augmentation without explicitly training the model for composition can lead to meaningful improvements on realistic tasks.\n2. I am uncertain about the broader impact of this work. To advance the frontier of model's reasoning capabilities, the current standard practice is to use reinforcement learning. When compute is limited,  distillation from stronger LLMs also achieve good result. I do not see clear evidence that the proposed method would bring better results on real-world tasks compared to these existing approaches. It would be helpful if the authors could discuss how their idea might be applied in more realistic training setups or combined with current methods.\n3. The discussion of related work is insufficient. The authors should at least include: (1) recent methods for training models to generate long CoTs, such as distillation , self-training [1], or reinforcement learning[2]; and (2) broader discussions on understanding and improving compositional generalization in LLMs.\n\n[1] Zelikman et al. STaR: Bootstrapping Reasoning With Reasoning\n\n[2] Guo et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"}, "questions": {"value": "1. Why is the performance of SFT with compositional supervision so low on the first three tasks (Table 1)? It would be helpful if the authors could provide more details on the setup of this baseline, including examples of training samples.\n2. It is somewhat surprising that augmenting data with random prefixes improves model performance, given the significant distribution shift between training and testing. Do the authors have an explanation for that? In addition, I am curious whether such training leads to a larger degradation on other capabilities (e.g., instruction following) compared to other baselines.\n3. For ComposableCoT-Merge, how is the scaling factor determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KfDO0u5rMO", "forum": "0xmvlDKDTL", "replyto": "0xmvlDKDTL", "signatures": ["ICLR.cc/2026/Conference/Submission9762/Reviewer_2jkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9762/Reviewer_2jkA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977821766, "cdate": 1761977821766, "tmdate": 1762921253687, "mdate": 1762921253687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claims a way to make chain-of-thought reasoning \"composable\": train on atomic tasks, add random \"proxy prefix\" strings in front of the CoT, tag them, then fine-tune the LLM so it learns to generate composable CoTs conditioned on the prefix. At test-time, there is a  concatenate strategy at the task level (via multitask learning or model-merging). On simple synthetic tasks: string ops, ASCII arithmetic, toy natural-language skills, the authors show often big improvements over the non-composable CoT strategy but only at the level of 2 compositions. Improvements wrt control baselines for 3 compositions is also observed, also to a lesser extent"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of compositional generalization is a core ML problem and I appreciate the authors trying to address it in the modern setting: standard LLMs have been shown to be incapable of large scale compositional reasoning. This approach is interesting and tries to leverage CoT that has been shown to help with logical/math reasoning for compositional generalization. \n2. Well written and motivated empirically.\n3. Strong results of the proposed approach over baselines are interesting to see, though notably janky at many places especially table 1 llama 2."}, "weaknesses": {"value": "1. The main weakness of this paper is that it doesn't experiment with compositionality enough. The tasks are also not compositional enough and there is a risk of templating/pattern-matching hacking going on here. Symbolic manipulation of some task with quantifiable controllable compositionality (e.g. n digit multiplication. Multiplication of n digits k times) would be interesting to see. 2 way compositional results are interesting to study as a starter but you should not stop at 3 way compositions. What about k-way compositions? I already see the improvement delta declining at 3 compositions.\n2. Results for small models have high variance. Table 1 llama and qwen 7b have a huge performance delta. The benefits of the proposed approach are unclear, especially in light of 1.\n3. Results on larger models are missing. Presumably compositional CoTs should yield even better performance deltas for larger models e.g 30b, 70b?\n4. Proxy prefixes can also be interpreted as random noise and this approach feels rather like prompt regularization. The experimental results again don't investigate the compositional utility of the approach.\n5. Just using thinking tokens ala., Deepseek R1 distillation, and seeing performance effects is unexplored. This is also a clear baseline."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s9VRPIDcFy", "forum": "0xmvlDKDTL", "replyto": "0xmvlDKDTL", "signatures": ["ICLR.cc/2026/Conference/Submission9762/Reviewer_KsW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9762/Reviewer_KsW6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013227148, "cdate": 1762013227148, "tmdate": 1762921253455, "mdate": 1762921253455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Composable CoT, a lightweight data augmentation that wraps atomic CoT traces with tags and inserts “proxy prefixes” (random-letter strings) so that training examples look like “a CoT conditioned on previous CoTs.” At test time, atomic CoT models are combined either via multitask learning (MTL) or Task Arithmetic–style model merging; limited compositional supervision is optionally added through rejection-sampling fine‑tuning (RFT). On synthetic string/arithmetic compositions and on Skill‑Mix (literary+rhetorical skills), Composable CoT improves zero‑shot and small‑shot composition over standard CoT training, with the largest reported gains coming from RFT on top of the composable formatting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Turning atomic CoT data into a “composable” format is easy to implement (two tags + random‑letter prefixes) and consistently lifts zero‑shot/limited‑shot composition across tasks and two 7B bases. The construction is clearly depicted (Fig. 2), and ablations show random letters are the most robust proxy prefix out‑of‑domain."}, "weaknesses": {"value": "1. \"Zero-shot\" claim is fragile due to validation-time merging sweeps. For Task Arithmetic, the paper sweeps $\\alpha, \\beta$ on a validation set for each task (App. G.4). If this validation set is the compositional task, tuning leaks target supervision into model selection and weakens the zero-shot claim; at minimum this needs to be clarified and a version without compositional validation should be reported.\n2. Heavy reliance on explicit tags and random prefixes; external validity is limited. The method depends on <prefix>/<suffix> markers and training on random-letter prefixes. This encourages learning a format protocol rather than discovering composition in untagged, natural inputs. The paper itself instantiates all intermediate tags as <prefix> and the final as <suffix> (Instantiation of Tags), underscoring the dependence on explicit scaffolding. Results without any tags or with naturally occurring preceding CoTs are not shown.\n3. Evidence of reading and re-using intermediate results is weak. The \"quality\" analysis checks whether both atomic CoT templates appear, not whether the model actually consumes the earlier step's outputs (step-level causal dependence or variable-passing). This leaves open the possibility that the model just regenerates both CoTs in the suffix.\n4. Many findings are unsurprising and the novelty is modest. The main lift plausibly comes from making multi‑CoT sequences in‑distribution via formatting. The work’s practical impact beyond controlled compositions may be limited without evidence on untagged, naturally compositional tasks (e.g., program‑of‑thought planning, tool‑use pipelines)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vs31IXBjQ5", "forum": "0xmvlDKDTL", "replyto": "0xmvlDKDTL", "signatures": ["ICLR.cc/2026/Conference/Submission9762/Reviewer_VYUV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9762/Reviewer_VYUV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762057754484, "cdate": 1762057754484, "tmdate": 1762921253151, "mdate": 1762921253151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}