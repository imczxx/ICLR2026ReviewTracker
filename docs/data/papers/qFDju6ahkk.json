{"id": "qFDju6ahkk", "number": 2276, "cdate": 1757046755357, "mdate": 1759898158704, "content": {"title": "Empowering Small VLMs to Think with Dynamic Memorization and Exploration", "abstract": "Small-scale Vision-Language Models (SVLMs) are exceptionally well-suited for proprietary tasks. Equipping them with thinking capabilities is a critical step to enhance their performance and reliability in these specific domains. However, existing training paradigms, including Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding the capacity of SVLMs. Consequently, directly applying these paradigms to SVLMs fails to instill the desired thinking abilities. A natural solution is to combine SFT and RLVR, leveraging their complementarity to reduce the dependence on model capacity. Yet the core challenge lies in managing the inherent trade-off: excessive reliance on SFT can force the model to memorize pseudo thinking traces, while over-emphasizing RLVR can lead to unstable exploration (*i.e.,* advantage collapse). To address this, we propose *DyME*, a novel training paradigm that **Dy**namically selects between **M**emorization (via SFT) and **E**xploration (via RLVR) at each optimization step, ensuring that every update contributes to the trade-off. Extensive experiments across diverse domains demonstrate that *DyME* consistently achieves this balance, and thus delivers substantial performance improvements on specialized tasks. These results establish *DyME* as a practical and effective solution for empowering SVLMs with reliable thinking capabilities.", "tldr": "", "keywords": ["GRPO", "SFT", "Small-scale VLM", "Computer Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ded8d9507716904d4b7ec3f8526014b1cbdcfc5d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles a critical and practical challenge: enabling reasoning (“thinking”) in Small-scale Vision-Language Models (SVLMs). The authors argue that existing training paradigms for Large VLMs—Supervised Fine-Tuning (SFT) on CoT data and Reinforcement Learning with Verifiable Reward (RLVR)—are ill-suited for SVLMs. SFT overwhelms small models, harming visual grounding, while RLVR often collapses due to poor instruction-following and unstable training.\nTo address this, the authors propose DyME (Dynamic Memorize–Explore), a novel training paradigm that dynamically switches between SFT and RLVR at each optimization step:\n- Dynamic Switching: If at least one of the multiple generated responses is correct, DyME enters Exploration mode (RLVR) to encourage diverse reasoning. If all fails, it reverts to Memorization mode (SFT) to learn from ground-truth traces.\n- Complementary Supervision: A visual checker rewards visually grounded reasoning, while a visual refiner enhances SFT targets using successful exploration traces.\nExperiments on three domains—medical VQA, chart understanding, and geometry reasoning—show that DyME yields substantial and consistent gains across multiple SVLMs, often matching or surpassing larger models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely and Meaningful Problem: Focuses on reasoning for small, efficient VLMs—highly relevant for real-world deployment on edge devices.\n2. Elegant and Effective Approach: The dynamic “memorize–explore” mechanism intuitively balances stability and exploration, well-suited to SVLM limitations.\n3. Strong Experimental Evidence:\n  - Baselines clearly show SFT/RLVR failures, motivating DyME.\n  - Consistent, significant improvements across all domains.\n  - Ablation studies confirm each component’s necessity and synergy.\n4. Excellent Clarity: The paper is clearly written, with strong visuals (notably Fig. 1) and a logical, accessible presentation."}, "weaknesses": {"value": "1. Reliance on External LLM: The visual checker and refiner rely on a large external model (Qwen2.5-14B), introducing extra complexity, cost, and dependency. This makes performance partly contingent on the external LLM’s capability, slightly undermining the goal of a self-contained small-model framework.\n2. Rigid Switching Heuristic: The binary rule (“if one correct → RLVR, else → SFT”) may cause abrupt shifts; a softer, reward-based switch could yield smoother training.\n3. Limited Task Generality: The method requires domain-specific Visual Fact extraction, which may hinder scalability to new, open-ended tasks."}, "questions": {"value": "1. External LLM Sensitivity: How would performance change if smaller or open-source models were used for the visual checker/refiner?\n2. Training Overhead: What is the computational and time cost of DyME relative to standard SFT and RLVR?\n3. Effect of K: How does the number of generated responses per step (K) affect stability, performance, and cost?\n4. Visual Fact Extraction: For novel tasks not covered in the paper (e.g., complex scene understanding  or physical reasoning), what is the anticipated process for extracting the Visual Facts? Does this step require significant manual design, or can it be automated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sVfH0s7hrW", "forum": "qFDju6ahkk", "replyto": "qFDju6ahkk", "signatures": ["ICLR.cc/2026/Conference/Submission2276/Reviewer_mKuR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2276/Reviewer_mKuR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959649509, "cdate": 1761959649509, "tmdate": 1762916170894, "mdate": 1762916170894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of instilling reasoning (\"thinking\") capabilities in SVLMs. SFT overwhelms the models' limited capacity, leading to \"pseudo thinking traces,\" while RLHR fails due to poor instruction adherence, causing \"advantage collapse.\"\n\nThus, the authors propose DyME (Dynamic Memorize-Explore), a novel training paradigm. DyME dynamically switches between an exploration (RLVR) mode and a memorization (SFT) mode at each optimization step. The switch is governed by a simple rule: if the model fails to produce any correct response in a batch, it falls back to SFT mode. If at least one response is correct, it uses RLVR mode.\n\nFurthermore, DyME introduces a \"vision supervision\" mechanism, composed of a \"Visual Checker\" and \"Visual Refiner\", which use an external LLM to (1) provide a more nuanced \"thinking reward\" for the RLVR mode and (2) dynamically refine the SFT ground-truth targets to be more visually grounded and consistent with successful exploration traces. \n\nExperiments across three domains show that DyME substantially improves the performance of SVLMs, whereas the SFT and RLVR baselines are shown to degrade performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a practical and important problem of enabling complex reasoning on small, efficient models.\n\n2. The paper is generally well-written, and the problem is clearly motivated. Figure 1, in particular, provides a good illustration of why existing paradigms might fail on SVLMs and how DyME tries to solve this.\n\n3. Strong empirical results. The authors show in Tab. 1 that standard SFT, RLVR, and a two-stage approach degrade the performance of SVLMs on these tasks, while DyME consistently provides significant gains.\n\n4. Effective ablation study. The ablation in Table 2 does a good job of validating the key components of DyME. It shows that removing either the memorization or exploration mode is catastrophic, supporting the need for a hybrid approach. It also demonstrates the effectiveness of the visual supervision modules."}, "weaknesses": {"value": "1. The \"vision supervision\" modules (Visual Checker and Refiner) involves additional dependencies. The modules are critical to the method's performance (as shown in Tab. 2), but they are implemented via prompting an external Qwen2.5-14B. This involves additional knowledges and causes unfair comparisons.\n\n2. Question regarding novelty of this work. Compared with existing hybrid SFT+RL methods, the authors claim the main novelty is the dynamic switching criterion. However, the criterion used (fall back to SFT if all $K$ samples are incorrect) is a relatively simple and heuristic. The paper note that prior hybrid methods are \"static,\" but this might be an oversimplification. A deeper comparison to other dynamic weighting schemes (e.g., PPO-SFT hybrids) may be helpful to fully position the novelty of this specific heuristic.\n\n3. The fact that all baselines (SFT, GRPO, Two-stage) fail so catastrophically (e.g., dropping average performance from 49.9 to 44.1 or 44.0 for SmolVLM) is surprising. While I understand the  limited capacity of SVLMs can lead to underperforming results, I wonder if this could be caused by under-optimized tuning. For example, SFT's failure is attributed to overwhelming the modell, but could a simpler baseline (e.g., SFT on far less data or shorter CoT) have been more effective? I would appreciate any deeper discussion into this concern.\n\n4. Clarity of \"Vision Supervision\" and lack of details. While the ablation shows the visual checker/refiner are important, their description in the main paper is high-level. More detail on the prompts, the failure modes of this LLM-based pipeline, and its reliability would be \nnecessary to make the method truly reproducible.\n\n5. The visual refiner/checker requires additional inference of a LVLM during the training of a LVLM. This seems time consuming and may harm the effectiveness of the proposed training pipeline. Could you quantify the computational cost of this pipeline? How critical is the choice of Qwen2.5-14B? What happens if a weaker/stronger model is used for the checker and refiner? Does the method still work?\n\n6. More investigation into the switching criterion. The binary switch (all fail vs. $\\ge 1$ success) is simple and effective. Did you experiment with other criteria (e.g., switching if the average reward of the batch is below a threshold, or using a \"budget\" for SFT steps) that might be more robust?\n\n7. It seems the authors omit the visual checker/refiner in the abstract. To fully reflect the contribution of this paper, they may consider adding this part into the text."}, "questions": {"value": "Please see the comments above regarding the weaknesses. I have written how each concern can be discussed and addressed in the rebuttal/revision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vSvJYUOvx6", "forum": "qFDju6ahkk", "replyto": "qFDju6ahkk", "signatures": ["ICLR.cc/2026/Conference/Submission2276/Reviewer_6oCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2276/Reviewer_6oCC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968456454, "cdate": 1761968456454, "tmdate": 1762916170754, "mdate": 1762916170754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new dynamic training paradigm that can switch between the supervised finetuning and the group relative policy optimization for a vision-language model. Specifically, the paper first  asks the small vision language model to generate the responses. When at least one is correct, the model will choose GRPO, otherwise it will use the SFT-based model. For GRPO reward, apart from the binary correctness, the paper also introduces a visual checker to evaluate the image grounding. The proposed method is evaluated on SLAKE, ChartQA, and MathVerse. The paper uses several VLM baselines to show its performance. The experiment results show that the small model can achieve comparable or even better performance on multiple tasks, when comparing with LVLMs. The experiment also includes ablation study,"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed new training procedures seem to be simple yet effective. DyME can be applied to SVLM and can achieve significant performance gains across different domains. The method can also reduce the advantage collapse and constrained exploration.\n2. The experiment is comprehensive. The paper compares the proposed training strategy with two-stage, GRPO, and SFT on three different models with 0.5-1B parameters. The ablation study shows the importance of the proposed training strategy and visual rewards.\n3. The paper provides additional training details and examples in the appendix. The illustrative figures help readers to understand the paper better."}, "weaknesses": {"value": "1. Some baselines are pretty old. The paper needs to include some newer LVLMs such as QWen-2.5VL, etc. The experiment section is also purely a quantitative evaluation. Some qualitative evaluation or human evaluation can help readers to understand the quality of the chain better. For example, the length of COT after using DyME compared to the two-stage. Adding additional experiments, such as pure textual or pure vision tasks, can help readers understand the performance gain better. The current evaluation focus on the VQA tasks, which are a bit limited.\n2. Some parts of the paper are not clearly written. For example, why use Geo170k for training but evaluate on MathVerse? The paper also mentioned the chartqa used relaxed correctness. What is the approximation used for the evaluation?\n3. The paper fails to show any code and model, making it hard for readers to reproduce results. The paper did not include reproducibility statement. The paper fails to include a use of LLMs section."}, "questions": {"value": "What is the performance gain of SVLM on pure text tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "10e0Yg56Pj", "forum": "qFDju6ahkk", "replyto": "qFDju6ahkk", "signatures": ["ICLR.cc/2026/Conference/Submission2276/Reviewer_y5pj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2276/Reviewer_y5pj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973998643, "cdate": 1761973998643, "tmdate": 1762916170303, "mdate": 1762916170303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}