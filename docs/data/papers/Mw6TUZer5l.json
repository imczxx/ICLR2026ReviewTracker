{"id": "Mw6TUZer5l", "number": 19975, "cdate": 1758301137236, "mdate": 1759897009198, "content": {"title": "The Surprising Soupability of Documents in State Space Models", "abstract": "We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled, via simple operations like averaging, into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We demonstrate that finetuned Mamba2 models with souped representations achieve competitive or superior performance across multi-hop QA, sparse retrieval, and long-document reasoning tasks compared to the standard monolithic encoding approach. For example, on the RACE and QuALITY benchmarks for long document question answering, our method substantially outperforms a traditional concatenation approach. Crucially, this modular design scales to hundreds of documents---we test up to 256---while delivering substantial savings in inference cost, unlocking new possibilities for large-scale corpus reasoning.", "tldr": "We introduce a distributed document processing framework, which merges independently computed document hidden states from fine-tuned Mamba models, enabling efficient inference across corpora.", "keywords": ["State Space Models", "Question-answering", "Long-context Reading Comprehension"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/faf44ffda29efec46dd1207c5b6eae0c6be98d3c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies whether hidden states computed from multiple different documents can be composed post hoc, for downstream tasks such as QA and long-document reasoning, in SSMs (in particular Mamba-2). Towards this end, they propose the \"souping\" of document states, which are encoded independently, via commutative operators like averaging. The paper finds that a zero-shot approach is ineffective, and that \"finetuning is crucial for unlocking soupability in pretrained SSMs\"."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper provides extensive experiments, across a variety of tasks, and scaling up to composing 256 documents.\n\nThe paper also studies how different fine-tuning approaches have varying degree of effects on the composability of the resulting states."}, "weaknesses": {"value": "The paper's central investigation into 'document souping' is presented without acknowledging that this method was already introduced and evaluated as a baseline in [1], published in ICLR 2025. This omission is rather severe, as it overlooks the most direct prior work.\n\nIn particular, both papers are motivated by the exact same problem: the inefficiency of the monolithic, concatenation-based context processing in SSMs.  Their proposed solutions are conceptually almost identical -- (1) pre-processing individual documents by encoding them as fixed-size states, and (2) composing them in a commutative manner at inference time. Furthermore, the method proposed in [1] appears in fact superior, being theoretically-grounded and working better in both zero-shot and fine-tuned settings.\n\nTo elaborate, the core method of this work, which it terms \"document souping\" with simple averaging as the most effective operator, appears to be functionally identical to the \"Soup\" method that was not only studied in an even earlier paper [2] published on arXiv, but also explicitly implemented and evaluated as a **baseline** in [1]. What is even more concerning is that even though [2] is cited in this work, the fact that the proposed method here is identical appears heavily downplayed, with the only mention being:\n\n> (Pi√≥ro et al., 2024) linearly combines task-specific hidden states for skill transfer. In contrast, our\nmethod focuses on merging hidden states from disjoint document chunks, enabling compositional\nreasoning across distributed corpora through simple aggregation strategies.\n\nConsequently I strongly believe that this work not only fails to frame its contributions appropriately in the context of prior art, but also as a result is limited in its novelty and contributions beyond an empirical deep dive into an existing baseline.\n\n\n[1] PICASO: Permutation-Invariant Context Composition with State Space Models. ICLR 2025.\n\n[2] State Soup: In-Context Skill Learning, Retrieval and Mixing. arXiv 2024."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XmTXWvpj11", "forum": "Mw6TUZer5l", "replyto": "Mw6TUZer5l", "signatures": ["ICLR.cc/2026/Conference/Submission19975/Reviewer_Ycg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19975/Reviewer_Ycg4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760916990364, "cdate": 1760916990364, "tmdate": 1762932863083, "mdate": 1762932863083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a problem of non-reusable representations of contextual corpora as a single fixed-length hidden state in the structured SSM. The proposed approach inspired by the model souping technique, allows the efficient procedure of encoding of contextual documents into reusable representations that can be pooled together for SSM reasoning. The experimental evaluations on multi-document and single long-document QA demonstrate that finetuned Mamba2 models with souped representations achieve competitive or superior performance compared to traditional concatenation-based encoding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated and clearly presents the core challenge of single nonchangeable hidden state encoding for large contextual corpora for the task that supposes the modularity of the encoded representations.\n2. The proposed mechanism is simple and well-described.\n3. The superior computational efficiency of the proposed method over existing approaches is experimentally supported."}, "weaknesses": {"value": "1. The limited generalizability and practical utility of the proposed method due to performance decrease in evaluations with the number of segments larger than used during training.\n2. No statistical test or confidence intervals were reported to support the significance of the experimental results.\n3. The paper lacks explicit comparisons of time and memory costs of the proposed method with document concatenation baselines to support the cost efficiency claim.\n4. The paper does not provide any comparisons with related RAG methods."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tu8OI1ipve", "forum": "Mw6TUZer5l", "replyto": "Mw6TUZer5l", "signatures": ["ICLR.cc/2026/Conference/Submission19975/Reviewer_Yxuq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19975/Reviewer_Yxuq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861959336, "cdate": 1761861959336, "tmdate": 1762932832425, "mdate": 1762932832425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to \"soup\" documents in SSM space and use them for various tasks. They show that although they cannot do them out of the box, SSMs can be trained to do so effectively. They show this in a variety of eval settings and with a variety of different ways/numbers of docs. \n\nOverall, I think this paper is promising but I am concerned about the baselines. I am willing to improve my score if those are included (regardless of whether the results are better/worse than the proposed method)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- I think the topic of this paper is unique, both quite interesting and has surprisingly good results\n- The proposed method seems to work across settings and as the number of documents increases\n- The paper is well written and even includes an eval with standard transformers"}, "weaknesses": {"value": "1. My biggest concern is the lack of a FT'd baseline for these approaches. It seems a crucial comparison that is left out in favor for various different versions of the proposed technique (e.g. Table 1 and Table 3). It's also possible that I'm misunderstanding the baselines there, so please correct me if I'm wrong. Since the proposed version does FT'ing, I would expect all evals to have a baseline of the non-soup with fine-tuning.\n2. [Minor] I think the experiment up to 256 docs needs to be in the main text, especially since it is referenced in the abstract. If accepted, I would strongly encourage the authors to do that."}, "questions": {"value": "My questions are in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dmskBLFbUH", "forum": "Mw6TUZer5l", "replyto": "Mw6TUZer5l", "signatures": ["ICLR.cc/2026/Conference/Submission19975/Reviewer_g79h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19975/Reviewer_g79h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943393244, "cdate": 1761943393244, "tmdate": 1762932796834, "mdate": 1762932796834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the method of document souping which combines hidden states from independently encoded documents in SSMs.  Each document is encoded using SSMs and the hidden representations are then pooled to be used a decoder conditioned on the pooled representations. The paper is then evaluated on Multi-Doc QA and Long Doc QA datasets, and outperforms the concat appraoch."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper shows the surprising result that the representations from SSMs can be pooled and used to answer question which allows for doucments to be parallely encoded offering speedups and cost savings. The paper shows the method scales to 256 documents and outperforms the concat appraoch on different datasets.  The idea and the results is interesting and would be of value."}, "weaknesses": {"value": "One consideration that I would want the authors to test is instead of having random distractors, have 5-10 documents that each answer a different question and pool them together and test the singular representation on the different questions for the documents. My worry is that training on these documents (specifically the full finetuning) might have introduced some bias about what documents are more likely to answer questions or not and this will test if information from all of the documents is retained or not. \n\nThe other thing not explicitly tested/mentioned is 1) the scaling with length of the documents and 2) how long are the documents, is it equal to the sequence length of the model for each dataset i.e. can I soup together 16 4k token length documents?"}, "questions": {"value": "What is the average length of the encoded document? \nWhat is the max length of the encoded documents is it equal to the max length of the model? \nHow well does the model preserve information across different documents souped together?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N9HczT4ivt", "forum": "Mw6TUZer5l", "replyto": "Mw6TUZer5l", "signatures": ["ICLR.cc/2026/Conference/Submission19975/Reviewer_2Lgv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19975/Reviewer_2Lgv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047147402, "cdate": 1762047147402, "tmdate": 1762932764359, "mdate": 1762932764359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}