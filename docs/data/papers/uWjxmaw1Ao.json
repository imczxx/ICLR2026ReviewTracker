{"id": "uWjxmaw1Ao", "number": 19042, "cdate": 1758293076865, "mdate": 1759897064332, "content": {"title": "Statistical Optimality of Newton-type Federated Learning with Heterogeneous Data", "abstract": "The mainstream federated learning algorithms only communicate the first-order information across the local devices, i.e., FedAvg and FedProx. However, only using first-order information, these methods are often inefficient and the impact of heterogeneous data is yet not precisely understood. This paper proposes an efficient federated Newton method (FedNewton), by sharing both first-order and second-order knowledge over heterogeneous data. In general kernel ridge regression setting, we derive the generalization bounds for FedNewton and obtain the minimax-optimal learning rates. For the first time, our results analytically quantify the impact of the number of local examples, the data heterogeneity and the model heterogeneity. Moreover, as long as the local sample size is not too small and data heterogeneity is moderate, the federated error in FedNewton decreases exponentially in terms of iterations. Extensive experimental results further validate our theoretical findings and illustrate the advantages of FedNewton over the first-order methods.", "tldr": "This paper proposes a federated Newton method (sharing first-order and second-order information) with generalization error bounds.", "keywords": ["Federated learning", "Second order optimization", "Generalizaton error bounds", "Newton's method"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ce54ab31ccc83808246907c156b302c28227ef8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose FedNewton, which communicates the global gradient and local inverse‑Hessian preconditioned increments. The theory gives minimax‑optimal generalization rates while quantifying effects of local sample size, covariate shift, and response shift. A key message appears to be that if local data are not too small and heterogeneity is modest, the federated error decays exponentially in $t$."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Prior second‑order FL work focused on optimization, but here we get excess‑risk bounds with minimax‑optimal rates.\n\n2. The claim that the proposed algorithm has an exponential decay in the federated error in terms of iterations is an astonishing and shocking result, if true."}, "weaknesses": {"value": "1. The paper is quite involved, and would benefit from more intuitive scaffolding. \n\n2. Can you clarify the statement that this second-order method drawing on local hessians is only \"2 times\" as compared to first-order FL algorithms?"}, "questions": {"value": "Please see weaknesses. Also, why is $C-C_{\\mathcal{D}}$ PSD in equation (11)?\n\nNote: I am not particularly knowledgeable about the area in which this paper is situated in. This is reflected in my confidence score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4Czy4Mxy58", "forum": "uWjxmaw1Ao", "replyto": "uWjxmaw1Ao", "signatures": ["ICLR.cc/2026/Conference/Submission19042/Reviewer_x34g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19042/Reviewer_x34g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760664219339, "cdate": 1760664219339, "tmdate": 1762931080224, "mdate": 1762931080224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedNewton, a second-order federated learning algorithm that shares both first-order (gradients) and second-order (Hessian) information across local devices. The authors analyze this method in the kernel ridge regression (KRR) setting and derive generalization bounds that quantify the impact of local sample size, data heterogeneity (covariate shift), and model heterogeneity (concept shift). The main theoretical contribution is establishing minimax-optimal learning rates for federated Newton methods and showing that under sufficient local samples and moderate heterogeneity, the federated error decreases exponentially."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This appears to be the first work providing rigorous generalization guarantees (not just optimization convergence) for Newton-type federated learning under both data and model heterogeneity. The gap between optimization and generalization analysis in federated learning is significant, and this work makes progress on bridging it.\n- The paper provides a unified treatment of both covariate shift and concept shift, with explicit quantification of their impacts on learning rates. The error decomposition in Theorems 1-2 cleanly separates these effects.\n- The authors extend beyond the standard $r \\in [1/2, 1]$ regularity condition to $r > 0, 2r + \\gamma ≥ 1$, which is more general than prior DKRR work (Zhang et al., 2015; Guo et al., 2017).\n- The paper provides explicit communication costs ($\\mathcal{O}(M)$) and shows that FedNewton achieves linear convergence when conditions are met, requiring fewer rounds than first-order methods' complexity."}, "weaknesses": {"value": "- The entire theoretical analysis is limited to squared loss and kernel ridge regression. While Remark 4 claims the algorithm applies to \"twice differentiable\" loss functions, no theoretical guarantees are provided for other losses. This severely limits practical applicability, especially for classification tasks, which dominate federated learning applications.\n- Computing $H^{-1}_{D_j, \\lambda}$ requires $\\mathcal{O}(|D_j|M^2 + M^3)$ operations. While Remark 1 mentions existing techniques (BFGS, L-BFGS), the paper dismisses this as \"beyond scope\". For practical federated learning with large $M$, this is a critical limitation that undermines the \"efficiency\" claims. The paper should either provide concrete solutions or temper its efficiency claims.\n- Assumption 1 (capacity condition) requires $\\max(\\mathcal{N}(\\lambda), \\mathcal{N}_1(\\lambda),...,\\mathcal{N}_m(\\lambda)) ≤ Q^2 \\lambda^{-\\gamma}$, constraining all local effective dimensions. Why is this reasonable when local distributions are heterogeneous?\n- Experiments use random Fourier features (finite M=200 or 2000), but theory assumes infinite-dimensional RKHS. Remark 6 briefly mentions finite-dimensional cases but doesn't provide the main results.\n- The initialization $w^0_{D_j, \\lambda} = H^{-1}_{D_j,λ} \\Phi^T_{D_j} y_{D_j}$ is non-standard and already requires expensive local computation before any communication.\n- Table 1 shows many recent Newton-type FL methods (FedNL, SHED, FedNS, Fed-sofia) but the paper only compares experimentally with FedAvg and FedProx. Direct empirical comparison with these second-order baselines is essential to validate the claimed advantages.\n- How sensitive is the method to hyperparameter choices ($\\sigma^2, \\lambda$)? The experiments mention grid search but don't discuss robustness.\n- Proposition 1 claims \"partitionability\" but this only holds for squared loss. This limitation should be emphasized.\n- No wall-clock time comparisons, only iteration counts. \n- Missing experiments on larger-scale datasets common in federated learning (e.g., CIFAR-10, FEMNIST)."}, "questions": {"value": "Refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IrDeA6rywN", "forum": "uWjxmaw1Ao", "replyto": "uWjxmaw1Ao", "signatures": ["ICLR.cc/2026/Conference/Submission19042/Reviewer_h24k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19042/Reviewer_h24k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761164232226, "cdate": 1761164232226, "tmdate": 1762931079868, "mdate": 1762931079868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FedNewton, a second-order federated learning algorithm that communicates local curvature information to achieve faster and statistically optimal convergence under heterogeneous data. The method bridges optimization and generalization by decomposing total error into centralized and federated components and establishes optimal convergence rates. Experiments on synthetic and LIBSVM datasets support the theoretical claims, showing improved accuracy and convergence over FedAvg and FedProx. Overall, the work makes a strong theoretical contribution, though experiments remain limited in scale."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a rigorous theoretical framework connecting optimization and generalization in second-order federated learning.\n2. The FedNewton algorithm (Algorithm 1, Figure 1) is well-designed, balancing curvature-based updates with communication efficiency.\n3. The error decomposition (Section 4) clearly explains the effects of heterogeneity on performance.\n4. The theoretical analysis is detailed, logically structured, and establishes minimax-optimal convergence rates.\n5. Experimental results (Figure 3, Table 1) align with the theory, showing consistent improvement over baseline methods.\n6. The comparative analysis (Table 1) clearly positions the work relative to other Newton-type and first-order approaches."}, "weaknesses": {"value": "1. Experiments are limited to small synthetic and LIBSVM datasets; larger or more diverse benchmarks like ( FEMNIST, CIFAR-FL) would strengthen the results.\n2. Computing local Hessian inverses remains expensive; the paper mentions approximations but provides no empirical evaluation.\n3. The heavy notation makes the theory difficult for non-specialists; brief intuitive explanations or visual aids would improve readability.\n4. Experimental plots (Figure 3) lack variance bars or confidence intervals, making it hard to assess robustness.\n5. The heterogeneity settings used are synthetic; connecting them to real-world non-IID data distributions would enhance practical relevance."}, "questions": {"value": "1. Can FedNewton use approximate or low-rank Hessian inverses without losing its reported performance?\n2. Does the observation that the method converges within two rounds hold across different datasets?\n3. Have you tested the approach with non-squared loss functions such as logistic or cross-entropy loss?\n4. How well do the theoretical results transfer to neural-network-based or kernelized models in practice?\n5. Include runtime and memory comparisons with FedAvg and FedProx for  example ( number of communication rounds vs. total time).\n6. Add simple convergence plots showing performance across different data heterogeneity levels (as in Figure 3).\n7. Discuss how FedNewton could be extended to non-convex or deep learning settings (Section 6)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R7U3oQn1Eo", "forum": "uWjxmaw1Ao", "replyto": "uWjxmaw1Ao", "signatures": ["ICLR.cc/2026/Conference/Submission19042/Reviewer_sTCB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19042/Reviewer_sTCB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915792718, "cdate": 1761915792718, "tmdate": 1762931079254, "mdate": 1762931079254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the statistical optimality of Newton-type federated learning (FL) algorithms under heterogeneous data distributions. The authors propose FedNewton, a second-order federated optimization method that leverages both global gradients and local Hessians to improve convergence and generalization. The authors further quantify how local sample size, data heterogeneity, and model heterogeneity jointly affect the excess risk and convergence behavior. Experimental results on synthetic and real-world datasets validate the theoretical findings, showing that FedNewton achieves exponential convergence with minimal communication rounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents the generalization analysis for Newton-type federated learning methods under data heterogeneity. \n2. The authors derive non-asymptotic excess risk bounds and demonstrate minimax-optimal learning rates under mild assumptions. The decomposition of federated error and centralized excess risk provides clear interpretability."}, "weaknesses": {"value": "--Limited diversity and scalability of experimental settings.\nThe experimental evaluation in Appendix A mainly uses a synthetic dataset and small-scale benchmarks from LIBSVM. These datasets are low-dimensional and domain-specific, which restricts the demonstration of FedNewton’s capability in large-scale or high-dimensional federated learning scenarios, such as image or language applications. Furthermore, all experiments focus on convex regression problems, without extending to non-convex architectures like neural networks. \n\n--Restrictive theoretical assumptions.\nThe theoretical analysis relies on kernel ridge regression with squared loss and assumes that all local functions lie in a reproducing kernel Hilbert space (RKHS). While this setting facilitates mathematical tractability, it is less reflective of practical federated learning where non-convex objectives, neural network architectures, or unbounded losses are common. \n\n--Incomplete communication–computation tradeoff analysis.\nSection 3 provides a complexity discussion but does not present any runtime or communication cost comparisons in the experiments. While the authors claim that FedNewton achieves similar per-round cost as first-order methods with exponentially faster convergence, this claim is not quantitatively supported. For example, the cost of computing and inverting local Hessians can be substantial for large feature dimensions. Without practical wall-clock evaluations or scalability analyses, it is unclear whether the method is truly more efficient in real federated environments."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kvbwlH5Svk", "forum": "uWjxmaw1Ao", "replyto": "uWjxmaw1Ao", "signatures": ["ICLR.cc/2026/Conference/Submission19042/Reviewer_bY7P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19042/Reviewer_bY7P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988652262, "cdate": 1761988652262, "tmdate": 1762931078650, "mdate": 1762931078650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}