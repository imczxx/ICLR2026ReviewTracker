{"id": "kxEM2vc7ne", "number": 12151, "cdate": 1758205985982, "mdate": 1759897528729, "content": {"title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops", "abstract": "Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose \\textbf{LingoLoop}, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a \\textbf{POS-Aware Delay Mechanism} to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a \\textbf{Generative Path Pruning Mechanism} that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments on models like Qwen2.5-VL-3B demonstrate LingoLoop's powerful ability to trap them in generative loops; it consistently drives them to their generation limits and, when those limits are relaxed, can induce outputs with up to \\textbf{367$\\times$} more tokens than clean inputs, triggering a commensurate surge in energy consumption. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment.", "tldr": "We propose LingoLoop, an attack that exploits MLLMs' verbosity vulnerabilities by manipulating token predictions and inducing loops, leading to resource exhaustion and highlighting the need for better defenses.", "keywords": ["Multimodal Large Language Models", "Energy-latency Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f44616030f68016762e735245a09b73a4775405.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents LingoLoop Attack, a novel inference-time adversarial framework targeting Multimodal Large Language Models (MLLMs). The attack combines two mechanisms:\n1. POS-Aware Delay Mechanism, which suppresses EOS (End-of-Sequence) token probability based on part-of-speech statistics to delay termination; and\n2. Generative Path Pruning Mechanism, which constrains the hidden-state L2 norms to induce representational collapse and repetitive looping generation.\nExtensive experiments on several state-of-the-art MLLMs (Qwen2.5-VL, InstructBLIP, InternVL3) show that this attack can dramatically increase output length (up to 367×) and energy consumption, exposing a potential inference-time denial-of-service (DoS) risk."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposes a creative and interpretable attack that integrates linguistic and representational perspectives.\n2. Systematically evaluated across multiple large MLLMs and datasets.\n3. Provides empirical evidence linking hidden-state variance reduction to looping output behavior.\n4. Offers a conceptual bridge between linguistic priors and generative dynamics, potentially valuable for robustness analysis and interpretability studies."}, "weaknesses": {"value": "1. Defense evaluation is insufficient and shallow.\nThe paper only tests heuristic decoding hyperparameters (repetition penalty, n-gram ban), without exploring input-level detection, adversarial training, or architectural regularization.\nThere is no analysis of defense–performance trade-offs or ablation on defense effectiveness across different models.\n2. The white-box assumption severely limits real-world applicability.\n3. The empirical evidence for causality between hidden-state compression and looping behavior remains correlational.\n4. Lack of cross-lingual or non-English experiments weakens generality claims."}, "questions": {"value": "1. Could a simple entropy-based or activation-monitoring defense mitigate looping?\n2. Does the POS-aware mechanism generalize across languages with different syntactic structures?\n3. How sensitive are results to decoder hyperparameters (temperature, top-p, etc.)?\n4. Can fine-tuning with diversity regularization or entropy constraints prevent hidden-state collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9unUYWJPBL", "forum": "kxEM2vc7ne", "replyto": "kxEM2vc7ne", "signatures": ["ICLR.cc/2026/Conference/Submission12151/Reviewer_eX4S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12151/Reviewer_eX4S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535554542, "cdate": 1761535554542, "tmdate": 1762923108256, "mdate": 1762923108256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose LingoLoop, an energy-latency attacks for MLLMs. The author analyze the MLLM internal behaviors and observ that token-level POS characteristics may influence the probability of generating EOS token and extreme output lengths often relies on repetitive or looping state. Linguistic prior suppression loss and repetition promotion loss are proposed to suppress the generation probability of EOS token and promote the repetitive generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The writing and presentation of the paper are excellent, with a clear and logical flow.\n- The loss design is highly feasible with rich empirical insight supported.\n- Experimental results demonstrate that the attack is effective under white-box scenarios."}, "weaknesses": {"value": "- **Threat model**. Very strong assumption on threat model. The author assume a white-box scenarios with full access to model architecture, parameters and gradients. However, for many business services, the attacker only knows which series of models the victim model belongs to, but does not know the specific architecture and parameters of the model. What’s more, in some cases, the models for business services are not open-sourced.\n- **Lack of model transfer attacks**. Although the attack is effective in white-box scenarios, to show the attack is practical, more transfer attacks should be considered, including the transfer attack between different series of MLLMs and different size of MLLMs. For instance, optimizing an adversarial inputs using Qwen2.5-VL-3B and try to attack Qwen2.5-VL-7B.\n- **Defenses considered**. Generative path pruning reduces the generation diversity, promoting repetitive generation, increasing the risk of attack detection. In sec 4.5, the author only considers the built-in mitigation methods, more detection methods have been considered in Appendix F, which shows high interception rate. The results demonstrate that the attack is not stealthy enough."}, "questions": {"value": "- What is the computation complexity of one adversarial sample for different models?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j1Daz4sY57", "forum": "kxEM2vc7ne", "replyto": "kxEM2vc7ne", "signatures": ["ICLR.cc/2026/Conference/Submission12151/Reviewer_Hnga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12151/Reviewer_Hnga"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547674446, "cdate": 1761547674446, "tmdate": 1762923107769, "mdate": 1762923107769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LingoLoop, a novel energy-latency (Denial-of-Service) attack designed to trap Multimodal Large Language Models (MLLMs) into generating excessively long and repetitive outputs, thereby exhausting computational resources. The authors identify that prior attacks are suboptimal because they (1) uniformly suppress the [EOS] token without considering its strong correlation with the Part-of-Speech (POS) tag of the preceding token, and (2) fail to actively induce the looping behavior necessary for sustained generation. LingoLoop addresses this with a two-part white-box attack: first, a \"POS-Aware Delay Mechanism\" uses a pre-computed statistical model to selectively suppress [EOS] probabilities based on the linguistic context. Second, a \"Generative Path Pruning Mechanism\" adds a loss term to constrain the L2 norm of hidden states, forcing the model's generative path into a \"state-space collapse\" that results in persistent, repetitive loops."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novelty and Significance: The paper identifies a sophisticated, two-fold vulnerability (POS-[EOS] correlation and hidden state dynamics) that is more nuanced than simple [EOS] suppression. This presents a significant and practical threat model for MLLMs, especially those deployed via metered or free APIs, where resource exhaustion directly translates to financial loss or service denial.\n\nQuality of Analysis: The attack's design is well-motivated by clear empirical analysis. The visualization of [EOS] probability versus POS tag (Fig 3) and the correlation between hidden state norms and output repetition (Fig 4) provide strong, intuitive justifications for the two proposed mechanisms.\n\nEffectiveness: The experimental results are striking. The attack is shown to be far more effective than the previous state-of-the-art (\"Verbose Images\"), consistently hitting the maximum token limits (Table 1) and demonstrating an ability to \"trap\" the model in a persistent loop that scales with relaxed token caps (Table 5). The demonstrated transferability to closed-source models like GPT-4o and Gemini 2.5 Pro (Fig 12) underscores its real-world relevance."}, "weaknesses": {"value": "Defense Evaluation is Limited: The paper demonstrates that simple defenses like repetition_penalty and no_repeat_ngram_size are ineffective, and that LLM judges misattribute the error (Appendix F). However, this leaves more robust defenses unexplored. The attack's premise is resource exhaustion, so a discussion of service-level defenses (e.g., per-user token/compute budgets, hard rate-limiting) is missing. The internal state monitoring defense (Appendix F.1) is also very simple; more advanced anomaly detection on hidden state sequences is not tested.\n\nReliance on White-Box Access: The primary attack requires full white-box access to model gradients and hidden states to craft the adversarial perturbation. While transferability is shown (a key strength), the paper would be more complete if it discussed the feasibility of black-box query-based methods for this specific attack, even if only to demonstrate their impracticality due to the complex loss function."}, "questions": {"value": "The POS-Aware Delay Mechanism relies on a \"Statistical Weight Pool\" computed from a large dataset. How robust is this pool to distribution shift? For instance, does a pool generated from general-purpose captions (MS-COCO) work effectively against models fine-tuned on a specific domain, like medical or technical diagrams, where linguistic patterns might differ?\n\nThe defense analysis in Appendix F.2.2 shows that text-only LLM judges (GPT-4o, Gemini 2.5) fail, attributing the attack to an \"internal glitch.\" Could this defense be made effective by providing the judge with multimodal context? For example, if the judge was given the input image and the text output, could it learn to identify this specific type of image-based attack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "enzC8w8gZs", "forum": "kxEM2vc7ne", "replyto": "kxEM2vc7ne", "signatures": ["ICLR.cc/2026/Conference/Submission12151/Reviewer_ERmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12151/Reviewer_ERmc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804479539, "cdate": 1761804479539, "tmdate": 1762923107375, "mdate": 1762923107375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates a new adversarial attack on multimodal large language models (MLLMs) — models that take both vision (or other modalities) and language input. The goal is to force the model to generate extremely verbose and repetitive output (thereby consuming excessive compute/energy) and potentially degrade service (resource exhaustion / denial-of-service scenario)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of leveraging part‐of‐speech signals to affect EOS‐generation probability, and combining that with hidden‐state norm constraints to force looping, is a creative contribution. It goes beyond prior “verbose image” attacks that treated the model output more uniformly.\n\n\n- The authors provide quantitative experiments across multiple MLLM models and datasets, report tokens, energy, latency, and compare against strong baselines. The ablation studies help isolate the contributions of each mechanism."}, "weaknesses": {"value": "- As acknowledged, the attack currently requires full knowledge of the model (architecture, parameters, gradients) which limits real‐world applicability, especially for closed‐source or API‐only models.\n\n- The attack’s impact is partly limited by the model’s or API’s maximum token limit; once maximum output length is hit, the attack cannot push further. In real‐world API usage, token/output limits are enforced."}, "questions": {"value": "Listed in Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PoXSBVdCzp", "forum": "kxEM2vc7ne", "replyto": "kxEM2vc7ne", "signatures": ["ICLR.cc/2026/Conference/Submission12151/Reviewer_ckUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12151/Reviewer_ckUG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914400830, "cdate": 1761914400830, "tmdate": 1762923106965, "mdate": 1762923106965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}