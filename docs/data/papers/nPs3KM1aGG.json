{"id": "nPs3KM1aGG", "number": 18162, "cdate": 1758284555223, "mdate": 1763348033032, "content": {"title": "More Bang for the Buck: Process Reward Modeling with Entropy-Driven Uncertainty", "abstract": "We introduce the Entropy-Driven Uncertainty Process Reward Model (EDU-PRM), a novel entropy-driven training framework for process reward modeling that enables dynamic, unce\nrtainty-aligned segmentation of complex reasoning steps, eliminating the need for costly manual step annotations.\nUnlike previous Process Reward Models (PRMs) that rely on static partitioning and human labeling, EDU‑PRM automatically anchors step boundaries at tokens with high predictive entropy, effectively capturing intrinsic logical transitions and facilitating efficient exploration of diverse reasoning paths.\nOn the ProcessBench benchmark, EDU-PRM outperforms strong public PRM baselines, such as Math-Shepherd PRM and Omega PRM, and EDU-PRM achieves comparable results with SOTA models while only using 1.5\\% training data.\nFurthermore, by leveraging our proposed EDU sampling strategy, we observe accuracy boosts from 64.7\\% to 67.3\\% for generative reasoning tasks, accompanied by a reduction of 32\\% in token usage.\nThese findings underscore the potential of EDU-PRM as a scalable and annotation-efficient paradigm for process supervision in mathematical reasoning, paving the way for more efficient and robust approaches to complex mathematical problem solving.", "tldr": "Entropy-Driven Uncertainty Process Reward Model automatically segments complex reasoning steps via entropy-based uncertainty, eliminating manual annotation and outperforming existing baselines on the test dataset.", "keywords": ["PRM;Process Rewards Model;Entropy"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44ceea74fac06e9cf1596950148efae2aba2eb39.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Entropy-Driven Uncertainty Process Reward Model (EDU-PRM). The core idea is to use token-level predictive entropy to dynamically and automatically segment reasoning steps. The method identifies high-entropy tokens as \"uncertainty anchors,\" which are assumed to mark natural logical transitions. \n\nThe PRM trained on this automatically generated data outperforms strong baselines on the ProcessBench benchmark. They also show their inference sampling strategies provide higher accuracy for fewer tokens compared to standard high-temperature sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core contribution—using predictive entropy to find \"uncertainty anchors\" for segmenting reasoning is a well-motivated alternative to arbitrary, rule-based partitioning. \n\n2. A very comprehensive experiments shows the performance of the new algorithm."}, "weaknesses": {"value": "1. The novelty of the core idea in this paper is not a fundamental breakthrough, as it is built based on a few previous work."}, "questions": {"value": "1. ' the training dataset comprises approximately 1.42M instances, with a label distribution of 52% hard and 48% soft labels.' How the hard and soft labels are generated? I assume they should be the same?\n\n2. When you run the comparison with other PRM models, do you use the data provided or you implement the algorithm by yourselves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6cyOZNehdG", "forum": "nPs3KM1aGG", "replyto": "nPs3KM1aGG", "signatures": ["ICLR.cc/2026/Conference/Submission18162/Reviewer_tccH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18162/Reviewer_tccH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879479146, "cdate": 1761879479146, "tmdate": 1762927914956, "mdate": 1762927914956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EDU-PRM, a process reward model that uses token-level entropy to identify key decision points in reasoning. It segments reasoning chains at high-entropy tokens and uses MCE to assign reward scores to each segment based on outcome correctness. The experimental results show that EDU-PRM achieves significant performance improvements on ProcessBench and multiple math reasoning benchmarks while reducing token consumption."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated, clearly written, and easy to follow.\n2. The idea of using high-entropy tokens to segment reasoning steps is simple yet intuitive, effectively reducing the reliance on manual annotations or LLM-based heuristics.\n3. Extensive experiments across multiple benchmarks demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. While the paper claims novelty in introducing entropy into process reward modeling, entropy-based approaches have been explored previously. The authors do not sufficiently discuss their method with closely related work, such as:\n\n    ○ Entropy-Regularized Process Reward Model\n\n    ○ Uncertainty-Aware Step-wise Verification with Generative Reward Models\n\n    ○ Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning\n\n2. The definition and application of the entropy threshold $\\tau(H)$ are confusing and under-specified. In Section 3.2, the authors claim $\\tau(H)$ is dynamically adjusted based on the maximum number of sampled branches, suggesting an adaptive design. Yet, no formula, algorithm, or implementation detail is provided. More confusingly, in Section 4.1, the authors mention using a fixed entropy threshold (entropy threshold = 1.0), and in Section 5 (Tables 4 and 5), their analysis is also conducted around different fixed threshold values. This inconsistency makes the thresholding mechanism conceptually vague, weakening the clarity and reproducibility of the method.\n3. The paper claims that EDU-PRM alleviates the issue of “cheating”, where high intermediate rewards do not necessarily correlate with correct final answers. However, its Monte Carlo Estimation Scoring (MCE) still relies on the correctness of the final answer to assign credit to intermediate segments. Consequently, if an incorrect reasoning step accidentally leads to a correct final answer, MCE may still assign it a high reward. Although entropy-based segmentation improves over heuristic splitting, the MCE mechanism itself does not fundamentally resolve this problem, and the paper should discuss this limitation more explicitly and cautiously."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b7zfJGr1TO", "forum": "nPs3KM1aGG", "replyto": "nPs3KM1aGG", "signatures": ["ICLR.cc/2026/Conference/Submission18162/Reviewer_ouDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18162/Reviewer_ouDV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997784446, "cdate": 1761997784446, "tmdate": 1762927914081, "mdate": 1762927914081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new sampling method for (i) generating a new dataset for PRM training and (2) test-time PRM inference. This method works by spanning parallel generations for tokens with high predictive entropy only (the “anchors”) and estimating the score for sub-trajectories between two anchor points using Monte Carlo estimation. The paper provides empirical results for the accuracy of PRMs on ProcessBench, as well as BoN results following the resultant PRMs and further analysis on scaling trends, branching, lexical analysis, etc."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The method seems to have promising gains on Best-of-N evaluation of the proposed PRM-72B model.\n\n- The paper brings a rich experimental analysis, both quantitatively and qualitatively."}, "weaknesses": {"value": "- The major concern is that the quantitative experiments do not bring any evidence of statistical significance. There are no error bars in any of the experiments. The paper states running 8 experimental seeds, but it only reports the average. From only the average it is unclear if the reported gains are meaningful or just observation noise.\n\n- It is also unclear if training all the baselines on the dataset generated by the EDU sampling (as stated in L258-259) is indeed a fair choice. The dataset is inherently biased towards the proposed heuristic and may benefit the proposed method\n\n- It is also not clear what is the methodological contribution in the paper. The Related Work section is superficial and does not contrast with the literature in the area, being limited to the two baselines adopted. My understanding is that the work has limited novelty, as using uncertainty (or confidence) heuristics to guide inference is an explored direction  [1, 2, 3], including in the specific context of PRMs [4].\n\n- The results related to high temperature sampling in the paper are limited for T = 0.7. It is important to analyze the impact of different temperatures in the method, as this is a quite sensitive hyperparameter. Furthermore, there is no mention on how the hypers of the proposed and comparison methods were selected. It is unclear if the tuning procedure was fair among the methods.\n\nMore specific weaknesses:\n- In Table 1, the 7B EDU-based models present a very low recall / F1 score. From my understanding of the problem setup, it means the PRMs are failed to identify several wrong steps. In any case, the paper should discuss the reason behind this, as the current takeaway is that the proposed method seems to fail for the 7B scale.\n\n- Nit: The paper describes that there is a Symbol set to avoid mathematical symbols (e.g., sum, integral) in the entropy calculation. But the list in Appendix A.4 does not bring the aforementioned math symbols.\n\n- During Introduction, the paper motivates the proposed method as a way to prevent “cheating” vulnerabilities in PRMs. The paper does not make clear how the method prevent such vulnerabilities, nor any of the experiments linked back to this claim. It is unclear if the proposed PRMs really addressed the defined issue, as the experiments only concentrate on final accuracy.\n\n- It is also hard to take any conclusion from Figure 5 when comparing EDU and P-EDU. Besides the lack of error bars, the curves look nearly identical, and the discussion is limited to analyzing specific points in the curves to make more general claims. My takeaway from the Figure is that P-EDU does not make a meaningful change in the final result, which seems to be the opposite to what is claimed.\n\nA final note: while I appreciate the efforts on providing a rich set of experiments, the paper discussion needs more polishing to condense the information in clear claims/takeaways. There are many different setups/metrics/analyses and it is crucial to better map experiments to specific claims and highlight them. Currently it is hard to filter this information, and the claims are in general vague (e.g., “This highlights EDU sampling’s superior capability to leverage additional tokens for sustained accuracy gains.” in L352)."}, "questions": {"value": "- What are the Qwen2.5-Math-PRM results for the BoN setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rHcjW89VE2", "forum": "nPs3KM1aGG", "replyto": "nPs3KM1aGG", "signatures": ["ICLR.cc/2026/Conference/Submission18162/Reviewer_MjAk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18162/Reviewer_MjAk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025622959, "cdate": 1762025622959, "tmdate": 1762927913591, "mdate": 1762927913591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper proposes EDU-PRM, a new way to train process reward models using token-level entropy to detect where a model is uncertain during reasoning.\n- This entropy-driven uncertainty automatically marks step boundaries and creates diverse reasoning paths without human or LLM annotations.\n- Each reasoning fragment is labeled automatically through Monte-Carlo estimation based on whether the final answer is correct.\n- The resulting EDU-PRM matches or nearly matches the performance of the large, fully supervised Qwen2.5-Math-PRM.\n- An enhanced version called Pruning-EDU further improves efficiency by cutting off low-confidence reasoning paths early, reducing token use with minimal accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- It eliminates the need for human or LLM annotations by automatically labeling reasoning steps using entropy and Monte Carlo estimation.\n- The entropy-based segmentation captures natural reasoning boundaries, improving how step-level rewards relate to final correctness.\n- The pruning and entropy-guided sampling reduce token usage and search complexity compared to exhaustive sampling or MCTS."}, "weaknesses": {"value": "- All experiments are restricted to math reasoning, so generalization to other domains remains unproven.\n- The Monte Carlo estimation can produce imperfect or misleading correctness scores for intermediate steps.\n- Performance depends on carefully tuning the entropy threshold that defines where to branch or segment reasoning."}, "questions": {"value": "- How do you choose the entropy threshold for step segmentation?\n- Since Monte Carlo estimation relies on final-answer correctness, how do you mitigate cases where an incorrect final answer arises from an otherwise correct partial reasoning step?\n- How do you decide the pruning threshold (e.g., PRM score < 0.2)?\n- Since your evaluation focuses on math, how could EDU-PRM be adapted to domains where final correctness cannot be automatically verified (e.g., commonsense or scientific reasoning)?\n- Can you share more qualitative examples where high-entropy points clearly align with human-intuitive reasoning transitions?\n- The authors show Gaussian-smoothed trends. Can you share the raw results before smoothing to see the original values?\n- In Line 13, unce rtainty-aligned -> uncertainty-aligned"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no particular ethical concern."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9BS2zYjIgk", "forum": "nPs3KM1aGG", "replyto": "nPs3KM1aGG", "signatures": ["ICLR.cc/2026/Conference/Submission18162/Reviewer_2VV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18162/Reviewer_2VV7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045932624, "cdate": 1762045932624, "tmdate": 1762927912959, "mdate": 1762927912959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}