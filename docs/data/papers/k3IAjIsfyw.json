{"id": "k3IAjIsfyw", "number": 15136, "cdate": 1758248120085, "mdate": 1759897325785, "content": {"title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Eviction", "abstract": "Key-Value (KV) caching is a widely adopted technique in large language models (LLMs) to accelerate long-context inference. While recent studies predominantly focus on question-dependent KV cache eviction where cache entries are evicted based on known queries. In this paper, however, we observe these approaches often fail in question-independent scenarios, such as multi-turn dialogues and chunk pre-caching in retrieval-augmented generation (RAG), where future queries remain unknown. Our empirical analysis reveals that most existing KV cache eviction methods underperform in this setting due to their heavy reliance on importance metrics derived from the attention score with question tokens. The core challenge here is to conduct well-founded estimation on token importance without access to future questions. To address this, we propose OracleKV for question-independent KV cache eviction. OracleKV operates by steering model's attention with an oracle guidance containing surface-level statistics of user preferences from large-scale real-world dialogues. Unlike existing methods, OracleKV operates at the data level, allowing seamless integration with other eviction algorithms in a plug-and-play manner. Experiments on several multi-turn and single-turn benchmarks demonstrate that OracleKV achieves higher accuracy-latency tradeoff than existing KV cache compression approaches. We hope our approach will expand the design space and serve as a solid baseline for future research in KV cache compression.", "tldr": "This work propose a plug-and-play data-level KV cache eviction method to enhance system efficiency in question-independent KV cache compression scenarios.", "keywords": ["Large Language Models", "KV Cache", "Machine Learning", "Efficient Machine Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c0106d900190ad794f4c98e6fd53674bb6f53953.pdf", "supplementary_material": "/attachment/8157207f643d377e4a72f3d791e2c3a4e9668332.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles a critical and underexplored problem in long-context LLM inference. The authors propose OracleKV, which introduces an oracle guidance — a short, synthetic context that statistically represents user question distributions. This “guidance” steers the attention distribution during prefilling, allowing the model to estimate token importance without access to the actual query. The method is data-level, plug-and-play, and model-agnostic. Extensive experiments on LongBench, RULER, SCBench, and Needle-In-A-Haystack show strong improvements under both 40% and 10% cache budgets, outperforming state-of-the-art baselines such as SnapKV, PyramidKV, AdaKV, and DuoAttention."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly identifies the limitations of existing KV cache compression in question-independent settings.\n\n2. The motivation is convincing and well illustrated.\n\n3. Extensive experiments across diverse benchmarks and models (LLaMA-3.1, Mistral, Qwen2.5) show consistent gains."}, "weaknesses": {"value": "The main contribution of the paper lies in introducing the oracle, which serves as a meta-question designed to retain important KV entries that can generalize across diverse testing scenarios. However, the construction process of the oracle remains unclear to me. \n\n1. The guidance template appears handcrafted and relies on external statistics.\n\n2. The generalization of these distributions to unseen domains is uncertain.\n\n3. Automation of oracle construction (Section G.2) is only lightly explored.\n\n4. Prefilling cost increases with guidance length (Figure 10), and scaling trade-offs are not deeply analyzed.\n\n5. A broader evaluation on even larger LLMs (≥30B) would strengthen generality claims.\n\n6. While Theorem 4.2 gives intuition, the formal proof and assumptions (semantic-type alignment, Assumption 4.4) are loose and may not hold empirically. The relationship between attention scores and semantic alignment is assumed, not demonstrated.\n\n7. The reported experimental results for several baselines are noticeably lower than those in prior works. For instance, SnapKV and PyramidKV achieve higher accuracies on LLaMA-3.1-8B-Instruct and Mistral-7B-Instruct-v0.2 on LongBench in previous papers [https://arxiv.org/pdf/2406.02069, https://arxiv.org/pdf/2502.14051, https://arxiv.org/pdf/2407.12820], even when evaluated under smaller KV-cache budgets (10% vs. 64/256).\n\n8. The authors should also test the methods on the long-generation tasks, such as reasoning benchmarks.\n\nOverall, the current oracle design appears overly reliant on prompt engineering, and its generalizability across all testing scenarios remains uncertain. A related work, Cartridge (https://github.com/HazyResearch/cartridges), also explores query-independent context compression. Although not originally developed for KV-cache settings, its self-study mechanism bears conceptual similarity to the oracle idea proposed in this paper, but is more systematically designed and less handcrafted."}, "questions": {"value": "1. How sensitive is OracleKV to the choice of oracle guidance? Could mismatched guidance distributions harm performance?\n\n2. Can OracleKV be trained or fine-tuned to learn task-specific guidance automatically rather than manually crafting prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "paMeVheWkz", "forum": "k3IAjIsfyw", "replyto": "k3IAjIsfyw", "signatures": ["ICLR.cc/2026/Conference/Submission15136/Reviewer_kvuV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15136/Reviewer_kvuV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761354405517, "cdate": 1761354405517, "tmdate": 1762925452154, "mdate": 1762925452154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuse on the question-independent KV cache eviction. It uses an oracle guidance to steer model’s attention and allows seamless integration with other algorithm. Then it selects KV entries semantically correlated with the guidance and evicts low-relevance entries until the cache fits the memory budget.\nThe paper provides theoretical justification (via KL divergence analysis) showing that aligning the semantic type distribution of retained KV entries with that of question-required entries improves predictive accuracy.\nExperimental evaluations across four benchmarks (LongBench, RULER, Needle-In-A-Haystack, SCBench) and three LLMs (LLaMA-3.1-8B-Instruct, Mistral-7B-Instruct-v0.2, Qwen2.5-7B-Instruct) demonstrate that OracleKV outperforms baselines (e.g., StreamingLLM, SnapKV, PyramidKV) in question-independent scenarios (e.g., multi-turn dialogues, RAG chunk pre-caching). It achieves better accuracy-latency tradeoffs, especially under extreme memory constraints (10% KV budget), and maintains compatibility with existing eviction algorithms via a plug-and-play design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. OracleKV introduces a novel paradigm for question-independent eviction by leveraging user preference statistics to guide attention. This differs from prior work that either relies on question-dependent scores or model-internal heuristics, filling a unique niche. \n\n2. Question-independent eviction is critical for scalable LLM deployment and OracleKV’s plug-and-play design lowers adoption barriers for existing frameworks. The work also opens new directions for future research.\n\n3. The paper’s structure and visualization make complex ideas accessible to easy understand."}, "weaknesses": {"value": "1. OracleKV degrades performance on code generation tasks, as the general oracle guidance disrupts code’s structural/syntactic regularity. The paper acknowledges this but does not explore a targeted solution beyond noting the \"no-free-lunch\" principle. A brief discussion of how to adapt guidance for code would strengthen robustness.\n\n2. The paper notes that longer oracle guidance increases pre-filling latency. While it recommends limiting guidance length to ≤128 tokens. Therefor, it is difficult to summarize an increasing number of task types.\n\n3. There no evaluation on ultra-long contexts to prove its scalability."}, "questions": {"value": "1. The paper notes OracleKV struggles with code tasks due to disrupted syntax. Could a code-specific oracle guidance mitigate this? If so, how would you balance task-specific guidance with the need for generalization across non-code tasks?\n\n2. Longer oracle guidance increases latency, but Section G.2 suggests LLM-generated guidance is competitive. Could you distill LLM-generated guidance into compact embeddings  to reduce pre-filling overhead while preserving performance?\n\n3. Qwen2.5-7B supports a 1M-token context window, but experiments use up to 128K tokens. How does OracleKV perform on ultra-long contexts? Does the optimal guidance length or semantic type distribution change with context length?\n\n4. The oracle guidance is derived from \"large-scale real-world dialogues\". Are these statistics domain-agnostic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dyUtiDkLWv", "forum": "k3IAjIsfyw", "replyto": "k3IAjIsfyw", "signatures": ["ICLR.cc/2026/Conference/Submission15136/Reviewer_WyvA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15136/Reviewer_WyvA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910567037, "cdate": 1761910567037, "tmdate": 1762925451707, "mdate": 1762925451707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OracleKV, a question-independent KV-cache eviction method for long-context LLM inference, addressing scenarios like multi-turn dialogue and RAG chunk pre-caching where the future query is unknown. The key idea is to append a short \"oracle guidance\" sequence, which is constructed from surface-level statistics of user preferences observed in large real-world dialog datasets, during prefill to steer attention toward token types (e.g., entities, numbers, sections). These token types are likely to matter, then retain KV entries with the highest guidance-conditioned attention. The method is interesting and uses the data, so it composes plug-and-play with existing eviction/selection schemes. A simple statistical model motivates the design: predictive accuracy grows with the overlap between retained and question-required cache entries, leading to the result that accuracy improves as the semantic-type distribution of retained entries aligns with that of required entries. The papers shows that OracleKV delivers better accuracy–latency and memory trade-offs than prior methods. Notably, at a 10% KV budget it improves average accuracy (e.g., +6.7% on Llama-3.1-8B) and shows very low degradation on several RULER subtasks even at 30% budget."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This is a clean idea that develops a question-agnostic KV selection that needs no model changes or fine-tuning and plugs into existing eviction policies.\n\n2. Strong, consistent gains across long-context benchmarks under tight KV budgets."}, "weaknesses": {"value": "1. This process is not dynamic or runtime. It depends on a hand-crafted, dataset-derived \"oracle guidance\" prior that may not generalize under domain/task shifts, and it introduces extra prefill tokens (which can increase costs) to steer attention.\n\n2. By being strictly question-agnostic, the underlying technique can retain irrelevant context on out-of-distribution queries. On the other hand, more adaptive and query-aware selectors could outperform when the query distribution diverges."}, "questions": {"value": "1. How exactly is the oracle-guidance prior built (token-type inventory, per-layer/head weighting, sequence length), and how sensitive are gains to misspecification? Please show ablations varying guidance composition/length and cross-domain shifts (code vs. narrative).\n\n2. What is the net systems cost of the added prefill tokens at different KV budgets, such as latency, throughput, memory traffic, and do RoPE/pos-encoding shifts from concatenation introduce distribution drift?\n\n3. How does OracleKV/AdaOracleKV interact with sliding-window attention, FlashAttention-3, and head pruning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RsBw5ckjEO", "forum": "k3IAjIsfyw", "replyto": "k3IAjIsfyw", "signatures": ["ICLR.cc/2026/Conference/Submission15136/Reviewer_Y32J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15136/Reviewer_Y32J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992231532, "cdate": 1761992231532, "tmdate": 1762925451257, "mdate": 1762925451257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OracleKV, a  framework for question-independent Key-Value (KV) cache eviction in large language models (LLMs). OracleKV leverages oracle guidance, i.e., data-level statistical priors derived from large-scale real-world dialogues, to estimate token importance without access to the future query. Empirical results across benchmarks such as LongBench, RULER, Needle-In-A-Haystack, and SCBench show consistent gains in retrieval accuracy and memory efficiency under low cache budgets across several LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. This paper tackles an important problem of sparse attention.\n\nS2. The paper is well written and structured with sufficient amount of discussion and details."}, "weaknesses": {"value": "W1. Though there are some formal analysis, the effectiveness of the formal analysis replies on a very strong key assumption that the oracle guidance can reflect the statistics of the future questions. The proof of the effectiveness of the oracle guidance is the key to the formal analysis, rather than the framework itself.\n\nW2. OracleKV highly relies on the effectiveness of the oracle guidance templates. Although section G.2 explores \"LLM-as-Guidance,\" results remain preliminary. Without an automated or learning-based mechanism, scalability and adaptability across domains can be limited.\n\nW3. Comparisons with recent sparse kv cache retrieval approaches, e.g., IceCache, ArkVale, MagicPig, InfiniGen, should also be included."}, "questions": {"value": "Q1. I appreciate that the authors also include anticipated side effects in the appendix. However, could you also provide a systematic failure case study to show that if there are cases whether the oracle guidance misalign with future questions?\n\nQ2. Can you explain in the main text how the oracle guidances are derived, and empirically show the sensitivity of the performance over a few different variance of the guidances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VMKKoh06ge", "forum": "k3IAjIsfyw", "replyto": "k3IAjIsfyw", "signatures": ["ICLR.cc/2026/Conference/Submission15136/Reviewer_pK1P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15136/Reviewer_pK1P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050987836, "cdate": 1762050987836, "tmdate": 1762925450748, "mdate": 1762925450748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}