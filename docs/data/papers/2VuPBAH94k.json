{"id": "2VuPBAH94k", "number": 5637, "cdate": 1757924358051, "mdate": 1763002096237, "content": {"title": "Revisiting Block-wise Interactions of MMDiT for Training-free Improved Synthesis", "abstract": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analysze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and strengthening textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes.\nBuilding on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench from 56.92\\% to 63.00\\% and GenEval from 66.42\\% to 71.63\\% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.", "tldr": "We analyze block-wise mechanisms in MMDiT diffusion models, showing how semantics and details emerge, and propose training-free strategies that improve alignment, editing, and efficiency without quality loss.", "keywords": ["Diffusion Models", "Training-free Synthesis", "Image Generation and Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/955d6578815cd734588df77a34e9076f9996cd9a.pdf", "supplementary_material": "/attachment/fd8ac2b4cabcedeffb32cb39f2fe5c6466e06b2f.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors investigate the role of different layers within the diffusion transformer by skipping a block, removing attention to the textual representations in a block, or enhancing the attention to the textual tokens in a block. The authors construct a dataset of 333 prompts with GPT-5 with attributes like color and shape, and use these interventions to find out what blocks are more important for each task. Later, the authors use the selected layers to improve generation, by enhancing the attention to textual tokens in the found layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors evaluate 3 common open source T2I models, analyze their behaviour and use the insights from their analysis to improve generation. The authors analyse the specific role of different layers, which is novel."}, "weaknesses": {"value": "The analysis of removing specific blocks was already presented in [1] on general prompts. While the main novelty of the authors in this paper in my opinion is to make a better localization, with specific layers attributed to specific tasks (like color and shape), the authors only use 333 prompts to do so. Since it’s done automatically, I would expect scale here. Moreover, the random baseline of selecting 5 random layers (table 6) looks to work well, almost as well as in the case you choose the layers. Moreover, the final results are evaluated on CLIP score to evaluate the performance of the improvement, while CLIP is considered poor for this kind of tasks [2]. Finally, no std is provided for the main results. A few more minor comments in the questions section.\n\n[1] Stable Flow: Vital Layers for Training-Free Image Editing (Avrahami 2025)\n[2] WHEN AND WHY VISION-LANGUAGE MODELS BEHAVE LIKE BAGS-OF-WORDS, AND WHAT TO DO\nABOUT IT? (Yuksekgonul, 2022)"}, "questions": {"value": "1. Section 2.2 “Disabling textual conditions is more disruptive than removing specific blocks” - removing specific blocks includes disabling textual conditions and disabling self-attention in the image. The fact that you find that disabling textual conditions is more disruptive is probably an artifact of the self-attention without condition. I don't see the value of this observation, please explain why you think this finding is interesting. \n2. Line 160: “we construct a challenging prompt dataset with GPT-5, comprising 333 diverse and difficult prompts across three attributes: color, amount, and spatial relationships” - How many of each category? How are these prompts constructed? \n3. How are blocks selected for each category? Do you use a threshold? How many prompts are used for each selection? \n4. The evaluation in the main results is done with CLIP score, known to be poor for such a nuanced task. Why not to use a better metric?\n5. Human preference - what is the agreement between human annotators? \n6. No STD in main tables (2,3) \n7. Acceleration improvement is not 2x in Table 5, while in the line 311 you say “Notably, when classifier-free guidance (CFG) is enabled during inference, we can achieve a ×2inference time acceleration on both the conditional and unconditional predictions.” why do the results differ so much? \n8. What blocks are selected for skipping? \n9. Line 428: “We can derive from the table that enhancing randomly selected or all blocks underperforms enhancing dedicated blocks identified from our analysis“ but Table 6: Random 5 blocks achieve almost the same improvement that you get. \n10. Minor: Fig 1 text in last row goes on the image. Fig 3: g,h,i are missing “)” on y axis"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kGWbylUar6", "forum": "2VuPBAH94k", "replyto": "2VuPBAH94k", "signatures": ["ICLR.cc/2026/Conference/Submission5637/Reviewer_xhmo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5637/Reviewer_xhmo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761290557412, "cdate": 1761290557412, "tmdate": 1762918170325, "mdate": 1762918170325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "5vfD0fsjSE", "forum": "2VuPBAH94k", "replyto": "2VuPBAH94k", "signatures": ["ICLR.cc/2026/Conference/Submission5637/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5637/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763002095346, "cdate": 1763002095346, "tmdate": 1763002095346, "mdate": 1763002095346, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores how block-wise manipulations—specifically removing blocks, disabling text conditioning, or enhancing it—impact the output of SOTA image generative models. The authors demonstrate that this analysis enables training-free applications, such as accelerating the inference process by removing less critical blocks and improving synthesis quality by enhancing text representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents an interesting use case for accelerating inference that only minimally impacts the quality of generated images, which may be applicable in situations.\n- The evaluation is performed on SOTA models, ensuring the relevance of the findings.\n- The writing is clear and the methodology is well-explained."}, "weaknesses": {"value": "- Sample size (3) for human evaluation is very small, authors should consider doing more extended study.\n- Similar methodology of exploring importance of specific blocks in diffusion models is already introduced in a numbers of works, such as [1], [2], [3] or [4]. Moreover these works explore the importance in light of many attributes not explored in this work, such as style, objects, safety, text. This would could largely benefit from extending the number of explored attributes.\n- Related works mentioned above use similar methodology of studying importance of blocks and thus findings from \"removing blocks\" and \"disabling text conditioning\" are not novel.\n- The results for the Qwen Image model (plots 3a, d, g) show that as the number of blocks increases, removing single blocks results in almost no distinguishable change in accuracy. This suggests the proposed analysis may become less informative as models continue to grow.\n- The observation that disabling textual conditions is more disruptive than removing blocks (Sec 2.2) seems largely attributable to the experimental setup. Overwriting text conditioning with zeros in a specific block causes this modified representation to propagate through all subsequent layers. It is therefore an expected outcome that this would cause a greater disruption than simply omitting one block's computation, and the direct comparison may be misleading.\n- The enhancing conditions approach yields unstable results, improving DINOv2 similarity for color and spatial attributes but degrading it for the amount attribute. This raises practical concerns, as it would be difficult to determine a priori if the method will provide a consistent performance boost for a given prompt.\n\n[1] Basu, Samyadeep, et al. \"On mechanistic knowledge localization in text-to-image generative models.\" _Forty-first International Conference on Machine Learning_. 2024.\n\n[2] Zarei, Arman, et al. \"Localizing Knowledge in Diffusion Transformers.\" _arXiv preprint arXiv:2505.18832_ (2025).\n\n[3] Staniszewski, Łukasz, et al. \"Precise Parameter Localization for Textual Generation in Diffusion Models.\" _The Thirteenth International Conference on Learning Representations_.\n\n[4] Avrahami, Omri, et al. \"Stable flow: Vital layers for training-free image editing.\" _Proceedings of the Computer Vision and Pattern Recognition Conference_. 2025."}, "questions": {"value": "- The DINOv2 similarity metric indicates that the \"enhancing conditions\" approach introduces a trade-off: while accuracy may increase for certain attributes, perceptual similarity decreases. Could you investigate this trade-off more thoroughly and discuss its implications?\n- How does the enhancing conditioning approach perform for other attributes not covered in the paper, such as style, object fidelity, or composition?\n- Given that the impact of removing single blocks diminishes in larger models, have you considered investigating alternative probing strategies, such as removing contiguous groups of blocks, which might better isolate where specific attributes are synthesized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QhKz4IigeH", "forum": "2VuPBAH94k", "replyto": "2VuPBAH94k", "signatures": ["ICLR.cc/2026/Conference/Submission5637/Reviewer_QnGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5637/Reviewer_QnGY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298351132, "cdate": 1761298351132, "tmdate": 1762918169963, "mdate": 1762918169963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies block-wise interactions in MMDiT-style diffusion transformers by systematically (i) removing blocks, (ii) disabling text conditions, and (iii) enhancing text hidden states in selected blocks. Using these insights, the authors propose training-free tweaks for text-image alignment, instruction-based editing (with Stable-Flow-style parallel generation and self-attention injection), and inference acceleration by skipping low-impact blocks. Main reported gains appear on T2I-CompBench++ and GenEval, while quality metrics (HPSv2, Aesthetics) are maintained. The acceleration scheme skips blocks and can combine with TeaCache."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis of remove/disable/enhance triad leads to intuitive takeaways: earlier blocks capture semantics (e.g., color, spatial relations) while later blocks refine details; disabling text is more damaging than dropping a block; and targeted enhancement can help.\n\n- The tweak is training-free and easy to use -- it is just element-wise scaling of text hidden states in selected blocks (optionally with token-level masks, Eq. 5), with no architectural changes.\n\n- The method improves Shape/Texture/Color on T2I benchmarks and delivers competitive editing results against Stable Flow, supported by both metrics and human preference."}, "weaknesses": {"value": "- The work appear to be incremental wrt Stable Flow. In addition, its editing setup also closely follows Stable Flow—parallel generation from source/target prompts and self-attention injection—with the main novelty being attribute-aware block selection and text amplification. The approach hinges on identifying “pivotal” blocks per attribute (color/amount/spatial). It is unclear how to scale this to more open-ended instructions without an impractically broad probing stage; the paper’s core analyses and reported wins emphasize those three structured categories.\n\n- Token-level masks (Eq. 5) are mainly demonstrated for the editing setup and amount-related attributes; broader trials for general T2I or diverse instructions are not shown in the main text (details are largely in supplementary token-localization and small amount-focused tables).\n\n- The paper positions itself as MMDiT-focused, but key techniques (sequence-level text scaling after text–image concatenation) also apply to DiT variants. The work does not analyze how multi-stream vs single-stream designs (e.g., early multi-stream/late single-stream mixes as reported for FLUX) affect cross-modal Q/K/V/MLP interactions; the mechanism-level picture remains surface-level, even though the paper assumes joint attention over concatenated tokens."}, "questions": {"value": "- How would you automate block selection for open-ended instructions beyond color/amount/spatial, without a costly per-task probing pass? Any proxy predictors that generalize?\n\n- For multi-stream vs single-stream hybrids (e.g., FLUX), can you provide evidence on where the gains come from in terms of Q/K/V/MLP pathway interactions? Does the “pivotal block” distribution shift compared to fully single-stream MMDiT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tZUpByVhcf", "forum": "2VuPBAH94k", "replyto": "2VuPBAH94k", "signatures": ["ICLR.cc/2026/Conference/Submission5637/Reviewer_UY8D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5637/Reviewer_UY8D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957319143, "cdate": 1761957319143, "tmdate": 1762918169608, "mdate": 1762918169608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}