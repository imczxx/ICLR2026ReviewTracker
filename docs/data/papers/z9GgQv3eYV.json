{"id": "z9GgQv3eYV", "number": 3081, "cdate": 1757329595621, "mdate": 1759898110086, "content": {"title": "Less is Not Worse: Effective Reasoning Without Complete Reasoning Traces", "abstract": "Large language models (LLMs) often produce lengthy reasoning traces with substantial token redundancy.\nWhile reasoning processes are widely adopted to tune LLMs as a post-training regime, it has been underexplored whether LLMs truly learn from the complete trajectory, particularly in supervised fine-tuning (SFT). We argue that, for mid-size LLMs commonly trained with SFT for reasoning, using full reasoning trajectories may harm performance because their limited capacity increases susceptibility to redundant intermediate steps.\nTo investigate, we first analyze the redundancy in thinking trajectories through attention maps and controlled token-removal studies, both of which show that intermediate tokens contribute minimally to reasoning quality.\nOur analyses suggest that the most redundant segments typically appear in the middle of reasoning traces, whereas the earlier and later segments are crucial for generating high-quality final outcomes.\nWe further posit that avoiding redundant intermediate information leads to exploiting the capability of LLMs to infer concise and coherent intermediate steps by utilizing the known start and end points.\nBased on the insights, we propose MidCut, a method that removes redundant middle steps during both training and inference. We demonstrate the effectiveness of MidCut in two scenarios for LLM reasoning: (1) SFT trained on s1K and OpenThoughts datasets for reasoning; and (2) decoding strategy for a test-time application.", "tldr": "", "keywords": ["reasoning trajectory", "large language model", "supervised fine-tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/413b36cfd49c0b45fb483332ea2020d6b646ebb9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MidCut, a method that removes redundant middle steps during both training and inference. The author demonstrate the effectiveness of MidCut in two scenarios for LLM reasoning, a new SFT training for reasoning; and a new decoding strategy for a test-time application."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The author finds a new index, or called rule, to reduce redundant reasoning patterns, and it can also improve the model’s ability to carry out intermediate reasoning when needed."}, "weaknesses": {"value": "There are so many papers which aims at finding the overthinking or underthinking in LRM. So many index and so many rules. Actually, the algorithms behind this are so similar. There are far too many such papers, making it even impossible to compare this type of work with similar ones. While this is indeed a very serious issue with LRM, I believe the current paper is more like a homework assignment than a paper accepted by a conference. It falls below the typical bar of ICLR."}, "questions": {"value": "There are far too many such papers, making it even impossible to compare this type of work with similar ones. While this is indeed a very serious issue with LRM, I believe the current paper is more like a homework assignment than a paper accepted by a conference. It falls below the typical bar of ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sQBEi0dEJs", "forum": "z9GgQv3eYV", "replyto": "z9GgQv3eYV", "signatures": ["ICLR.cc/2026/Conference/Submission3081/Reviewer_ZoqT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3081/Reviewer_ZoqT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761112693670, "cdate": 1761112693670, "tmdate": 1762916543149, "mdate": 1762916543149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MidCut, a method that removes intermediate reasoning steps during both training and inference, enabling large language models to use more compact reasoning traces without sacrificing accuracy.\n\nThe central insight is that redundant content predominantly appears in the middle portions of reasoning sequences, whereas the early and final segments are essential for maintaining reasoning quality and correctness.\n\nExperimental results demonstrate that MidCut-SFT enhances accuracy while substantially reducing the number of training tokens, and that MidCut-Decoding—by trimming 25% to 75% of intermediate reasoning steps—achieves nearly identical performance compared to full reasoning traces."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides an insightful analysis of the attention weight patterns within reasoning traces, offering empirical evidence that supports the motivation and effectiveness of the proposed approach.\n2. The proposed MidCut-SFT and MidCut-Decoding methods present complementary training- and inference-time solutions, respectively, enabling improvements in both model training and inference efficiency."}, "weaknesses": {"value": "1. The experimental evaluation lacks diversity in model architectures. All experiments are conducted on the Qwen series models (Qwen2.5-32B, Qwen3-8B, Qwen3-4B), leaving the generalizability of the MidCut approach to other model families (e.g., LLaMA series) unverified.\n2. The selected datasets (AIME, MATH, GPQA-D) are domain-specific and heavily focused on mathematical and scientific reasoning. The effectiveness of MidCut on more general-purpose or language-centric tasks remains unexplored.\n3. The paper focuses primarily on accuracy metrics, without providing experimental evidence on efficiency-related aspects such as training time, inference throughput, or latency (e.g., first-token generation time). These measurements are essential to fully validate the claimed improvements in efficiency."}, "questions": {"value": "1. Has the effectiveness of the MidCut approach been verified on other model architectures, such as the LLaMA series? It would be valuable to understand whether the proposed method generalizes beyond the Qwen family.\n2. How does MidCut perform on broader and simpler tasks (e.g., TruthfulQA)? Would aggressively trimming intermediate reasoning traces lead to noticeable performance degradation when tasks require shorter or less complex reasoning?\n3. What are the concrete advantages of MidCut-SFT in terms of actual training efficiency, such as total training time or resource consumption?\n4. How does MidCut-Decoding affect inference efficiency, particularly in terms of throughput and first-token generation latency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qqkqDuyPjz", "forum": "z9GgQv3eYV", "replyto": "z9GgQv3eYV", "signatures": ["ICLR.cc/2026/Conference/Submission3081/Reviewer_nRie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3081/Reviewer_nRie"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634212327, "cdate": 1761634212327, "tmdate": 1762916542980, "mdate": 1762916542980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the redundancy and its impact in the Chain-of-Thought (CoT) reasoning used to train Large Language Models (LLMs). Through attention-based and token-removal analyses, it is found that the middle part of the reasoning trajectory typically contains the most redundant segments, while the beginning and end segments are crucial for generating high-quality final results. Based on this insight, the paper proposes a simple method called MidCut to synchronously prune redundant intermediate steps during both training and inference. The authors demonstrate the effectiveness of MidCut in two scenarios: MidCut-SFT (a data preprocessing technique for training) and MidCut-Decoding (an inference-time strategy). The results indicate that this method can improve both inference performance and training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple and Effective Method: The work systematically reveals the existence of redundant information in reasoning trajectories and, consequently, proposes a very simple, low-cost, and easily reproducible method. It performs only \"region-level\" trimming without requiring additional scorers or RL controllers, bringing consistent benefits across multiple models/datasets, proving its practical effectiveness.\n2. Sufficient Empirical Validation: The paper provides strong experimental support for the redundancy of intermediate steps through attention weight analysis and knockout experiments, demonstrating that LLMs pay less attention to intermediate steps and that removing them has a minimal impact on the final answer quality.\n3. Comprehensive Experiments: The authors conduct a comprehensive evaluation of MidCut on various models (Qwen series), datasets (s1K-1.1 and OpenThoughts3), and challenging reasoning benchmarks (AIME24, GPQA-D, MATH). MidCut-SFT consistently outperforms the baseline (training with full trajectories) and other trimming strategies (e.g., random removal or LLM-based compression), strongly supporting the method's value. The inference stage also benefits, as MidCut-Decoding achieves almost the same accuracy as the full trajectory when trimming 25%–75% of the middle segment, implying direct computational savings on the deployment side."}, "weaknesses": {"value": "1. The paper emphasizes that MidCut benefits medium-sized LLMs due to their limited capacity, but lacks direct comparative experiments with larger or smaller models to clearly delineate the scope of MidCut's applicability across different model scales. If experiments showed the advantage of MidCut disappearing on larger models, it would more strongly support \"capacity limitation\" as the key factor.\n2. The best-performing variant, \"step-level filtering,\" relies on preserving the first and last $$n$$ steps. Appendix B.3 sets $$n=100$$ and $$n=200$$ for the two datasets, respectively, but lacks explanation for this choice or related experimental analysis. How sensitive is model performance to variations in $$n$$? How should the optimal $$n$$ be selected for a new dataset? Including an analysis of this hyperparameter sensitivity would make the paper more convincing.\n3.  A variant of MidCut-SFT is similarity filtering, aimed at removing repetitive reasoning patterns. However, Table 3 shows that similarity filtering performs worse than simple step-length or token trimming. Analysis should be provided for the failure of similarity filtering to explain why content-based semantic filtering is less effective than simple position-based trimming."}, "questions": {"value": "1. Section 4.2 and the conclusion mention that MidCut-Decoding can reduce computational overhead and latency. However, the paper only shows its impact on accuracy but does not provide actual efficiency improvement data (e.g., percentage reduction in generated tokens during inference, specific latency reduction times, or throughput improvements). Supplementing experiments with this data is suggested to more fully demonstrate its \"effectiveness.\"\n2. The discovered \"U\"-shaped attention curve (i.e., the beginning and end are important, the middle is not) is very similar to the \"Lost in the Middle\" phenomenon in long-context processing. Is there a possible connection between the two phenomena? Including a discussion on this could increase the depth of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KSzZtuta2g", "forum": "z9GgQv3eYV", "replyto": "z9GgQv3eYV", "signatures": ["ICLR.cc/2026/Conference/Submission3081/Reviewer_ShYq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3081/Reviewer_ShYq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891985714, "cdate": 1761891985714, "tmdate": 1762916542789, "mdate": 1762916542789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a simple yet effective method to improve training accuracy and inference efficiency by simply remove intermediate reasoning steps of offline collected SFT trajectories and online generated thinking steps. MidCut-SFT improves accuracy than the LLM-based compression method and random compression with fewer training tokens. The authors only report the accuracy preserving of MidCut-Decoding for inference. What are the the inference efficiency and other advantages of MidCut-Decoding?\n\nGiven the widely studied underthinking and overthinking mechanism of LRMs, removing intermediate reasoning during SFT data processing is straightforward. More clarification of the novelty is needed.\n\nFour simple reasoning trajectory filtering methods are mentioned in the main part but not compared in the main results. In addition, it is not clear how \"ours\" is defined in Table 1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Motivated by the relatively lower importance of intermediate thinking segments, the authors propose to remove the redundant middle steps during both training and inference to improve the training effectiveness and inference efficiency. The method is simple but effective. \n\nThe proposed MidCut-SFT is more effective than the LLM-based compression and random compression baselines on MATH and science datasets."}, "weaknesses": {"value": "1. LLM based compression only compress the center content of the whole reasoning trajectories by 50%. It would be more fair to instruct LLMs to directly process and compress the whole trajectories with the same compression ratio settings.\n\n2. The settings of the \"Base\" baseline are not clear. Is it the open-sourced LLMs or the fine-tuned versions of them using the whole and long trajectories?\n\n3. More evaluations on general datasets are recommended to analyze the effects of the proposed SFT data processing method.\n\n4. Data pre-processing and cleaning is extremely critical for LLM training. More comparison of other existing data filtering methods is recommended to validate the effectiveness of the proposed filtering method."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y3k8NSa5Ra", "forum": "z9GgQv3eYV", "replyto": "z9GgQv3eYV", "signatures": ["ICLR.cc/2026/Conference/Submission3081/Reviewer_UqZg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3081/Reviewer_UqZg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999272105, "cdate": 1761999272105, "tmdate": 1762916542566, "mdate": 1762916542566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}