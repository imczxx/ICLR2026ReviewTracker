{"id": "Q6gNNXEb3D", "number": 11800, "cdate": 1758203911982, "mdate": 1759897554022, "content": {"title": "Efficient Blockwise Diverse Active Learning", "abstract": "Active learning (AL) techniques are known for selecting the most informative data points from large datasets, thereby enhancing model performance with fewer labeled samples. This makes AL particularly useful in tasks where labeling is limited or resource-intensive. However, most existing effective methods rely on uncertainty scores to select samples, often overlooking diversity, which results in redundant selections, especially when the batch size is small compared to the overall dataset. This paper introduces Efficient Blockwise Diverse Active Learning (EBDAL), a generalizable framework that combines uncertainty with diversity-based selection to overcome these limitations. By partitioning the dataset into blocks via a clustering strategy, we ensure diverse sampling within each block, enabling more efficient handling of large-scale datasets. To quantify diversity, we minimize the Maximum Mean Discrepancy (MMD) between the selected subset and the full dataset, which is then reformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem. The resulting QUBO is submodular, which permits an efficient greedy algorithm. We further demonstrate feasibility on real quantum hardware through an end-to-end selection experiment. Our experimental results demonstrate that EBDAL not only improves the accuracy of uncertainty-based strategies but also outperforms a wide range of selection methods, achieving substantial computational speedups. The findings highlight EBDAL’s robustness, efficiency, and adaptability across various datasets.", "tldr": "", "keywords": ["Active Learning; Subset Selection; quantum acceleration; Maximum Mean Discrepancy"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a0c7b38ff27f681a594cfc7c8d0f47d0e80f28a.pdf", "supplementary_material": "/attachment/d93d84a28f90d86921840cefcb8951ee2ed92080.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces Efficient Blockwise Diverse Active Learning, or EBDAL, a framework that combines uncertainty-based sample selection with diversity to improve the efficiency of active learning. Instead of relying solely on uncertainty, EBDAL ensures representativeness by minimizing the Maximum Mean Discrepancy between the selected subset and the full dataset. The dataset is divided into clusters or blocks, and the method performs diversity-based selection within each block, which allows it to scale to large datasets while maintaining diversity. The MMD objective is formulated as a Quadratic Unconstrained Binary Optimization problem, enabling efficient greedy optimization and compatibility with quantum solvers. Theoretical analysis connects MMD minimization with data coverage, showing that the blockwise objective provides a close approximation to the global one."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a clear and well-motivated integration of uncertainty and diversity, addressing a common problem in active learning where purely uncertainty-driven sampling leads to redundancy. \n\nThe theoretical development is rigorous and builds a strong link between MMD minimization and data coverage, providing an interpretable reason for why the method improves performance. \n\nThe inclusion of ablation studies makes the contribution of each component transparent."}, "weaknesses": {"value": "The evaluation of the paper is not fully aligned with best practices in active learning research. As highlighted by [1] common pitfalls in active learning evaluation include:\n* Lack of evaluated data distribution settings: the paper does not evaluate on imbalanced datasets. \n* Lack of evaluated query sizes: the paper uses a fixed query size per dataset (50 for MNIST, FMNIST and SVHN, 100 for CIFAR10 and CIFAR100)\n* Neglection of the classifier configuration: The paper does not have development datasets on which the hyperparameters were tuned, and roll out datasets where the performance is tested\n* Neglection of alternative training paradigms: pre trained backbones were not tested.\n\nLack of baselines: the paper is lacking baselines like random sampling (no AL), margin selection [2], CoreSet[3], TypiClust [4], TCM [5].\n\nThe use of quantum solvers is only briefly demonstrated and lacks a detailed performance comparison, making it feel more like a side experiment than a core contribution. \nAlthough the paper discusses computational complexity theoretically, empirical timing results are missing, leaving the real-world efficiency unclear.\n\nThe reliance on uncertainty filtering assumes well-calibrated uncertainty estimates, but the paper does not discuss how the method behaves under poor calibration. The K-means clustering used to create blocks is heuristic and may lead to uneven partitions, which can affect the approximation guarantees; no sensitivity analysis is provided to study this. \n\nFrom a novelty standpoint, the theoretical contributions are adaptations of existing ideas from coreset.\n\nTable 1 could include error bars or standard deviations to reflect variability.\n\nNo codebase provided, making it hard to reproduce the results.\n\n[1] Lüth, Bungert, Klein, and Jaeger (2023) propose a systematic evaluation framework for active learning and analyze common pitfalls in AL literature.\n\n[2] Bahri, Dara; Jiang, Heinrich; Schuster, Tal; Rostamizadeh, Afshin (2022). Is margin all you need? An extensive empirical study of active learning on tabular data. arXiv preprint arXiv:2210.03822.\n\n[3] Sener, Ozan and Savarese, Silvio (2017). Active Learning for Convolutional Neural Networks: A Core-Set Approach. arXiv preprint arXiv:1708.00489. \n\n[4] Hacohen, Guy; Dekel, Avihu; Weinshall, Daphna (2022). Active Learning on a Budget: Opposite Strategies Suit High and Low Budgets. In Proceedings of the 39th International Conference on Machine Learning (PMLR). \n\n[5] Doucet, Paul; Estermann, Benjamin; Aczél, Till; Wattenhofer, Roger (2024). Bridging Diversity and Uncertainty in Active Learning with Self-Supervised Pre-Training (TCM). arXiv preprint arXiv:2403.03728."}, "questions": {"value": "Does it outperform old and suggested baselines on new datasets, other query sizes and with pre-trained models?\n\nHow robust is EBDAL to miscalibrated uncertainty estimates, especially when uncertainty filtering is the first step of selection?\n\nCan the authors provide empirical runtime comparisons between EBDAL and baselines to substantiate the claimed computational efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBd7LUczfF", "forum": "Q6gNNXEb3D", "replyto": "Q6gNNXEb3D", "signatures": ["ICLR.cc/2026/Conference/Submission11800/Reviewer_hxXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11800/Reviewer_hxXJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659249802, "cdate": 1761659249802, "tmdate": 1762922822670, "mdate": 1762922822670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a blockwise active-learning scheme that (i) filters a high-uncertainty candidate set per block, then (ii) selects within each block to minimize MMD between the selected subset and the pool. The MMD objective is rewritten as a QUBO; the authors argue submodularity of a corresponding quadratic set function permits a greedy method, and they also claim an \"end-to-end\" selection on real quantum hardware. Experiments on five small/medium image datasets show consistent AUC gains over uncertainty-only baselines, plus ablations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a formal approximation bound (Theorem 2) to justify the blockwise optimization of MMD. \n2. The blockwise design is practical, offering a computationally efficient alternative to optimizing over the entire candidate set at once. The paper also provides a theoretical analysis of the error bound for this blockwise MMD approximation.\n3. The empirical results across several datasets consistently support the effectiveness of the proposed method compared to the chosen baselines."}, "weaknesses": {"value": "1. The core idea of combining uncertainty sampling with MMD-based diversity is well-established in the active learning literature [1-3]. Formulating the MMD objective as a quadratic program is also a known technique. The primary contribution appears to be the blockwise optimization, which feels more like an engineering improvement for efficiency rather than a fundamental algorithmic advance.\n\n2. While the blockwise approach improves efficiency, it is a relatively standard heuristic for managing the high computational complexity of the full optimization. Furthermore, the motivation for selecting an equal number of points from each block is not well-justified. It's unclear why this constraint is beneficial or how it interacts with the underlying data distribution. It is more like to meet the condition in the theoretical analyses.\n\n3. The rationale for objective (4), which appears to select a set with minimal training loss, is unconvincing. Minimizing training loss on a candidate set does not necessarily correlate with improved generalization error for the model. This objective seems counter-intuitive, as it might favor \"easy\" samples the model already understands, rather than informative ones that would improve its decision boundary.\n\n4. The efficiency of the proposed method has not been well justified. The algorithm requires re-clustering the entire unlabeled pool $\\mathcal{D}_U$ at every AL round (Algorithm 1, step 5). This K-means step has a high computational cost. Besides, a comprehensive wall-clock time benchmark for a full acquisition step is missing (Algorithm 1, steps 4-13), including uncertainty calculation, the $O(N)$ clustering step, kernel computation, and the final selection.\n\n[1] Chattopadhyay, Rita, et al. \"Batch mode active sampling based on marginal probability distribution matching.\" Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. 2012.\n\n[2] Wang, Zheng, and Jieping Ye. \"Querying discriminative and representative samples for batch mode active learning.\" Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. 2013.\n\n[3] Wang, Zengmao, et al. \"Incorporating distribution matching into uncertainty for multiple kernel active learning.\" IEEE Transactions on Knowledge and Data Engineering 33.1 (2019): 128-142."}, "questions": {"value": "Why was the Uherding (Bae et al., 2025) baseline, which is cited as recent work, omitted from the experiments on MNIST and FashionMNIST in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bsk4528W6g", "forum": "Q6gNNXEb3D", "replyto": "Q6gNNXEb3D", "signatures": ["ICLR.cc/2026/Conference/Submission11800/Reviewer_HHYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11800/Reviewer_HHYM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731838372, "cdate": 1761731838372, "tmdate": 1762922822133, "mdate": 1762922822133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Efficient Blockwise Diverse Active Learning (EBDAL)**, which combines uncertainty filtering with a blockwise MMD-based selection strategy. The core idea is to partition the unlabeled pool into several blocks, solve a QUBO-formulated MMD diversity objective in each block, and aggregate the selected samples to approximate global coverage with reduced complexity (`O(sum n_b^2)`).  \nThe paper presents theoretical analyses connecting coverage probability `ρ(r)` with a cross-term `δ_MMD`, a blockwise approximation theorem for global MMD, and empirical results showing faster runtime and comparable accuracy to full MMD selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a creative integration of uncertainty filtering and diversity sampling.  \n- The blockwise formulation is computationally appealing and intuitive.  \n- The presentation and visual layout are professional and easy to read."}, "weaknesses": {"value": "- Multiple inconsistencies across Lemma 1, Theorem 1, and Theorem 2. Key terms are undefined, and some inequalities are mathematically invalid or too loose.  \n- The experimental setup is weak and does not convincingly support the efficiency or effectiveness claims.  \n  - Datasets are small (CIFAR-10, SVHN), lacking large-scale or non-visual tasks.  \n  - Performance improvements are marginal (1–2%) with overlapping standard deviations.  \n  - Baselines are outdated.  \n  - No ablation study to isolate the effects of blockwise partitioning, uncertainty filtering, or QUBO optimization.  \n  - Efficiency claims lack reproducibility (no batch-size, cache, or memory metrics; “0.2 ms per quantum call” is unsupported).  \n-  Theorems assume fixed kernel space, while implementation uses embedding space.  \n- The paper presents heuristic ideas as formal theorems, which overstates the level of rigor."}, "questions": {"value": "1. Please clarify the relationship between `p` and `1−ε` in Lemma 1. Are they independent parameters or linked via coverage probability?  \n2. Can the authors define `δ_MMD` explicitly and show conditions ensuring `ρ(r) ≤ 1`?  \n3. Why is the factor “2” required in Theorem 2? Would `MMD^2(S,X) ≤ Σ_b w_b MMD^2(S_b,X_b)` not suffice?  \n4. Under what settings (λ range, normalization) is the QUBO objective truly submodular?  \n5. Are the theoretical results intended to apply to the embedding space used in experiments? If so, how does varying `σ_b` affect the derived bounds?  \n6. Could the authors provide reproducible runtime measurements, hardware configurations, and statistical tests to validate the efficiency claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v4jBScorIX", "forum": "Q6gNNXEb3D", "replyto": "Q6gNNXEb3D", "signatures": ["ICLR.cc/2026/Conference/Submission11800/Reviewer_61SX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11800/Reviewer_61SX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796162242, "cdate": 1761796162242, "tmdate": 1762922821571, "mdate": 1762922821571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EBDAL (Efficient Blockwise Diverse Active Learning), a framework that explicitly integrates both uncertainty scores and diversity scores when selecting samples for active learning.   EBDAL’s contribution using splitting the dataset into blocks to speed up processing and contribute toward diversity and ensuring diversity within each block.  Diversity is quantified by using Maximum Mean Discrepancy (MMD) within each block.  The required computations are formalized as a Quadratic Unconstrained Binary Optimization (QUBO) problem, whose objective function is submodular.  A bounded approximation guarantee showing that blockwise MMD closely tracks the global MMD, providing a theoretical foundation for the approach.  Experimental results are given on standard image datasets, including implementing the QUBO algorithm on a quantum computer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- a bounded approximation guarantee showing that blockwise MMD closely tracks the global MMD,  which provides a good theortical foundation for the algorithm\n\n- investigating a quantum QUBO implementation of EBDAL\n\n- The EBDAL algorithm was clearly described"}, "weaknesses": {"value": "- the related work should have included a better description of other active learning algorithms that integrate uncertainty scores and diversity scores and how EBDAL differs (perhaps VAAL, SIMILAR, BMDR, etc.)\n\n- the connection with quantum computing implementations of active learning in general and QUBO/Ising in particular did not seem to me to be well integrated with the rest of the paper, but almost an after thought\n\n- in several of the experiments the advantages of EBDAL seem relatively minor compared to the baseline (* vs *-EBDAL) and given the complexity of implementing EBDAL, it wasn't obvious to me that the extra effort was justified"}, "questions": {"value": "- I think the paper would be stronger if the quantum implementation of the QUBO algorithm was described with a bit more context and more detail was provided.  As presented it seemed almost an afterthought, not one of the main contributions as explained (line 82-85)\n\n- one of the main contributions is described (lines 73-74) \"substantially reduces small-batch redundancy,\" but I didn't see in the paper how that was quantified"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "g9YLwVpuWg", "forum": "Q6gNNXEb3D", "replyto": "Q6gNNXEb3D", "signatures": ["ICLR.cc/2026/Conference/Submission11800/Reviewer_yb4g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11800/Reviewer_yb4g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950632228, "cdate": 1761950632228, "tmdate": 1762922821090, "mdate": 1762922821090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}