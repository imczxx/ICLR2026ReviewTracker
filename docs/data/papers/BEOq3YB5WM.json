{"id": "BEOq3YB5WM", "number": 7191, "cdate": 1758011061781, "mdate": 1763036427362, "content": {"title": "MUOT-CLIP: Enhancing Few-Shot Adaptation of CLIP via Inter- and Intra- Modality Unbalanced Optimal Transport", "abstract": "Contrastive Language-Image Pre-training (CLIP)  has demonstrated remarkable zero-shot capabilities across a variety of domains. To enhance its performance in data-scarce settings, few-shot adaptation methods have been developed. Other than fine-tuning the parameters (e.g., the adapter-based approach), prompt learning methods learn proper prompts to minimize the distance between the visual feature and the textual feature. Optimal Transport (OT) has proven highly effective as a measurement metric for evaluating the feature space of CLIP. However, classical OT, which forces equality constraints on both the source and target weights of the transport plan, is susceptible to noises (e.g., the misleading local regions in images and unrelated words in prompts). Furthermore, both the adapter-based and prompt learning methods usually overlook the modality gap existing in the feature space and thus risk to obtain suboptimal performance. In this paper, we extend the formulation of classical OT to unbalanced optimal transport (UOT) for better measurement. The UOT based distance measure can filter out noises adaptively. To boost the few-shot adaptation performance, a framework that measures both the inter- and intra- **M**odality distance based on **UOT** for **CLIP** is proposed, which is termed **MUOT-CLIP**. In addition, a scalable UOT solver with entropy regularization term is used for the efficient optimization of the model. Compared with the state-of-the-art methods, MUOT-CLIP consistently exhibits favorable performance on the few-shot classification benchmark of 11 datasets.", "tldr": "Enhancing the few-shot adaptation performance of CLIP via inter- and intra- modality unbalanced optimal transport.", "keywords": ["Vision-Language Models", "Few-Shot Classification", "Prompt Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d4e3e16ff3f5378e21be2723bcccabf9aef03dc9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MUOT-CLIP, a novel prompt learning framework that enhances the few-shot adaptation capability of CLIP by formulating both inter-modality (text-image) and intra-modality (image-image) distance measures using Unbalanced Optimal Transport (UOT). The authors argue that classical OT, as used in prior work like PLOT, is suboptimal for this task as it cannot filter out noisy or misleading features. MUOT-CLIP introduces a three-step process: UOT-guided prompt learning, prototype image retrieval, and a combined UOT-based inference, and demonstrates state-of-the-art performance on 11 few-shot classification benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of replacing Classical OT with UOT for measuring feature distances in few-shot learning is well-motivated and novel. The argument that UOT can adaptively filter noise by relaxing marginal constraints is convincing and addresses a clear weakness in the PLOT method.\n\nThe paper provides extensive experiments across 11 diverse datasets and multiple shot settings (1 to 16). The results are compelling, showing consistent and often significant improvements over strong baselines, including adapter-based (Tip-Adapter, CLIP-Adapter), prompt-based (CoOp, PLOT), and linear probing (LP++) methods.\n\nThe combination of inter- and intra-modality UOT during inference is a clever design to mitigate the modality gap issue. The ablation studies (Table 2) convincingly show that both components contribute positively to the final performance."}, "weaknesses": {"value": "Comparison to Broader Baselines: While the selected baselines are well-established, the field of efficient adaptation for vision-language models has progressed rapidly. To more firmly position the contribution of MUOT-CLIP within the current state-of-the-art, I would recommend including comparisons with more recent works from 2024 (and even 2025, if available). It would provide a more up-to-date and compelling performance benchmark. In addition, the comparison is comprehensive within the specific line of CLIP adaptation, but it could be slightly broader. For instance, a comparison with a simple fine-tuning baseline is missing. \n\nClarity on Retrieval Mechanism: The Memory and Retrieval (M&R) step for obtaining prototype images is described at a high level (e.g., \"full retrieval or d(F_i^s, G_k^s) guided partial retrieval\"). The specific implementation used for the main results and its impact on performance could be described more precisely."}, "questions": {"value": "In the M&R step for intra-modality distance, what was the exact retrieval strategy (full or partial) used for the main results in Table 1? Was the performance sensitive to this choice, and did you experiment with more sophisticated retrieval mechanisms?\n\nThe parameter μ in Eq. (8), which balances the inter- and intra-modality distances, is crucial. How was this parameter set in your experiments? Was it tuned per dataset/shot, or was a fixed value used? An ablation on its sensitivity would be informative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A6jVGzFiYz", "forum": "BEOq3YB5WM", "replyto": "BEOq3YB5WM", "signatures": ["ICLR.cc/2026/Conference/Submission7191/Reviewer_eVu4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7191/Reviewer_eVu4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620876734, "cdate": 1761620876734, "tmdate": 1762919346522, "mdate": 1762919346522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "M6Cmsn2vFF", "forum": "BEOq3YB5WM", "replyto": "BEOq3YB5WM", "signatures": ["ICLR.cc/2026/Conference/Submission7191/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7191/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763036426698, "cdate": 1763036426698, "tmdate": 1763036426698, "mdate": 1763036426698, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MUOT-CLIP, a few-shot adaptation framework for CLIP that introduces Unbalanced Optimal Transport (UOT) to measure both inter-modality and intra-modality distances. Compared with the classical OT, MUOT-CLIP relaxes the equality constraints of OT to adaptively filter such noise and mitigate the modality gap between visual and textual embeddings. The authors further develop a scalable Sinkhorn-like UOT solver with entropy regularization for efficiency. Experiments on 11 standard datasets demonstrate the effectiveness of the proposed MUOT-CLIP."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear; the authors identify meaningful shortcomings of classical OT and modality gap in CLIP adaptation.\n2. The paper is clearly written overall with good structure, which is easy to follow.\n3. The paper provides detailed and convincing theoretical analysis for the proposed UOT formulation. The derivations of the optimization process and the scalable Sinkhorn-like solver are well presented."}, "weaknesses": {"value": "1. The related work section provides only a brief overview of Optimal Transport and lacks sufficient discussion. A deeper review would strengthen the paper’s technical grounding and contextual relevance.\n2. The experiments are only conducted on CLIP-ResNet-50, which restricts the generality of the conclusions. Results on other common backbones such as CLIP-ViT are missing, as well as evaluations on benchmarks with distribution shift, like ImageNet-A/V/K/S. Including these results would provide stronger evidence for the robustness and scalability of the proposed method.\n3. Most of the baselines are from 2022–2024, while several recent few-shot or prompt-learning approaches are not included. This limits the credibility of the claimed SOTA performance. The authors should update the comparisons to reflect the current landscape of CLIP adaptation methods.\n4. Replacing classical OT with unbalanced OT is a relatively straightforward extension rather than a fundamentally new framework. The architectural design largely follows existing work, with only minor modifications in the transport formulation. Therefore, the authors need to carefully demonstrate the novelty of the proposed method."}, "questions": {"value": "please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AdmfiZ5z1N", "forum": "BEOq3YB5WM", "replyto": "BEOq3YB5WM", "signatures": ["ICLR.cc/2026/Conference/Submission7191/Reviewer_r8Ja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7191/Reviewer_r8Ja"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707350251, "cdate": 1761707350251, "tmdate": 1762919346236, "mdate": 1762919346236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MUOT-CLIP, a novel prompt-based optimal transport (OT) method for few-shot classification in VLMs. Unlike classical OT, the method relaxes transport constraints via UOT to mitigate noisy distance measurements. Additionally, the authors introduce inter- and intra-modality UOT-based inference. Experiments on few-shot classification benchmarks confirm its effectiveness."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-\tThe written and figures are clear."}, "weaknesses": {"value": "-\tThe motivation is unclear. The reason for using Optimal Transport (OT) to address few-shot adaptation under CLIP is not well justified. \n-\tThere are significant issues with the reported results. The Tip-Adapter-F performance in Table 1 does not match the original paper’s findings (e.g., the original reports 75.81 average accuracy for 16-shot, while this paper’s results diverge).\n-\tThe experiments are insufficient:\n    - Only few-shot classification results are provided. Additional evaluations (e.g., OOD generalization, base-to-new generalization, or results across different architectures) are needed to demonstrate the method’s effectiveness.\n    - The baselines are outdated (2022–2023 works). Comparisons with recent state-of-the-art methods are lacking. Notably, some training-free approaches [1,2] even outperform this work in few-shot classification.\n    - Need additional ablation of UOT and OT.\n-\tTraining time and GPU memory usage comparisons between MUOT-CLIP and other baselines should be included.\n-\tThe proposed approach appears to combine the drawbacks of both prompt-based and cache-based methods, resulting in slow training and additional memory consumption.\n\n[1] ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models. In CVPR 25.\n[2] A hard-to-beat baseline for training-free clip-based adaptation. In ICLR 24."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xavJDk3199", "forum": "BEOq3YB5WM", "replyto": "BEOq3YB5WM", "signatures": ["ICLR.cc/2026/Conference/Submission7191/Reviewer_oj6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7191/Reviewer_oj6h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985278194, "cdate": 1761985278194, "tmdate": 1762919345763, "mdate": 1762919345763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "See Questions"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "See Questions"}, "weaknesses": {"value": "See Questions"}, "questions": {"value": "After reading the paper, I have the following comments and suggestions. I hope the authors could address them carefully.\n\n- Q1. The abstract is overly long in its background and motivation, while the technical description of the proposed method is minimal—only about three lines. A more balanced structure is recommended, with a clearer summary of the core contributions.\n\n- Q2. The motivation presented in the abstract—“classical OT, which forces equality constraints on both the source and target weights of the transport plan, is susceptible to noises”—is not clearly explained. Additionally, the illustrative example mentioned should be accompanied by a corresponding figure in the main text to aid reader understanding.\n\n- Q3. CLIP was introduced in 2021, and CLIP-based few-shot learning has since become a very active area of research. However, the related work section mainly discusses well-known methods such as CoOp, CoCoOp, and TIP-Adapter, which is insufficient. Many recent and strongly relevant works are missing from the discussion. For example, include but not limited to: SgVA-CLIP [1], Amu-tuning [2], TIMO [3].\nA more comprehensive and up-to-date literature review is necessary.\n\n- Q4. The paper does not clearly explain how Figure 1 is generated. More details about the setup and methodology behind the figure are needed.\n\n- Q5. I am curious about how the proposed MUOT method is implemented in code. The paper would benefit from a more transparent explanation of its computational aspects or implementation details.\n\n- Q6. Minor issues:\n\n(a) There are extra spaces before punctuation marks (e.g., Line 151).\n\n(b) Some symbols and abbreviations (e.g., i.e.) are not properly italicized.\n\n\nOverall, the paper is well presented. I will consider adjusting my score based on the authors' rebuttal and how well they address the concerns above.\n\n-------\n\n[1] Sgva-clip: Semantic-guided visual adapting of vision-language models for few-shot image classification. TMM 2023.\n\n[2] Amu-tuning: Effective logit bias for clip-based few-shot learning. CVPR 2024\n\n[3] TIMO: Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot Classification with CLIP. AAAI 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "See Questions"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4CzKmKOeB8", "forum": "BEOq3YB5WM", "replyto": "BEOq3YB5WM", "signatures": ["ICLR.cc/2026/Conference/Submission7191/Reviewer_xR3t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7191/Reviewer_xR3t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762505410180, "cdate": 1762505410180, "tmdate": 1762919344823, "mdate": 1762919344823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}