{"id": "c7rcjMTe4e", "number": 14223, "cdate": 1758230583195, "mdate": 1759897382826, "content": {"title": "Boosting Adversarial Robustness and Generalization with Dictionary Structure", "abstract": "This work investigates a \nnovel approach to boost adversarial robustness and generalization by incorporating structural prior into the design of deep learning models.\nSpecifically, our study surprisingly reveals that existing dictionary learning-inspired convolutional neural networks (CNNs) provide a false sense of security against adversarial attacks. To address this, we propose Elastic Dictionary Learning Networks (EDLNets), a novel ResNet architecture that significantly enhances adversarial robustness and generalization. \nExtensive and reliable experiments demonstrate consistent and significant performance improvement on open robustness leaderboards such as RobustBench, surpassing state-of-the-art baselines. To the best of our knowledge, this is the first work to discover and validate that \ndictionary structure can reliably enhance deep learning robustness under strong adaptive attacks, unveiling a promising direction for future research.", "tldr": "", "keywords": ["Adversarial Robustness", "Structural Prior", "Adversarial Training"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e55d753d503ab44a8a781a1c291636e066be4e8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper revisits dictionary learning as a potential structural prior to improve adversarial robustness. The authors first show that prior dictionary-learning-based CNNs exhibit a false sense of security. Then, this paper proposes Elastic Dictionary Learning (EDL) to balance nature and robust performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ Theory is consistent with good logic.\n+ The RISTA optimization is technically sound and provides convergence guarantees."}, "weaknesses": {"value": "This paper has drawbacks in both writing and experiments.\n\nThe presentation is unclear. I had to read the Introduction multiple times to understand what problem the paper is actually trying to formulate.\n+ The introduction mixes multiple ideas, such as robustness plateau, reliance on generative data, robust overfitting, and dictionary priors, but lacks a coherent logical flow. I recommend authors to take a look of C.A.R.S.[1] to improve the Introduction part.\n+ The phrase 'a false sense of security' is used with different meanings. I can understand that in the experiment section, it refers to gradient obfuscation. But in Section 3.2, what does this refer to (line 150)? Authors should include the citation of  [2].\n+ In line 145, what is the setting of this kind of adaptive attack? It's necessary to identify the adaptive attack's settings based on [3].\n\nExperiments:\n+ All experiments are conducted on CIFAR-10/100 and Tiny-ImageNet with small ResNet backbones, which limits the generality of the conclusions.\n+ Based on Table 7, the proposed Elastic DL layer introduces roughly 50% additional inference cost compared with standard CNNs, which is not practical to implement.\n+ Whether this method can be extended to Vision Transformers or other attention-based models?\n\n[1] Swales, John. \"Create a research space (CARS) model of research introductions.\" Writing about writing: A college reader (2014): 12-15.\n\n[2] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" International conference on machine learning. PMLR, 2018.\n\n[3] Tramer, Florian, et al. \"On adaptive attacks to adversarial example defenses.\" Advances in neural information processing systems 33 (2020): 1633-1645."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XouT4m7wUR", "forum": "c7rcjMTe4e", "replyto": "c7rcjMTe4e", "signatures": ["ICLR.cc/2026/Conference/Submission14223/Reviewer_zDRN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14223/Reviewer_zDRN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955048383, "cdate": 1761955048383, "tmdate": 1762924678489, "mdate": 1762924678489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that prior dictionary-learning-inspired CNNs (e.g., SDNet) give a false sense of robustness: they handle random corruptions but collapse under adaptive attacks. It proposes Elastic Dictionary Learning (EDL) layers that replace convolutions. Each EDL layer solves, for a feature tensor $x$, a mixed $\\ell_2-\\ell_1$ reconstruction with sparsity,\n\n$$\\min_z \\frac{\\beta}{2}\\left\\|x-A^{\\star}(z)\\right\\|_2^{2}+\\frac{1-\\beta}{2}\\left\\|x-A^*(z)\\right\\|_1+\\lambda\\|z\\|_1,$$\n\nand is unrolled with a reweighted ISTA (RISTA) update that uses per-iteration weights $w=1 /\\left(2\\left|x-A^*(z)\\right|\\right)$. Layer-wise $\\beta$ is learned. The authors claim substantial gains in robust accuracy across CIFAR-10/100 and Tiny-ImageNet, often when combining EDL with adversarial training baselines (PGD-AT, TRADES, AWP, HAT, PORT). They present mitigation of robust overfitting (Table 3), strong AutoAttack numbers (Tables 4-6), multidataset/backbone ablations (Tables 8-10), certified robustness via randomized smoothing (Fig. 6), and simple checks intended to rule out gradient obfuscation (transfer attacks, a zero-order gradient match in Table 12)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1- Clear diagnosis + constructive fix. The paper empirically shows Vanilla DL (SDNet) ails under PGD even when good against random noise (Table 1, p. 3) and then designs EDL to trade off $\\ell_2$ and $\\ell_1$ fidelity (Alg. $1+$ Eq. (6), p. 5).\n\n2- Simple, modular layer that plugs into ResNets. Fig. 1 (p. 4) and Fig. 12 (p. 15) clearly show the drop-in EDL layer and the end-to-end training pipeline (Fig. 11, p. 14). This architectural modularity is attractive for adoption.\n\n3- Broad empirical sweep. Results cover three datasets and four backbones, multiple norms (‚Ñì‚àû/‚Ñì2/‚Ñì1), and combinations with diverse AT baselines. In particular, Table 4 (p. 7) shows sizable AutoAttack gains when adding EDL to PGD-AT/TRADES/HAT/PORT; Tables 5‚Äì6 (p. 7) compare to RobustBench-style leaderboards at multiple budgets.\n\n4- Overfitting mitigation. Table 3 (p. 6) and Fig. 2 (p. 7) show that EDL reduces the BEST‚ÄìFINAL gap and lifts final robust accuracy compared with popular regularizers and data augments.\n\n5- Interpretability signals. The paper provides embedding-difference profiles (Fig. 5, p. 8) and detailed hidden-state visualizations (Figs. 19‚Äì20, pp. 27‚Äì28), which suggest attacks affect EDL representations less.\n\n6- Some robustness hygiene. The authors include transfer-attack comparisons (Fig. 7, p. 8) and a zero-order vs autograd gradient agreement (Table 12, p. 26), and they plot RISTA convergence of the unrolled layer (Fig. 8, p. 9).\n\n7- Runtime disclosure. Table 7 (p. 9) reports inference costs as EDL layers are stacked; overhead is 1‚Äì3√ó vs plain ResNets and only modest vs Vanilla DL."}, "weaknesses": {"value": "1- Evaluation transparency gaps (AA settings, AT details).\nAutoAttack. While AA results are reported (Tables 4‚Äì6), the exact AA configuration (version, components enabled, checks for catastrophic overstatement) is not fully specified in the main text. Precise Œµ-schedules, per-attack budgets, and restarters for PGD are scattered; PGD settings for the leaderboard comparison are summarized but could still allow optimistic numbers if not standardized.\n\n2- Potential confounding from training protocol. The key robustness curves switch from Vanilla DL pretraining to EDL fine-tuning at epoch 150 (Fig. 2 and Fig. 14, pp. 7, 21). It is unclear whether the gains come from the structural prior itself or from regularization effects of unrolling, reset-like dynamics, or extra optimization steps. A controlled study that trains EDL from scratch under the same schedule as baselines is missing.\n\n3- Theory is promising but incomplete.\nConvergence/conditioning. The RISTA update depends on weights $w=1 /(2 \\mid x- \\left.A^*(z) \\mid\\right)$. There is no regularization floor to prevent blow-up when the residual is near zero, and step-size/Lipschitz conditions for global convergence of the nonsmooth, reweighted problem are not provided. Lemma 4.1 (p. 4) gives a local quadratic upper bound but end-to-end convergence guarantees (with learned $A, \\beta$ ) are absent.\n\n4- Obfuscation checks are not exhaustive. The zero-order match (Table 12, p. 26) shows local gradient agreement, and transfer attacks are included (Fig. 7), but EOT for any stochasticity, black-box query-budget tests, step-size sensitivity, and multi-restart PGD sweeps are not systematically presented. Given the large reported gains, a fuller obfuscation checklist is warranted.\n\n5- Accounting for capacity \\& parameters. Replacing convolutions with EDL (unrolled iterations, extra tensors $w$, learnable $\\beta$ ) likely changes parameter counts and memory footprint. Only runtime is quantified (Table 7); parameter and activation memory overheads, training wall-time, and throughput are not. Fairness of comparisons-especially to SOTA AT-needs these numbers.\n\n6- Certified robustness methodology is under-specified. Fig. 6 (p. 8) shows better certified accuracy with randomized smoothing, but the noise level ùúé, base classifier, sample counts, and confidence computation are not detailed, making it hard to assess comparability to standard smoothing reports.\n\n7- Generality beyond small images. All results are on CIFAR-10/100 and Tiny-ImageNet; no ImageNet-1k training, detection/segmentation, or non-vision tasks. The approach might scale, but evidence is missing. (A few ImageNet visualization examples appear in Fig. 22, p. 30, but not full training.)\n\n8- Ablations on $\\beta$ and unrolling depth. The layer-wise learnable $\\beta$ is central, yet there is no distributional analysis of learned $\\beta$ across depth, no freezing vs learning ablation, and limited study of T (number of ISTA steps), step-sizes $t$, or the shrinkage schedule $\\lambda_t$.\n\n9- Reconstruction claims need stronger quantification. The recovered-noise analysis (Table 13, p. 29; Figs. 21‚Äì24) is interesting but uses aggregate norms vs ‚Äúadaptive noise.‚Äù It does not show per-example causal links between better noise recovery and AA success/failure, nor how this behaves under distribution shift."}, "questions": {"value": "Please resolve the aforementioned weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DfZs8b74Gp", "forum": "c7rcjMTe4e", "replyto": "c7rcjMTe4e", "signatures": ["ICLR.cc/2026/Conference/Submission14223/Reviewer_9QCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14223/Reviewer_9QCz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996943069, "cdate": 1761996943069, "tmdate": 1762924678056, "mdate": 1762924678056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes combining Convolutional Neural Networks (CNNs) with Dictionary Learning (DL) to improve model performance and robustness. By integrating a DL module into a CNN, the network learns features that are both discriminative for classification and sparse and reconstructable, making them more stable under noise or adversarial attacks. This approach enhances the model‚Äôs ability to capture meaningful structures in data while maintaining strong accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper includes extensive experiments to support its main idea, including ablation studies, visualizations, and comparisons with baseline models. These experiments demonstrate how each component of the proposed method contributes to improved performance and help illustrate the interpretability and robustness of the learned features."}, "weaknesses": {"value": "Dictionary Learning (DL) models require more complex and computationally intensive calculations compared to regular neural networks. Even with unrolled inference, the EDL layer adds approximately 2‚Äì3√ó more computation than a standard convolution block. However, their overall performance is generally lower, as traditional DL methods are difficult to train end-to-end and struggle to match the efficiency and scalability of standard deep neural networks."}, "questions": {"value": "How does it compare with a regular CNN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lx26qt6pff", "forum": "c7rcjMTe4e", "replyto": "c7rcjMTe4e", "signatures": ["ICLR.cc/2026/Conference/Submission14223/Reviewer_zRTd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14223/Reviewer_zRTd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998311536, "cdate": 1761998311536, "tmdate": 1762924677617, "mdate": 1762924677617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}