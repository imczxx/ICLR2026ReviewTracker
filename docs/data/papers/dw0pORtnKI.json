{"id": "dw0pORtnKI", "number": 3756, "cdate": 1757513897480, "mdate": 1759898071661, "content": {"title": "MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning", "abstract": "Reasoning-augmented machine learning systems have shown improved performance in various domains, including image generation. However, existing reasoning-based methods for image generation either restrict reasoning to a single modality (image or text) or rely on high-quality reasoning data for fine-tuning. To tackle these limitations, we propose MILR, a test-time method that jointly reasons over image and text in a unified latent vector space. Reasoning in MILR is performed by searching through vector representations of discrete image and text tokens. Practically, this is implemented via the policy gradient method, guided by an image quality critic.\nWe instantiate MILR within the unified multimodal understanding and generation (MUG) framework that natively supports language reasoning before image synthesis and thus facilitates cross-modal reasoning. The intermediate model outputs, which are to be optimized, serve as the unified latent space, enabling MILR to operate entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE, achieving state-of-the-art results on all benchmarks. Notably, on knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over the baseline by 80%. Our further analysis indicates that joint reasoning in the unified latent space is the key to its strong performance. Moreover, our qualitative studies reveal MILR's non-trivial ability in temporal and cultural reasoning, highlighting the efficacy of our reasoning method.", "tldr": "A training-free test-time latent reasoning method that jointly optimizes text and image latent representations for state-of-the-art multimodal generation.", "keywords": ["Image Generation", "Test-Time", "Latent Reasoning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c5a002df37d0b7631cd8b4c34b00964d4cb7a1d.pdf", "supplementary_material": "/attachment/9454af5e4a1a8ccb68a094ba5768cce083567fdc.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MILR, a test-time reasoning method that enhances image generation by jointly reasoning over image and text in a unified latent vector space. Utilizing a policy gradient method guided by an image quality critic, MILR operates within the Multimodal Understanding and Generation (MUG) framework, optimizing intermediate model outputs at test time without fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "# Strengths:\n(+) MILR achieves superior performance across all tested benchmarks, outperforming both training-based and test-time reasoning models.\n\n(+) The method's test-time optimization avoids the need for curated reasoning data or model fine-tuning, making it cost-effective and practical.\n\n(+) Qualitative studies highlight MILR's nontrivial abilities in geometric, temporal, and cultural reasoning, enhancing its versatility."}, "weaknesses": {"value": "# Weakness:\n\n(-) The reliance on a reward model for optimization may introduce bias, potentially limiting exploration of the generative capacity of MUG.\n\n(-) The method's efficiency and performance depend on empirical hyperparameter settings (e.g., λt = 0.2, λv = 0.02), which may not generalize across all scenarios.\n\n(-) The paper lacks a detailed discussion on computational costs and scalability, which could be a concern for large-scale applications. Besides, this paper claims latent test-time reasoning. I wonder about its cost (GPU Memory, Per Time, Reward Loss across time and step).\n\n# Minor Weakness:\n\n(-) Fig. 2 appears to have an issue, as it suggests the reward model only receives the final image, while the text tokens should also be input to assess compatibility accurately.\n\n(-) The format of data input to the reward model is unclear; if it takes in data format like final text and final image, the computational load seems excessive—why not directly calculate the loss based on the latent vector z?"}, "questions": {"value": "# Questions:\n1. How does MILR handle cases where the reward model provides inconsistent feedback across iterations?\n\n2. What are the potential impacts of varying λt and λv values on different types of image generation tasks?\n\n3. Can MILR be adapted to work with other MUG frameworks beyond Janus-Pro, and if so, what modifications would be required?\n\n4. How does the method perform on real-time image generation tasks with strict latency constraints?\n\n5. I read the code in the supplementary material. Are there plans to release the code to reproduce this work?\n\n\nOverall, I think this work is interesting. If the author could address my concerns, I am willing to increase my rating to 8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g2lmmwgVBy", "forum": "dw0pORtnKI", "replyto": "dw0pORtnKI", "signatures": ["ICLR.cc/2026/Conference/Submission3756/Reviewer_Cur9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3756/Reviewer_Cur9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761209846984, "cdate": 1761209846984, "tmdate": 1762916968260, "mdate": 1762916968260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MILR, a test-time optimization method for text-to-image generation that performs joint reasoning over text and image in a unified latent space. Concretely, the authors take intermediate hidden states z = [z(t); z(v)] from a unified multimodal understanding-and-generation (MUG) model (Janus-Pro; Chen et al., 2025) and iteratively update a prefix of the text and image token latents using REINFORCE-style policy gradients (Williams, 1992) to maximize a reward measuring text–image compatibility. The approach leaves model weights frozen and only optimizes latents at inference time. The paper reports strong gains on three benchmarks—GenEval (Ghosh et al., 2023), T2I-CompBench (Huang et al., 2023), and WISE (Niu et al., 2025)—and analyzes hyperparameters (prefix lengths λt≈0.2, λv≈0.02; steps up to 16) and ablations (text-only, image-only).\n\n  Empirically, Janus-Pro-7B+MILR achieves 0.95 overall on GenEval (vs. 0.78 base Janus-Pro-7B; larger category jumps include Counting +0.34, Position +0.21, Attribute Binding +0.27), outperforms test-time strategies like PARM and ReflectionFlow (both ~0.91), and approaches training-time RL methods such as Flow-GRPO (0.95). On T2I-CompBench, MILR improves overall from 0.3921 to 0.5325; on WISE, from 0.35 to 0.63 (the paper states +80% over its base and +16.7% over T2I-R1). The authors discuss failure modes (textual/visual reasoning collapse, reward hacking) and note reliance on reward models, sometimes using the benchmark’s evaluator (“oracle” reward)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear technical core: optimizing a prefix of multimodal token latents z(t), z(v) via policy-gradient updates at test time, without finetuning parameters. The optimization target and the forward path are precisely stated (Eq. (3)–(6)).\n- Strong empirical gains across three widely used evaluations: GenEval (NeurIPS 2023; object-focused alignment), T2I-CompBench (NeurIPS 2023; compositionality), and WISE (Niu et al., 2025; knowledge-intensive prompts). Results are specific and category-level improvements are reported.\n- Solid ablations: text-only vs. image-only vs. joint, token prefix ratios (λt, λv), and step counts; joint optimization consistently performs best and compute scaling is characterized (best around 16 steps).\n- Practicality: method is test-time-only and uses a single A100 in reporting; no curated reasoning data required, contrasting with GRPO/DPO-style training (e.g., Flow-GRPO; Liu et al., 2025a; T2I-R1; Jiang et al., 2025).\n- Thoughtful discussion of failure modes (reasoning collapse, reward hacking) with qualitative evidence; acknowledges reward-model dependence."}, "weaknesses": {"value": "- Reward reliance / oracle leakage: Many claims hinge on using the benchmark’s evaluator as the reward (“OracleReward”), which risks overfitting/reward hacking and inflating benchmark scores (authors themselves show mis-evaluations in spatial relations). More evidence with non-oracle, public reward models would strengthen claims of generality beyond GenEval’s metric.\n- Base-model coupling: All main results use Janus-Pro. The approach should be validated on a diffusion-based MUG (e.g., Show-o or a diffusion-tokenizer system) to test portability; current evidence is limited to one AR MUG paradigm.\n- Baseline fairness and compute: Best-of-N and ReflectionFlow/PARM comparisons need tighter compute normalization. The paper mentions “comparable compute (N=T=20)” with early stopping, but wall-clock, sample counts, and variance should be reported per benchmark to rule out search budget confounds.\n- Unified-latent novelty vs. prior latent/test-time reasoning: Prior latent-space test-time computation and “latent reasoning” lines (e.g., Geiping et al., 2025; Hao et al., 2024; Shen et al., 2025; Dao & Gu, 2024) are cited but the empirical isolation of “unified cross-modal latent” as the key driver remains partial. The ablation “w/o image” vs. “w/o text” helps, but an ablation that replaces unified latents with modality-specific latents in separate loops would more directly test the unified-space hypothesis.\n- Reward models and robustness: MixedReward and other non-oracle critics improve over baseline but still lag the oracle. Robustness across unseen prompts/domains (outside the three benchmarks) is not demonstrated.\n - Tuning on GenEval validation: λt/λv tuned on a GenEval split (then used elsewhere) may introduce slight bias. Cross-validated tuning or tuning-once on a separate development set would reduce concerns."}, "questions": {"value": "- Compute fairness: For Best-of-N, PARM, and ReflectionFlow, please report per-sample wall-clock time, total forward passes, and early-stop statistics. Are the search budgets strictly matched across methods and benchmarks?\n  - Generality to diffusion MUG: Can you show MILR with a diffusion-based unified generator (e.g., Show-o or a diffusion-tokenizer model) to demonstrate portability beyond Janus-Pro?\n  - Reward robustness: Can you report full results with only non-oracle rewards (e.g., MixedReward) across all three benchmarks, including failure modes and qualitative examples? Any signs of reward hacking under these critics?\n  - Unified vs. separated latent loops: Could you add an ablation that optimizes text and image latents in separate spaces without sharing a unified latent layer, to isolate the benefit of the unified representation more directly?\n  - Sensitivity to prefix choice: Beyond contiguous prefixes, did you try structured subsets (e.g., entropy or attention-based selection for z(v)) that may better target global-structure tokens? You mention random subsets are worse; a principled selection could help.\n  - Out-of-benchmark generalization: Any tests on prompts outside the evaluator’s training (for OracleReward), or user study-aligned metrics (e.g., paired preference) to complement automated scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ytSoAJtnWg", "forum": "dw0pORtnKI", "replyto": "dw0pORtnKI", "signatures": ["ICLR.cc/2026/Conference/Submission3756/Reviewer_ncVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3756/Reviewer_ncVe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627563105, "cdate": 1761627563105, "tmdate": 1762916968089, "mdate": 1762916968089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MILR, a novel method designed to improve text-to-image generation. The core contribution is a test-time optimization technique that reasons jointly over text and image representations within a unified latent space. Instead of refining raw text or image pixels, MILR operates on the intermediate latent vectors of a pre-trained MUG model. It employs a policy gradient method to iteratively update these latent vectors before they are decoded into the final output."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel and elegant proposal for test-time latent reasoning, which enhances powerful pre-trained models without requiring any fine-tuning.  \n\nImpressive empirical results across multiple challenging benchmarks."}, "weaknesses": {"value": "The term \"reasoning\" is debatable here. The process is more accurately described as a guided latent space search that optimizes a latent vector to maximize a reward, rather than a structured, logical thought process (like reasoning defination in LLM).\n\nThe most significant flaw is that the headline, state-of-the-art results reported in Tables 1 and 2 are achieved using the benchmark's own evaluation toolkit as the reward model. This  setup is not representative of any realistic application. In practice, a perfect reward function is never available."}, "questions": {"value": "Could you elaborate on the choice of REINFORCE as the optimization algorithm and the choice of the pre-trained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FPj1Aylp4V", "forum": "dw0pORtnKI", "replyto": "dw0pORtnKI", "signatures": ["ICLR.cc/2026/Conference/Submission3756/Reviewer_uX5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3756/Reviewer_uX5G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922692407, "cdate": 1761922692407, "tmdate": 1762916967857, "mdate": 1762916967857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce MILR, a novel method for improving multimodal image generation during test time. In this approach, they perform reasoning in latent space jointly over unified image and text. The use REINFORCE method to optimize the intermediate latent vectors, with the reward model that assesses how well the generated image aligns with the input.\nThey show that MILR significantly improves the performance of Janus-Pro models on several benchmarks, achieving state of the art results."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Their approach of performing joint reasoning in latent space is very interesting and novel, as well as, achieves state of the art performance.\n- The paper is well written, and the experiments are thorough."}, "weaknesses": {"value": "- The authors did not discuss about latency. Given that their technique involves iterative optimization at test time, there will be significant increase in latency.\n- This method is heavily dependent on the reward function. There is a possibility of reward hacking, as authors also mentioned. One way to resolve it would be to add a regularizer like penalize semantic incoherence."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZC0RHGfyHh", "forum": "dw0pORtnKI", "replyto": "dw0pORtnKI", "signatures": ["ICLR.cc/2026/Conference/Submission3756/Reviewer_bDbb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3756/Reviewer_bDbb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973079824, "cdate": 1761973079824, "tmdate": 1762916967345, "mdate": 1762916967345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}