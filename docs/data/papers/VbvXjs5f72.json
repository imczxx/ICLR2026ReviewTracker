{"id": "VbvXjs5f72", "number": 16872, "cdate": 1758269737011, "mdate": 1759897214429, "content": {"title": "Beyond Masks: Efficient, Flexible Diffusion Language Models via Deletion-Insertion Processes", "abstract": "While Masked Diffusion Language Models (MDLMs) relying on token masking and unmasking have shown promise in language modeling, their computational efficiency and generation flexibility remain constrained by the masking paradigm. In this paper, we propose Deletion-Insertion Diffusion language models (DID) that rigorously formulate token deletion and insertion as discrete diffusion processes, replacing the masking and unmasking processes in current MDLMs. DID improves training and inference efficiency by eliminating two major sources of computational overhead in MDLMs: the computations on non-informative 1) $\\texttt{\\<MASK\\>}$ tokens inherent to its paradigm, and 2) $\\texttt{\\<PAD\\>}$ tokens introduced in variable-length settings. Furthermore, DID offers greater flexibility by: 1) natively supporting variable-length sequences without requiring fixed-length padding, and 2) an intrinsic self-correction mechanism during generation due to insertion that dynamically adjusts token positions. To train DID, we design a score-based approach that assigns scores to token insertion operations and derive appropriate training objectives. The objectives involve subsequence counting problems, which we efficiently solve via a parallelized dynamic programming algorithm. Our experiments across fixed and variable-length settings demonstrate the advantage of DID over baselines of MDLMs and existing insertion-based LMs, in terms of modeling performance, sampling quality, and training/inference speed.", "tldr": "", "keywords": ["diffusion language model", "deletion-insertion process", "denoising score entropy"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3250018b6defbac0dcfa34f3756a3b316bcac7fd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Deletion-Insertion Diffusion (DID) language models, which formulate token deletion and insertion as discrete diffusion processes. This approach eliminates the need for MASK and PAD tokens, thereby improving computational efficiency and generative flexibility over standard Diffusion Language Models (DLMs). The authors report that their experiments demonstrate DID's superior efficiency and flexibility compared to MDLMs and other insertion-based baselines across tasks in language/length modeling, generation quality, and training/inference speed."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* The paper is exceptionally thorough, and the appendix provides numerous additional interesting findings.\n* The proposed method effectively addresses two significant limitations of existing diffusion language models (DLMs): managing variable-length generation and the computational waste associated with MASK tokens.\n* The application of dynamic programming for training diffusion models is original and creative"}, "weaknesses": {"value": "* The discussion of prior work on insertion-based LLMs feels underdeveloped. This concept has been explored in several papers (e.g., [1, 2, 3]), and the current draft would benefit from a more thorough discussion beyond the most recent related work.\n\n* The choice of evaluation metrics could be strengthened. The use of GPT2-XL as a base model is questionable given the existence of much better pretrained models. Furthermore, the use of unigram entropy seems less informative than computing a joint entropy, which could also be accomplished using an LLM. While these choices may follow existing conventions, adopting more robust metrics and baselines would significantly increase the credibility of the results.\n\n* Appendix H does include the examples of RADD. A more direct qualitative comparison could be achieved by conditioning on OWT prefixes from the validation set and sampling from each model.\n\n[1] Lu, Sidi, Tao Meng, and Nanyun Peng. \"Insnet: An efficient, flexible, and performant insertion-based text generation model.\" Advances in Neural Information Processing Systems 35 (2022): 7011-7023.\n\n[2] Gu, Jiatao, Changhan Wang, and Junbo Zhao. \"Levenshtein transformer.\" Advances in neural information processing systems 32 (2019).\n\n[3] Stern, Mitchell, et al. \"Insertion transformer: Flexible sequence generation via insertion operations.\" International Conference on Machine Learning. PMLR, 2019."}, "questions": {"value": "1. Could the authors please provide the exact training and inference algorithms, ideally in pseudocode? While they are presumably similar to SEDD, explicit algorithms would greatly improve clarity.\n\n2. What are the precise inputs and outputs to the Transformer model during training? Given that the sequence length changes, how is potential GPU underutilization (due to variable batch sizes or padding) managed?\n\n3. As seen in Table 2, DID's generative perplexity appears to plateau (relative to RADD) at a high number of denoising steps (e.g., 512 and 1024). Do the authors have an explanation for this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AdjYrPeoSi", "forum": "VbvXjs5f72", "replyto": "VbvXjs5f72", "signatures": ["ICLR.cc/2026/Conference/Submission16872/Reviewer_cWDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16872/Reviewer_cWDw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613306141, "cdate": 1761613306141, "tmdate": 1762926894503, "mdate": 1762926894503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a diffusion language model that features a deletion based forward process and an insertion based reverse process. A core advantage of the proposed framework lies in that it does not require the modeling of mask tokens, thus achieving variable-length modeling while obtaining speedup. The training objective is derived in a principle manner from score entropy, and a DP algorithm has been proposed to help efficiently obtain the loss function. Experiments show that DID achieves lower perplexity while unlocking variable-length modeling and bringing certain speedup in training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed insertion-deletion based diffusion framework is promising that enables variable-length modeling and relieves from modeling mask tokens.\n\n2. The empirical performance of the model is competitive. It achieves lower perplexity compared with fixed-length mask diffusion models.\n\n3. The additional speedup in training brought by the model is favorable."}, "weaknesses": {"value": "1. Lack of investigations on inference strategies. For instance, it would be interesting to show how the sample quality changes with the number of inference steps (or equivalently, inference-compute).\n\n2. More theoretical insights and discussions on the connections/advantages over MDLMs are missing. Please refer to Q2, 3 and 4 for more details."}, "questions": {"value": "1. How does the proposed approach perform under different inference-time compute budgets (e.g. number of steps)?\n\n2. It is still a bit unclear what exactly helps DID to achieve lower perplexity than MDLMs. Is it fundamentally due to the removal of mask tokens or something else? Adding more theoretical insights would be helpful.\n\n3. What is the connection between DID and MDLMs in terms of modeling? Is it possible to construct a one-one mapping for each deletion-insertion process vs mask/unmask process in MDLMs? If not, what leads to the gap?\n\n4. I am a bit confused why it is beneficial (or at least claimed to be) to remove PAD tokens. To me having PAD tokens does have some benefits even for DID since the model will then force to stop when it inserts a PAD token which may mitigate certain ambiguity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jqYHdGmYy7", "forum": "VbvXjs5f72", "replyto": "VbvXjs5f72", "signatures": ["ICLR.cc/2026/Conference/Submission16872/Reviewer_orpS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16872/Reviewer_orpS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985381133, "cdate": 1761985381133, "tmdate": 1762926894087, "mdate": 1762926894087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DID, a novel algorithm for text generation using discrete diffusion models, which enables the generation of variable-length text through an insertion-deletion process. DID is theoretically grounded with an ELBO-like training objective and demonstrates promising performances on both fixed-length text and variable-length text generation tasks. DID enjoys decent inference and training speed compared with pure mask-based discrete diffusion approaches, suggesting the method's advantage."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- DID is a novel, theoretically grounded approach that tackles the important problem of variable-length generation of texts using discrete diffusion, bridging an important gap to the traditional AR methods.\n- The technical details are sound with a self-contained derivation.\n- The numerical experiments are comprehensive, and the results are promising."}, "weaknesses": {"value": "- While the experiments are comprehensive, the results could be made more convincing by comparing with more discrete diffusion model baselines other than MDM, such as MDM-prime [1], Block Diffusion [2], etc. \n- The paper could use more space to discuss differences & advantages compared with other variable-length discrete diffusion approaches. The existing discussion is nice, but it's not entirely clear what the advantage of DID is compared to existing approaches, especially EditFlow [3], which also uses an insertion-deletion process. \n- The paper claims that the auxiliary process introduced in EditFlow is ineffective and introduces additional variance, but I feel like DID suffers from a similar issue due to the need to compute the number $N(x_t, x_0)$. These both originate from the randomness that connects $x_t$ to $x_0$, and it's unclear to me at this moment why the DID approach is better.\n- The paper could comment more on the speed-up (in both training and inference) of DID compared with traditional MDM, and have a more thorough analysis of why DID enjoys such acceleration.\n\n\nReferences:\n[1] Chao, Chen-Hao, et al. \"Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking.\" \n[2] Arriola, Marianne, et al. \"Block diffusion: Interpolating between autoregressive and diffusion language models.\"\n[3] Havasi, Marton, et al. \"Edit Flows: Flow Matching with Edit Operations.\""}, "questions": {"value": "Besides the points mentioned in the weakness section, I would appreciate clarification on the following questions to better my understanding of the paper:\n- Regarding the computation of $N(x_t, x_0)$, in practice, how much computational overhead does this incur, and is it expensive?\n- Tables 3 & 4 mentioned the training/inference acceleration enjoyed by DID. Can you comment on why DID enjoys such a significant speedup compared to MDM?\n- Table 4 mentions that with 64 NFE, DID can generate sequences with ~180 tokens and have decent ppl. How is DID capable of modeling token interdependence/joint distribution that allows such high-quality fast sampling?\n- How's DID's actual sampling throughput compared with MDM? How much acceleration does it enjoy in GPU clock time in terms of inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rgpTPJdnX0", "forum": "VbvXjs5f72", "replyto": "VbvXjs5f72", "signatures": ["ICLR.cc/2026/Conference/Submission16872/Reviewer_ciUs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16872/Reviewer_ciUs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225770841, "cdate": 1762225770841, "tmdate": 1762926893660, "mdate": 1762926893660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a deletion–insertion diffusion approach for language modeling that replaces mask/unmask steps with a forward deletion process and a reverse insertion process. It introduces a training objective tailored to insertions (with a simplified version for fixed-length data) and a GPU-friendly dynamic-programming routine to make training practical. Experiments on fixed- and variable-length text report competitive or better quality than masked-diffusion baselines at similar compute, faster training and inference, and more faithful control of generated lengths. Overall, it’s a clean reformulation with a credible efficiency story."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear, well-motivated reformulation that naturally supports variable length; practical training and sampling mechanics that fit modern accelerators; consistent efficiency gains with competitive quality; writing is clear and limitations are acknowledged."}, "weaknesses": {"value": "Evidence is concentrated on relatively small models and moderate sequence lengths, so scalability to long contexts and larger models is uncertain; evaluation leans on automatic metrics with a single external scorer and no human assessment; baseline alignment (steps, precision, compute) and ablations could be tighter to isolate where gains come from; the added bookkeeping likely introduces overhead whose impact isn’t fully profiled."}, "questions": {"value": "Could the you comment on how sensitive the results are to the number of denoising rounds and the chosen step schedule?\n\nCan  you share observations about scaling to longer contexts and larger models, including memory use and latency?\n\nWould it be possible to provide runs that more strictly align compute, precision, and sampling settings across baselines to clarify where the improvements originate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HWmUbJM0gG", "forum": "VbvXjs5f72", "replyto": "VbvXjs5f72", "signatures": ["ICLR.cc/2026/Conference/Submission16872/Reviewer_Bfo3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16872/Reviewer_Bfo3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762363737349, "cdate": 1762363737349, "tmdate": 1762926893124, "mdate": 1762926893124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Deletion-Insertion Diffusion (DID), a discrete diffusion paradigm for language modeling. DID replaces the standard masking/unmasking process of masked diffusion language models (MDLMs) with deletion/insertion processes. The key idea is that by deleting tokens in the forward process and inserting them in the reverse process, the model can naturally handle variable-length sequences without need for special mask or pad tokens. THe paper formalizes deletion/insertion as a continuous-time discrete diffusion process, presents a tractable objective for optimization, and presents empirical results that show that DID is faster (training/inference) and better at variable-length modeling than MDLM baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The overall idea of the proposed method is creative and explores a new direction for language diffusion models. I see the main strengths of the paper as follows:\n\n- method: the shift from masking to pure deletion/insertion for diffusion LMs is intuitive, and the mathematical formulation of the insertion score and the derivation of the objective via subsequence counts seem correct\n- efficiency: the paper makes a strong case for the inefficiency of MDLMs due to mask and pad tokens, and the reported speedups (up to about 3x for training, and about 4x for inference) are substantial and practically important\n- clarity: the paper is very well-written, albeit quite dense; the motivation is clear, the derivation of the method from standard discrete diffusion makes sense, and the connection between the math and the implementation is well-explained"}, "weaknesses": {"value": "I believe there are a few weaknesses:\n\n- experiments: the experiments are relatively smalls-scale (OpenWebText, Stories) compared to state-of-the-art LLM research; while this is fine for providing a proof of concept, it's unclear if the reported efficiency gains hold or if new instabilities arise at larger scales\n- complexity: this may be a nitpick, but the DP approach seems much more complicated and harder to implement than, say, standard cross-entropy or MDLM objectives"}, "questions": {"value": "- Are there any unique notable failure modes of the proposed method? Does the approach ever get stuck inserting repetitive loops in the middle of sentences?\n- How does the computational overhead of the DP approach scale with the sequence length?\n- Please comment on the weaknesses listed above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yBiyu9DDFU", "forum": "VbvXjs5f72", "replyto": "VbvXjs5f72", "signatures": ["ICLR.cc/2026/Conference/Submission16872/Reviewer_YTEE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16872/Reviewer_YTEE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762754297954, "cdate": 1762754297954, "tmdate": 1762926892786, "mdate": 1762926892786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}