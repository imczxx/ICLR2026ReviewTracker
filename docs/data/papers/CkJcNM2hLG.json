{"id": "CkJcNM2hLG", "number": 12075, "cdate": 1758205523548, "mdate": 1759897535708, "content": {"title": "Analysis of an Idealized Stochastic Polyak Method and its Application to Black-Box Model Distillation", "abstract": "We provide a general convergence theorem of an idealized stochastic Polyak step size called SPS*. Besides convexity, we only assume a local expected gradient bound, that includes locally smooth and locally Lipschitz losses as special cases. We refer to SPS* as idealized because it requires access to the loss for every training batch evaluated at a solution. It is also ideal, in that it achieves the optimal lower bound for globally Lipschitz function, and is the first Polyak step size to have a $\\mathcal{O}(1/\\sqrt{t})$\n anytime convergence in the smooth setting. We show how to combine SPS* with momentum to achieve the same favorable rates for the last iterate. We conclude with several experiments to validate our theory, and a more practical setting showing how we can distill a teacher GPT-2 model into a smaller student model without any hyperparameter tuning.", "tldr": "How to use the knowledge of optimal values to design efficient optimization methods", "keywords": ["Polyak Stepsizes", "anytime convergence", "convex analysis", "primal averaing"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f94e5b107c4c3efefc02c81f4be1138df3d4861c.pdf", "supplementary_material": "/attachment/1d99f851f05c3c18f12c124af37a0b5ee04ec7c7.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a general convergence theorem for an idealized stochastic Polyak step size method, called SPS*. The analysis is for convex function with a local expected gradient bound, it covers both locally smooth and locally Lipschitz losses. SPS* is idealized since it requires access to the loss at the optimal solution. The paper establishes O(1/sqrt(t)) convergence rate for globally Lipschitz functions and Polyak step size. The paper also combines SPS* with momentum, also establishing  O(1/sqrt(t)) convergence rate. There are some experiments, showing comparable performance to SGD and ADAM, but less sensitivity to parameter tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "There is clear novelty in the introduction of the idealized stochastic Polyak step size (SPS*). This particular formulation, together with the general convergence theorem under minimal assumptions, appears to be new and fills a gap in existing research on adaptive step sizes. The authors extend the concept of Polyak step sizes in a way that achieves optimal convergence rates and integrates naturally with momentum, which enhances its theoretical appeal.\n\nAnother strength of the paper is in its focus on reducing sensitivity to hyperparameter tuning, a long-standing issue in stochastic optimization. By proposing a theoretically grounded approach that adapts the step size automatically, the work contributes to developing more robust algorithms that can perform well without extensive manual tuning."}, "weaknesses": {"value": "One weakness of the paper is that SPS* is presented in an idealized form requiring access to the loss evaluated at the optimal solution for each training batch. This makes it impractical in real-world scenarios, where such information is unavailable, and limits the immediate applicability of the proposed method. Although the authors acknowledge this and suggest directions for more practical adaptations, the gap between the idealized theory and an implementable algorithm remains significant.\n\nMoreover, the main application discussed where this step-size selection is possible is model distillation. As far as I know, model distillation is always associated with deep learning where the loss functions are non-convex. All the theoretical results are for convex problems, so the paper, or at least the theory, does not apply to this main application example considered in the paper. So it seems a bit of a speculative contribution, not a good alignment between the mutation and what is actually done.\n\nIt could be discussed better what is the theoretical contribution of the paper, or compare better to the state of the art. The paper is not improving the convergence rate compared to the state of the art, it is just establishing for a particular SPS similar convergence rate can be achieved, or that is my understanding. It would be good if the theoretical results could be put in better context with existing literature."}, "questions": {"value": "For Theorem 3.2 it is highlighted that the convergence rate is for the last iterate. Which is in some sense correct, there is some convergence for last iterate, but since on the left side there is the average of tB(t) for all previous iterations, it is difficult to see how much gap will be created by this term, and it seems to have similar effect as the averaging, or even worse, since to me, this might grow to infinity as T grows, since t is in there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BCYFNNkMnn", "forum": "CkJcNM2hLG", "replyto": "CkJcNM2hLG", "signatures": ["ICLR.cc/2026/Conference/Submission12075/Reviewer_LzVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12075/Reviewer_LzVz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744355956, "cdate": 1761744355956, "tmdate": 1762923047174, "mdate": 1762923047174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the convergence rates of an idealized Polyak step size for stochastic gradient descent, which uses the instantaneous loss at the global optimum. For Lipschitz convex, smooth convex, and strongly convex loss functions, the idealized algorithm converges at the optimal rates, while being adaptive to problem parameters. The paper also presents an anytime version of plain SGD to obtain anytime convergence. The paper demonstrates the application of the proposed method for the interpolation setting and a distillation setup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written. It provides sufficient review of the literature, and provides detailed proofs for most statements, in a structured manner. I like how the authors first derive a master theorem 2.1 with a general conditions, and then specializes it to different settings. I went through most proofs in Appendices B and C smoothly.\n\n2. As far as related works mentioned in the paper, the rates achieved by the current paper is as good or better in settings listed in Table 1."}, "weaknesses": {"value": "While the convergence rates are good, the disadvantage is the assumption on knowing $f_t (x_*)$ in equation (2). While I do appreciate the empirical results of the method on two tasks, theoretically the assumption limits the applicability of the method to general machine learning losses. Given the assumption, it is less surprising that the method achieves better rates than previous methods, which use more easily available quantities in their stepsizes."}, "questions": {"value": "1. See weakness above.\n\n2. With acceleration, in the $L$-smooth setup, SGD can achieve $O(\\frac{LD^2}{t^2})$ rate *on the optimization error* (first term in the rate) while the statistical error (second term) still decays like $O(\\frac{1}{\\sqrt{t}})$. Do you think it is possible to achieve that with the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sRlELeBCXB", "forum": "CkJcNM2hLG", "replyto": "CkJcNM2hLG", "signatures": ["ICLR.cc/2026/Conference/Submission12075/Reviewer_BoT6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12075/Reviewer_BoT6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767537266, "cdate": 1761767537266, "tmdate": 1762923046754, "mdate": 1762923046754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical analysis of an idealized stochastic Polyak step size SPS*, an adaptive method for stochastic optimization. The method is called idealized because it assumes access to the loss value at the optimal solution for every training batch.\n\nThe authors provide a convergence theorem for SPS* and demonstrate that it achieves optimal or best known anytime convergence rates for various classes of convex functions, including Lipschitz and smooth functions. These results only require local assumptions in a ball around the solution.\n\nThe paper introduces IAM, a new variant of SPS* that incorporates momentum. They prove that the last iterate of IAM achieves the same fast and adaptive convergence rates that SPS* achieves for the average iterate, which is often more practical.\n\nThe primary practical application demonstrated is in knowledge distillation. The authors propose using the loss of a large pre-trained teacher model on a given data batch as an approximation for the optimal loss required by SPS* to train a smaller student model. IAM method effectively trains a student GPT-2 model on several language datasets, achieves performance competitive with fine-tuned Adam and SGD, yet IAM does not require tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper has a strong theory. The convergence analysis of SPS* under local assumptions is a valuable contribution to the optimization field. The variant of knowledge distillation is novel, practical and interesting. The IAM method works well without tuning against strong tuned baselines on distilling tasks."}, "weaknesses": {"value": "The practical use-case of the method (distilling models) seems niche to me. The optimal loss value for every training batch is unknown for real ML problems which makes the method indeed idealized.\n\nFurther, the need for the parameter tuning is replaced with selecting and deploying a suitable teacher model. It feels like a more complex dependency to me, not a hyperparameter-free approach at all. This is a far more significant hyperparameter choice than a learning rate.\n\nAlso, the assumption that teachers loss is approximately a students loss at optimum seems like a strong one. If the student model is much smaller than the teacher, it may be incapable of achieving it. \n\nI think the loss landscapes of teacher and student can be very different and teacher can be a poor optimization guide."}, "questions": {"value": "How does the optimizer behave in the regime when the student model is much smaller than the teacher?\n\nHow does the method degrade as the student capacity to match the teacher loss diminishes?\n\nWhat happens if the teacher model is only slightly better than a baseline student?\n\nWhat happens when their architectures are different?\n\nWill the convergence guarantee still hold under the approximation of a batch optimum, or does it converge to a neighborhood of the solution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iqVLEgeXxZ", "forum": "CkJcNM2hLG", "replyto": "CkJcNM2hLG", "signatures": ["ICLR.cc/2026/Conference/Submission12075/Reviewer_hVNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12075/Reviewer_hVNM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906050011, "cdate": 1761906050011, "tmdate": 1762923046301, "mdate": 1762923046301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a variant of Polyak stepsize for stochastic convex optimization, named ideal stochastic Polyak stepsize (SPS*), which computes the on-the-fly step size as $\\gamma_t = (f_t(x_t) - f_t(x_*))_+ / \\\\|g_t\\\\|^2$ with the caveat that the minimum to the stochastic loss $f\\_t$, namely $ f\\_t(x\\_ * ) $, is known. It also incorporates SPS* with momentum and proposes an iterative averaging method named IAM. Moreover, this paper provides concreate theoretical and empirical analysis of the proposed methods. On the theory side, it provides convergence guarantee of both SPS* and IAM under different local assumptions, such as local Lipschitzness, local smoothness, and local strongly convexity. Empirically, this paper provides experiment results on blackbox distillation tasks of language models, which is a task where $f(x\\_*)$ is known. Under this task, SGD and Adam with IAM schedule (which does not require any lr tuning) outperforms other schedules whose base learning rate is carefully tuned."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written. Table 1 clearly highlights previous results in related works and how the new results improve the current literature. The paper also motivates the choice of the exact formula of SPS* and IAM. The main results are also clearly presented are easy to follow.\n- The theoretical analysis is solid and novel. The convergence analysis covers many settings where the loss is assumed to be locally Lipschitz, smooth, or strongly convex. The convergence rates under smooth assumption are in particular notable because these rates exhibit interpolation between an accelerated rate $O(1/t)$ and the standard rate $O(\\sqrt{\\Delta_*}/\\sqrt{t})$ depending on the (dataset) interpolation constant $\\Delta_*$. As $\\delta_*\\to 0$, these rates automatically adapts to the accelerated rate. Moreover, the convergence analysis of SPS with momentum is novel and is rarely studies in prior works.\n- The empirical result on blackbox distillation is encouraging: IAM doesn't require tuning on learning rate, yet it outperforms the cosine decay with warmup schedule that requires a careful tuning on base lr via grid search."}, "weaknesses": {"value": "- As the paper has clearly pointed out, SPS* and IAM requires knowing the stochastic loss minimum $f_t(x_*)$ in each iteration, thus limiting its practical applications other than the interpolation problem and blackbox distillation problem where such minimum is given. However, I personally find this limitation only a minor limitation because SPS* can be a target formula to be learned on-the-fly by more practical schedules.\n- While the convergence analysis only requires an assumption of local bound as defined in eq (7), which later translates to local Lipschitzness or local smoothness assumptions, these assumptions on made on the stochastic loss $f_t$. On the other hand, while previous analysis usually makes global assumptions on Lipschitzness or smoothness, such assumptions are on the expected loss $\\mathbb{E}[f_t]$. Moreover, I find the term \"local\" a bit confusing because it actually refers to the entire bounded domain $B(x_*,D)$ where $D=\\\\|x_0-x_*\\\\|$ instead some small neighborhood around $x_*$, especially when the iterates are monotonically converging to $x_*$. \n- Regarding experiment results, the current baseline only considers constant schedule and cosine decay with linear warmup schedule. However, I think it's worth comparing to other popular schedules for better comparison, such as linear decay and WSD (trapezoid-shape warmup-stable-decay). Especially the latter schedule is observed to be better than cosine annealing for certain tasks."}, "questions": {"value": "- See previous section for most questions.\n- Regarding Thm 3.2, what's the order of the Bregman divergence terms? Does it cancel out certain terms and improves the overall rate if it's moved to RHS of the bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mC2FfTmxln", "forum": "CkJcNM2hLG", "replyto": "CkJcNM2hLG", "signatures": ["ICLR.cc/2026/Conference/Submission12075/Reviewer_oMJJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12075/Reviewer_oMJJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978138605, "cdate": 1761978138605, "tmdate": 1762923045730, "mdate": 1762923045730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}