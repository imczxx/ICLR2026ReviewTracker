{"id": "Fchtg9ejki", "number": 9432, "cdate": 1758122239836, "mdate": 1760542507417, "content": {"title": "Deformable Linear Object Manipulations with Differentiable Physics", "abstract": "We address the challenge of enabling robots to manipulate deformable linear objects (DLOs), such as wires, ropes, and rubber bands. Prior work in this domain has primarily focused on narrow, task-specific problems, often relying on real-world demonstrations or handcrafted heuristics. Such approaches, however, do not scale to the diverse range of materials and tasks encountered in practice, where collecting sufficiently varied real-world data is impractical. Moreover, existing simulation environments provide limited support for the broad spectrum of material behaviors required for generalizable DLO manipulation. To overcome these limitations, we introduce a differentiable physics simulator specifically designed for versatile DLO manipulation. Our simulator models a wide range of material properties—including extensibility, inextensibility, elasticity, bending plasticity, and interactions with both rigid and deformable objects—thereby establishing a robust foundation for learning and evaluating manipulation skills. Building on this simulator, we propose a benchmark suite of representative DLO manipulation tasks that highlight their unique challenges. We further evaluate multiple policy learning algorithms on these tasks. The results show that reinforcement learning can learn closed-loop policies but requires prohibitively large amounts of data. In contrast, trajectory optimization is more efficient: gradient-based methods achieve the best sample efficiency when gradients are available, while sampling-based approaches are broadly applicable but less efficient.", "tldr": "We introduce a differentiable simulation environment and benchmark for robotic learning in the manipulation of deformable linear objects.", "keywords": ["Deformable Linear Object Manipulation", "Differentiable Physics"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f46c70a1c597ec5fd5a872b869a05ca37f3b49e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents DLO-Lab, a differentiable simulator, and nine benchmark tasks. In the evaluation, the authors tried out PPO/SAC, CMA-ES, and gradient-based trajectory optimization on their benchmarks. A single slingshot sim-to-real example is also shown. In addition, the authors adopt an LLM “agent” to decompose tasks into subtasks and outputs grasp points as re-planning after each subtask."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Task suite.** The tasks cover a wide range of linear deformable manipulation (routing, wrapping, separating, slingshot). Short descriptions of rewards for each task are provided.\n- **Method coverage.** RL (PPO/SAC) and planning-based methods (CMA-ES, gradient-based trajectory optimization) are evaluated."}, "weaknesses": {"value": "- **Task difficulty/coverage is unclear.**\nFor RL, generalizability is important, but the paper does not show it in the task design. It is unclear how results change under large or medium randomized initial states and targets (e.g., different ball/box placements and angles in Slingshot). The main text points to an appendix for setups, but the results include no robustness evaluation across diverse configurations.\n\n- **Sim-to-real details are insufficient.** \nThe paper claims the sim-to-real gap is “manageable” based on one Slingshot demo, but there is no description of the system-ID procedure and no statistics on hardware success. Table 3 lists simulation parameters (e.g., slingshot stretching 8e5, bending 1e5), but the paper does not explain how these values were obtained or validated against the real setup. This limits sientific value and reusability.\n\n- **Experiment setup details are missing.**\nNowadays, simulator speed matters, especially for RL. With a massively parallel setup, one can expect PPO to solve these tasks. The problem is that, for deformables, it is not trivial to run as many parallel environments as rigid-body cases. The paper should report #parallel envs, step time/FPS, and wall-clock. These numbers are also essential for the planning baselines to judge how fast (or real-time) they can produce trajectories.\n\n- **Baselines are outdated**\nDeformable manipulation has progressed recently in both model-free RL (HEPi, Hoang et al., ICLR 2025) and model-based methods with simulation gradients (SAPO, Xing et al., ICLR 2025). The authors evaluate only outdated baselines.\n\n- **LLM agent usage is under-specified.**\nSection 4.4 says the agent decomposes the task and outputs grasp points, re-planning after each subtask. However, the paper does not state whether the authors train or use one controller per task or different controllers per subtask, whether subtask-specific rewards are used, or whether the LLM agent is called during RL training rollouts or only at evaluation.\n\n- **Videos for both success and failure cases**.\nFor robotic manipulation, videos are necessary to understand what “success” looks like and to judge jerkiness and/or stability. The paper unfortunately does not provide them.\n\n-------------------------------\n\n**Minor points:**\n\n**Reporting**: Table 2 shows “maximum reward within a fixed number of episodes” with mean ± std, but the number of seeds and exact episode budgets per method are not stated. This weakens interpretability and reproducibility.\n\n**Metrics**: heavy emphasis on reward; success rates would provide better intuition.\n\n-------------------------------\n\n(Hoang et al. 2025). Geometry-aware RL for manipulation of varying shapes and deformable objects. ICLR 2025\n\n(Xing et. al. 2025). Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation. ICLR 2025"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3bXI5yWS6N", "forum": "Fchtg9ejki", "replyto": "Fchtg9ejki", "signatures": ["ICLR.cc/2026/Conference/Submission9432/Reviewer_ind7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9432/Reviewer_ind7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761313894338, "cdate": 1761313894338, "tmdate": 1762921032714, "mdate": 1762921032714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a suite of deformable linear object manipulations asks, which are built with differentiable simulation Genesis. The main difference comparing to peer works lies in the support of plastic deformation and loop topologies. The proposed tasks cover a range of skills of wiring, coiling, wrapping a linear deformable object against some ambient rigid bodies. Reward design for each task is presented. The paper also includes a decomposition of the multi-stage manipulation task by querying an LLM. The performance with RL and evolution strategy methods are reported. A sim-to-real case is demonstrated on the slingshot task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Simulating and benchmarking linear deformable object manipulation is an important topic and has much relevance to machine learning to robotics.\n2. The technical details are easy to grasp. An implementation as an integral part of open-sourced genesis simulation is benefiting to open and reproducible research."}, "weaknesses": {"value": "1. Limited scientific novelty. All technical methods are well known and the work seems like an aggregation of them for the application to linear deformable objects.\n2. Vague contributions. The paper seems trying to include many points that are distant from the core theme of simulating/benchmarking linear deformable manipulation. What is the purpose of the agentic decomposition from LLM? How does it benefit the task scope that the paper is working towards?\n3. Relevance of benchmarking tasks. From a robotics perspective, it is unclear how the proposed task suite is capturing the main challenges/key points in manipulating linear deformable objects. And the tasks appear a bit artificial given it is not hard to find industrial or household cases that have a better resemblance of reality, such as cable plugging or tying shoelaces. \n4. Unclear message from the skill learning results. As a benchmark, I suppose the tasks should challenge the existing standard methods while the results in Figure 4 seem that they are already solvable? Are the increasing return curves indicating task successes here? What would the authors expect the users to do with the benchmark if standard methods are already reaching \"competitive performance\"?"}, "questions": {"value": "1. Can the paper make a concise and clear statement about its scientific contribution?\n2. What is the relevance of the proposed task suite to real-world scenarios?\n3. Can the LLM-based task decomposition and proposed grasping points be shown to be necessary or useful for a benchmark paper?\n4. What are the benefits of the proposed task suites comparing to the peers besides the support of \"plastic bending\" and \"loop topology\". In the end, neither of the two features look like something unimaginable if the exiting \"labs\" got some extensions. Why a new \"lab\" is necessary here? Are there any other arguments from runtime performance and robustness perspectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rdx1s2Ejwa", "forum": "Fchtg9ejki", "replyto": "Fchtg9ejki", "signatures": ["ICLR.cc/2026/Conference/Submission9432/Reviewer_jgdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9432/Reviewer_jgdN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905770474, "cdate": 1761905770474, "tmdate": 1762921032463, "mdate": 1762921032463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel differentiable physics simulator for Deformable Linear Objects (DLOs) aimed at addressing the limitations of existing simulation environments for robotic manipulation. The primary contribution is a unified framework, \"DLO-Lab,\" that models a wide and comprehensive range of DLO physical properties, including elasticity, inextensibility, bending plasticity, and loop topologies. They propose a benchmark suite of nine diverse DLO manipulation tasks designed to test these varied physical properties and interactions. The paper evaluates the performance of different policy learning paradigms on this benchmark: reinforcement learning (PPO, SAC), sampling-based trajectory optimization (CMA-ES), and gradient-based trajectory optimization. The results show that gradient-based methods are the most sample-efficient when gradients are available, while CMA-ES is a more robust general-purpose optimizer, particularly for tasks with sparse contact."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The framework incorporates several features that were not realized at the same time with other simulators, for example, full differentiability with a rich set of physical properties, bending plasticity, and loop topology, as well as coupling with both rigid and other soft-body materials (MPM), as shown in Table 1.\n\n\nFor the paper clarity, the motivation is strong and well-articulated. Visual aids like Table 1 (comparison to prior work) and Figure 2 (task illustrations) are highly effective at communicating the paper's contributions and scope."}, "weaknesses": {"value": "1) Sim-to-Real evaluation is limited\n\nThe real-world experiment (Sec 5.3) can be a sound proof of concept, but the results shown in the paper are very limited. It demonstrates the transfer of a single open-loop trajectory for one task, which validates the simulator's kinematics and some dynamics but does not validate its suitability for closed-loop control, where a policy would need to react to perception feedback. Inaccuracies in simulated contact forces or friction models, for example, would only be exposed in a closed-loop setting. Additionally, we would like to see the comparison between the simulator and the real in the video.\n\n2) Disconnection of agent task decomposition\n\nSection 4.4 introduces the LLM-based agent for task decomposition, which seems disconnected from the paper's core contribution (the simulator and benchmark).. It's an application of the benchmark, but it's presented as a key feature of the simulator. The implementation details are missing from the text, for example, the details of models, prompt engineering, and robustness.\n\n\n3) Low reproducibility of the results\n\nThe paper does not provide the code or much information about the implementation details, which makes it hard for readers to utilize the proposed framework or reproduce results."}, "questions": {"value": "1) How robust is the framework the paper proposes? I would like to see the robustness of changing physical parameters, for example, transferring policies for more contact-heavy tasks, or tasks where friction dynamics are more critical.\n\n2) Regarding the Agent Task Decomposition (Sec 4.4), what is the precise role of this agent? Is it used to generate the trajectories for the benchmarked algorithms (PPO, CMA-ES, GD)? Or is it a separate, high-level planner that would use policies trained by these methods as sub-skills?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HbUsOI51ky", "forum": "Fchtg9ejki", "replyto": "Fchtg9ejki", "signatures": ["ICLR.cc/2026/Conference/Submission9432/Reviewer_GzRa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9432/Reviewer_GzRa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989841230, "cdate": 1761989841230, "tmdate": 1762921032136, "mdate": 1762921032136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on manipulation of deformable linear objects (DLOs), such as ropes and cables. The authors introduce a differentiable physics simulator tailored for DLOs, enabling gradient-based optimization for planning and learning. The simulator models elastic and frictional behaviors through a differentiable mass-spring representation, making it suitable for both analytical gradient computation and policy training.\n\nThe authors also present a benchmark suite of DLO manipulation tasks, including rope straightening, knot tying, threading, and shape matching. The benchmark systematically evaluates several representative policy learning paradigms, including reinforcement learning (RL), trajectory optimization, and sampling-based motion planning. The evaluation highlights the limitations of existing approaches and motivates future algorithmic innovations for deformable object manipulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(+) The introduction of a differentiable simulator for DLOs is highly valuable for the community. It provides a reproducible and extensible software for studying deformable object control, a domain that has historically lacked standardized tools.\n\n(+) The authors evaluate a wide spectrum of methods (RL, trajectory optimization, and sampling-based approaches) under a unified environment, offering an insightful comparison of their respective strengths and weaknesses.\n\n(+) By leveraging a differentiable mass-spring formulation, the simulator enables gradient-based learning and efficient optimization, a clear improvement over prior non-differentiable simulators like PyBullet or SOFA.\n\n(+) Well-structured benchmark tasks: The benchmark tasks span a range of difficulty levels and manipulation types, providing a clear gradient for research progression."}, "weaknesses": {"value": "(-) Lack of methodological novelty. The paper’s main contribution lies in the simulator and benchmark design; no new algorithm or modeling technique is proposed beyond existing differentiable physics formulations.\n\n(-) Limited insight into algorithmic takeaways. While multiple learning paradigms are compared, the discussion lacks deeper analysis or clear conclusions about why certain methods succeed or fail, or what principles should guide future work.\n\n(-) Weak real-world validation. Although the simulator is claimed to be physically accurate, real-world experiments are minimal and qualitative. A stronger demonstration of sim-to-real consistency would substantially improve the paper’s credibility.\n\n(-) Scalability and efficiency concerns. The computational cost of differentiable simulation for long DLOs or contact-rich scenarios is not clearly discussed. Real-time feasibility remains uncertain.\n\n(-) Limited generalization across object materials: The experiments are primarily conducted with homogeneous ropes; the simulator’s ability to handle variable stiffness or heterogeneous materials is not tested."}, "questions": {"value": "Is the simulator capable of handling self-collisions or topological changes (e.g., forming or untying knots)?\n\nHow well does the differentiable simulator match real-world dynamics? Are there quantitative comparisons between simulated and physical trajectories?\n\nWhich policy learning algorithms benefited most from differentiable gradients, and which still relied heavily on sampling?\n\nCould the simulator be extended to sheet-like deformable objects (e.g., cloth) or more complex soft structures?\n\nWhat is the expected runtime per simulation step, and how does it scale with DLO length and resolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EvK6NAAhsr", "forum": "Fchtg9ejki", "replyto": "Fchtg9ejki", "signatures": ["ICLR.cc/2026/Conference/Submission9432/Reviewer_Fuxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9432/Reviewer_Fuxw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762732210732, "cdate": 1762732210732, "tmdate": 1762921029149, "mdate": 1762921029149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}