{"id": "DMoSFziBfu", "number": 3834, "cdate": 1757544553905, "mdate": 1759898067603, "content": {"title": "On the Role of Temperature Sampling in Test-Time Scaling", "abstract": "Large language models (LLMs) can improve reasoning at inference time through test-time scaling (TTS), where multiple reasoning traces are generated and the best one is selected. Prior work shows that increasing the number of samples $K$ steadily improves accuracy. In this paper, we demonstrate that this trend does not hold indefinitely: at large $K$, further scaling yields no gains, and certain hard questions remain unsolved regardless of the number of traces. Interestingly, we find that different sampling temperatures solve different subsets of problems, meaning single-temperature scaling explores only part of a model’s potential. We therefore propose scaling along the temperature dimension, which enlarges the reasoning boundary of LLMs. Temperature scaling enables base models to reach performance comparable to reinforcement learning (RL)-trained counterparts, without additional post-training. We further provide a comprehensive analysis of this phenomenon and design a multi-temperature voting method that reduces the overhead of temperature scaling. Overall, our findings suggest that TTS is more powerful than previously thought, and that temperature scaling offers a simple and effective way to unlock the latent potential of base models.", "tldr": "", "keywords": ["Test-Time Scaling", "Inference-Time Compute", "Scaling Law", "Sampling  Methods", "Evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/123a08f1927e140fcd6086db0570278759940b31.pdf", "supplementary_material": "/attachment/17dbf9f56605b60d624c6bed184279b55509f73b.zip"}, "replies": [{"content": {"summary": {"value": "This paper re-examines the Test-Time Scaling of Large Language Models for reasoning tasks, designed an efficient multi-temperature voting method with early exit for easy questions, reducing computational overhead while preserving performance gains. Experiments across multiple models and datasets validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The comparative experiments are relatively comprehensive; the proposed scheme requires no additional training and is orthogonal to most existing TTS methods.  \n2) An entropy-based analysis and problem taxonomy are conducted to reveal the dynamic behavior of temperature scaling, constituting a key contribution of this work."}, "weaknesses": {"value": "1) All models used in the comparison belong to the Qwen family, which is insufficient to demonstrate the generality of the proposed method. It is recommended to include models of different families for validation.  \n2) The main text lacks a formal description of the proposed method, making its exact implementation difficult to grasp.  \n3) There is no ablation study on the hyper-parameters (e.g., cross-temp threshold vs. intra-temp threshold) and their impact on performance and efficiency, hindering an understanding of how each component contributes to the gains."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XP5fIDloAd", "forum": "DMoSFziBfu", "replyto": "DMoSFziBfu", "signatures": ["ICLR.cc/2026/Conference/Submission3834/Reviewer_j3rw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3834/Reviewer_j3rw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903037443, "cdate": 1761903037443, "tmdate": 1762917057140, "mdate": 1762917057140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of temperature sampling in the test-time scaling (TTS) paradigm for large language models (LLMs). Traditionally, TTS boosts reasoning performance by generating multiple samples and selecting the best one via a verifier. The authors show that beyond a certain K, accuracy plateaus, and some problems remain unsolved regardless of further sampling. The key insight is that different sampling temperatures T lead to the solution of different subsets of problems, implying that a single-temperature TTS explores only a part of the model’s reasoning space. The authors propose multi-temperature scaling, where samples are drawn from multiple temperatures to expand the model’s “reasoning boundary.” They show empirically that temperature scaling allows base models to achieve performance comparable to RL-trained models, without additional fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The observation is interesting and novel.\n- The figures are informative and well-presented.\n- The authors conduct experiments across multiple domains.\n- The paper is easy to follow."}, "weaknesses": {"value": "- The major concern is that the experiments are only conducted on Qwen3 series models. However, different models may have different properties regarding temperature scaling. For example, the recommended temperature for Qwen3 and DeepSeek-R1-Distill series models are different [1].\n- The experiments are only restricted to models up to 8B parameters. However, larger models (e.g., 70B+) may have different behaviors with respect to temperature scaling.\n- The paper lacks a theoretical framework explaining why certain temperatures preferentially solve specific hard problems.\n\n[1] POLARIS: A POst-training recipe for scaling reinforcement Learning on Advanced ReasonIng modelS."}, "questions": {"value": "- Can the authors give some high-level points of why using different temperatures helps? For example, is it because different temperatures lead to more diverse reasoning paths, or because certain temperatures are better suited for specific types of problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pC4lG5vbXk", "forum": "DMoSFziBfu", "replyto": "DMoSFziBfu", "signatures": ["ICLR.cc/2026/Conference/Submission3834/Reviewer_3DQD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3834/Reviewer_3DQD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994816084, "cdate": 1761994816084, "tmdate": 1762917056890, "mdate": 1762917056890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the limitations of current test-time scaling (TTS) methods, which typically sample at only a single temperature. The authors propose scaling along the temperature dimension—that is, sampling at various different temperatures—to capture the union of all problems the model is capable of solving. The experiments demonstrate that this simple \"temperature scaling\" approach can allow a base model to match or even exceed the performance of models fine-tuned with RL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's core hypothesis and observations are concise and meaningful. The idea that a model's set of solvable problems is temperature-dependent, and that the union of these sets represents the model's true capability, is a valuable insight.\n\n- The figures in the paper are clear, well-executed, and effectively support the analysis and conclusions. Figure 3, in particular, does an excellent job of visualizing and explaining the core findings."}, "weaknesses": {"value": "- High Computational Cost: The method is computationally expensive as it requires extensive sampling across a wide range of different temperatures to achieve its full effect.\n\n- Lack of Deep Explanation: The paper lacks a deep explanation for why different problems require different temperatures to be solved. The analysis describes the resulting entropy dynamics (what is happening) but doesn't fully explain the root cause (why this dependency exists). Entropy seems to be a consequence of this phenomenon, not the cause.\n\n- Novelty of Claims: The conclusion that a base model with sufficient repeated sampling can outperform an RL-tuned model is not an entirely novel finding in the field.\n\n- Potential for Cherry-picking in Figure 3d: It is unclear how the specific value of $k$ (e.g., $k=128$) was chosen for the Pass@k comparison in Figure 3d. It is plausible that as $k$ increases, repeated sampling at any single temperature might eventually surpass the RL model's performance. The comparison at $k=128$ feels somewhat selective and may not present the full picture."}, "questions": {"value": "There is a body of existing work focused on teaching models to perform dynamic temperature sampling (i.e., learning to adjust temperature during generation). Could the authors discuss this line of research and elaborate on its connection to their findings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tm5N9A4763", "forum": "DMoSFziBfu", "replyto": "DMoSFziBfu", "signatures": ["ICLR.cc/2026/Conference/Submission3834/Reviewer_zrJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3834/Reviewer_zrJ8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101616137, "cdate": 1762101616137, "tmdate": 1762917055266, "mdate": 1762917055266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on enhancing the reasoning performance of large language models through temperature scaling. The authors propose an innovative method that adjusts the temperature parameter during the testing phase to expand the model's reasoning capabilities. The study demonstrates that a multi-temperature strategy significantly outperforms a single-temperature strategy. This approach can achieve performance comparable to models trained with reinforcement learning, solely through appropriate temperature configuration during testing, without additional training. This method is both innovative and practical, warranting further exploration and discussion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Introduces an innovative temperature scaling mechanism during the reasoning process.\nThe experimental design is sound, validating the effectiveness of the method across multiple datasets without increasing training burden.\nProvides a detailed entropy analysis revealing the theoretical mechanism behind temperature scaling."}, "weaknesses": {"value": "Terms like \"reasoning boundary\" and \"upper bound\" are ambiguously defined; the measurement criteria for the budget (such as the number of tokens, decoding steps, etc.) are not clearly specified, which may lead to unfair comparisons.\nInsufficient comparison with strong baselines. Lacks rigorous comparisons under equivalent reasoning budgets with methods like single high-temperature self-consistency, multi-temperature grid + voting, best-of-N/majority voting, strong verifier-assisted methods, and RL models.\nKey methodological details are incomplete. Lacks principles for selecting the temperature set, adaptive budget allocation, detailed voting mechanism, and sensitivity analysis for early exit strategy"}, "questions": {"value": "It is recommended to rigorously define terms such as \"reasoning boundary\" and \"upper bound,\" and to clearly specify the metrics used for budgeting (e.g., total number of generated tokens, number of decoding steps), ensuring consistency across all comparative experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QnhUavxv76", "forum": "DMoSFziBfu", "replyto": "DMoSFziBfu", "signatures": ["ICLR.cc/2026/Conference/Submission3834/Reviewer_xcmB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3834/Reviewer_xcmB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762493397758, "cdate": 1762493397758, "tmdate": 1762917054746, "mdate": 1762917054746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}