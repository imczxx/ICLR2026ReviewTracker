{"id": "1dNbK58bB9", "number": 22599, "cdate": 1758333323313, "mdate": 1759896857479, "content": {"title": "Physics-Informed Neural Networks with Learnable Loss Balancing and Transfer Learning", "abstract": "We propose a self-supervised physics-informed neural network (PINN) framework that adaptively balances physics-based and data-driven supervision for scientific machine learning under data scarcity. Unlike prior PINNs that rely on fixed or heuristic weighting of physics residuals and data loss, our approach introduces a learnable blending neuron that dynamically adjusts the relative contribution of each term based on their uncertainties. This mechanism enables stable training and improved generalization without manual tuning. To further enhance efficiency, we integrate a transfer learning strategy that reuses representations from related domains and adapts them to new physical systems with limited data. We validate the framework for the prediction of heat transfer in liquid-metal miniature heat sinks using only 87 CFD datapoints, where the adaptive PINN achieves an error $<8\\%$, outperforming shallow neural networks, kernel methods, and physics-only baselines. Our framework provides a general recipe for embedding physics adaptively into neural networks, offering a robust and reproducible approach for data-scarce problems across various scientific domains, including fluid dynamics and material modeling.", "tldr": "", "keywords": ["Physics-Informed Neural Networks", "Self-Supervised Learning", "Transfer Learning", "Scientific Machine Learning", "Heat Transfer", "Computational Fluid Dynamics"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab8fa5429de76687a91e0323ab37d1222ee81105.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a self-supervised physics-informed neural network (PINN) that adaptively balances data-driven and physics-based losses through a learnable “blending neuron,” removing the need for manual weight tuning between the two. The authors further combine this mechanism with transfer learning to improve generalization under extreme data scarcity. The paper reports improved robustness and comparable accuracy to kernel and standard NN baselines, claiming that the adaptive PINN provides a reliable, interpretable approach for small-data scientific ML tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "In my opinion, this work is not ready to publish. (No Strengths)"}, "weaknesses": {"value": "**Weakness**\n\n- The manuscript suffers from serious readability issues and lacks proper organization. Tables and figures are not placed where they are referenced in the text, and some even appear in inappropriate sections. For instance, Figures 5 and 6 are inserted between the reference entries.\n\n\n- The references are poorly managed. The paper fails to cite the seminal work that originally proposed Physics-Informed Neural Networks [1]. Moreover, one of the cited works, “Sifan Wang, Yujun Teng, and Paris Perdikaris. *When and why PINNs fail to train: A neural tangent kernel perspective.* In International Conference on Learning Representations (ICLR), 2021.”, is incorrectly labeled as an ICLR publication, when in fact it was published in the *Journal of Computational Physics* in 2022 [2].\n\n- The experimental section is severely underdeveloped. The number of experiments is too small to support the claims, and key implementation details such as the experimental settings, boundary conditions, sampling procedures, and model configurations are inadequately described.\n\n[1] Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.\" Journal of Computational physics 378 (2019): 686-707.\n\n[2] Wang, Sifan, Xinling Yu, and Paris Perdikaris. \"When and why PINNs fail to train: A neural tangent kernel perspective.\" Journal of Computational Physics 449 (2022): 110768."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Pizqgj34NO", "forum": "1dNbK58bB9", "replyto": "1dNbK58bB9", "signatures": ["ICLR.cc/2026/Conference/Submission22599/Reviewer_knDQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22599/Reviewer_knDQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760523972990, "cdate": 1760523972990, "tmdate": 1762942297563, "mdate": 1762942297563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-supervised physics-informed neural network (PINN) framework that adaptively balances physics-based and data-driven supervision for scientific machine learning under data scarcity."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper needs significant revisions."}, "weaknesses": {"value": "1. The experiments do not adequately support the paper's claims. For instance, the paper asserts that the proposed approach enables stable training and improved generalization without manual tuning, yet I found no experimental results demonstrating these capabilities.\n\n2. The paper's organization and writing quality are inadequate. More critically, given that adaptive weights are presented as a key novelty, the paper should formally describe this mechanism as its main technical content. However, I could not find any rigorous mathematical formulation of this approach anywhere in the manuscript.\n\n3. In its current form, this work reads more like an incomplete undergraduate student's senior project than a publication-ready contribution. The paper requires substantial revision before it can be considered suitable for publication."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hr6PBot2m6", "forum": "1dNbK58bB9", "replyto": "1dNbK58bB9", "signatures": ["ICLR.cc/2026/Conference/Submission22599/Reviewer_TGbo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22599/Reviewer_TGbo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760828753846, "cdate": 1760828753846, "tmdate": 1762942296961, "mdate": 1762942296961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces several techniques to improve training of PINNs for CFD problems. The paper introduces blending neuron which are used to balance between each type of training loss in PINN training, and perform transfer learning to initialise the PINN parameters which that trained in another similar pINN problem. The paper shows how these techniques can improve training of PINNs on CFD problems."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The problem considered is real, since PINNs can be difficult to train especially on CFD cases.\n\nThere seems to be some results that demonstrate the use of the proposed methods are able to improve on when the methods are not used."}, "weaknesses": {"value": "The paper only considers CFD problems, which is a much smaller scope than what the paper title suggests. It would be much more convincing to also perform the same loss balancing mechanism on other PINN training scenarios as well.\n\nAdditionally, it is not clear how the methods in the paper are novel. Balancing of loss functions are already done in more systematic ways in existing PINN works, and transfer learning for PINNs are already a setting that have been considered in other PINN works. A better description to how this paper is beyond just a blend of existing methods would be appreciated.\n\nSec 3.1 -- depending on the scope of the problem this part may not be necessary in the main text and better relegated to the appendix instead. Some parts within Sec 3.2 are also the same, since are common in PINN training anyway.\n\nFig 1 -- flow chart is confusing to follow. Would prefer it written as a pseudocode, or at least to have the accompanying text (Sec 3.4) to describe the algorithm more clearly.\n\nAll results lack error bars.\n\nThe experiments are insufficient. There are other methods for training to improve PINN convergence which involves balancing of loss functions which are not considered by the paper. Comparisons against these benchmarks would make the paper more convincing.\n\nThe reported results are poorly explained. The MAPE are given, but the error of which quantity is being reported here? This part is not mentioned in the paper. Furthermore, in transfer learning settings, what is the transfer learning being done from, and to which setting?\n\nFigs 5 and 6 -- not clear as to what the different points mean. Results may be better shown as a table."}, "questions": {"value": "1. Around Line 156 -- the use of neurons are not clear as to why they are used. If they are just back-propped, would the NN just not prioritise the loss factor that are smaller, hence ignoring the large error term and making that part not learned? This aspect is unclear, and clarifications should have been added to the paper.\n\n2. Related to the above -- what values of the neurons are learned? How does alpha evolve throughout the training process? Can any interpretation be taken from this value, and how would it compare with the papers that manually tune this or automatically select based on other heuristics?\n\n3. How is the training time when the blending neurons are used, compared to using existing PINN training methods? Additional results on resource usage would be interesting.\n\n4. What are the interpretations for Fig 3 and 4? Are these KDE values used during the training anywhere or are just for better understanding for the training process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "52kegyHYR2", "forum": "1dNbK58bB9", "replyto": "1dNbK58bB9", "signatures": ["ICLR.cc/2026/Conference/Submission22599/Reviewer_BFCN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22599/Reviewer_BFCN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555204543, "cdate": 1761555204543, "tmdate": 1762942296610, "mdate": 1762942296610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-supervised PINN with a learnable neuron that adaptively balances data and physics losses, as well as uses transfer learning techniques to reuse representations from related physical domains and fine tune only layers specific to new physical systems. While the implementation is straightforward, the novelty of the contribution is limited."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The workflow and method section is easy to understand.\n2. Clearly presented the motivation and experimental settings."}, "weaknesses": {"value": "1. The experimental section should also compare against other self-weighting methods as baselines. In addition, several important PINN variants are missing from the baselines, such as hard-constrained approaches.\n\n2. The experimental section includes only one example. While valuable, it is narrow in scope.\n\n3. PINNs can fail when the PDE coefficient is large, leading to ill-conditioning. Additional experiments demonstrating how the proposed method mitigates this issue would be better."}, "questions": {"value": "I am not sure why this paper compares against GP and SVR. As I understand it, these are not neural methods for solving forward problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ApyhctpiTP", "forum": "1dNbK58bB9", "replyto": "1dNbK58bB9", "signatures": ["ICLR.cc/2026/Conference/Submission22599/Reviewer_ggPw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22599/Reviewer_ggPw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878679704, "cdate": 1761878679704, "tmdate": 1762942296207, "mdate": 1762942296207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}