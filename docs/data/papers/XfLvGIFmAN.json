{"id": "XfLvGIFmAN", "number": 7673, "cdate": 1758031395929, "mdate": 1759897839967, "content": {"title": "Spectral Attention Steering for Prompt Highlighting", "abstract": "Steering a large language model's attention towards user-specified highlighted text is a critical capability. Existing prompt highlighting methods are incompatible with modern efficient attention mechanisms like Flash Attention due to their reliance on post-hoc matrix editing. We introduce Spectral Editing Key Amplification (SEKA), a training-free steering method that tackles this by directly editing key embeddings before attention computation. SEKA learns universal relevance subspaces offline via spectral decomposition. We extend this to Adaptive SEKA (AdaSEKA), a query-adaptive variant that uses a training-free routing mechanism to dynamically combine multiple expert subspaces based on the prompt's semantic intent. Our experiments show both methods significantly outperform strong baselines on standard steering benchmarks while adding much lower latency and memory overhead, ensuring full compatibility with optimised attention.", "tldr": "We propose SEKA, a training-free method that steers attention by editing key embeddings pre-computation.  It achieves better performance with negligible overhead.", "keywords": ["Spectral learning", "Attention steering", "Large language models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b33261ec28edf5dcae3d39bad52204e3fd3c2f7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SEKA, a new attention-steering approach designed for prompt highlighting. Motivated by the observation that most existing attention-steering methods operate directly on the attention matrix, which limits compatibility with efficient inference mechanisms such as FlashAttention, the authors propose modifying the key embeddings instead, prior to attention computation. Experiments demonstrate that SEKA and its adaptive variant, AdaSEKA, achieve promising performance on standard benchmarks and effectively mitigate positional bias."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is well-founded. Previous attention-steering methods are relatively direct and often incompatible with mainstream acceleration frameworks, revealing a significant gap between steering effectiveness and efficient inference.\n\n2. The proposed SEKA and its variant offer a simple yet effective approach. By modifying the key embeddings along directions associated with the highlighted context, the model can better emphasize relevant information.\n\n3. The approach demonstrates strong empirical performance, achieving near-perfect scores on multiple benchmarks and effectively alleviating positional bias."}, "weaknesses": {"value": "1. The construction of the relevance context appears tedious, and the overall pipeline is not clearly explained. The process of building positive and negative embeddings is a key component of both SEKA and AdaSEKA, yet it is not described in the main text, and the Appendix also lacks sufficient details. It remains unclear whether the creation of this synthetic dataset is required for each benchmark individually, and how the authors match pairs of context (C), question (Q), and answer (A), i.e., whether these are randomly sampled from the test set or selected according to specific criteria. Although SEKA enables efficient inference, the offline construction of such a synthetic dataset could potentially introduce significant preprocessing overhead.\n\n2. The trade-off between the positive and negative directions in SEKA seems somewhat naive. The authors simply assign equal weights (0.5) to both directions, without further justification or analysis. It would be valuable to understand whether the positive embeddings play a more critical role in emphasizing relevant information, and whether asymmetric weighting might lead to better performance or stability.\n\n3. The paper introduces a large number of hyperparameters, yet provides no ablation studies or sensitivity analyses to assess their impact. In particular, parameters such as γ (for singular vector selection) in SEKA, the number of experts (m) in AdaSEKA, and the head selection threshold (δₘᵢₙ) are not thoroughly discussed. Moreover, the authors do not specify the exact configurations of these hyperparameters in the experiments, making it difficult to evaluate the robustness and reproducibility of the results."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WQ871Fr0Gq", "forum": "XfLvGIFmAN", "replyto": "XfLvGIFmAN", "signatures": ["ICLR.cc/2026/Conference/Submission7673/Reviewer_JuJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7673/Reviewer_JuJy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381379453, "cdate": 1761381379453, "tmdate": 1762919737990, "mdate": 1762919737990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free attention–steering method and a query-adaptive variant to highlight user-specified spans by intervening on the attention inputs rather than post-hoc editing attention matrices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By editing keys before attention, the approach avoids material incompatibilities with FlashAttention‐style kernels and sidesteps full matrix storage required by prior post-hoc methods.\n2. Offline spectral learning over contrastive triplets is training-free at run time and generalizes across model sizes and two model families.\n3. Strong empirical results with ablations and overhead accounting."}, "weaknesses": {"value": "1. The approach relies on constructing synthetic contrastive triplets and performing SVD computations for each layer and head, as well as for every expert in the AdaSEKA variant.\n2. Relevance triplets are synthetically constructed, and the steering effectiveness may hinge on these triplets.\n3. The method involves tuning several thresholds and gain parameters for each model and task using dev sets. However, the paper provides limited analysis of parameter sensitivity or stability, and the optimal value of $\\delta_{\\min}$ appears to vary with model size.\n4. The evaluation is limited to highlight-friendly setups and short-form QA/rewriting."}, "questions": {"value": "1. Does SEKA increase the probability of obeying unsafe highlighted instructions? Any mitigation via head filtering or gain caps?\n2. How often “steering all passages” degrades the middle in long-context QA?\n3. Beyond heatmaps and PCA shifts, can you show that edits causally increase attention mass or logit contribution from highlighted keys to answer tokens in specific heads and layers, and that those heads are sufficient via knock-out and knock-in?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fQuzCwxszd", "forum": "XfLvGIFmAN", "replyto": "XfLvGIFmAN", "signatures": ["ICLR.cc/2026/Conference/Submission7673/Reviewer_Z3wt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7673/Reviewer_Z3wt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913087681, "cdate": 1761913087681, "tmdate": 1762919737679, "mdate": 1762919737679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discussed an improved attention steering techniques that combines the existing post-hoc attention steering with spectral decomposition on the key embedding. The results show that SEKA, especially its variant AdaSEKA, outperforms the existing attention steering approaches, SPA and PASTA on five instruction following tasks for most of the models. Since the SEKA performs decomposition and steering on the key embedding instead of the full attention matrix, it is also compatible with modern attention implementation such as FlashAttention. This advantage gives SEKA better real-applicability than the existing methods like PASTA that relies on the full attention modifications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. It is a clever integration of attention steering and spectral editing. The implementation of the method is well presented in the paper and is easy to reproduce.\n2. The SEKA is compatible with advanced attention implementation that does not compute the full attention matrix. As a result, the method also has lower computation overhead compared to PASTA and SPA that require full attention editing.\n3. The adaptive expert projection routing reduce the overhead in fine-tuning steering hyperparameters.\n4. The performance of SEKA and AdaSEKA is robust for most models on most tasks. The fact that the SEKA can help model fetch information presented in the middle of a long input context, reversing the U-shape performance (lost in the middle) of existing method, highlights this method can be used for long-document QA."}, "weaknesses": {"value": "1. Generalizability to unseen instructionw? The paper showed their method's generalizability in CounterFact tasks through paraphrase scores that measure performance on human-rewritten versions of the original questions. However, does the projection learned on one task or a collection of tasks generalize to the unseen instruction types.\n2. It is unclear what's the relationship between the amount of data used in finding \"relevance subspace\" for a given instruction and the performance of the SEKA method. If projection matrix is domain-specific, how many pairs of instruction following responses and non-following responses are needed for SEKA to be effective.\n3. The benefits of learned projections do not seem to be significant on the Qwen3 family."}, "questions": {"value": "1. In table 2, why does PASTA underperform the model's original performance on pronoun changing tasks for Qwen3-14B and Gemma3-12B?\n2. pg 6, ln 281, given all tested models are base models, what does \"empty response\" mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zqtk7A86cT", "forum": "XfLvGIFmAN", "replyto": "XfLvGIFmAN", "signatures": ["ICLR.cc/2026/Conference/Submission7673/Reviewer_XAQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7673/Reviewer_XAQm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941077924, "cdate": 1761941077924, "tmdate": 1762919737349, "mdate": 1762919737349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel, training-free attention steering method called SEKA and its adaptive variant AdaSEKA for prompt highlighting. The core innovation lies in shifting from the traditional paradigm of \"post-editing\" the attention matrix to directly editing key vectors before attention computation. It learns a universal \"relevance subspace\" via offline spectral decomposition and constructs projection matrices to amplify the features of highlighted tokens. Experiments demonstrate that the method outperforms existing approaches on multiple standard benchmarks while adding negligible latency and maintaining full compatibility with modern efficient attention mechanisms like Flash Attention. Its primary contribution is a new paradigm for efficient, precise, and general-purpose attention steering."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- High Originality. Achieves a paradigm breakthrough by shifting from \"editing attention output\" to \"editing attention input.\" The concept is novel.\n-  Exceptional Efficiency and Compatibility\n    \\par Near-zero latency overhead and full compatibility with Flash Attention represent a decisive advantage over methods like PASTA, showing great practical potential.\n-  Comprehensive and Robust Experiments\n    \\par Validation across multiple tasks, models, and metrics consistently demonstrates the method's effectiveness and generality. Ablation studies thoroughly confirm the value of the core components.\n- Advanced Adaptive Mechanism\n    \\par AdaSEKA's query-aware routing mechanism dynamically combines experts, reducing the need for manual hyperparameter tuning and improving the method's intelligence and applicability."}, "weaknesses": {"value": "- While the geometric interpretation is intuitive, the paper would be further strengthened by a linguistic or semantic characterization of the learned relevance subspace, for instance, by analyzing the most affected tokens or nearest neighbors in embedding space. Suggestion: Incorporate semantic analysis of the projection directions (e.g., via nearest-neighbor word analysis of projected keys).\n- The construction of experts in AdaSEKA relies on manual task division. Currently, experts are constructed based on different datasets, with no exploration of automatic clustering or dynamic expert generation.\n- Significant performance fluctuations in certain tasks: For instance, in the Pronoun Changing task on Gemma-3-12B, AdaSEKA exhibits noticeable performance volatility. Suggestion: Analyze the impact of model architecture on the method and develop a more stable routing mechanism.\n- It would be helpful to discuss the data efficiency of the offline spectral learning process, such as how projection stability changes with sample size. While the current approach utilizes specially constructed synthetic datasets, it lacks systematic analysis of how varying data quantities affect the quality of learned subspace representations. This gap introduces uncertainties in the reproducibility and generalizability of the method in practical applications."}, "questions": {"value": "- The rationale behind data construction. SEKA relies on an offline-constructed synthetic dataset to learn a \"universal\" relevance subspace. How do the authors justify that this specific construction of contrastive prompts can capture a sufficiently broad notion of relevance to generalize to unseen, and potentially more complex, tasks?\n- Interpretability of Projection Directions. Beyond the geometric interpretation, do the primary directions of the learned positive/negative projection matrices correspond to identifiable linguistic features? Have the authors attempted to analyze these directions, for example, by examining the nearest neighbors in the vocabulary space of key vectors that are most amplified/diminished?\n- Synergy with Positional Bias Methods. Given that SEKA operates on Key vectors and methods like \"Found-in-the-Middle\" calibrate positional bias, do the authors believe that combining SEKA with explicit positional bias calibration methods could yield further performance gains? Is this a worthwhile direction for future work?\n- Could the source code be made publicly available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KC85WyM110", "forum": "XfLvGIFmAN", "replyto": "XfLvGIFmAN", "signatures": ["ICLR.cc/2026/Conference/Submission7673/Reviewer_c6N4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7673/Reviewer_c6N4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001257391, "cdate": 1762001257391, "tmdate": 1762919736994, "mdate": 1762919736994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}