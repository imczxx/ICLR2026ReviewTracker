{"id": "16MJNrz0xV", "number": 2318, "cdate": 1757057934179, "mdate": 1759898155985, "content": {"title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization", "abstract": "Designing protein sequences with optimal energetic stability is a key challenge in protein inverse folding, as current deep learning methods are primarily trained by maximizing sequence recovery rates, often neglecting the energy of the generated sequences. This work aims to overcome this limitation by developing a model that directly generates low-energy, stable protein sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused on generating low-energy, high-stability protein sequences. Our core innovation lies in: First, integrating Markov Bridges with Direct Preference Optimization (DPO), where energy-based preferences are used to fine-tune the Markov Bridge model. The Markov Bridge initiates optimization from an information-rich prior sequence, providing DPO with a pool of structurally plausible sequence candidates. Second, an explicit energy constraint loss is introduced, which enhances the energy-driven nature of DPO based on prior sequences. This enables the model to effectively learn energy representations from a wealth of prior knowledge. It can also directly predict sequence energy values, thereby capturing quantitative features of the energy landscape. Our evaluations demonstrate that EnerBridge-DPO can design protein complex sequences with lower energy while maintaining sequence recovery rates comparable to state-of-the-art models, and accurately predicts $\\Delta \\Delta G$ values between various sequences.", "tldr": "", "keywords": ["Inverse Folding", "Markov Bridge", "Direct Preference Optimization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d59ca5935bdd4500221b8e2a8bb89313568139ca.pdf", "supplementary_material": "/attachment/af0026d5e04bdbf89d64f955aff938712b6d25d3.zip"}, "replies": [{"content": {"summary": {"value": "EnerBridge-DPO introduces an energy-guided protein inverse folding framework that integrates Markov Bridges with Direct Preference Optimization to generate low-energy, high-stability protein sequences while preserving structural fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Combines Markov Bridge generative modeling with Direct Preference Optimization, introducing energy-based fine-tuning into protein inverse folding for the first time.\n+ Incorporates explicit energy constraints and ΔΔG prediction, aligning learned representations with biophysical energy landscapes.\n+ Demonstrates lower energy, stable protein designs, and competitive recovery rates across multiple benchmarks with solid ablation analyses."}, "weaknesses": {"value": "+ The model’s energy improvements rely on computational predictors (FoldX, Rosetta, BA-Cycle) without experimental or molecular dynamics confirmation.\n\n+ DPO fine-tuning depends on precomputed or predicted energy scores, which may introduce bias and limit generalization to unseen proteins.\n+ The paper lacks discussion on computational cost, hyperparameter sensitivity (e.g., β in DPO), and robustness across large or diverse protein complexes."}, "questions": {"value": "1. How sensitive is EnerBridge-DPO to the β hyperparameter in the DPO term? Does higher β harm diversity?\n\n2. Could the model generalize to de novo backbones not present in the training set?\n\n3. Is the energy predictor differentiable and updated during DPO, or fixed as an external oracle?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Inyr5stler", "forum": "16MJNrz0xV", "replyto": "16MJNrz0xV", "signatures": ["ICLR.cc/2026/Conference/Submission2318/Reviewer_S1Fo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2318/Reviewer_S1Fo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877856952, "cdate": 1761877856952, "tmdate": 1762916191114, "mdate": 1762916191114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EnerBridge-DPO, a protein inverse-folding framework that unifies Markov bridge generative modeling with Direct Preference Optimization (DPO) and Boltzmann-aligned energy constraints.\nThe method attempts to address a current inverse-folding limitation — they optimize for sequence recovery but neglect thermodynamic stability. In this paper they focus on binding free energy of protein complexes. \n\nEnerBridge-DPO proceeds in two stages:\nMarkov Bridge Pre-training: Trains AdaLN-Bias and Cross-Attention Adapter layers on top of frozen ESM2 weights to iteratively transition the predicted PiFold sequence into the true native sequence.\n\nBridge-DPO Fine-tuning: Fine-tunes the Markov Bridge model using DPO guided by ∆∆Gbind values for protein complexes. Here, DPO steers the bridge prediction towards a lower-energy sequence.\n\nExperiments on BindingGym, SKEMPI, and PDB benchmarks show improved sequence recovery, lower predicted binding energies, and accurate ΔΔG prediction compared to existing baselines like ProteinMPNN and Bridge-IF."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Combination of Markov Bridge and DPO.\nThe combination of Markov bridges (for structured stochastic refinement) with DPO (for preference-based alignment) is new and elegant. The paper demonstrates a theoretically consistent formulation where probabilistic modeling (via bridge processes) and preference learning (via DPO) jointly improve protein design.\n\nInteresting empirical results.\nAblation studies confirm the necessity of both DPO fine-tuning and energy supervision in various downstream results.\n\nClean mathematical formulation.\nThe paper provides a clear derivation of the bridge process and adapts the DPO loss to the inverse-folding domain, including justification of each term. \n\nComputational efficiency.\nTraining with only T = 25 timesteps, a cosine noise schedule, and simple Adam/Noam optimization shows practical efficiency — feasible for wider adoption."}, "weaknesses": {"value": "Energy-aware learning objective.\nThe inclusion of a Boltzmann-aligned energy loss attempts to ground the generative process in physical thermodynamics in order to make the model interpretable and biologically relevant. However, the assumption that model probability strongly correlates with free energy is a poor decision by the author for several reasons. The biggest reason is that the datasets used for ∆∆Gbind are very noisy datasets with heterogenous analytical methods used for data collection, they often use proxies rather than actually measuring ∆∆G, these measurements are often dependent on the temperature, buffer, etc making it hard to aggregate data between different labs and proteins. \n\nAblations on DPO temperature and preference data.\nIt is unclear how sensitive performance is to the β parameter or the construction of winner–loser pairs. More analysis here would help assess robustness.\n\nLimited experimental scope.\nEvaluation focuses mainly on sequence recovery and ΔΔGbind prediction; it would strengthen the paper to test downstream structure quality (e.g., AlphaFold2-refolded RMSD) or binding specificity. Using stability ∆∆G data would better demonstrate effectiveness of the DPO fine-tuning."}, "questions": {"value": "Ambiguous difference in sequence pairs.\nHow many mutations are there between the positive and negative pairs in the BindingGym data used for DPO? I didn't see any stats for this in the paper. I know most of SKEMPI is single point mutations. Being able to show performance improvements for larger sequence differences (double/triple/etc mutants) compared to single point mutants might be an application that this method makes a meaningful improvement. Separating improvements for single point vs higher order mutants would improve benchmarking and evaluation of the method. \n\nBetter downstream evaluation results are needed. \nWhile the method in itself (DPO + Markov bridges) is interesting, the downstream results are not very impressive, thus, questioning its utility. Table 2 shows modest improvements but the std is so large that I doubt it is statistically significant. Additionally, 3-fold cross validation is known to have serious data leakage and the performance improvements are marginal. In summary, seems like a lot of method development work for unimpressive results that are marginally better than baselines and are most likely the result of significant hyperparameter tuning. Please provide additional experiments that demonstrate true, unquestionable performance improvement in a downstream protein task. \n\nGeneralization: \nDoes this method work if you use ProteinMPNN or some other inverse folding framework? I don't see any ablations for changing the input prior sequence distribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t4DaJuMsKh", "forum": "16MJNrz0xV", "replyto": "16MJNrz0xV", "signatures": ["ICLR.cc/2026/Conference/Submission2318/Reviewer_DcbP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2318/Reviewer_DcbP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027550363, "cdate": 1762027550363, "tmdate": 1762916190976, "mdate": 1762916190976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose EnerBridge- DPO, an inverse folding framework focused on generating low-energy, high-stability protein sequences. The method first pretrains a generative diffusion bridge mode that refines a structure-conditioned prior sequence from PiFold, then fine-tunes with an energy-guided DPO that prefers lower-energy sequences. An explicit energy constraint loss is introduced, compelling the model to learn and predict quantitative energy features.  The Experimental results demonstrate that EnerBridge-DPO designs protein complex sequences with lower energy compared to existing methods, while maintaining comparable sequence recovery."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation is well grounded: effective sequence design should explicitly favor lower-energy sequences."}, "weaknesses": {"value": "1. The methodological novelty appears very limited. The bridge-based generative component and several architectural/training choices (e.g., Markov Bridge formulation, PLM backbone with AdaLN-Bias and structural adapters, frozen base weights) closely track Bridge-IF [1], and the added DPO fine-tuning for lower energy reads as a relatively incremental extension rather than a fundamentally new framework.\n\n2. The evaluation omits designability metrics, which are critical alongside stability/energy. Assessing only recovery/perplexity and energy leaves an incomplete picture of practical design performance. The authors should report standard designability measures (e.g., diversity, success rate under structure prediction, foldability metrics such as pLDDT/TM-score distributions for generated sequences) to substantiate claims about usable sequence design.\n\n3. The ΔΔG prediction gains over BA-DDG in Table 3 are modest and appear incremental.\n\n4. The paper’s primary contribution appears to be applying DPO to fine-tune a Bridge Diffusion model that is mainly derived from Bridge-IF. However, the presentation implies that the authors also developed the underlying Bridge Diffusion base model—both in Figure 2 and in the descriptions of design choices for their Bridge Diffusion architecture in Section 3.2.3. This framing is misleading and dishonest. Please see the “Details of Ethics Concerns” section for specifics. Given the above issues, I recommend a Strong Reject."}, "questions": {"value": "For ΔΔG prediction, what fraction of the SKEMPI pairs overlap structurally or sequentially with training data used for pretraining or DPO?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "A core component of the submission—the Markov Bridge architecture and its training setup—appears substantially derived from Bridge-IF [1], but the manuscript does not consistently or explicitly attribute these design choices and adopts a tone suggesting independent development. Three specific issues illustrate this concern:\n\n1. Figure 2 is highly similar to Figure 2 in Bridge-IF, with the primary change being a rotation of the layout (left-to-right here versus top-to-bottom in [1]).\n\n2. The methodological exposition in Section 3.2.3 closely parallels Bridge-IF’s Sections 4.3, 4.3.1, and 4.3.2: both employ a pretrained PLM as the backbone for approximating the reverse bridge process and tailor the Transformer blocks with  AdaLN-Bias for timestep conditioning alongside structural cross-attention adapters.\n\n3. Both works freeze the base PLM weights during training.\n\nReference:\n\n[1] Zhu et al., Bridge-IF: Learning Inverse Protein Folding with Markov Bridges. NeurIPS 2024."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lJXkOWoQGN", "forum": "16MJNrz0xV", "replyto": "16MJNrz0xV", "signatures": ["ICLR.cc/2026/Conference/Submission2318/Reviewer_aLtH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2318/Reviewer_aLtH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762793265041, "cdate": 1762793265041, "tmdate": 1763003887601, "mdate": 1763003887601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}