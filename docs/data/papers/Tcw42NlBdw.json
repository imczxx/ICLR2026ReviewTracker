{"id": "Tcw42NlBdw", "number": 18013, "cdate": 1758282922440, "mdate": 1759897139450, "content": {"title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning", "abstract": "Value decomposition has been extensively studied as a core approach for cooperative multi-agent reinforcement learning (MARL) under the CTDE paradigm. Despite this progress, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), a framework that successively learns multiple sub-value functions to retain information about alternative high-value actions. By incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\\text{tot}}$ to adjust quickly when the optimal action changes. Extensive experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance.", "tldr": "S2Q stores values of multiple subactions, enabling efficient adjustment when the optimality of value function shifts via exploration.", "keywords": ["Multi-Agent Reinforcement Learning", "Value Decomposition", "Centralized Training with Decentralized Execution", "Exploration"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c05f8e0fd7be8d20f6924faa5b5a15c51765b94.pdf", "supplementary_material": "/attachment/8a5da92f0d5b0fd91c2e8a03718a3c0972bc0be8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes S2Q, a new framework for cooperative MARL under CTDE settings. This paper retains and learns from suboptimal actions so that the system can better adapt when the optimal joint policy changes during the training. Instead of relying on single best action, S2Q successively trains multiple sub-value functions that each caputure distinct high-value joint actions. By combining them through softmax-based behaviour policy, the method encourages persistent exploration and rapid adjustment to new optima. Experiments on SMAC, SMAC-Comm, and GRF shows outperformance to baseline algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: The paper presents an original idea of retaining suboptimal actions through the proposed S2Q framework. This introduces a new perspective in value decomposition MARL by modeling multiple high-value modes instead of a single optimal action, addressing a long-standing issue of adaptability under shifting optima.\n2. Quality: The technical formulation is solid and well-motivated, supported by a clear derivation of objectives and loss functions. The experimental setup is comprehensive, evaluating the method on multiple challenging benchmarks and performing ablations to validate each component.\n3. Clarity: The paper is clearly written, with well-organized sections, intuitive figures, and a motivating example (the payoff matrix) that effectively conveys the main idea. The methodology and algorithm are clearly explained and reproducible, with good visual aids to guide understanding.\n4. Significance: The contribution is significant for cooperative MARL. It addresses a key limitation in existing value decomposition approaches, poor adaptability when optimal actions shift. The framework’s generality and empirical robustness indicate potential impact on future MARL research."}, "weaknesses": {"value": "1. Limited theoretical depth. While the paper provides a solid conceptual and empirical contribution, the theoretical analysis of why successive sub-value learning guarantees adaptability or improved convergence remains shallow. The paper lacks formal results on convergence properties, stability, or value approximation bounds for S2Q, which would strengthen its scientific aspect.\n2. Incremental novelty relative to prior frameworks. Although the idea of retaining suboptimal actions is interesting, the implementation may be viewed as an extension of WQMIX with auxiliary sub-networks and a Softmax exploration mechanism. The paper could better clarify how S2Q fundamentally differs from or improves upon multi-head or ensemble-based exploration methods beyond heuristic layering.\n3. Computational efficiency discussion. The paper acknowledges increased computation and memory overhead but only qualitatively. A quantitative analysis comparing GPU memory, or training wall-clock time to QMIX/WQMIX would make the practicality argument more convincing, especially for scaling to large agent teams.\n4. Clarity of communication variant. The role of the encoder–decoder communication module is somewhat underexplained. It is unclear how much of the improvement in SMAC-Comm tasks comes from the communication architecture itself versus the S2Q mechanism. A clearer disentanglement or comparison to standalone communication baselines would improve interpretability.\n5. Missing experiment details. Although the codebases are described in the Appendix individually, it seems that the performance in Figure 5 of the QMIX algorithm originated from pymarl2 does not achieve the performance stated in RIIT[1]. A detailed training parameters or any modifications are better to be discussed.\n\n[1] Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning"}, "questions": {"value": "Based on the weaknesses described above, I have some questions for the authors.\n\n1. Could the authors provide a more formal justification or theoretical insight into why maintaining multiple sub-value functions helps track shifting optima more effectively?\n2. Are there any guarantees on the convergence or stability of S2Q, particularly when sub-value suppression is applied iteratively? A sketch of theoretical grounding could greatly strengthen the contribution.\n3. Could the authors provide a direct comparison or at least a discussion to clarify whether S2Q’s performance gains come from retaining suboptimal modes or simply from having additional representational capacity?\n4. The paper mentions that the computational overhead is “moderate.” Could the authors quantify this or training time ratio relative to QMIX or WQMIX?\n5. How does the overhead scale with the number of sub-value networks K and the number of agents N? This would help assess the method’s practicality in larger-scale MARL tasks.\n6. Have the authors considered testing in deliberately non-stationary environments, such as mixed multiple opponent strategies in SMAC-Hard environment,to validate S2Q’s adaptability?\n7. In SMAC-Comm results, how much improvement arises from the encoder–decoder communication mechanism versus the S2Q framework itself?\n8. The paper positions S2Q as addressing the issue of “shifting optima.” Could the authors better relate this to other known issues such as non-stationarity, policy co-adaptation, or value landscape drift in MARL? Clarifying this link may help highlight the conceptual novelty and general applicability of S2Q.\n\nIf all the concerns are well-addressed, I would consider raising the score. Thank you."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o48FEII87c", "forum": "Tcw42NlBdw", "replyto": "Tcw42NlBdw", "signatures": ["ICLR.cc/2026/Conference/Submission18013/Reviewer_44v8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18013/Reviewer_44v8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809231743, "cdate": 1761809231743, "tmdate": 1762927805128, "mdate": 1762927805128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to overcome value function shifts during training in CTDE MARL, named S2Q. The proposed method learns a set of several sub-value functions, where each aims to identify different suboptimal actions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well written and provides a very extensive set of experiments and ablations. The results are consistently strong across very diverse environments. While apparently simple, i find clever the idea to surpress the optimal actions in the calculations of subsequent value functions and the performances show big improvements in a range of tasks."}, "weaknesses": {"value": "The authors could have provided a deeper analysis of the scalability of the proposed method, since it requires sequential computations using sub-networks. I.e, since there is a mixing for each Q, up until what point can k scale? In the communication encoder-decoder module in figure 3, the authors could have provided a better description of the architecture of these modules. \n\nPlease find below some more specific questions."}, "questions": {"value": "1. Could the authors describe another more practical scenario aside from the provided matrix games where the optimal action shift might happen? For example in the experimented environments such as SMAC, when could such shift happen?\n2. i find interesting the sudden boost around 1M timesteps in figure 6 for the task bane_vs_hM; have the authors explored why this is seen only in this specific environment, instead of a more linearly increasing curve?\n3. how can you find the best k? is there any guarantees (theoretical maybe) that for higher k the variance and learning instability will increase (as mentioned in lines 454-455)?\n4. considering the sub value functions are learned in a sequential manner, how does the proposed method relate to the higher order q-learning approach presented in [1]?\n\n[1] https://arxiv.org/abs/2304.13383"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5yHyhOTrVe", "forum": "Tcw42NlBdw", "replyto": "Tcw42NlBdw", "signatures": ["ICLR.cc/2026/Conference/Submission18013/Reviewer_xg2N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18013/Reviewer_xg2N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910298320, "cdate": 1761910298320, "tmdate": 1762927804298, "mdate": 1762927804298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Successive Sub-value Q-learning (S2Q) for cooperative MARL under CTDE. Instead of committing to a single estimated-optimal joint action, S2Q learns a sequence of monotonic, value-decomposition subnetworks that each focuses on a different high-value mode by suppressing actions selected by earlier subnetworks in its TD objective. To synchronize the index k during training, the authors use an encoder–decoder that reconstructs the global state and approximates the Softmax distribution. Experiments on SMAC Hard+, GRF, SMAC-Comm, and SMACv2 report higher win rates and faster convergence than strong baselines. Ablations suggest the Softmax selection and successive learning are key. Compute overhead is reported as modest while reaching 50% win rate substantially faster than QMIX."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The toy matrix-game illustrates why retaining information about nearby high-value actions can help when the optimum shifts. S2Q operationalizes this via a suppressed TD objective and Softmax-guided behavior policy. The algorithmic presentation (Alg. 1; eqs. (B.2–B.5)) is easy to follow. \n\n2. Results span SMAC Hard+, GRF, SMAC-Comm (with a “-Comm” variant), and SMACv2, showing consistent gains and faster learning, not only final win rates. The compute table quantifies overhead.\n\n3. Removing Softmax selection or randomizing $k$ materially hurts performance. Analysis of $K$ and temperature $T$ is informative. Sensitivity of suppression $\\alpha$ and weighting $w_c$ appears in the appendix.\n\n4. Default evaluation needs no communication relying on $Q_0^{sub}$, which is attractive compared to methods that require message passing at inference."}, "weaknesses": {"value": "1. The paper claims theoretical/empirical analyses, but no formal result is provided to justify that minimizing the modified TD with the suppression term reliably extracts distinct top-k modes under the IGM constraint or preserves contraction/stability properties. A small lemma would strengthen the case.\n\n2. S2Q learns an encoder–decoder to approximate $P_t$ and reconstruct $s$, which provides additional supervision such as cross-entropy and reconstruction. Several non-communication baselines do not leverage comparable auxiliary signals, raising comparability questions. A variant without the encoder–decoder, or with matched auxiliaries for baselines would calibrate the gain attributable to successive sub-values and auxiliary training.\n\n3. While $K$ and $T$ are studied, they’re analyzed mainly on 6h vs 8z. It would be helpful to report these sweeps across more tasks (e.g., MMM2, GRF academy 4v3) and include a “no-comm” ablation that samples $k$ independently to show the necessity of synchronization, and an oracle $P_t$ using the true Softmax to bound performance. \n\n4. In Eq. (2)/App. B.2, the target for previously selected actions is reduced by $\\alpha Q^*$, which can change the scale and sign of TD targets. There is no analysis of potential instability or bias this introduces, particularly with function approximation. Clarifying normalization or clipping strategies, and why the scheme does not collapse would help. In addition, this method shares the similar idea with [1], it would be better to compare with this baseline.\n\n[1] Wan, L., Liu, Z., Chen, X., Wang, H., & Lan, X. (2021). Greedy-based value representation for optimal coordination in multi-agent reinforcement learning. arXiv preprint arXiv:2112.04454."}, "questions": {"value": "1. Can you formalize guarantees (even under simplified assumptions) that successive suppression + weighting (B.5) yields distinct top-k action sets, or at least that $Q_0^{sub}$ is not harmed by the auxiliary subnetworks? A proposition about bias introduced by the suppression term would be valuable.\n\n2. What happens if you remove communication during training and have agents sample k independently, or conversely provide baselines with a matched auxiliary (state reconstruction)? Please include a “no-comm” S2Q variant and, if possible, an oracle $P_t$ variant to bound gains.\n\n3. You show $K=2,T=0.1$ works broadly (Tab. E.2). Could you include per-map sweeps (or at least MMM2 and academy 4v3) to show consistency and to better justify the choice of $K$?\n\n4. Since the paper cites “MARL as sequence modeling”, could you comment on (or include) a representative sequence-model baseline and discuss compatibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HyLE3eNef9", "forum": "Tcw42NlBdw", "replyto": "Tcw42NlBdw", "signatures": ["ICLR.cc/2026/Conference/Submission18013/Reviewer_3XJX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18013/Reviewer_3XJX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050816179, "cdate": 1762050816179, "tmdate": 1762927803838, "mdate": 1762927803838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Successive Sub-value Q-learning (S2Q), a framework that successively learns multiple subvalue functions to retain information about alternative high-value actions. By incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables Qtot to adjust quickly when the optimal action changes. Experimental results show that S2Q outperforms other recent MARL methods on the StarCraft II Multi-Agent Challenges and Google Research Football."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Dynamic value functions\n\n    S2Q overcomes the limitation that conventional methods do not explicitly track suboptimal actions. When the optimal action changes, S2Q can immediately leverage the corresponding sub-value function and guide Q^{tot} to adapt. \n\n- Introducing communication during training\n\n    S2Q explicitly executes tracked suboptimal actions with priority determined by a Softmax distribution P_t over their Q^{∗} values, thereby enabling exploration of a wider range of spaces than conventional ϵ-greedy exploration."}, "weaknesses": {"value": "- Old Benchmarks \n\n    The StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) is an old benchmark. It is advised to report the experimental results on the recently proposed SMAC-Hard benchmark [1].\n\n    [1] SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC, arXiv:2412.17707."}, "questions": {"value": "Figure 1 demonstrates changes in the payoff matrix. Could you provide concrete examples of how the value function dynamically evolves in SMAC-Hard or GRF environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RghlyVWgIY", "forum": "Tcw42NlBdw", "replyto": "Tcw42NlBdw", "signatures": ["ICLR.cc/2026/Conference/Submission18013/Reviewer_nTYN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18013/Reviewer_nTYN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762273018885, "cdate": 1762273018885, "tmdate": 1762927803537, "mdate": 1762927803537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}