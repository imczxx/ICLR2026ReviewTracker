{"id": "kByN4v0M3e", "number": 25049, "cdate": 1758363497157, "mdate": 1759896736358, "content": {"title": "Recurrent Action Transformer with Memory", "abstract": "Transformers have become increasingly popular in offline reinforcement learning (RL) due to their ability to treat agent trajectories as sequences, reframing policy learning as a sequence modeling task. However, in partially observable environments (POMDPs), effective decision-making depends on retaining information about past events - something that standard transformers struggle with due to the quadratic complexity of self-attention, which limits their context length. One solution to this problem is to extend transformers with memory mechanisms. We propose the Recurrent Action Transformer with Memory (RATE), a novel transformer-based architecture for offline RL that incorporates a recurrent memory mechanism designed to regulate information retention. We evaluate RATE across a diverse set of environments: memory-intensive tasks (ViZDoom-Two-Colors, T-Maze, Memory Maze, Minigrid-Memory, and POPGym), as well as standard Atari and MuJoCo benchmarks. Our comprehensive experiments demonstrate that RATE significantly improves performance in memory-dependent settings while remaining competitive on standard tasks across a broad range of baselines. These findings underscore the pivotal role of integrated memory mechanisms in offline RL and establish RATE as a unified, high-capacity architecture for effective decision-making over extended horizons.", "tldr": "The paper proposes Recurrent Action Transformer with Memory - a transformer model with recurrent memory and a procedure for training it for memory-intensive environments in an Offline RL setting.", "keywords": ["RL", "Offline RL", "Memory", "Transformers", "POMDP"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f12dedfe8e5f78bdbcafe02b45269bd395a16e7d.pdf", "supplementary_material": "/attachment/73f84c75ae7e38b617ea2aa219acb4de58b82d54.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Recurrent Action Transformer with Memory (RATE), a transformer-based architecture for offline reinforcement learning (RL) in partially observable and memory-intensive environments. RATE incorporates three memory mechanisms includinlearned memory embeddings, recurrent caching of hidden states, and the Memory Retention Valve (MRV), a cross-attention module designed for selective retention of key information across long sequences. The method is evaluated on a diverse suite of tasks, including ViZDoom-Two-Colors, T-Maze, Minigrid-Memory, Memory Maze, POPGym, as well as Atari and MuJoCo benchmarks. Experimental results and ablation studies are provided, alongside theoretical analysis on memory preservation by the MRV."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-\tThe paper presents a well-motivated and thoughtfully engineered hybrid memory mechanism.\n-\tThe empirical study is extensive, covering a comprehensive battery of memory-intensive RL tasks (ViZDoom-Two-Colors, T-Maze, Minigrid-Memory, Memory Maze, POPGym), and standard MDPs (Atari, MuJoCo).\n-\tThe paper provides a theoretical analysis characterizing"}, "weaknesses": {"value": "- Although the motivation is clear, it seems like the performance of RATE relied heavily on previously introduced memory embeddings and cached hidden states, as shown in the ablation study RQ1. This raises a question about the degree of novelty and contribution of the MRV component itself versus inherited advantages from prior architectures.  \n- If the sole contribution of the paper is the Memory Retention Valve (MRV), the manuscript should state this explicitly and carefully articulate the hypothesis for why MRV produces the reported gains. Concretely, the authors should explain the mechanisms by which MRV improves performance (e.g., how it interacts with learned memory embeddings and cached hidden states to prevent overwriting or improve retrieval)\n- Minor: The paper is quite dense, which makes some figures difficult to read due to the small font and cramped layout. While the inclusion of many experiments is commendable, improving figure clarity would help accessibility. In Figure 5, the y-axis label at 0.0 overlaps with the x-axis tick at 〖10〗^1. Slightly adjusting the axis limits or label offsets may resolve this issue.\n- Minor: The authors could include one or two sentences explaining what is meant by “cached hidden states”, as this is not a widely standardized term. Providing a brief description in the main text (rather than requiring readers to refer back to the original paper) would improve readability and self-containment."}, "questions": {"value": "-\tThe Decision Transformer (DT) underperformance on memory-dependent tasks such as T-Maze may stem not from memory limitations, but rather from its lack of trajectory stitching ability; that is, its inability to combine underperforming trajectories into coherent long-horizon trajectories. In cases where underperforming trajectories are more frequent than successful ones, DT will often follow the underperforming paths, further degrading performance.  The results would be more convincing if the authors compared against (applying on top of) more advanced DT architectures, such as Reinformer [1] or Elastic DT (EDT) [2], which explicitly address the trajectory stitching issue.\n-\tIn Table 13, RATE includes extra components such as MRV and memory embeddings, yet it often shows lower training time, inference latency, and GPU memory usage compared to DT. Could the authors clarify why adding these components does not increase computational cost, and how RATE achieves higher efficiency despite the additional modules?\n\n[1] Zhuang, Z., Peng, D., Liu, J., Zhang, Z., & Wang, D. (2024). Reinformer: Max-return sequence modeling for offline rl. arXiv preprint arXiv:2405.08740.\n\n[2] Wu, Y. H., Wang, X., & Hamaya, M. (2023). Elastic decision transformer. Advances in neural information processing systems, 36, 18532-18550."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gh9zDPPEsz", "forum": "kByN4v0M3e", "replyto": "kByN4v0M3e", "signatures": ["ICLR.cc/2026/Conference/Submission25049/Reviewer_kH4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25049/Reviewer_kH4W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691792654, "cdate": 1761691792654, "tmdate": 1762943301889, "mdate": 1762943301889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RATE (Recurrent Action Transformer with Memory), a transformer-based architecture for offline reinforcement learning (RL) that explicitly integrates memory mechanisms to improve performance in partially observable and long-horizon tasks. RATE processes trajectories in segments and passes information between them through memory embeddings, cached hidden states, and a novel Memory Retention Valve (MRV)—a cross-attention module controlling what information to retain or overwrite. The authors provide a theoretical bound guaranteeing partial memory preservation between updates and show strong results across memory-intensive environments (T-Maze, ViZDoom-Two-Colors, Memory Maze, Minigrid-Memory, POPGym) while remaining competitive on standard Atari and MuJoCo benchmarks. Extensive ablations isolate the contributions of MRV and memory embeddings and demonstrate RATE’s robustness in sparse, long-horizon POMDPs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong motivation and clear contribution. The paper identifies a concrete weakness of Decision Transformer (DT)—limited context for long-term dependencies—and addresses it with a principled, integrated memory mechanism rather than ad hoc recurrence.\n\n- Theoretical component. Theorem 1 provides a nontrivial formal result bounding memory preservation under α-alignment, adding rigor beyond empirical evidence.\n\n- Comprehensive experimental coverage. RATE is evaluated on a wide spectrum of domains, from classic MDPs to highly memory-dependent tasks, showing consistent advantages and strong extrapolation (up to 9600 steps in T-Maze)."}, "weaknesses": {"value": "- Incremental novelty. RATE mainly extends prior memory-augmented transformers (RMT, TrXL, Compressive Transformer) by combining existing mechanisms; MRV, while effective, is a variant of cross-attention gating rather than a fundamentally new idea.\n\n- Theoretical analysis scope. The preservation theorem bounds norm differences but does not connect to RL performance or convergence guarantees, limiting theoretical depth.\n\n- Computational and scaling costs. The paper does not quantify the runtime or memory overhead of segment recurrence and MRV compared with DT or RMT, which is crucial for practical adoption.\n\n- Conceptual overlap with existing work. RATE’s motivation and mechanism strongly resemble RMT (2022) with an added gating step; the paper could better articulate its distinct advantages and design rationale.\n\n- Lack of reference: It is noted that this paper compare their method with \"Decision Mamba\". However, there is another existing work which also adopts mamba as its backbone [1].\n\n[1] Lv, Qi, et al. \"Decision mamba: A multi-grained state space model with self-evolution regularization for offline RL.\" Advances in Neural Information Processing Systems 37 (2024): 22827-22849."}, "questions": {"value": "1. How large is the compute/memory overhead of MRV and recurrent caching relative to standard DT or RMT during training and inference?\n\n2. Can MRV’s gating dynamics be visualized—e.g., which segments’ information it chooses to retain or forget?\n\n3. How sensitive is performance to the number of memory tokens m and segment length K?\n\n4. How does RATE behave under noisy or corrupted observation sequences beyond the artificial noise ablation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1zMEAcST2D", "forum": "kByN4v0M3e", "replyto": "kByN4v0M3e", "signatures": ["ICLR.cc/2026/Conference/Submission25049/Reviewer_xQWj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25049/Reviewer_xQWj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741984536, "cdate": 1761741984536, "tmdate": 1762943301520, "mdate": 1762943301520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes augmenting transformers with a memory mechanism to tackle the problem of training RL agents in partially observable environments, where the transformer-based policy suffers from trajectory length. Experiments across various benchmark sets showed that the proposed model improves performance in settings that require memory to solve."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed mechanism, Memory Retention Valve (MRV), is sound and empirically proven to be effective.\n- Figure 2 showed a clear visualisation of RATE and DT when processing information in the task T-Maze.\n- Experiments compared the approach with sufficient baselines across different types, such as transformer-based models, recurrent models, and state space models.\n- Ablation study, especially the first part with noise mem. tokens and noise caching empirically showed the contributions of components in RATE. Different architectural choices are also analysed in the last ablation study section."}, "weaknesses": {"value": "See in the questions section."}, "questions": {"value": "Minors: The algorithm 1 can be clearer in term of describing the outputs. The output of the algorithm 1 is inside the for loop."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7BkBnY41Ew", "forum": "kByN4v0M3e", "replyto": "kByN4v0M3e", "signatures": ["ICLR.cc/2026/Conference/Submission25049/Reviewer_tfiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25049/Reviewer_tfiE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890750019, "cdate": 1761890750019, "tmdate": 1762943301288, "mdate": 1762943301288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors focus on solving POMDPs using transformers, in offline RL scenarios. They propose the Recurrent Action Transformer with Memory (RATE), which implements various modules described below. In particular, the authors focus on efficiently processing long contexts.\n\nThe proposed Memory Retention Valve (MRV) allows information to flow between sliding attention windows. Normal transformers are $O(n^2)$ space complexity, where $n$ is the length of the trajectory. However, by integrating recurrence through the memory retention valve, one can decouple $n$ from the trajectory length. \n\nThe proposed method breaks a trajectory into fixed-length, non-overlapping segments. Memory tokens $M$ span these fixed-length segments, enabling context beyond the sliding window. Each segment goes through self attention to produce embedding tokens $S$. I will admit it is not entirely clear to me what happens next. It appears that the current memory tokens $M$ for the segment $n$ are concatenated twice to either end of the current embedding tokens $S$ (Algorithm 1, line 6). But these are all initialized to 0 (Algorithm 1 line 3).\n\nThe authors go on to prove how quickly inputs vanish through the MRV. To do so, they make the assumptions that the rows of each memory token $M$ are $ell_2$-normalized. I will admit the proof was a bit difficult to follow and I did not check it thoroughly.\n\nFinally, the authors compare their method to more standard models such as the Decision Transformer, Transformer XL, Decision Mamba, GRU, and LSTM. They perform significant evaluation and analysis on T-Maze and related tasks, finding that the proposed method achieves state of the art results. They report similar results on Minigrid and POPGym baselines. \n\nFinally, they perform ablations on their model by replacing various memory model components with random noise. They perform an upper-bound estimate by turning POMDPs into MDPs and training the Decision Transformers on them. Finally, they provide ablation for various gating mechanisms for the MRV."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "Transformers struggle with long contexts, and in RL this is especially painful due to poor sample efficiency. Introducing recurrence into a transformer is a good way to fix this issue.\n\nIn general, the authors provide a good amount of evidence for their method. They compare against other memory models and even training algorithms. Critically, they compare against TrXL which is similar to their method. They provide ablations for each part of their proposed method, and even test different gating methods for the MRV."}, "weaknesses": {"value": "Although the experiments and models appear well-done, I had a lot of trouble reading this paper. I think the text is quite confusing and could use a lot of polish. Please see the comments below, this is not an exhaustive list but just some representative examples:\n- Figure 1 is too blurry and small to read, even after zooming to the maximum allowed\n- Experimental setup and results in the approach (section 3, lines 131-139) seem misplaced\n- The text surrounding the MRV is difficult to understand (lines 141-160, algorithm 1)\n    - Using the same terms to refer to different quantities is the main confusion: MRV takes $M_n, M_{n+1}$ as input, but also outputs $M_{n+1}$\n    - Could use a figure to help the readers understand \n- Section 3.1 is jarring and also a bit difficult to read, I think it should be moved to the appendix\n- Figure 3 left figures are too small and blurry to read at max zoom\n- Figure 6 is too small and blurry to read at max zoom\n\nI cannot recommend acceptance without a substantial rewrite. That said, I want to emphasize that I think there is some good work here. I just wish it was more accessible."}, "questions": {"value": "> unlike static recurrence, it preserves sparse, long-range information. \n\nCan you explain what static recurrence means here?\n\n> prevents catastrophic overwriting of memory by preserving alignment between consecutive memory states.\n\nSimilarly, what is \"alignment\" referring to here?\n\nIn algorithm 1:\n\n> 6: $\\mathrm{concat}(M_n, S_n, M_n)$\n\nIs this a bug? Should it be $\\mathrm{concat}(M_n, S_n, M_{n+1})$?\n\n> $\\hat{a}_n \\rightarrow \\mathcal{L}(a_n, \\hat{a}_n)$\n\nI am not familiar with this notation. I know $\\leftarrow$ is assignment but what does $\\rightarrow$ mean here? \n\n> **Output:**\n\nHow are you returning from inside a for-loop at each iteration? Is this a generator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JosbuqaS5R", "forum": "kByN4v0M3e", "replyto": "kByN4v0M3e", "signatures": ["ICLR.cc/2026/Conference/Submission25049/Reviewer_o12m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25049/Reviewer_o12m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893328461, "cdate": 1761893328461, "tmdate": 1762943300802, "mdate": 1762943300802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}