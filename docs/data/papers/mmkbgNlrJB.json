{"id": "mmkbgNlrJB", "number": 5039, "cdate": 1757838205102, "mdate": 1759897998685, "content": {"title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "abstract": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a novel RL framework that incentivizes the capabilities of LLMs to use a real search engine with simulated searches during training. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both useful and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model’s reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.", "tldr": "", "keywords": ["Retrieval Augmented Generation", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4f4767f0dd6d60b1ee800f6fc2108248bf752e0.pdf", "supplementary_material": "/attachment/b53c3ab82882f9b97373cf3a68664bb63fb1047d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ZeroSearch, an RL framework that trains LLMs to develop search capabilities without interacting with a real search engine. The method replaces the real search environment (e.g., Google via SerpAPI) with a simulated search LLM, which is fine-tuned to generate both “useful” and “noisy” documents. During RL training, a curriculum rollout progressively increases noise levels to encourage robustness. Experiments shows improved Exact Match (EM) performance with zero API cost."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Addresses an important practical bottleneck in RL-based search training — high API costs and unstable document quality — with a controlled simulation-based setup.\n* The curriculum degradation and document quality control mechanisms are conceptually reasonable and could improve stability in RL training.\n* The writing and experimental organization are clear and the comparisons cover multiple model families (Qwen, LLaMA)."}, "weaknesses": {"value": "1. **Metric validity (critical)**\n\nThe paper exclusively reports Exact Match (EM) as its main evaluation metric. However, EM doesn't capture search quality,  **especially for prompting-based method** (an LLM fine-tuned w/ EM could outperform prompting-based method w/ search), so the performance table is not trustworthy.\n\n2. **Unfair baseline setup**\n\nFor non-multi-turn baselines like RAG, the comparison is imbalanced. If ZeroSearch effectively accesses 4 turns × 3 documents = 12 documents (at max.), then RAG should also be given the top 12 documents (from one-shot retrieval) for a fair comparison. Otherwise, the improvement might come from context size rather than genuine search ability.\n\n3. **Missing key baseline (RAG + SFT)**\n\nThe paper omits a critical baseline — RAG + SFT (see *\"Baselines with retrievals.\"* in Self-RAG [1]), i.e., fine-tuning on the top retrieved documents with the ground-truth answer as supervision. This baseline is widely recognized and provides a fair supervised comparison without RL. Without it, the claimed gains from ZeroSearch cannot be meaningfully attributed to the proposed RL method.\n\n4. **Limited novelty**\n\nThe main “innovation” — using a fine-tuned LLM to simulate a search engine — is conceptually incremental. Prior works already train RL agents on retrieval tasks; substituting the search engine with a simulator mainly reduces cost, rather than introducing a new algorithmic principle. The contribution feels more like engineering optimization than a conceptual breakthrough.\n\n5. **Training inefficiency**\n\nAs noted by [2, 3], training uniformly on all examples (easy and hard) is inefficient. ZeroSearch’s curriculum schedule is fixed and lacks adaptive difficulty adjustment, potentially wasting computation and limiting scalability.\n\n\n[1] Asai et al., Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection., ICLR 2024.\n\n[2] Jiang et al., s3: You Don’t Need That Much Data to Train a Search Agent via RL., EMNLP 2025.\n\n[3] Mei, et al., \"AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning.\" arXiv 2025.08."}, "questions": {"value": "1. Was Search-R1 trained using SerpAPI (Google Search) or a static Wikipedia corpus? Section 4.3 states all evaluations use SerpAPI, but it’s **unclear for training**. If Search-R1 was trained on Wikipedia, the comparison is unfair because the domain and style differ significantly.\n\n2. How sensitive are the results to the number of documents per turn or the curriculum parameters (e.g., noise probability schedule)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "83C8dWRxE4", "forum": "mmkbgNlrJB", "replyto": "mmkbgNlrJB", "signatures": ["ICLR.cc/2026/Conference/Submission5039/Reviewer_pjfW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5039/Reviewer_pjfW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670455242, "cdate": 1761670455242, "tmdate": 1762917837033, "mdate": 1762917837033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **ZeroSearch**, a RL framework designed to equip LLMs with the ability to use search engines when their internal knowledge is insufficient. The authors identify two main challenges with prior search-augmented RL setups: 1) the high cost of real API calls and 2) the uncontrolled quality of retrieved documents, and address these by introducing a simulated retrieval module. This module is another LLM, fine-tuned to act as a noisy search engine, generating both relevant and irrelevant documents to mimic real search conditions. During training, the policy model interacts with this simulator through a curriculum rollout strategy that gradually increases retrieval noise, allowing the model to learn effective search behaviors in a controlled environment. Experiments across multiple QA benchmarks and model families show that ZeroSearch matches the performance of models trained with real search engines when using a 7B retrieval simulator, and surpasses them when scaling to 14B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written, and the proposed approach is clearly presented.\n- The main idea of the framework is interesting and well motivated. Since the goal of RL training here is to teach the model how to use a search tool, simulating search with a fixed, noisy model that provides outdated or imperfect information effectively avoids the cost of real API calls while improving robustness to inconsistent or low-quality retrievals.\n- The experimental section is extensive and promising, covering a diverse set of baselines, and datasets. The results are consistently strong and demonstrate both the strength and scalability of the approach.\n- The cost analysis of Appendix E is informative, clearly demonstrating the efficiency of ZeroSearch. The comparison shows that fine-tuning the retrieval module is substantially less expensive than performing repeated real search API calls during RL training."}, "weaknesses": {"value": "- The paper would benefit from quantitative results on the robustness of the reward choice. The authors mention that exact matching was prone to reward hacking, but this claim is only stated qualitatively. Including some quantitative results, or an additional ablation study comparing reward formulations, would make the argument stronger and more convincing."}, "questions": {"value": "- Why did the authors directly chose a RL approach over a SFT strategy, as used in works like Toolformer [1]? Given that the search simulating model is already trained, it should be relatively inexpensive to generate large amounts of multi-turn synthetic data without relying on real APIs. Would performing SFT on such self-generated interactions also yield a strong model, while avoiding the need to design nuanced reward functions?\n\n[1] Schick, Timo, et al. \"Toolformer: Language models can teach themselves to use tools.\" Advances in Neural Information Processing Systems 36 (2023): 68539-68551."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xYnpUdocrr", "forum": "mmkbgNlrJB", "replyto": "mmkbgNlrJB", "signatures": ["ICLR.cc/2026/Conference/Submission5039/Reviewer_ckY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5039/Reviewer_ckY5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034004454, "cdate": 1762034004454, "tmdate": 1762917836500, "mdate": 1762917836500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ZeroSearch, a reinforcement learning (RL) framework for training LLMs to use search tools without using with a real search engine. This is motivated by two major challenges in calling searching engines: (1) unpredictable document quality and (2) high API costs from frequent search calls in RL rollouts. ZeroSearch works by first fine-tuning an LLM to act as a simulated retrieval module, which learns to generate both useful and noisy documents in response to a query. During training, it uses a curriculum-based strategy, starting with helpful documents and gradually increasing noisy contexts. Therefore, ZeroSearch progressively challenges the policy model and improve the models searching / context understanding. Experiments show ZeroSearch can reduce API costs and and achieve similar performance to models trained with a real search engine."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed ZeroSearch trains the model using a simulated search engine, which removes the high costs of making frequent search calls during RL training.\n\n2. Despite not using a real search engine, it solves the problem by using a simulating LLM and control the quality of generated documents with a curriculum learning-based rollout mechanism.\n\n3. The proposed method is effective on single and multi-hop QA tasks, with experiments showing that simulator search engines can match the performance of a real search engine."}, "weaknesses": {"value": "1. The training of ZeroSearch still requires extensive engine calls, which is used to curate the training data for the search simulation LLM. Consider that Search-R1 can converge within a few hundred steps, it is questionable if ZeroSearch really saves on the search costs.\n\n2. While ZeroSearch can reduce API costs to train the policy model, the method introduces a new, significant computational cost in training and deploying a separate LLM as the simulation server, and the costs of which should be discussed in more details."}, "questions": {"value": "1. How much engine calls are required to curate the training data for the search simulation LLM? How does it compares to the Search-R1 training paradigm with actual seach engine / embedding model retrieval?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ok2oy25pqp", "forum": "mmkbgNlrJB", "replyto": "mmkbgNlrJB", "signatures": ["ICLR.cc/2026/Conference/Submission5039/Reviewer_S5aY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5039/Reviewer_S5aY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762039707556, "cdate": 1762039707556, "tmdate": 1762917836255, "mdate": 1762917836255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Typically, training retrieval-augmented models with RL requires querying a retriever (or, say, a Web search API) for tens or hundreds of thousands of times. The authors argue that this can be prohibitive and that the lack of control over the retrieved document quality (i.e., the fact that even excellent search queries can fail to retrieve good documents) pose challenges in practice.\n\nTo tackle this, the authors introduce a method in which a small LLM can be trained (via behavior cloning / SFT) to imitate the behavior of the search engine API, at a low cost, and in a way that permits controlling the 'quality' of the retrieved documents (i.e., noisy vs. relevant). Using this search engine replacement at training time allows the authors to initiate the RL of the query generator with high-quality (fake) search results and to progressively degrades this quality. It also lowers the cost of calling Web search APIs.\n\nThe authors find that RL training with this module works well in practice, and even outperforms related work that trains directly using the search APIs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The problem explored in this work is relatively novel. While this may not be the first time that someone trains a model to imitate a system or API, it is new to my knowledge in this domain (e.g., for RL towards RAG).\n\n2) The ideas and presentation are simple and easy to follow. The results are compared against several recent related papers, and it is easy to appreciate from them that the core of this methods 'works' on the benchmarks selected."}, "weaknesses": {"value": "Weakness 1:\n\nThe authors offer surprisingly little insight into why or how this method works at all. In the general case, it is simply not reasonable to expect that one can imitate an arbitrary search API with imitation learning. This is because there's, in principle, irreducible information in the search engine's knowledge of the world or certain domains that are simply not available in the model's pre-training data.\n\nThis argument does not imply that the method proposed in this paper will fail, because perhaps all that's needed for successful RL is that the questions in the corpus are answerable by the model or perhaps even just that the shape and format of the API responses are reasonable.\n\nBut we see from Table 3 that direct answer (or CoT) with the model itself has very low accuracy, i.e. the model seemingly has limited knowledge of these topics (at least when they're not decomposed). How do the authors explain this? What does the LLM-based search module actually output in practice; how correct or incorrect are its answers and how does that factor into the RL reward?\n\nTo me, this is a concerning weakness. As a reader, I lack a reason to accept how or why this method can work in the first place, which puts more weight behind Weakness 2 below.\n\n\nWeakness 2:\n\nThe benchmarks used like NQ, TriviaQA, etc, while consistent with some recent papers, are all likely contaminated, old, and too easy. (There's no issue in including these tasks; what I take issue with is the absence of anything else.) I have to wonder whether the true explanation for weakness 1 is something along the lines of \"the models know a whole lot about the topics in these datasets\".\n\nFor example, the literature on HotPotQA from 2018-2021 would routinely achieve scores like 45-65%, even using much smaller and fundamentally weaker models, like BERT. Of course, much of this can be attributed to searching a Wikipedia dump versus searching the Web with a Web Search API, but the main point here is that with Weakness 1 in place, it's hard to understand how interesting a 34% on this benchmark is supposed to be. Aren't at least 30% of the questions in the benchmark answerable by the models (or even memorized by the models?) with some multi-hop decomposition?\n\nUltimately, I think Weaknesses 1 and 2 should be taken together holistically. I would be somewhat less concerned about the benchmarks if the _mechanism_ or even _patterns_ by which the method works were understood."}, "questions": {"value": "See Weakness 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2zybwu9VnV", "forum": "mmkbgNlrJB", "replyto": "mmkbgNlrJB", "signatures": ["ICLR.cc/2026/Conference/Submission5039/Reviewer_vjRe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5039/Reviewer_vjRe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762259365921, "cdate": 1762259365921, "tmdate": 1762917835854, "mdate": 1762917835854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}