{"id": "meRHki4HwQ", "number": 4395, "cdate": 1757671897732, "mdate": 1759898034853, "content": {"title": "MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark", "abstract": "The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question–answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning research. Code and benchmark can be found at https://anonymous.4open.science/r/MDAR-8981.", "tldr": "We introduce MDAR, a benchmark that reflects the capabilities of LALMs in handling challenging, multi-scene, and dynamically evolving audio reasoning tasks, and promotes the development of benchmarks in the field of audio reasoning research.", "keywords": ["Benchmark", "LALMs", "Audio reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a653fa060f357446d60638ed097efb56e822539.pdf", "supplementary_material": "/attachment/479e1c3d37d9982732b056c13db45c5a55374af0.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmark, MDAR, to evaluate audio reasoning capabilities of LALMs. \nMDAR emphasizes complex tasks with multi-scene and potentially dynamically evolving contexts. \nMDAR contains 3K curated samples for 5 categories and 3 question types. \nThe paper evaluates MDAR on 26 SOTA LALMs and find these models have limitations on these tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Overall, the paper aims at studying a very important problem in the audio understanding domain. It remains a challenge to evaluate and enhance the complex reasoning abilities of current LALMs, and this paper proposes a solid benchmakr for more holistic evaluation of this area. \n\n- The test data construction pipeline is very clear, transparent, and reproduce-able. The taxonomy of different types of test samples (different categories and question types) could benefit the community to conduct more fine-grained analysis and evaluation of their models.  These also complement existing benchmarks (e.g. MMAU and MMAR) that have different taxonomies (e.g. mainly on audio domains). \n\n- The baseline evaluation is quite extensive. The paper evaluates many open and closed models and the results are valuable. In addition to just overall accuracies, the paper further conducts detail analysis such as error types in Fig 6. The results are very valuable as it tells us that most mistakes are related to mis-reasoning and some are mis-perception. These could benefit further RL design choices of LALM post training."}, "weaknesses": {"value": "- As stated in L215, test samples are all from Chinese films and therefore the distribution can be biased. This could limit the versatility of the proposed benchmark, and may also be unfair to LALMs that are only trained on English corpus / data. \n\n- While Fig 3 shows the distribution and other strengths of the proposed MDAR, it is hard to quantitively prove how much better MDAR is compared to existing benchmarks like MMAU and MMAR. To me it sounds more like a complement to those benchmarks. I suggest the authors to conduct a rigorous human study on how MDAR is better than prior benchmarks, despite that MDAR covers more skills and contains longer audio. Similarly, the paper claims the samples are complex, multi-scene, and the context can be dynamic, but these require rigorous proof. \n\n- The evaluation of open-ended samples is based on LLM-as-a-judge. This can be unstable and unreliable in terms of rigorous comparison. \n\n- The experimental analysis is not extensive at all. While there are main accuracies and error analysis, there is no in-depth analysis on different types of errors. For instance, why do the models have these errors? What are the causes of these errors -- capacity issues, hallucination, randomness when responding, or others? It worths studying these deeper information so that the community could further improve their models. \n\n- Following the above point, the paper does not indicate readers how to build better LALMs. Should we use better SFT or better RL? Should we scale the base LLM or not? What types of data to curate to fill the gap? Does Fig 6 indicate we should blend more text-specific data? How to design efficient methods for reliable understanding of longer audio? I would expect the paper to show some route for building better LALMs -- while the ultimate goal is always to have more and higher-quality data in the end -- the benchmark analysis should guide us to reach the goal more efficiently and effectively with principled recipes and experiences."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FSrNAnPg7B", "forum": "meRHki4HwQ", "replyto": "meRHki4HwQ", "signatures": ["ICLR.cc/2026/Conference/Submission4395/Reviewer_zBaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4395/Reviewer_zBaU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950806817, "cdate": 1761950806817, "tmdate": 1762917337209, "mdate": 1762917337209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MDAR, a new benchmark designed to evaluate multi-audio, multi-scene reasoning abilities in LALMs. Unlike prior benchmarks that focus on single-audio or localized perception tasks, MDAR tests whether models can reason compositionally across multiple audio scenes involving speech, environmental sounds, and music. The benchmark aims to push beyond simple perception toward holistic scene understanding, including temporal order, causal interactions, and semantic relationships between events. The final dataset contains 8,524 QA pairs across five task types: causal reasoning, temporal reasoning, multi-speaker dialogues, cross-scene summarization, and auditory anomaly detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- To the best of my knowledge, this is the first benchmark to target multi-scene dynamic reasoning, a previously unaddressed challenge for LALMs. MMAU-Pro released recently addresses this, but I understand it may be parallel work.\n- Well-structured taxonomy of reasoning skills.\n- Balanced synthetic-real composition pipeline ensuring diversity and controlled difficulty.\n- The dataset also has an open and MCQ part, which is exciting and important.\n- Comprehensive evaluation across open- and closed-source systems."}, "weaknesses": {"value": "- Heavy reliance on synthetic compositions may limit real-world generalization.\n- I do not have many weaknesses to point out.\n- Is the dataset in Mandarin? I see the benchmark and looks in Supplementary and looks like its all Mandarin?"}, "questions": {"value": "- I am curious how multi-audio evaluation was done on models like AF2 and AF3 and maybe Qwen-3-omni, I did not find a straightforward way to do this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iqtTU5ntOm", "forum": "meRHki4HwQ", "replyto": "meRHki4HwQ", "signatures": ["ICLR.cc/2026/Conference/Submission4395/Reviewer_JsvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4395/Reviewer_JsvT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965708316, "cdate": 1761965708316, "tmdate": 1762917337018, "mdate": 1762917337018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MDAR (Multi-scene Dynamic Audio Reasoning), a benchmark targeting complex, real-world audio reasoning across three complementary test suites: MDAR-main (1,500 single-choice QAs across five categories), MDAR-open (500 open-ended QAs scored by an LLM judge), and MDAR-multi (825 multi-audio, multi-answer QAs emphasizing cross-clip reasoning). Data are drawn from ~20–40s segments of Chinese movies with speaker diarization and global clustering; FunASR supplies transcripts; Gemini-2.5-Pro and Qwen-2.5-VL generate multimodal descriptions and help author questions/distractors under an expert-designed taxonomy and multi-step QA screening. MDAR reports diverse metrics (regex-based accuracy for single-choice, LLM-judge 0–10 for open-ended, and EM/JI/Precision/Recall for multi-audio). Baselines show the task is hard: best non-cascaded model in MDAR-main (Qwen2.5-Omni) reaches ~76.7%; MDAR-open tops out near 7.46/10; ablations (e.g., replacing audio with Gaussian noise) substantially degrade accuracy, underscoring audio dependence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The three suites (single-choice, open-ended, multi-audio) cover perception --> understanding --> cross-clip reasoning, with five high-level categories (scene, social, event, temporal, safety).\n- Well-defined metrics: Regex-based accuracy for MDAR-main, an LLM-judge rubric for MDAR-open, and EM/JI/Precision/Recall for MDAR-multi are specified.\n- The data generation pipeline is strong and well thought out."}, "weaknesses": {"value": "- One of my biggest concerns is that all the QA are in Chinese in the supplementary material, but the examples show in the paper are in English. If the questions are fed to models like Audio Flamingo 3, which are not trained on QA pairs in the specific language, that might not be a fair evaluation. Also, the authors should explicitly mention that the QAs are in Chinese.\n- The evaluated methods score significantly higher than they do on the benchmarks already released, which questions the difficulty of the benchmark.\n- Since, most of the benchmarks is derived from movie clips, it would benefit from having questions around music (background/foreground).\n- No correlation is shown between the Human & LLM as a judge. What is the correlation (e.g., Spearman/Pearson, Krippendorff’s α) between the LLM-judge scores and a human panel across categories?\n- Mixing accuracy (closed-set), judge scores (open-ended), and set metrics (multi-audio) complicates aggregate interpretation; there’s no unified “overall” score or calibration across metrics."}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J1ZAZmXgZe", "forum": "meRHki4HwQ", "replyto": "meRHki4HwQ", "signatures": ["ICLR.cc/2026/Conference/Submission4395/Reviewer_g5UY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4395/Reviewer_g5UY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762118704494, "cdate": 1762118704494, "tmdate": 1762917336788, "mdate": 1762917336788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MDAR which is a benchmark for multi-scene, dynamically evolving audio reasoning. It contains 3000 question-ans pairs spanning five different categories (Scene Understanding, Social Relationships, Event Reasoning, Temporal Reasoning, Anomaly and  Safety) and three formats: single choice on a single audio (also called MDAR-main), open-ended responses (MDAR-open), and (iii) a new multi-audio, multiple-choice setting (MDAR-multi). The data prep pipeline takes 500 Chinese films, samples 20 - 40s clips, applies speaker diarization and global segment clustering, then uses ASR plus LLMs (Gemini/Qwen) with expert screening to finalize questions and distractors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Dynamic, multi-scene coverage across five reasoning axes and three formats\n- Longer and information dense clips (average 25.11s) compared to the prior work, raising temporal reasoning difficulty in the benchmakr\n- Comprehensive evaluation of 26 models\n- Systematic evaluation of both perceptual and high-level reasoning abilities\n- Highlights key areas for improvement in next-generation audio reasoning agents"}, "weaknesses": {"value": "- Chinese movies as the metadata source biases the benchmark towards a particular accent/way of speaking. The benchmark should be diverse in the data to avoid biased results\n- The SOTA performance on the benchmark closing in on 80%\n- (General comment) The paper has a low excitement factor and raises a concern of lack of novelty with it being yet another benchmark. Even the paper/analysis/diagrams format exactly follows MMAU\n- MMAU pro has not been included in the comparison with existing benchmarks\n- error analysis has been done by Gemini-2.5-flash - ideally the errors should be identified and categorised by a human to get an accurate analysis. what prompt was used for this analysis? what are the examples of gemini-2.5-flash failing to do the required job?"}, "questions": {"value": "1. How do you define complex reasoning and how is it different from reasoning? The paper mentions the word complex multiple times but it needs to define what it means for audio\n2. How difficult is it to improve performance on the benchmark?\n3. fig 3b total questions don't sum up to 3000. can you confirm?\n4. the paper selects best prompt for accuracy, how sensitive are rankings to different prompts? \n5. how much would the human eval score be?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EiZIlG29yF", "forum": "meRHki4HwQ", "replyto": "meRHki4HwQ", "signatures": ["ICLR.cc/2026/Conference/Submission4395/Reviewer_eGBs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4395/Reviewer_eGBs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762736354442, "cdate": 1762736354442, "tmdate": 1762917336568, "mdate": 1762917336568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}