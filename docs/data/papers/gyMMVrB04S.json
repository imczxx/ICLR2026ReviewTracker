{"id": "gyMMVrB04S", "number": 21225, "cdate": 1758315128694, "mdate": 1759896933364, "content": {"title": "SNOWFL: Efficient and Heterogeneous Federated Learning with SNIP-Owen-values", "abstract": "Cross-device federated learning often faces heterogeneous clients. These clients carry data with very different values for training high-performance, generalized global models, calling for effective contribution estimation mechanisms. Width scaling with thinner subnetworks and depth scaling via early exits enable participation for heterogeneous clients but still suffer from (i) noisy aggregation across mismatched subnetworks, (ii) under-trained deep layers when few clients reach them, and (iii) costly, client-isolated contribution estimates. We propose SNOWFL, which pairs server-side single-shot pruning at initialization pruning (SNIP) with coalition-structured Owen valuation. SNIP uses a small public, unlabeled set to score connections by loss sensitivity and produce layer-consistent width masks per tier aligned with fixed early exits. During training, we estimate client contributions by first computing Owen values for coalitions and then allocating credit within each coalition via update alignment and diversity. These contribution estimates will be used in both weighted aggregation and drive capacity-aware reassignment. We prove nonconvex convergence to stationarity and, under strong convexity on the retained subspace, linear convergence to a neighborhood. Under matched FLOPs and parameter budgets, SNOWFL achieves state-of-the-art accuracy on vision and language benchmarks, improving strong heterogeneous baselines by up to 15%, while valuation remains data-free except for the small public samples used once for initialization.", "tldr": "", "keywords": ["Federated learning", "system heterogeneity", "heterogeneous model scaling", "pruning", "early-exit networks", "client contribution valuation", "Owen value"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/209b85426ff0e46523301224a68b4b368641f450.pdf", "supplementary_material": "/attachment/b3535522c72b6f49a19c9dd79fbd58c3af6ba408.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an integrated framework that combines system heterogeneity and contribution estimation in federated learning. First, before training, the method measures channel-wise saliency using public data to pre-select which channels to prune and where to place exit points. For each tier group corresponding to a different resource capability, it then fixes the channel masks and exit positions that can fit within the given resource budget. Second, the framework measures client contributions in two stages: first, it evaluates the contribution of each tier as a whole, and then it measures the contribution of individual clients within each tier. The measured contributions are then used to compute aggregation weights during training or to reassign clients across tiers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* It is novel in that it simultaneously considers both contribution estimation and system heterogeneity in federated learning.\n\n* It leverages only the gradients on public data and the local updates, without incurring any additional privacy cost.\n\n* The experiments demonstrate that the proposed method outperforms system heterogeneity–related baselines under the same resource budget."}, "weaknesses": {"value": "**Necessity of public data.** While the reliance on public data can itself be considered a limitation, a more concerning issue is that pruning is performed before training. This requires the assumption that the public dataset used for pruning adequately represents the actual client data used during training, an assumption that is unlikely to hold in practice. Even in BN calibration, the approach relies on the assumption that the public data are well aligned with the private client data.\n\n**The contribution of the saliency-guided pruning at initialization is not sufficiently convincing.** Fixing both the network width and depth before training makes the method overly dependent on public data, and the experimental results provided are insufficient to adequately analyze its effectiveness.\n\n**Although many of the detailed components of the method are empirically determined, the paper does not provide sufficient discussion or experimental evidence to justify these choices.** For example, in Equation (10), the peer-diversity term and the global alignment term appear somewhat conflicting, and several hyperparameters are introduced without any accompanying ablation study. Moreover, the experiments mainly present overall results, offering only limited evidence of the proposed method’s effectiveness.\n\n**Writing.** The paper lacks sufficient detail in describing the method and experimental settings. For example, the client reassignment section is not clearly explained, and it remains unclear how each client’s resource capability is constrained or how the public dataset is constructed and utilized."}, "questions": {"value": "* Given the models and datasets used, the absolute accuracy values reported in Table 1 (the main results) seem unusually low. What could be the reason for this?\n* What is the purpose of applying clipping in Equation (7)?\n* How were the clients’ resource budgets constrained in the experimental setting?\n* In SnowFL, client reassignment appears to ensure that each sub-model is trained not only on fixed clients’ data but also on data from a diverse set of clients. Under what conditions or environments were the baselines evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LKbwIOKVME", "forum": "gyMMVrB04S", "replyto": "gyMMVrB04S", "signatures": ["ICLR.cc/2026/Conference/Submission21225/Reviewer_P3b2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21225/Reviewer_P3b2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553701998, "cdate": 1761553701998, "tmdate": 1762941633379, "mdate": 1762941633379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The SNOWFL mainly combines three concepts into a single heterogeneous FL framework, 1) width-reduction (prunning), 2) depth-reduction (multi-exit networks), and 3) client valuation and weighted aggregation. Overall, although none of the three are new, a unified framework that integrates all of them can be regarded as a fairly strong contribution. The paper mostly focuses on the third component, how to prune before starting the training using minimal global data and how to aggregate the locally trained models based on their contributions. The proposed framework is compared with some recently proposed heterogeneous FL methods and theoretically analyzed in terms of grouping convergence where Owen value matches the Shapley value."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  As described in paper summary, this work provides a unified parameter-efficient heterogeneous FL framework which combines three different techniques. I appreciate the general framework and analysis.\n\n2. The Owen-style grouping mechanism is not popularly used in FL community, and introducing the technique to FL community is a strong contribution.\n\n3. The paper contains solid theoretical analysis (though it is not included in the main text)."}, "weaknesses": {"value": "1. The SNOWFL mainly combines three concepts into a single hetergeneous FL framework, 1) width-reduction (prunning), 2) depth-reduction (multi-exit networks), and 3) client valuation and weighted aggregation. Overall, although none of the three are new, a unified framework that integrates all of them can be regarded as a fairly strong contribution. The paper mostly focuses on the third component, how to \n\n2. The authors argue that the depth values (exit placements) are fixed at the architecture level. However, it is not convincing because the depth directly determines how much system resources are required to train the network. The prunning is applied to individual subnetworks because the depth is assumed to be determined in advance. I believe this design overly simplifies the problem. Users may need to determine the appropriate width and depth jointly, taking into account their available resources such as memory capacity or network bandwidth. In this case, Phase 1 of SNOWFL may need to be modified.\n\n3. Based on my experiences, BN statistics quality plays a key role in achieving good model accuracy. However, the empirical study in this paper does not show its impact of BN calibration. It appears as a single subsection, and thus I assume it is a critical component of SNOWFL. How is the performance affected by this calibration? The authors should provide more empirical results regarding this feature.\n\n4. Section 4.5 looks redundant. SNOWFL employs SNIP and it has been discussed already. Per-round valuation cost would be better to be discussed in section 4.3 to make the section self-contained. Privacy also can be discussed when introducing each phase.\n\n5. The theoretical analysis is critical information which supports the efficacy of Owen valuation and Shapley allocation, but it only appears in the appendix. I understand that the allowed page count is insufficient always, however the authors should have included at least the summary or the key results in the main text. Currently, due to the lack of any theoretical justifications, the proposed method is not convincing enough.\n\n6. The empirical study also has several issues. First, the comparison lacks a few critical related works in heterogeneous FL, shown as follows. FjORD is a representative width reduction-based heterogeneous FL method and EmbracingFL is a recently proposed depth reduction-based heterogeneous FL method. Comparing SNOWFL with them will make the empirical results more powerful.\n\n[1] Horvath et al., FjORD:  fair and accurate federated learning under heterogeneous targets with Ordered Dropout, NeurIPS 2021.\n\n[2] Lee et al., Embracing FL: Enabling Weak Client Participation via Partial Model Training, IEEE Trans. on Mobile Computing, 2024.\n\n7. While the empirical results look promising, the range of experimental settings is too limited. There are only two tables that directly compare SNOWFL with other heterogeneous FL methods in terms of model accuracy. What about the effectiveness of the proposed Owen valuation and Sharpley allocation as compared to other parameter valuation or weighted aggregation methods? E.g., is it always better than gradient-norm based parameter importance metrics [3,4]? What about the weight-to-gradient ratio in [5]? What about just a simple uniform random sampling or loss-based aggregation? There are many interesting and critical experiments that should have been considered. I see some ablation studies are discussed in the appendix, but they do not include these key results.\n\n[3] Li et al., Enhancing Large Language Model Performance with Gradient-Based Parameter Selection, arXiv.\n\n[4] Zhang et al., Gradient-based Parameter Selection for Efficient Fine-Tuning, CVPR 2024.\n\n[5] Kim et al., layer-wise update aggregation with recycling for communication-efficient federated learning, NeurIPS 2025.\n\n8. Algorithm 1 is written too verbally. Some lines could be replaced with just pointing out equations. Its readability is seriously poor.\n\n9. Overall, Section 2 and 3 take up too much space and it results in pushing key results to the appendix. I strongly recommend re-writing those section concisely and bring the important results back to the main text.\n\nDue to the several limitations above, I cannot give a positive score for now. I will check the rebuttal and re-evaluate this work."}, "questions": {"value": "My questions are included in the above weakness section. Please carefully address them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "nclFXFCAVl", "forum": "gyMMVrB04S", "replyto": "gyMMVrB04S", "signatures": ["ICLR.cc/2026/Conference/Submission21225/Reviewer_v5aC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21225/Reviewer_v5aC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637106811, "cdate": 1761637106811, "tmdate": 1762941633101, "mdate": 1762941633101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SNOWFL, a framework designed to make federated learning (FL) more efficient and robust to device heterogeneity. The method assigns clients subnetworks of different sizes using a one-time SNIP-based pruning step at the server, which generates tiered masks that define each client’s capacity level. A small public or unlabeled dataset is used for batch normalization calibration to ensure consistent activation statistics across tiers. To evaluate client contributions fairly, SNOWFL employs an Owen-value–based scheme, which first measures the collective contribution of each tier and then distributes value within the tier based on gradient alignment and diversity. The framework aims to reduce training cost and improve fairness without additional communication overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses an important practical issue in cross-device FL: heterogeneous compute and how to engage weak devices without hurting the global model. \n2. The pipeline is relatively easy to implement, making it more deployable in practice. \n4. Experiments are broad and include ablations that show each component contributes."}, "weaknesses": {"value": "The paper rediscovers an important fact that consensus of masks is essential in model pruning in FL. The paper compares to several heterogeneity baselines (e.g., DepthFL) but omits many other relevant works: SparseFL [1], EmbracingFL [2], PriSM [3], etc. \n\nParticularly, SparseFL was the first work that demonstrates that even when data across clients is significantly non-IID, a consensus in sparsity masks for local training is essential. [1] develops a consensus strategy without requiring any public datasets. This work simply generalizes the idea to having more than one consensus mask. \n\nEmbracingFL proposed a new idea, where instead of doing early exit, one can instead allocate only the output side subset of layers to clients for local training. In EmbracingFL, one doesn't need additional BN harmonization as the method implicitly takes care of that. Works like these and their follow ups have been ignored in the paper. \n\nPriSM proposes a SVD based model principal component dropout strategy for creating sub-models for clients that are good approximations of the global model. Additionally, clients' models together provide an excellent coverage of the all the principal components of the global model, thus providing an effective way to preserve global model performance even in heavily resource (compute/memory/communication) constrained settings. \n\nOther weaknesses are as follows:\n\n1. Heavy reliance on a public / unlabeled dataset at server: SNOWFL’s SNIP masks and BN “harmonization” use a public set. This is central to performance but is unrealistic in many cross-device settings, introduces clear bias risks (public set not representative), and creates an attack surface (adversarial or poisoned public set). The paper notes the choice but does not quantify robustness to different or adversarial public sets.\n2. Novelty is incremental: Components (SNIP pruning at init, early exits, contribution weighting via Shapley/Owen) are all existing techniques; SNOWFL’s contribution is their combination plus some pragmatic design choices. The paper lacks a new algorithmic/principled mechanism that meaningfully advances the state of the art beyond engineering integration. The theoretical results are also boilerplate adaptations of standard FL proofs to masked exits.\n3. Computation & scalability of per-round valuation. Tier-level Shapley via MC permutations and the within-tier allocation are costly (authors note this can be reduced), but the practical wall-clock cost, memory and communication overheads are not measured. For large settings, this could be prohibitive. The assumptions used in convergence theorems are too strong for practice.\n4. Sensitivity and robustness analyses are limited and key hyperparameters lack sensitivity studies. The ablations show removing SNIP/Owen hurts, but they don’t show failure modes (e.g., poor public set → collapse).\n5. Empirical claims are uneven across datasets. The paper should acknowledge where it doesn’t dominate with justifications.\n\n[1] Revisiting Sparsity Hunting in Federated Learning: Why does Sparsity Consensus Matter? (TMLR 2023)\n[2] Embracing federated learning: Enabling weak client participation via partial model training. (IEEE TMC 2024)\n[3] Overcoming Resource Constraints in Federated Learning: Large Models Can Be Trained with only Weak Clients. (TMLR 2023)"}, "questions": {"value": "Please address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xXNU2TQudP", "forum": "gyMMVrB04S", "replyto": "gyMMVrB04S", "signatures": ["ICLR.cc/2026/Conference/Submission21225/Reviewer_eVeh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21225/Reviewer_eVeh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896700426, "cdate": 1761896700426, "tmdate": 1762941632813, "mdate": 1762941632813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SNOWFL (SNip-OWen-values Federated Learning) addresses heterogeneous federated learning where clients have varying computational capabilities. The paper's key innovation combines two main components: a) SNIP-based pruning at initialization: Uses server-side Single-shot Network Pruning to create task-aware, layer-consistent width masks for different client tiers, aligned with fixed early exits. This is done once using a small public/unlabeled dataset, avoiding expensive iterative pruning; b) Owen value-based contribution estimation: Extends Shapley values to coalition structures (client tiers), first computing group-level contributions via quotient-game Shapley, then allocating within groups based on update alignment and diversity. These contributions drive both weighted aggregation and capacity-aware client reassignment. Under matched FLOPs and parameter budgets across vision (CIFAR-10/100, FEMNIST) and language (Shakespeare) benchmarks with non-IID data, SNOWFL achieves state-of-the-art accuracy, improving over strong baselines by up to 15% relative improvement. Ablations confirm both components contribute (Owen has larger standalone effect), and the paper provides convergence guarantees showing nonconvex convergence to stationarity and linear convergence under strong convexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Good empirical results - The quantitative improvements are substantial: up to 15% relative gain (9.1 absolute points on CIFAR-10 α=0.1: 45.9% vs 36.9%) represents meaningful progress over recent strong baselines. The consistency across datasets (vision and language) and heterogeneity levels strengthens the claim. \n\n2. Comprehensive evaluation: Authors thoroughly validate the experimental design with ablations isolating components (Table 3), sensitivity studies (M, T_reg), per-tier analysis, and reproducible code\n\n3. Novel synthesis: While neither component is new individually, their integration is creative: (1) using SNIP server-side with public data to generate tier-compatible subnetworks is a fresh take on pruning-at-initialization in FL, avoiding the client-data dependency and iterative retraining of prior work; (2) adapting Owen values to naturally arising coalition structures (client tiers) rather than treating clients independently is conceptually elegant and computationally sensible"}, "weaknesses": {"value": "1. High complexity: 10+ hyperparameters (M, T_reg, T_warm, γ_t, α_t, ρ, λ_b, etc.), multi-stage pipeline (Phase I SNIP + Phase II Owen + BN calibration), coordinate-wise masked aggregation—significant implementation burden without clear tuning guidance\n\n2. Uneven gains: FEMNIST improvement negligible (84.2% vs 84.2% ReeFL), slower early convergence (Figure 1), no statistical testing or error bars—unclear when SNOWFL helps vs simpler methods\n\n3. Public data dependency: Requires task-relevant public/unlabeled set for SNIP and BN calibration; sensitivity to set size/quality not studied; may not be available or well-matched in practice\n\n4. Incomplete efficiency analysis: No wall-clock runtime, communication cost, or per-round overhead comparison; Owen valuation cost (M permutations) not quantified vs baselines"}, "questions": {"value": "1. When does SNOWFL help? Why marginal gains on FEMNIST but strong on CIFAR? Can you characterize problem settings (data heterogeneity, model capacity, tier count) where SNOWFL outperforms simpler baselines like ReeFL?\n\n2. Simplified variant? Can you ablate to \"minimal SNOWFL\" (e.g., fixed tiers + uniform Owen, or SNIP-only without contribution weighting) to isolate essential components and reduce hyperparameter burden?\n\n3. Public data sensitivity: How does performance degrade with smaller/mismatched public sets? What happens if public data is unavailable : can synthetic data or server-side aggregates substitute?\n\n4. Practical overhead: What are actual wall-clock training times and communication costs vs baselines? How does Owen valuation cost scale with M, G, |S_t|—is it negligible or a bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1fOEinEdTz", "forum": "gyMMVrB04S", "replyto": "gyMMVrB04S", "signatures": ["ICLR.cc/2026/Conference/Submission21225/Reviewer_bnZC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21225/Reviewer_bnZC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762108659596, "cdate": 1762108659596, "tmdate": 1762941632473, "mdate": 1762941632473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}