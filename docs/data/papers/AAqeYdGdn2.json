{"id": "AAqeYdGdn2", "number": 4490, "cdate": 1757689310317, "mdate": 1763641754602, "content": {"title": "PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric Depth Estimation", "abstract": "While current high-resolution depth estimation methods achieve strong results, they often suffer from computational inefficiencies due to reliance on heavyweight models and multiple inference steps, increasing inference time. To address this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner models with lightweight encoders. This reduces model size and inference time but introduces noisy features. To overcome this, we propose a Coarse-to-Fine (C2F) module with a Guided Denoising Unit for refining and denoising the refiner features and a Noisy Pretraining strategy to pretrain the refiner branch to fully exploit the potential of the lightweight refiner branch. Additionally, we propose to adopt the Scale-and-Shift Invariant Gradient Matching (SSIGM) loss within local windows to enhance synthetic-to-real domain transfer. PRV2 outperforms state-of-the-art depth estimation methods on UnrealStereo4K in both accuracy and speed, using fewer parameters and faster inference. It also shows improved depth boundary delineation on real-world datasets like CityScapes, demonstrating its effectiveness.", "tldr": "", "keywords": ["Depth Estimation", "High Resolution"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e9ce24fbc53d3f44f2ae339adb840ea19f6c9dd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends PatchRefiner (ECCV 2024) for high-resolution monocular metric depth estimation. PatchRefiner V2 replaces the heavy refinement encoder with a lightweight alternative to significantly reduce runtime and memory usage during both training and inference. To compensate for the reduced capacity, the method introduces a coarse-to-fine module that injects coarse-level features to guide and denoise refinement features, along with a noisy pretraining strategy to improve the refiner's robustness. The paper also adopts an improved local-window SSIGM loss to enhance synthetic-to-real transfer. Experiments on UnrealStereo4K and Cityscapes demonstrate improved accuracy and efficiency compared to the original PatchRefiner."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The proposed PatchRefiner V2 achieves impressive improvements in runtime and memory efficiency over the original PatchRefiner, while also achieving equal or better accuracy. This makes high-resolution monocular depth estimation more feasible for practical use.\n\n- The coarse-to-fine module is well-motivated and effectively compensates for the reduced capacity of the lightweight refiner. The guided denoising design is clearly presented, and its contribution is supported by thorough ablation studies.\n\n- The noisy pretraining strategy is simple yet empirically effective. Pretraining the refinement branch with randomized coarse features leads to a more robust model without requiring additional external data or complex setups.\n\n- The local SSIGM loss provides more precise supervision. Enforcing scale-and-shift consistency at a local spatial level results in sharper depth boundaries and avoids introducing scale error.\n\n- The paper is well-written and easy to follow, with clear figures and architectural diagrams that effectively explain the design choices.\n\n- The visual annotations (e.g., the snails and lightning bolts in Fig. 1) clearly and intuitively highlight the performance and efficiency differences. The visualization of features in Fig. 2 clearly presents the motivation and effectiveness of the fusion model. These figures improved readability and made the narrative smoother and more engaging."}, "weaknesses": {"value": "- Discussion of Related Work Could Be Expanded.\nThe motivation and effectiveness of the proposed coarse-to-fine module are clearly presented, and the design is well-justified. However, there are existing two-branch feature fusion strategies in related areas (e.g., [1] and [2]). While these works focus on different tasks and modalities, a brief discussion comparing the design philosophy or fusion flow direction could further clarify the novelty of the proposed C2F module and situate the contribution more explicitly in the broader literature.\n\n[1] Bi-SSC: Geometric-Semantic Bidirectional Fusion for 3D Scene Completion\n\n[2] FFB6D: Full Flow Bidirectional Fusion for 6D Pose Estimation\n\n- Clarification of PRV2’s Advantage Over High-Resolution Backbone Models.\nThe improvement of PRV2 over DepthPro is quite substantial, which strongly supports the value of the refinement design. Since DepthPro is already a high-resolution metric depth model, it would be helpful for the paper to provide a bit more insight into why PRV2 achieves such notable gains when refining DepthPro outputs. \n\n- While PRV2 is not intended to be a general-purpose “zero-shot” depth refiner, a short discussion of the expected generalization behavior could help guide future follow-up work aiming toward more foundational refinement pipelines.\n\n\n- There is a small typo in Table 2: in the caption, “GM and wins.” can be removed for clarity.\n\n- Since the updated local-window SSIGM loss is one of the key improvements, adding a brief pseudo-code snippet in the supplement would make re-implementation easier. This would improve the usability."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BAVb119Lxl", "forum": "AAqeYdGdn2", "replyto": "AAqeYdGdn2", "signatures": ["ICLR.cc/2026/Conference/Submission4490/Reviewer_pdNs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4490/Reviewer_pdNs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630375681, "cdate": 1761630375681, "tmdate": 1762917396659, "mdate": 1762917396659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PatchRefiner V2 (PRV2), an enhanced framework for high-resolution monocular depth estimation that aims to address the computational inefficiencies of its predecessor, PatchRefiner (PRV1). The core contributions are a lightweight refiner branch, a novel Coarse-to-Fine (C2F) module with Guided Denoising Units (GDUs), a Noisy Pretraining (NP) strategy, and a local Scale-and-Shift Invariant Gradient Matching (local SSIGM) loss. The paper is well-structured, the problem is clearly motivated, and the experimental evaluation is comprehensive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper effectively identifies the critical bottlenecks of PRV1—high inference time, large memory footprint, and the inability for end-to-end training—due to using a heavyweight base model for patch-level refinement. The motivation for replacing it with a lightweight encoder is well-justified and addresses a practical need for real-world applications.\n2.The idea of using coarse features to \"denoise\" the features from a lightweight encoder is intuitive and effective. The GDU mechanism is clearly explained and visualized showing a tangible improvement in feature quality.\n3. Noisy Pretraining is a simple yet clever strategy to force the refiner branch to learn robust, depth-relevant features from the high-resolution input itself. The ablation studies strongly validate its importance.\n4. Extending the SSI loss to the gradient domain and applying it within local windows is a thoughtful approach to improve boundary accuracy without compromising global scale. The significant improvement in the boundary F1 score on CityScapes is a key result.\n5. The paper provides thorough quantitative and qualitative evidence. The results on UnrealStereo4K are impressive, demonstrating that PRV2 can achieve state-of-the-art or comparable accuracy with a massive reduction in parameters (up to 9.2x) and inference time (up to 10.7x faster). The ablation studies are systematic and clearly demonstrate the contribution of each proposed component (C2F, NP, E2E training, local SSIGM).\n6. The inclusion of experiments on a real-world dataset (CityScapes) and the analysis of boundary quality are highly valuable and demonstrate the method's practical utility.\n7. The method is described in sufficient detail, with clear diagrams and mathematical formulations for the GDU and local SSIGM loss. The implementation details provided in Section 4.2 are adequate for reproduction."}, "weaknesses": {"value": "1The GDU is a central component, but the ablation only compares it to one alternative. A more detailed analysis, for instance, comparing the proposed sigmoid-based gating to an additive fusion or an attention-based mechanism, would provide deeper insights into why the current design is optimal.\n2.  While the overall framework is much faster, the specific computational cost introduced by the C2F module and the local SSIGM loss (during training) is not discussed. A brief note on their relative overhead would be useful for readers considering implementation.\n3. The experiments are focused on UnrealStereo4K and CityScapes. While the results on CityScapes show good synthetic-to-real transfer, a brief zero-shot evaluation on other standard depth benchmarks (e.g., KITTI, NYUv2) would more strongly demonstrate the generalizability and robustness of the learned representations, especially given the use of the local SSIGM loss.\n4. The field of efficient high-resolution vision is rapidly evolving. A discussion of how PRV2 compares to other contemporary lightweight or patch-based refinement approaches (beyond PRV1 and PatchFusion) would better situate its contributions within the current research landscape.\n5. Some arxiv papers are actually published on important conferences, and please cite the published information, not arxiv."}, "questions": {"value": "While the term noisy features is used to describe the output of the lightweight encoder, a more precise characterization would strengthen the argument. Is this noise in the traditional sense (random, high-frequency artifacts), or is it a lack of depth-specific semantic structure? A brief quantitative analysis (e.g., using feature similarity metrics) could complement the visual evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Iprnd78qLi", "forum": "AAqeYdGdn2", "replyto": "AAqeYdGdn2", "signatures": ["ICLR.cc/2026/Conference/Submission4490/Reviewer_JkXG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4490/Reviewer_JkXG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786173427, "cdate": 1761786173427, "tmdate": 1762917395055, "mdate": 1762917395055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a refined version of PatchRefiner, aiming to accelerate inference by replacing the original patch refiner with a lightweight network. The proposed approach incorporates a coarse-to-fine module, a guided denoising unit, and a noisy pre-training strategy. With these enhancements, PatchRefiner V2 achieves substantial speed improvements while delivering superior performance compared to its predecessor."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and clearly organized.\n- The proposed improvements to PatchRefiner demonstrate both significant acceleration in inference speed and enhanced performance."}, "weaknesses": {"value": "1. At Line 377, the authors state that the Cityscapes dataset is used for synthetic-to-real transfer evaluation. However, quantitative comparisons with other methods are missing in both the main text and the supplementary material, which limits the completeness of the evaluation.\n2. Only quantitative results on the in-domain UnrealStereo4K dataset are reported. Considering that the base model, ZoeDepth, is a generalizable depth estimator, it would be valuable to include experiments under cross-dataset settings to provide a more comprehensive assessment of the proposed method. \n3. At Line 365, the authors claim that local SSIGM performs better than matching gradients over the entire map. However, as shown in Table 3, the variant with zero windows does not exhibit significant performance degradation compared to local variants, and the influence of window size and number of windows on performance appears minimal. This observation reduces the perceived effectiveness of the proposed local SSIGM loss."}, "questions": {"value": "1. Regarding the noisy pre-training strategy, it would be helpful to clarify whether the type of noise used during training influences the final performance. For instance, how would the results change if Gaussian noise were replaced with uniform noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fn6vjrxTmu", "forum": "AAqeYdGdn2", "replyto": "AAqeYdGdn2", "signatures": ["ICLR.cc/2026/Conference/Submission4490/Reviewer_JEWx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4490/Reviewer_JEWx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897683886, "cdate": 1761897683886, "tmdate": 1762917394780, "mdate": 1762917394780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed PatchRefiner V2(PRV2) for real-domain high-resolution metric depth estimation. PRV2 replaces a heavyweight refiner  with lightweight encoder and adds a coarse-to-fine (C2F) block with a guided denoising unit. The paper also present noisy pretraining. Experiments on UnrealStreao4K  (synthetic) and Cityscapes (real) report the efficiency of PRV2."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Experimental results\nOn UnrealStereo4K, PRV2 delivers state-of-the-art accuracy while using fewer parameters and achieving faster inference than strong baselines.\n\n2. Simple design & recipe\nA modular architecture and a minimal noisy-pretraining scheme make the method easy to reproduce and extend into existing pipelines."}, "weaknesses": {"value": "1. Benchmark coverage is narrow (mainly UnrealStereo4K)\nThe main SOTA claims are substantiated primarily on one synthetic dataset, while the real-domain evidence is limited to Cityscapes.\n\n2. Incomplete comparators for 2024–2025 SOTA\nUnrealStereo4K comparisons largely focus on ZoeDepth/ZoeDepth+PF/ZoeDepth+PRV1. Please add or discuss comparisons (or a justified protocol mismatch) against strong recent depth estimation methods (e.g., Marigold, SharpDepth, ...) matched resolution/compute, and clarify where a fair comparison is infeasible.\n\n3. Ablations split across datasets create interpretation friction\nCore architectural ablations (C2F/NP) are on UnrealStereo4K, while loss/boundary analyses are on Cityscapes, which makes it hard to see how each module contributes on the same data. Please add a unified ablation table on one dataset (preferably a real set) so readers can read row-wise improvements coherently\n\n4. Contribution novelty leans engineering rather than conceptual\nGDU-style gating and the NP strategy are practical and effective, but feel incremental relative to prior guided fusion paradigms\n\n5. Timing definition under-reports full pipeline cost\nThe paper defines T as the refiner-branch time per image; please also report end-to-end wall-clock (coarse + all patch refinements) and memory for a fair, reproducible comparison for follow-up research"}, "questions": {"value": "No questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IldGx3D97S", "forum": "AAqeYdGdn2", "replyto": "AAqeYdGdn2", "signatures": ["ICLR.cc/2026/Conference/Submission4490/Reviewer_k7TN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4490/Reviewer_k7TN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236481437, "cdate": 1762236481437, "tmdate": 1762917394555, "mdate": 1762917394555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}