{"id": "1w1jCfYM8P", "number": 770, "cdate": 1756817622751, "mdate": 1759898242617, "content": {"title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance", "abstract": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present $\\textbf{ProMoE}$, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to $\\textit{first}$ partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and $\\textit{second}$ refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.", "tldr": "We present ProMoE, a Mixture-of-Experts framework featuring a two-step router with explicit routing guidance, promoting expert specialization in MoE-based DiT models.", "keywords": ["Image Generation", "Mixture-of-Experts", "Diffusion Transformer"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62a94399b83f8fb638de558e61272a7f4b5b0d1c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present ProMoE, a two-step routing framework for Diffusion-MoE models. In the first step, tokens are routed according to functional heterogeneity, sending unconditional tokens to dedicated unconditional experts. The remaining conditional tokens are then routed using a cosine similarity router to experts, with an additional routing contrastive loss designed to enhance intra-expert semantic similarity and inter-expert semantic differences."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Experimental results on ImageNet are strong, with uniform improvements in generation quality and diversity\n\n2. The authors address scalability comprehensively, with experiments covering four model sizes. \n\n3. The paper is clear and easy to follow"}, "weaknesses": {"value": "**Limited technical novelty**. The authors claim the prototypical routing mechanism is novel, but I struggle to see the novelty here. The learnable prototypes appear to just be standard learnable expert embeddings and semantic similarity is computed using a cosine similarity between inputs and expert embeddings, which is fairly conventional in MoE [1,2, 3]. The authors do use an identity activation function which is non-standard, but this alone does not really qualify the entire routing mechanism as novel in my view. \n\n**Missing experimental results**. The reported results in Fig 3 show that ProMoE indeed offers substantive improvements over dense and MoE baselines up to 500k training steps, but the performance differences do appear to be converging on one another very quickly, with the differences starting to look more marginal towards 500K and with increasing cfg to 1.5. Given that the samples generated in figure 4 required 2 million training steps and a cfg of 4.0, it raises the question of whether the performance gains shown in Fig 3 and Table 4 are meaningful, as it seems the model is unlikely to be near convergence at 500K samples. Indeed, the loss curves seem to suggest that even at 1.2M steps the model is far from convergence. Given the trend in performance visible in Fig 3, it looks possible that at 2M and cfg=4.0 the improvement of ProMoE may no longer substantive, but the authors haven't included these important results. \n\n**Single dataset for experimental validation**. The authors present their experimental results on just ImageNet-1K. Though the authors do a good job of validating at multiple model sizes, the empirical contribution would be much more persuasive if the findings could be validated across multiple datasets. \n\n\n[1] On the representation collapse of spare moe [Chi et al, NeurIPS 2022]\n[2] Statistical advantages of perturbing cosine router in moe [Nguyen et al, ICLR 2025]\n[3] Sparse moe are domain generalizable learners [Li et al, ICLR 2023]"}, "questions": {"value": "I'd strongly recommend the authors to include analysis of the kind seen in Table 4 but at training step=2M and cfg=4.0 across the MoE baselines and the dense baseline. This would provide a comprehensive analysis of the empirical benefits of ProMoE at convergence. Just choosing one size, ideally the largest, would be sufficient. If the authors can demonstrate the strong empirical results hold up at higher training step and cfg settings, I would consider raising my score, but for now it seems possible that the reported gains are too far from convergence to be meaningful. \n\nIf the authors could include an additional dataset that would also help boost the empirical contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NRWso52MR7", "forum": "1w1jCfYM8P", "replyto": "1w1jCfYM8P", "signatures": ["ICLR.cc/2026/Conference/Submission770/Reviewer_vTUm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission770/Reviewer_vTUm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378718510, "cdate": 1761378718510, "tmdate": 1762915600942, "mdate": 1762915600942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ProMoE, a framework that successfully applies Mixture-of-Experts (MoE) to Diffusion Transformers (DiTs), addressing why previous attempts failed. The authors argue that visual tokens, unlike language tokens, have high redundancy and functional differences, which hinders expert specialization. ProMoE solves this with a novel two-step router that first separates tokens by function and then assigns them to experts based on semantic content using learnable prototypes. This guided approach, enhanced by a new contrastive loss, enables strong expert specialization and achieves state-of-the-art results on ImageNet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper clearly diagnoses the problem of vision MoE and proposes an innovative ProMoE to solve it.\n\n- The ProMoE achieves validated, state-of-the-art results on the ImageNet benchmark.\n\n- The presentation is clear and easy to understand."}, "weaknesses": {"value": "- What is the fundamental difference between prototypical routing and conventional MoE routing mechanisms, such as one using a standard linear layer? The paper introduces \"learnable prototypes\", but this seems functionally very similar to using the learnable weights of a linear layer to calculate token-expert affinities. Could you clarify what makes this prototypical approach a genuine innovation, rather than just a conceptual re-framing of a standard linear gating mechanism?\n\n- The routing mechanism in ProMoE appears to rely on pre-defined structures tailored for specific categories, unlike the autonomous expert specialization seen in LLMs. This raises questions about its generalizabilityâ€”how would ProMoE handle open-ended conditional inputs, such as a natural-language prompt, rather than predefined categories? This design appears less flexible and general.\n\n- How is the number of experts determined, and what is the rationale for that specific choice? Are there more detailed ablation studies on the impact of varying the number of experts?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UlA1kykaPg", "forum": "1w1jCfYM8P", "replyto": "1w1jCfYM8P", "signatures": ["ICLR.cc/2026/Conference/Submission770/Reviewer_NvNP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission770/Reviewer_NvNP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806287021, "cdate": 1761806287021, "tmdate": 1762915600631, "mdate": 1762915600631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ProMoE, a novel MoE framework for Diffusion Transformers that addresses the failure of prior MoE designs in vision via a two-step router with explicit routing guidance. It introduces conditional routing to separate functional roles and prototypical routing with learnable prototypes, enhanced by a routing contrastive loss."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper effectively addresses the core challenges of visual token redundancy and functional heterogeneity in Diffusion Transformers, introducing mechanisms that enable true expert specialization within the Mixture-of-Experts framework.\n2. The proposed method demonstrates strong and consistent scaling behavior across multiple model sizes, validating its robustness and efficiency under both Rectified Flow and DDPM training paradigms."}, "weaknesses": {"value": "1. The experiments are conducted solely on ImageNet-1K for class-conditional generation, without evaluations on other datasets or modalities, which limits the evidence of generalization.\n2. The paper does not report quantitative expert utilization, such as the proportion of tokens or capacity per expert, making it hard to assess balance and specialization."}, "questions": {"value": "1.Could the authors provide quantitative statistics of expert utilization (e.g., token-per-expert ratios or activation entropy) to substantiate the claimed specialization and balance?\n2.Could the authors compare ProMoE with other unsupervised clustering methods that support top-K routing, such as GMM or deep clustering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TApOnZ1qIg", "forum": "1w1jCfYM8P", "replyto": "1w1jCfYM8P", "signatures": ["ICLR.cc/2026/Conference/Submission770/Reviewer_1uA8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission770/Reviewer_1uA8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905752048, "cdate": 1761905752048, "tmdate": 1762915600296, "mdate": 1762915600296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a new expert routing method for Diffusion Transformers, by treating conditional and unconditional tokens independently. Routing guidance and contrastive learning are further introduced to enhance the performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The rationale for separating conditional and unconditional tokens is clear and well-founded.  \n2. The investigation into routing guidance and load balancing is insightful and valuable."}, "weaknesses": {"value": "1. It would be beneficial to include ablation studies on dense models with conditional routing to determine whether the performance gain stems solely from conditional routing itself or requires combination with routing enhancements.\n\n2. Since one key advantage of MoE models is improved computational efficiency, the authors are encouraged to report training and inference times, as well as FLOPs, in comparison to both dense models and other MoE variants."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Oyce5uxnhm", "forum": "1w1jCfYM8P", "replyto": "1w1jCfYM8P", "signatures": ["ICLR.cc/2026/Conference/Submission770/Reviewer_4DXQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission770/Reviewer_4DXQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969264077, "cdate": 1761969264077, "tmdate": 1762915600110, "mdate": 1762915600110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}