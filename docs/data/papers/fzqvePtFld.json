{"id": "fzqvePtFld", "number": 20810, "cdate": 1758310482813, "mdate": 1759896957433, "content": {"title": "CRISP: Clustering Multi-Vector Representations for Denoising and Pruning", "abstract": "Multi-vector models, such as ColBERT, are a significant advancement in neural information retrieval (IR), delivering state-of-the-art performance by representing queries and documents by multiple contextualized token-level embeddings. \nHowever, this increased representation size introduces considerable storage and computational overheads which have hindered widespread adoption in practice. A common approach to mitigate this overhead is to cluster the model's frozen vectors, but this strategy's effectiveness is fundamentally limited by the intrinsic clusterability of these embeddings.\n In this work, we introduce \\crisp\\ (Clustered Representations with Intrinsic Structure Pruning), a novel multi-vector training method which learns inherently clusterable representations directly within the end-to-end training process. By integrating clustering into the training phase rather than imposing it post-hoc, \\crisp\\ significantly outperforms post-hoc clustering at all representation sizes, as well as other token pruning methods.\n On the BEIR retrieval benchmarks, \\crisp\\ achieves a significant rate of \\textbf{~3x} reduction in the number of vectors \\textit{while outperforming} the original unpruned model. This indicates that learned clustering effectively denoises the model by filtering irrelevant information, thereby generating more robust multi-vector representations. With more aggressive clustering, \\crisp\\ achieves an \\textbf{11x} reduction in the number of vectors with only a $3.6\\%$ quality loss.", "tldr": "Multi-vector representation pruning for late-interaction retrieval models via a new clustering-based training method that produces fewer vectors with better quality.", "keywords": ["Multi-Vector Embedding", "Late Interaction", "Representation Pruning", "Information Retrieval", "Clustering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2f847e9533c0c10df1de8297845c981d9a46147.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In information retrieval (IR) tasks, the efficiency of multi-vector retrieval is heavily influenced by the number of vectors used. This paper introduces CRISP (Clustered Representations with Intrinsic Structure Pruning), which aims to improve the clustering efficiency of multi-vector retrieval models. CRISP learns clusterable representations end-to-end during training, unlike post-hoc clustering or token pruning methods that operate on frozen embeddings. Experiments were conducted on the BEIR benchmark using the Gemma2B model."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents the idea of integrating clustering into the training process, enabling the model to learn representations that are inherently more suitable for clustering.\n\n- Overall, the paper is clearly written and easy to follow."}, "weaknesses": {"value": "- Insufficient baseline comparison: The authors only compare their method with other clustering-based approaches and very simple fixed-token pruning methods. However, recent research has actively explored dynamic token and representation compression techniques that enhance efficiency (e.g., [1]). Such methods may in fact address the three key challenges listed in lines 125–139 more directly, by compressing redundancy while minimising information loss, thereby improving computational efficiency. If the paper compares CRISP against token pruning methods as baselines for efficient IR, it should also compare with dynamic token compression approaches.\n\n- Limited model diversity: Aside from the external baselines, both the proposed and baseline models appear to rely solely on Gemma2B. Since CRISP is designed as an end-to-end training method, demonstrating that it generalises across different model architectures would strengthen its contribution, particularly given that the authors note the method is highly sensitive to hyperparameter tuning (lines 425–426).\n\n- Minor editing issues: When a citation functions as part of a sentence, the paper should use \\citep{} (see ICLR 2026 formatting instructions, Section 4.1). The description of Tables 1 and 2 (lines 286–287) is duplicated later in lines 338–339.\n\n\n[1] Nawrot, Piotr, et al. “Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.” International Conference on Machine Learning. PMLR, 2024."}, "questions": {"value": "- As mentioned in the weaknesses, could the authors compare CRISP with dynamic token compression methods?\n\n- Could the authors apply the proposed training approach to models other than Gemma2B to demonstrate broader generalisability?\n\n- In lines 431–432, the comparison to post-hoc clustering is discussed but does not appear in Table 3. Was there a reason why those methods could not be evaluated under the same experimental setting in Table 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cyf0dczW5Y", "forum": "fzqvePtFld", "replyto": "fzqvePtFld", "signatures": ["ICLR.cc/2026/Conference/Submission20810/Reviewer_Thz6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20810/Reviewer_Thz6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003679507, "cdate": 1762003679507, "tmdate": 1762999986723, "mdate": 1762999986723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for training multi-vector retrieval methods to produce naturally \"clusterable\" vector representations, to enable reducing the number of vector embeddings per document (or query) to be as little as 3-11x fewer than the standard approach, while seeking to preserve (or even slightly improve) quality.\n\nTo do so, the authors apply k-means clustering per sequence (query or document) during encoding at training time. Each sequence is thus represented as a set of the average of all vectors per cluster, instead of one vector per token. Contrastive-esque training proceeds as usual otherwise, with the Chamfer/MaxSim similarity.\n\nThis method (CRISP) is applied to fine-tuning Gemma2-based multi-vector retrievers. The CRISP model is competitive with the multi-vector baseline while compressing document representations by 3x; more aggressive compression (11x) is possible at only a 3-4% drop in quality. As the authors argue, this is indeed a substantial improvement over pre-existing \"post-hoc\" clustering methods, which degrade more quickly even with less aggressive compression."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method proposed is very simple (and hence likely to be picked up and generate impact) and quite effective, as shown by the results on BEIR. The authors consider a number of reasonable baselines and train all models in a fair manner. This could enable a fair and scientific comparison, though it has the disadvantage of being somewhat disconnected from the broader literature on training techniques for many of these models."}, "weaknesses": {"value": "The paper offers limited insight about the cost or complexity of running k-means clustering during training. How expensive or complex is this? Were any special tricks necessary for dealing with the gradient propagation? Why wasn't it done before, at least so effectively? Are there any theoretical or conceptual concerns that should be considered in clustering _per document_ versus clustering across the corpus?\n\nBEIR is a fairly old and \"easy\" / rather statured benchmark at this point. What about the various more modern IR benchmarks, e.g. like BRIGHT?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6lkaiv2OaU", "forum": "fzqvePtFld", "replyto": "fzqvePtFld", "signatures": ["ICLR.cc/2026/Conference/Submission20810/Reviewer_m7WV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20810/Reviewer_m7WV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762262903079, "cdate": 1762262903079, "tmdate": 1762935725421, "mdate": 1762935725421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CRISP, an approach that integrates clustering into the training process of multi-vector retrieval models to learn inherently clusterable token representations. Instead of applying post-hoc clustering or token pruning, CRISP encourages structured representation learning via a clustering loss (Chamfer distance) and a pruning mechanism that discards redundant tokens. Experiments on the BEIR benchmark aim to show that CRISP achieves competitive retrieval performance with significantly fewer tokens, suggesting that learned clusterable representations can improve both efficiency and robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and practical problem in neural information retrieval: the efficiency and redundancy of token-level multi-vector representations.\n\n2. The experimental section is  covering multiple BEIR tasks and including comparisons with  multi-vector and single-vector models, including the pruned variants."}, "weaknesses": {"value": "1. The main concern with this paper lies in its lack of substantial methodological innovation. While CRISP presents a training paradigm that integrates clustering directly into the end-to-end learning process of multi-vector retrieval models, this idea is not fundamentally new. Prior works, such as “Deep Clustering for Unsupervised Learning of Visual Features” and subsequent extensions, have already explored integrating k-means-style clustering objectives into representation learning frameworks. In the context of information retrieval, several studies have similarly incorporated clustering or token grouping mechanisms during training to achieve efficiency gains.\nTherefore, the paper’s main contribution appears incremental, and it remains unclear what is technically novel beyond applying a well-known clustering regularization idea to multi-vector retrieval. The authors are encouraged to clearly articulate the unique algorithmic contributions or theoretical insights that distinguish CRISP from existing clustering-based compression or pruning methods.\n\n2. The paper provides limited understanding of why clustering improves performance or efficiency. Although empirical results suggest that CRISP can act as a denoising mechanism, the analysis of this phenomenon is superficial. There is no visualization or quantitative exploration of the learned cluster structures, token distribution changes, or training dynamics that might explain how clusterable representations enhance retrieval robustness. \n\n3. The experimental evaluation compares CRISP primarily against simple fixed-selection pruning methods, which are relatively outdated and limited in scope. This makes it difficult to position CRISP fairly within the growing body of work on efficient multi-vector and token pruning techniques. Many recent approaches have investigated learnable pruning, dynamic token reduction, and attention-based selection mechanisms that offer stronger baselines for comparison. Evaluating CRISP against these state-of-the-art pruning and compression methods would provide a more convincing demonstration of its advantages and clarify its practical relevance. \n\n4. The paper does not fully adhere to the ICLR formatting and style guidelines, which affects its overall readability and professionalism."}, "questions": {"value": "Please see above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eUnrjl1IJn", "forum": "fzqvePtFld", "replyto": "fzqvePtFld", "signatures": ["ICLR.cc/2026/Conference/Submission20810/Reviewer_6Wpt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20810/Reviewer_6Wpt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327025707, "cdate": 1762327025707, "tmdate": 1762935679166, "mdate": 1762935679166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}