{"id": "7Mbz5uSf2J", "number": 13862, "cdate": 1758223942595, "mdate": 1763598518734, "content": {"title": "Decoupling Dynamical Richness from Representation Learning: Towards Practical Measurement", "abstract": "Dynamic feature transformation (the rich regime) does not always align with predictive performance (better representation), yet accuracy is often used as a proxy for richness, limiting analysis of their relationship. We propose a computationally efficient, performance-independent metric of richness grounded in the low-rank bias of rich dynamics, which recovers neural collapse as a special case. The metric is empirically more stable than existing alternatives and captures known lazy-to-rich transitions (e.g., grokking) without relying on accuracy. We further use it to examine how training factors (e.g., learning rate) relate to richness, confirming recognized assumptions and highlighting new observations (e.g., batch normalization promote rich dynamics). An eigendecomposition-based visualization is also introduced to support interpretability, together providing a diagnostic tool for studying the relationship between training factors, dynamics, and representations.", "tldr": "Practical method to quantify dynamical richness independent of performance.", "keywords": ["training dynamics", "representation learning", "lazy/rich regime", "neural collapse", "grokking", "kernel methods"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4913fe4f1343a4e8458cbefe56f03c59d7b48e2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new metric for quantifying rich (feature) learning. The authors motivate this by distinguishing between two perspectives on feature learning: a representation perspective (focus on usefulness of features for performance/generalization) and a dynamics perspective (focus on the transformation of features). They show that although it is often associated with representational usefulness, rich learning doesn’t necessarily lead to generalization and instead reflects an inductive bias towards certain solutions. Thus, the paper focuses on dynamic feature learning, introducing a computationally-efficient and performance-independent metric that effectively compares the network activations before and after the last layer. They show that this metric generalizes neural collapse, a phenomenon associated with rich learning. By comparing to prior measures of rich dynamics, they illustrate special cases where their metric captures dynamical richness, while others do not. They also conduct experiments across several different models and datasets, demonstrating how different training conditions affect performance and richness separately. Finally, the authors provide complementary visualization methods, make it easier to see the low-rank feature bias of rich dynamics."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The presentation and clarity of the paper is excellent. It is well-written, clear, and accessible. The paper is well-motivated, with a comprehensive introduction and background. \n\nThe soundness of the paper is also excellent. The authors take care to support each of their claims with ample evidence and solid experiments/methodology. The paper identifies an important distinction in perspectives on feature learning (dynamics- versus representation-focused) that challenges implicit assumptions about rich learning necessarily leading to improved performance/generalization and provides experiments to support this. The metric introduced has good potential for studying feature learning, as it is efficient to compute and captures rich dynamics in settings where prior metrics fail. The authors provide well-thought experiments to demonstrate the utility of their metric and support their central claims, showing that it captures grokking, neural collapse, and other known feature learning phenomenon. They also provide helpful visualization methods, which they show the utility of in interpreting different network behavior. The paper offers a good contribution to the ICLR community and I recommend it for acceptance. It was a pleasure to read and I thank the authors for their solid work."}, "weaknesses": {"value": "To clarify, I think these points are relatively specific (possibly only applying to certain edge cases) and that the work as a whole still stands well despite some potential limitations.\n\nOne potential weakness of the metric introduced is that it’s limited in its generality (to orthogonal and isotropic target functions). However, this is properly acknowledged by the authors and the metric still captures many classification tasks. \n\nAs the authors also state, their metric depends on a comparison between the last two layers. Thus, two networks with different feature learning behavior in earlier layers may appear identical if their final two layers are similar. I’m wondering if there would be any principled way to applying the same approach to other layers in the network? And are there concrete settings where the comparison between the last two layers is a major limiting assumption?\n\nAs far as I understand, the metric is based on the low-rank bias of rich dynamics. I know that this might be rare in practice, but would the metric still work if the target function is full-rank? Is the assumption of low-rank dynamics potentially limiting?\n\nThe authors state that in Figure 5 “feature quality correlates with feature intensity during training, with larger features improving faster… the correlation between quality and intensity during training has not been previously observed or studied.” Isn’t this expected, as it’s known that large features (singular values) are learned first (Saxe et al., 2014)? Or, do you mean that eigenvector alignment is occurring simultaneously with singular vector scaling? In Atanasov et al., 2022, they show that for anisotropic data, “the NTK must necessarily change its eigenvectors when the loss is significantly decreasing, destroying the silent alignment phenomenon.” I’m a bit skeptical of the statement that this correlation has not been observed or studied."}, "questions": {"value": "1. Would the metric still work if the target function is full-rank?\n2. Would be any principled way to applying the same approach to other layers in the network? And are there concrete settings where the comparison between the last two layers is a limiting assumption?\n\nSuggestion: there are a few typos but nothing major"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pIj1LgdzSc", "forum": "7Mbz5uSf2J", "replyto": "7Mbz5uSf2J", "signatures": ["ICLR.cc/2026/Conference/Submission13862/Reviewer_cwga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13862/Reviewer_cwga"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575188488, "cdate": 1761575188488, "tmdate": 1762924380804, "mdate": 1762924380804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Comments (first half)"}, "comment": {"value": "We thank the reviewers for taking the time to review our paper and for highlighting three aspects of our work: the assumption of isotropic targets, the role of intermediate layers, and accessibility to a broader audience. In response, we have **(i)** added a paragraph in **Appendix D** explaining the standards in neural collapse (NC) literature and describing the challenges posed by anisotropic targets, **(ii)** included a new **Appendix I** (Intermediate Layer Features) reporting additional experiments and analysis on intermediate layers, and **(iii)** added a new **Appendix J** (Accuracy of Approximation Methods) that introduces the Nyström method and other approximations, along with additional references to the relevant appendices. We address each of these points in detail below.   \n\nWe also note that the first two aspects, as noted by the reviewers, are explicitly listed as limitations in the main text, and that the original submission already included an extensive pedagogical appendix with a glossary, technical background, and a gentle introduction to kernel methods and neural collapse.\n\nFinally, we note that all modifications and additions in the revised version are highlighted in red.\n\n# 1. Non-isotropic target\nAs stated at the beginning of Section 2 and in the limitation paragraph, our paper’s scope is on isotropic outputs. We emphasize that while this is a limitation, it aligns with the prevailing standards in the NC literature [1-4], and is a much milder assumption compared to the standards in the literature on lazy/rich dynamics [5-7]. Additionally, handling anisotropy is an open challenge in the NC literature. We added this discussion in a new Appendix D.4. \n\n**In Neural Collapse**: Neural collapse requires a slightly stronger assumption — balanced classification and the terminal phase of training. For example, neural collapse cannot handle scalar-output regression, which our formalism can handle (see Appendix D.3). Because simplex ETF structures rely significantly on the symmetry, proposing a plausible extension for imbalanced tasks is an active area of research on its own [8-12]. Additionally, it suffers from the minority collapse where multiple classes collapse to a single vector for a finite dataset [8]. This becomes particularly a problem for a long-tailed target (e.g., natural language dataset), which is also an active area of research [12]. \n\n**In lazy/rich dynamics**: Theories from lazy/rich dynamics prove less useful as they often require even less practical constraints (e.g. isotropic input, infinitesimal initialization, 2-layer linear networks). Removing the constraints (e.g., isotropic input) remains challenging even for two-layer linear networks [13], often requiring conjectures about the underlying feature learning mechanism. \n\n\n# 2. Analysis of intermediate layers\nIn the revised version, we added Appendix I to examine intermediate layers. Extending our method to intermediate layers is straightforward: simply replace the last-layer features $\\phi(x)$ with those from an intermediate layer when defining $\\mathcal{T}$ (Eq. 58). However, such an extension presents three major caveats: limited theoretical grounding, higher computational cost, and reduced empirical utility for our goal.\n\n**Theoretical limitation**: Our method relies on linear correlations (or kernel methods). For the last layer, features are linearly transformed to the output, validating this approach; this justification weakens once nonlinear transformations intervene, as in intermediate layers.\n\n**Computational cost**: Our metric scales quadratically with the width. The last layer is typically narrow ($\\sim 10^3$), whereas intermediate layers—especially in CNNs—are much wider (Fig. 22), making the computation substantially more expensive.\n\n**Empirical limitation**: Although we observe phenomena such as a depth-wise transition from many less-relevant features to fewer more-relevant ones (Fig. 18) and minimal learning in earlier layers under the lazy regime (Fig. 20), these observations do not provide additional discriminatory power for lazy vs. rich dynamics beyond what the last layer already offers. While potentially useful for future studies of rich-regime representations, intermediate-layer analysis is less suitable for efficiently detecting lazy/rich dynamics — **our primary goal**.\n\nIn Fig. 21, we experimented with greedy layerwise training [14], in which each layer is trained with an auxiliary classifier, frozen, and followed by training the next layer. Under this procedure, each trained layer displays dynamics similar to the penultimate layer, and feature collapse emerges earlier than under full backpropagation, accompanied by reduced overall performance. Although limited, this experiment suggests that the last two layers exhibit the strongest low-rank bias, further supporting our focus on the last layer."}}, "id": "lNIahnCgo7", "forum": "7Mbz5uSf2J", "replyto": "7Mbz5uSf2J", "signatures": ["ICLR.cc/2026/Conference/Submission13862/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13862/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13862/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763554107728, "cdate": 1763554107728, "tmdate": 1763557020241, "mdate": 1763557020241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a performance-independent metric for dynamical richness (DLR) that compares the feature kernel built from penultimate-layer activations with a “minimum projection” operator determined by the network’s learned function. Intuitively, truly rich dynamics should learn only the minimal features needed to span the learned function space; DLR quantifies the gap to this ideal and is normalized in [0,1] (lower is richer). The authors show that when the feature operator attains this minimum form, neural collapse conditions follow as a special case, linking the metric to established phenomena. Empirically, DLR is lightweight to compute and tracks lazy-to-rich transitions across MLPs/CNNs/Transformers, and an eigendecomposition-based visualization complements the scalar score."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality. Recasts richness as low-rank alignment between features and the learned function via a principled MP-operator; this is a fresh angle relative to NTK-deviation or label-based collapse metrics. The reduction to neural collapse provides a clean conceptual bridge.\n2. Quality. The metric is simple, normalized, and computationally cheap, enabling use on standard vision models. Comparative experiments are thoughtful and reveal expected patterns.\n3. Clarity. The paper is well structured and the three-panel eigendecomposition plots (quality/utilization/eigenvalues) are helpful.\n4. Significance. A lightweight, performance-independent diagnostic for dynamics could become a standard tool, much as CKA did for representational comparisons."}, "weaknesses": {"value": "1. DLR inspects only the final-layer features; rich dynamics might manifest earlier and be attenuated by a constrained head. Consider a hierarchical variant (layer-wise DLR or block-DLR) and show whether conclusions persist will be helpful.\n2. The technical background is not much sufficient for readers to capture the essence. More straightforward interpretation and introduction are helpful.\n3. The MP-operator and some guarantees rely on orthogonal/isotropic targets and supervised, one-hot settings; this narrows immediate applicability (e.g., class imbalance, multilabel, regression, self-supervised).\n4. Broader settings (imbalance/SSL), deeper ablations (layer-wise, NTK-feasible baselines), and stronger analysis of assumptions would raise confidence and impact.\n5. Writing/format polish. Minor issues distract: e.g., “batch nomralization” typo in Table 3; occasional spacing/notation inconsistencies around figures/equations; some insufficient definitions/clarifications on heavy-weight mathematical notations.."}, "questions": {"value": "1. Can DLR be generalized to non-isotropic/imbalanced tasks (e.g., via class-reweighted inner products or whitening), and does the neural-collapse link still hold?\n2. What happens if you compute DLR per layer (or per block) and aggregate? This could localize where richness emerges and inform architecture design.\n3. Can you theoretically connect decreasing DLR with transition conditions in lazy-to-rich analyses/grokking, beyond empirical alignment?\n4. Why CKA over alternatives (e.g., centered HSIC variants)? Any cases where CKA’s centering removes signal that matters for DLR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concern."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TMhYO85reC", "forum": "7Mbz5uSf2J", "replyto": "7Mbz5uSf2J", "signatures": ["ICLR.cc/2026/Conference/Submission13862/Reviewer_Czxk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13862/Reviewer_Czxk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718690436, "cdate": 1761718690436, "tmdate": 1762924380368, "mdate": 1762924380368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DLR (Dynamical Low-Rank measure), a computationally efficient and performance-independent metric for quantifying dynamical richness in neural networks. The metric is grounded in the low-rank bias characteristic of rich dynamics and compares activations before and after the last layer via a functional kernel operator. The authors show that DLR reduces to neural collapse as a special case and empirically validate that it captures known lazy-to-rich transitions (e.g., grokking) without relying on accuracy. They further introduce an eigendecomposition-based visualization tool to enhance interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Important problem: Decoupling dynamical richness from representation quality addresses a fundamental issue in understanding neural network training. The observation that rich dynamics ≠ better generalization (Figure 1) is compelling.\n- Computationally efficient: The O(p²C) complexity is a significant improvement over NTK-based methods, making the metric practical for modern architectures.\n- Strong theoretical grounding: The connection to neural collapse (Propositions 1 and 2) provides solid theoretical foundation, while extending applicability beyond classification (e.g., regression with scalar output).\n- Performance-independent: Unlike existing metrics (Sinit, parameter norm, NC1), DLR doesn't rely on accuracy, initial kernel, or class labels, making it more robust (Tables 1 and 2)."}, "weaknesses": {"value": "- Limited scope: The current formulation only applies to orthogonal and isotropic target functions. While this covers many classification tasks, the restriction is significant. The authors acknowledge this but don't provide a clear path to generalization.\n- Empirical validation: While the authors demonstrate DLR's utility across diverse settings (grokking, learning rate variations, weight decay, batch norm) the experiments conducted are of small scale and somewhat artificial. It would be interesting to see this applied to more modern scenarios, i.e. bigger models and more difficult datasets.\n- Last-layer focus: By only examining last-layer features, the metric misses dynamics in earlier layers. While this is a trade-off for efficiency, it's unclear how much information is lost.\n- Limited theoretical analysis: Beyond the connection to neural collapse, there's limited theoretical characterization of when DLR accurately reflects \"richness\" or what DLR values imply about learning.\n- Comparisons could be broader: NTK-based measures are set aside due to computational cost; although justified, a scaled-down NTK comparison on smaller models (which the authors already use) would strengthen claims that DLR is a better proxy rather than merely cheaper.\n- Clarity: While the method is well motivated theoretically and relative to prior work, the paper is occasionally difficult to follow. The bra-ket notation and operator formalism, while mathematically precise, may limit accessibility for a broader audience. The empirical approximations (Appendix E) are much clearer and could be introduced earlier to improve intuition. At times, the paper presents symbols and equations with insufficient context or motivation. Furthermore, the term dynamical richness is used somewhat loosely throughout; a dedicated subsection defining and contrasting rich vs lazy regimes would help.\n- Unexplored temporal dynamics: The paper treats DLR at convergence or snapshot points, but since it is dynamical, a time-series analysis (e.g., how DLR evolves over training) would provide stronger evidence of what “rich” dynamics actually look like."}, "questions": {"value": "- Typos: 049 \"a dynamical richness (metric?) that\", 373 \"Our visualizes (visualizations)\"\n- How does the metric behave with different loss functions beyond MSE? The cross-entropy results are mentioned but not thoroughly analyzed.\n- Metric sensitivity/ablation: More detail on estimation stability (sample size n for Nyström, dependence on width p, class count C) and hyperparameter sensitivity would help practitioners choose sampling parameters confidently.\n- The batch normalization finding (last row of Table 3) is correlational. The paper doesn't establish that batch norm causes rich dynamics or explain the mechanism.\n- The paper doesn't provide clear guidance on what intermediate DLR values mean.\n- Interpretability link not quantified: The eigendecomposition visualization is qualitative; quantitative metrics (e.g., subspace alignment or effective rank) could have strengthened claims of interpretability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Gbi009MEJ", "forum": "7Mbz5uSf2J", "replyto": "7Mbz5uSf2J", "signatures": ["ICLR.cc/2026/Conference/Submission13862/Reviewer_ztAm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13862/Reviewer_ztAm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784387877, "cdate": 1761784387877, "tmdate": 1762924379589, "mdate": 1762924379589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors develop the ***low-rank measure*** \\$\\mathcal{D}_{LR}\\$ to measure the richness of the representations learned by a neural network, independently of its performance. Using it, and comparing it with the metrics from the existing literature, they can efficiently measure whether a model is in the *lazy* or *rich* regime.\n\nThe authors first introduce \\$\\mathcal{D}_{LR}\\$ theoretically, basing it on the concept of low-rank bias where in rich dynamics, the rank of the features learned by the model before the final linear layer should be similar to the dimensionality of the output, indicating well-separated classes. The metric itself is simple, as it is based on the CKA between the representations after the penultimate and the ultimate layer. It is therefore also inexpensive to compute.\n\nThey then compare their metric with other ones from the rich dynamics literature. Across two different test cases, they demonstrate it better illustrates rich-vs-lazy dynamics than other metrics. They further test their metric on realistic use-cases, showing with different examples how it represents richness, and where it is related (or not) to model performance.\n\nThe last part of the paper focuses on visualisations to explain shifts in the learning dynamics of neural networks. The authors separate three different metrics through eigendecomposition of kernel: cumulative quality, cumulative utilisation and relative eigenvalue. They demonstrate how they relate to rich dynamics."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "I think that this paper is a strong paper, which proposes a new metric for dynamic richness and demonstrates how it works better than other examples from recent literature.\n\nS1: The *low-rank metric* is simple to understand and inexpensive to compute. The authors give good intuition on how it works, why it works, and bounds on computation complexity.\n\nS2: Theoretical foundations for the *low-rank metric* look solid. Although the authors make some assumptions to make calculations tractable, formulations are very general and encompass a large portion of neural networks.\n\nS3: Empirical evidence covers the validity of the metric, comparison to existing other metrics from the literature, and practical use cases. The authors further provide statistical significance results. I think it strongly supports the main claims of the paper. Furthermore, the authors propose visualisation methods for increased interpretability of their results.\n\nS4: The literature review seems extensive, and most claims are backed through proofs or citations. This work is well contextualised within the existing literature.\n\nS5: I strongly commend the authors’ work on making their paper clear and easy to read. Writing and presentation are of high quality. It allows readers to more easily understand the theoretical framework around the development of the *low-rank metric*."}, "weaknesses": {"value": "I have not found major weaknesses in this paper. The following points are either minor or nitpicks.\n\nW1: I feel like the authors should find a clearer name and/or an acronym to designate the *low-rank metric*, which is often designated as “the metric” or as its mathematical notation \\$\\mathcal{D}_{LR}\\$ throughout the paper. Such a name/acronym would make it easier to designate and reference in text, discussions and future work that will rely on it."}, "questions": {"value": "Q1: It is not immediately clear to me how exactly the metrics introduced in Section 5 relate to the *low-rank metric*. They are derived from \\$\\mathcal{T}\\$’s eigendecomposition and the learned/target functions, but I have difficulties directly linking them to \\$\\mathcal{D}_{LR}\\$.\n\nQ2: The authors mention two main limitations: focus on last-layer dynamics, and validity constrained to orthogonal and isotropic target functions. Regarding the second point: the authors mentions that “while this covers most classification tasks, a more general setup would be preferable”. Could the authors please explain in what cases their method would not work, and/or would not be theoretically justified, and give some examples? Also, are there practical scenarios where this limitation could lead to misleading conclusions if the metric is applied without caution? If relevant, including such details in the paper could help practitioners understand when to avoid or adapt the use of this metric in their work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BT0id3HGQP", "forum": "7Mbz5uSf2J", "replyto": "7Mbz5uSf2J", "signatures": ["ICLR.cc/2026/Conference/Submission13862/Reviewer_GFrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13862/Reviewer_GFrJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807928547, "cdate": 1761807928547, "tmdate": 1762924378946, "mdate": 1762924378946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}