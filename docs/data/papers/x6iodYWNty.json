{"id": "x6iodYWNty", "number": 2301, "cdate": 1757054488137, "mdate": 1763560483100, "content": {"title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning", "abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.", "tldr": "", "keywords": ["Homotopy System", "Graduated optimization", "Reinforcement Learning", "Polynomial Equitions System", "Gaussian Homotopy", "Sampling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3396a75b75e7618ba71f9276991c208575dc5227.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "A number of problems in different domains can be framed as homotopy\ninterpolations between an easily solved source problem and a complex target\nproblem. Such problems are typically solved with a predictor-corrector\nframework. Here, a solution is found to the initial easy problem. Then, the\npredictor advances the problem following the homotopy interpolation, and the\ncorrector modifies the solution to fit the new problem.\n\nPrior work has treated these homotopy problems separately in different domains.\nThis paper proposes to unite a number of these problems under a single\nframework. It then proposes a neural predictor-corrector framework, which uses\nreinforcement learning to train a policy that outputs the next level for the\npredictor and the next tolerance for the corrector --- previously, these values\nwere determined via heuristics. Experiments show that the final RL agent\neffectively learns to set these values."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This application of reinforcement learning seems quite novel to me. I\n   appreciate that PPO is able to be used \"out of the box\" to solve the problem.\n2. The problems chosen for the paper are quite diverse, which shows the\n   applicability of the framework. For example, I could see the application to\n   sampling / Langevin dynamics could eventually tie into generative modeling.\n3. The computational requirements for this method are quite low (lines 314-315),\n   making the research more accessible.\n4. The limitation of sensitivity to reward scale is acknowledged in the\n   appendix.\n5. The ablation of the state components helps in showing that all parts of the\n   state are necessary for NPC.\n\nOverall, I found the paper quite easy to read despite not having much\nfamiliarity with homotopy interpolations myself."}, "weaknesses": {"value": "My main concern is that it seems the experiments were only conducted over one\ntrial, as I could not find any mention of repetitions, and no error bars are\nreported in the tables. It is advisable to conduct multiple trials of each\nalgorithm and use statistical testing to check that differences between\nalgorithms are significant.\n\nI have also listed several questions in the section below; these are minor points that I think would be useful to address in the updated paper."}, "questions": {"value": "1. I am unclear if the Homotopy Paradigm discussed in Sec. 3.1 is a novel\n   contribution of this work. Is the term \"Homotopy Paradigm\" already widely\n   used in the literature? If so, could a citation for it be added in Sec. 3.1?\n   I think I am confused because the first sentence of the introduction\n   (line 32) seems to indicate that the Homotopy paradigm is already well-known,\n   while lines 121-122 indicate that it is a perspective introduced in this\n   work. Assuming this perspective is novel, it may be good to list it as a\n   contribution in the introduction. Right now, the first contribution of the\n   introduction (line 82) makes it clear that unifying the approaches under the\n   homotopy paradigm is novel, but it is unclear if the homotopy paradigm itself\n   is novel.\n2. Based on Appendix D, it seems the NPC framework shifts the manual effort from\n   tuning the parameters for the predictor and corrector to tuning the reward\n   scales for the RL training. What do you see as the benefits and tradeoffs of\n   this shift? Might there be cases where it is easier to just use the\n   heuristics? When is it better to try to tune the rewards?\n3. Table 6 shows that removing certain components from the state can increase\n   the number of iterations required more than removing other components, e.g.,\n   removing the corrector's tolerance increases by 64, while removing the\n   homotopy level only increases by 21. Does this say anything about which parts\n   of the state are most essential to NPC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lJfAjMiaho", "forum": "x6iodYWNty", "replyto": "x6iodYWNty", "signatures": ["ICLR.cc/2026/Conference/Submission2301/Reviewer_CRzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2301/Reviewer_CRzY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761184760013, "cdate": 1761184760013, "tmdate": 1762916184217, "mdate": 1762916184217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A number of problems in different domains can be framed as homotopy\ninterpolations between an easily solved source problem and a complex target\nproblem. Such problems are typically solved with a predictor-corrector\nframework. Here, a solution is found to the initial easy problem. Then, the\npredictor advances the problem following the homotopy interpolation, and the\ncorrector modifies the solution to fit the new problem.\n\nPrior work has treated these homotopy problems separately in different domains.\nThis paper proposes to unite a number of these problems under a single\nframework. It then proposes a neural predictor-corrector framework, which uses\nreinforcement learning to train a policy that outputs the next level for the\npredictor and the next tolerance for the corrector --- previously, these values\nwere determined via heuristics. Experiments show that the final RL agent\neffectively learns to set these values."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This application of reinforcement learning seems quite novel to me. I\n   appreciate that PPO is able to be used \"out of the box\" to solve the problem.\n2. The problems chosen for the paper are quite diverse, which shows the\n   applicability of the framework. For example, I could see the application to\n   sampling / Langevin dynamics could eventually tie into generative modeling.\n3. The computational requirements for this method are quite low (lines 314-315),\n   making the research more accessible.\n4. The limitation of sensitivity to reward scale is acknowledged in the\n   appendix.\n5. The ablation of the state components helps in showing that all parts of the\n   state are necessary for NPC.\n\nOverall, I found the paper quite easy to read despite not having much\nfamiliarity with homotopy interpolations myself."}, "weaknesses": {"value": "My main concern is that it seems the experiments were only conducted over one\ntrial, as I could not find any mention of repetitions, and no error bars are\nreported in the tables. It is advisable to conduct multiple trials of each\nalgorithm and use statistical testing to check that differences between\nalgorithms are significant.\n\nI have also listed several questions in the section below; these are minor points that I think would be useful to address in the updated paper."}, "questions": {"value": "1. I am unclear if the Homotopy Paradigm discussed in Sec. 3.1 is a novel\n   contribution of this work. Is the term \"Homotopy Paradigm\" already widely\n   used in the literature? If so, could a citation for it be added in Sec. 3.1?\n   I think I am confused because the first sentence of the introduction\n   (line 32) seems to indicate that the Homotopy paradigm is already well-known,\n   while lines 121-122 indicate that it is a perspective introduced in this\n   work. Assuming this perspective is novel, it may be good to list it as a\n   contribution in the introduction. Right now, the first contribution of the\n   introduction (line 82) makes it clear that unifying the approaches under the\n   homotopy paradigm is novel, but it is unclear if the homotopy paradigm itself\n   is novel.\n2. Based on Appendix D, it seems the NPC framework shifts the manual effort from\n   tuning the parameters for the predictor and corrector to tuning the reward\n   scales for the RL training. What do you see as the benefits and tradeoffs of\n   this shift? Might there be cases where it is easier to just use the\n   heuristics? When is it better to try to tune the rewards?\n3. Table 6 shows that removing certain components from the state can increase\n   the number of iterations required more than removing other components, e.g.,\n   removing the corrector's tolerance increases by 64, while removing the\n   homotopy level only increases by 21. Does this say anything about which parts\n   of the state are most essential to NPC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lJfAjMiaho", "forum": "x6iodYWNty", "replyto": "x6iodYWNty", "signatures": ["ICLR.cc/2026/Conference/Submission2301/Reviewer_CRzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2301/Reviewer_CRzY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761184760013, "cdate": 1761184760013, "tmdate": 1763714771148, "mdate": 1763714771148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The homotopy paradigm is a principle to solve optimization problems by transporting solutions to problems from a simple family to a target problem domain. The paper discusses application areas in which this principle is applied: annealing in optimization and sampling. The predictor corrector (PC) method is a family of methods that solve homotopy problems by predicting a new location of a solution and then correcting that prediction. Existing PC approaches typically rely on a an interpolation schedule that must be chosen in advance. While some domains offer natural choices, others do not as the authors claim. Therefore, the authors propose to learn an interpolation using RL, which is implemented by learning the predictor and corrector steps using a neural network. The evaluation of the paper contains tasks from 4 domains and ablation studies that showcase the generalizability of the approach and competitive performance. The ablations provide some insights into the importance of the individual components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a nice unifying perspective that was at least new to me.\n- The method seems to improve over baselines on the reported problems. For some of the domains they evaluate on (GNC, root finding) I am not sure how well chosen the baselines are but that is rather on me.\n- The method is pretty simple, and thus seems to be easy to reimplement. I am surprised that almost no hyperparameters must be changed from the stablebaselines defaults.\n- Overall the paper is pretty clear and well written."}, "weaknesses": {"value": "### Method\n- The paper presents a very interesting and simple idea: use (reinforcement) learning to improve optimization and sampling methods. While I am not aware of any paper that discusses this under a single umbrella of homotopy problems, I have seen works that use learning to propose sampling steps, eg [1, 2, 3, 4, 5]. Some of these methods are mentioned in the related work section, yet I think it should become more clear that the objective of the paper is a unifying perspective.\n\n### Wording:\n- \"Because the predictor-corrector procedure is non-differentiable and early decisions influence the entire trajectory, supervised or self-supervised training is inadequate\" I think its totally fair that you use RL, but if you had data for supervised training, I think you could actually train in supervised fashion as your NN will be differentiable, no?\n- I am not sure if you can call the predictor and corrector schedules in Song et al. (2020) \"handcrafted heuristics \" as you state in your text, but their design choices are theoretically well motivated. Generally, using a linear schedule makes sense for many problems I would say.\n\n### Evaluation\n- The global optimzation problems are only in 2d, which seems pretty low to me, given that other communities like the derivative-free optimization community optimize on those functions in >10d. I would be interested in seeing the results on higher dimensions. But I think this point alone is not enough for rejection in my eyes.\n- I think the results are not the strongest on every task, for instance on the sampling problems, PGS seems to be on par at least. But I think this is a nitpick, since the approach seems to be more motivated to convince in its generality.\n\n### Clarity\n- While the high level idea of the paper is clear to me, some details of the algorithm are not: Why do you predict the corrector actions in line 3 of the algorithm? Is it always that single action you apply in line 7? If you predict both steps at once why have them separate at all? It would make much more sense to me if you first predicted the predictor step and then iteratively multiple different corrector steps based on H.\n\n### Minor\n- You should mention in the main paper that the functions you test on are 2d in Section 5.3.\n\n---\n### Sources\n[1] Richter, Lorenz, and Julius Berner. \"Improved sampling via learned diffusions.\" _arXiv preprint arXiv:2307.01198_ (2023).\n\n[2] Wang, Congye, et al. \"Reinforcement learning for adaptive MCMC.\" _arXiv preprint arXiv:2405.13574_ (2024).\n\n[3] Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, and Qingfu Zhang. Continuation path learning for homotopy optimization. In International Conference on Machine Learning, pp. 21288–21311. PMLR, 2023.\n\n[4] Ichikawa, Yuma. \"Controlling continuous relaxation for combinatorial optimization.\" _Advances in Neural Information Processing Systems_ 37 (2024): 47189-47216.\n\n[5] Hruby, Petr, et al. \"Learning to solve hard minimal problems.\" _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022."}, "questions": {"value": "- How do you weigh the two terms in your reward formulation? A specific equation would be very helpful.\n- For how long did you train your model? I am not familiar with the defaults of stable baselines and the info is not listed in the appendix.\n- How did you choose your kernel for computing the KSD in the evaluation?\n- As you state yourself, there are prior methods that use learning in the context of PC, but which seem to not generalize as well as yours as you point out in line 111. It would be interesting to see a comparison here, to understand better at which cost the generalization of your method comes, if an at all, especially as it is one of the key claimed contributions. Have you done such comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "74lI9KDZf7", "forum": "x6iodYWNty", "replyto": "x6iodYWNty", "signatures": ["ICLR.cc/2026/Conference/Submission2301/Reviewer_pEeU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2301/Reviewer_pEeU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761202290823, "cdate": 1761202290823, "tmdate": 1762916183927, "mdate": 1762916183927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Neural Predictor-Corrector (NPC) framework that leverages reinforcement learning (RL) to address homotopy problems across diverse domains, including robust optimization, global optimization, polynomial root-finding, and sampling. The core idea is to unify these traditionally independent homotopy tasks under a shared predictor-corrector (PC) structure, replacing hand-crafted heuristics for step-size selection and iteration termination with RL-learned adaptive policies. The authors employ an amortized training regime to enable one-time offline training and deployment on unseen instances, and validate NPC through experiments showing improved efficiency and stability compared to classical baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unified Framework for Diverse Homotopy Tasks: The paper identifies and formalizes the common PC structure underlying homotopy problems in optimization, root-finding, and sampling—an insight that helps consolidate fragmented research in these domains and highlights potential generalizability across tasks.\n2. Empirical Validation Across Domains: The authors conduct comprehensive experiments on four representative homotopy tasks (Graduated Non-Convexity, Gaussian Homotopy, Homotopy Continuation, Annealed Langevin Dynamics) and provide detailed ablation studies (e.g., RL state component analysis) to support the effectiveness of NPC in improving efficiency while preserving solution accuracy.\n3. Amortized Training for Practical Deployment: The amortized training design addresses a key limitation of task-specific learning methods by enabling deployment on unseen instances without per-task fine-tuning, which enhances the practical utility of the framework for real-world applications."}, "weaknesses": {"value": "1. Limited Novelty in RL for Optimization/Sampling: The core premise of applying RL to improve optimization or sampling workflows is not new. As noted in the paper’s related work, prior studies (e.g., Li, 2019; Belder et al., 2023; Ye et al., 2025) have already explored RL for adaptive parameter tuning, optimizer design, and schedule prediction in similar problem spaces. The paper does not sufficiently distinguish NPC from these existing RL-driven optimization/sampling frameworks beyond its focus on homotopy-specific PC structures.\n2. Incremental Improvement Over Traditional Methods: NPC largely builds on the well-established PC algorithm for homotopy problems and only replaces heuristic step-size/termination rules with RL policies—this constitutes a relatively minor modification rather than a paradigm shift. The framework does not introduce new theoretical insights into homotopy methods or RL for sequential decision-making; instead, it refines existing components with incremental adjustments, limiting its contribution to methodological advancement.\n3. Dependence on Manual Reward Scaling: A critical practical limitation is the need for manual tuning of reward scales for each problem instance (detailed in Appendix A), which undermines the framework’s claim of being a “general solver.” This manual step not only increases the barrier to deployment but also contrasts with the goal of automating heuristic-driven decisions—an issue that the paper acknowledges but does not meaningfully address beyond proposing future work."}, "questions": {"value": "1. Generalization to Non-Homotopy PC Tasks: The paper emphasizes unification across homotopy problems, but many non-homotopy tasks (e.g., iterative convex optimization, SGD with adaptive learning rates) also use PC-like structures. Does NPC’s RL policy generalize to these non-homotopy PC tasks, or is it inherently tied to the homotopy interpolation paradigm? If not, what limits its generalizability?\n2. Comparison to Learning-Based PC Baselines: The paper compares NPC to classical PC methods but only briefly mentions learning-based baselines (e.g., Simulator HC for polynomial root-finding). Could the authors include a more detailed comparison to these learning-based alternatives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9pvpZkuqWm", "forum": "x6iodYWNty", "replyto": "x6iodYWNty", "signatures": ["ICLR.cc/2026/Conference/Submission2301/Reviewer_7Gmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2301/Reviewer_7Gmc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988770952, "cdate": 1761988770952, "tmdate": 1762916183703, "mdate": 1762916183703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We are sincerely grateful to the reviewers for their insightful comments and valuable suggestions. In response, we have carefully revised the manuscript. The key changes are as follows:\n\n1. Add more experiments:\n    - Add a learning-based baseline, CPL (7Gmc-Q2; Sec 5.3 Tab.3).\n    - Add a comparison of different reward scaling (7Gmc-W3, CRzY-Q2; Appendix D Tab.7).\n    - Add high-dimensional experiments on non-convex function minimization benchmarks (pEeU-W4; Appendix E Tab.9).\n    - Add supplementary box plots for selected experiments (CRzY-W1; Appendix E Fig.5).\n2. Add more discussion and details:\n    - Include the explicit equation for the reward function (pEeU-Q1; Sec 4.2).\n    - Clarify that results represent the average over independent runs (CRzY-W1; Sec 5.1).\n    - Expand the discussion on the ablation study results (CRzY-Q3; Sec 5.6).\n    - Specify the kernel used for KSD computation (pEeU-Q3; Appendix A.4.3).\n    - Emphasize that the unifying perspective is the core contribution (pEeU-W1; Abstract, Sec 1), clarify the term \"homotopy paradigm\" (CRzY-Q1; Sec 1), and supplement the related work (pEeU-W1; Sec 2, Appendix C).\n\nThanks again for all the effort and time, and we look forward to further discussions if there are any more questions."}}, "id": "foCeN5xqTh", "forum": "x6iodYWNty", "replyto": "x6iodYWNty", "signatures": ["ICLR.cc/2026/Conference/Submission2301/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2301/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission2301/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763560883704, "cdate": 1763560883704, "tmdate": 1763560883704, "mdate": 1763560883704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}