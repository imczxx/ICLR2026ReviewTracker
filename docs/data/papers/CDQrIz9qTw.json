{"id": "CDQrIz9qTw", "number": 1238, "cdate": 1756866990874, "mdate": 1763719599460, "content": {"title": "AdaSine-LoRA: Adaptive Frequency Modulation for Nonlinear Low-Rank Adaptation", "abstract": "Low-Rank Adaptation (LoRA) has emerged as an efficient fine-tuning paradigm for large models by injecting trainable low-rank updates into frozen weights. However, the inherent linearity and low-rank nature of LoRA restrict its capacity to capture complex nonlinear semantics. Recent work demonstrates that applying nonlinear functions such as sine to the low-rank component can significantly enhance its expressiveness. Despite this, these methods typically rely on a static frequency, failing to accommodate the input-dependent variations in optimal perturbation scale. In this paper, we propose AdaSine-LoRA, a novel framework that integrates adaptive frequency modulation into the sine-activated LoRA formulation. Instead of using a fixed global frequency, our method dynamically generates a frequency coefficient conditioned on the input, enabling input-aware control over the perturbation pattern. We analyze the relationship between frequency and the effective rank of the perturbed weight space, and empirically demonstrate that adaptive frequency leads to consistently improved performance across diverse tasks with minimal parameter overhead. Our approach provides a lightweight yet effective mechanism to enhance LoRA's expressivity by aligning perturbation dynamics with task-specific input structures. Our extensive experiments demonstrate that this design consistently outperforms LoRA and its variants across a wide range of downstream tasks, including large language model fine-tuning and visual instruction tuning.", "tldr": "", "keywords": ["Parameter-Efficient Fine-tuning", "Low-Rank Adaptation", "Adaptive Nonlinear Modulation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ef53029be04791a9fe849f5e937bb9d4d287c29.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AdaSine-LoRA, which enhances Sine-LoRA and Sine-DoRA (proposed in [1]) by simultaneously making the frequency hyperparameter learnable and reducing memory requirements by applying the sine function on the adapter's outputs instead of the full weight delta.\n\nExperiments show that AdaSine-LoRA improves performance over Sine-LoRA and Sine-DoRA on multiple standard text classification, language generation, and visual question answering tasks, while at the same time reducing memory requirements to almost the same level as plain LoRA.\n\n[1] Ji et al.: _Efficient Learning with Sine-Activated Low-Rank Matrices_. ICLR 2025"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**(S1)** The paper directly addresses one important limitation of the Sine-LoRA paper. Most reviewers at that time asked how $\\omega$ is set and if there is a way to make it learnable [a]. This paper provides the answer to this question.\n\n**(S2)** The experiments are comprehensive, covering text classification, language generation, and vqa. While showing more experiments and benchmarks is never bad, the provided results enable a good overview of the proposed method's performance.\n\n**(S3)** The efficiency analysis in Sec. 4.3 is very valuable. This kind of analysis should exist in any paper proposing a new peft method.\n\n**(S4)** Explanations in the paper are clear, and figures are clear as well as visually pleasing\n\n### References:\n[a] https://openreview.net/forum?id=cWGCkd7mCp"}, "weaknesses": {"value": "**(W1)** While the paper addresses and solves a clear limitation of prior work (Sine-LoRA) and is original in this sense, making a hyperparameter trainable with a linear transform + sigmoid and changing the point where sine is applied does not strike as a particularly significant advancement of peft research. While it is an improvement of Sine-LoRA, it is not as independent a contribution as, for example, Sine-LoRA.\n\n**(W2)** In this regard, I am also curious about the choice of baselines: No experiment compares to Sine-DoRA, which was also described in [1] and achieved consistently better performance than DoRA. It would have been better to include this as well. Then, Tab. 1-3 all choose different sets of baselines: Tab. 2 doesn't include DoRA, while Tab. 3 doesn't include Sine-LoRA. I recommend always keeping the same set of baselines for better evaluating the respective strengths and weaknesses, or at least motivating why, in some settings, certain baselines are not suitable.\n\n**(W3)** I didn't find details which value $\\omega$ was used for the results in Tab. 1 and Tab. 2. Fig. 4 clearly demonstrates that we need a different value for each task, but was this ablation performed?\n\n**(W4)** The introduction lists 3 main contributions: Making $\\omega$ trainable, applying sine after linear projection, and theoretical analysis. Theoretical analysis is only in the appendix and does not appear in the main paper, so I feel listing this as a main contribution is somewhat misleading, at least it doesn't fit the current format. Consider dedicating a part of the main paper to theoretical analysis or consider removing it from the main contributions.\n\n**(W5)** Making $\\omega$ trainable and applying sine after linear projection are both listed as separate main contributions, but never evaluated separately, although this is possible by evaluating a variant of Eq. 8 with $\\omega_0 \\cdot \\sigma(W_{\\omega} x)$ fixed to some constant. This is already different from Sine-LoRA.\n\n**(W6)** In Fig. 5 (Sec. 4.3), the conclusion that AdaSine-LoRA is not sensitive to choice of $\\omega_0$ is strong. Roughly estimating, the difference between best and worst scores is roughly 1% on all tasks, which is in many cases higher than the advantage of AdaSine-LoRA over DoRA in Tab. 1. Consider discussing the effect and cost of tuning $\\omega_0$ in more detail. It seems especially problematic that performance is not monotonic in $\\omega_0$, at some point higher or lower $\\omega_0$ always leads to lower performance.\n\n**(W7)** One important and fundamental limitation of AdaSine-LoRA is that the weight delta can no longer be merged into the original weights, because we need the input projected by the weight delta independently for sine application. While this is less problematic at train time, it becomes a problem for inference: Inference will be significantly slower, requires higher memory, and incurs more complex deployment due to special architecture, as we always need to process adapters differently. This is not discussed or mentioned in the paper. Furthermore, it constitutes a clear disadvantage of AdaSine-LoRA compared to Sine-LoRA, which still allows merging the weight delta after training.\n\n**(W8)** Fig. 9 is missing one panel."}, "questions": {"value": "To be transparent, I currently see many weaknesses, and I think it requires a _very strong_ rebuttal to change my opinion. I think addressing the following questions would be useful to address to make the paper stronger:\n  * Use a unified set of baselines in all experiments. Always add Sine-LoRA, Sine-DoRA, and AdaSine-LoRA. For Sine-LoRA and Sine-DoRA, ablate $\\omega$ and report the best performing models. I am aware that doing these experiments in full is very costly, so this is meant as the ideal case.\n  * Add an ablation of the two contributions: Learning $\\omega$ and applying sine not to weight delta, but to projected inputs.\n  * Evaluate and discuss the inference-time overhead of AdaSine-LoRA. However, I think this is a serious and fundamental limitation. But maybe there is a solution?\n  * Strengthen arguments regarding the significance of the proposed method: Which other new approaches does this inform or enable? Is there anything else to learn, except that we can solve how to choose $\\omega$ in Sine-LoRA?\n * Clarify the role of theoretical contribution in the paper\n * Clarify the advantage of AdaSine-LoRA over DoRA with (a-priori) fixed $\\omega_0$ (Tab. 5)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K0jKxh6BXr", "forum": "CDQrIz9qTw", "replyto": "CDQrIz9qTw", "signatures": ["ICLR.cc/2026/Conference/Submission1238/Reviewer_o1LX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1238/Reviewer_o1LX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760881706576, "cdate": 1760881706576, "tmdate": 1762915714594, "mdate": 1762915714594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds on sine-based LoRA by introducing a token-adaptive frequency to enhance nonlinear expressivity while keeping compute/memory close to standard LoRA. Concretely, it introduces the adaptive update by first computing BAx and then applying a scaled sine with a learned, token-dependent frequency. Experiments demonstrate that the method achieve better performance while having similar computational cost to LoRA."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is very clear and well-structured, making the paper easy to follow.\n- The method proposed consistently achieves better performance than LoRA with similar computational cost.\n- The effectiveness of the method is demonstrated through comprehensive experiments across various tasks."}, "weaknesses": {"value": "- The proposed method is essntially an extension of LoRA via a specific nonlinearity. The insights on other design of fine-tuning method may be limited.\n- In section 2.3, the effect of frequency in the sine-based LoRA is discussed as the motivation of the proposed method. However, the proposed method has an essentially difference to the previous one: the sine function is applied after the computation of weight-token product. I think at least the comparison to $\\Delta Wx= \\frac 1 g \\sin (w_0 BAx)$ should be included and discussed to fill this gap. This is the ablation of removing only the adaptive frequency, but keeps the non-linearity the same as the proposed method."}, "questions": {"value": "- As mentioned in weaknesses, what is the effect of removing only the adaptive frequency machanism? \n- What is the main advantage of using sine function for non-linearity rather than other non-linear functions? \n-  Compared to LoRA where the weight update is unconstrained, the update in the proposed method is bounded by $1/g$. How does the scaling factor $g$ affects the performance? Can the authors provide a sensitivity study on $g$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tKE2szNbuU", "forum": "CDQrIz9qTw", "replyto": "CDQrIz9qTw", "signatures": ["ICLR.cc/2026/Conference/Submission1238/Reviewer_tmUH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1238/Reviewer_tmUH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468279895, "cdate": 1761468279895, "tmdate": 1762915714425, "mdate": 1762915714425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AdaSine-LoRA, an adaptive frequency-modulated extension to the LoRA paradigm for parameter-efficient fine-tuning of large neural networks. AdaSine-LoRA replaces the static, globally fixed sine frequency in nonlinear LoRA with an input-dependent, token-wise frequency coefficient, which is mapped from the input via a lightweight function. The work provides theoretical links between frequency and effective rank, a memory-efficient implementation strategy, and demonstrates empirical gains over standard LoRA and recent nonlinear variants on a broad suite of large language and vision-language model tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear motivation and insightful theoretical analysis: The paper identifies the limitation of fixed-frequency sine-activated LoRA, where its expressiveness and generalization can be bottlenecked by a single frequency poorly aligned with diverse input data. This insight is well-motivated by controlled experiments, as shown in Figure 2 (frequency vs. rank), Figure 3 (rank vs. accuracy), and Figure 4 (dataset-dependent optimal frequency).\n\n2. Methodological clarity and soundness: The adaptive frequency design is straightforward, computationally lightweight (just a single linear projection and normalization; see Section 3, Equation describing $\\tilde{\\omega}(x)$ and the final update rule), and theoretically justified. The paper makes a concerted effort to both theoretically and empirically link frequency modulation to representational rank, as made precise in supplemental theoretical results (Appendix A.2.2).\n\n3. Broad empirical results: Extensive experiments are conducted. Table 1 and Table 2 show consistent task-wise improvements from AdaSine-LoRA over both linear and nonlinear LoRA baselines, for multiple ranks and tasks. Gains are both numerically meaningful and achieved with minimal parameter overhead. Table 3 further validates generalization to multimodal (vision-language) instruction tuning, where AdaSine-LoRA outperforms not only LoRA variants, but even full fine-tuning in average accuracy."}, "weaknesses": {"value": "1. [Main Concern] Novelty: This paper builds upon previous analyses that explored the use of sine functions within the LoRA framework. Specifically, it extends prior work by introducing a modification where the previously fixed weight is now treated as a trainable parameter. While this represents a logical progression from earlier approaches, the contribution appears relatively incremental and may not provide substantial novelty beyond the existing literature. Moreover, using a nonlinear activation function is explored in multiple previous works, rendering this work's contribution rather incremental.\n\n2. Theoretical analysis lacks tightness and depth in main text: While the appendix provides additional theoretical insights, much of the theoretical analysis in the main text (e.g., the sigmoid-approximated transition in effective rank) is heuristic and does not provide concrete guarantees on downstream metrics (such as task loss or generalization error). A more detailed analysis connecting effective rank, representational power, and downstream task accuracy, potentially with bounds, would make the contribution firmer.\n\n3. Ablation studies missed: The method uses a linear projection followed by a sigmoid and scaling to predict the adaptive frequency (Section 3). However, there is limited justification or ablation of this choice compared to, e.g., a small MLP, normalization schemes other than sigmoid, or nonlinearity-free mappings. Do more complex mappings or normalization methods harm or help? Moreover, ablation studies on other activation function other than the sine function are also missed."}, "questions": {"value": "1. The reviewer is surprised to observe that AdaSine-LoRA reportedly achieves substantial reductions in memory usage and computational cost compared to Sine-LoRA. Could the authors provide a detailed analysis or explanation for the underlying reasons behind this improvement?\n\n2. The authors mention that using the raw output of ω(x) may lead to unstable training dynamics, particularly in the early stages when the model weights are randomly initialized in Line 218. To address this, they propose applying a sigmoid function. However, would it be possible to mitigate this instability by initializing the weights to zero or to very small values instead? A discussion or comparison of this alternative approach would be helpful to better understand the necessity and effectiveness of the sigmoid transformation.\n\n3. How sensitive is the performance to the structure of the frequency mapping (e.g., a single linear projection vs. a small MLP or alternative normalization)? An ablation study here would clarify if a more expressive nonlinearity or normalization improves or harms empirical results and training stability.\n\n4. Can the authors more rigorously link empirical gains to their observed/claimed increases in effective rank, e.g., through controlled analyses comparing models matched in parameter count and activation norm? Are there cases where higher effective rank might hurt (e.g., overfitting), or is there a provable connection to data complexity?\n\n5. Could the authors provide a more comprehensive comparison with other studies that explore nonlinear activation functions within similar contexts? Such a discussion would help clarify how the proposed approach relates to and improves upon existing methods.\nMoreover, would it be possible to address the same issue by **simply allowing the weight w to be trainable** in the baseline Sine-LoRA model? If so, how does the proposed method differ in terms of effectiveness or theoretical justification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7k3RdyREF5", "forum": "CDQrIz9qTw", "replyto": "CDQrIz9qTw", "signatures": ["ICLR.cc/2026/Conference/Submission1238/Reviewer_sMqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1238/Reviewer_sMqx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554433976, "cdate": 1761554433976, "tmdate": 1762915714257, "mdate": 1762915714257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AdaSine-LoRA, an adaptive extension of Sine-LoRA that introduces an input-dependent frequency modulation mechanism to enhance nonlinear low-rank adaptation. Instead of using a fixed global frequency parameter ω\\omegaω, AdaSine-LoRA learns a lightweight frequency predictor ω(x)=Wωx\\omega(x) = W_\\omega xω(x)=Wωx, enabling each input token to have its own modulation strength."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes an adaptively modulated frequency mechanism for nonlinear low-rank adaptation, which extends Sine-LoRA in a simple yet effective way.\n2. Theoretical analysis is well presented and provides clear justification for how input-dependent frequency improves expressive rank and stability."}, "weaknesses": {"value": "1. It would be helpful to include a visualization of the learned frequency ω(x) to demonstrate how it varies across inputs and layers, validating the adaptivity claimed by AdaSine-LoRA.\n2. Although the method is tested on multiple domains, a more thorough comparison with the latest SOTA low-rank approximation and adaptation methods would strengthen the empirical validation."}, "questions": {"value": "1. It would be helpful to include a visualization of the learned frequency ω(x) to demonstrate how it varies across inputs and layers, validating the adaptivity claimed by AdaSine-LoRA.\n2. Although the method is tested on multiple domains, a more thorough comparison with the latest SOTA low-rank approximation and adaptation methods would strengthen the empirical validation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kbRRJBU1vX", "forum": "CDQrIz9qTw", "replyto": "CDQrIz9qTw", "signatures": ["ICLR.cc/2026/Conference/Submission1238/Reviewer_PHfB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1238/Reviewer_PHfB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762084291357, "cdate": 1762084291357, "tmdate": 1762915714089, "mdate": 1762915714089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}