{"id": "4nNqVpC3td", "number": 13696, "cdate": 1758221047909, "mdate": 1759897418973, "content": {"title": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis", "abstract": "With the widespread adoption of LLMs, LoRA has become a dominant method for PEFT, and its initialization methods have attracted increasing attention. However, existing methods have notable limitations: \nmany methods do not incorporate target-domain data, while gradient-based methods exploit data only at a shallow level by relying on one-step gradient decomposition, \nwhich remains unsatisfactory due to the weak empirical performance of the one-step fine-tuning model that serves as their basis, as well as the fact that these methods either lack a rigorous theoretical foundation or depend heavily on restrictive isotropic assumptions.\nIn this paper, we establish a theoretical framework for data-aware LoRA initialization based on asymptotic analysis. Starting from a general optimization objective that minimizes the expectation of the parameter discrepancy between the fine-tuned and target models, we derive an optimization problem with two components: a bias term, which is related to the parameter distance between the fine-tuned and target models, and is approximated using a Fisher–gradient formulation to preserve anisotropy; and a variance term, which accounts for the uncertainty introduced by sampling stochasticity through the Fisher information.\nBy solving this problem, we obtain an optimal initialization strategy for LoRA.\nBuilding on this theoretical framework, we develop an efficient algorithm, LoRA-DA, which estimates the terms in the optimization problem from a small set of target domain samples and obtains the optimal LoRA initialization. Empirical results across multiple benchmarks demonstrate that LoRA-DA consistently improves final accuracy over existing initialization methods. Additional studies show faster, more stable convergence, robustness across ranks, and only a small initialization overhead for LoRA-DA.\nThe source code will be released upon publication.", "tldr": "", "keywords": ["LoRA", "Asymptotic Analysis", "Initialization", "Fisher information"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c781ed3a6cf9abecc78358b44e15a49333391b6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LoRA-DA, a data-aware initialization method for Low-Rank Adaptation.\nBy performing an asymptotic analysis of the fine-tuning objective, the authors decompose parameter error into bias and variance, and use the Fisher Information Matrix to derive an optimal initialization that aligns LoRA subspaces with informative directions.\nThe approach efficiently estimates Fisher curvature using K-FAC, adds only ~6% overhead, and achieves consistent improvements (0.3–1.0%) on multiple NLP and reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Provides a theoretical foundation for LoRA initialization via bias–variance decomposition.\n\n* Introduces a data-aware, Fisher-based initialization scheme that is simple and efficient.\n\n* Demonstrates consistent gains across tasks and good robustness under different ranks.\n\n* Well-written and clearly presented."}, "weaknesses": {"value": "* Incremental contribution, conceptually close to gradient-based methods like LoRA-GA.\n\n* Limited scale, improvements on large LLMs are modest.\n\n* Missing deeper analysis of Fisher estimation cost and stability."}, "questions": {"value": "* How sensitive is the performance to the sample size used for Fisher estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "68VlrjAA8q", "forum": "4nNqVpC3td", "replyto": "4nNqVpC3td", "signatures": ["ICLR.cc/2026/Conference/Submission13696/Reviewer_61Fr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13696/Reviewer_61Fr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563842486, "cdate": 1761563842486, "tmdate": 1762924248720, "mdate": 1762924248720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LoRA-DA, a theoretically motivated, data-aware initialization strategy for LoRA and LoRA-FA. Building on asymptotic analysis of the maximum likelihood estimator, the authors decompose the expected parameter discrepancy between the fine-tuned and target models into two components: a variance term (modeled by Fisher information) and a bias term (approximated via a Fisher-gradient formulation to preserve anisotropy). The optimal initialization corresponds to minimizing a quadratic form involving an Initialization Guidance Matrix, whose smallest eigenvectors determine the LoRA subspace. The resulting algorithm uses a small subset of target-domain samples to estimate Fisher and gradient statistics, showing improved convergence speed and accuracy on NLU and NLG benchmarks such as Commonsense170K, GSM8K, and MATH. The method achieves marginal but consistent gains over prior data-agnostic and gradient-based initializations while maintaining low computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this manuscript is fine. \n\n2. Across multiple benchmarks and under LoRA-FA, LoRA-DA consistently matches or surpasses strong baselines, confirming its practical utility.\n\n3. The proposed method enjoy low computational overhead."}, "weaknesses": {"value": "1.  My concern is that the Fisher-based variance term is borrowed directly from unconstrained MLE (Eq. 3–4) without adjusting for the rank-restricted parameterization W = W_0 + AB. This makes Eq. (13)/(19) formally inconsistent with the assumption A^\\top A = I_r.\n\n2. The substitution H_0 \\approx J(W_0) (Eq. 16) assumes the Hessian equals the Fisher matrix, which only holds under exact log-likelihood modeling and regularity assumptions. For non-i.i.d. situation, this equality fails. No empirical evidence validates it."}, "questions": {"value": "1. Have you tested LoRA-DA on larger LLMs?\n\n2. Please refer to Weakness for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9sGLrtXO0V", "forum": "4nNqVpC3td", "replyto": "4nNqVpC3td", "signatures": ["ICLR.cc/2026/Conference/Submission13696/Reviewer_3eyM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13696/Reviewer_3eyM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918889030, "cdate": 1761918889030, "tmdate": 1762924248305, "mdate": 1762924248305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LoRA-DA, a novel data-aware initialization method for LoRA. The approach is theoretically grounded in an asymptotic analysis, wherein the $B$ module is frozen while the algorithm determines the optimal initialization of the $A$ module that minimizes the expected Frobenius norm to the true parameter. Owing to this design, the proposed initialization method is architecture-agnostic. Experimental results on both natural language understanding and generation tasks demonstrate substantial performance gains over previous state-of-the-art methods, including LoRA, PiSSA, MiLoRA, and LoRA-One."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "LoRA-DA presents a more principled initialization strategy than prior works, as it explicitly minimizes the distance to the true parameter. The theoretical development is coherent and generally well-motivated, though certain aspects of the proofs require clarification and stronger justification (as discussed in the next section). Empirically, the method shows competitive performance and, in many cases, outperforms state-of-the-art initialization approaches on both natural language understanding and generation tasks. The ablation studies are detailed and informative, which further strengthens the empirical validation. Overall, the paper is clearly written and well-organized, facilitating comprehension of both the theoretical and experimental contributions."}, "weaknesses": {"value": "Please correct me if I'm wrong, but there were a few issues with the theoretical proof, which, if addressed, could strengthen the analysis of this paper: \n\n+ Line 886 - line 888: the result in Equation 25 was adapted from two references. However, the reviewer has carefully investigated the references and found no clear results close to the result presented in Equation 25. It would be great if the authors could provide further clarification on which theorem in these two references was adapted to this result. \n\n+ From the Equation in lines 941-942 to the Equation in lines 943-944: where did the term $O(1/\\sqrt{N})$ arise after the expansion? Moreover, it appears that many other terms after expanding the square are omitted. This step needs further clarification. \n\n+ Similarly, from lines 949-950 to lines 951-952, the terms related to $\\frac{\\partial P_{X; W_0}}{\\partial W}(W_{tgt}-W_0)$ are also ignored. Hence, it seems that in these two steps, a lot of approximations have been made, which potentially harm the correctness of the theoretical development."}, "questions": {"value": "Besides the issues related to the proof raised above, I only have one additional question: Why do we need an error of $O(1/\\sqrt{N})$ in Lemma 5? It appears that it is sufficient to have an error of $O(1/N)$ to conclude Theorem 1. If these theoretical issues are addressed, I am willing to adjust my evaluation accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B9taQYfNKi", "forum": "4nNqVpC3td", "replyto": "4nNqVpC3td", "signatures": ["ICLR.cc/2026/Conference/Submission13696/Reviewer_jRd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13696/Reviewer_jRd3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970809377, "cdate": 1761970809377, "tmdate": 1762924248022, "mdate": 1762924248022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce LoRA-DA, a data-aware initialization method for Low-Rank Adaptation (LoRA) in parameter-efficient fine-tuning (PEFT) of large language models (LLMs). It addresses limitations in existing initializations—such as data-agnostic approaches (e.g., PiSSA, MiLoRA) or shallow gradient-based ones (e.g., LoRA-One)—by deriving an optimal strategy via asymptotic analysis. The framework minimizes the expected parameter discrepancy between fine-tuned and target models, decomposing it into a bias term (approximated with Fisher-gradient to preserve anisotropy) and a variance term (capturing sampling stochasticity via Fisher information). An efficient algorithm estimates these terms from a small set of target-domain samples (e.g., 256). Evaluations on natural language understanding (NLU) benchmarks and natural language generation (NLG) tasks using LLaMA2-7B show consistent accuracy gains, faster and more stable convergence, rank robustness, and minimal overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper provides a rigorous asymptotic framework with proofs for optimal initialization. In addition, it also includes comprehensive experiments across NLU/NLG, ablations, rank sensitivity, and comparisons to strong baselines.\n\n2. The proposed method requires few samples for initialization with low overhead (e.g., ~2 mins on GSM8K), compatible with standard LoRA/LoRA-FA, and demonstrates real-world benefits like faster convergence in visualizations."}, "weaknesses": {"value": "1. The theories rely on MLE asymptotic normality and Fisher approximations, which may falter in non-iid, or highly non-Gaussian settings.\n\n2. While overhead is claimed small, there is no direct FLOPs/runtime comparisons to baselines.\n\n3. The limitations of the proposed method have not been discussed in the paper."}, "questions": {"value": "1. The Fisher-gradient approximation preserves anisotropy—could you compare it empirically to raw gradients beyond the degenerate case in Remark 3?\n\n3. For larger models (e.g., 10B+ parameters), does the eigenvalue computation (LOBPCG) scale well, and what are potential bottlenecks?\n\n4. How does LoRA-DA extend to other PEFT methods like DoRA and VeRA?\n\n5. The variance term models sampling stochasticity—have you analyzed its impact in noisy or domain-shifted datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HGQHT0retg", "forum": "4nNqVpC3td", "replyto": "4nNqVpC3td", "signatures": ["ICLR.cc/2026/Conference/Submission13696/Reviewer_6SDh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13696/Reviewer_6SDh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974371423, "cdate": 1761974371423, "tmdate": 1762924247793, "mdate": 1762924247793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}