{"id": "wBaNdkd4sr", "number": 2641, "cdate": 1757176632363, "mdate": 1759898136095, "content": {"title": "Runtime Adaptive Pruning for LLM Inference", "abstract": "Large language models (LLMs) excel at language understanding and generation, but their enormous computational and memory requirements hinder deployment. \nCompression offers a potential solution to mitigate these constraints. However, most existing methods rely on fixed heuristics and thus fail to adapt to runtime memory variations or heterogeneous KV-cache demands arising from diverse user requests.\nTo address these limitations, we propose RAP, an elastic pruning framework driven by reinforcement learning (RL) that dynamically adjusts compression strategies in a runtime-aware manner. \nSpecifically, RAP dynamically tracks the evolving ratio between model parameters and KV‑cache across practical execution. Recognizing that FFNs house most parameters, whereas parameter‑light attention layers dominate KV‑cache formation, the RL agent retains only those components that maximize utility within the current memory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art baselines, marking the first time to jointly consider model weights and KV-cache on the fly.", "tldr": "An RL‑guided elastic pruner that jointly trims weights and KV‑cache on the fly, cutting memory while speeding LLM inference", "keywords": ["Elastic pruning", "Runtime‑aware", "RL‑guided optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18ad5a55d8da4a9d82e6395b0de09469dd6824de.pdf", "supplementary_material": "/attachment/361c62c29782a4d00d1b95b148165cdda3a50625.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RAP (Runtime-Adaptive Pruning), a reinforcement learning-based framework for dynamically pruning Large Language Models (LLMs) during inference. Unlike existing static pruning methods, RAP adapts pruning decisions based on runtime conditions including memory constraints, batch sizes, and sequence lengths. The framework uses Greedy Sequential Importance (GSI) analysis to iteratively evaluate block importance and an RL agent to select which transformer blocks to prune. While the paper shows improvements over baselines on Llama and Qwen models, there are significant concerns about the evaluation methodology and practical applicability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper effectively demonstrates that memory bottlenecks shift dynamically between parameters and KV cache depending on workload characteristics.This is a valuable insight for the community.\n2. The evaluation covers multiple model families (Llama2, Llama3, Qwen) and includes both generation quality metrics and downstream task performance across 7 benchmarks.\n3. The three key observations in Section3 provide compelling evidence for adaptive approaches, particularly the insight that KV cache becomes the dominant memory bottleneck at larger batch sizes and sequence lengths."}, "weaknesses": {"value": "1. The paper fails to properly isolate the RL agent's contribution. While it compares against one-shot GSI and random selection, it critically lacks comparison with iterative GSI without RL (i.e., greedily removing blocks with highest GSI scores and re-evaluating). The comparison to random selection is hardly competitive and doesn't demonstrate the value of the learned policy. Without this ablation, it's unclear whether the RL agent adds any value over simply following GSI scores greedily.\n\n2. Computing GSI requires running inference through the full model initially, meaning the method can only be deployed on machines with enough memory for the unpruned model. This defeats the primary purpose - you cannot use RAP on memory-constrained devices where the full model doesn't fit, which is exactly where such methods are most needed. This is a critical flaw that severely limits practical deployment.\n\n3. The paper claims minimal overhead by focusing on the RL agent's 18K parameters, but completely ignores that GSI requires multiple forward passes through the model for importance evaluation. Each GSI iteration requires a full forward pass, making the actual latency overhead potentially orders of magnitude higher than reported. This is a serious misrepresentation of the computational cost.\n\n4. The paper doesn't clarify which baseline methods were designed to handle parameters + KV cache pruning vs just parameter pruning. If baselines only prune parameters while RAP prunes both, this creates an unfair advantage. Even if all methods prune parameters + KV cache but baselines were not designed to work for KV cache, it is still an unfair comparison. The evaluation protocol makes KV cache pruning the most important component, potentially biasing results if baselines aren't designed for this\n\n5. The paper's valuable contribution - demonstrating that \"KV cache pruning is an important part of model pruning for memory optimization\" - is buried in the experimental analysis rather than being a central claim. This insight about the dominance of KV cache in memory bottlenecks could be the paper's great contribution if properly emphasized.\n\n6. GSI is essentially iterative pruning with re-evaluation (a known technique), and the RL formulation uses standard DQN without innovations. The combination doesn't justify the complexity when simpler approaches might work equally well.\n\n7. No evaluation on truly dynamic workloads despite \"runtime-adaptive\" claims"}, "questions": {"value": "1. Can you provide results for iterative GSI without RL (greedily selecting highest GSI scores with re-evaluation)? This is essential to understand if the RL agent adds value beyond following GSI scores.\n\n2. How do you address the fundamental issue that GSI requires full model memory initially? Can the method work on devices where the full model doesn't fit? If the target deployment device cannot run the full model, how can RAP be used at all given GSI's requirements?\n\n3. What is the actual wall-clock latency overhead including all GSI computations? How many forward passes are required in practice?\n\n4. Please provide a clear table showing: 1/ which baseline methods prune parameters only vs parameters + KV cache, 2/ which methods were originally designed for KV cache pruning, 3/ whether you modified any baselines to handle KV cache"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rwv1aKZkOx", "forum": "wBaNdkd4sr", "replyto": "wBaNdkd4sr", "signatures": ["ICLR.cc/2026/Conference/Submission2641/Reviewer_Hb4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2641/Reviewer_Hb4M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666338779, "cdate": 1761666338779, "tmdate": 1762916315268, "mdate": 1762916315268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RAP, a runtime-adaptive pruning framework that (i) computes Greedy Sequential Importance scores for FFN/MHA blocks; and (ii) uses a lightweight RL controller to pick a pruning policy per request under a memory budget that includes both parameters and KV-cache."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Considering parameters and KV cache as the target is novel as most pruning work optimizes only weights.\n\n2.  Design with MLP with small overhead makes it easy and efficient to deploy.\n\n3. The ablation study shows the effectiveness of this method."}, "weaknesses": {"value": "1. Only zero-shot short-answer benchmarks. But long-context tasks (where KV matters) or real generation quality would better show the purported advantage.\n\n2. Need end-to-end latency and throughput comparison.  Real-world servers aslo care about tokens/sec and tail latency besides memory savings.\n\n3. If heads or layers are dropped at runtime, how are pre-existing KV tensors handled across decoding steps?\n\n4. If GSI already orders blocks and the agent “iteratively removes the least important,” where does RL refine this order?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZWUWn9v5KT", "forum": "wBaNdkd4sr", "replyto": "wBaNdkd4sr", "signatures": ["ICLR.cc/2026/Conference/Submission2641/Reviewer_mLUh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2641/Reviewer_mLUh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696245275, "cdate": 1761696245275, "tmdate": 1762916315133, "mdate": 1762916315133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAP,  framework for the runtime-adaptive pruning of Large Language Models (LLMs) based on RL. The core motivation is that real-world inference workloads and system memory availability are highly dynamic, which static pruning strategies cannot accommodate. RAP introduces a Greedy Sequential Importance (GSI) algorithm to better assess block importance and an RL agent that observes real-time system state and request characteristics to dynamically select which MHA or FFN blocks to prune, covering input-driven and system-level variance. Experiments show that RAP outperforms static pruning baselines under this budget-aware evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The shift from evaluating at a \"fixed sparsity ratio\" to a \"fixed memory budget\" is interesting. It more accurately reflects the deployment constraints on resource-limited devices.\n- The results clearly show that RAP makes more intelligent pruning decisions than static baselines, especially under aggressive memory budget.\n- The paper includes thorough ablation studies that demonstrate the necessity of both the GSI component and the RL agent."}, "weaknesses": {"value": "-  The entire motivation is built on optimizing for a Memory Budget. However, it fails to compare against the most effective and widely-adopted technique post-training quantization (e.g., INT4). A simple INT4 quantized model would occupy a smaller memory footprint than RAP's pruned FP16 model under the same budget.\n\n- In real-world applications, FP16 would not be deployed.  A convincing demonstration of RAP's value would be to show that it can further reduce memory on top of a quantized model.\n\n- The core idea is training a single and adaptive policy to replace a collection of static configurations. This concept was explored by the Once-for-All. The authors should discuss the connection to Once-for-All.\n\n- While the memory budget focus is practical, the complete absence of a standard fixed-sparsity comparison makes it difficult to isolate and appreciate the core algorithmic improvement.\n\nOnce-for-All: Train One Network and Specialize it for Efficient Deployment. ICLR 2020"}, "questions": {"value": "- How does RAP compare to a strong INT4 quantization baseline?  Can you show the performance of \"INT4 + RAP\" to demonstrate complementary benefits?\n\n- What is the computational cost of GSI?\n\n- I am curious whether, at the same parameter sparsity level (e.g., 30%), does RAP's GSI-based policy yield higher accuracy than a one-shot importance scoring method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "llcmODKX87", "forum": "wBaNdkd4sr", "replyto": "wBaNdkd4sr", "signatures": ["ICLR.cc/2026/Conference/Submission2641/Reviewer_SaEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2641/Reviewer_SaEz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980871475, "cdate": 1761980871475, "tmdate": 1762916314987, "mdate": 1762916314987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}