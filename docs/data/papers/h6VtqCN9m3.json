{"id": "h6VtqCN9m3", "number": 16040, "cdate": 1758258958362, "mdate": 1759897266141, "content": {"title": "Certified Evaluation of Model-Level Explanations for Graph Neural Networks", "abstract": "Model-level explanations for Graph Neural Networks (GNNs) aim to identify class-discriminative motifs that capture how a classifier recognizes a target class. Because the true motifs relied on by the classifier are unobservable, most approaches evaluate explanations by their target class score. However, class score alone is not sufficient as high-scoring explanations may be pathological or may fail to reflect the full range of motifs recognized by the classifier. To bridge this gap, this work introduces sufficiency risk as a formal criterion for whether explanations adequately represent the classifier’s reasoning, and derives distribution-free certificates that upper-bound this risk. Building on this foundation, three metrics are introduced: Coverage, Greedy Gain Area (GGA), and Overlap which operationalize the certificates to assess sufficiency, efficiency, and redundancy in explanations. To ensure practical utility, finite-sample concentration bounds are developed for these metrics, providing confidence intervals that enable statistically reliable comparison between explainers. Experiments on synthetic data and with three state-of-the-art explainers on four real-world datasets demonstrate that these metrics reveal differences in explanation quality hidden by class scores alone. Designed to complement class score, they constitute the first theoretically certified framework for evaluating model-level explanations of GNNs.", "tldr": "", "keywords": ["Model-Level Explanations for GNNs", "Theory of XAI", "Evaluation of GNN Explainablity"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8cf5695060ef5f1d3c7534d048eb080f7a05372d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In short, the paper proposes a new way of evaluating model-level explanations.\n\nMore in detail, the paper introduces sufficiency risk as a metric that equals zero for optimal explanations. Then, since the metric is difficult to compute, the paper introduces practical metrics, like Coverage, Greedy Gain Area, and Overlap. Coverage, in particular, is shown to upper-bound sufficiency risk, making it a principled way to estimate the risk. Convergence and approximation bounds from finite samples are further discussed, showing how the metrics can be efficiently and reliably estimated from limited data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- I agree that there is a fundamental lack in evaluation metrics for model-level explainers, and the paper proposes to fill the gap with an interesting suite of metrics. I think this contribution fills a big gap in the current literature.\n\n- I also agree that the usual approach of evaluating the classifier score metric cannot capture all the nuances of the explanation. The paper describes this issue in detail and motivates well why the literature should move beyond that.\n\n- The proposed metrics are principled, and the authors show how to reliably estimate their values, and also show how metrics complement each other.\n\n\nThis paper **has the potential to be a cornerstone paper** in the context of model-level explainer, and I liked a lot the motivation part. Nonetheless, **when it comes to the results and implementation side, I'm left with some concerns**, detailed below."}, "weaknesses": {"value": "- **W0**: The authors say that estimating sufficiency risk is particularly challenging, as it involves a conditional expectation. However, I struggle to understand why the provided formulation is the only possible way to define the sufficiency risk. Do you really need to formulate it as a conditional expectation? I think more details should be provided about the rationale of this formulation. Also, the authors write:\n\n> Low values of $SR_c(.)$ indicate that motifs faithfully capture the classifier’s reasoning.\n\nHowever, the authors claim that class core alone is not sufficient to distinguish good from bad explanations; therefore, **how can sufficiency risk alone be indicative of an explanation's faithfulness, given that it is an MSE between class scores?**\n\n\n- **W1**: Authors define the proxy membership function $M_r$ as a proximity-based check detailed in lines 188-191. However, I feel that a natural baseline for this would be to run some subgraph isomorphism check, which, albeit more computationally expensive, does not require defining a hyperparameter $r^*$ and is not approximated. I was expecting the authors to at least compare their proposed methodology with that simple baseline, to show it is actually better/more efficient than such a baseline. \n\n- **W2**: Line 196 says *the residual noise in $M_r$ is independent of $Y$ satisfying the assumption in Theorem 1*. However, I believe that considering the embeddings given by $\\phi$ does not make $M_r$ independent of $Y$, as $Y$ is still predicted from $\\phi$. So, I believe there is still a correlation between embeddings and classifier scores. Actually, embeddings contain all the information needed by the classifier to predict $Y$, making it clearly non-uncorrelated with $Y$.\n\n- **W3:** In lines 105-106, the authors write: \"*While this avoids unrealistic motifs, extracted patterns often cover only a limited subset of instances and may fail to generalize across the class*\". Since this is a strong statement showing key limitations of discovery-based approaches, some evidence or literature in support of the claim should be provided.\n\n- **W4:** Greedy Gain Area seems to favor explanations where few motifs cover a large portion of the input graphs (*it is high when a few motifs account for most of the explanatory power and low when many motifs are required*). Nonetheless, I believe that the opposite should be actually preferred: If only a few motifs cover the majority of graphs, then the other motifs present in the explanation are redundant, therefore indicating an explanation with useless motifs.\n\n- **W5:** The authors consider only a very simple model with a linear classification head in the experiments, allowing exact computation of $r*$. It is not clear how these results generalize when the exact computation of $r*$ is not possible, and how the approximation of $r*$ for more complex models will impact the metric values. \n\n- **W6:** It is unclear from the text that the 4Shapes dataset is actually proposed by the authors. Also, several implementational details are missing in the main text, like a proposed description of how motifs look like (their name is not very indicative), the test accuracy reached by the trained GNN, and the specific architecture of that GNN. Some of these details are reported in the Appendix, but no explicit link to that is presented in the main text, making it hard to parse the details coherently. \n\n- **W7:** The sentence in line 349: \n\n> For evaluation, each class is assigned two explanation sets: a good set containing 5 instances of the class-specific motif without the BA backbone and a bad set containing 5 random BA graphs.\n\nis not clear. What do you mean by evaluation here? Also, why does the good set of explanations contain 5 instances of the class-specific motif instead of just 1? Aren't these motifs always the same for a fixed class?\n\n- **W8:** In line 354, the authors say:\n\n> As shown in Figs. 1a–1b, good sets achieve both high class scores and coverage, while random BA graphs may reach high class scores but always yield zero coverage.\n\nis not true. In fact, Fig. 1a shows indeed very low coverage results, but for Class 2, the score is greater than zero.\n\n\n- **W9:** In line 353, the authors claim that \n\n>  random BA graphs may reach high class scores\n\nas in Fig. 1b class scores for Class 2 are considerably high, and this is presented as a weakness of the class score metric previously adopted by other papers. Nonetheless, **I feel this does not necessarily show the intended failure case**. In fact, a valid strategy for the GNN to solve the 4Shapes task is to recognize three out of four motifs, and predict the fourth class when such three motifs are not there in the graph. I suspect this is what is happening in Class 2 in the figure. Therefore, an explanation highlighting a random BA graph is actually a good explanation for Class 2, as it indeed does not contain any of the three other motifs, and this would invalidate your claim that the Class Score can be erroneously high for *bad* explanations. \n\n\n- **W10:** In line 377, the authors write:\n\n> For each class, we compare a unimodal explanation set (containing a single motif)\n\nbut it is unclear which single motif they are referring to, as classes have 2 possible motifs appearing together. In general, this experimental part should be better described and formalized more clearly.\n\n\n- **W11:** Authors provide a link to an anonymous GitHub repo, but the link is provided as a hyperref, which stops people reading the paper on paper from actually seeing the link. Also, I checked the repo, and **the files *metrics.py* and *requirements.txt* are empty**.\n\n**Minors**\n\n- **M1:** In line 51, the authors write that a uniform objective of model-level explainers is to generate motifs attaining a high target class score. Nonetheless, alternative formulations exist, like Azzolin et al. 2023 (already cited), and [1] which I think should be added to the discussion.\n\n- **M2:** Citation format is not consistent across the paper. I would suggest sticking to *\\citep{}* for non-in-text citations.\n\n- **M3:** Some theoretical results, like Theorem 2, are introduced abruptly. A more gentle and intuitive description would be better.\n\n- **M4:** Line 228, in particular the statement *admit efficient spectral-norm based estimates*, needs some references in support.\n\n- **M5:** Authors use the word *classifier score* and *class score* interchangably, which creates confusion. I would suggest sticking to only one way of referring to that concept.\n\n\n\n\n\n[1] GraphTrail: Translating {GNN} Predictions into Human-Interpretable Logical Rules. NeurIPS 2024.\n\n\n\n**In summary**, I believe the paper in the current format is not solid enough for acceptance, but I remain open to reconsidering the score if the authors improve it."}, "questions": {"value": "- Q1: I'm expecting that $SR_c(M*, E_c)$ to be always zero, as $M^*$ is the true membership function. Is this correct? \n\n- Q2: How is the membership function $M$ defined for disconnected motifs appearing in the same sample for a certain class, like *square AND lollipop*? And how does the proposed proxy membership function $M_r$ cope with that situation? Examples of such explanations can be found in Azzolin et al. 2023 and [1].\n\n- Q3: Fig 1c shows that the embedding-motif distances separate quite well good from bad explanations. Hence, why not use that as a metric by itself, by setting a threshold on the distance to separate good from bad explanations?\n\n- Q4: Do the authors think that evaluating a model-level explanation by feeding the classifier with only the individual motif may cause some OOD issues in the evaluation, so that model's prediction changes not because the motif is not representative of what the model is doing, but just because the input sample is OOD?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3lUkm99bBz", "forum": "h6VtqCN9m3", "replyto": "h6VtqCN9m3", "signatures": ["ICLR.cc/2026/Conference/Submission16040/Reviewer_GqAd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16040/Reviewer_GqAd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760705130470, "cdate": 1760705130470, "tmdate": 1762926240962, "mdate": 1762926240962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a principled evaluation framework for model-level explanations on graph neural networks (GNNs), introducing\ntheoretical guarantees via sufficiency risk and three novel metrics: Coverage, Greedy Gain Area (GGA), and Overlap. The work\naddresses a critical gap in evaluating model-level explanations beyond classification scores, with rigorous theoretical foundations\nand extensive experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The formalization of sufficiency risk provides a rigorous foundation for evaluating explanation quality. Theorems 1-3 and Proposition 1-2 establish certified bounds linking the proposed metrics to sufficiency risk, ensuring reliablity of metric even in a finite sample scenario.\n\nS2. The three metrics offer complementary views of explanation quality (sufficiency, efficiency, non-redundancy), which reveals unfaithfulness, redundancy and mode collapse in the methods relying only on class score through extensive experiments.\n\nS3. Experiments on 2 synthetic and 4 real-world datasets convincingly demonstrate the metrics' ability to expose limitations of class-score-based evaluation on model-level GNN explanations, such as pathological explanations and pattern collapse."}, "weaknesses": {"value": "W1. The proxy function $M_r$ relies on nearest-neighbor distance in the embedding space. While simple and computationally efficient,\nthe paper doesn’t give the reason of choice and whether the measure is optimal for all GNN architectures or datasets or not.\n\nW2. The description of “For each target class, an explanation set of ten motifs is generated by each explainer.” on page 8 indicates that\nthe current approach considers model-level explanations typically using small M(~10 motifs) in practice, as observed in the experiments. However, the computational complexity of GGA is related to O(NM^2) in Appendix F. A large explanation sets may lead to low scalability. Why does the paper define a ten motifs scenario in practice?\n\nW3. Experiments only use GIN architectures (In Appendix D.2). The theoretical bounds depend on the Lipschitz constant of the\nclassifier head (In Theorem 2), which may vary with architecture (e.g., GAT or GCN), affecting metric stability."}, "questions": {"value": "See the questions associated with Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SAHOiIysca", "forum": "h6VtqCN9m3", "replyto": "h6VtqCN9m3", "signatures": ["ICLR.cc/2026/Conference/Submission16040/Reviewer_JtxQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16040/Reviewer_JtxQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684586871, "cdate": 1761684586871, "tmdate": 1762926240422, "mdate": 1762926240422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the long-standing problem of evaluating model-level explanations for Graph Neural Networks (GNNs). Unlike instance-level explainers, which clarify decisions for individual samples, they focus on the explainers that aim to uncover class-discriminative motifs or graph patterns that the classifier consistently relies on. Current evaluation practices hinge almost entirely on target class scores, assuming high-scoring motifs faithfully represent the model’s reasoning. The authors argue this is flawed: high scores can result from pathological motifs detached from the true data distribution. To address this, they propose a theoretically grounded evaluation framework supported by distribution-free certificates that upper-bound an explanation’s insufficiency, or sufficiency risk."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper tackles a very important and long standing problem, namely, evaluating model-level GNN explanations. \n2. The paper proposed three reasonable metrics.\n3. The paper presented time analysis of the metrics."}, "weaknesses": {"value": "1. The usage of the term \"model-level\" in this paper is not accurate. They develop metrics (Coverage, Greedy Gain Area, Overlap) that assume the explanation set is a collection of motifs (subgraphs) that represent class-discriminative patterns. However, many so-called “model-level explainer” methods do not output exactly “motifs” in this sense (i.e., compact sub-graph patterns extracted from or representative of the dataset). For example: XGNN generates class-specific instances (i.e. synthetic graphs) rather than mining motifs present in the training set. GCNeuron may produce Boolean rules or logic combinations rather than explicit graph motifs. The claim that their evaluation metrics are “for model-level explanations” is true only if the explainer produces motif-style explanations. If an explainer produces rules, logic formulas, or generated graphs that don’t align with “motifs from data”, then the metrics may not apply cleanly. The story told by the paper implies the metrics apply to all model‐level explainers, but they in fact only apply to motif‐extraction style ones. This needs to be clarified in the title, abstract, introduction, etc., instead of keeping it vague or overclaim. \n2. Presentation needs improvement. Citation style needs to be corrected.\n3. The three metrics, Coverage, GGA, and Overlap, are intuitive and could be justified directly from geometric reasoning. The introduction of sufficiency risk and distriThere is a fundamental mismatch between the proposed metrics and the explainers evaluated. \n4. The metrics are designed for motif-extraction-based model-level explanations, where each explanation represents a class-discriminative subgraph sampled from or near the data distribution. However, the tested methods (XGNN, GNNInterpreter, PAGE) are generation-based and produce synthetic or prototype instances rather than extracted motifs. As a result, the evaluation does not truly measure what the proposed metrics are meant to assess. The reported results cannot substantiate the claimed effectiveness of the framework, since the metrics and the explainers operate on fundamentally different types of outputs.bution-free certificates adds complexity without offering real insight or practical necessity."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tnnHuztRDX", "forum": "h6VtqCN9m3", "replyto": "h6VtqCN9m3", "signatures": ["ICLR.cc/2026/Conference/Submission16040/Reviewer_jLqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16040/Reviewer_jLqD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778726149, "cdate": 1761778726149, "tmdate": 1762926239566, "mdate": 1762926239566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces three metrics, coverage, greedy gain area (GCA), and Overlap, to characterize the quality of the model-level explanations for GNNs. Coverage quantifies the likelihood a class is covered by a motif, determined by a threshold r and a distance measure (nearest motif). A tightness analysis verifies a suggested universal radius r for the sufficiency risk estimation.  GCA, based on coverage analysis, gives an area under the coverage curve. Overlap measures the possible redundancy of the motifs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Principled metric system and efficient assessment are important topics for graph learning. \nS2. The paper provides an optimality analysis of parameter choices, such as r, and provides proper upper and lower bounds. \nS3. The metrics are simple and generally applicable."}, "weaknesses": {"value": "W1. There is a lack of time-cost analysis for the assessment process. \nW2. Only the proposed metrics are measured; the methods lack more insightful justification from third-party measurements regarding consistency or any contradictory observations.  \nW3. It seems all or some of these measures are highly correlated. A necessary correlation or orthogonality analysis is needed."}, "questions": {"value": "D1. It is not yet clear how the proposed metrics may encourage existing methods to seek computable optimal explanations under such measures, or whether they are conflicting or highly correlated, thereby indicating suboptimal yet Pareto-optimal solutions. An in-depth meta-measure analysis to verify potential trade-offs is needed. Other properties of metric systems, such as their susceptibility to scaling factors and user-defined preferences, and their invariance properties (will the value change significantly, suggesting different quality simply because a different distance function is used?) under the replacement of task-driven distance measures, deserve in-depth discussion. \n\nD2. How the bound analysis helps in practice should be elaborated. It may suggest proper normalized measures—and indicate a fair assessment? The advantages of the proposed metrics, compared with other commonly adopted ones, should be discussed. \n\nD3. How easy is it to compute such measures? What's the time cost? This needs further analysis. \n\nD4. More insightful results could be summarized from Table 1. As is, only the results are shown, with little further analysis of how these measures help recommend or suggest promising solutions. \n\nD5. More state-of-the-art GNN explainers should be compared and experimentally tested to show the benefit of the proposed measures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "trTUU6PR3E", "forum": "h6VtqCN9m3", "replyto": "h6VtqCN9m3", "signatures": ["ICLR.cc/2026/Conference/Submission16040/Reviewer_qamU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16040/Reviewer_qamU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185556566, "cdate": 1762185556566, "tmdate": 1762926238542, "mdate": 1762926238542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}