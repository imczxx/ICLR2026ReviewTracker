{"id": "nwXCmnZ35w", "number": 10715, "cdate": 1758180177053, "mdate": 1759897633731, "content": {"title": "Agent²: An Agent-Generates-Agent Framework for Reinforcement Learning Automation", "abstract": "Reinforcement learning (RL) agent development traditionally requires substantial expertise and iterative effort, often leading to high failure rates and limited accessibility. This paper introduces Agent$^2$, an LLM-driven agent-generates-agent framework for fully automated RL agent design. Agent$^2$ autonomously translates natural language task descriptions and environment code into executable RL solutions without human intervention. \n\nThe framework adopts a dual-agent architecture: a Generator Agent that analyzes tasks and designs agents, and a Target Agent that is automatically generated and executed. To better support automation, RL development is decomposed into two stages—MDP modeling and algorithmic optimization—facilitating targeted and effective agent generation. Built on the Model Context Protocol, Agent$^2$ provides a unified framework for standardized agent creation across diverse environments and algorithms, incorporating adaptive training management and intelligent feedback analysis for continuous refinement.\n\nExtensive experiments on benchmarks including MuJoCo, MetaDrive, MPE, and SMAC show that Agent$^2$ outperforms manually designed baselines across all tasks, achieving up to 55\\% performance improvement with consistent average gains. By enabling a closed-loop, end-to-end automation pipeline, this work advances a new paradigm in which agents can design and optimize other agents, underscoring the potential of agent-generates-agent systems for automated AI development.", "tldr": "", "keywords": ["Agent-Generates-Agent", "Automated RL Design", "LLM-Driven Automation", "Autonomous AI Development", "Self-Improving Systems"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2590815dd6a88b4bab256bd4a86141133d18292e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present Agent^2, an LLM-based framework for creating MDPs and agent architectures (including hyperparameters) to automate the reinforcement learning agent development pipeline. The authors compare the results of their method (i.e. the performance of the Target Agents generated by Agent^2) to the baseline benchmark agents for various environments, including those in MuJoCo, MetaDrive, MPE, and SMAC.\n\n1. **What is the specific question and/or problem tackled by the paper?**\n    \n    The problem of automating RL agent design.\n    \n2. **Is the approach well motivated, including being well-placed in the literature?**\n    \n    It appears that the work is well-placed in the literature.\n    \n3. **Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.**\n    \n    The Target Agent performance beats the baselines in the vast majority of environments.\n    \n4. **What is the significance of the work? Does it contribute new knowledge and sufficient value to the community?**\n    \n    The paper contributes significant value by virtue of automating the design of RL agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is structured and written in a logical way, the algorithms are generally clear and possible to follow, the results are interpretable and clear, and the originality is there."}, "weaknesses": {"value": "The paper lacks some examples of some of the outputs generated by the LLMs, e.g. L_algo and L_analysis, which would aid in understanding what the language agents are doing. Overall the paper is presenting a huge framework. Supplemental materials should be submitted and reviewed."}, "questions": {"value": "Examples of L_analysis, L_algo, etc. for an example environment would be useful to include in the appendix.\n\nAnalysis of the Task-To-MDP Mapping would be useful and interesting.\n\nSupplementary material would be useful. Where is the codebase? Rejecting until the codebase can be verified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CAdNQJylw5", "forum": "nwXCmnZ35w", "replyto": "nwXCmnZ35w", "signatures": ["ICLR.cc/2026/Conference/Submission10715/Reviewer_HBay"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10715/Reviewer_HBay"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761270649353, "cdate": 1761270649353, "tmdate": 1762921949391, "mdate": 1762921949391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a framework for automatically designing the formalization of an RL problem and selecting the most suitable algorithm to solve it. They make use of an LLM approach, where the user is required to specify the problem, and the final outcome is a formalization (via code) of the (problem, algorithm) pair. They tested their approach vs. benchmarks present in the literature and showed that it is providing a better solution than the ones using standard parameter configurations"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The framework is clearly presented and described\n- The work goes in the direction of the democratization of RL"}, "weaknesses": {"value": "- The English should be polished\n- If I understood correctly, the final outcome is a set of properly designed prompts, which does not seem like a rigorous methodology\n- The level of the user and the  knowledge required to describe the problem is not discussed or assessed\n- The pseudocode is not explained properly\n- No considerations about the difference in terms of modeling between the proposed solution and the benchmark have been provided\n- No clear information about the explanation of the models selected is provided\n- No code or tool is provided or claimed will be distributed"}, "questions": {"value": "1) In the MDP modeling, how can one determine the best features for the problem at this stage?\n2) What is info in f_rew?\n3) It is not clear what the verification operator is doing. Even if the meaning is evident, how it would verify that a model is appropriate and functioning is not clear from your description. The pseudocode is not helping the understanding\n4) It is not clear to me if the LLM component is using a general-purpose LLM or models that you trained or fine-tuned over the specific RL setting.\n5) Is it really necessary to do hyperparameter tuning with an LLM? You should motivate why it is better than using standard optimization procedures.\n6) Algorithm 2: The pseudocode you provided is too generic. You should describe it more specifically, especially when specifying the termination conditions.\n7) Are the results in Table 1 the result of a single run learning procedure, or is the uncertainty about them available somehow?\n8) What are you going to release as a tool to be used by the RL community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NatGhudIsw", "forum": "nwXCmnZ35w", "replyto": "nwXCmnZ35w", "signatures": ["ICLR.cc/2026/Conference/Submission10715/Reviewer_7D2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10715/Reviewer_7D2m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549988091, "cdate": 1761549988091, "tmdate": 1762921948811, "mdate": 1762921948811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Agent². It is a framework that uses an LLM to design and train the RL agent for certain tasks. The LLM analyzes the environment, defines the MDP elements (observations, actions, rewards) and picks an algorithm and hyperparameters to train the agent. Experiments on several benchmarks show consistent improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The two-step procedure of MDP design and algorithmic optimization is reasonable. The writing is easy to understand.\n\nThe evaluation is convincing. Both single and multi-agent domains are covered. Strong baselines are compared with. The results support that the proposed approach outperforms the baselines most of the time. Ablation studies are also included."}, "weaknesses": {"value": "The paper is lacking many training details in general. The appendix only provides the prompts and many details remain unclear. For instance, what do all the Layers specifically mean in Listing 5 and how are they converted into a real network? How large are they? Given only the current version of the paper, the reproducibility is limited.\n\nAs we know the LLMs are fairly random, it is important to have multiple runs of the whole framework (call LLM multiple times) and report the mean and standard deviation in table 1. Currently it only reports a single run and is thus less convincing. This point also goes back to the reproducibility.\n\nI am not sure if the authors have taken the computation on the LLM calls into account when comparing with the baselines. The LLM calls spend extra compute and wall time."}, "questions": {"value": "What happens if the LLM produces faulty output? For example, if the generation is not executable, how is that situation handled?\n\nDid you make sure that the generated network architecture is of a similar size to those baselines? I didn't find details regarding this point in the paper.\n\nWhy are the results in Figure 2 not matched to those in table 1? Agent² seems to achieve below 3500 in PPO-ant, but in table 1 it is higher than that. I may be wrong in understanding the figure or table."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cCMlFWmh5D", "forum": "nwXCmnZ35w", "replyto": "nwXCmnZ35w", "signatures": ["ICLR.cc/2026/Conference/Submission10715/Reviewer_R2dm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10715/Reviewer_R2dm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785338062, "cdate": 1761785338062, "tmdate": 1762921948444, "mdate": 1762921948444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a method named $\\text{Agent}^2$, an LLM-driven 'agent-generates-agent' framework, which aims to achieve full automation of Reinforcement Learning (RL) agent design. As the authors do not provide their codes, I highly doubt its veracity because the authors did not provide the code"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is easy to follow\n- This algorithm is still quite creative"}, "weaknesses": {"value": "- The framework heavily relies on the code synthesis and reasoning capabilities of the underlying foundation model (LLM), and the inherent limitations of the LLM directly impact the framework's reliability\n- The closed-loop iterative verification and improvement process introduces very high computational complexity\n- As the code was not provided, the veracity of the experimental results remains questionable"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5wrqczzu3Z", "forum": "nwXCmnZ35w", "replyto": "nwXCmnZ35w", "signatures": ["ICLR.cc/2026/Conference/Submission10715/Reviewer_ZYPR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10715/Reviewer_ZYPR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932486058, "cdate": 1761932486058, "tmdate": 1762921948072, "mdate": 1762921948072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}