{"id": "sl7s5KRnyh", "number": 4523, "cdate": 1757697276836, "mdate": 1759898028373, "content": {"title": "VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Vision Backbones", "abstract": "Recent studies have indicated that vision models pre-trained on images can serve as time series foundation models (TSFMs) by reformulating time series forecasting (TSF) as image reconstruction.  However, effective cross-modal transfer from vision to time series remains challenging due to three discrepancies:  (1) the data-modality gap between structured, bounded image data and unbounded, heterogeneous time series;  (2) the multivariate-forecasting gap between fixed RGB-three-channel vision models and time series with arbitrary numbers of variates; and (3) the probabilistic-forecasting gap between the deterministic outputs of vision models and the requirement for uncertainty-aware probabilistic predictions. \nTo bridge these gaps, we propose VisonTS++, a TSFM based on continual pre-training of a vision model on large-scale time series.  Our approach introduces three key innovations:  (1) vision-model-based filtering to identify high-quality sequences to stabilize pre-training and mitigate modality gap;  (2) colorized multivariate conversion, encoding multivariate series as multi-subfigure RGB images to enhance cross-variate modeling;  (3) multi-quantile forecasting, using parallel reconstruction heads to generate quantile forecasts without parametric assumptions. \nExperiments show that VisionTS++ achieves state-of-the-art performance in both in-distribution and out-of-distribution forecasting, outperforming specialized TSFMs by 6%–44% in MSE reduction and ranking first in GIFT-Eval benchmark which comprises 23 datasets across 7 domains.  Our work demonstrates that with appropriate adaptation, vision models can effectively generalize to TSF, thus advancing the pursuit of universal TSFMs.", "tldr": "We propose VisionTS++,  which bridges vision and time series via continual pre-training, using data filtering, colorized multivariate conversion, and multi-quantile forecasting to achieve universal time series foundation modeling.", "keywords": ["Time Series Forecasting", "Time Series Foundation Model"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/918506ac1f9a934f9f80a53368956e528d37f1d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work explores the application potential of vision pre-trained model as time series foundation model. Addressing the challenges in transferring vision models to time series tasks—including 1) the data-modality gap between image data and time series, 2) the multivariate-forecasting gap between RGB channels in vision models and time series structure, and 3) the probabilistic-forecasting gap between vision models and time series predictions—this research proposes the ​VisionTS++​​ model. By leveraging vision models, VisionTS++ represents multivariate time series as multiple RGB sub-images to enhance cross-variable dependency modeling and identify salient temporal features. Additionally, it incorporates multi-quantile probabilistic forecasting. This work further investigates the effectiveness of adapting vision models as temporal foundation models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed multi-quantile forecasting provides a solution for converting images into probabilistic predictions that reflect the uncertainty in time series forecasting.\n\n2.The \"Vision-Model-Based Filtering\" applied to time series appears to be a reasonable and necessary processing step, ensuring that the unbounded nature of time series values can adapt to the bounded pattern of raw image pixels.\n3.Extensive experiments on multiple datasets (including the Monash Benchmark, LTSF, and probabilistic forecasting datasets) and competitive results in both in-distribution forecasting and out-of-distribution forecasting validate the model's strong performance.\n\n4.The paper is well-written, complete, and easy to follow."}, "weaknesses": {"value": "1.While the Color-as-boundary strategy appears reasonable, it may exhibit inherent limitations when the number of time series variables significantly exceeds the available RGB channels: On one hand, the intrinsic properties of RGB channels may introduce implicit color bias, causing the model to overemphasize variables associated with certain channels and impair the balance of feature extraction. On the other hand, color reuse may lead to erroneous associations between non-adjacent variables, resulting in the model capturing false dependencies and interfering with the modeling of genuine variable relationships. Moreover, the randomness in color assignment lacks consideration of variable semantic correlations, which can easily lead to scenarios where highly correlated variables are assigned vastly different colors or weakly correlated variables share similar colors, thereby undermining the model's stability and robustness in high-dimensional settings.\n\n2.Although the \"Vision-Model-Based Filtering\" serves as a reasonable truncation method to reconcile the numerical characteristics of time series with the input compatibility of vision models, it remains questionable whether such normalization adequately accounts for the diversity of time series distributions. For instance, in domains such as healthcare and finance, time series may contain extreme values of significant importance or exhibit upward or downward trends. It is unclear whether the filtering approach remains compatible in such cases. Additionally, other filtering methods worthy of consideration could be explored to accommodate a broader range of time series types.\n\n3.While VisionTS++ explores more suitable input representations for time series data and improvements to the vision model's output head, the MAE backbone remains largely unmodified. Investigating the potential of more diverse vision backbones for time series applications may help enhance forecasting performance. Furthermore, incorporating comparisons with recent vision-based related work [1] would strengthen the rigor and comprehensiveness of the study.\n\n[1] Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting.  ICML2025"}, "questions": {"value": "In Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TYboYell8s", "forum": "sl7s5KRnyh", "replyto": "sl7s5KRnyh", "signatures": ["ICLR.cc/2026/Conference/Submission4523/Reviewer_r4WL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4523/Reviewer_r4WL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761214741016, "cdate": 1761214741016, "tmdate": 1762917422878, "mdate": 1762917422878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VisionTS++, a new time-series foundation model that adapts pretrained vision backbones through continual pre-training on large-scale time-series data. The authors identify three critical challenges—data-modality gap, multivariate-forecasting gap, and probabilistic-forecasting gap—when transferring vision models to time-series forecasting. VisionTS++ addresses these issues via three key components: Vision-Model-Based Filtering, Colorized Multivariate Conversion, and Multi-Quantile Forecasting. The proposed method achieves strong results on 23 datasets across multiple benchmarks, showing consistent improvement over other TSFMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies major limitations when applying vision models to time-series forecasting and systematically attempts to address them.\n2. Incorporating probabilistic forecasting into the framework is novel and refreshing, extending beyond conventional deterministic designs."}, "weaknesses": {"value": "1. The study only evaluates MAE-based VisionTS++, without validating other vision backbones(SimMIM, BootMAE, etc.). This limits the generality of the proposed framework.\n2. The paper does not discuss the computational cost of continual pre-training, raising concerns about training efficiency and scalability."}, "questions": {"value": "1. VisionTS++ appears to use longer or variable look-back windows compared to baselines. Does this lead to unfair comparisons? Moreover, how does the model’s performance vary with respect to input window length?\n\n2. The Vision-Model-Based Filtering step discards samples incompatible with vision model constraints. Does this imply that VisionTS++ cannot handle such series at all? The normalization parameter γ seems manually set—could adjusting it recover those filtered samples?\n\n3. The Colorized Multivariate Conversion introduces several potential issues:\n\n   a) The artificial spatial adjacency between variables might induce bias, as neighboring pixels are assumed to be correlated in image models.\n\n   b) The color assignment scheme lacks theoretical justification and could introduce additional bias.\n\n   c) For high-dimensional series (M > 224), some variables cannot be represented within a fixed image size. How is this issue handled?\n\n4. Using multiple heads in Multi-Quantile Forecasting could exacerbate the computational burden of VisionTS++. Has the efficiency–accuracy trade-off been analyzed?\n\n5. As shown in Figure 2 and Appendix E, the model seems to rely heavily on periodic horizontal replication patterns. If the image conversion were not aligned with the true periodicity, would VisionTS++ still perform effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GqBzri68EQ", "forum": "sl7s5KRnyh", "replyto": "sl7s5KRnyh", "signatures": ["ICLR.cc/2026/Conference/Submission4523/Reviewer_GUdJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4523/Reviewer_GUdJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761340013272, "cdate": 1761340013272, "tmdate": 1762917422618, "mdate": 1762917422618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the data–modality gap, the multivariate–forecasting gap, and the probabilistic–forecasting gap, this paper proposes VisionTS++, a vision-model-based time series foundation model trained through large-scale pretraining. VisionTS++ consists of three key components: vision-model-based filtering, colorized multivariate time series conversion, and multi-quantile forecasting. After pretraining, VisionTS++ achieves strong forecasting performance in both in-distribution and out-of-distribution scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new vision-model-based time series foundation model, which achieves good forecasting performance across multiple benchmarks.\n2. The paper clearly identifies the key challenges of applying vision models to time series analysis, including the data–modality gap and the multivariate–forecasting gap.\n3. The overall writing of the paper is clear and well-organized."}, "weaknesses": {"value": "1. The paper presents an incremental improvement over VisionTS, with the proposed modules—vision-model-based filtering, colorized multivariate conversion, and multi-quantile forecasting—being relatively straightforward. The filtering module performs simple threshold-based filtering; the multivariate conversion resembles prior vision-based time series models such as ViTST (NeurIPS 2023); and the multi-quantile forecasting capability has already been incorporated in most recent TSFMs.\n2. Regarding vision-model-based filtering, the method relies on a simple threshold mechanism, which may inadvertently remove large but legitimate variations (e.g., those caused by major events or holidays), potentially leading to inaccurate predictions in such cases.\n3. For multivariate conversion, the method concatenates different time series variables into an image representation. While the vision model may implicitly capture variable dependencies, the approach does not explicitly model variable correlations, which could lead to suboptimal channel correlation modeling. It would be helpful if the authors could provide further evidence or analysis to support the model’s effectiveness in this regard.\n4. Table 1 and 2 lacks comparisons with more recent baselines, such as Sundial (ICML 2025).\n5. The paper also lacks an efficiency analysis compared with other TSFMs."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sl5CYJXx5m", "forum": "sl7s5KRnyh", "replyto": "sl7s5KRnyh", "signatures": ["ICLR.cc/2026/Conference/Submission4523/Reviewer_qqnm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4523/Reviewer_qqnm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966859831, "cdate": 1761966859831, "tmdate": 1762917422362, "mdate": 1762917422362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VisionTS++ adapts a Masked Auto-Encoder (MAE) pre-trained on ImageNet to universal time-series forecasting via three tweaks: (i) vision-based filtering that discards sequences whose normalised values fall outside the pixel range expected by the MAE, (ii) “colourised” multi-subfigure images that stack an arbitrary number of variates into one 3-channel picture, and (iii) some parallel reconstruction heads trained with quantile loss to deliver probabilistic forecasts without parametric assumptions. After continual pre-training on 231 B points (LOTSA) the model is evaluated on Monash (29 data-sets), LTSF, PF and GIFT-Eval benchmarks, beating recent TSFMs by 6–44 % MSE and ranking first on most tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) first paper that systematically closes the data-range, multivariate and probabilistic gaps when turning an off-the-shelf vision backbone into a competitive TSFM.\n(2) no new attention layers, no patch re-design; only lightweight heads and input/output converters—easy to reproduce.\n(3) SOTA on 4 widely used benchmarks (31/62 first places on LTSF, best nMAE on Monash, top CRPS on PF, 1st on GIFT-Eval) with both base and large variants.\n(4) removing filtering (−7 %), colourisation (−12 %) or multi-quantile heads (−10 %) consistently hurts, validating each gimmick."}, "weaknesses": {"value": "(1) the core idea (TS ➔ image ➔ MAE) is identical; improvements come from three engineering accessories rather than a new modelling principle.\n(2) no analysis of why pixel-range filtering or random RGB boundaries should be optimal; no guarantee that vision inductive biases align with temporal dynamics.\n(3) only forecasting; classification, anomaly detection or irregular sampling not tested.\n(4) no study on (i) #quantile heads h, (ii) alternative change-point or range-based filters, (iii) image size W or subfigure layout, (iv) datasets outside LOTSA/GIFT.\n(5) several typos (“appedix”, “quarntie”), some tables missing std-dev, captions duplicated."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TV3FHrCb2q", "forum": "sl7s5KRnyh", "replyto": "sl7s5KRnyh", "signatures": ["ICLR.cc/2026/Conference/Submission4523/Reviewer_PTdf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4523/Reviewer_PTdf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075846118, "cdate": 1762075846118, "tmdate": 1762917422149, "mdate": 1762917422149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}