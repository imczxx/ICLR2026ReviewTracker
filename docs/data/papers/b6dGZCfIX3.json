{"id": "b6dGZCfIX3", "number": 15342, "cdate": 1758250406182, "mdate": 1759897312778, "content": {"title": "Selective sparsity in Fourier Neural Operator Networks to accelerate Partial Differential Equation solving", "abstract": "Fourier Neural Operators (FNOs) have emerged as a powerful framework for learning solution operators of partial differential equations (PDEs). However, reliance on dense spectral representations leads to high computational cost and limited interpretability.  We propose a Spectrally-Sparsified Fourier Neural Operator (SS-FNO) that achieves state-of-the-art accuracy while substantially reducing spectral complexity. Our approach augments each FNO layer with a lightweight sparse selector with a diagonal gating mechanism whose implicit bias under stochastic gradient descent drives many frequency weights toward zero. This induces automatic pruning of uninformative Fourier modes, yielding a compact operator representation that is both efficient and interpretable. We validate SS-FNO on benchmark PDEs, including the Burgers’ equation, Darcy flow, and Navier–Stokes equations. Across all cases, SS-FNO matches or exceeds the predictive accuracy of standard FNOs while reducing the number of active frequency modes, reducing the memory footprint and the computation cost. By demonstrating that accurate operator learning does not require dense spectral representations, our work highlights spectral sparsity as a principled path toward scalable and interpretable neural operator models.", "tldr": "Using implicitly regularized Diagonal Networks, we enforce a spectral sparsity constraint in Fourier Neural Operators for accelerated PDE learning.", "keywords": ["Partial Differential Equations", "Fourier Neural Operator", "Diagonal Networks", "Sparsity"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba84b05c9aadd0599678a2fd18861eca767250fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Spectrally-Sparsified Fourier Neural Operator (SS-FNO), which addresses an important limitation of standard FNOs: their reliance on dense spectral representations that leads to high computational costs. The key innovation is the integration of lightweight sparse selectors using 2-layer Linear Diagonal Networks (LDNs) in the Fourier domain. These diagonal networks leverage an implicit bias toward sparsity during stochastic gradient descent training, automatically pruning uninformative frequency modes without requiring explicit regularization penalties. The authors validate SS-FNO on three benchmark PDEs Burgers' equation, Darcy flow, and Navier-Stokes equations demonstrating that it matches or exceeds standard FNO accuracy while substantially reducing the number of active frequency modes, parameters, and computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core idea here is actually pretty clever using diagonal linear networks to implicitly induce sparsity in the Fourier domain without explicit regularization is neat, and I appreciate that the authors grounded this in existing theory about implicit biases in gradient descent. The experimental setup is solid; they follow the original FNO protocol exactly, which makes the comparisons fair, and they test on three different types of PDEs (Burgers, Darcy, Navier-Stokes) across multiple resolutions. The results do show consistent parameter reduction, especially dramatic in the 3D Navier-Stokes case where they cut parameters by like 70%. I also liked Figure 4 showing which frequencies get selected that's genuinely useful for interpretability and could help us understand what the model is actually learning about these physical systems."}, "weaknesses": {"value": "Honestly, the actual computational gains are pretty underwhelming in several cases. Look at Table 1 for Burgers equation at the highest resolution, they barely save any FLOPs (139.43 vs 143.54, like 3% savings) despite using way fewer modes. The authors hand-wave this away by saying most computation is in the lifting layers, but that's kind of a big deal! If I'm a practitioner trying to speed up my PDE solver, I care about actual wall-clock time, not just the number of parameters. They don't report any training time comparisons either, which seems like a major omission given that they start with way more modes (1000 vs 32). How much longer does training take? How much more memory do I need? These are critical practical questions that aren't answered. Also, the accuracy improvements are marginal at best in many cases SS-FNO actually does slightly worse (look at Table 2, several cases show higher error for SS-FNO). So I'm left wondering: what's the actual benefit here? Slightly fewer parameters but similar or sometimes worse accuracy and unclear computational savings?\n\nThe paper also feels incomplete in several ways. The pruning procedure is barely explained; what threshold do they use for \"negligible\" coefficients? How do they fold β into R? Is there any fine-tuning after pruning? These details matter for reproducibility. There's also basically no analysis of *why* certain frequencies are selected or what this tells us about the PDEs. They claim interpretability as a major benefit, but beyond showing a plot of which frequencies survived, they don't actually interpret anything. Do these patterns make physical sense? Are they consistent across runs? Do they relate to known properties of these equations? Without this analysis, the interpretability claim feels hollow. And where are the comparisons to other sparsification methods? Just saying \"we don't use hard regularization\" isn't enough they should compare against L1 regularization, magnitude pruning, or other standard techniques to show their implicit approach is actually better. As it stands, I'm not convinced this is a significant enough improvement over standard FNO to warrant publication at a top venue."}, "questions": {"value": "Some questions I had in mind!\n\n1. **Can you provide actual wall-clock training and inference times?** You mention that SS-FNO starts with many more modes (like 1000 vs 32), which must significantly increase training time and memory usage. How long does it actually take to train SS-FNO compared to standard FNO? And more importantly, what are the actual inference speedups in wall-clock time, not just FLOPs? Because looking at Table 1, the FLOP savings seem pretty minimal in some cases, so I'm curious if there's any real-world speed benefit.\n\n2. **How exactly does the pruning work, and how sensitive are results to this choice?** The paper is really vague about the post-training pruning procedure. What specific threshold do you use to decide which coefficients are \"negligible\"? Is it a fixed value like |β| < 0.01, or a percentile-based cutoff? Do you prune layer-by-layer or globally? And critically, do you do any fine-tuning after pruning? I'm worried that a lot of the performance might depend on these unspecified details.\n\n3. **Why not compare against other sparsification methods?** You only mention ST-FNO as related work, but there are tons of ways to induce sparsityL1 regularization, magnitude pruning, lottery ticket methods, etc. Can you show that your implicit regularization approach actually outperforms these simpler baselines? Without this comparison, it's hard to evaluate whether the added complexity of the diagonal networks is actually necessary.\n\n4. **Can you actually interpret the learned sparsity patterns?** You claim interpretability as a major contribution, but Figure 4 just shows which frequencies survived without any analysis of *why*. Do the selected frequencies correspond to physically meaningful scales in these PDEs? For example, in Navier-Stokes with viscosity 10^-3, do the pruned high frequencies align with scales below the Kolmogorov microscale? Are the patterns consistent across different training runs, or do you get different sparsity structures each time?\n\n5. **Why does SS-FNO sometimes perform worse than standard FNO?** In several experiments (like Darcy flow in Table 2), SS-FNO actually has higher error than FNO. What's going on there? Is this just noise, or is there something about the sparse selection that's hurting performance in certain cases? And given that the accuracy is often similar or worse while training is presumably more expensive, what's the actual value proposition here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DxMM249cZk", "forum": "b6dGZCfIX3", "replyto": "b6dGZCfIX3", "signatures": ["ICLR.cc/2026/Conference/Submission15342/Reviewer_zAqe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15342/Reviewer_zAqe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761438470524, "cdate": 1761438470524, "tmdate": 1762925633405, "mdate": 1762925633405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SS-FNO (Spectrally Sparsified Fourier Neural Operator), a novel architecture for efficiently solving PDEs. SS-FNO employs a “sparse selector” mechanism that automatically prunes less important frequency components during training, induced by stochastic gradient descent. As a result, SS-FNO produces a more efficient & effective solution than FNO."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. SS-FNO requires fewer active frequency modes, reducing both memory footprint and computational cost.\n2. The sparse selector quantifies the importance of each frequency, allowing insight into the model’s mechanisms."}, "weaknesses": {"value": "1. The improvements are incremental, as they build on an established method (FNO) rather than a completely new paradigm. Pruning unimportant channels and frequencies is nothing but a mature technique in computer vision.\n2. The SS-FNO demonstrates limited improvement in both efficiency and effectiveness against FNO.\n3. Also, only FNO is selected as the baseline, making it hard to evaluate the proposed SS-FNO in the bigger picture."}, "questions": {"value": "See cons"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Im9FdIW0nh", "forum": "b6dGZCfIX3", "replyto": "b6dGZCfIX3", "signatures": ["ICLR.cc/2026/Conference/Submission15342/Reviewer_7tY6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15342/Reviewer_7tY6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565617128, "cdate": 1761565617128, "tmdate": 1762925632982, "mdate": 1762925632982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Spectrally-Sparsified Fourier Neural Operators (SS-FNOs), which learn to selectively maintain the most relevant Fourier modes during training through sparse selectors. The method achieves comparable accuracy to standard FNOs while significantly reducing the spectral complexity, leading to more compact and interpretable models. Benchmarks on several PDEs demonstrate that SS-FNOs maintain predictive performance with faster inference and improved interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors evaluate SS-FNO across multiple PDE benchmarks (Burgers’, Darcy flow, and Navier–Stokes), demonstrating comparable accuracy under substantial parameter reduction.\n2. The learned spectral weights offer interpretable insights into which frequency components are essential for each task.\n3. SS-FNO reduces parameters significantly, making FNOs more compact and potentially deployable for large-scale PDE problems."}, "weaknesses": {"value": "The paper lacks ablation studies that verify whether the learned spectral sparsity truly contributes to the model’s performance. It can compare with Low-frequency and random pruning baseline. Additionally, it would be insightful to compare the learned spectral weights with the true dominant modes from the underlying physics."}, "questions": {"value": "1. The paper reports FLOPs and parameter reductions but does not provide wall-clock inference time. It would strengthen the claim of “faster inference” to include empirical timing results.\n2. The proposed method maintains a larger modes during training. Does this lead to higher computational cost or memory usage compared to FNOs? Reporting training time (per epoch) would be informative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8dGgneeD8h", "forum": "b6dGZCfIX3", "replyto": "b6dGZCfIX3", "signatures": ["ICLR.cc/2026/Conference/Submission15342/Reviewer_qC5z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15342/Reviewer_qC5z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967337007, "cdate": 1761967337007, "tmdate": 1762925632542, "mdate": 1762925632542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Spectrally-Sparsified Fourier Neural Operator (SS-FNO), a modification of the FNO designed to induce sparsity in the spectral domain. The core of the method is the addition of a lightweight, 2-layer Linear Diagonal Network (LDN) to each Fourier layer, which acts as a \"sparse selector\" for the frequency modes. The authors leverage the known implicit bias of LDNs trained with stochastic gradient descent, which naturally encourages an L1-like sparsity penalty without requiring an explicit regularization term in the loss function. The workflow involves training an over-parameterized SS-FNO with a large number of modes, allowing the LDN to automatically drive uninformative mode coefficients to zero. This is followed by a post-training pruning step to remove these negligible modes, resulting in a compressed, efficient model for deployment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of the paper is its demonstrated ability to significantly compress the FNO architecture. The method achieves a substantial reduction in the number of active Fourier modes across all benchmarks (e.g., from 32 to as few as 9 in the 1D Burgers' case). This compression is achieved with a negligible impact on predictive accuracy. The experimental results show that the SS-FNO consistently \"matches or exceeds\" the accuracy of the standard FNO. The method for inducing sparsity is theoretically grounded. Instead of relying on a hand-tuned L1 penalty in the loss function, the authors cleverly leverage the implicit bias of gradient descent on Linear Diagonal Networks (LDNs)."}, "weaknesses": {"value": "1. The paper's contribution, while effective, feels incremental. The authors themselves state it is an \"incremental improvement of the FNO\". The core FNO architecture is unchanged, and the paper essentially presents a structured, implicit pruning technique.\n\n2. The experimental validation is weak. The only baseline used is the original FNO from 2021. This is insufficient. As a paper focused on FNO compression, it fails to compare against any other FNO compression or sparsity-inducing techniques. Prominent and highly relevant baselines are missing, such as tensor-factorized FNOs (e.g., TFNO, MG-TFNO), which are known to achieve massive compression and performance gains. Without this comparison, it is impossible to assess if this complex, implicit bias-based method is superior to more direct approaches like tensor decomposition or other pruning strategies.\n\n3. The paper's claims of computational reduction are not fully convincing.\n- Training Cost: The SS-FNO method is more expensive to train than the baseline, as it requires starting with an \"overparametrized setting\" (e.g., 1000 modes for Burgers' vs. 32 for FNO).\n- Inference Cost: The reported MFLOPs/GFLOPs reductions are modest in some high-resolution cases (e.g., 1D Burgers and 2D Darcy) and the paper admits this. More critically, the resulting sparsity patterns are highly irregular (e.g., mode counts of (26, 10), (26, 11), etc.), which are known to map very inefficiently to modern GPU hardware (e.g., tensor cores) and can introduce significant overhead from irregular memory access. The paper provides no wall-clock runtime comparison to verify if the theoretical FLOPs reduction translates to a real-world speedup."}, "questions": {"value": "1. Figure 4 shows that for the Burgers' equation, the SS-FNO automatically learns to keep far fewer modes (e.g., ~10-20) than the baseline FNO's fixed truncation of 32. This suggests the baseline FNO may be poorly tuned. What is the performance of the standard FNO if its truncation hyperparameter kmax is simply set to 20, or the other sparse counts discovered by SS-FNO?\n\n2. The learned sparse mode counts are highly irregular. FNO implementations typically use mode counts that are powers of 2 (or other hardware-friendly numbers) to maximize computational efficiency on GPUs. Do these irregular, sparse-mode tensors actually lead to a practical, wall-clock speedup during inference, or does the overhead from gather/scatter operations and inefficient tensor core usage negate the theoretical FLOPs reduction?\n\n3. The paper's core premise is the benefit of implicit regularization from LDNs over \"hard regularization constraints\". However, this claim is never substantiated. A critical missing ablation study would compare SS-FNO against a standard FNO trained with a simple, explicit L1 penalty on the mode weights. This would clarify if the complexity of the LDN selector is truly necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QnMrQRnwYt", "forum": "b6dGZCfIX3", "replyto": "b6dGZCfIX3", "signatures": ["ICLR.cc/2026/Conference/Submission15342/Reviewer_HhnP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15342/Reviewer_HhnP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972004464, "cdate": 1761972004464, "tmdate": 1762925631980, "mdate": 1762925631980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}