{"id": "XK2IC585o8", "number": 11229, "cdate": 1758193884785, "mdate": 1759897599810, "content": {"title": "Continual-Mega: A Large-scale Benchmark for Generalizable Continual Anomaly Detection", "abstract": "In this paper, we introduce a new benchmark for continual learning in anomaly detection, aimed at better reflecting real-world deployment scenarios. Our benchmark, Continual-MEGA, includes a large and diverse dataset that significantly expands existing evaluation settings by combining carefully curated existing datasets with our newly proposed dataset, ContinualAD. Beyond standard continual learning settings that increase the number of classes, we additionally propose a scenario that evaluates zero-shot generalization to unseen classes—those not encountered during continual adaptation, reflecting recent advances in continual zero-shot research and its highlighting practical significance. This setting introduces a new agenda for the anomaly detection field, and we conduct extensive evaluations of various existing anomaly detection algorithms designed for continual or zero-shot scenarios, as well as our proposed baseline methods. From our experiments, we derive three key findings: (1) existing methods exhibit significant limitations, particularly in pixel-level defect localization, (2) the proposed ContinualAD dataset is effective for the proposed benchmarking scenario, and (3) our baseline method suggests a promising direction for designing CLIP-based continual and generalizable frameworks through simple adaptation combined with feature synthesis.", "tldr": "We introduce a large-scale benchmark and few-shot continual learning scenario for industrial anomaly detection, enabling robust evaluation of both adaptation and zero-shot generalization across diverse and unseen classes.", "keywords": ["Anomaly Detection", "Continual Learning", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60c3b41451af3b382bcecb4332550b933dafbff9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new dataset for continual learning and anomaly detection. The authors combine it with existing datasets to create a diverse set of scenarios, forming a novel benchmark. One of these scenarios is specifically designed to evaluate zero-shot generalization to unseen classes. The benchmark is used to evaluate both existing methods from the literature and a novel baseline method proposed by the authors."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel dataset in the context of continual learning for anomaly detection, and the authors further demonstrate that it exhibits greater variance than existing datasets, potentially enabling better model generalization. As such, the dataset constitutes a valuable contribution. In addition, they propose a novel benchmark and introduce a new method that achieves performance comparable to existing approaches, with the exception of the zero-shot scenario, where it shows improved generalization."}, "weaknesses": {"value": "The paper is very poorly written, making it difficult to follow the explanations and appreciate the contributions it brings. In particular, Section 3, which describes the proposed method, is inadequately explained, making it challenging to understand how the method works and what conceptual innovations it offers compared to existing approaches. Additionally, no numerical implementation details are provided, and since the code is not shared, the method as presented is not reproducible, which constitutes my main concern. To address this, I would recommend including a figure in Section 3 to visually illustrate the method's workflow, as well as a clearer explanation of its key ideas in the main text with respect to related methods. For reproducibility, I suggest adding a detailed implementation section (perhaps in the appendix) and/or including a repository with the code in the supplementary materials.\nThe introduction also suffers from several issues. Many claims are not supported by references, including:\n- “It is widely applied in automated detection across diverse domains, including industrial and agricultural products, medical images, and other application areas”\n- “Due to the complexity and variety of real-world environments, anomaly detection models need to recognize a wide range of defects.”\n- “From a dataset perspective, the inherent difficulty in collecting large numbers of samples, particularly defective ones, makes AD more challenging than general vision tasks.”\n- “This limitation has motivated recent research to explore continual, zero-shot, and few-shot learning settings as strategies to overcome data scarcity.”,\n- This whole paragraph: “Due to these limitations, anomaly detection systems deployed in real-world environments often face sequentially arriving tasks, where new object categories or defect types emerge over time. In such scenarios, retraining models from scratch for every new task is computationally expensive and can be impractical. Furthermore, models trained in this way often suffer from catastrophic forgetting, where performance on previously learned tasks significantly degrades when new tasks are introduced. Continual learning aims to address these challenges by enabling the models to incrementally adapt to new data while preserving knowledge of previously seen tasks. However, in many practical cases, some tasks or defect types may not be observed during training at all, which requires models to generalize to the entirely unseen classes(i.e., tasks or defect types).”\nMoreover, the referencing format is incorrect, as most citations in the introduction should be in parentheses but are not. There is also a typo on line 089, where the term “continuous zero-shot learning” is used instead of the correct term “continual zero-shot learning,” which is a central theme to the paper.\nAdditional minor concerns include the lack of clarity in Figure 4, which is difficult to read, and the odd placement of the related work section, which disrupts the logical flow of the paper."}, "questions": {"value": "What are conceptual novelties introduced by the method, with respect to the literature? Also, please address the concerns in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nzSNkw4Tgq", "forum": "XK2IC585o8", "replyto": "XK2IC585o8", "signatures": ["ICLR.cc/2026/Conference/Submission11229/Reviewer_9vYt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11229/Reviewer_9vYt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761302383531, "cdate": 1761302383531, "tmdate": 1762922393492, "mdate": 1762922393492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Summary Response"}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful feedback. We will clarify the points below in the discussion and reflect them in the revision.\n\n- **Protocol clarity.** All methods—including MVFA—use the same continual stream and matched compute; we will state this explicitly in Implementation Details.\n- **Scenarios.** Scenario 1 evaluates in-stream continual behavior using all datasets. Scenario 2 adapts first, then evaluates zero-shot on held-out MVTec-AD and VisA. Scenario 3 repeats Scenario 2 with ContinualAD removed from the stream to reveal the effect of reduced diversity.\n- **Method (Section 3).** We will add two overview figures (adapter training; inference-time adapter mixture) with a clear flow description.\n- **New experiments.** We will provide learning-curve plots across increments that show current-task performance and the retention of earlier tasks as measured by FM. We will also conduct a component ablation study.\n- **Artifacts.** We will release evaluation code and checkpoints for Scenario 2 with 30 classes per task, and share the Continual-MEGA data via anonymous repositories."}}, "id": "P1yL1m0d6E", "forum": "XK2IC585o8", "replyto": "XK2IC585o8", "signatures": ["ICLR.cc/2026/Conference/Submission11229/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11229/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11229/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763223618307, "cdate": 1763223618307, "tmdate": 1763223618307, "mdate": 1763223618307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new anomaly detection (AD) dataset called Continual-AD. It combines this new dataset with a number of exiting ones, in order to define a benchmark, Continual-MEGA, for assessing continual anomaly detection. This continual AD benchmark assesses the ability of AD methods to 1) maintain performance at AD on existing objects when learning to perform AD for new objects, and 2) the ability to generalise AD to new categories in a zero-shot manner. A number of existing AD methods are assessed using the new benchmark, and a novel method for continual AD is introduced that is shown to perform well in comparison to the alternative methods that have been tested."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a new problem within the domain of Anomaly Detection, which is relevant to practical applications of AD in industry.\n\nThe proposed method produces strong performance."}, "weaknesses": {"value": "The paper is quite challenging to read, and could be better structured to present information in a more logical and more easily comprehensible way.\n\nTable 1 seems redundant given that the same information is also present in table 2.\n\nThe tables and figures use font sizes that are not clearly legible.\n\nThe caption for Fig.4 fails to fully describe what is shown, and the 7 colored backgrounds in this figure do not seem to correspond to the 4 sub-sets of tasks described in the corresponding main text."}, "questions": {"value": "Currently the paper defines two main testing scenarios. Would it not be possible to have a single testing method that assessed both continual and zero-shot learning performance?\n\nWhile it is unclear from the description given in the main text, it would seem from the headings in Table 4 that scenario 2 does assess both continual and zero-shot performance. If so, what is the point of scenario 1?\n\nThe headings in Table 5 suggest that just as many tasks have been used for training and testing as in Table 4. How is this possible if the Continual-AD dataset has been removed? What useful information does Table 5 show that has not already been shown in Table 4?\n\nThe section starting l.258 defines four metrics: ACC and FM both applied at image and pixel level. It is therefore confusing that the tables contain 6 metrics: the 4 defined and their averages. Why are the averages included? Does it make sense to average image-level and pixel-level metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "51DL7FhK15", "forum": "XK2IC585o8", "replyto": "XK2IC585o8", "signatures": ["ICLR.cc/2026/Conference/Submission11229/Reviewer_NFVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11229/Reviewer_NFVn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761303679307, "cdate": 1761303679307, "tmdate": 1762922393015, "mdate": 1762922393015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The empirical evidence largely supports the core claims. The benchmark is carefully assembled and the continual/CZSL protocols are well-motivated. The evaluation spans representative AD families with reasonable metrics (image AUROC, pixel AP, forgetting). My main reservation is fairness: the proposed baseline appears to use a much larger training budget than competing methods, and there is no component ablation to pinpoint what actually drives its gains. With clearer budgeting and ablations, this would be stronger."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Large, diverse benchmark with realistic continual and CZSL settings that better reflect deployment.\n2. Broad, carefully reported comparisons across method families with appropriate metrics (image AUROC, pixel AP, forgetting).\n3. Clear empirical takeaways on generalization vs. forgetting, highlighting where current methods break.\n4. A strong, reproducible CLIP-based baseline that others can extend; code/benchmark availability increases impact."}, "weaknesses": {"value": "1. Training-budget mismatch likely benefits the proposed baseline; needs a strictly matched compute comparison.\n2. No ablations to disentangle the effects of adapters, mixing strategy, and synthetic feature generation.\n3. Limited documentation of the new dataset in the main text (how anomalies are obtained, per-class stats, representative examples).\n4. Task split construction and order sensitivity are under-specified, making reproducibility and robustness hard to assess."}, "questions": {"value": "Can you report results under a strictly matched training budget (same steps, batch size, augmentation, early stopping) and show learning curves?\n\nPlease provide component ablations (removing adapters, mixing, or synthesis; varying adapter placement/size; prompting variants) with mean±std over seeds.\n\nHow exactly are anomalies in ContinualAD obtained (real vs. synthetic)? Can you add normal/anomaly examples, per-class counts, device diversity, and annotation protocol?\n\nHow were Base/New classes and task orders chosen, and how robust are results to permuting the order or changing increment sizes/held-out datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qe4an7xucY", "forum": "XK2IC585o8", "replyto": "XK2IC585o8", "signatures": ["ICLR.cc/2026/Conference/Submission11229/Reviewer_oSR8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11229/Reviewer_oSR8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995119941, "cdate": 1761995119941, "tmdate": 1762922392492, "mdate": 1762922392492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Continual-MEGA, a large-scale benchmark for continual anomaly detection, and proposes a CLIP-based baseline method (ADCT). The benchmark combines seven datasets and defines three scenarios: (1) continual AD, (2) continual → zero-shot generalization, and (3) dataset ablation. Experiments compare ADCT with supervised AD methods, zero-shot VLM-based methods, and continual-learning baselines. The authors conclude that existing methods “struggle” under large-scale continual settings and that ADCT performs more robustly."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A new large-scale benchmark that unifies multiple AD datasets and defines reproducible task streams.\n- Dataset and benchmark release (if completed) could be a useful resource for the community."}, "weaknesses": {"value": "- The paper claims that existing methods fail in continual AD, but Table 3 shows that MVFA (CVPR 2024 Spotlight), a non-continual zero-shot VLM-based method, performs competitively with the proposed ADCT. This contradicts the central claim that new continual-learning methods are required. If a zero-shot method performs as well as the proposed continual method, the necessity of the benchmark and ADCT is not established.\n- Evaluation in Scenario 2/3 artificially disadvantages MVFA, leading to invalid conclusions. In Table 4, the authors argue that MVFA “degrades under continual settings” when MVTec-AD and VisA are excluded from training. However, MVFA is a data-driven zero-shot method that relies on broad pretraining and domain coverage. Removing the datasets it normally uses does not demonstrate a failure of the method—it only reflects a benchmark-induced data restriction. The paper mistakenly interprets data ablation as methodological weakness, which is not a scientifically valid conclusion.\n- The proposed method does not significantly outperform strong baselines. Improvements over MVFA and other VLM-based AD models are small and not statistically validated. Since MVFA matches or exceeds ADCT in multiple metrics, the claimed superiority of the proposed method is unclear.\n- The necessity of continual learning setting is not justified. The paper assumes continual AD is required in real deployment. In practice, most industrial and medical AD systems use zero-shot generalization or few-shot adaptation, not lifelong incremental class learning. The experimental results themselves reinforce this: a zero-shot method works well, so the paper has not shown that continual learning is the right paradigm.\n- Unclear experimental protocol for MVFA. The paper never specifies whether MVFA was evaluated strictly zero-shot, whether any prompt tuning or finetuning was applied, or whether it saw the same task stream. Fairness of comparison is questionable."}, "questions": {"value": "- Was MVFA evaluated strictly zero-shot, or was any adaptation applied? If zero-shot, does its strong performance in Table 3 contradict the claim that continual learning is needed?\n- Can the authors show before vs. after continual results to justify the “continual improves zero-shot” claim?\n- What concrete industrial/medical deployment requires continual AD rather than batch retraining or few-shot updating?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TlJie6ob4O", "forum": "XK2IC585o8", "replyto": "XK2IC585o8", "signatures": ["ICLR.cc/2026/Conference/Submission11229/Reviewer_HbVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11229/Reviewer_HbVx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083670250, "cdate": 1762083670250, "tmdate": 1762922391923, "mdate": 1762922391923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}