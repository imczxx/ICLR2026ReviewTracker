{"id": "kPgLp47bJf", "number": 4812, "cdate": 1757770680807, "mdate": 1763614064330, "content": {"title": "Router Choice Matters: Rank-Aware Post-Training Quantization for MoE Models", "abstract": "Quantizing Mixture-of-Experts (MoE) language models is challenging since router errors cascade into expert selection and dominate accuracy loss. We study this effect and show that preserving router decisions of the selected experts yields the largest gains, with most errors arising as near-neighbor rank flips around the top-$k$ experts. Motivated by these observations, we present ExpertQuant, a training-free, calibration-only post-training quantization (PTQ) framework tailored to MoE. ExpertQuant combines (i) Expert-Aware Scale to accommodate heterogeneous activation ranges and two router-alignment objectives between quantized and full-precision models: (ii) Rank-Aware Jaccard Loss, which aligns the top-$k$ expert rank, and (iii) Gap Hinge Loss, which preserves score margins between consecutive experts to suppress rank flipping. Across OLMoE, DeepSeek-MoE, and Qwen3-MoE, ExpertQuant consistently reduces perplexity on C4 and WikiText-2 and improves zero-shot accuracy under W4A4 and W4A8, with similar trends at lower bit-widths. The framework requires no retraining, integrates seamlessly with existing MoE, and demonstrates that stabilizing router rankings during calibration is key to accurate low-bit MoE inference.", "tldr": "", "keywords": ["Mixture-of-Experts", "Post-training Quantization", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4135f171beacbf29ffa68305a66259d56070c0a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the post-training quantization (PTQ) problem of Mixture-of-Experts (MoE) models. The authors point out that the main challenge of MoE quantization does not lie in the experts themselves, but in the error amplification effect of the router—once the router selects the wrong experts, the entire forward propagation is affected, leading to severe accuracy degradation. Therefore, preserving the router’s top-k ranking consistency and margin is crucial for low-bit MoE inference.\nTo address this, the authors propose ExpertQuant, a training-free, calibration-set-based PTQ framework that consists of three components: ES, RAJ, and GH. These modules work collaboratively to enhance routing consistency and downstream accuracy. Experiments on three MoE models show that ExpertQuant achieves significant performance improvements under both W4A8 and W4A4 quantization settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses the insufficiently explored issue of router ranking stability, filling an important gap in router optimization for MoE quantization.\n\n2. The RAJ and GH losses function in a complementary manner, ensuring both correct expert selection and stable routing order.\n\n3. The experimental evaluation is comprehensive, comparing multiple models across various datasets in terms of accuracy, and also reporting the match score to reflect the routing consistency with the full-precision model. The authors have provided reproducible code"}, "weaknesses": {"value": "1. The method appears to be a derivative of EAQuant, integrating two strategies that focus solely on router selection. However, the essence of MoE lies in the dynamic utilization of experts — intuitively, differences in expert usage frequency are likely to affect quantization performance. Despite this, the paper’s analysis remains limited to router quantization optimization, without addressing the variation in quantization effects across different experts, making the approach rather narrow.\n\n2. The paper does not explain how the proposed methods influence the computation of quantization parameters. For instance, in MSE-based  approaches, it remains unclear how the introduced losses affect scale estimation. No concrete derivation or theoretical analysis is provided. From a code perspective, it seems that the authors have modified and optimized their work based on EAQuant. However, the authors have not conducted derivations regarding the feasibility of applying this approach to advanced methods such as AWQ or GPTQ. \n\n3. The evaluation is restricted to language-based MoE models and does not extend to multimodal settings. Given that the heterogeneity of multimodal data may further exacerbate routing instability, it remains uncertain whether the proposed method can generalize and account for modality-specific influences on routing behavior."}, "questions": {"value": "1. Could the paper provide a derivation showing how the proposed loss affects scale computation under standard min-max or MSE-based quantization, as well as theoretical integration with advanced methods such as GPTQ or AWQ? It would also be valuable to include ablation experiments comparing these variants.\n\n2. The hyperparameters — α = 0.6 for ES, β = 0.95 for RAJ, and a loss weight ratio of 1:1 — are determined empirically through experimental search. Could the authors offer theoretical insights explaining why this combination performs best? Furthermore, if applied to different models, would these parameters need to be re-tuned, thereby increasing cross-model adaptation costs?\n\n3. Could the paper clarify the boundary conditions of the RAJ method—for instance, whether RAJ remains effective when the number of experts is very large? Additionally, it would be helpful to justify the choice of RAJ similarity, and whether alternative measures such as cosine similarity or other metrics have been tested or considered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DxWY2MK7FR", "forum": "kPgLp47bJf", "replyto": "kPgLp47bJf", "signatures": ["ICLR.cc/2026/Conference/Submission4812/Reviewer_QsxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4812/Reviewer_QsxY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760583153800, "cdate": 1760583153800, "tmdate": 1762917590573, "mdate": 1762917590573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies PTQ for MoE and identifies the router (expert score/rank mismatch with full-precision model) as the main source of quantization-induced accuracy loss. The authors propose ExpertQuant, a PTQ framework optimized for expert rank maintaining. Experiments on OLMoE, DeepSeek-MoE, and Qwen3-MoE show consistent improvements in perplexity and zero-shot accuracy under low-bit settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's core strength lies in its empirical validation that router instability is the main bottleneck in MoE quantization. The specific insight that errors are dominated by \"near-neighbor rank flips\" is a new perspective.\n2. The framework's effectiveness is demonstrated across multiple state-of-the-art MoE models and a wide range of benchmarks. The paper includes thorough ablation studies and a \"Perfect Match\" routing experiment, which strongly support the central hypothesis."}, "weaknesses": {"value": "1. Requires careful tuning of hyperparameters (e.g. $\\beta, \\lambda$ ratios), which limits generalization and usability.\n2. Lacks theoretical explanation of why rank preservation leads to accuracy recovery.\n3. Lacks of efficiency experiments.\n4. Most of the ideas in this work have appeared in EAQuant, which weakens the contribution of this work to some extent."}, "questions": {"value": "1. Why use channel smoothing instead of rotation-based approaches (e.g. Hadamard transform)  to alleviate outlier problems? I think roation-based methods don't have the expert-conflict problem, which was seen as a challenge in this work(\"EXPERT-AWARE SCALE\" is proposed to solve this).\n2. The router is typically not quantized in MoE quantization. Figure 1 is confusing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VeXMaT7QJW", "forum": "kPgLp47bJf", "replyto": "kPgLp47bJf", "signatures": ["ICLR.cc/2026/Conference/Submission4812/Reviewer_tXdu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4812/Reviewer_tXdu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665943948, "cdate": 1761665943948, "tmdate": 1762917590015, "mdate": 1762917590015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study centers on the post-training quantization (PTQ) challenge specific to Mixture-of-Experts (MoE) models. The researchers highlight that the primary difficulty in quantizing MoE models is not the experts themselves, but rather the router’s tendency to amplify errors. If the router makes incorrect expert selections, it disrupts the entire forward propagation process, resulting in substantial drops in model accuracy. Thus, maintaining the router’s top-k ranking consistency and the margin between rankings is essential for enabling low-bit inference of MoE models. To tackle this issue, the authors introduce ExpertQuant—a PTQ framework that requires no additional training and relies on a calibration dataset. This framework comprises three key components: ES, RAJ, and GH. These modules operate in tandem to improve both the consistency of routing decisions and the model’s downstream task accuracy. Testing across three distinct MoE models demonstrates that ExpertQuant delivers notable performance gains when operating under both W4A8 and W4A4 quantization configurations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a critical yet underexplored bottleneck in MoE model quantization: router-induced error cascading rather than expert-level inefficiencies. Through controlled studies (e.g., Figure 1, which shows preserving the router in full precision yields ~1.50% higher accuracy than unquantizing other modules like attention projections) and confusion matrix analyses (Figures 2, 9, 10), it empirically demonstrates that quantization errors primarily manifest as \"near-neighbor rank flips\" around top-k experts. \n2. The RAJ and GH losses operate in a complementary fashion, working together to guarantee both the accuracy of expert selection and the stability of routing order for MoE models. The paper’s experimental validation is thorough: it assesses performance across multiple MoE architectures, evaluates accuracy on diverse datasets, and incorporates the Match Score as a metric to quantify how well the quantized router’s decisions align with those of the full-precision model. Additionally, the authors have made reproducible code available to support the transparency and replicability of their findings."}, "weaknesses": {"value": "1. The paper focuses heavily on optimizing router quantization (via RAJ and GH losses) but overlooks a core attribute of MoE architectures: variations in expert usage frequency and their impact on quantization performance. As the critique notes, MoE’s essence lies in dynamic expert utilization, yet the study provides no analysis of how differing expert usage patterns (e.g., frequently vs. rarely activated experts) affect quantization efficacy. It only addresses router-related errors and does not explore or mitigate disparities in quantization effects across individual experts—leaving a gap in its coverage of MoE-specific challenges and narrowing the method’s comprehensiveness.\n2. The paper’s evaluation is limited to language-only MoE models (OLMoE, DeepSeek-MoE, Qwen3-MoE) and does not extend to multimodal MoE settings. Given that multimodal data heterogeneity could exacerbate routing instability, the method’s ability to generalize to non-language modalities remains unproven. \n3. The boundary conditions of RAJ (e.g., efficacy with extremely large expert counts) and the justification for choosing RAJ similarity over alternatives (e.g., cosine similarity) are not clarified or tested, limiting confidence in its robustness across diverse scenarios.\nQuestions"}, "questions": {"value": "1. MoE architectures are increasingly scaling to thousands of experts (e.g., larger Switch Transformer variants). Could you clarify: (1) Whether RAJ and GH losses maintain their effectiveness when the number of experts is drastically increased (e.g., 1k+ experts), where near-neighbor rank flips might become more frequent or impactful? (2) Does the Expert-Aware Scale (ES) incur additional computational overhead or require hyperparameter retuning when adapting to such large expert pools, and if so, how is this trade-off managed?\n2. The paper proposes RAJ and GH losses to stabilize routing but provides no theoretical derivation explaining how these losses influence the core quantization parameters (e.g., scale and zero-point) in min-max or MSE-based PTQ. For example: (1) How do RAJ’s rank-alignment objectives and GH’s margin-preservation constraints modify the calculation of per-expert/per-channel scales in ES? (2) Can you provide mathematical proof or analysis showing that these losses minimize quantization error propagation in the router, rather than relying solely on empirical ablation results? This would strengthen the rationale for your method’s design beyond experimental observation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qWQ96cOdRR", "forum": "kPgLp47bJf", "replyto": "kPgLp47bJf", "signatures": ["ICLR.cc/2026/Conference/Submission4812/Reviewer_XAzX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4812/Reviewer_XAzX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873231816, "cdate": 1761873231816, "tmdate": 1762917589390, "mdate": 1762917589390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes that router ranking instability caused by quantization is the main reason for the performance degradation of MoE. It combines expert-aware scaling and rank-aware loss to achieve training-independent MoE quantization. This significantly reduces quantization error while maintaining high inference accuracy across several mainstream MoE models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies and systematically analyzes the router sensitivity problem in MoE quantization.\n2. Discrepancies in input distribution among experts were identified, and Expert-Aware Scale (ES) was proposed to address this issue.\n3. All calibration targets (ES, RAJ, GH) can be completed in the post-training phase without backpropagation or additional fine-tuning."}, "weaknesses": {"value": "1. The experimental design is incomplete, and the core arguments are not sufficiently verified. The paper claims that \"router decision-making is a key factor in the quantization degradation of MoE,\" but the experiment in Figure 1 only shows a partial comparison of q/k/v/router, without clearly stating whether shared and non-shared experts are quantified, and the experimental section also does not fully explain.\n2. The performance gain of routers is limited, and the conclusion is overstated. As shown in Figure 1, maintaining full precision in the router only brings about a +1% improvement in accuracy, while expert quantization errors typically cause a 5–10% decrease.\n3. Lack of comprehensive quantization and interactive ablation analysis: In actual deployments, when both the router and experts perform quantization simultaneously, the benefits of improved router consistency are often overshadowed by expert errors.\n4. Expert-Aware Scale (ES) increases complexity and raises questions about its engineering feasibility. ES introduces an independent scale for each expert and each channel. Although mathematically equivalent to reparameterization, in practice, additional parameters need to be stored, and the inference path involves an extra activation scaling. It cannot be integrated into the static weight graph like AWQ. The paper does not discuss these additional costs, nor does it provide an assessment of the inference overhead."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RBGf8vjNWh", "forum": "kPgLp47bJf", "replyto": "kPgLp47bJf", "signatures": ["ICLR.cc/2026/Conference/Submission4812/Reviewer_5sFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4812/Reviewer_5sFX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894654995, "cdate": 1761894654995, "tmdate": 1762917588937, "mdate": 1762917588937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}