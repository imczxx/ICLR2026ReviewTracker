{"id": "E5mL07Fbq8", "number": 5698, "cdate": 1757927658301, "mdate": 1759897959915, "content": {"title": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy", "abstract": "Large Language Models (LLMs) are being increasingly deployed in real-world applications, but they remain susceptible to hallucinations, which produce fluent yet incorrect responses and lead to erroneous decision-making. Uncertainty estimation is a feasible approach to detect such hallucinations. For example, semantic entropy estimates uncertainty by considering the semantic diversity across multiple sampled responses, thus identifying hallucinations. However, semantic entropy relies on post-softmax probabilities and fails to capture the model's inherent uncertainty, causing it to be ineffective in certain scenarios. To address this issue, we introduce Semantic Energy, a novel uncertainty estimation framework that leverages the inherent confidence of LLMs by operating directly on logits of penultimate layer. By combining semantic clustering with a Boltzmann-inspired energy distribution, our method better captures uncertainty in cases where semantic entropy fails. Experiments across multiple benchmarks show that Semantic Energy significantly improves hallucination detection and uncertainty estimation, offering more reliable signals for downstream applications such as hallucination detection.", "tldr": "", "keywords": ["hallucination", "semantic entropy"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dad4e502aadb97ef529e3675ef5e4a979f772ace.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes semantic energy, an alternative to semantic entropy for uncertainty quantification in open-response QA.\n\nI think the idea shows promise, but the paper needs more work, both in experimentation and exposition. For exposition, it is not clear exactly how their estimator is calculated. For experimentation, the results are only on 2 datasets with 2 models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* UQ for LLMs is an area of growing interest\n* The authors show improved performance on two datasets, using two models"}, "weaknesses": {"value": "* Lack of experiments: The paper only evaluates on two models, using two LLMs, and only compares against (one variant of) semantic entropy. I understand that not all researchers have the same access to resources... but this isn’t nearly enough to evaluate whether semantic energy outperforms semantic entropy. Further, I disagree that it is “sufficient” to compare just with semantic entropy, particularly with so few experiments — it is hard to know whether these are cases where semantic entropy does particularly poorly vs logit-based approaches such as mean token entropy, sequence probability, or perplexity. See under “questions” for more.\n* Lack of experimental details:\n    * There are a number of implementations of semantic entropy, some of which use logits, others use sample counts. Which is being used?\n    * How many samples are used for clustering? What method is used for clustering (deberta, LLM-as-a-judge, something else?)\n    * The authors mention that in some datasets, half of questions with zero estimated semantic entropy are wrong. I expected to see some numbers for the datasets in question.\n    * In the single-cluster results, what is the accuracy? \n    * For the single-cluster results, how are you calculating FPR@TPR=0.95? I assume you are either including all observations, no observations, or randomly sampling 95% of the data to get a TPR of 95%. But in either of these scenarios, I am surprised to see a FPR@TPR=0.95 of exactly 95%. \n\n\n* Clarity: The section on the estimator is not very clear:\n    * I wasn’t sure what the part about the uncertainty in theta refers to. We are working with a non-Bayesian model, so there is no uncertainty about the model parameters.\n    * $Z_{\\theta, t}$ is not defined, and it is not clear what the subset $\\mathcal{V}$ is (or, how it relates to uncertainty about the model parameters)\n    * Eq 19 doesn’t make sense — it only considers the kth cluster, and doesn’t indicate how we combine multiple clusters. As a result, I am unsure as to what the authors’ estimator actually is, and how it combines the energies/log likelihoods for the individual clusters."}, "questions": {"value": "* I would like to see the authors better describe their estimator -- this would allow me to better evaluate the paper, and I could revisit my review with greater understanding of the methodology\n* In terms of experiments, it would be nice to see a more thorough consideration of when semantic energy works, and why semantic energy works. For the former, the authors really need more datasets and models, plus more comparison methods. Some quesitons I would like to see answered are:\n    * does semantic energy work well in cases where other logit-based methods like perplexity or mean token entropy work well? \n    * as we increase the number of samples, do semantic entropy and semantic energy converge to the same value? (I think we would expect this).\n    * Related to the above: The authors argue that the reason for energy working better is due to logits carrying more information about uncertainty than the probabilities. It is not clear that to me that this is the limiting factor here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0DAHAxyUKw", "forum": "E5mL07Fbq8", "replyto": "E5mL07Fbq8", "signatures": ["ICLR.cc/2026/Conference/Submission5698/Reviewer_DB7j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5698/Reviewer_DB7j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761005856984, "cdate": 1761005856984, "tmdate": 1762918202674, "mdate": 1762918202674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Semantic Energy, a new uncertainty estimation method for Large Language Models (LLMs). The method aims to overcome the limitations of Semantic Entropy (Farquhar et al., 2024), which relies on post-softmax probabilities and therefore cannot capture the model’s inherent uncertainty. Instead, the authors propose operating directly on the logits of the penultimate layer and defining an energy-based uncertainty score inspired by the Boltzmann distribution. By integrating semantic clustering with this logit-based energy formulation, the approach intends to capture both semantic and epistemic uncertainty. Experiments on Qwen3-8B and ERNIE-21B-A3B models, using TriviaQA and CSQA, show consistent improvements in AUROC and AUPR for hallucination detection. The paper also presents ablations (e.g., single-cluster cases, semantic vs. non-semantic variants) and extends the idea to a Fermi-Dirac formulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear and well connected to a real shortcoming of Semantic Entropy.\n2. The derivation of the energy-based formulation is conceptually elegant and mathematically consistent.\n3. The paper is very well written and easy to follow.\n4. Experiments show consistent improvements over a strong baseline (Semantic Entropy), across two models and two datasets.\n5. The ablations are thorough and empirically convincing.\n6. The Fermi-Dirac extension, though exploratory, demonstrates the authors’ awareness of alternative formulations of the method."}, "weaknesses": {"value": "1. The empirical scope is relatively narrow: only two models and two QA datasets are tested. While both are multilingual, the generalization to other domains (e.g., reasoning, dialogue, factuality) remains unclear.\n2. Comparisons are limited to Semantic Entropy; other strong baselines such as Logit-based OOD detectors, Semantic Uncertainty (Kuhn et al., 2024), Sample Consistency (Lyu et al., 2025), IDK-token (Cohen et al. 2024), or Self-Reflective Uncertainties (Kirchhof et al., 2025) are missing.\n3. Computational cost is not discussed — there's no significant computational gap comparing to semantic entropy of course, however, the proposed approach still requires multiple response samplings and semantic clustering, which can be expensive for large-scale applications.\n4. The “think mode” experiment is interesting but underexplored — it would be valuable to clarify how the model’s internal reasoning sequence affects the uncertainty estimation beyond empirical observation."}, "questions": {"value": "1. How sensitive is the method to the sampling temperature and the number of samples? Did you try higher temperatures or fewer samples to see how stable Semantic Energy remains?\n2. Did you compare your approach with LogToKU (Ma et al., 2025) directly on the same datasets? The methods seem conceptually related.\n3. Can the Fermi-Dirac variant be quantitatively compared to the Boltzmann version (e.g., on AUROC/AUPR) to justify its inclusion beyond theoretical curiosity?\n4. Did you consider evaluating on datasets that measure long-form generation (e.g., HaluBench or FEVER) to assess scalability and context length sensitivity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eXoOPbBe3H", "forum": "E5mL07Fbq8", "replyto": "E5mL07Fbq8", "signatures": ["ICLR.cc/2026/Conference/Submission5698/Reviewer_Podw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5698/Reviewer_Podw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746202632, "cdate": 1761746202632, "tmdate": 1762918202384, "mdate": 1762918202384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose combining the core idea of \"semantic entropy\" (computing entropy after clustering different generations by their semantic meaning) with the idea of using \"energy\" (i.e. using logits before they're normalized into logprobs with a softmax) as a measure of uncertainty in natural-language generation QA tasks.\nAuthors claim empirical improvements in UQ performance of Semantic Energy over Semantic Entropy."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1. I find the core idea (as described in the summary) interesting (but incremental) and I think it's worthwhile effort to evaluate it."}, "weaknesses": {"value": "W1. Incremental contribution.  \nThe core idea is incremental, given existence of Semantic Entropy (SE) (Kuhn et al.), and LogTokU (Ma et al., 2025), which uses logits as UQ-score in NLG settings (albeit the evaluation in that paper is also rather lacking).  \n\nW2. Lacking appropriate empirical comparison to prior work.  \n\nW2a. More methods.  \nThe key results in Tables 1 and 2 should incorporate at the very least LogTokU, this key ablation cannot be conducted just in the form of Figure 2. \nI'd particularly like to see a comparison to KLE (Nikitin et al., 2024) which based on other papers in my batch seems to be one of the SOTA extensions of SE.\n\nW2b. More models.\n\nW2c. More datasets.\n\nW2d. Confidence intervals. \n\nW2e. There are codebases available which would make the process rather straightforward, given the method proposed is rather simple to implement once the Semantic-Clustering is computed: https://github.com/AlexanderVNikitin/kernel-language-entropy\n\n\nW3. Vague (and at times incorrect) discussion/description disguised in technical terminology.  \n\nW3a. e.g. \"fails to capture the model's inherent uncertainty\", \"leverages the inherent confidence\".  \n\nW3b. L198-204: The discussion of aleatoric and epistemic uncertainty and assumed causation in (1) \"limited exposure\" is not supported by either citations or empirical evidence. One could argue that SE captures a combination of both aleatoric and epistemic uncertainty - how would authors respond to this?  \n\nW3c. L224: \"the error of the approximated partition function\" - is there a \"true value of the partition function\"? How is it defined? \n\nW3d. Section 3.2.\n\nW3da. I understand that $\\mathcal{V}$ is the token dictionary. I don't understand what is $\\mathbb{V}$ - could the authors explain? It seems to me that contemporary tokenizers can tokenize any sequence of strings we would like to model.\n\nW3db. eq 13 - authors surely don't mean a joint probability distribution over the token x_t^i and the parameters of the LLM.\n\nW3dc. To my best judgement, the mathematics of eqs 14-16 is simply incorrect. I don't see any way how it could be derived. Some of it can be fixed, but I think the entire discussion of Sec 3.2.2 is superfluous and trying to add mathematical rigour to motivation that is driven by analogy that is not strictly speaking riguorous. Authors can find similar exposition of the motivation (but in my opinion correct) e.g. in the by now \"reference citation\" of Liu et al., 2020, which authors cite. Ultimately, eq 19 doesn't really depend on anything that came before and that's the UQ-score/estimator proposed and used in Section 4."}, "questions": {"value": "See Weaknesses.\n\nTypos:  \n- L074 \"semantic sampling\" -> \"semantic clustering\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2XOOFeIVVe", "forum": "E5mL07Fbq8", "replyto": "E5mL07Fbq8", "signatures": ["ICLR.cc/2026/Conference/Submission5698/Reviewer_FSzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5698/Reviewer_FSzi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762466121463, "cdate": 1762466121463, "tmdate": 1762918202069, "mdate": 1762918202069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}