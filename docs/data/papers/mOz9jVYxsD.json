{"id": "mOz9jVYxsD", "number": 8268, "cdate": 1758076914237, "mdate": 1763664249165, "content": {"title": "Context Tokens are Anchors: Understanding the Repetition Curse in Diffusion MLLMs from an Information Flow Perspective", "abstract": "Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the Repeat Curse. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model’s growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present CoTA, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code will be made available.", "tldr": "Understanding and Mitigating the “Repetition Curse” from the Perspective of Information Flow.", "keywords": ["Diffusion Multimodal Large Language Models; Information flow"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ecc6c09052f8f5dfcd11c7acd1f86bf18526f21e.pdf", "supplementary_material": "/attachment/cc6e71458ea336590fb726a62f91387b8b62cead.zip"}, "replies": [{"content": {"summary": {"value": "The paper successfully identifies and names the phenomenon of cache-induced repetition in dMLLMs during inference, referred to as the “Repeat Curse,” demonstrating a strong awareness of the problem setting.\nBy examining information entropy and attention flow, the paper reveals the mechanism behind repetitive generation, showcasing solid theoretical explanatory power. The paper proposes the insight that “Context Tokens are Anchors,” uncovering how context tokens act as anchors across layers and stabilize the decoding process. The proposed CoTA module (CTAE + CTEV) is a plug-and-play solution that requires no additional training, making it highly practical for real-world deployment. The work not only validates the mitigation effect on repetition but also demonstrates strong generalization across different tasks and compatibility with existing cache strategies."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "From the full workflow of \"Problem Definition → Mechanism Analysis → Method Design → Experimental Validation → Detail Supplementation\", this work features a complete structure, rigorous logic, clear innovation, and strong practicality. Its core contributions—revealing the repetition mechanism of dMLLMs + caching and proposing CoTA, a lightweight deduplication method—hold both academic and application value."}, "weaknesses": {"value": "1. First of all, this should not be the first time that the “repeat curse” problem has been raised. It would be better to include some citations and add an analysis of previous methods that address the repeat curse in the Related Work section. In particular, please highlight solutions proposed for LLMs and MLLMs. This can help readers understand that your approach is not just a direct adaptation from LLMs to dMLLMs, but rather an innovation tailored specifically to the structure of dMLLMs.\n2. You could also consider adding another attention map after applying CoTA as a supplement."}, "questions": {"value": "The types of repetition I've seen roughly include local word repetition, sentence pattern looping, and semantic looping. This article seems to only observe local word repetition. Is it because dLLMs only have this type of repetition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9qZxKb2EXo", "forum": "mOz9jVYxsD", "replyto": "mOz9jVYxsD", "signatures": ["ICLR.cc/2026/Conference/Submission8268/Reviewer_bFZ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8268/Reviewer_bFZ4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760603653789, "cdate": 1760603653789, "tmdate": 1762920206696, "mdate": 1762920206696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies repetition in diffusion-based MLLMs with cache (“Repeat Curse”) and analyzes it via information flow. The authors observe that (i) context tokens serve as “anchors” that aggregate information and guide predictions; (ii) under normal decoding, the entropy of context tokens decreases and converges across deeper layers; (iii) with cache enabled, attention becomes randomized and context-token entropy fails to converge, which correlates with repetition. Building on these observations, they propose CoTA, consisting of Context-Token Attention Enhancement (CTAE) and Context-Token Entropy-guided Voting (CTEV). Experiments on several multimodal benchmarks and a 500-image captioning evaluation show reduced repetition and preserved task performance when using cache."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear empirical characterization of repetition with interpretable metrics and visualizations.\n2. A training-free, plug-in style method (CoTA) that targets the identified failure mode and is simple to implement.\n3. Results suggest repetition can be mitigated without large performance drops."}, "weaknesses": {"value": "1. Mechanism not adequately explained.\nThe paper does not convincingly explain why cache introduction leads to non-convergence of context-token entropy in deeper layers. Current evidence is largely correlational (entropy trajectories + repetition metrics) without ablations that isolate which cache component (e.g., recomputation interval, similarity thresholds, reuse policy) drives the phenomenon. A causal link is missing.\n2. Lack of generality (single model + single cache).\nAll main claims—the “Repeat Curse,” the information-flow analysis, and CoTA efficacy—are validated on a single model (LLaDA-V 8B) and a single cache method (dLLM-Cache). This undermines the generality of the conclusions. Although the conclusion section acknowledges this, the paper still reads as over-claimed relative to its evidence base.\n3. CTEV design choices under-justified.\nCTEV aggregates cumulative entropy from layers 26–30. The choice appears ad-hoc and may be tightly coupled to a 32-layer backbone. No sensitivity analysis (different windows, learned weights, different depths) is reported, so robustness remains unclear.\n4. Presentation/editing issues.\nPage 2, paragraph 2: figure references are incorrect/mixed, making the early narrative confusing."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eeopqy8HOz", "forum": "mOz9jVYxsD", "replyto": "mOz9jVYxsD", "signatures": ["ICLR.cc/2026/Conference/Submission8268/Reviewer_XZLN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8268/Reviewer_XZLN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974792746, "cdate": 1761974792746, "tmdate": 1762920206353, "mdate": 1762920206353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the repetition curse that arises when caching is applied to accelerate inference in multimodal diffusion large language models (dMLLMs). While caching improves efficiency, it leads to repetitive text generation. The authors propose **CoTA (Context Tokens are Anchors)** to mitigate this issue by preserving attention coefficients of context tokens (typically 3 neighboring tokens) and downweighting attention for keys that are far from the current position. Context tokens are heuristically chosen based on proximity to the query position. Experiments on 500 randomly sampled images from the COCO dataset for image captioning demonstrate that CoTA alleviates repetition while maintaining generated caption quality."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper makes an interesting observation that enabling caching in multimodal diffusion large language models can distort attention coefficients, leading to degraded generation quality.  \nTo address this, the authors propose an attention reweighting mechanism that enhances the attention coefficients of a few key context tokens (typically 3), termed **context anchors**, to preserve semantic consistency while maintaining the efficiency benefits of caching."}, "weaknesses": {"value": "1. **Line 82:** The text refers to Figure 2.c, but there is no subfigure (c) in Figure 2.  \n\n2. **Generalization:** Is the proposed approach generic, or is it specific to LLaDA? Can it also be applied to other diffusion large-language models?  \n\n3. **Line 139:** The authors mention prior studies showing the existence of certain anchor tokens in autoregressive models. However, the related works section does not discuss anchors in diffusion language models, such as ADLM (https://openreview.net/pdf?id=E8adS5srds). It is important to include related work on anchoring in diffusion models since this paper primarily focuses on context tokens as anchors in diffusion language models, which was first proposed in ADLM.   \n\n4. **Figure 3:** The font is too small and difficult to read.  \n\n5. **Line 280 Typo:** “Equation 5 6” → “Equations 5 and 6.”  \n\n6. **Figure 4 Incorrect labeling:** Subfigure “(a), (a)” → “(a), (b).”  \n\n7. **Applicability of Attention Enhancement:** The main idea of attention enhancement is relevant only when caching changes the original attention pattern. Recent models such as BD3LM can better handle caching, so this problem may not arise. Can the authors verify that their observations hold for such models?  \n\n8. **Equation 7:** Why is the entropy normalized by $\\log(V)$?  \n\n9. **Ablation Study (Table 4):** Increasing the number of context tokens from 3 to 5 drastically reduces performance, which seems counterintuitive and contradicts the principle of attention mechanisms that capture long-range dependencies. Could the authors explain this behavior?"}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X56ExzT360", "forum": "mOz9jVYxsD", "replyto": "mOz9jVYxsD", "signatures": ["ICLR.cc/2026/Conference/Submission8268/Reviewer_bzXt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8268/Reviewer_bzXt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031320850, "cdate": 1762031320850, "tmdate": 1762920206018, "mdate": 1762920206018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies the \"Repeat Curse,\" a text repetition issue caused by caching in diffusion-based Multimodal Large Language Models (dMLLMs). The authors analyze this from an information flow perspective, finding that caching disrupts the role of \"context tokens\" as information anchors and prevents their entropy from converging in deeper layers. To solve this, they propose CoTA, a training-free method with two components: Context-token Attention Enhancement (CTAE) to restore information flow, and Context-token Entropy-guided Voting (CTEV) to penalize uncertain predictions during decoding. Experiments show CoTA effectively reduces repetition and restores model performance on various multimodal benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses a critical and practical issue (inference acceleration side-effects) in the emerging field of dMLLMs.\n2. Provides a technically sound analysis of the problem's root cause using information flow and entropy, which is more insightful than a purely phenomenological study.\n3. Proposes a novel, training-free, and plug-and-play method (CoTA) that is shown to be highly effective.\n4. The experiments are well-designed, featuring custom metrics to quantify the problem, extensive benchmark validation, and thorough ablation studies."}, "weaknesses": {"value": "1. The study's conclusions are drawn from a single dMLLM architecture (LLaDA-V 8B), making it unclear if the \"Repeat Curse\" is a universal issue.\n2. The analysis is tied to a single caching method (dLLM-Cache). The findings may not hold for other acceleration strategies.\n3. Defining \"context tokens\" as a fixed-size local window feels empirical. The paper lacks a deeper theoretical justification for why these specific tokens act as anchors.\n4. The paper does not investigate whether \"anchor\" tokens or repeating tokens possess specific linguistic properties (e.g., function words vs. content words).\n5. The experiments do not compare CoTA against classic, simpler methods for controlling repetition, such as n-gram penalties or blocking.\nThe CTAE component is conceptually similar to attention biasing methods. A discussion of these related techniques is missing."}, "questions": {"value": "1. How do you expect your findings to generalize to other dMLLM architectures (e.g., JanusFlow) and different model sizes?\n2. Would the \"Repeat Curse\" still occur with alternative caching strategies, perhaps those with more adaptive update policies?\n3. Why a fixed local window for \"context tokens\"? Have you investigated if the \"anchor\" role is dynamic or could involve non-adjacent tokens?\n4. Did your analysis reveal if certain types of words (e.g., common function words like \"the\") are more prone to exhibiting high entropy and causing repetition?\n5. What was the rationale for choosing layers 26-30 for the entropy calculation in CTEV? How sensitive is the performance to this specific layer range?\n6. Could you comment on how CoTA compares to simpler decoding-time baselines like n-gram blocking, particularly regarding the trade-off between repetition control and text quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LsiZM1Q1kR", "forum": "mOz9jVYxsD", "replyto": "mOz9jVYxsD", "signatures": ["ICLR.cc/2026/Conference/Submission8268/Reviewer_qq7Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8268/Reviewer_qq7Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180706944, "cdate": 1762180706944, "tmdate": 1762920204651, "mdate": 1762920204651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}