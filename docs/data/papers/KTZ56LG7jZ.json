{"id": "KTZ56LG7jZ", "number": 10859, "cdate": 1758183565577, "mdate": 1759897624287, "content": {"title": "PILOT-Bench: Probabilistic Interaction for LLM Operations in Tool-driven Scenarios", "abstract": "Large language models (LLMs) have shown capabilities in tool utilization and workflow orchestration, yet existing evaluation benchmarks focus on idealized scenarios with deterministic tool behaviors and precise guidance, which may not capture real-world deployment complexities. We introduce PILOT-Bench, a benchmark that evaluates LLM workflow execution under simulated realistic conditions of guidance quality variability and tool execution uncertainty. The benchmark incorporates three key aspects: (1) modeling of probabilistic tool behaviors through parameterized error models that may reflect real-world API failure patterns, (2) evaluation of LLMs' workflow execution capabilities in environments where both guidance and tool behavior approximate practical scenarios, and (3) provision of both optimal and perturbed workflow guidance to assess robustness to instruction quality variations. Our construction pipeline generates 5,040 tasks from a tool library of 30 APIs. The evaluation conducted across ten widely used large language models under conditions of probabilistic tool failures and varying instruction quality reveals notable performance differences. Specifically, optimal workflow prompts achieve an average success rate of 71.8\\%, Chain-of-Thought prompts yield an average success rate of 59.4\\%, and flawed workflow prompts result in an average success rate of 61.5\\%. Our benchmark is available at https://github.com/PilotBenchAnonymous/PilotBench.", "tldr": "We propose PILOT-Bench, a benchmark that evaluates LLM workflow execution under simulated realistic conditions with probabilistic tool failures and variable instruction quality.", "keywords": ["Workflow execution", "LLM robustness", "Probabilistic Tool Behavior"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64b8d9f11cc585be36bad852c22ceea7fa708605.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark PILOT-Bench, designed to evaluate the workflow execution capability of LLMs in tool-driven scenarios that more closely approximate real-world complexity. The authors argue that existing benchmarks primarily focus on idealized settings where tool behavior is deterministic and guidance is precise. To address this, PILOT-Bench introduces two core real-world challenges:\n\n1.  Tool Execution Uncertainty: The benchmark simulates probabilistic tool behavior by parameterizing error models, reflecting failure modes that APIs may exhibit in the real world.\n2.  Variability in Guidance Quality: The benchmark provides workflow instructions of varying quality, including \"Optimal Workflow\" prompts and \"Defective Workflow\" prompts, to assess the model's robustness to changes in instruction quality.\n\nThe benchmark comprises a tool library of 30 APIs, from which 5,040 tasks are generated. A significant contribution is that the Optimal Workflows are generated using a MDP framework that considers tool dependencies and stochastic behavior, with policy optimization performed using PPO. The Defective Workflows are created by systematically injecting seven categories of controlled perturbations into the optimal workflows."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper points out the limitations of existing tool-use benchmarks. By focusing on probabilistic tool failures and variability in instruction quality, the benchmark addresses two core challenges for LLM agents in practical deployment.\n2.  The experimental results are insightful. For instance, the high robustness exhibited by some state-of-the-art models when given defective prompts suggests that robustness to defective guidance might be an ability dimension independent of general tool-use capability. Furthermore, the negative impact of task complexity on performance aligns with expectations."}, "weaknesses": {"value": "1.  One of the most confusing yet undiscussed results in the paper is that the \"Complete Success Rate\" of the Baseline (which only contains the task description) is actually higher than that of the Optimal Workflow for several state-of-the-art models. This directly challenges the premise that the MDP-generated workflow is \"optimal.\" The authors do not seem to discuss this phenomenon. Does this imply that the level of detail and complexity in the optimal workflow actually interferes with or confuses the model, or is my understanding incorrect?\n2.  While the introduction of probabilistic failure is a good mechanism, I believe the simulation is still relatively simple. The testing environment uses a base success rate of 0.8 with a penalty adjustment based on dependencies and historical failures. Real-world API failures are often not independent and can depend on factors like rate limits or specific errors caused by invalid inputs. The current model captures five failure types, but the simulation logic seems to simplify this into a single probability calculation.\n3.  The \"optimal\" workflow is an artifact generated by an MDP agent that maximizes its own reward function within its training environment. It is unclear whether this strategy corresponds to the \"gold standard\" or the truly most efficient workflow as perceived by a human expert. Its optimality is relative to its training process."}, "questions": {"value": "Please refer the weakness :-)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hdx6xVbwH7", "forum": "KTZ56LG7jZ", "replyto": "KTZ56LG7jZ", "signatures": ["ICLR.cc/2026/Conference/Submission10859/Reviewer_5aFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10859/Reviewer_5aFN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561127120, "cdate": 1761561127120, "tmdate": 1762922076067, "mdate": 1762922076067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a benchmark named PILOT-Bench for evaluating large language models in tool-driven workflow scenarios under uncertainty. The benchmark comprises 5,040 tasks across 30 APIs, testing robustness to flawed instructions and stochastic tool behavior. Experiments across ten LLMs reveal success rates of 71.8% for optimal, 59.4% for Chain-of-Thought, and 61.5% for flawed workflows."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a comprehensive benchmark for evaluating LLM workflow execution under uncertain tool-use conditions, encompassing 5,040 tasks and 30 tool APIs.\n2. The evaluation includes a wide range of models, covering both proprietary and open-source LLMs."}, "weaknesses": {"value": "1. The result analysis lacks depth—although extensive data are presented (Sections 4.2–4.4), the discussion does not yield clear or insightful conclusions.\n2. The claimed advantages over existing benchmarks (e.g., ToolBench) are limited, as prior works also incorporate probabilistic tool failures and instruction variability."}, "questions": {"value": "Line 352 references the Qwen2.5 series, yet only Qwen2.5-32B results are reported."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d4LkQoAO3w", "forum": "KTZ56LG7jZ", "replyto": "KTZ56LG7jZ", "signatures": ["ICLR.cc/2026/Conference/Submission10859/Reviewer_eCNp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10859/Reviewer_eCNp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923984342, "cdate": 1761923984342, "tmdate": 1762922074915, "mdate": 1762922074915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark PILOT-Bench for evaluating LLM workflow execution under probabilistic tool behaviors and variable instruction quality. PILOT-Bench contains 5,040 tasks generated from a library of 30 APIs with explicit probabilistic error modes such as TIMEOUT, DEPENDENCY ERROR, and INVALID INPUT. Experiments show that current LLMs performance differences when models encounter probabilistic tool failures and varying instruction quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark explicitly models stochastic tool behaviors and noisy instructions, which is novel from previous tool learning benchmarks. The probabilistic simulator and MDP-based workflow construction are well-motivated and technically sound.\n2. The automated generation pipeline is clearly described. The inclusion of both optimal and flawed workflows enables the study of robustness to imperfect guidance, which is highly relevant for practical agents.\n3. The experimental evaluation shows results of ten diverse LLMs which is solid and sound."}, "weaknesses": {"value": "1. All experiments rely on a simulated environment. Demonstrations on real API tools or open-ended workflows would better substantiate the validity.\n2. Writing has room to improve. Some sections  repeat definitions of tool errors and workflow generation, which is redundant."}, "questions": {"value": "Could the benchmark be extended to adaptive settings where the LLM can retry or re-plan after a failure? Reference settings are like ToolEVO [1].\n\n[1] Learning Evolving Tools for Large Language Models. ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QYqm8wRtnZ", "forum": "KTZ56LG7jZ", "replyto": "KTZ56LG7jZ", "signatures": ["ICLR.cc/2026/Conference/Submission10859/Reviewer_NTap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10859/Reviewer_NTap"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988231687, "cdate": 1761988231687, "tmdate": 1762922074190, "mdate": 1762922074190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}