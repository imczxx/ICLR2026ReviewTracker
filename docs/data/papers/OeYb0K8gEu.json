{"id": "OeYb0K8gEu", "number": 3232, "cdate": 1757382925126, "mdate": 1763095344848, "content": {"title": "P-GSPO: Parameterized Group Sequence Policy Optimization for Length-Sensitive Reasoning", "abstract": "Policy optimization for LLM reasoning faces a core trade-off between length sensitivity and update stability. Methods that preserve length sensitivity, such as\nGRPO without length normalization, keep valuable signal for deep multi-step\nreasoning but lead to high-variance, unstable updates. Methods that enforce\nrigid length normalization, such as GSPO / GMPO, stabilize training but become\nlength-blind and suppress credit for thorough reasoning. We introduce P-GSPO\n(Parameterized Group Sequence Policy Optimization), a single-parameter framework that turns this dilemma into a tunable axis. Instead of all-or-nothing normalization, P-GSPO applies a power-law normalization whose strength is controlled by a parameter, directly regulating how sequence length scales the policy\nupdate. This recovers the unstable, fully length-sensitive regime and the stable,\nlength-blind regime as endpoints, while exposing a spectrum of balanced operating points. Integrated into masked diffusion LLMs within the d1 framework,\nP-GSPO yields large gains where length-blindness is most damaging (+19.9 on\nCountdown, +15.9 on Sudoku) and consistent improvements on math benchmarks\n(GSM8K, MATH). The takeaway is concise but profound: explicitly modeling\nand controlling the influence of length is key to achieving both stable training and\nstrong reasoning. All code will be released.", "tldr": "Task-Adaptive Policy Optimization via Parameterized Length Normalization", "keywords": ["Reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6c6c0dfd60096fd359d637341d5a7cafced68ceb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes P-GSPO, a parameterized variant of GSPO to balance length sensitivity and training stability in reinforcement learning for reasoning tasks. The idea is to introduce a tunable exponent $\\alpha$ that interpolates between fully length-sensitive and length-blind updates. The paper shows promising results on masked diffusion LLMs for both synthetic and math reasoning benchmarks such as GSM8K and MATH."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important problem on accuracy and length tradeoff with a good explanation on length normalization.\n- The paper is well-written with both synthetic and real-world dataset experiments on several reasoning benchmarks."}, "weaknesses": {"value": "- The experiments are limited only on masked diffusion LLMs, with no results presented for decoder only LLMs.\n- Claims of stable training are not supported by quantitative stability measures, making it hard to assess. I am also not sure if Figure 4 shows stable training or not.\n- I am also a bit confused by the explanation between Line 318-323. I am under the impression that for Sudoku and countdown, the authors suggest the longer the response is, the better the performance is. However, Table 1 shows that for these tasks, shorter response actually significantly outperforms the baseline."}, "questions": {"value": "1. Can authors show effectiveness of their approach on models like Llama or Qwen under LLM post-training framework? If the proposed method is generally applicable to length normalization, it should show improvement for both frameworks.\n2. Can authors quantify it (e.g., regress correctness on length under different $\\alpha$, or equal-length controls) to show gains aren’t just verbosity? That is, given the same sampling budget, the method still shows an advantage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "phXkTmta0T", "forum": "OeYb0K8gEu", "replyto": "OeYb0K8gEu", "signatures": ["ICLR.cc/2026/Conference/Submission3232/Reviewer_xMBL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3232/Reviewer_xMBL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767568311, "cdate": 1761767568311, "tmdate": 1762916615869, "mdate": 1762916615869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "KLimbEyBgf", "forum": "OeYb0K8gEu", "replyto": "OeYb0K8gEu", "signatures": ["ICLR.cc/2026/Conference/Submission3232/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3232/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763095344005, "cdate": 1763095344005, "tmdate": 1763095344005, "mdate": 1763095344005, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes P-GSPO, a parameterized extension of Group Sequence Policy Optimization that allows controlling how sequence length influences the policy update. The key idea is a single parameter alpha that interpolates between the unstable length-sensitive regime (GRPO) and the stable, length-blind regime (GSPO). The method is tested within masked diffusion LLMs using the d1 framework and evaluated on some reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation is clear: current methods either lose important length information or are unstable. The theory is solid, with a clear formulation of the length influence function and its trade-offs. The paper is easy to follow, and the algorithm is simple to use. Experiments cover several reasoning tasks and show improvements, especially on procedural tasks where length and quality are closely linked."}, "weaknesses": {"value": "#### Major comments\n\nMy main concern is with the experimental section, which feels incomplete.\n\n\n- Section 4.1 mentions that the best checkpoint among different sequence lengths is used, but from Table 1 and Figure 3 this seems true only for P-GSPO, not for diffu-GRPO. This makes the comparison a bit unclear.\n\n- The diffu-GRPO results appear lower than those in Zhao et al. (2025b). Are these reproduced numbers or from the original paper? If reproduced, do they use identical hyperparameters?\n\n- Table 1 and Figure 2 display nearly identical information (accuracy vs. tokens). One of them could be removed or replaced with something more informative, such as a deeper ablation.\n\n- Since $\\alpha$=0 and $\\alpha$=1 correspond to GRPO and GSPO, it would be helpful to include those cases into table 1 or Figure 2 explicitly to verify that P-GSPO smoothly recovers both limits.\n\n- The MATH numbers in Figure 3 do not seem to match those in Table 1. It’s not clear if they come from different checkpoints or sequence lengths.\n\n- All experiments use diffusion-based LLMs. It would strengthen the paper to include at least one test on an autoregressive model to show that the benefit is algorithmic, not tied to the architecture.\n\n--- \n#### Minor comments\n\n- In Figure 3, some figure labels overlap and are hard to read.\n\n- In Figure 3, subplots use different x-axis scales, which makes it difficult to compare across sequence lengths."}, "questions": {"value": "- How sensitive is performance to $\\alpha$ around 0.6?\n- Equation 4 assumes token log-ratios have independent variance \\$\\sigma^2\\$. Is this realistic for reasoning chains? How does correlation between tokens affect the variance-stability trade-off? \n- The diffu-GRPO baseline comes from Zhao et al. (2025b). What was their reported performance? Are the baseline numbers reproductions or taken from the original paper? If reproduced, are they using the same hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oTE3ifnwtI", "forum": "OeYb0K8gEu", "replyto": "OeYb0K8gEu", "signatures": ["ICLR.cc/2026/Conference/Submission3232/Reviewer_H9mR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3232/Reviewer_H9mR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927935189, "cdate": 1761927935189, "tmdate": 1762916614579, "mdate": 1762916614579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes P-GSPO, a single-parameter sequence-level policy optimization method that interpolates between token-level product-of-ratios updates and sequence-level geometric-mean normalization. The core idea is to scale the importance ratio by a length influence function $g(L,\\\\alpha)=L^{1-\\\\alpha}$, yielding\n$\\\\log r_\\\\alpha = L^{1-\\\\alpha} \\\\bar s,$\nwhich makes the stability versus length sensitivity trade-off explicit; under independence assumptions, the variance scales as $\\\\mathrm{Var}(\\\\log r_\\\\alpha)=\\\\sigma^2 L^{1-2\\\\alpha}$, so larger $\\\\alpha$ reduces variance. The algorithm computes $r_\\\\alpha$ before clipping and uses GSPO-style sequence clipping with a KL penalty in the d1 masked diffusion LLM framework. Empirically, with $\\\\alpha=0.6$ the method improves over a diffu-GRPO baseline on GSM8K, MATH500, Countdown, and Sudoku, with large gains on procedural tasks; however, the reported diffu-GRPO baseline is substantially below the original d1 numbers on Countdown, which weakens the strength of the claimed gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Clear formalization of the length influence function $g(L,\\\\alpha)$ with explicit variance scaling.\n* Simple drop-in modification to the GSPO objective with clipping and KL."}, "weaknesses": {"value": "* Countdown baseline appears under-trained relative to d1. P-GSPO reports diffu-GRPO 22.7 percent at 512, while d1 reports 37.1 percent for the same model and length. The main claimed gains on Countdown depend on this gap.\n* Training hyperparameters and reward compositions for the reproduced diffu-GRPO baseline are not specified; only “identical infrastructure (8 GPUs)” is stated. Key d1 settings that affect outcomes are documented in detail but not confirmed here.\n* No appendix with full configs or scripts is provided to verify reproducibility. The PDF ends at references."}, "questions": {"value": "* Please reconcile the Countdown baseline: why is your diffu-GRPO at 512 tokens 22.7 percent while d1 reports 37.1 percent under the same model and evaluation length? Specify all training hyperparameters, reward components, steps, and checkpoint selection for this baseline.\n* Did you match the d1 diffu-GRPO training settings, including LoRA rank and scaling, learning rate $5\\\\times10^{-6}$, $p_{\\\\text{mask}}=0.15$, and per-task step counts (GSM8K 7700, MATH500 6600, Countdown 5000, Sudoku 3800)? If not, please detail deviations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sRxzEq4PCq", "forum": "OeYb0K8gEu", "replyto": "OeYb0K8gEu", "signatures": ["ICLR.cc/2026/Conference/Submission3232/Reviewer_oVxN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3232/Reviewer_oVxN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932584915, "cdate": 1761932584915, "tmdate": 1762916613503, "mdate": 1762916613503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Parameterized GSPO (P-GSPO), a sequence-level policy-optimization objective for reasoning RL that introduces a single parameter $\\\\alpha$ to control length normalization of the log importance ratio. The authors analyze how $\\\\alpha$ affects a “length influence” term and the variance of the sequence ratio, and they instantiate P-GSPO in the d1 masked-diffusion LM setting with standard clipping/KL regularization. Empirically, they report more stable training and better outcomes on length-coupled reasoning tasks (and some math benchmarks) relative to GRPO/GSPO-style baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Simple, clean approach.** The method exposes a single, interpretable parameter that lets practitioners choose the degree of length normalization of the log ratio.\n- **Demonstrated effectiveness in diffusion LMs.** Experiments within the masked-diffusion LM setting show that the approach is workable and beneficial under that training regime."}, "weaknesses": {"value": "- **Motivation is underdeveloped.** The paper briefly asserts that GSPO-like objectives “suppress credit for thorough reasoning” and bias toward short solutions, but does not concretely explain the mechanism or show evidence of the downstream consequences. The paper would be much stronger with the essential analysis (quoting other's analysis works as well) that motivates the proposed approach. Current one is weak. \n- **Theory stops short of gradient-level implications.** The analysis focuses on the sequence-ratio and a length-influence function, but does not unpack how $\\\\alpha$ changes the gradient of the P-GSPO loss. Deriving or empirically probing the gradient scaling would help to clarify why/when P-GSPO improves optimization.\n- **Scope limited to diffusion LMs without clear rationale.** The algorithm is presented generically but only evaluated (and seemingly targeted) for masked-diffusion LMs, with little discussion of why AR LMs would be unsuitable or how performance would translate. Providing results or at least a reasoned analysis for AR LMs (or explaining concrete obstacles) would be helpful."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3bTW9z1hyO", "forum": "OeYb0K8gEu", "replyto": "OeYb0K8gEu", "signatures": ["ICLR.cc/2026/Conference/Submission3232/Reviewer_YJwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3232/Reviewer_YJwT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989926217, "cdate": 1761989926217, "tmdate": 1762916612968, "mdate": 1762916612968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}