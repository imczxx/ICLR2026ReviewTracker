{"id": "IqXlvYA7En", "number": 24500, "cdate": 1758357439610, "mdate": 1759896762836, "content": {"title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss", "abstract": "Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.", "tldr": "", "keywords": ["Language Models", "Autoregressive Language Models", "Autoregressive Image Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8266052f897cc1c59a8f1d2ea389c23e1b479bc4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper theoretically and empirically investigates how autoregressive image generators incorporating diffusion loss can mitigate conditional inconsistency during generation. It provides a rigorous analysis comparing conditional diffusion models and AR diffusion with diffusion loss, proving that autoregressive patch denoising refines condition distributions and that the influence of condition errors decays exponentially during iteration. To address residual condition inconsistency, the authors introduce a condition refinement method based on Optimal Transport formulated as a Wasserstein Gradient Flow, proving convergence toward the ideal condition distribution. Experiments on ImageNet show superior FID and IS scores over existing diffusion and AR baselines, supporting their theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a solid theoretical framework that connects autoregressive modeling, diffusion loss, and conditional refinement through explicit mathematical proofs and lemmas.\n\n2. It introduces a novel condition refinement approach using Optimal Transport and Wasserstein Gradient Flow, offering a principled solution to condition inconsistency.\n\n3. The quantitative performance on ImageNet is competitive or superior to major diffusion and AR baselines, confirming practical benefits of the proposed method."}, "weaknesses": {"value": "1. The experimental scope is limited: evaluations are conducted only on ImageNet 256×256 and with moderate-scale models, without testing scalability to larger or multimodal setups.\n\n2. Despite extensive theory, the method’s implementation details (e.g., computational cost of OT refinement, convergence sensitivity to hyper-parameters) are underexplained.\n\n3. Some notation and theoretical transitions are difficult to follow and may reduce accessibility for non-mathematical readers.\n\n4. The impact of the OT regularization term on actual generation diversity and efficiency is not sufficiently analyzed, only FID/IS metrics are shown."}, "questions": {"value": "Same as weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fugoOE7SDi", "forum": "IqXlvYA7En", "replyto": "IqXlvYA7En", "signatures": ["ICLR.cc/2026/Conference/Submission24500/Reviewer_i7t2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24500/Reviewer_i7t2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415770165, "cdate": 1761415770165, "tmdate": 1762943103818, "mdate": 1762943103818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an important and quite novel problem in autoregressive (AR) image generation. The authors point out that the conditional errors accumulated from previously generated patches may cause inconsistency, and they theoretically analyze how the patch denoising process in AR models can alleviate such conditional errors. Moreover, they prove that the influence of conditional error decays exponentially with iterations. The proposed conditional optimization method based on Optimal Transport (OT) and formulated as Wasserstein Gradient Flow (WGF) is elegant, and the paper shows it can converge toward the ideal conditional distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "（1）The theoretical part is quite impressive. Especially Theorem 2 gives a deep understanding about how the conditional influence (the gradient norm) exponentially decays, which brings new insights into the stability of AR generative models. The combination of OT and WGF for refining conditional distribution looks creative and convincing.\n\n（2）The experimental results are strong. On ImageNet 256x256, the FID score reaches 1.31, which is very competitive compared with existing works."}, "weaknesses": {"value": "（1） Algorithm 1 seems to describe a nested loop structure. I am a bit worry that the computation cost could be large, maybe even K times T slower than the standard AR model. Some clarification or runtime comparison could be helpful.\n\n(2) It is a bit unclear what “Baseline (CDM)” (FID 3.26) and “Baseline” (FID 2.02) exactly mean. Does “Baseline” refer to the AR model without OT refinement? Since the paper’s best FID (1.31) is quite good, some ablation study would help to show how much improvement really comes from the proposed OT method, rather than from the backbone MAR model itself.\n\n（3）The experiments are mainly done on ImageNet. It would be nice if the authors could test at least one more dataset or show more model comparison to make the results more solid."}, "questions": {"value": "Mainly about the points above, especially clarification of baselines and computational complexity.\n\nOverall, I think this paper has strong theoretical contribution and interesting methodology. With a bit more experiment and clarification, it could be a quite nice work."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hIMEmOS04g", "forum": "IqXlvYA7En", "replyto": "IqXlvYA7En", "signatures": ["ICLR.cc/2026/Conference/Submission24500/Reviewer_KiVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24500/Reviewer_KiVa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791743834, "cdate": 1761791743834, "tmdate": 1762943103524, "mdate": 1762943103524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on condition error issues in autoregressive image generation with diffusion loss. It first conducts a theoretical analysis of diffusion models and autoregressive models with diffusion loss, demonstrating that patch denoising optimization in autoregressive models can effectively mitigate condition errors and form a stable condition distribution, while autoregressive condition generation refines conditions to make condition error influence decay exponentially. It further proposes a OT-Based refinement approach, theoretically proving that formulating this refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution. Experiments on ImageNet show the superiority of the proposed method over existing diffusion and autoregressive models with diffusion loss methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical framework for autoregressive image modeling with diffusion loss is both sound and novel. The theory is rigorous and clearly connects diffusion loss to autoregressive conditional modeling. The mathematical exposition is clear and technically solid.\n\n2. The proposed ideas of autoregressive patch-wise denoising and OT-based condition refinement are conceptually well-motivated.\n\n3. The paper provides a rigorous theoretical analysis demonstrating that the patch-wise denoising optimization in autoregressive models effectively mitigates condition errors.\n\n4. It further establishes a mathematically consistent framework linking energy optimization to Optimal Transport (OT) regularization, offering a clear and unified theoretical explanation of the condition error phenomenon."}, "weaknesses": {"value": "1. The empirical validation does not fully match the strength of the theory. The main theory predicts (i) conditional score norm decays exponentially as AR iterations progress and (ii) OT refinement decreases condition inconsistency (Sinkhorn divergence) monotonically. The paper lacks direct empirical plots that verify these claims.\n\n2. Lack of important experiments at higher resolutions, such as the ImageNet 512 × 512 experiment.\n\n3. The comparison against stronger and more recent baselines (after 2025) is missing, weakening the empirical significance of the claims.\n\n4. A figure of the framework is needed to outline the designed methods, including autoregressive patch denoising and OT-based methods."}, "questions": {"value": "1. See the weaknesses section for detail.\n\n2. The proposed theory and method are developed only in the context of autoregressive models with diffusion loss. It remains unclear whether the approach has broader applicability beyond this specific model class.\n\n3. In Table 1, the distinction between the two baselines is unclear, and it is not stated which baseline “Ours” is built upon. The difference between Ours and Ours (MAR) is also ambiguous. Without proper citations or explanations, the table is confusing to readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oCpYoEqqm0", "forum": "IqXlvYA7En", "replyto": "IqXlvYA7En", "signatures": ["ICLR.cc/2026/Conference/Submission24500/Reviewer_Tnpe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24500/Reviewer_Tnpe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839189954, "cdate": 1761839189954, "tmdate": 1762943103252, "mdate": 1762943103252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper conducts a thorough theoretical analysis of autoregressive models with diffusion loss, contrasting them with standard conditional diffusion models. The central thesis is that the patch-by-patch denoising optimization in autoregressive frameworks serves as an effective mechanism for refining the guiding condition, leading to a more stable condition distribution and mitigating errors. This work provides a solid theoretical foundation for understanding and improving conditional autoregressive generation by framing condition refinement as a distribution-level optimization problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's primary strength lies in its rigorous theoretical contributions. The formalization of the condition refinement process as a Wasserstein Gradient Flow is both elegant and novel, providing a principled guarantee of convergence that is often missing in heuristic-based approaches. The detailed lemmas and theorems build a convincing mathematical argument.\n2. The paper is logically well-organized. It seamlessly transitions from a comparative analysis of diffusion models, to the definition of conditional error, to the proposal of the OT-based solution, and finally to empirical validation. The argument is self-contained and easy to follow for readers with the requisite background."}, "weaknesses": {"value": "1. Limited Experimental Scale: As the authors acknowledge in Appendix B, the experiments are confined to the 256x256 resolution on ImageNet. While this is a standard benchmark, state-of-the-art generative modeling research is increasingly focused on higher resolutions and larger models. The absence of such experiments may leave questions about the method's scalability and generalizability.\n2. Readability and Accessibility: The theoretical sections are dense and assume significant familiarity with advanced mathematical concepts like Optimal Transport and Wasserstein Gradient Flows. While precise, this may limit the paper's accessibility. A more intuitive explanation or a high-level overview of why OT is the right tool for this problem could broaden the paper's impact.\n3. Lack of Ablation Studies: The paper presents the final model's performance but would benefit from ablation studies that isolate the impact of the core contribution—the OT-based refinement. For instance, an experiment comparing the full model against a version without the WGF optimization would more clearly quantify the gains from this specific module."}, "questions": {"value": "1. Could you provide a more intuitive, high-level explanation at the beginning of Section 4 to bridge the gap between \"condition inconsistency\" and the OT framework?\n2. What is the computational overhead of the proposed OT refinement step? How does it affect the overall image generation latency compared to the baseline MAR model?\n3. The concept of \"extraneous information\" is central to your motivation. Is it possible to visualize this phenomenon? For instance, by projecting the condition vectors (c_i) before and after refinement into a 2D space, or by showing how the generated patch changes with and without refinement at an intermediate step. This would make the problem much more tangible.\n4. The OT optimization introduces several hyperparameters (e.g., λ, ηk). How sensitive is the model's performance to the choice of these parameters? Is there a robust range for their values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "noLRUz8liw", "forum": "IqXlvYA7En", "replyto": "IqXlvYA7En", "signatures": ["ICLR.cc/2026/Conference/Submission24500/Reviewer_65aN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24500/Reviewer_65aN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916160629, "cdate": 1761916160629, "tmdate": 1762943103061, "mdate": 1762943103061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}