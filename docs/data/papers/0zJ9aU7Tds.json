{"id": "0zJ9aU7Tds", "number": 3409, "cdate": 1757419560527, "mdate": 1763043589542, "content": {"title": "Think Out Loud, Pause in Silence: Confidence-Guided Reflect–Pause–Abort for Robust  Audio Perceptual Understanding", "abstract": "Large Audio Language Models (LALMs) mainly fail for two errors: perceptual errors misidentifying background sounds or speaker turns, and reasoning errors drifting rationales that decouple from acoustic evidence. To address these issues, we propose an adaptive framework that couples perceptual grounding with computation that expands only when needed. First, we introduce **PAQA**, a Perceptually grounded Audio QA dataset of 7,470 multiple-choice items that pairs multi-speaker, background-rich audio with stepwise reasoning and reflection annotations, enabling supervision of verifiable audio-grounded rationales. On the modeling side, we propose **ConfAudio**, which unifies explicit, reflective reasoning (fine-tuned on PAQA) with implicit, pause-driven latent computation trained via GRPO. A confidence-aware controller monitors lowest-group-confidence (LGC) during decoding to insert pauses when uncertainty rises and to abort unstable trajectories, thereby reallocating compute toward hard perceptual segments. To stabilize the training process, we design **a composite reward function** that balances answer correctness, reasoning–answer consistency with perceptual robustness, and output format. Across PAQA, MMAU-mini, and MMAR, ConfAudio consistently improves both accuracy and consistency, particularly in noisy, multi-speaker conditions. Our results demonstrate that confidence-guided, adaptive reasoning—grounded in verifiable acoustic evidence—mitigates the dominant perceptual and reasoning failure modes in audio question answering.", "tldr": "Think Out Loud, Pause in Silence: Confidence-Guided Reflect–Pause–Abort for Robust  Audio Perceptual Understanding", "keywords": ["Large Audio Language Models", "Latent Reasoning", "Reinforcement Learning", "Audio Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b9f892cb9253aaf2b86727d269ae4d20267d634.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ConfAudio, an adaptive framework that integrates explicit reflective reasoning with implicit pause-driven latent computation for audio-language models. A confidence-aware controller monitors the Lowest Group Confidence (LGC) to trigger PAUSE or ABORT during decoding, reallocating compute to perceptually challenging segments. The authors also introduce PAQA, a 7,470-sample Audio-QA dataset with perceptual grounding and reasoning annotations. Experiments on PAQA, MMAU-mini, and MMAR show consistent gains in accuracy and reasoning consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The combination of explicit reflection and pause-based latent computation is useful and effectively addresses perceptual errors in multi-speaker or noisy audio.\n\n(2) The LGC mechanism provides an interpretable control signal for adaptive computation.\n\n(3) PAQA is a well-motivated dataset focusing on perceptual and reasoning alignment.\n\n(4) The method yields noticeable empirical gains on multiple audio QA benchmarks."}, "weaknesses": {"value": "(1) The paper repeatedly claims to reduce perceptual errors but does not report WER, CER, or any other speech-level metric. Without these measures, it is unclear whether ConfAudio truly improves perceptual understanding or simply overfits to dataset bias.\n\n(2) The paper mentions that increasing pause tokens “roughly doubles training time,” but provides no inference-time measurements of latency, throughput, or compute–accuracy trade-offs.\n\n(3) The paper never visualizes when and how often PAUSE and ABORT are triggered, nor their effects on hidden-state trajectories or reasoning quality. It is unclear whether latent reasoning genuinely occurs or if pauses merely prolong decoding.\n\n(4) The definition of LGC is incomplete. Parameters such as window size, stride, smoothing, and thresholds are not reported. No sensitivity, calibration, or robustness analysis is conducted to validate its reliability or to measure false triggers.\n\n(5) The method combines reflective fine-tuning and GRPO reinforcement post-training, but omits key details such as learning rate, batch size, gradient clipping, and reward variance reduction. There is no evidence that GRPO converges reliably under confidence gating.\n\n(6) The ABORT mechanism may prematurely terminate valid reasoning chains, as the authors note qualitatively, but no quantitative statistics or examples are given to assess its impact on correctness.\n\n(7) Incomplete baselines. Several strong models are missing from comparison, including Audio Flamingo 3, Baichuan-Omni-1.5, Qwen2.5-Omni, GPT-4o Audio, and Gemini 2.5 Pro. Including these baselines would clarify whether ConfAudio’s gains are competitive at the current frontier.\n\n(8) PAQA is described as containing both “7,470” and “8k” items in different sections. The paper does not specify train/dev/test splits, speaker overlap, augmentation procedure, or MUSAN license terms, which weakens reproducibility.\n\n(9) The composite GRPO reward is described qualitatively as balancing correctness, consistency, and format, but lacks explicit coefficients or normalization. No ablation quantifies the contribution of each term or checks for saturation or instability."}, "questions": {"value": "(1) Could the authors report WER or other perceptual metrics to substantiate the claim of improved perceptual robustness?\n\n(2) Please provide quantitative measurements of inference latency, throughput, and accuracy trade-offs when varying pause or abort thresholds.\n\n(3) How frequently are PAUSE and ABORT tokens triggered, and how do they affect reasoning quality or hidden-state trajectories?\n\n(4) What are the specific parameters for LGC (window size, stride, smoothing, thresholds), and how sensitive is performance to these values?\n\n(5) Could the authors include GRPO training details (learning rate, batch size, gradient clipping, reward variance control) and provide convergence or stability plots?\n\n(6) How often does the ABORT mechanism terminate correct reasoning, and can the authors include examples of such cases?\n\n(7) Will the authors extend baseline comparisons to include recent large-scale audio reasoning systems such as Audio Flamingo 3, Baichuan-Omni-1.5, Qwen2.5-Omni, GPT-4o Audio, and Gemini 2.5 Pro?\n\n(8) Please clarify PAQA’s final dataset size, splits, and license terms, and describe how speaker overlap and augmentation are handled.\n\n(9) Could the authors specify the exact GRPO reward formula, coefficients, and normalization, and add ablations to show each term’s contribution?\n\nIf the authors can address all the issues and questions raised above with thorough analyses, additional experiments, and clearer reporting, I will raise my overall score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CuN6FmhAzR", "forum": "0zJ9aU7Tds", "replyto": "0zJ9aU7Tds", "signatures": ["ICLR.cc/2026/Conference/Submission3409/Reviewer_tKQW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3409/Reviewer_tKQW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760669759055, "cdate": 1760669759055, "tmdate": 1762916710984, "mdate": 1762916710984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles perceptual and reasoning errors in Large Audio Language Models (LALMs). It introduces PAQA, a new dataset of 7,470 items with noisy, multi-speaker audio and reflection annotations. It also proposes the ConfAudio framework, which unifies explicit \"think out loud\" reflective reasoning with implicit \"pause in silence\" latent computation. A confidence-guided controller adaptively inserts pauses or aborts generation based on uncertainty , improving performance on challenging audio benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduced the PAQA dataset, featuring 7,470 items with multi-speaker, background-rich audio and reflection annotations, which facilitates future research.\n2. Proposed the ConfAudio framework, which originally unifies explicit reflection (\"Think Out Loud\") and implicit, confidence-guided pause-driven computation (\"Pause in Silence\") to solve key perceptual and reasoning errors.\n3. Demonstrated consistent improvements in accuracy and consistency across multiple challenging benchmarks (MMAU Test-mini, MMAR) against strong baselines."}, "weaknesses": {"value": "1. The paper's overall presentation quality is low, which obstructs understanding. Key illustrations, such as Figure 2 (the framework overview) and Figure 4 (ablation results), suffer from low resolution, rendering text and labels blurry and difficult to read.\n2. The paper proposes a sophisticated composite reward function with several novel components, including \"BGS robustness\" and \"Speaker-ASR fidelity\". However, it fails to provide any ablation studies that isolate the impact of these specific reward components. It is unclear how much the \"Speaker-ASR fidelity\" reward.\n3. While the PAQA dataset is a primary contribution, the paper omits crucial statistical information. There is no description of the audio data's characteristics, such as the total duration (in minutes), or the distribution (average, min, max) of lengths."}, "questions": {"value": "see Weakness 2 and Weakness 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2da0RSDjsH", "forum": "0zJ9aU7Tds", "replyto": "0zJ9aU7Tds", "signatures": ["ICLR.cc/2026/Conference/Submission3409/Reviewer_H9ap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3409/Reviewer_H9ap"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749551996, "cdate": 1761749551996, "tmdate": 1762916710615, "mdate": 1762916710615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles a challenge in audio understanding with a focus on perceptual errors, especially when there is enironmental sound or complex speaker context. The paper curates a 7K dataset for this challenge with rich reasoning and reflection annotations. The paper also proposes a pause-and-reflect framework for computation allocation and more accurate reasoning. The paper uses a standard RL method, GRPO, with a custom reward function that considers several aspects of the output quality. Finally, the paper verifies the effectiveness of the proposed method on MMAU and MMAR compared to the base Qwen2-Audio model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The construction of the PAQA dataset requires heavy engineering.\n- The proposed method with pausing and reflection has not been explored in the audio understanding field. \n- Experiments show gains on MMAU and MMAR."}, "weaknesses": {"value": "- First of all, the authors are likely not honest in the use of LLMs. For example, section 5.1 is a typical output from LLM -- it makes no sense and contains factual errors. \n- The novelty of the paper is limited. The method is heavily based on the \"Think before you speak\" paper in ICLR 2024. Besides, the data curation is engineering-focused and less innovative in terms of methodology. \n- While the GRPO part of the paper seems novel (in terms of reward design), I disagree with several designs. First, the BGS robustness reward discourages reasoning based on background sound, but in certain cases the background sound can be useful and we should not add this inductive bias to the model. If background sound is really not desired, one can use a denoising model to pre-process. Second, the length reward is too empirical. The reasoning length should be decided by the model and can be different for very different tasks. All in all, these designs add too much inductive bias to the model and may result in benchmark hacking. \n- For results, the paper only reports poor results on MMAU and MMAR. There are numerous models with higher numbers not reported in Table 2, and many of them outperform the proposed method. \n\nThere are also some minor issues\n- The writing of the paper is not clear. The method is not understandable without going back-and-forth to the references. This harms the readability of the paper.\n- The mathematical expressions are sometimes not understandable -- e.g. L202-203."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper seems to paste AI generated paragraphs in the draft. Section 5.1 is completely wrong, and the first three items in References are completely wrong. These are typical outputs of an AI assistant being asked to write a summary."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BT4eigkiZR", "forum": "0zJ9aU7Tds", "replyto": "0zJ9aU7Tds", "signatures": ["ICLR.cc/2026/Conference/Submission3409/Reviewer_9EhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3409/Reviewer_9EhQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889015454, "cdate": 1761889015454, "tmdate": 1762916710348, "mdate": 1762916710348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses common failures in audio language models, specifically errors in perceiving sounds and in reasoning logically from audio evidence. The authors propose a system called ConfAudio, which is designed to reason more carefully. They also introduce a new dataset called PAQA to train and test their model on challenging audio with multiple speakers and background noise. The main idea behind ConfAudio is that the model can pause to \"think\" silently when it's not confident about an answer, and it can also reflect on its initial reasoning to correct mistakes. The authors present experiments showing that their method performs better than existing models on several audio question-answering tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper correctly identifies that models often struggle with noisy, complex audio and can \"hallucinate\" answers that don't match the acoustic evidence.\n2. The idea of a model that can pause, reflect, and correct itself when it lacks confidence is appealing. The distinction between explicit reflection and implicit \"pausing\" is an interesting concept.\n3. The design of the PAQA dataset, which focuses on multi-speaker and background-rich audio, is well-motivated"}, "weaknesses": {"value": "The paper contains a significant number of citations to non-existent scientific papers. This is a serious breach of academic integrity. It prevents reviewers and readers from verifying the paper's claims, understanding its relationship to prior work, and trusting the authors' research. This issue alone is grounds for rejection."}, "questions": {"value": "I was curious about the training collapse and recovery shown in Figure 8. It shows an interesting dynamic related to the length reward. While the model stabilized, did this \"shock\" during training have any lasting impact on the final model's capabilities or stability? Have you considered alternative reward shaping strategies, such as a smoother penalty function, to avoid such \"shock\"?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "Paper uses fabricated references. Copying this from my official comment:\n\nDear PCs, SACs and ACs,\n\nI am reporting a potential Code of Ethics concern regarding fabricated or incorrect references in this submission.\n\nWhile reviewing the bibliography, I found multiple entries that do not correspond to the cited works. These are not minor formatting issues. **They include completely incorrect author lists, incorrect titles, and incorrect arXiv identifiers. The cited entries misrepresent actual papers in the field. This indicates a lack of scientific rigor and raises concerns about the reliability of the submission.**\n\nBelow is a partial list of problematic references identified (I checked them from bottom to top). Each entry does not match the real publication associated with the listed arXiv identifier or subject area:\n\n```\nLin Zhou, Yujia Peng, Rui Chen, Ziyang Ma, and Yuexian Zou. Omni-R1: GRPO fine-tuning of\nqwen2.5-omni for audio QA. arXiv:2504.12207.\n\nLi Zhang, Yujia Peng, Rui Chen, Ziyang Ma, and Yuexian Zou. Audio-reasoner: Large-scale\nstructured CoT for audio reasoning. arXiv:2406.06317.\n\nWenhao Yu, Qi Zhu, Chuang Niu, Sharath Chandra Raparthy, Kristian Greenewald, Hao Wang,\nYoon Kim, and Tommi Jaakkola. Efficient reinforcement learning for long-chain reasoning. arXiv:2502.12345.\n\nTian Xu, Yujia Peng, Rui Chen, Ziyang Ma, and Yuexian Zou. Audio-thinker: Adaptive reflective\nreasoning for audio-language models. arXiv:2502.14457.\n\nYifei Wang, Yutong Wang, Zhengyang Zhou, Yifan Zhang, Zhengjue Wang, Zhiheng Xi, Chenjun\nXiao, and Yang Yuan. Deep think with confidence: Uncertainty-aware reasoning in LLMs via\nscaling verification, 2025.\n\nHao Wang, Yujia Peng, Rui Chen, Ziyang Ma, and Yuexian Zou. Audio-CoT: Chain-of-thought\nsupervision for audio-language models. arXiv:2401.13969.\n\nYue Tang, Hongyu Lan, Guangzhi Sun, Xianjun Xia, Mengyue Wu, Yuping Wang, Jun Zhang,\nZejun Ma, and Yuexian Zou. SALMONN: Speech-audio-language modeling for open-ended\nunderstanding. arXiv:2307.00162.\n```\n\nSome of the papers listed here do not exist with the stated authors or titles.\n\nI have already marked this in the Ethics section of my review. I am escalating here for Program Chair and Senior Area Chair awareness and guidance on appropriate handling.\n\nPlease advise if additional documentation or checks are required.\n\nI also want to ask on how to proceed with the review because I'm not sure how to approach the rest of the paper, knowing that the authors did such a thing with the references. Initially, I had some suggestions for new experiments or missing ablations. But at this point, I'm not even sure I could overcome this and trust the experimental setup of this paper at all."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6EBoMlIvvd", "forum": "0zJ9aU7Tds", "replyto": "0zJ9aU7Tds", "signatures": ["ICLR.cc/2026/Conference/Submission3409/Reviewer_vhJ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3409/Reviewer_vhJ5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154615857, "cdate": 1762154615857, "tmdate": 1762916710107, "mdate": 1762916710107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}