{"id": "iY0Wi8UIjA", "number": 21471, "cdate": 1758317966236, "mdate": 1759896920199, "content": {"title": "SLIM: Structure-aware Low-rank Inference Model", "abstract": "This paper introduces a new method for the low-rank compression of large language models. Existing techniques typically compress the weights individually, overlooking the internal dependencies within a transformer block. To address this limitation, we formulate a joint optimization problem to find the optimal low-rank weights for an entire transformer block, thereby minimizing the output reconstruction error. Our formulation allows the incorporation of key architectural elements, including residual connections and normalizations. We then introduce SLIM, an efficient algorithm to solve this optimization problem. Experimental results demonstrate that our method consistently achieves task accuracy improvements of over 5\\% compared to existing techniques across a range of compression ratios and model families.", "tldr": "", "keywords": ["Post-training Model Compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d05f439644d8af916d181b9c7a34add68bb77da0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper takes the propagation of low-rank approximation error in transformer and proposes to sequentially update each weight matrix.\n\nSpecifically, this paper shows each step reduces to solving a regularized rank-constrained linear regression problem with a closed-form solution and also shows that a gradient-descent–based refinement can further enhance the performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper brings an insight into the low-rank compression for neural networks that have sequential structures. It introduces a sequential optimization at the transformer block level.\n2. SLIM is compatible with many other compression techniques, such as pruning and quantization.\n3. The gain obtained on pretrained model can be maintained on instruction following fine-tuning."}, "weaknesses": {"value": "1. The value of the close-form solution is undermined by the gradient-based solution. See Questions 5 and 6.\n2. Experiment setups are unclear, which undermines the reproducibility. See Questions 1, 2, and 3,"}, "questions": {"value": "1. What sample data did you use for the low-rank approximation? How did you choose it and why did you use it?\n2. What is the initialization of the low-rank weights in gradient-based optimization of Equation (4)? Is it random, truncated SVD, or the close-form solution?\n3. For the 10 epochs, did you repeat {optimize Block 1, ..., optimize Block N} for 10 times or optimize Block 1 for 10 times and then optimize Block 2 for 10 times, ...?\n4. What is the MSE of SLIM (no OPT)?  \n5. What is the computational complexity of the gradient-based optimization of Equation (4)? It should be $O(d^2)$?\n6. If the complexity of gradient-based algorithm is $O(d^2)$, what is the value or insight of the close form derivation given the close-form algorithm is much more complex and performs worse than the gradient-based algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kD8pylgPzF", "forum": "iY0Wi8UIjA", "replyto": "iY0Wi8UIjA", "signatures": ["ICLR.cc/2026/Conference/Submission21471/Reviewer_BECz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21471/Reviewer_BECz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537694178, "cdate": 1761537694178, "tmdate": 1762941795336, "mdate": 1762941795336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies low rank compression of LLMs. They propose a new method based on reduced rank regression, and provide numerical experiments for different LLMs with up to 7B parameters. The proposed method performs well compared to the baselines such as SVD-LLM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The numerical experiments appear to be reasonable. There is a diverse choice of models and benchmarks, though some ablations are missing."}, "weaknesses": {"value": "I think the paper has very limited technical contribution.\n\n- Handling of residuals and normalization: The paper's main claim is that it performs compression based on the whole transformer block instead of a layerwise scheme. However, this claim is not really supported by the paper. In particular, under the sequential compression that this paper studies, the compression happens for one layer at a time, which is just layerwise. Additionally, the residual connections end up as a bias term that does not change the problem fundamentally, and the normalization terms are just approximated out of existence, I believe simplifying it to the choice of $\\eta$.\n\n- The effect of $\\eta$ is not studied as far as I can tell. Given that normalization is just tuning $\\eta$, I think some investigation is required here.\n\n- Comparison with pruning methods: I would like to ask the authors to compare with additional pruning methods, given that LLM-Pruner has memory issues. For example, https://arxiv.org/abs/2406.07831."}, "questions": {"value": "- Could the author please provide the ablations requested above?\n\n- What happens in terms of quality if one just uses reduced rank regression directly without considering normalization and residual connections? Could the authors please provide some numerical results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NdrOoxh0Bd", "forum": "iY0Wi8UIjA", "replyto": "iY0Wi8UIjA", "signatures": ["ICLR.cc/2026/Conference/Submission21471/Reviewer_aVy8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21471/Reviewer_aVy8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586265113, "cdate": 1761586265113, "tmdate": 1762941795111, "mdate": 1762941795111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper optimizes the traditional low-rank decomposition methods by jointly optimizing it across the entire transformer block. The author constructs a structure-aware optimization algorithm that consider the residual connections and RMS Norms in the optimization. Experiments across different models and datasets shows SLIM can outperform the baselines over different settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper offers a novel perspective that jointly optimize the SVD across after an entire transformer block. This idea is intuitive and clearly presented in the paper.\n- Wide ablation studies and analysis. Experiments have shown great potentials for this methods to be applied together with other methods, such as quantization. \n- Structure-aware methods. As shown in the ablation, this methods respects the residual connections and RMS-Norm of the transformer model, which is novel and critical."}, "weaknesses": {"value": "- Only Wikitext-2 experiment on other model families. Wikitext-2 is a very easy task, while you have included strong models that can solve complicated math and reasoning tasks. I wonder whether you can provide more experiments on other more challenging tasks.\n- The greedy algorithm undermines the goal of join optimization and how far are we from achieving the global optimization>"}, "questions": {"value": "- Can't we just align the f(x) where f(x) is each sub-components? I feel like optimizing across an entire layer is equivalent to optimizing across each sub-layer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "28JF4ikpMv", "forum": "iY0Wi8UIjA", "replyto": "iY0Wi8UIjA", "signatures": ["ICLR.cc/2026/Conference/Submission21471/Reviewer_KGra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21471/Reviewer_KGra"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952165157, "cdate": 1761952165157, "tmdate": 1762941794734, "mdate": 1762941794734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SLIM (Structure-aware Low-rank Inference Model), a new method for compressing large language models by performing joint low-rank optimization across entire transformer blocks instead of compressing each weight matrix independently. SLIM formulates a block-level objective that minimizes output reconstruction error while explicitly accounting for residual connections and normalization layers such as RMSNorm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed joint optimization framework explicitly incorporates key architectural elements such as residual connections and normalization.\n2. The method achieves consistent and significant accuracy improvements (over 5%) across multiple LLMs and benchmarks under equal compression ratios."}, "weaknesses": {"value": "1. The main weakness of this paper is that its primary contribution—joint optimization of low-rank weights across transformer blocks—has already been explored in prior works [1] and [2]. Both studies consider the cumulative effects of cascading reconstruction errors, and the formulations are highly similar: Equation 6 in [1] and Equation 12 in [2] are nearly identical to Equation 6 in this paper. Furthermore, [1] provides a closed-form solution for low-rank compression, where Equation 11 in this paper closely parallels Equations 4 and 5 in [1]. It seems the authors were not aware of these earlier works, as they are not discussed in the article, which makes the only new contribution the inclusion of residual connections and normalization layers.\n2. Lines 237–241 should be revised, as this paper is not the first to introduce the concept of block-level joint optimization for low-rank compression.\n3. More advanced low rank compression method [1,3] should be compared.\n\n\n[1] Zhao, Jialin, Yingtao Zhang, and Carlo Vittorio Cannistraci. \"Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models.\" Forty-second International Conference on Machine Learning.\n\n[2] Wei, Jiateng, et al. \"Structured optimal brain pruning for large language models.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.\n\n[3] Wang, Jingcun, et al. \"Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "Check above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nJA4b27VDR", "forum": "iY0Wi8UIjA", "replyto": "iY0Wi8UIjA", "signatures": ["ICLR.cc/2026/Conference/Submission21471/Reviewer_caDe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21471/Reviewer_caDe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998411099, "cdate": 1761998411099, "tmdate": 1762941794346, "mdate": 1762941794346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}