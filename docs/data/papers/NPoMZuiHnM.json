{"id": "NPoMZuiHnM", "number": 4341, "cdate": 1757664971341, "mdate": 1759898038748, "content": {"title": "HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization", "abstract": "Low-Rank Adaptation (LoRA) has emerged as a widely used technique for adapting large language models (LLMs) to new domains, due to its modular design and broad availability on platforms such as HuggingFace. This availability has motivated efforts to reuse existing LoRAs for domain generalization. \nHowever, existing methods often rely on explicit task labels or additional training, which are impractical for deployment. Moreover, they typically activate a fixed number of entire LoRA modules, leading to parameter redundancy or insufficiency that degrade performance.\nIn this paper, we propose HiLoRA, a training-free framework that performs adaptive hierarchical routing over LoRA pools. Drawing on structural properties of LoRA, we define rank-one components (ROCs), in which each rank parameter is regarded as an independent unit. For a given input sequence, HiLoRA first adaptively selects a subset of LoRAs and determines their ROC allocation based on Gaussian likelihoods at the sequence level. At the token level, it further refines routing by activating only the most informative ROCs.\nWe further provide theoretical guarantees that HiLoRA selects the most relevant LoRAs with high probability.\nExtensive experiments show that HiLoRA achieves substantial improvements in domain generalization, with accuracy gains of up to $55\\% over state-of-the-art baselines, while maintaining comparable inference throughput.", "tldr": "", "keywords": ["Low-Rank Adaptation;Hierarchical Routing;Domain Generation;Task-Specific Adaptation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc69e29a4daba254ee0a99db83487a5d772363bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes HiLoRA, a training-free framework that addresses domain generalization without task labels or additional training through adaptive hierarchical routing in LoRA pools. The core concept of HiLoRA is to treat each rank-one component (ROC) in LoRA as a basic unit, dynamically selecting the most relevant ROCs at both sequence and token levels. Specifically, at the sequence level, HiLoRA calculates similarity between input and each LoRA based on Gaussian likelihood, thereby selecting a subset of LoRA and assigning appropriate ROC quantities. At the token level, the framework further refines routing by activating only the most informative ROCs. Experimental results demonstrate that HiLoRA improves accuracy by 55% on LLaMA2-7B and 13% on FLAN-T5-large."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This study investigates a critical issue in the domain generalization of LoRA. Through detailed analysis of LoRA parameters, three key observations on ROC characteristics were identified, leading to the development of the HiLoRA framework. This clarifies the motivation behind the work and provides a clear logical flow.\n2. The method's error bound is theoretically demonstrated under different scenarios.\n3.The authors have conducted extensive experiments to demonstrate the strong performance of HiLoRA compared to baseline methods."}, "weaknesses": {"value": "1.It lacks analysis or experiments have been conducted for large-scale deployment. When processing large batch inference data, the throughput of this method appears difficult to guarantee. \n2.The model architecture and scale verified in this paper are limited. We hope to see its performance on larger models or MoE architectures."}, "questions": {"value": "1. How to guarantee the performance on large batching inference.\n2. Is it possible to analyze the issue of load balancing of the routing mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HSIJmCKwLd", "forum": "NPoMZuiHnM", "replyto": "NPoMZuiHnM", "signatures": ["ICLR.cc/2026/Conference/Submission4341/Reviewer_xGHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4341/Reviewer_xGHJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651486016, "cdate": 1761651486016, "tmdate": 1762917307245, "mdate": 1762917307245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiLoRA, a training-free framework that leverages existing Low-Rank Adaptation (LoRA) module pools to achieve domain generalization. The core idea involves performing adaptive hierarchical routing: first, at the sequence level, each LoRA is modeled as a Gaussian distribution to compute the likelihood of the input, thereby screening a relevant subset of LoRAs while determining the budget for \"Rank-One Components (ROCs)\"—defined as the fundamental building blocks of LoRA; second, at the token level, the selection is refined by activating the most responsive ROCs based on the down-projected vector of each token. The authors provide theoretical guarantees for the LoRA selection process and demonstrate through experiments that HiLoRA outper"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Interesting Hierarchical Framework\n\n    The two-stage routing mechanism proposed in the paper is logically rigorous.\n\n    a.  First, using Rank-One Components (ROCs) as the minimal fundamental units for routing is a key conceptual contribution (Section 2 \"Dyadic Product Representation\", Section 3.1), breaking through the previous coarser-grained full-module routing pattern and achieving finer adaptation.\n\n    b.  Second, the hierarchical design combines practicality and effectiveness (Figure 3, Section 3.1), first narrowing the vast search space at the sequence level (\"input-aware ROC allocation\"), then performing low-cost fine optimization at the token level (\"token-level ROC routing\"), cleverly balancing expressive power and computational complexity.\n\n    c.  Third, the training-free nature of the framework is an important practical advantage (Abstract, Section 3.1), allowing direct deployment on existing community-built LoRA hubs (e.g., HuggingFace) without expensive gradient-based training for the gating module, lowering the application barrier.\n\n2.  Theoretical Support for the Routing Mechanism\n\n    In this field which is strongly empirically oriented, the paper provides formal theoretical analysis for the design choices.\n\n    a.  First, the authors derived error bounds for the LoRA identification process for in-distribution (ID) and out-of-distribution (OOD) inputs (Section 3.4, Theorems 1 and 2), formally guaranteeing high-probability screening of the most relevant LoRA, enhancing the rigor of the method.\n\n    b.  Second, the theory explicitly links routing errors to interpretable factors, such as task distribution separability (Bhattacharyya distance Bij in Theorem 1) and the size k of the selected LoRA set (Note in Section 3.4), providing valuable insights into the applicability conditions of HiLoRA.\n\n    c.  Third, the use of Gaussian likelihood to measure similarity in Section 3.2 is not a simple heuristic but forms a coherent and principled scheme."}, "weaknesses": {"value": "### 1. Dependence on Original Training Data Samples\nThe practicality of this method is limited by a critical assumption: the sequence-level routing mechanism relies entirely on fitting a Gaussian distribution using $m$ samples from each LoRA's original training dataset (Sec. 3.2, Lines 209-212). In many real-world scenarios, these training datasets are proprietary, private, or completely unavailable. Although the paper acknowledges this issue in the conclusion (Sec. 5), it does not address it through experiments. This significantly restricts the claimed \"training-free\" and \"plug-and-play\" applicability of the method.\n\n### 2. Limited Generalizability of Motivating Observations\nThe core motivation for ROC-level routing is based on limited empirical evidence. The key observations regarding the randomness of down-projection vectors and the clustering of up-projection vectors are derived solely from PCA visualizations of five Natural Language Inference (NLI) tasks and selected layers (Fig. 2 in Sec. 3.1; Figs. 7 & 8 in Appendix C.2). This evidence is insufficiently comprehensive to establish these properties as universal characteristics of LoRAs across all tasks and model architectures.\n\n### 3. Insufficient Hyperparameter Sensitivity Analysis\nThe robustness of the method has not been fully validated due to the lack of sensitivity analysis for several key hyperparameters. First, the number of samples $m$ is fixed at 20 (Sec. 4.1), and its impact has not been investigated. Second, the base number of experts $k$ for LoRA-level routing is fixed at 3 (Sec. 4.1, Line 355); although Eq. 4 provides some adaptability for unseen tasks, performance still depends on this base value. Given that $k$ is crucial for both performance and efficiency, no ablation study has been conducted on it. Finally, the ablation study for the scaling factor $\\gamma$ is only performed in the within-cluster setting of the NLI cluster (Fig. 6), making it impossible to determine whether $\\gamma=40\\%$ is optimal for other task clusters with different structures (e.g., translation tasks) or for the more challenging cross-cluster setting.\n\n### 4. Unvalidated Scalability to Large LoRA Pools\nThe paper claims to address the challenge of repositories containing \"thousands of task-specific LoRAs\" (Sec. 1, Line 38), but experiments are only conducted on relatively small LoRA pools: the LoRA pools used in experiments have sizes of 50 (for LLaMA2-7B) and 33 (for FLAN-T5-large) (Sec. 4.2). The throughput analysis in Fig. 5 shows a clear downward trend in throughput as the number of seen tasks increases to 40, yet it does not evaluate how throughput changes with the total pool size $I$. The cost of computing Gaussian likelihoods (Sec. 3.2) and down-projections (Sec. 3.3) scales linearly with $I$.\n\n### 5. Ambiguity and Complexity of Theoretical Guarantees\nWhile the inclusion of theoretical analysis is a strength, its presentation and practical validation are weak. Theorem 2 (Sec. 3.4, Eq. 6) has high mathematical complexity; despite a brief explanation, its components (e.g., $M_j^\\alpha$, $h_j^\\alpha$) remain difficult to intuitively understand. Additionally, the crucial assumption $M_j^\\alpha > 0$ is presented without any discussion of how frequently it holds in practice.\n\n### 6. Inconsistent Performance Gains and Model Dependency\nThe effectiveness of the method is highly dependent on the base model: HiLoRA shows significant performance gains on LLaMA2-7B, but the gains are much more modest on FLAN-T5-large, and it is often not the top-performing method (Tables 1 & 2). The paper explains that T5 has already undergone extensive instruction tuning (Lines 373-375), which is a plausible reason but also suggests that the utility of this method may be limited to base models with relatively weak initial capabilities.\n\n### 7. Lack of Statistical Significance Testing\nExperimental results are presented as single-run point estimates. Due to the stochasticity introduced by multinomial sampling for ROC allocation (Sec. 3.2), there is likely variance in the results. Without error bars or standard deviations from multiple runs, it is difficult to assess the statistical significance of the reported performance gains over baseline methods, especially when the gain margins are small (e.g., the results of FLAN-T5-large in Table 2).\n\n### 8. Unvalidated Impact of Key Components\nThe contribution of certain design choices has not been empirically validated: a variance normalization step is introduced in Sec. 3.3, Lines 260-269 to stabilize performance, with reference to [Zhao et al., 2025a]. However, its actual impact within the HiLoRA framework has not been measured.\n\n### 9. Potentially Unfair Comparison for ROC-Level Baselines\nThe setup for ROC-level baseline methods may not be optimal. Methods such as LEGO and HiLoRA-ROC are assigned a fixed budget of $k=24$ ROCs (Sec. 4.1, Line 357), which contrasts with HiLoRA's adaptive budget. It is possible that $k=24$ is not the optimal choice for these baselines, thereby undermining their performance.\n\n### 10. Dependency on a Specific High-Quality Embedding Model\nThe sequence-level routing heavily relies on the selected embedding model: the method uses instructor-base (a powerful instruction-tuned encoder) to generate task-aware embeddings (Sec. 4.1, Lines 351-353). The high quality of these embeddings is likely essential for the effectiveness of Gaussian likelihood matching, and the method's performance may degrade significantly when using more standard, off-the-shelf sentence encoders.\n\n### 11. Outdated and Insufficient Tested Models\nThe number of tested models is outdated and insufficient; the paper still uses LLaMA2 instead of current mainstream models, failing to align with the latest model development trends and limiting the persuasiveness of its performance generalizability."}, "questions": {"value": "1. Sequence-level routing requires 20 samples ($m=20$) from the original training data of each LoRA. How would the method perform when these samples are often unavailable for publicly shared LoRAs? Have you considered using proxy data from relevant public datasets, and what would be the expected impact?\n2. Could you conduct a sensitivity analysis on the number of samples $m$ used for fitting the Gaussian distribution? How does the performance degrade when $m$ decreases (e.g., to 5 or 1), and at what value of $m$ would the sequence-level routing fail?\n3. The selection of instructor-base as the embedding model seems crucial. How robust is HiLoRA’s performance to the choice of this embedding model? What would happen to the results if a conventional non-instruction-tuned sentence encoder is used instead?\n4. The paper’s motivation mentions that it is scalable to \"thousands\" of LoRAs, but the experiments are only based on a LoRA pool with a maximum size of 50. When the LoRA pool size $I$ increases to hundreds or thousands, what are the scaling trends of inference throughput and task routing accuracy?\n5. Regarding the theoretical analysis, can you empirically verify the $M_j^\\alpha > 0$ assumption in Theorem 2 using the task pairs from your experiments? How frequently does this condition hold in practice, and what implications would there be if it does not hold?\n6. The optimal value of the scaling factor $\\gamma$ (40%) is determined based on an ablation experiment on the NLI cluster (Fig. 6). Is this optimal value still applicable to task clusters with different structures (e.g., Translation or Struct-to-Text tasks), especially in the more challenging cross-cluster setting?\n7. The minimum number of selected LoRAs (hyperparameter $k$) in Eq. 4 is fixed at 3. Could you conduct a sensitivity analysis on this parameter to show its impact on the performance of both seen and unseen tasks?\n8. To what extent does the current method depend on a pre-trained LoRA pool? How does its performance compare to that of models trained on large amounts of general data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SK38UIrrFB", "forum": "NPoMZuiHnM", "replyto": "NPoMZuiHnM", "signatures": ["ICLR.cc/2026/Conference/Submission4341/Reviewer_TB3c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4341/Reviewer_TB3c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830255358, "cdate": 1761830255358, "tmdate": 1762917306940, "mdate": 1762917306940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HiLoRA, a training-free hierarchical LoRA routing framework designed to achieve cross-domain generalization. Its core idea is to decompose LoRA into Rank-One Components (ROCs) and perform adaptive routing at both the sequence level and token level. The authors provide theoretical error bounds and demonstrate significant performance improvements across multiple task clusters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Motivation is reasonable: As LoRA modules in the community continue to grow in number, how to reuse them without labels or training is a genuine and important problem.\nThe method design exhibits certain innovation: Representing LoRA as Gaussian distributions and dynamically determining the number of activations and ROC budget using log-likelihood constitutes a novel attempt."}, "weaknesses": {"value": "1. The analysis of inference efficiency is severely lacking, and the conclusions may be misleading; it is recommended to include a comparison of inference times across tasks.\n2. The experimental results are not significant. Although the model demonstrates performance improvements as a training-free method, it appears to exhibit a considerable performance gap compared to LoRA, and requires more resources than LoRA, raising doubts about its practical utility.\n3. HiLoRA requires each LoRA to provide training samples for fitting Gaussian distributions, which may be infeasible in practice.\n4. Token-level routing lacks theoretical justification: the paper’s theoretical analysis only covers sequence-level LoRA selection, while the token-level top-k ROC selection is entirely heuristic and may introduce noise."}, "questions": {"value": "1. It is recommended to include an analysis of inference efficiency.\n\n2. It is also recommended to conduct experiments on newer, larger models (e.g., 14B-parameter models) to demonstrate the method’s effectiveness—particularly in scenarios where the resources required for training or fine-tuning are substantial.\n\n3. It is recommended to open-source the code to increase the feasibility of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U1DcIK6zSi", "forum": "NPoMZuiHnM", "replyto": "NPoMZuiHnM", "signatures": ["ICLR.cc/2026/Conference/Submission4341/Reviewer_inFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4341/Reviewer_inFY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921462438, "cdate": 1761921462438, "tmdate": 1762917306054, "mdate": 1762917306054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HiLoRA, a method for routing an unlabeled set of LoRAs for unseen input sequences. HiLoRA is based on the observation that LoRAs can be broken down into ROCs, which are found to be clustered based on task semantic. HiLoRA routes the input at both the sequence level and the token level, leading to strong downstream performance on unseen tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important and practical problem of routing unlabeled LoRAs\n- The core insights seem novel (to the best of my knowledge), leading to a novel routing technique\n- The proposed hierarchical routing mechanism is effective and performant, justified by thorough empirical experiments\n- The routing mechanism is theoretically grounded at the sequence level while per-token routing is proved to be important empirically"}, "weaknesses": {"value": "- The paper tries to tackle unlabeled LoRAs routing problem but assumes access to training samples of each LoRA. Doesn't this setting have an even stricter assumption than the labeled counter part? Can the authors elaborate?\n- In Section 3.1, which is the core of the paper, line 162-174 have some logical leaps that are not easy to follow though I think the conclusions drawn are still valid but need further clarification (Observation #1 and #2) and verifications (Observation #3). Please also see related questions listed below.\n- Line 218 - 236 describe many arbitrary design choices without much justifications. For instance, why the method uses the defined ROC budget $O(x)$ rather than a static number of ROCs? Answering this would require stronger justification and additional empirical results.\n\n\n\n#### Typos\n- $rd$ should be $2rd$ at line 114\n\nThe paper shows strong performance on unseen downstream tasks. I think the biggest concern I have is that the presentation in many places are not easily understandable, though it might stem from my limited technical knowledge. I'm willing to increase the score if the authors adequately address my concerns and questions."}, "questions": {"value": "> \"The down-projection vectors of ROCs exhibit strong randomness and show little alignment with task semantics. This confirms that down-projection vector a primarily functions as a scaling factor, rather than encoding domain-specific information.\" - Line 162\n\nI don't think that the fact that different ranks extract different features clearly lead to the conclusion asserted in this sentence. Can the authors clarify?\n\n\n> In contrast, the up-projection vectors of ROCs within a given LoRA exhibit clear task-dependent patterns. These vectors often form multiple distinct clusters, with each cluster representing a different semantic fragment of the LoRA’s adaptive capacity - Line 165\n\nThe observation that the $B$ matrix is linearly clustered is surprising to me. It implies that the $B$ matrix is lower-rank than the LoRA rank $r$.\n\n> For domain generalization, activating an entire LoRA introduces parameter redundancy and interference, since unrelated clusters are involved simultaneously. - Line 167\n\nCan the authors empirically confirm that clusters of the $B$ matrices correspond to similar LoRA training tasks?\n\n> Theorem 1 and Theorem 2 highlight two key insights. (i) When domains are well separated and the LoRA pool spans diverse tasks, the error bounds are tight, ensuring strong guarantees in both ID and OOD cases. - Line 304\n\nIsn't the tightness of the bound a theoretical property?\n\n> This condition is often met in practice, as task domains are generally distinguishable, and open-source repositories already provide a rich collection of LoRAs across diverse tasks. - Line 306\n\nThe claims here are subjective and not verified. Can the authors empirically justify these claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0S8fOrL9hj", "forum": "NPoMZuiHnM", "replyto": "NPoMZuiHnM", "signatures": ["ICLR.cc/2026/Conference/Submission4341/Reviewer_8TbR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4341/Reviewer_8TbR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992836826, "cdate": 1761992836826, "tmdate": 1762917305359, "mdate": 1762917305359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}