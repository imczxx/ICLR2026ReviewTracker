{"id": "2PP70tFY0S", "number": 17603, "cdate": 1758278165681, "mdate": 1759897165345, "content": {"title": "The First Impression Problem: Internal Bias Triggers Overthinking in Reasoning Models", "abstract": "Reasoning models often exhibit overthinking, characterized by redundant reasoning steps. We identify \\emph{internal bias} elicited by \nthe input question as a key trigger of such behavior. Upon encountering a problem, the model immediately forms a preliminary guess about the answer, which we term an internal bias since it may not be explicitly generated, and it arises without systematic reasoning. When this guess conflicts with its subsequent reasoning, the model tends to engage in excessive reflection, resulting in wasted computation. We validate the association between internal bias and overthinking across multiple models and diverse reasoning tasks. To demonstrate the causal relationship more rigorously, we conduct two counterfactual interventions, showing that removing the input question after the model reduces the redundant reasoning across various complex reasoning tasks, and manually injecting bias affects overthinking accordingly. Further interpretability experiments suggest that excessive attention to the input question serves as a key mechanism through which internal bias influences subsequent reasoning trajectories. Finally, we evaluated several methods aimed at mitigating overthinking, yet the influence of internal bias persisted under all conditions.", "tldr": "", "keywords": ["Large Language Model", "Large Reasoning Model", "Overthinking"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8223d159e41cfb165e157cef3d98e88e048cca5a.pdf", "supplementary_material": "/attachment/b3f65c60c9a8aacaf7dde95f9286e381cfdae9a8.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of overthinking in reasoning models (where they produce large amount of chain of thought unnecessarily). They find that a driver of this phenomenon is the fact that LLMs often have an initial bias/distribution over answers which they tend towards / reason towards when producing chain of thought reasoning. They look at both correlation and causal studies to validate these findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The overall result is informative and interesting. It’s a good insight that updates how I think about reasoning models\n* I think the paper points to an important place where we could look to find more nefarious cases of unfaithful reasoning in LLMs, or find cases where the model might be prone to not going with its reasoning when it should. This might help with other research areas like in AI safety and chain of thought monitoring / unfaithfulness\n* I think the insight here will be a good launching point for further experiments/analysis (like those suggested in the rest of my review)"}, "weaknesses": {"value": "* The way LLMs are behaving here seems pretty reasonable to me (rather than an underling problem) — after all, if the model gets a counterintuitive result, shouldn’t it question the final result? (I think humans would do so in similar circumstances, and this is often how people realize problems in their reasoning.) I think it’s possible that some of the time this is desirable, and other times it’s undesirable. So I might update the framing to some extent to reflect that this isn’t always bad. It would be nice to study the cases where the overthinking / biased reasoning leads to something concretely bad (e.g., unfaithfulness in the model’s reasoning about why it’s rethinking it’s answer\n* This paper reads to me more like a smaller, but notable/clear/useful, scientific result. So I think it’s a nice contribution, and I learned something useful from the paper, but I’m not sure I’d give it a very high rating since it studies a fairly specific phenomenon. I think a broader analysis of when/why models overthink or reason in unfaithful ways or produce post-hoc justified reasoning could be a way to make this paper even more impactful\n* It seems possible to use an LLM to classify the reasoning for overthinking driven by bias towards a certain answer. (Maybe I missed this in the paper?)"}, "questions": {"value": "1. Do you have ideas for other reasons which might cause models to overthink, or produce post-hoc justifications of their pre-existing guesses of answers?\n2. Do you have any insights into when models tend to really question their own reasoning due to a pre-existing belief, vs. when they’re fine to override their pre-existing guess?\n3. How often do LLMs explicitly verbalize the fact that they are thinking more because of thinking the final conclusion is wrong?\n4. Can you clarify why removing the question is a notable thing to study? I might be misunderstanding the set up, but shouldn’t the question be necessary to answer the question at all? (Could be worth clarifying this experimental set up in the paper)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JAvRTm4NTE", "forum": "2PP70tFY0S", "replyto": "2PP70tFY0S", "signatures": ["ICLR.cc/2026/Conference/Submission17603/Reviewer_cuhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17603/Reviewer_cuhQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578694653, "cdate": 1761578694653, "tmdate": 1762927472182, "mdate": 1762927472182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates why reasoning-oriented large language models (LLMs) tend to overthink: producing unnecessarily long or repetitive chains of thought. The authors propose that this behavior stems from an internal bias, a “first impression” prediction the model forms immediately upon reading a question. Through extensive experiments across multiple reasoning benchmarks and models, they show that the larger the mismatch between this initial bias and the final answer, the more reflection tokens and longer reasoning the model generates. Counterfactual tests, such as removing the question after the first step or injecting correct/wrong biases, demonstrate that this bias causally influences reasoning length. Attention analysis further reveals that reflection tokens overly focus on the question text, reinforcing the internal bias. The work concludes that internal bias is a primary cause of overthinking, suggesting new directions for improving efficiency and reliability in reasoning LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Overthinking/efficiency is a core issue in current reasoning-model research; the work provides both diagnostic tools and actionable insight. The work identifies “internal bias” as a distinct, measurable construct that explains known behavioral patterns (overthinking, parroting).\n\n2. The experiments are with multiple model sizes, tasks, which show the same trend, enhancing their reliability.\n\n3. The paper is overall well-writen with clear logic."}, "weaknesses": {"value": "1. While the notion of internal bias is intuitively appealing, its current formulation may be overly simplistic. It remains unclear whether the direct-answer bias obtained from a zero-shot query truly corresponds to the latent representations guiding the model during long chain-of-thought reasoning. The paper lacks deeper theoretical or mechanistic analysis to characterize how conflicts between these two internal states concretely lead to overthinking behavior.\n\n2. The study compellingly diagnoses internal bias as a cause of overthinking, but it does not explore how this signal could be operationalized to improve reasoning efficiency in practice. For example, could internal-bias estimates guide adaptive stopping, selective attention, or early-exit strategies? Providing even preliminary ideas or prototypes in this direction would strengthen the paper’s practical relevance.\n\n3. The causal link between removing the input question and eliminating internal bias is not fully justified. Intuitively, removing the question may simply reduce the model’s use of contextual information—thus shortening reasoning—without specifically targeting internal bias. Moreover, Table 2 reports only aggregate performance after this intervention; it would be informative to decompose the changes (e.g., how many cases shift from correct → incorrect vs. incorrect → correct) to clarify whether the method truly mitigates harmful bias rather than indiscriminately truncating reasoning."}, "questions": {"value": "Please refer to the three weaknesses points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fy1Ct2W8Ef", "forum": "2PP70tFY0S", "replyto": "2PP70tFY0S", "signatures": ["ICLR.cc/2026/Conference/Submission17603/Reviewer_Zn9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17603/Reviewer_Zn9G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887205134, "cdate": 1761887205134, "tmdate": 1762927471130, "mdate": 1762927471130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the pervasive \"overthinking\" phenomenon in Large Language Models (LLMs) when performing reasoning tasks. The authors introduce a core concept: \"The First Impression Problem,\" where a model forms an \"Internal Bias\" or preliminary guess immediately upon reading the input question, without systematic reasoning. Experimental evidence suggests that when this internal bias conflicts with the subsequent systematic reasoning, the model tends to engage in excessive reflection and redundant computations, leading to an unnecessary increase in the length of the reasoning chain and wasted computational resources. The paper validates the association and causal mechanism between internal bias and overthinking through verification across multiple models and tasks, counterfactual interventions, and interpretability analysis, concluding that existing overthinking mitigation methods often fail to address this underlying issue."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) By designing ingenious counterfactual intervention experiments (e.g., removing the input question), the authors effectively demonstrate that internal bias is a causal driver of overthinking, not merely a correlated phenomenon. This significantly strengthens the credibility of the conclusion.\n2) The paper employs attention mechanism analysis (e.g., excessive attention to the input question) to shed light on the specific mechanism by which internal bias influences subsequent reasoning trajectories, offering a new perspective for exploring the models' internal workings.\n3) Experiments cover a range of major LLMs, including GPT-4, DeepSeek-R1, and Llama-2, and are validated on diverse reasoning tasks like GSM8K and BigBench, establishing the generality of \"The First Impression Problem.\""}, "weaknesses": {"value": "1) The paper defines internal bias as \"a preliminary guess formed without systematic reasoning.\" However, how is this internal bias precisely measured or approximated in practice? Although the authors infer Bias Conflict based on whether the first reasoning step conflicts with the final answer (a post-hoc approach), this might not fully capture the \"internal\" and \"not explicitly generated\" initial guess. There is a lack of more direct, microscopic quantification methods for \"internal bias\" based on internal activations or representations, slightly undermining the rigor of the core concept.\n2) The paper defines redundant reasoning steps as overthinking. However, these redundant steps could, at times, simply be a model's \"overfitting\" to lengthy Chain-of-Thought (CoT) examples in the training data. The authors need to more explicitly argue whether these redundant steps truly represent the model's internal \"reflection\" mechanism or are merely an imitation of a verbose template. For example, do the redundant steps contain genuine logic for \"self-correction\" or \"refutation\"?\n3) The latter part of the paper evaluates several existing overthinking mitigation methods (e.g., Self-Refine, R-PRM), noting that \"the influence of internal bias persists.\" This conclusion is somewhat general. Specifically, under what conditions do these mitigation methods fail? Is it because their design inherently ignores internal bias, or is internal bias so deeply rooted that any post-processing is difficult to eliminate? More detailed failure case analyses should be provided.\n4) Experimental results seem to suggest that the overthinking problem is more severe in larger models (e.g., GPT-4). The paper lacks an in-depth exploration of this phenomenon. Does internal bias become stronger and more stubborn as model capability increases? This is crucial for improving future LLM architectures and training."}, "questions": {"value": "1) Besides approximating \"Internal Bias Conflict\" using the conflict between the first generated reasoning step and the final answer, have you explored other finer-grained metrics? For instance, before the first token is generated, have you analyzed specific hidden layer activations (such as the norm or sparsity of Attention or FFN layers) to quantify the intensity of the \"initial guess\"?\n2) In the counterfactual intervention experiment in Section 4.2 (i.e., removing the input question), what is the final accuracy (not just the reasoning chain length) of the intervened model on relevant reasoning tasks (e.g., GSM8K)? Please provide the data. If accuracy decreases, please discuss the practical feasibility of this intervention as a mitigation method.\n3) What is your explanation for the phenomenon where larger models (e.g., GPT-4) appear more susceptible to internal bias-driven overthinking than smaller models (e.g., Llama-2 7B)? Is this related to data distribution in large-scale training or to stronger emergent capabilities?\n4) Given your findings, what specific negative consequences (beyond wasted computation) might this \"First Impression Problem\" and overthinking introduce when LLMs are applied in latency-sensitive scenarios (e.g., real-time robotic control or conversational systems)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4BBXAjHKwU", "forum": "2PP70tFY0S", "replyto": "2PP70tFY0S", "signatures": ["ICLR.cc/2026/Conference/Submission17603/Reviewer_KaQM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17603/Reviewer_KaQM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959655785, "cdate": 1761959655785, "tmdate": 1762927470511, "mdate": 1762927470511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the phenomenon of overthinking from the perspective of internal bias. The authors conduct experiments on the impact of internal bias on LLMs' reasoning. They also applied counterfactual interventions for further study. Analysis on attention is also presented. Overthinking mitigation methods are also studied on whether they work well or not."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overthinking is an important problem to solve. The authors studied this phenomenon from a unique perspective.\n2. The experiments are overall sound, providing empirical evidence on existence of internal bias. \n3. Counterfactual interventions and study on attention are good, providing further evidence of authors' claim. They also studied whether the current mitigation techniques are sufficient to mitigate internal bias.\n4. The paper's appendix contains many details and supplemental material is provided, contributing to good reproducibility."}, "weaknesses": {"value": "1. The authors should discuss the related work more thoroughly to verify their novelty. Although I do not find a work that is identical to this paper, there are already several studies on LLMs' faithfulness, which I think is closely related to the concept of internal bias. Even the discussion of related work on overthinking and bias is not thorough enough.\n2. The core method is purely prompt-based and may be over simple. Forcing a “don’t think” template may not faithfully capture the model’s latent first guess. Although the later interpretability analysis on attention somehow mitigates this gap, I think it is not sufficient. The authors can consider ablation studies like applying few-shot demonstrations. Also you can try methods working on LLMs' internal activation instead of simple prompt engineering.\n3. Section 6's interpretability analysis looks interesting. However, it is also rudimentary since it is only based on simple strawberry demonstration and a simple CharCount dataset. The paper could benefit from studies across different datasets and different models. In addition, I think it is also important to see how the FFN component is related to the internal bias, since FFN plays a crucial role in storing knowledge.\n4. Table 1 only includes 3 datasets and 3 models, which I believe is not sufficient. How would the model perform in other datasets and other domains like coding?\n5. It is unclear what contributed to the internal bias phenomenon. Is model architecture or training methodology related to internal bias? I think it is also an important question to discuss."}, "questions": {"value": "1. Is the internal bias phenomenon correlated with model types or task domains? How general is the phenomenon?\n2. How does your prompt-based Direct Answer methods compare with other activation-based methods like logit lens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nM1hvA2mG1", "forum": "2PP70tFY0S", "replyto": "2PP70tFY0S", "signatures": ["ICLR.cc/2026/Conference/Submission17603/Reviewer_CBzM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17603/Reviewer_CBzM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989159182, "cdate": 1761989159182, "tmdate": 1762927469189, "mdate": 1762927469189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}