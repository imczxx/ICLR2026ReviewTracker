{"id": "jqbYkKrP7k", "number": 482, "cdate": 1756742000370, "mdate": 1759898258380, "content": {"title": "APEX: One-Step High-Resolution Image Synthesis", "abstract": "The pursuit of efficient text-to-image synthesis has driven the field toward a few-step generation paradigm, yet this endeavor is hampered by a persistent trilemma: achieving high fidelity, inference efficiency, and training efficiency simultaneously remains elusive.\nCurrent approaches are often forced into a difficult trade-off. \nWhile methods employing external discriminators can produce high-fidelity one-step generations, they suffer from significant drawbacks, including training instability, high GPU memory costs, and slow convergence. \nConversely, alternative paradigms like consistency distillation, though easier to train, often struggle to achieve high quality in one-step generation.\nThese challenges have restricted the scalability and broader application of one-step generative models.\nIn this work, we present APEX, a method that resolves this trilemma.\nThe core innovation is a self-condition-shifting adversarial mechanism that completely obviates the need for an external discriminator.\nBy eliminating this discriminator bottleneck, APEX achieves exceptional training efficiency and stability. \nThis design makes it particularly well-suited for both full-parameter and LoRA-based tuning of large-scale generative models, offering a truly end-to-end solution.\nExperimentally, APEX demonstrates state-of-the-art (SOTA) performance, delivering high-fidelity synthesis with just a single function evaluation (NFE=1), yields a 15.33x speedup over the original Qwen-Image 20B. \nOur 0.6B model improves upon substantially larger models, such as FLUX Schnell 12B in few-step generation. \nWe further showcase its efficiency by achieving a GenEval score of 0.89 on the Qwen-Image (original 50 NFE is 0.87) for 1 NFE, 20B model with LoRA tuning in just 6 hours. \nAPEX effectively reshapes the trade-off between training cost, inference speed, and generation quality in large text-to-image generative models.", "tldr": "", "keywords": ["Diffusion", "T2I"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56c59e9ca08dec144eed2d5640a53538f78de66c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an improved strategy for distilling T2I diffusion models for few-step sampling. The core idea is to update the training loss in such a manner that the velocity field of the local predictions during sampling is smooth (enabling larger sampling steps) and introducing a discriminator-free adversarial objective which ensures high quality of the final samples. The evaluation on several datasets and using 1 and 2 NFE shows improved performance across various baselines and underlying models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated and presented. The introduced updates to the distillation training seem novel and the ablation studies do show their effectiveness. The evaluation is exhaustive, covering several baselines, models, and evaluation metrics.\nThe results seem to be very strong, on par with many of the SOTA baselines while using fewer or the same number of NFE.\nNot having to rely on additional external models for, e.g., adversarial losses is a big plus."}, "weaknesses": {"value": "What is the cost of the distillation compared to other distillation approaches? While there does not seem to be a need for additional models besides the teacher (EMA?) model, it would be interesting to have a comparison of NFEs/memory requirements for one training step of this approach compared to other approaches such as adversarial distillation or consistency models.\n\nWhat are the limitations of this method? Are there any specific limitations during training or inference, or around other things related to this approach?"}, "questions": {"value": "How stable is the training? Are there any specific tricks necessary or is training generally stable?\n\nHow does the model perform when you increase the number of NFE, e.g., to 8 or more steps? Does the performance increase further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NOXP3ARAP8", "forum": "jqbYkKrP7k", "replyto": "jqbYkKrP7k", "signatures": ["ICLR.cc/2026/Conference/Submission482/Reviewer_sm4A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission482/Reviewer_sm4A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874620498, "cdate": 1761874620498, "tmdate": 1762915528294, "mdate": 1762915528294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "APEX is a new text-to-image synthesis method designed to solve the trilemma of high fidelity, inference efficiency, and training efficiency, which current adversarial (slow, unstable) and distillation (low quality) methods fail to achieve simultaneously. The core innovation is a self-condition-shifting adversarial mechanism that eliminates the external discriminator. This design ensures exceptional training stability and efficiency, making it ideal for LoRA tuning. Experimentally, APEX achieves SOTA high-fidelity synthesis with NFE=1, offering a 15.33x speedup over Qwen-Image 20B. It also improves GenEval scores on the 20B model in just 6 hours of LoRA tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper successfully identifies and frames a critical and highly relevant challenge—the \"trilemma\" concerning fidelity, inference speed, and training stability in one-step generation—making a significant motivational contribution to the research community.\n\n2. The work is presented with exceptional technical clarity; the novel mechanisms are thoroughly explained, and the overall structure and accompanying figures are highly effective, making the complex concepts very accessible to the reader."}, "weaknesses": {"value": "The training data utilized (ShareGPT-4o and BLIP-3o) appears to be highly specialized for the GenEval benchmark, which is also the primary benchmark where the paper achieves SOTA results. Given that the performance on the more general DPGBench is suboptimal, it raises a concern that the observed performance gain might be due to overfitting on the highly specialized training data rather than a generalizable technical improvement. Further experiments on a broader range of established benchmarks are necessary to confirm the robustness of the APEX method."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YkeqTQoA16", "forum": "jqbYkKrP7k", "replyto": "jqbYkKrP7k", "signatures": ["ICLR.cc/2026/Conference/Submission482/Reviewer_qTjA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission482/Reviewer_qTjA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925284701, "cdate": 1761925284701, "tmdate": 1762915528158, "mdate": 1762915528158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current one-step generation methods either introduce training instability, high GPU memory costs, and slow convergence, or struggle to generate high-quality images. This paper proposes APEX, which uses standard diffusion/flow loss and DMD-like loss onto a single model. This manner leads to training effectiveness, efficiency and stability. Experiments on large-scale models such as Qwen-Image 20B validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and the method is simple and easy to follow\n2. Compared to methods based on f-divergence, this method does not need to train multiple models; compared to consistency models, it achieves high-quality generation\n3. Experiments verify the ability of scaling to large models like Qwen-Image 20B, and achieving good performance"}, "weaknesses": {"value": "Overall, this method improves upon f-divergence methods like DMD, VSD and SiD, towards training with only one model. The adversarial mechanism proposed in this paper is an existing approach in those methods. So the paper's contribution is the one-model feature."}, "questions": {"value": "1. In the endpoint objective, why is $x\\_{\\text{fake}}$ from a buffer of recent generations, rather than generating online but with stop gradient?\n2. Why do you use  $c_{\\text{pert}} = Ac + b$ as the condition rather than other choices like an addtional flag?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QeYgno7mq2", "forum": "jqbYkKrP7k", "replyto": "jqbYkKrP7k", "signatures": ["ICLR.cc/2026/Conference/Submission482/Reviewer_Cu5P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission482/Reviewer_Cu5P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972584394, "cdate": 1761972584394, "tmdate": 1762915528028, "mdate": 1762915528028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces APEX, a method for efficient one-step text-to-image synthesis that addresses the fundamental trade-off between path integrability and endpoint fidelity in generative models. The core innovation lies in two complementary mechanisms: (1) a higher-order path self-consistency constraint that regularizes path curvature for numerical stability under large discretization steps, and (2) a discriminator-free self-condition-shifting adversarial mechanism that ensures high perceptual quality without the training instabilities of traditional GANs. APEX achieves state-of-the-art performance with NFE=1, demonstrating a 15.33x speedup over Qwen-Image 20B while maintaining comparable quality (GenEval score of 0.89 vs 0.87)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  It is interesting to categorize many existing works into path integrability and endpoint fidelity.\n-  APEX demonstrates impressive performance across multiple scales.\n-  Detailed ablations on examining training steps, loss component weights, and hyperparameters, providing good understanding into what drives performance."}, "weaknesses": {"value": "- Unconvincing Evaluation: The paper claims they mainly evaluate the model by the GenEval score. However, the GenEval score does not measure image fidelity, leading to insufficient evaluation. The other used FID/clip score metrics are also less convincing to evaluate the modern text-to-image models. More importantly, the training is performed on Bilp-3o, which contains text-image pairs that were specifically generated by GenEval prompts. As far as I know, training on Bilp-3o can lead to a high GenEval score (Table 4 in the paper also shows the phenomenon), making the evaluation by GenEval unfair. \n- CTM [a], MeanFlow [b], and IMM [c] should all belong to methods that simultaneously consider path integrability and endpoint fidelity. However, this paper significantly lacks discussion and comparison regarding the aforementioned works.\n- The quality of the visualized samples looks mediocre and is not as impressive as the evaluation scores.\n- No visual comparison with recent strong baselines.\n- The mechanism of the so-called self-adversarial objective is unclear. And I do not see where the \"adversarial\" component is.\n\n[a] Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion\n\n[b] Mean Flows for One-step Generative Modeling\n\n[c] Inductive Moment Matching"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fSnhowl2Az", "forum": "jqbYkKrP7k", "replyto": "jqbYkKrP7k", "signatures": ["ICLR.cc/2026/Conference/Submission482/Reviewer_pjrF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission482/Reviewer_pjrF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991563668, "cdate": 1761991563668, "tmdate": 1762915527833, "mdate": 1762915527833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}