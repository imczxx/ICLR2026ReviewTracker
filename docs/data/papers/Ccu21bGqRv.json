{"id": "Ccu21bGqRv", "number": 18892, "cdate": 1758291799819, "mdate": 1759897075028, "content": {"title": "Generative Adversarial Optimization: Dual-Reward Reinforcement Learning for Mathematics Reasoning", "abstract": "Despite recent progress achieved by large language models (LLMs), their remarkable mathematics reasoning abilities are largely dependent on fine-tuning on the annotated data, lacking generalization on out-of-distribution tasks. To address this, current methods adopt reinforcement learning (RL) to incentivize the latent capabilities of LLMs, mitigating the need for annotations. However, they often suffer from uncontrollable data difficulty and limited initial capabilities. In this paper, we propose Generative Adversarial Optimization (GAO), a novel reinforcement learning framework consists of a problem poser and a problem solver which are optimized by dual-reward iteratively. Specifically, the poser attempts to propose challenging problems to stump the solver, while the solver strives to solve them. The complete adversarial process is recorded to generate bidirectional rewards, enabling both the poser and solver to co-evolve through this competitive interaction. Experimental results show that GAO achieves state-of-the-art performance compared to previous models of the same size, even without relying on proprietary LLMs.", "tldr": "The game-theoretic approach empowers the model to achieve state-of-the-art performance in mathematical problem-solving.", "keywords": ["LLM Reasoning Model", "Math"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a691ecef0b7e36d876bcbc887a9776bf6692725.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper declares that existing reinforcement learning (RL) frameworks face two challenges:\n\n1. The uncontrollable difficulty of training data, which leads to skewed reward distributions and low learning efficiency.\n2. The limited capability ceiling imposed by the initial abilities of large language models (LLMs).\n\nTo address these issues, this paper proposes Generative Adversarial Optimization (GAO), a dual-reward RL framework consisting of a poser and a solver. Both are initialized through supervised fine-tuning (SFT) to enhance baseline capabilities, and the GAO algorithm filters problems based on the pass rate so that the poser generates progressively harder tasks while the solver improves by solving them. Experiments demonstrate that GAO achieves state-of-the-art (SOTA) performance on multiple benchmarks compared to previous models of the same size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall idea of this paper is reasonable. The two challenges indeed exist, and GAO alleviates or addresses them.\n2. The experimental results are promising. GAO outperforms existing models of the same size, demonstrating its effectiveness."}, "weaknesses": {"value": "**Ablation study is missing.** The paper mentions an ablation study when listing its contributions, but no corresponding experiments or results are presented later. Despite the overall promising experimental results, an ablation study is still needed to analyze the contribution of each proposed component, for example, the effects of SFT initialization and difficulty-based filtering.\n\n**Dependence on external models.** The GAO framework has an inherent limitation: its strong dependence on external models. The GAO framework relies on consistency-voting reward signals from more powerful LLM judges (Qwen3-235B-A22B and DeepSeek-R1). If these judges contain systematic biases, the solver may end up learning answers that align with \"what the judges believe to be correct\" rather than absolute mathematical truth, thereby undermining the objectivity of learning."}, "questions": {"value": "1. GAO employs voting from external reasoning experts, known as the LLM-as-a-Judge technique. This inherently imposes an upper bound on reasoning performance, since GAO cannot surpass the reasoning capabilities of its voter models. Consequently, when training larger models with the goal of exceeding all existing models, GAO would likely lose its effectiveness. How can this limitation be addressed?\n\n2. According to Table 1, GAO underperforms Qwen3-14B on GSM8K and MATH500, and its average performance across all benchmarks is only slightly higher than that of Qwen3-14B. This raises the question of whether the performance gains introduced by GAO are substantial compared to the improvements brought by the scaling law."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pw8rSkcuV9", "forum": "Ccu21bGqRv", "replyto": "Ccu21bGqRv", "signatures": ["ICLR.cc/2026/Conference/Submission18892/Reviewer_BHnG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18892/Reviewer_BHnG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657805395, "cdate": 1761657805395, "tmdate": 1762930863756, "mdate": 1762930863756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GAO, an adversarial reinforcement learning framework designed to enhance the mathematical reasoning abilities of LLMs.\nGAO trains a Poser that generates challenging problems and a Solver that learns to solve them, enabling both models to co-evolve through iterative competition."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces GAO, a novel dual-reward reinforcement learning framework that jointly trains a problem poser and a solver through adversarial interaction.\nIt effectively generates adaptive, challenging mathematical problems that continuously expose the solver’s weaknesses, leading to strong gains in mathematical reasoning without requiring manually annotated data."}, "weaknesses": {"value": "- The paper does not compare the poser’s reasoning performance with the solver trained on poser-generated problems. Without such a comparison, it remains unclear why we cannot simply use the poser itself for reasoning instead of training the solver.\n- The paper lacks direct ablation studies, so it is unclear which components of GAO actually contribute to the performance improvements.\n- The paper does not quantitatively show how valid the generated problems were, nor how invalid the discarded problems actually were.\n- Training both the Solver and the Poser roughly doubles the computational cost compared to standard training.\n- The idea of improving performance through competition is not particularly novel.\n    - https://arxiv.org/abs/2404.10642\n    - https://arxiv.org/abs/2311.08107\n    - https://www.arxiv.org/abs/2510.18407\n    - https://arxiv.org/abs/2504.19162"}, "questions": {"value": "- Qwen2.5-72B-Instruct may have been trained on mathematical datasets. If that is the case, the method indirectly relies on supervised mathematical data, making the “annotation-free” claim overstated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B7i5uwKNHF", "forum": "Ccu21bGqRv", "replyto": "Ccu21bGqRv", "signatures": ["ICLR.cc/2026/Conference/Submission18892/Reviewer_TLo1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18892/Reviewer_TLo1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858651455, "cdate": 1761858651455, "tmdate": 1762930863299, "mdate": 1762930863299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper describes a method that uses GAN like optimization called GAO, which iteratively trains a solver and poser, where the poser tries to raise questions the solver cannot solve, and the solver tries to solve the questions. Gain improvement over some baselines on math datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Improvement over baseline on a collection of datasets\n2. The idea of self-training like methods does not rely on data collection"}, "weaknesses": {"value": "1. Crispy evaluation against baselines. Key ablations of the poser are missing (e.g. how would it work without filtering (both large model filtering and difficulty filtering)? \n2. The method itself relies heavily on forward inference of large scale close models. No evidence that the current poser works better than simple synthetic data based methods, such as completely removing the poser and use deepseek or qwen72B for posing questions and apply filtering similarly, as the inference burden is similar.\n3. Only experiments on qwen3, raising concerns of generalization."}, "questions": {"value": "1. Does the current method works better than simple synthetic data generation methods? (make data with deepseek, filter out questions of larger models acc less than 20% or more than 60%, similar as you do)\n2. Does it work on other models?\n3. How would the performance change as the filter bar changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fApceYK01L", "forum": "Ccu21bGqRv", "replyto": "Ccu21bGqRv", "signatures": ["ICLR.cc/2026/Conference/Submission18892/Reviewer_rjqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18892/Reviewer_rjqK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002720030, "cdate": 1762002720030, "tmdate": 1762930862766, "mdate": 1762930862766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}