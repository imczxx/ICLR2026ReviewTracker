{"id": "DqfKOHqzh9", "number": 24422, "cdate": 1758356774198, "mdate": 1759896767009, "content": {"title": "TokenSculpt: Pruning with Min-Max Spatio-Temporal Duplication for Video Grounding", "abstract": "Visual token pruning is essential for reducing computational overhead in multimodal large language models (MLLMs), especially for videos where visual tokens outnumber text ones. Existing pruning methods, typically based on attention or similarity, barely consider the spatiotemporal structure of videos and may incorrectly merge low-similarity or irrelevant tokens, leading to information loss. We propose TokenSculpt, a structure-aware pruning approach designed for video inputs. It aggregates tokens based on similarity while explicitly avoiding low-similarity merges, and applies a bipartite matching strategy to uniformly sample tokens across spatial and temporal dimensions. This design helps preserve the structural integrity of video representations. Additionally, TokenSculpt is compatible with Flash-Attention, enabling efficient integration into modern MLLMs. Experiments across multiple video-language tasks show that TokenSculpt consistently outperforms prior methods. It achieves an average improvement of 2.9% over baselines. While particularly effective in redundant video settings, it also performs well across a range of scenarios. Our approach provides an efficient and scalable solution for video token pruning and improves performance in grounding and related video-language understanding tasks.", "tldr": "SOTA token pruning method for grounding tasks.", "keywords": ["Token Pruning", "Spatial Grounding", "Temporal Grounding", "MLLM Acceleration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ede7009a6b5f0bf5cdc5361b4047922e0f728778.pdf", "supplementary_material": "/attachment/ade62d9344e41794b4c7797b6e80c3e9adbbaf22.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TokenSculpt, a novel, training-free token pruning method for video understanding in multimodal large language models (MLLMs). The core idea is to preserve the spatio-temporal structure of video inputs during token compression by combining two key mechanisms: (1) Min-Max Duplication, which identifies and retains the earliest/latest and top-left/bottom-right tokens (i.e., boundary anchors) along temporal and spatial dimensions to maintain event boundaries; and (2) bipartite matching-based uniform sampling, which ensures balanced token distribution across time and space to avoid over-aggregation. Experimental results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental validation is rigorous, spanning multiple models, datasets, and token budgets. The results are consistent and show clear gains, especially in challenging low-budget regimes.\n2. The Min-Max Duplication mechanism is a creative and principled solution to preserve spatio-temporal boundaries, a key insight missing in prior pruning methods.\n3. As video inputs grow longer and higher-resolution, efficient token compression is crucial. TokenSculpt offers a practical, effective, and general solution that improves performance while reducing cost."}, "weaknesses": {"value": "1. The experiments focus on moderately long videos. It would be valuable to test TokenSculpt on extremely long videos (e.g., >10 minutes) to assess its robustness under extreme compression.\n2. While the method is efficient, the bipartite matching step for uniform sampling may introduce non-trivial overhead. A brief runtime analysis or comparison would help assess practical efficiency.\\\n3. While the method is empirically strong, a more formal analysis of why Min-Max Duplication preserves grounding information would strengthen the theoretical foundation."}, "questions": {"value": "1. Since TokenSculpt explicitly manipulates token positions, how does it interact with the positional encoding scheme in the MLLM (e.g., RoPE in Qwen)? Does reordering or merging affect the positional signal, and if so, how is this mitigated?\n2. How sensitive is TokenSculpt to the definition of \"min\" and \"max\" in non-rectangular or irregularly shaped regions?  Additionally, how accurately does TokenSculpt identify event boundaries across temporal and spatial axes?\n3. Could the method be extended to dynamically adjust the number of anchors based on scene complexity ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1SWLaoCuZ2", "forum": "DqfKOHqzh9", "replyto": "DqfKOHqzh9", "signatures": ["ICLR.cc/2026/Conference/Submission24422/Reviewer_KDoL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24422/Reviewer_KDoL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727466181, "cdate": 1761727466181, "tmdate": 1762943077801, "mdate": 1762943077801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TokenSculpt, a training-free token pruning method for video-based multimodal large language models. It preserves spatio-temporal boundaries through three lightweight steps: min-max boundary token selection, balanced sampling, and structure-guided merging. Without retraining, TokenSculpt can be applied to models like Qwen2.5-VL and VideoChat-R1, achieving up to 1.8× speedup while maintaining or improving performance under heavy token reduction."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear Writing:** The paper is well-organized and clearly written. The motivation, methodology, and experimental setup are easy to follow.\n2. **Insightful Observation:** The authors make an original observation in Sec 3.3 and Figure 3, offering an intuitive explanation of token redundancy in video MLLMs. \n3. **Novel and Targeted Method:** The proposed approach is novel and effectively addresses the key limitations of existing pruning baselines—attention-based methods that are incompatible with efficient attention mechanisms, and similarity-based methods that destroy spatio-temporal structure."}, "weaknesses": {"value": "1.  **Insufficient Analysis:** In Table 1, TokenSculpt shows inconsistent trends across temporal video grounding tasks — performance on Charades-STA remains nearly unchanged as tokens are reduced, while on ActivityNet, the mIoU significantly increases (28.13 → 41.16). The paper lacks an in-depth analysis to explain this discrepancy. It would be helpful to clarify what causes this variation — for example, whether it is related to differences in average video duration or the initial token length distribution between datasets."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3gajKPLNpa", "forum": "DqfKOHqzh9", "replyto": "DqfKOHqzh9", "signatures": ["ICLR.cc/2026/Conference/Submission24422/Reviewer_rAus"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24422/Reviewer_rAus"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978823963, "cdate": 1761978823963, "tmdate": 1762943077541, "mdate": 1762943077541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TokenSculpt, a token pruning method for video inputs in multimodal large language models (MLLMs) that reduces visual token count through min-max guided, similarity-based token merging. To preserve spatial structure, the merged tokens are reordered according to their original positional indices. TokenSculpt is also designed to be compatible with FlashAttention for improved computational efficiency. The method is evaluated on two video grounding benchmarks and three video question-answering benchmarks, demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is clearly written and well-structured, making it easy to follow.\n* The core idea is intuitive, and the inclusion of experiments across varying token retain rates provides a clear and informative picture of the method’s trade-offs and potential.\n* The method demonstrates satisfactory efficiency, enhancing its practical applicability in real-world video understanding scenarios with MLLMs."}, "weaknesses": {"value": "* TokenSculpt demonstrates strong performance on video grounding tasks compared to baselines like DART and ToME, but its results on long-form video QA benchmarks, such as VideoMME-Long and EgoSchema, are less competitive. This raises the question of whether the method is inherently limited in handling extended temporal contexts. Additional experiments and deeper analysis on long-video understanding scenarios would be valuable to clarify this limitation.\n* It is somewhat surprising that the similarity-based sampling strategy underperforms uniform or even random sampling on Charades-STA (Table 3), contrary to typical expectations for such methods. Is this due to dataset-specific characteristics, e.g., sparse or ambiguous activity boundaries, or could it indicate issues with the pivot selection or similarity computation in the current implementation? Further investigation into this behaviour would strengthen the paper’s empirical foundation.\n* In terms of accuracy, TokenSculpt is largely comparable to ToME across multiple settings (Table 1). However, Table 2 suggests that ToME is slightly more efficient in terms of inference speed or memory usage. This trade-off makes it difficult to conclusively determine which method is preferable."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BG5gezRl7L", "forum": "DqfKOHqzh9", "replyto": "DqfKOHqzh9", "signatures": ["ICLR.cc/2026/Conference/Submission24422/Reviewer_5R5q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24422/Reviewer_5R5q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979893895, "cdate": 1761979893895, "tmdate": 1762943077291, "mdate": 1762943077291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TokenSculpt, a structure-aware visual token pruning method for video grouding. \n\nIt introduces a **Min-Max Duplication** mechanism that preserves spatial-temporal anchors (start and end points of events) and combines it with **uniform sampling and similarity-based merging** to maintain video structure after heavy compression.\n\n The method is plug-and-play, compatible with Flash-Attention, and achieves notable improvements over ToME and DART across multiple video grounding benchmarks (e.g., Charades-STA, VideoMME, EgoSchema) while using only one-third of the original visual tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. TokenSculpt remains compatible with FlashAttention for efficiency.\n2. This work conduct comprehensive experiments on multiple benchmarks include VideoMME.\n3. The method is plug-and-play, have somewhat generality."}, "weaknesses": {"value": "1. While the paper claims to preserve the spatiotemporal structure of video representations, the uniform sampling step in TokenSculpt seems to contradict this goal. Uniformly sampling tokens across spatial and temporal dimensions can disrupt dense or motion-rich regions, breaking the natural continuity of video features. As shown in Table 3, applying uniform sampling on Qwen2.5-VL 3B leads to a notable performance drop. It raises questions about whether such a coarse, structure-agnostic procedure is necessary—or whether it undermines the very goal of structure-aware token pruning.\n\n2. The method shows a significant decline on Charades-STA. A deeper explanation is needed—whether due to dataset characteristics (e.g., temporal grounding difficulty) or inherent method limitations.\n\n3. Efficiency results are only reported for 33 % token retention. Analyses at other pruning ratios (e.g., 45 %, 66 %) would help illustrate robustness and trade-offs between accuracy and efficiency.\n\n4. The paper claims Min–Max Duplication helps preserve structure, but lacks details or evidence showing that the selected Min–Max anchor tokens are truly representative of spatial–temporal diversity."}, "questions": {"value": "1. How does uniform sampling preserve spatiotemporal continuity, given that it may disrupt motion-dense regions (Table 3)?\n\n2. Why does TokenSculpt perform notably worse on Charades-STA—dataset bias or method limitation?\n\n3. Can the authors provide efficiency analyses at other pruning ratios (e.g., 45 %, 66 %) to show robustness?\n\n4. What evidence supports that the Min–Max anchors truly capture diverse spatial-temporal structures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jULXY8C3ZH", "forum": "DqfKOHqzh9", "replyto": "DqfKOHqzh9", "signatures": ["ICLR.cc/2026/Conference/Submission24422/Reviewer_3JTt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24422/Reviewer_3JTt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141155779, "cdate": 1762141155779, "tmdate": 1762943076876, "mdate": 1762943076876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}