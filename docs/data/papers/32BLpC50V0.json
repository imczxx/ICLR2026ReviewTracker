{"id": "32BLpC50V0", "number": 5182, "cdate": 1757862266153, "mdate": 1759897990269, "content": {"title": "MAGE: Multi-scale Autoregressive Generation for Offline Reinforcement Learning", "abstract": "Generative models have gained significant traction in offline reinforcement learning (RL) due to their ability to model complex trajectory distributions. However, existing generation-based approaches still struggle with long-horizon tasks characterized by sparse rewards. Some hierarchical generation methods have been developed to mitigate this issue by decomposing the original problem into shorter-horizon subproblems using one policy and generating detailed actions with another. While effective, these methods often overlook the multi-scale temporal structure inherent in trajectories, resulting in suboptimal performance. To overcome these limitations, we propose MAGE, a Multi-scale Autoregressive GEneration-based offline RL method. MAGE incorporates a condition-guided multi-scale autoencoder to learn hierarchical trajectory representations, along with a multi-scale transformer that autoregressively generates trajectory representations from coarse to fine temporal scales. MAGE effectively captures temporal dependencies of trajectories at multiple resolutions. Additionally, a condition-guided decoder is employed to exert precise control over short-term behaviors. Extensive experiments on five offline RL benchmarks against fifteen baseline algorithms show that MAGE successfully integrates multi-scale trajectory modeling with conditional guidance, generating coherent and controllable trajectories in long-horizon sparse-reward settings.", "tldr": "A multi-scale auto-regressive generation method for offline RL", "keywords": ["Offline Reinforcement Learning; Auto-Regressive; Multi-Scale; Long-horizon"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c9717d0e352cd8c06effd4b2ecaf756b9b945ff.pdf", "supplementary_material": "/attachment/267181e94219346aadcaaececdf275cfeeaef282.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents MAGE, a novel generation-based offline reinforcement learning framework designed to tackle long-horizon tasks with sparse rewards. Unlike prior hierarchical generation methods that fail to capture the intrinsic multi-scale temporal structure of trajectories, MAGE introduces a condition-guided multi-scale autoencoder for learning hierarchical trajectory representations and a multi-scale transformer that autoregressively generates trajectories from coarse to fine temporal scales. This approach effectively models temporal dependencies across resolutions while enabling precise short-term control via a condition-guided decoder. Extensive experiments on five offline RL benchmarks against fifteen baselines demonstrate that MAGE produces coherent and controllable trajectories, achieving state-of-the-art performance in challenging long-horizon, sparse-reward settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed idea is novel, adapting the concept of multi-scale generation to the offline RL setting, which was originally explored in some computer vision literature (e.g., [1]).  \n- The empirical evaluation is comprehensive, covering a diverse set of baselines (including model-free, autoregressive, and generative approaches) and a wide variety of offline RL benchmarks such as maze navigation, MuJoCo locomotion, Adroit manipulation, and kitchen environments.  \n- The paper is clearly written and easy to follow, supported by intuitive illustrations (Figure 1), a well-designed toy experiment that highlights the advantages of the proposed method (Figure 2), and a clear architectural overview (Figure 3).\n\n[1] Tian, K., Jiang, Y., Yuan, Z., Peng, B., & Wang, L. (2024). Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37, 84839-84865."}, "weaknesses": {"value": "- Since the authors claim that the proposed method performs well on long-horizon tasks, it would be interesting to evaluate it on more challenging maze tasks in OGBench [1], such as *antmaze-giant-navigate*, *antmaze-teleport-navigate*, and *humanoidmaze-giant-navigate*.  \n- There are a few typos in the paper — for example, “Casual” should be corrected to “Causal” in Figure 3(b).\n\n[1] Park, S., Frans, K., Eysenbach, B., & Levine, S. (2024). Ogbench: Benchmarking offline goal-conditioned rl. arXiv preprint arXiv:2410.20092."}, "questions": {"value": "- Could the authors provide additional results on more challenging tasks from OGBench to further demonstrate the effectiveness of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZBxY1rf2aO", "forum": "32BLpC50V0", "replyto": "32BLpC50V0", "signatures": ["ICLR.cc/2026/Conference/Submission5182/Reviewer_da3r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5182/Reviewer_da3r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760921456458, "cdate": 1760921456458, "tmdate": 1762917932822, "mdate": 1762917932822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MAGE, a novel approach for offline RL by introducing a multi-scale autoencoder and a transformer to generate trajectories from coarse to fine scale. It achieves strong empirical results on several offline RL benchmarks, which require planning on long horizons with sparse rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Hierarchical generation with autoregressive models is a relevant topic in offline RL.\n\n- Strong empirical results in several offline RL benchmarks.\n\n- Idea is clear and also it exhibits short inference time, which is a main bottleneck of diffusion-based method for offline RL."}, "weaknesses": {"value": "- While authors list up hyperparameters for each task in the appendix, several crucial hyperparameters are missing: RTG conditioning value, $\\lambda_{\\text{cond}}$, and number of temporal scales (K). \n\n- It would be better to evaluate the method with much larger mazes, such as pointmaze-giant and antmaze-giant suggested in OGBench, to clearly verify the effectiveness of the proposed method in long-horizon settings."}, "questions": {"value": "- It would be nice to place the overview figure in the appendix to the main text. It is hard to figure the main concept without that figure.\n\n- It seems that with a very small number of K (e.g.,1,2), it achieves better performance compared to the baselines. Could the authors explain why such a phenomenon is possible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "By2bzZb840", "forum": "32BLpC50V0", "replyto": "32BLpC50V0", "signatures": ["ICLR.cc/2026/Conference/Submission5182/Reviewer_gDvQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5182/Reviewer_gDvQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722747075, "cdate": 1761722747075, "tmdate": 1762917932173, "mdate": 1762917932173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MAGE, which (i) learns multi-scale discrete trajectory tokens via a condition-guided autoencoder, and (ii) performs coarse-to-fine autoregressive (AR) generation across scales with a Transformer. A lightweight adapter and an initial-condition consistency loss enforce alignment with the input condition (initial state and RTG). Discrete tokens are trained with cross-entropy and Gumbel-Softmax/STE. Experiments on long-horizon suites (Adroit, Franka Kitchen, AntMaze, Maze2D/Multi2D) show strong gains, with ablations on the number of scales, sequence schemes, and conditioning terms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a coherent multi-scale discrete trajectory representation coupled with cross-scale AR generation (global dependencies at coarse scales; local refinements at fine scales).\n2. This paper proposes a condition-guided decoder with explicit initial-condition loss to reduce quantization/AR mismatch.\n3. This work presents strong empirical results on long-horizon, sparse-reward tasks, plus ablations on hierarchy depth and conditioning."}, "weaknesses": {"value": "1. No RTG-value generalization study. The method conditions on RTG but does not report performance vs. different target RTGs (e.g., low/medium/high percentiles or out-of-distribution RTGs).\n2. The same framework requires different codebook sizes, hierarchy depth K, and network depth across suites, indicating high sensitivity to task distribution and substantial tuning burden.\n3. Using inverse dynamics cloning plus conditional reconstruction encourages alignment with the behaviour distribution, which can yield trajectories that “look aligned” but do not guarantee high-return action. Is it possible to use value/success reweighting?\n4. The model conditions only on (s0, R) and does not explicitly encode goals or intermediate waypoints. In sparse-reward settings, if the target or waypoint distribution shifts, and especially when new destinations are not well represented in the codebook, I wonder if the performance is likely to degrade. Moreover, the coarse-to-fine AR procedure locks in global structure at coarse scales, while fine scales can only make local corrections, making early mistakes difficult to recover over long horizons."}, "questions": {"value": "1. Related to weakness 3, is it possible to use value/success reweighting?\n2. Related to weakness 4, in sparse-reward settings, if the target or waypoint distribution shifts, and especially when new destinations are not well represented in the codebook, I wonder if the performance is likely to degrade."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VUJIQayOgN", "forum": "32BLpC50V0", "replyto": "32BLpC50V0", "signatures": ["ICLR.cc/2026/Conference/Submission5182/Reviewer_6ooy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5182/Reviewer_6ooy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975137885, "cdate": 1761975137885, "tmdate": 1762917931695, "mdate": 1762917931695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles challenges in offline reinforcement learning, particularly for long-horizon tasks with sparse rewards, where existing generative models struggle due to inadequate modeling of multi-scale temporal dependencies. It introduces MAGE, a method that uses a multi-scale autoencoder to encode trajectories into hierarchical token maps at varying temporal resolutions and a multi-scale transformer to autoregressively generate these maps from coarse to fine scales, conditioned on initial state and return-to-go, with a condition-guided decoder for precise trajectory control. Key contributions include integrating multi-scale trajectory modeling with conditional guidance to produce coherent trajectories. Main results from experiments on five benchmarks demonstrate superior performance against fifteen baselines, especially in sparse-reward settings, with fast inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The multi-scale autoencoder captures hierarchical temporal dependencies by encoding trajectories into token maps from coarse to fine resolutions, enabling better handling of long-term structures, which enhances novelty by extending autoregressive models like VAR to temporal domains in RL.\n- Figure 1 effectively contrasts MAGE's hierarchical generation with Decision Transformer's sequential and Decision Diffuser's all-at-once approaches, immediately conveying the core conceptual contribution.\n- Temporal scale analysis demonstrates performance improvement with increasing K up to 8 scales, confirming the benefit of multi-scale modeling."}, "weaknesses": {"value": "- The paper credits VAR for visual autoregressive modeling and states in the appendix that MAGE is \"implemented based on the source code of... VAR\". The core idea of MAGE (multi-scale, coarse-to-fine autoregressive generation) appears to be a direct application of the VAR architecture to RL trajectories. The paper should more clearly delineate its own novel contributions from the base architecture it adapts.\n- The final MAGE system is highly complex, involving a multi-scale VQ-VAE, a multi-scale autoregressive transformer, a separate latent inverse dynamics model, and an \"augmented decoder D' with a parameter-efficient refinement module\"for the $\\mathcal{L}_{cond}$ loss.\n- The necessity of the latent inverse dynamics model $I(Z)$ is not fully justified. The autoencoder's decoder $\\mathcal{D}$ already reconstructs the state trajectory $\\hat{\\tau}$. Why not decode actions as well, or infer them from consecutive reconstructed states? The ablation in Table 19 only compares a latent vs. explicit inverse model, but not the necessity of an inverse model at all.\n- The introduction of a separate adapter module just to implement the $\\mathcal{L}_{cond}$ loss adds complexity. It is not clear why this loss could not be applied directly to the main decoder $D$, or why this adapter-based approach is superior.\n- No formal analysis or theoretical bounds explaining why the specific hierarchical factorization improves long-horizon credit assignment or reduces error propagation compared to flat models.\n- The optimal number of scales K appears task-dependent but lacks principled guidance—no connection to trajectory length, reward sparsity, or environment dynamics is established.\n- The condition loss L_cond (Equation 7) only constrains the initial timestep, but provides no guarantee that subsequent states respect environment dynamics or maintain trajectory coherence beyond t=0."}, "questions": {"value": "- Could you please precisely summarize the novel algorithmic contributions of MAGE that are distinct from the VAR architecture?\n- The paper states that the action is determined by a latent inverse dynamics model $a = I(Z)$. The latent representation $Z$ is defined as the set of multi-scale latents $Z = (z_1, \\dots, z_K)$. How are these separate latent vectors aggregated (e.g., concatenation, summation, pooling) to form the single input $Z$ for the model $I$? This seems to be a critical implementation detail that is not specified.\n- Have you performed experiments comparing MAGE (with its latent inverse model) to a simpler variant where the main decoder $\\mathcal{D}$ is also trained to reconstruct actions, e.g., by modeling (R, S, A) triplets as in the ablated scheme from Table 6?\n- What is the specific advantage of using a separate adapter module for this task? Why not apply the $\\mathcal{L}_{cond}$ loss directly to the main decoder $\\mathcal{D}$ and update its parameters? Does the adapter-based approach offer better stability, prevent catastrophic forgetting of the trajectory prior, or lead to better performance?\n- Have the authors experimented with varying the number of scales K, and if so, how does performance change?\n- Is there a plan to release the code or datasets upon acceptance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0mIMeCy9t0", "forum": "32BLpC50V0", "replyto": "32BLpC50V0", "signatures": ["ICLR.cc/2026/Conference/Submission5182/Reviewer_8qyC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5182/Reviewer_8qyC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762701059379, "cdate": 1762701059379, "tmdate": 1762917931285, "mdate": 1762917931285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}