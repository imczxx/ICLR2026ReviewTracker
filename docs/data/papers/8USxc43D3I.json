{"id": "8USxc43D3I", "number": 24750, "cdate": 1758359947011, "mdate": 1763733105495, "content": {"title": "HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games", "abstract": "Large Reasoning Models (LRMs) have demonstrated impressive performance on complex tasks, including logical puzzle games that require deriving solutions satisfying all constraints. However, whether they can flexibly apply appropriate rules to varying conditions, particularly when faced with non-canonical game variants, remains an open question. Existing corpora focus on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats and memorization of solution patterns, which can mask deficiencies in understanding novel rules or adapting strategies to new variants. To address this, we introduce **HardcoreLogic**, a challenging benchmark of over 5,000 puzzles across 10 games, designed to test the robustness of LRMs on the \"long-tail\" of logical games. HardcoreLogic systematically transforms canonical puzzles through three dimensions: **Increased Complexity (IC)**, **Uncommon Elements (UE)**, and **Unsolvable Puzzles (UP)**, reducing reliance on shortcut memorization. Evaluations on a diverse set of LRMs reveal significant performance drops, even for models achieving top scores on existing benchmarks, indicating heavy reliance on memorized stereotypes. While increased complexity is the dominant source of difficulty, models also struggle with subtle rule variations that do not necessarily increase puzzle difficulty. Our systematic error analysis on solvable and unsolvable puzzles further highlights gaps in genuine reasoning. Overall, HardcoreLogic exposes the limitations of current LRMs and establishes a benchmark for advancing high-level logical reasoning.", "tldr": "We propose HardcoreLogic, a logic puzzle game benchmark with non-canonical long-tail puzzles that evaluates the reasoning capability robustness of LLM/LRMs.", "keywords": ["long-tail benchmark", "logic puzzle games", "large reasoning model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ff3867784576a5c216b179e90084457473c226b.pdf", "supplementary_material": "/attachment/94e0448353ccae0cb8f87dd93f8b2209e63e60fc.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors introduce HardcoreLogic, a new benchmark of puzzle and algorithmic reasoning problems that systematically expands the complexity of several well-known puzzle datasets (e.g., Zebralogic, Enigmata, Hanoi game and the others shown in Table 1). In particular, they use the taxonomy of transformations detailed in Section 2.2, which includes increasing problem complexity (via increases to the search space and depth of reasoning), constraint strengthening, modificaitons to question forms, rule variation, and introduce an unsolvable puzzle setting that are not (as I understand) native to any of the existing datasets.\n\nGiven that most datasets are carefully contructed using algorithmic tools like Z3, creating such modifications is not an altogether straightforward task. Figure 2 shows the increase in the search space size of the resulting puzzles, and Figure 3 very nicely illustrates the relative differences (in the green and purple plots) in the numbers of SAT conflicts and decisions (often a proxy for empirical hardness of a problem) between the old and new datasets. Computationally, these problems are indeed considerably more difficult (I'm naturally wondering, however, how such added complexity increases the raw size of the input problem. Perhaps I missed this detail somewhere?)\n\nThey experiment with a comprehensive set of both commerical and open weight large reasoning models (GPT-5, GPT-oss, deepseek, qwen3, etc.) and a single non-reasoning model kimi-k2. Figure 4 shows the aggregate performance of different models across old and new puzzle tasks, where all models suffer significant reductions in performance (some of the particular details here are interesting, such as the relative robustness of the GPT models and Grok). Dataset-level results are shown in Figure 5, which does indicate that the degree of difference is quite mixed across these different tasks. \n\nTo better understand the effects of the particular puzzle transformations on model performance, they fit a linear regression to each puzzle task, the results of which are reported in Figure 6. I find this analysis to be very interesting and unique, as it could provide a recipe for how to create an even harder set of tasks.  They also report some interesting error analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-- A **new set of hard puzzle datasets** that systematically improve the complexity and difficulty of several well benchmarks. I could imagine this dataset being the new standard resource for these class of problems. \n\n-- **Compelling empirical evidence** about the hardness of the tasks, with very interesting analysis concerning the factors that contribute to the increased difficulty and current error cases. \n\n-- A very clear and easy to read paper that will be approachable for those not working directly in this area."}, "weaknesses": {"value": "-- (**minor**) Whenever complexity is increased, especially for transformations involving novel rules (i.e., their *uncommon elements*), one runs the risk of introducing ambiguous or impossible to understand constraints. I'd be curious to know if anything was done to mitigate such issues (e.g., manual analysis of some kind or human evaluation) and would like to have the authors address this."}, "questions": {"value": "-- Did you experiment any anthropic models?\n\n-- My question from above: how does the increase complexity affect the input size of each problem? I worry that making problems too complex might result in puzzles that are very cumbersome and far too verbose to be realistic.  \n\n-- Why did the transformations have such a small effect on the skyscaper tasks (reported in Figure 5)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1hAOSDVZ0p", "forum": "8USxc43D3I", "replyto": "8USxc43D3I", "signatures": ["ICLR.cc/2026/Conference/Submission24750/Reviewer_eXAq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24750/Reviewer_eXAq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761275646140, "cdate": 1761275646140, "tmdate": 1762943183962, "mdate": 1762943183962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduce HardcoreLogic, a challenging benchmark comprising over 5,000 long-tail variants of 10 logical puzzle games, designed to test the robustness and genuine high-level logical reasoning of Large Reasoning Models . To bypass model reliance on memorized canonical patterns, the benchmark systematically transforms puzzles using Increased Complexity, Uncommon Elements, and Unsolvable Puzzles. Evaluations on a diverse set of LRMs, including SOTA models like GPT-5, revealed a significant performance degradation, indicating that current models struggle with less conventional scenarios and often rely on memorization rather than flexible reasoning. The dominant source of difficulty is the Increased Complexity, which tests the model’s ability to handle larger search spaces and greater constraint entanglement."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work introduce a challenging benchmark, which helps evaluate the reasoning and generalization capabilities of LRMs.\n2. The statistics and the analysis on the model responses are informative, which reveals the weaknesses of the LRMs."}, "weaknesses": {"value": "1. The data construction process is not so clear. It is not clear whether datasets are currates manually, rule-based, or by LLMs. How do you guarantee the validity of the transformed questions and answers.\n2. Some important experimental and analysis are also missing. How do you evaluate the correctness of the responses? \n3. For the error analysis, GPT-5 was used to do the classification. It is not clear how robust is such setting. Manual analysis may be necessary for more reliable analysis. In section 4.3, are the analysis conducted by humans or LLMs? How many samples do you use for the analysis?\n4. One important concern of this work is the novelty. The main contribution of this is a transformed benchmark that is more difficult and diverse but still similar to the original benchmark, which limits its novelty."}, "questions": {"value": "When curating the benchmark, it would be great for show more examples so that the readers can understand the details of the data construction process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EkrJDFEZ3y", "forum": "8USxc43D3I", "replyto": "8USxc43D3I", "signatures": ["ICLR.cc/2026/Conference/Submission24750/Reviewer_xWwM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24750/Reviewer_xWwM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489678516, "cdate": 1761489678516, "tmdate": 1762943183641, "mdate": 1762943183641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HardcoreLogic, a new benchmark with 5k puzzles spanning over 10 logic puzzle games designed to probe reasoning models on long-tail of logical. The benchmark is constructed by systematically applying long-tail transformations on exiting logical games/puzzles in primarily three ways: Increased Complexity, Uncommon Elements, and Unsolvable Puzzles, and evaluates a broad set of open and closed models. The study reports substantial accuracy drops relative to “Original” datasets, analyses transformation-induced difficulty and provides analysis on error types and model behaviours for solvable/unsolvable puzzle cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces HardcoreLogic, a valuable new resource for the community. By evaluating on proposed long-tailed transformed logic puzzles with increased difficulty, unknown element, and addition of unsolvable puzzles, the benchmark highlights through their evaluations and error analysis, that a lot of the time the reasoning models rely on their memorised experiences rather than genuine reasoning over the given problem. \n- The inclusion of unsolvable instances to evaluate ability of LLMs reason and comprehend insufficiency of information is uncommon and valuable.\n- The analysis is thorough, investigating performance across different game types, long-tail transformations and models (open/closed), with insightful error analysis on models and why they perform poorly on HardcoreLogic.\n- The paper is well-written, the methodology is sound, and the results are clear and impactful."}, "weaknesses": {"value": "- The qualitative proxies for hardness (e.g., search-space scale, Z3 decisions/conflicts, generated/expanded nodes) are informative, but the paper does not show whether they correlate with what humans perceive hard or LLM error rates. Without such correlations, it is hard to conclude the transformations reliably push puzzles into the “long-tail”.\n- Error analysis lacks reliability and confidence reporting. Although GPT-5 is used as an annotator, the resulting error analysis is hard to trust without inter-annotator agreement or multiple human/LLM annotators. The claims of the benchmark can be vastly improved with improving the error analysis and using human experts (or use of multiple LLM-as-judge models), and proving confidence scores."}, "questions": {"value": "- The paper lack head-to-head comparison of error types between the HardcoreLogic and the original problems. Does factual errors increase in HardcoreLogic vs original? Does Brute-force errors exists in LRMs when solving the original problems?\n- What is Z3? not defined anywhere in the paper.\n- Figure 7 missing label for orange error type."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wyCd96xfGB", "forum": "8USxc43D3I", "replyto": "8USxc43D3I", "signatures": ["ICLR.cc/2026/Conference/Submission24750/Reviewer_izsk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24750/Reviewer_izsk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814691383, "cdate": 1761814691383, "tmdate": 1762943183378, "mdate": 1762943183378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds a new benchmark, HardcoreLogic, to test whether large reasoning models (LRMs) can truly reason or are just memorizing common puzzle patterns. The authors build a dataset of over 5,000 puzzles across 10 different games such as Sudoku, ZebraLogic, and Minesweeper. These are drawn from or aligned with earlier logic-reasoning datasets (like Enigmata, ZebraLogic), but then systematically transformed. Unlike existing datasets, HardcoreLogic includes three types of harder variants: Increased Complexity (IC) – bigger puzzles or denser constraints; Uncommon Elements (UE) – unusual formats or new rules, and Unsolvable Puzzles (UP) – puzzles with no valid answer. When tested on this benchmark, even top-performing models that excel on standard puzzles showed large drops in accuracy, especially when the puzzles were slightly altered or unsolvable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly identifies a real hole in current LRM evaluations: most logic-puzzle benchmarks stay in the “canonical” zone, so they don’t reveal whether models can handle odd, rare, or slightly rule-tweaked versions.\n\n- The benchmark spans 10 games from 6 categories (logic, grid, search, pattern, graph, sequential), so the paper’s claims aren’t tied to a single puzzle formalism. That’s good evidence that the problem is about LRMs’ robustness, not about one idiosyncratic game. It  totals >5,000 puzzles. \n\n-  The experiments directly compare Original vs. HardcoreLogic and show that all models, including SOTA, lose accuracy when puzzles are made longer-tail."}, "weaknesses": {"value": "- Section 2 claims that puzzles are extended along three “orthogonal” dimensions — Increased Complexity (IC: IC1, IC2), Uncommon Elements (UE: UE1, UE2), and Unsolvable Puzzles (UP). **Why are they orthogonal?**\nHowever, Section 4 immediately notes that “puzzles may also have two different long-tail transformation attributes at the same time.”\nThis means the core experimental factor is conflated: when accuracy drops, we cannot uniquely attribute the drop to IC1 vs. UE1, because several knobs were turned at once. Nevertheless, Section 4.1 fits a linear model and makes a fairly strong statement that IC1 “has the greatest comprehensive impact.” That claim seems stronger than the actual design supports. \nOn top of that, the paper doesn’t report confidence intervals or any regularization that would stabilize the estimates, so the single set of coefficients they show looks more definite than it really is.\n\n\n- In Sec. 3, “a generation run is considered correct if and only if the model successfully finishes reasoning and produces a correct answer” under a 32,768-token budget. This policy conflates two different failures: the model logically failed, and the model found the answer but over-generated verification and hit the budget.\nGiven that many LRMs today use self-critique / multi-step CoT, the current metric likely underestimates the real reasoning capability on the hardest instances. A secondary metric (“first correct step,” “correct intermediate state”) would make Section 3 more credible.\n\n- The error analysis in Sec. 4.2 is a nice idea (six failure types), but it is built on a limited set of erroneous responses and on automatic judgments. With such a small pool and no human inter-annotator agreement, fine-grained statements like “stronger models show more brute-force errors” should be treated as hypotheses, not as solid findings. There is no human inter-annotator agreement reported.\n\n- Sec. 2 says UPs “deliberately lack a valid solution” to test whether models detect inconsistency. But the main text does not spell out whether UPs are generated via contradiction, under-specification, or conflicting hybrids. In Sec. 4, however, models are evaluated on whether they “justify” unsolvability. Without a clear taxonomy of UP generation in the main paper, it is hard to know whether model mistakes are due to weak inconsistency detection or to task underspecification.\n\n\n\n\n- Section 3 explicitly says that, due to cost, closed-source models are tested on a small number of cases and that per-game analysis “mainly focuses on open-source models.” Yet the intro-level narrative is still all models, including SOTA, degrade. With such small samples, long-tail variance can dominate; strong statements about closed models should be toned down or supported with stratified sampling."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SnS7TJtOM4", "forum": "8USxc43D3I", "replyto": "8USxc43D3I", "signatures": ["ICLR.cc/2026/Conference/Submission24750/Reviewer_na2W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24750/Reviewer_na2W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980261445, "cdate": 1761980261445, "tmdate": 1762943183137, "mdate": 1762943183137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to reviewers and AC (and thank you for your feedback and supervision!)"}, "comment": {"value": "We are grateful to all reviewers for their valuable feedback and recognition of the significance of our work, including **addressing a critical gap in LRM evaluation** (`na2W`, `izsk`), **providing a valuable and challenging benchmark** (`izsk`, `xWwM`, `eXAq`), **systematic construction and scope coverage** (`na2W`, `eXAq`), **inclusion of unsolvable instances** (`izsk`), **thorough and insightful analysis** (`na2W`, `izsk`, `eXAq`) and **high presentation quality** (`izsk`, `eXAq`).\n\nIn response to the reviewers' comments and questions, we provide rebuttal arguments including (but not limited to):\n\n- Clarification of **transformation dimensions** and **cross-dimensional effects** (`na2W`);\n- Additional analysis regarding **reasoning length constraint** (`na2W`);\n- **Strengthened error analysis** with more details (`na2W`, `izsk`, `xWwM`);\n- Further explanation on unsolvable puzzles and results on closed-source model (`na2W`);\n- Introduction to **complexity metrics** and their **relation to empirical difficulty** (`izsk`);\n- Explanation of **data construction** and **output evaluation** procedures (`xWwM`);\n- **Detailed emphasis on the novelty of our work** (`xWwM`);\n- Quantitative analysis of **transformation comprehensibility** (`eXAq`);\n- Clarification of the experiment results on Skyscraper (`eXAq`).\n\nWe hope these responses address your key concerns, and we welcome any further feedback or clarification requests, for which we are glad to provide necessary additional information."}}, "id": "eoZdxqRzL9", "forum": "8USxc43D3I", "replyto": "8USxc43D3I", "signatures": ["ICLR.cc/2026/Conference/Submission24750/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24750/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission24750/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763732591762, "cdate": 1763732591762, "tmdate": 1763732591762, "mdate": 1763732591762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}