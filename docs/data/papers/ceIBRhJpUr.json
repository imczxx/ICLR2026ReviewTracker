{"id": "ceIBRhJpUr", "number": 10292, "cdate": 1758166306875, "mdate": 1759897660242, "content": {"title": "$\\mathbf{Li_2}$: A Framework on Dynamics of Feature Emergence and Delayed Generalization", "abstract": "While the phenomenon of grokking, i.e., delayed generalization, has been studied extensively, it remains an open question whether there is a mathematical framework to characterize what kind of features emerge, how and in which conditions it happens from training, for complex structured inputs. We propose a novel framework, named $\\mathbf{Li_2}$, that captures three key stages for the grokking behavior of 2-layer nonlinear networks: (I) \\underline{\\textbf{L}}azy learning, (II) \\underline{\\textbf{i}}ndependent feature learning and (III) \\underline{\\textbf{i}}nteractive feature learning, characterized by the structure of \\emph{backpropagated gradient} $G_F$ across layers. In (I), $G_F$ is random, and top layer overfits to random hidden representation. In (II), the gradient of each node (column of $G_F$) only depends on its own activation, and thus each hidden node learns their representation independently from $G_F$, which now carries information about target labels, thanks to weight decay. Interestingly, the independent dynamics follows exactly the \\emph{gradient ascent} of an energy function $\\mathcal{E}$, and its local maxima are precisely the emerging features. We study whether these local-optima induced features are generalizable, their representation power, and how they change on sample size, in group arithmetic tasks. Finally, in (III), we provably show how hidden nodes interact, and how $G_F$ changes to focus on missing features that need to be learned. Our study sheds lights on roles played by key hyperparameters such as weight decay, learning rate and sample sizes in grokking, leads to provable scaling laws of memorization and generalization, and reveals the underlying cause why recent optimizers such as Muon can be effective, from the first principles of gradient dynamics. Our analysis can be extended to multi-layer architectures.", "tldr": "We study the gradient dynamics of grokking for 2-layer networks and proposes a mathematical framework to explain feature learning process.", "keywords": ["grokking", "gradient dynamics", "generalization", "memorization", "modular addition", "scaling laws"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5b8f3521548f8cf965a5ac069b7fedfbc7b56c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a three-stage framework to theoretically study grokking using two-layer neural network with MSE loss and quadratic activation functions. It find connections between stage II and the optimization of an energy function. It rigorously analyzes the role of learning rate, weight decay, sample size, and proposes a scaling law of generalization/memorization boundary. It also theoretically analyzes the benefits of Muon optimizer in the feature learning regime."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The first work to rigorously analyze the role of learning rate, weight decay, and width on grokking dynamics.\n2. Interesting and novel analysis of the interactive feature learning regime.\n3. First theoretical analysis of Muon in the feature learning regime.\n4. The theoretical results of two types of memorization is intriguing."}, "weaknesses": {"value": "1. MSE loss is uncommon in the training of deep learning models, though it's fine for the ease of theoretical analysis.\n2. The theory needs a nonzero weight decay to provably show the three phases, while in practice, weight decay is unnecessary for grokking to occur.\n3. Several assumptions need to be justified."}, "questions": {"value": "1. In stage I, why the activations F are mostly unchanged? Any empirical observations to support such assumption? I guess it needs assumptions on the learning rate, weight decay, and initialization scale of V.\n2. In Lemma 1, why can we assume W always follow normal distribution at each step? Besides that, W is assumed to follow normal, but in the proof (line 714-716), you assume w_i follow N(0, I), which is stronger.\n3. In Lemma 1, why can we assume $<x_i, x_{i'}> = \\rho$ ? Does this assumption hold in any synthetic tasks that show grokking?\n4. In Theorem 5, what are focused and spreading memorization? What's the difference between memorization and overfitting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VfFiROjr46", "forum": "ceIBRhJpUr", "replyto": "ceIBRhJpUr", "signatures": ["ICLR.cc/2026/Conference/Submission10292/Reviewer_aGxx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10292/Reviewer_aGxx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203337635, "cdate": 1761203337635, "tmdate": 1762921644922, "mdate": 1762921644922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a 3-stage framework to understand the emergence of feature learning in large-width two-layer neural networks, named **Li$_{2}$**. This consists of:\n\n1. Lazy regime: first-layer weights are effectively random. The second layer fits the data using random features. The back-prop term to the hidden layer, $GF$, carries little usable structure and, without weight decay, can vanish at the lazy fixed point (ridge solution).\n2. Independent: With weight decay \\(\\eta>0\\), \\(GF\\) acquires target structure. Under some assumptions and large width \\(K\\), it simplifies to\n$$\nGF = \\frac{\\eta}{(Kc_1+\\eta)(nc_2+\\eta)}\\tilde Y\\tilde Y^{\\top}F,\n$$\nso each neuron follows gradient **ascent** on the single-neuron energy\n$$\nE(w)=\\frac{1}{2}\\big\\|\\tilde Y^{\\top}\\sigma(Xw)\\big\\|_2^2,\n$$\ni.e., neurons learn useful directions independently.\n3. Interactive: As several features are learned, neuron–neuron interactions reshape $GF$: similar features repel, and the signal is steered toward missing directions. \n\nThis is discussed explicitly in a modular arithmetic task and quadratic activation $\\sigma(x)=x^{2}$. Using group-theoretical tools, it is shwon that local maxima of $E$ align with group irreducible representations which for this task coincide with Fourier modes, yielding closed-form descriptions of learned features and their attained energies $E^\\star$. \n\nFinally, an extension to the multi-layer case is also discussed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and provides a fairly generic picture of the mechanism for feature learning in two-layer neural networks, which is task independent and rely only on an investigation of the gradients."}, "weaknesses": {"value": "The results for each section rely on different assumptions, which makes **Li$_{2}$** look more like different patches of results rather than a unified picture. Some of the observations appearing in this work have also appeared in other works in the feature learning literature, and more throughout comparison is lacking. I expand on these two points below."}, "questions": {"value": "1. Different results in this work appear to rely on different assumptions about $n,d,K,M$, and it is not immediately clear whether these are mutually consistent. For instance, Lemma 1 assumes that $K$ is sufficiently large and that $x_i^\\top x_j=\\rho$, which is only possible when $n<d$ unless the $x_i$ are degenerate. By contrast, Theorem 2 and Corollary 1 take $d=2M$ and $n=M^2$, which is consistent with Lemma 1 only for $M<2$. Similarly, Theorem 4 requires $n\\gtrsim d_k^{,2} M \\log(\\delta/\\delta)$ via Matrix Bernstein. Overall, I found it confusing to determine whether the regimes considered for the different phases of Li$_2$ hold simultaneously.\n\n2. Feature learning for two-layer neural networks has been studied extensively in recent years, with several works arriving at a picture closely related to **$Li$_2$**. For example:\n\n- One line of work systematically analyzes one-pass SGD dynamics in teacher–student settings (a 2LNN learning another, not necessarily identical, 2LNN). These studies show that the dynamics decompose into plateau / saddle-to-saddle phases: first-layer weights move within a neighborhood of zero (the *mediocrity phase*), then individual neurons correlate with target directions independently before finally coalescing (the *specialization phase*); see Saad and Solla (1995), Goldt et al. (2019), and Arnaboldi et al. (2023). While these works differ technically (finite-width networks, one-pass SGD), the overall mechanism is closely related, with a key difference being the absence of an overfitting phase under one-pass SGD.\n\n- Another closely related line of work considers feature learning during the first few steps of GD with aggressive learning rates (Damian, 2022; Ba et al., 2022; Dandi et al., 2024). The energy in Eq. (8), often called the ``Correlation loss,’’ is a common approximation in this literature because it yields exact weak-recovery thresholds for the initial gradient steps. A notable observation is that, after a single aggressive step, the gradient can be asymptotically characterized—depending on sample complexity—by a rank-one matrix that correlates with the target, enabling the network to express nonlinear components with limited data. Is this the same mechanism as in Theorem 3?\n\n- More recently, Montanari and Urbani (2025) studied a related teacher–student setting for full-batch GD on a wide 2LNN learning a single neuron, and identified three timescales: a lazy timescale, a generalization timescale, and an overfitting timescale (motivating early stopping). (There is no specialization here because the target has a single neuron.) In Li$_2$, overfitting is associated with the lazy phase. How can this be reconciled with Montanari and Urbani (2025)? How should we understand the benefits of early stopping within the Li$_2$ framework?\n\n**References**\n\n- (Saad and Solla 1995) Dynamics of On-Line Gradient Descent Learning for Multilayer Neural Networks. NeurIPS 1995.\n- (Goldt et al. 2019) Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. NeurIPS 2019\n- (Arnaboldi et al. 2023) From high-dimensional & mean-field dynamics to dimensionless ODEs: A unifying approach to SGD in two-layers networks. COLT 2023\n- (Damian 2022) Neural Networks can Learn Representations with Gradient Descent. COLT 2022.\n- (Ba et al. 2022) High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation. NeurIPS 2022.\n- (Dandi et al. 2024) How Two-Layer Neural Networks Learn, One (Giant) Step at a Time. JMLR 2024\n- (Montanari and Urbani 2025) Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks. arXiv 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6gZeGtrD5O", "forum": "ceIBRhJpUr", "replyto": "ceIBRhJpUr", "signatures": ["ICLR.cc/2026/Conference/Submission10292/Reviewer_NQrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10292/Reviewer_NQrJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579410641, "cdate": 1761579410641, "tmdate": 1762921644498, "mdate": 1762921644498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes grokking dynamics in the presence of regularization. They identify three phases in the learning process - lazy learning regime and independent and interactive feature learning regime, and prove various theorems which govern different aspects of the training dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The theorems proved are mathematically rigorous, with detailed proofs in the appendices. \n2) The scaling law analysis in Section 5.4 will be useful since it gives a first-principles derivation of the scaling phenomenon.\n3) Extensions to modern optimizers and deeper networks were adequately discussed."}, "weaknesses": {"value": "1) My main concern is that the key observation (that there is a lazy and rich learning regime) was already reported in [1], who study this in the context of polynomial regression. While the current paper offers mathematically rigorous proofs, the distinction with [1] needs to be more elaborately discussed in the main text.\n2) The mathematical framework presented displays the three stages, but fails to offer more insight on what drives these transitions or when do these transitions. For example, based on lines 471-475, it seems like it's the (inverse) learning rate which sets the scale for when these transitions occur, but it would be great if this could be explained in more detail.\n3) How does one relate the top-down modulation in Sec 6 to the continuous feature learning observed in [2]?\n4) Discussion on limitations not found in main text.\n\n\n[1] Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, and Cengiz Pehlevan. Grokking as the transition from lazy to rich training dynamics, 2024. \n[2] Gromov, A. (2023). Grokking modular arithmetic. arXiv preprint arXiv:2301.02679."}, "questions": {"value": "1) Line 105-106 : I suggest modifying \"grokking mostly happens ... regularization\" to \"grokking is accelerated ... regularization\"\n2) Why are the axes cut off till epoch 300 in Fig 2 last column?\n3) Since the discussion on Thm 3 involves optimization in the complex domain, were any follow-up experiments done with complex weights?\n4) The existence of maxima for the ascent functions is interesting (Thms 4-5). What can be said about the network's ability to find these maxima?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ahK71Ntm8y", "forum": "ceIBRhJpUr", "replyto": "ceIBRhJpUr", "signatures": ["ICLR.cc/2026/Conference/Submission10292/Reviewer_sYi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10292/Reviewer_sYi6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675298795, "cdate": 1761675298795, "tmdate": 1762921644093, "mdate": 1762921644093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **Li²**, a mathematical framework to explain grokking (delayed generalization) in two-layer nonlinear networks via the structure of the back-propagated gradient matrix (G_F). Training is decomposed into three phases: **(I) Lazy learning**, where (G_F) is effectively random and the top layer overfits random hidden representations; **(II) Independent feature learning**, where each hidden unit learns independently because each column of (G_F) depends only on its own activation—weight decay injects label signal, the dynamics become exact gradient ascent on an energy (E), and the local maxima of (E) coincide with the emergent features; and **(III) Interactive feature learning**, where hidden units begin to interact and (G_F) reorients toward missing features that must be acquired for generalization. On group-arithmetic tasks, the authors analyze when these energy-induced features generalize, their representational power, and how they vary with sample size. The framework yields **provable scaling laws** for memorization and generalization as functions of weight decay, learning rate, and data size, and offers a first-principles explanation for the effectiveness of optimizers such as **Muon**. The analysis is argued to extend to deeper architectures."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is overall solid and offers a detailed study of the dynamics of two-layer neural networks.\n2. It provides experiments that validate the theoretical results."}, "weaknesses": {"value": "1. The writing feels very rushed: many symbols are undefined or unexplained, making the paper hard to follow and overly dense.\n2. The theoretical setup is quite restricted; for example, a projection function is deliberately designed so that the hidden layer receives gradients that are random noise.\n3. It is unclear whether the group (arithmetic) example pertains only to Stage II or also extends to Stage III.\n4. The theoretical analysis of Muon appears disconnected from the main body of the paper, and the setup and assumptions of Theorem 8 are entirely unclear.\n5. The relationships among the three stages are not well articulated; the analysis reads like heuristic case-by-case treatment rather than a genuinely unified three-stage dynamics analysis.\n6. The abstract mentions scaling laws, but it is not evident where in the paper this is actually developed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aCOq4joPpA", "forum": "ceIBRhJpUr", "replyto": "ceIBRhJpUr", "signatures": ["ICLR.cc/2026/Conference/Submission10292/Reviewer_LnxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10292/Reviewer_LnxJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903642028, "cdate": 1761903642028, "tmdate": 1762921643670, "mdate": 1762921643670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}