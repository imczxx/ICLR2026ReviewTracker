{"id": "vd4bU3uq8a", "number": 10979, "cdate": 1758186079165, "mdate": 1759897616845, "content": {"title": "Which Formal Languages Can Large Language Models Learn in Context?", "abstract": "In-context learning (ICL) drives much of the practical utility of large language models (LLMs), but its limitations---particularly on tasks requiring algorithmic reasoning---lack a precise characterization. Theoretically, transformer networks with unlimited chain-of-thought tokens in their output should be able to simulate any learning algorithm, but recent work has found that LLMs fall far short in practice. In this paper, we contribute to the growing body of work examining this discrepancy by evaluating the ICL capabilities of several LLMs (ChatGPT, DeepSeek, Qwen, and Llama) on a suite of formal language recognition tasks, which provide a controlled testbed for assessing reasoning ability grounded in the theory of computation. Our experiments span a range of language classes, namely regular, deterministic context-free, context-free, and context-sensitive languages. Bearing in mind recent work showing that a transformer network’s expressive power increases with the number of padding tokens in its input, we test several ways of encoding exemplars that result in varying numbers of input tokens. To test the role of chain-of-thought, we also test prompts that require the model to produce an output immediately after reading the input and prompts that permit unrestricted reasoning before a label is produced. We find that pretrained LLMs perform very poorly on these reasoning tasks in all cases, only successfully learning the language of binary strings that begin with a 1. Also, contrary to expectation, adding padding and chain-of-thought tokens does not consistently improve accuracy. Still, ICL with pretrained LLMs is consistently more accurate than training a small transformer from scratch on the same data, suggesting that pretraining imbues transformers with a learning mechanism that is at least more sample efficient than training from scratch. These results reveal a disconnect between theoretical models of transformer capacity and the practical behavior of LLMs in ICL. Our code is publicly available.", "tldr": "We test the in context learning abilities of a range of modern LLMs on a wide range of formal languages.", "keywords": ["in-context learning", "chatgpt", "deepseek", "llama", "formal languages"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7994b508d62cccb49002d8057834dccca8bc310a.pdf", "supplementary_material": "/attachment/f37334937b1a8f98f7045cafc8db37032891c074.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an empirical evaluation of in-context learning for formal language recognition. The authors investigate the performance of various models, including pretrained large language models and Transformers trained from scratch. The study concludes that current LLMs exhibit significant weaknesses in this domain. Furthermore, the paper explores the impact of different tokenization schemes and chain-of-thought reasoning, finding that neither strategy yields consistent accuracy improvements. The core contribution is an empirical characterization of this limitation in modern LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental setup is applied across a diverse set of language models, including both open- and closed-source systems. This breadth strengthens the generalizability and credibility of the findings.\n\n2. The paper identifies a task domain (formal language recognition) where a gap exists between current theoretical results and empirical performance."}, "weaknesses": {"value": "1. Given its submission to the learning theory track, the paper's contribution would be substantially strengthened by the inclusion of theoretical analysis. The current work is purely empirical, which may not align with the expectations for this area. The authors might consider either adding theoretical analyses or reframing the paper's contribution.\n\n2. The finding that pretrained LLMs struggle with a specialized task like formal language recognition is not unexpected. A significant limitation of the current study is the omission of experiments on finetuned models. Evaluating the in-context learning capabilities of LLMs after finetuning on this task would provide a much stronger test of their limitations and significantly enhance the paper's contribution.\n\n3. The exposition of the string encoding strategies in Section 4.1 would benefit from significant clarification. As written, the descriptions are difficult to follow and appear to contain inconsistencies.\n\n   - The distinction between the \"one-to-one\" and \"many-to-one\" strategies in Table 1 is unclear.\n\n   - There appears to be an inconsistency in Section 4.1.1. Line 242 states that symbols are mapped to \"distinct single-character tokens,\" yet the \"many-to-one\" example in Table 1 shows a three-character token. This also seems to conflict with the description on lines 246-247 (\"potentially allowing multiple symbols to be tokenized as a single token\").\n\n4. The \"one-to-many\" encoding strategy (Section 4.1.3) raises methodological concerns. The introduction of random filler tokens ($\\delta$) is a highly unconventional approach that diverges significantly from standard pretraining or testing procedures. It is unclear what the motivation for this strategy is, and it may introduce confounding variables that make the results difficult to interpret. The paper would be strengthened by either a clearer justification for this method or by replacing it with a more standard tokenization approach.\n\n5. Lines 298-301 appear to have a copy-paste error: \"if $y=0$ or $y=0$\" and \"if $y=1$ or $y=1$\"."}, "questions": {"value": "1. Could the authors comment on the omission of finetuning experiments? As noted in the weaknesses, this seems to be a critical missing piece for evaluating the true capabilities of LLMs on this task.\n\n2. Could the authors provide a thorough revision of Section 4.1 to clarify the string encoding strategies? This should include resolving the apparent inconsistencies between the text (e.g., Line 242) and the examples (Table 1).\n\n3. What is the hypothesis behind the \"one-to-many\" encoding strategy? How do the authors account for the potential impact of the random filler tokens and the train-test tokenization mismatch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "is0PgL9wbk", "forum": "vd4bU3uq8a", "replyto": "vd4bU3uq8a", "signatures": ["ICLR.cc/2026/Conference/Submission10979/Reviewer_opGK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10979/Reviewer_opGK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581582969, "cdate": 1761581582969, "tmdate": 1762922172946, "mdate": 1762922172946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work evaluates four families of LLMs on formal language recognition task using the FLaRe benchmark, under the in-context-learning (ICL) setting. They investigated the impact of input padding length and the role of chain-of-thought. The results showed that with ICL, current LLMs are more sample-efficient than transformers trained from scratch. Increasing the input or output budget does not guarantee consistent ICL accuracy gain."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: investigate this discrepancy between the theoretical headroom and actual performance of ICL.\n2. Various experimental design to investigate the impact of input padding length and prompt strategies.\n3. Extensive experiments across 4 families of LLMs of different sizes."}, "weaknesses": {"value": "1.\tIn Sec 4.1, the motivation behind is that adding input padding tokens may improve performance, as stated in the abstract. It seems this point is not supported by any references.\n2.\tIn Table 2, each accuracy is in fact the highest accuracy among all the settings (input decoding strategy, prompting strategy). The performance comparison between different models is not under the **same** setting.\n3.\tThe result analysis seems incomplete: it lacks more fine-grained level analysis within the Chomsky hierarchy. There is no qualitative analysis."}, "questions": {"value": "1.\tI find that even for humans (such as myself), it is difficult to find the answers in Examples E.1 and E.2, because I am not aware of the rules or patterns of this specific formal language. The same applies to an LLM. Therefore, my question is: in your experiments, the LLMs were not provided with an explanation of the given formal language—under this setting, what is the implication of the model’s performance?\n\n2.\tIn Sec 4.1, the one to many encoding strategy does add input computing budget, but it also makes the question more complex (e.g. the instance in Table 1, to correctly understand, the model must consider the two tokens “a” and “_p” together as a single symbol). Does this strategy introduce additional bias that may degrade model performance (i.e. more computing power but the model gets more confused)?\n\n3.\tA trivial question: the motivation about this paper includes evaluating the reasoning ability of LLMs. Why not evaluate recent large reasoning models, such as DeepSeek-R1, OpenAI o1?\n\n4.\tIn Sec 5, the ICL example size is 100. Is it a design choice? Have you compared with other choices?\n\n5.\tIs there any specific reason that the authors conduct the experiments on a single FLR benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rsU01paV3c", "forum": "vd4bU3uq8a", "replyto": "vd4bU3uq8a", "signatures": ["ICLR.cc/2026/Conference/Submission10979/Reviewer_aYf9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10979/Reviewer_aYf9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811737630, "cdate": 1761811737630, "tmdate": 1762922172597, "mdate": 1762922172597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors evaluate the performance of ICL on a variety of existing formal language benchmarks that capture canonical problems from various levels of the Chomsky hierarchy. They obfuscate these problems via symbol permutation and vary both symbol encoding and whether the models are acting in chain-of-thought. They find that these LLMs cannot perfectly solve all but one of the tasks, and in general, substantially underperform the best performance an even much smaller transformer can provide if trained on the underlying data distribution. They find that, however, this ICL generally outperforms training a transformer from scratch on just the examples provided to ICL. Additionally, they find, in contradiction of high level heuristics commonly believed in the field of LLM prompting, that on these tasks COT has limited effect, and that reducing the number of tokens in the input appears to improve performance generally."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The overall approach of this paper is precise and directed at solving the problem it poses. It has a clear, formal description of both the problem and experiments.\n\nThe comparison to both a small transformer train from scratch on the exact data and a larger sample representing the data distribution is interesting and provides two distinctly helpful points of comparison.\n\nOne -> many and many -> one are interesting points of comparison."}, "weaknesses": {"value": "In the abstract and introduction, it is stated that “ICL with pretrained LLMs is consistently more accurate than training a small transformer from scratch on the same data” and “pretrained LLMs are markedly more sample-efficient than transformers trained from scratch on the same data.” The second of these accurately and precisely describes the result, however, the first of these has the (probably unintentional) effect of implying that the small transformers are trained in a standard way on the same data distribution, rather than trained in an unusual way (with only 100 samples) on the same exact data points. The fact that training transformers from scratch with many examples outperforms ICL should also be discussed more prominently in the introduction, to properly contextualize this finding. Training on 100 examples is extremely unusual and it is unsurprising this performs quite poorly. [To be clear, I acknowledge that situations with only 100 examples are common, motivating the use of ICL and making this a valid comparison to include in the paper; this weakness is entirely in phrasing and the omission of the 10000 sample experiment from the introduction.]\n\nNo reasoning models are included in this evaluation, and the only proprietary model included is the relatively weak gpt-4o-mini. The inclusion of gpt-5-mini or o3-mini might be helpful as a point of reference.\n\nA baseline that might be useful is something similar to Ayurek 2024 [1]. This involves training models on the structure of formal language ICL tasks. You might do that, or perhaps just train a small transformer on formal language tasks in general rather than one specific task, then fine-tune on one specific task. This would allow you to isolate the degree to which 100 examples are insufficient for a small transformer to learn the language, or whether training a transformer from scratch requires more than 100 examples in general.\n\nMinor:\n113-116: Gupta 2025 does investigate recognition via the Transducer task (where the task is to classify a sequence as belonging to a language or not based on its prefixes); the more relevant difference seems to be the difference in languages (randomly sampled regular languages vs a richer but non-randomly sampled set of more canonical languages from throughout the Chomsky Hierarchy). This means “previously unstudied angle: in-context formal language recognition” from earlier in the introduction should be rephrased.\n\nA discussion of [1] should be included in related work.\n\nTables: a lack of error bars on the numbers makes it difficult to determine whether certain comparisons are meaningful or not.\n\n[1]: Akyürek, Ekin, et al. \"In-context language learning: Architectures and algorithms.\" arXiv preprint arXiv:2401.12973 (2024)."}, "questions": {"value": "The result about many -> one being superior to one -> one and one -> many is striking; do you have any hypotheses on why this might be the case? I have seen some prior work with similar (though unsystematic) results on how “better” tokenization makes this kind of task worse (e.g., the Commas appendix of Gupta 2025), but have never found a compelling reason as to why."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1f7r6j3Tow", "forum": "vd4bU3uq8a", "replyto": "vd4bU3uq8a", "signatures": ["ICLR.cc/2026/Conference/Submission10979/Reviewer_o9G9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10979/Reviewer_o9G9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845241232, "cdate": 1761845241232, "tmdate": 1762922172010, "mdate": 1762922172010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the ICL capabilities of LLMs in learning formal languages. By considering different string encoding and prompting strategies, the authors conduct extensive empirical experiments across the Chomsky hierarchy. The results suggest that the success of ICL in recognizing formal languages stems from better approximation rather than greater computational power."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The experiments are extensive, covering various string encoding strategies, prompting methods, languages across the Chomsky hierarchy, and multiple models.\n\n- The findings provide useful insights into the mechanism of LLM ICL, suggesting that LLMs might not perform actual computation and learning."}, "weaknesses": {"value": "- Although the empirical study is extensive, the paper lacks in-depth analysis of the results. For instance, while different models show varying performance across languages, the paper does not discuss why a certain model outperforms others for one language but underperforms for another.\n\n- The paper does not offer a theoretical explanation for the mechanisms underlying the observed empirical results."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UC2KTOgY5y", "forum": "vd4bU3uq8a", "replyto": "vd4bU3uq8a", "signatures": ["ICLR.cc/2026/Conference/Submission10979/Reviewer_Dj3D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10979/Reviewer_Dj3D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898590810, "cdate": 1761898590810, "tmdate": 1762922168766, "mdate": 1762922168766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}