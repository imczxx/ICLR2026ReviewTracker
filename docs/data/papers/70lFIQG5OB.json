{"id": "70lFIQG5OB", "number": 4992, "cdate": 1757829302370, "mdate": 1763691452624, "content": {"title": "Language-guided Open-world Video Anomaly Detection under Weak Supervision", "abstract": "Video anomaly detection (VAD) aims to detect anomalies that deviate from what is expected. In open-world scenarios, the expected events may change as requirements change. For example, not wearing a mask may be considered abnormal during a flu outbreak but normal otherwise. However, existing methods assume that the definition of anomalies is invariable, and thus are not applicable to the open world. To address this, we propose a novel open-world VAD paradigm with variable definitions, allowing guided detection through user-provided natural language at inference time. This paradigm necessitates establishing a robust mapping from video and textual definition to anomaly scores. Therefore, we propose LaGoVAD (**La**nguage-**g**uided **O**pen-world **V**ideo **A**nomaly **D**etector), a model that dynamically adapts anomaly definitions under weak supervision with two regularization strategies: diversifying the relative durations of anomalies via dynamic video synthesis, and enhancing feature robustness through contrastive learning with negative mining. Training such adaptable models requires diverse anomaly definitions, but existing datasets typically provide labels without semantic descriptions. To bridge this gap, we collect PreVAD (**Pre**-training **V**ideo **A**nomaly **D**ataset), the largest and most diverse video anomaly dataset to date, featuring 35,279 annotated videos with multi-level category labels and descriptions that explicitly define anomalies. Zero-shot experiments on seven datasets demonstrate LaGoVAD's SOTA performance. Our dataset and code will be released.", "tldr": "", "keywords": ["video anomaly detection", "weakly-supervised", "multimodal"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a611aae496851f7db4052849e12262ee166655dd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LaGoVAD, a framework for video anomaly detection (VAD) in open-world scenarios, where anomaly definitions can change dynamically according to user-specified natural language input. Also, the authors introduce PreVAD, a dataset with multi-level anomaly categories and textual descriptions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: The paper reformulates VAD as a mapping Φ:(V, Z)→Y, explicitly modeling dynamic anomaly definitions to address concept drift.\n\n- Quality: Extensive zero-shot and drift@5 protocols on seven datasets demonstrate SOTA results across both detection and classification metrics (Tabs. 2–4).\n\n- Clarity: The paper is well organized.\n\n- Significance: Dynamic video synthesis and contrastive loss with hard negative mining jointly improve generalization under weak supervision. PreVAD introduced by this paper is a large and semantically rich VAD dataset with 35,279 videos and detailed anomaly descriptions. Users can specify anomalies in natural language, allowing adaptive definitions at inference time."}, "weaknesses": {"value": "- Limited theoretical justification of concept-drift handling. While Eq. (4) formalizes the dependency on Z, there is no quantitative analysis showing explicit mitigation of Ptrain ≠ Ptest beyond empirical gains.\n- The cross-modal analysis is weak. The fusion mechanism is relatively simple, and its contribution is not isolated in ablations beyond $L$dvs / $L$neg.\n- The dataset annotation reliability is questionable. No quality metrics are presented."}, "questions": {"value": "- Can authors provide with quantitative analysis of concept-drift mitigation?\n- Can authors discuss and test failure cases where user-provided definitions are ambiguous or contradictory?\n- Can LaGoVAD work in real-time video streams where the user updates or changes the anomaly definition while the system is running?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "- The paper does not specify any anonymization, consent, or privacy-preserving mechanisms in dataset collection or model deployment. \n\n- The Ethics Statement in the paper acknowledges possible misuse but lacks detailed mitigation guidelines. \n\n- The PreVAD dataset includes human-labeled and MLLM-generated annotations for video anomalies but provides no details about annotator compensation or consent."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bFmBaPk5hH", "forum": "70lFIQG5OB", "replyto": "70lFIQG5OB", "signatures": ["ICLR.cc/2026/Conference/Submission4992/Reviewer_vsXK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4992/Reviewer_vsXK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733846204, "cdate": 1761733846204, "tmdate": 1762917812106, "mdate": 1762917812106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a language-guided open-world video anomaly detection framework that dynamically adapts to user-defined anomalies via natural language. The authors propose LaGoVAD, a model incorporating dynamic video synthesis and contrastive learning with hard negative mining, and contribute PreVAD—a large-scale dataset with semantic anomaly descriptions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel formulation of concept drift as a conditional modeling problem.  \n- Well-designed regularization strategies to combat overfitting.  \n- Comprehensive evaluation across 7 datasets with a dedicated concept drift protocol.  \n- PreVAD is a significant contribution to the community."}, "weaknesses": {"value": "1. **Limited Evaluation of Prompt Robustness**: The model is only tested with manually curated, category-level prompts. Its performance under noisy, ambiguous, or open-domain user descriptions—common in real-world use—remains unverified.\n\n2. **Semantic Consistency in Video Synthesis**: The dynamic video synthesis module relies solely on CLIP-based global features for segment retrieval. This may lead to semantically inconsistent videos under domain shifts (e.g., real-world vs. synthetic data), potentially undermining the regularization effect. The paper lacks analysis of synthesis quality or failure cases in cross-domain settings.\n\n3. **Risk of Noisy Hard Negatives**: The contrastive learning strategy samples hard negatives from normal segments within anomaly videos. However, in cases of ambiguous normal-abnormal boundaries (e.g., “pedestrian on road”), these segments may be mislabeled, introducing noise into the contrastive objective. The impact of such false negatives is not discussed.\n\n4. **Architectural Simplicity**: While the model effectively instantiates the proposed paradigm, its architecture remains relatively simple. More sophisticated multimodal fusion mechanisms or temporal modeling approaches could further enhance performance."}, "questions": {"value": "1. How does the contrastive learning module handle potential mislabeling of hard negatives in ambiguous scenarios?  \n2. Has any prompt normalization or robustness strategy been considered to handle diverse or noisy user inputs?  \n3. Could finer-grained retrieval mechanisms (e.g., action-based or scene-graph matching) improve semantic coherence in synthesized videos under domain shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w61vfXvWON", "forum": "70lFIQG5OB", "replyto": "70lFIQG5OB", "signatures": ["ICLR.cc/2026/Conference/Submission4992/Reviewer_BtGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4992/Reviewer_BtGb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750184702, "cdate": 1761750184702, "tmdate": 1762917811807, "mdate": 1762917811807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of open-world video anomaly detection (VAD) under weak supervision, focusing on the critical and practical issue of concept drift—where the definition of anomalies evolves over time and across scenes. The main contributions include a newly proposed large-scale video anomaly dataset and two novel training strategies. The dataset is characterized by its use of language-based, variable definitions of anomalies. The authors also introduce two training techniques: dynamic video synthesis and contrastive learning with negative mining."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The problem addressed is both interesting and important in the field of VAD.\n3. The experiments are comprehensive and the zero-shot performance demonstrated is promising."}, "weaknesses": {"value": "1. The evaluation under Protocol 2 is incomplete. As currently designed, Protocol 2 only considers concept drift in one direction—where abnormal events in the pre-training dataset (PreVAD) may shift to normal. The reverse scenario, in which normal events become abnormal, is not evaluated. It would be beneficial to include this additional evaluation.\n2. While the two proposed regularization strategies are technically sound, both video splicing and hard negative mining have been extensively explored in prior work. More importantly, the connection between these strategies and the issue of concept drift, which is a key characteristic of the proposed datase, remains unclear.\n3. The performance improvements are not substantial. As shown in Table 4, LaGoVAD only achieves marginal gains over VadCLIP in terms of AP on XD-drift@5 and AUC on MSAD-drift@5."}, "questions": {"value": "1. What training set was used for VadCLIP in Table 4? Was VadCLIP pre-trained on XD-violence or PreVAD?\n2. Did you experiment with textual prompt ensembling during inference? How does the performance differ when using class names versus anomaly descriptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fXrJbHoTIs", "forum": "70lFIQG5OB", "replyto": "70lFIQG5OB", "signatures": ["ICLR.cc/2026/Conference/Submission4992/Reviewer_2UdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4992/Reviewer_2UdC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753106877, "cdate": 1761753106877, "tmdate": 1762917811225, "mdate": 1762917811225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a language-guided open-world video anomaly detection framework named  LaGoVAD, which enables models to adapt to dynamically changing definitions of anomaly. Unlike traditional VAD methods that assume fixed anomaly categories, LaGoVAD conditions detection on both video content and user-provided textual definitions, effectively addressing the problem of concept drift. The model employs two key strategies—dynamic video synthesis for data diversification and contrastive learning with hard negative mining for robust cross-modal alignment. To support training, the authors introduce PreVAD, a large-scale dataset with 35K videos and fine-grained textual annotations. Extensive zero-shot experiments across seven datasets show that LaGoVAD achieves state-of-the-art performance in both cross-domain and dynamic definition scenarios, advancing the field toward user-controllable, open-world anomaly detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes a significant contribution to the field of video anomaly detection by addressing one of its fundamental challenges — the context-dependent and evolving nature of anomaly definitions in real-world, open-world scenarios. Instead of assuming a fixed anomaly category, the authors propose a novel language-guided paradigm that allows dynamic redefinition of anomalies through natural language, effectively tackling the long-standing problem of concept drift.\n2. This paper introduces the PreVAD, a large-scale, multi-domain dataset with rich textual annotations and hierarchical labeling. This dataset fills an important gap in current VAD research, where existing datasets are often limited in scale, diversity, and semantic granularity.\n3. The proposed baseline model, LaGoVAD, is well-designed and demonstrates strong empirical performance on both the new dataset and multiple public benchmarks. Its integration of dynamic video synthesis and contrastive learning with hard negative mining represents a thoughtful and effective combination that improves cross-modal alignment and temporal robustness."}, "weaknesses": {"value": "1. In the current era of Vision-Language Models, the problem of definition drift can often be effectively mitigated by prompt engineering and leveraging pretrained multimodal knowledge from large models. Recent training-free approaches such as LAVAD, SUVAD, and VERA have already explored similar directions, showing that dynamically redefining anomalies through prompts is feasible without additional training. Although the authors explicitly avoid using large language models, the deliberate omission or insufficient discussion of these related works is problematic and weakens the positioning of this paper within the existing research landscape.\n\n2. The proposed framework, while conceptually sound, remains relatively simple in design. Its core components—language conditioning, dynamic video synthesis, and contrastive alignment—are incremental extensions of existing VAD or multimodal alignment methods rather than substantial breakthroughs. As a result, the framework provides only limited innovation in addressing the core challenge of definition drift, offering more of an engineering adaptation than a novel theoretical advancement.\n\n3. The paper claims that modeling the mapping $\\Phi :(V,Z)\\rightarrow Y$ can “effectively avoid concept drift,” but provides no theoretical analysis or formal guarantees to support this statement. It remains unclear whether this approach eliminates concept drift or merely alleviates it empirically. Moreover, the paper does not analyze model behavior when the test-time definition $Z_{test}$ is semantically distant from those seen during training, which is crucial for validating true open-world generalization.\n\n4. While the introduction of the PreVAD dataset is commendable, the paper does not explain how annotation consistency and hallucination control were ensured when using large language models or human-in-the-loop annotation processes. Without a transparent validation protocol, the reliability and semantic precision of the textual anomaly descriptions remain uncertain."}, "questions": {"value": "1. As pointed out in the weaknesses, in the era of powerful Vision-Language Models, definition drift can often be addressed through prompting and leveraging pretrained multimodal knowledge, as shown in recent training-free approaches such as LAVAD, SUVAD, and VERA. Since the proposed task setup overlaps with the goals of these works, could the authors clarify why these methods were not discussed or compared, and explain what conceptual or empirical advantages LaGoVAD offers over such VLM-based prompt-driven solutions?\n2. The paper claims that explicitly modeling the mapping $\\Phi :(V,Z)\\rightarrow Y$ can effectively avoid concept drift, yet this statement is not theoretically or empirically justified. Could the authors elaborate on whether this modeling truly eliminates concept drift or merely mitigates it in practice, and provide further analysis on how the model behaves when the test-time definition $Z_{test}$ is semantically distant from training-time definitions $Z_{train}$?\n3. Regarding the construction of the PreVAD dataset, the paper does not provide sufficient information about how annotation consistency and reliability were maintained, especially if large language models or automated tools were involved. Could the authors clarify what steps were taken to ensure annotation coherence, prevent hallucinated descriptions, and validate the semantic accuracy of the generated textual labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ApN8JTCsZb", "forum": "70lFIQG5OB", "replyto": "70lFIQG5OB", "signatures": ["ICLR.cc/2026/Conference/Submission4992/Reviewer_yCS1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4992/Reviewer_yCS1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915195625, "cdate": 1761915195625, "tmdate": 1762917810434, "mdate": 1762917810434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}