{"id": "lAd8vjQhQG", "number": 7757, "cdate": 1758034900888, "mdate": 1759897834377, "content": {"title": "Reasoning Capabilities of Large Language Models in Dynamic Games of Imperfect Information: A Case Study on Dou Dizhu", "abstract": "The performance of Large Language Models (LLMs) in dynamic games of imperfect information, which demand deep strategic reasoning, remains an under-explored area. This paper investigates this challenge using the popular card game Dou Dizhu as a representative testbed, aiming to enhance the reasoning and decision-making abilities of LLMs in such complex scenarios. First, we establish a rigorous and fair benchmark using a duplicate round-robin tournament to comprehensively evaluate the performance of several state-of-the-art LLMs. This evaluation provided a clear performance baseline and revealed that while these top-tier models are powerful, their significant computational cost motivates the development of smaller, more efficient alternatives. Furthermore, we propose a novel data construction framework designed to bridge the information gap. Its core consists of two unique data curation mechanisms tailored for such games: globally optimal decision alignment via symmetric information and real-time in-game feedback augmentation. By fine-tuning a smaller-scale model on a structured curriculum—comprising this curated data alongside victorious game data—we demonstrate a significant enhancement in gameplay proficiency. The model exhibits a substantially reduced decision error rate and a strategic robustness that significantly outperforms baseline models. Code is available in the Supplementary Material.", "tldr": "We fine-tune a small LLM on a novel dataset, curated via multi-agent feedback and \"post-hoc validation\" with perfect information, significantly enhancing its strategic foresight and reducing decision errors in dynamic games of imperfect information.", "keywords": ["Large Language Models (LLMs)", "Imperfect-Information Games", "Strategic Reasoning", "Curriculum Learning", "Multi-agent Systems"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9556a131bdad9bbe28c8327911d0d34f53b7434b.pdf", "supplementary_material": "/attachment/39e6cac3ab3e2404f789dd09366608e14703eddf.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies LLM strategic reasoning in a dynamic imperfect-information game (Dou Dizhu). The authors implement a duplicate round-robin tournament framework with a \"duplicate score\" metric (which directly reflects how good players are). They propose a data-centric training recipe for a small student model that was distilled from DeepSeek R1 and GLM-4.5 internal gaming records with the \"Globally Optimal and Feedback-Augmented\" (GOFA) strategies: (1) Globally Optimal: post-hoc symmetric-information re-evaluation (\"God’s-eye view\") and (2) Feedback-Augmented: real-time teammate/opponent feedback. Their results show that the small distilled model (Qwen3‐4B‐GOFA) exhibits exceptional improvements across the three dimensions of performance, efficiency, and reliability against Qwen counterparts."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The metric design is controllable: the duplicate format and round-robin seating reduce variance and position bias. The final \"duplicate score\" metric across all deals reflects better decision-making rather than luck.\n2. They include an ablation comparison with the triangular duel, showing continuous gains and error reductions (Base → Victorious → GOFA).\n3. The duplicate score for the 4B student model improves significantly with fewer errors and more efficient outputs, providing a low-cost training pipeline to improve small models in gaming environments compared to RL or self-play."}, "weaknesses": {"value": "1. **Unclear and Unsubstantiated Evaluation Metrics.**\nThe definition and presentation of the evaluation metrics are unclear, high-level, and conceptual. Section 3.1 (Evaluation Metrics) introduces \"Score\" as the first metric, but this metric is never used or reported in the experimental results. All quantitative analyses in Section 4 rely solely on “Average Duplicate Score”, which itself is never formally defined.\nFor the \"Duplicate Score\", although the authors briefly describe a “duplicate-round” setup and provide Figure 2 as an illustration, neither the figure nor the metric is mathematically specified or explained. The absence of a clear formula and the illustration of figures make the evaluation ambiguous, weakening the empirical rigor of the entire study.\n\n\n2. **Missing Strong Baselines and Cross-Section Comparisons.**\nThe paper claims the first fair benchmark but does not compare to strong Dou Dizhu agents from RL (e.g., DouZero[1]). Furthermore, Section 4.1 evaluates teacher models against existing SOTA models, but Section 4.2 evaluates only the student model against other Qwn counterparts, with no overlap or cross-comparison between the two groups. Therefore, I cannot determine where the trained models stand relative to existing SOTA models. The current results only show that the student model surpasses some of its Qwen counterparts, but its absolute performance level is unclear. \n\n\n3. **Distribution-Dependent and Non-Comparable Evaluation Metric.**\nThe “Duplicate Score” metric has a fundamental limitation: It measures only relative performance rather than absolute capability. This score depends on the strength distribution of the opponents, so the values are not globally comparable across different model sets. Therefore, I think this metric (together with weakness 2) cannot support claims of “expert-level parity”. For benchmarking, it needs a more meaningful and reproducible evaluation metric. For example, the author can use a fixed standardized baseline model and evaluate all other models against it. Or simply include all pairwise matchups among evaluated models, but this is unscalable and computationally expensive.\n\n\n[1] DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning. Zha et al."}, "questions": {"value": "See weaknesses.\n\n1. Could you provide a formal mathematical definition of the Average Duplicate Score metrics?\n\n2. How do you ensure comparability of Duplicate Scores across tournaments with different opponent sets?\n\n3. Can you provide results comparing Qwen3-4B-GOFA directly against teacher or SOTA baselines?\n\n4. Have you compared your models against RL-based Dou Dizhu agents such as DouZero or human expert data?\n\n5. Could you elaborate on the GOFA filtering process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EnWkhl7ZTl", "forum": "lAd8vjQhQG", "replyto": "lAd8vjQhQG", "signatures": ["ICLR.cc/2026/Conference/Submission7757/Reviewer_fnX9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7757/Reviewer_fnX9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760860770584, "cdate": 1760860770584, "tmdate": 1762919801582, "mdate": 1762919801582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to enhance the strategic reasoning capabilities of Large Language Models (LLMs) in the Dou Dizhu scenario. Overall, it constructs rollout data using expert LLMs, filters the data through two mechanisms: globally optimal decision alignment via symmetric information and real-time in-game feedback augmentation, and finally fine-tunes the LLM with this curated data to improve its performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method demonstrates certain effectiveness."}, "weaknesses": {"value": "* Constructing game environment rollout data with expert models and then fine-tuning smaller LLMs to boost performance is a fairly common practice, lacking innovation.\n* Even though the paper proposes methods for filtering data and improving data quality, the overall innovation is still low. Approaches like curriculum learning are also well-established, and no uniqueness tailored to the Dou Dizhu scenario is observed.\n* While the paper indeed conducts comprehensive work around data construction, its innovation falls far short of the requirements for ICLR."}, "questions": {"value": "1. The benchmark evaluation uses only 200 unique deals across 20 tournaments, which appears insufficient for a high-variance card game to establish statistically robust conclusions. The paper provides no confidence intervals, p-values, or statistical significance tests to validate that the observed performance differences are not due to random chance. Given the stochastic nature of card distribution and the relatively small sample size, the reliability of the duplicate score rankings remains questionable.\n2. The paper compares exclusively against general-purpose LLMs but omits comparisons with specialized Dou Dizhu AI systems like DouZero, which the authors cite as achieving superhuman performance through deep reinforcement learning. Without benchmarking against state-of-the-art specialized agents, it is impossible to assess whether the proposed approach truly achieves \"expert-level\" performance or merely represents the best performance within the limited LLM paradigm. This omission significantly weakens the claims about the model's strategic capabilities.\n3. The evaluation framework entirely excludes human players of varying skill levels, making claims about \"expert-level reasoning\" unsubstantiated. The paper provides no empirical evidence comparing their best model (Qwen3-4B-GOFA with +17.25 duplicate score) against amateur, intermediate, or professional human Dou Dizhu players. This absence is particularly problematic given the authors' acknowledgment in Section 6 that their model's performance against \"the broader, and at times deeply irrational, spectrum of strategies exhibited by human players remains to be further validated.\"\n4. The ablation study only compares three coarse-grained configurations (baseline, victorious data, full GOFA) without isolating the individual contributions of the two core mechanisms: globally optimal decision alignment and real-time feedback augmentation. The paper provides no experiments showing the impact of using only one mechanism versus the other, making it impossible to determine which component is more critical or whether both are necessary. This limits understanding of the framework's key drivers and prevents principled refinement of the methodology.\n5. The validation set comprises only 1,000 samples (~2.4% of the 41,884 GOFA samples), which may be insufficient to reliably guide model selection and early stopping for such a complex strategic task. Additionally, the paper lacks critical training details such as the number of training epochs, convergence curves, overfitting analysis, and the specific checkpoint selection criteria beyond \"highest decision accuracy on validation set.\" The claim of training for \"approximately one epoch\" is vague and raises concerns about reproducibility and whether the model was adequately trained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qKGfkkhZk0", "forum": "lAd8vjQhQG", "replyto": "lAd8vjQhQG", "signatures": ["ICLR.cc/2026/Conference/Submission7757/Reviewer_eiBc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7757/Reviewer_eiBc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537553901, "cdate": 1761537553901, "tmdate": 1762919801027, "mdate": 1762919801027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tests LLM strategy in Dou Dizhu using a duplicate tournament, GOFA data (God’s-eye check + in-game feedback), and a simple curriculum to boost Qwen3-4B. They claim better decision error rate and stronger robustness than baselines, but there exists flaws in design and conclusion."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Focusing on imperfect-information games fills a gap. Most prior LLM reasoning research targets perfect-information tasks or simple NLP tasks, while dynamic multi-agent imperfect-information scenarios are more representative of real-world strategic decision-making.  \n2. The \"duplicate round-robin\" format is a thoughtful attempt to measure skill rather than luck, which aligns with the need for fair LLM evaluation in stochastic games.  \n3. Fine-tuning a small 4B-parameter model instead of relying on large, costly LLMs like GPT-5 addresses a practical constraint relevant for deploying strategic AI agents."}, "weaknesses": {"value": "1. The paper’s curriculum assumes \"easy-to-hard\" learning is optimal, but lacks an ablation control group. Without this control, it is impossible to verify:  Whether skipping the first stage leads to convergence failure;  or whether direct GOFA fine-tuning is better.  \n2. The duplicate score lacks statistical significance analysis. For example, the paper reports a score jump from -65.80 to 17.25 on Qwen3-4B-GOFA, but nostandard deviation. No analysis of round-to-round stability. \n3. The construction way of validation set leads to a self-referential bias. The model’s decision accuracy is defined as alignment with GOFA annotations, but training and validation data share the same distribution. This only measures how well the model fits the training data, not how well it generalizes to unseen scenarios.\n4. The \"real-time in-game feedback\" relies on LLMs to score decisions, but no calibration or consistency checks are performed: no disclosure of whether the scoring LLMs were trained on human-annotated good/bad decision samples; no cross-LLM consistency analysis; no clarity on scoring dimensions.\n5. The God’s-eye view validation is fundamentally misaligned with real Dou Dizhu gameplay, also no quantification of this mismatch.\n6. The paper claims the data framework \"can be extended to other imperfect-information games\"but provides no evidence, since the key mechanisms are Dou Dizhu-specific.  \n7. The paper does not explain how to extract core decision tokens from LLM outputs:  \n- Is structured output enforced via prompts? Or is keyword matching used?  \n- No quantification of how extraction methods affect loss calculation"}, "questions": {"value": "1. Will you add an ablation group that fine-tunes Qwen3-4B directly on GOFA data?\n2. Can you provide 95% confidence intervals,standard deviation, and p-values for the duplicate scores of all models?\n3. Will you collect an independent human expert validation set  and report the model’s accuracy on this set?\n4. Can you disclose (1) how scoring LLMs were calibrated; (2) cross-LLM consistency; (3) specific scoring dimensions?  \n5. What is the overlap between \"God’s-eye optimal decisions\" and \"top human players’ decisions under local information\"? If overlap is low, how will you adjust the GOFA framework to align with real imperfect-information gameplay?  \n6. Can you conduct a small-scale experiment? \n7. Can you detail the method used to extract decision tokens from LLM outputs? How different extraction methods affect model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5TeYPqSbKg", "forum": "lAd8vjQhQG", "replyto": "lAd8vjQhQG", "signatures": ["ICLR.cc/2026/Conference/Submission7757/Reviewer_rUtg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7757/Reviewer_rUtg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728527267, "cdate": 1761728527267, "tmdate": 1762919800515, "mdate": 1762919800515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors investigates this challenge using the popular card game Dou Dizhu as a representative testbed, aiming to enhance the reasoning and decision-making abilities of LLMs in such complex scenarios. First, the authors establish a rigorous and fair benchmark using a duplicate round-robin tournament to comprehensively evaluate the performance of several state-of-the-art LLMs. This evaluation provided a clear performance baseline and revealed that while these top-tier models are powerful, their significant computational cost motivates the development of smaller, more efficient alternatives. Furthermore, the authors propose a novel data\nconstruction framework designed to bridge the information gap. Its core consists of two unique data curation mechanisms tailored for such games: globally optimal decision alignment via symmetric information and real-time in-game feedback augmentation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "First, the authors establish a rigorous and fair benchmark using a duplicate round-robin tournament to comprehensively evaluate the performance of several state-of-the-art LLMs. This evaluation provided a clear performance baseline and revealed that while these top-tier models are powerful, their significant computational cost motivates the development of smaller, more efficient alternatives. \n\nSecond, the authors propose a novel data construction framework designed to bridge the information gap. Its core consists of two unique data curation mechanisms tailored for such games: globally optimal decision alignment via symmetric information and real-time in-game feedback augmentation."}, "weaknesses": {"value": "1. Using Dou Di Zhu to evaluate LLMs has some flaws: Only 3 LLMs can be involved in a single round, therefore, comparing more than 3 LLMs is difficult\n2. Some related papers are not cited, such as Empowering LLMs in Decision Games through Algorithmic Data Synthesis, LLM-Based Explicit Models of Opponents for Multi-Agent Games, uno arena for evaluating sequential decision-making capability of large language models."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tGmwelnJ6D", "forum": "lAd8vjQhQG", "replyto": "lAd8vjQhQG", "signatures": ["ICLR.cc/2026/Conference/Submission7757/Reviewer_X6Dt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7757/Reviewer_X6Dt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968839589, "cdate": 1761968839589, "tmdate": 1762919799915, "mdate": 1762919799915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}