{"id": "8zzfTSVds2", "number": 24015, "cdate": 1758351710787, "mdate": 1759896786401, "content": {"title": "Lightweight Spatio-Temporal Modeling via Temporally Shifted Distillation for Real-Time Accident Anticipation", "abstract": "Anticipating traffic accidents in real time is critical for intelligent transportation systems, yet remains challenging under edge-device constraints. We propose a lightweight spatio-temporal framework that introduces a temporally shifted distillation strategy, enabling a student model to acquire predictive temporal dynamics from a frozen image-based teacher without requiring a video pre-trained teacher. The student combines a RepMixer spatial encoding with a RWKV-inspired recurrent module for efficient long-range temporal reasoning. To enhance robustness under partial observability, we design a masking memory strategy that leverages memory retention to reconstruct missing visual tokens, effectively simulating occlusions and future events. In addition, multi-modal vision-language supervision enriches semantic grounding. Our framework achieves state-of-the-art performance on multiple real-world dashcam benchmarks while sustaining real-time inference on resource-limited platforms such as the NVIDIA Jetson Orin Nano. Remarkably, it is 3-7$\\times$ smaller than leading approaches yet delivers superior accuracy and earlier anticipation, underscoring its practicality for deployment in intelligent vehicles.", "tldr": "A lightweight, real-time accident predictor trained via novel temporally shifted distillation, combining efficient spatial encoding and recurrent temporal modeling, running on edge devices.", "keywords": ["lightweight spatio-temporal modeling", "model distillation", "accident anticipation", "edge deployment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b12eb30c0c29196fbe0053e2a8d2a501e51456f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a lightweight spatio-temporal framework for real-time traffic accident anticipation, particularly suited for deployment on resource-constrained edge devices. The key contributions are: (1) a novel temporally shifted distillation strategy that enables a student model to learn predictive temporal dynamics from a frozen image-based teacher, eliminating the need for a video pre-trained teacher and making it ideal for small datasets and low-resource settings; (2) a hybrid architecture that combines RepMixer spatial encoding with an RWKV-inspired recurrent module for efficient long-range temporal reasoning, achieving low computational complexity; (3) a masked memory strategy that simulates occlusions and partial visibility to improve robustness under real-world conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper innovatively redefines temporal learning through temporally shifted distillation, removing the need for video-pretrained teachers and enabling real-time performance under constraints.\n2. Comprehensive experiments and ablation studies show improvements in mAP and mTTA with fewer parameters.\n3. Clear structure, well-explained components, and visual aids make the paper accessible, though some concepts remain high-level."}, "weaknesses": {"value": "1. The stitching method of the spatio-temporal scanning pattern seems somewhat arbitrary. There might be continuous information between feature sub-blocks that is not utilized. Is this an existing design approach?\n\n2. In lines 231-236, I don't understand why only KV is involved in the calculation and not the Q parameter.\n\n3. In lines 241-260, can the masked part be considered multi-stage, requiring multiple training iterations? Does this introduce higher training costs compared to other models? Is this reflected in the paper?\n\n4. Lines 264-269: Are there applications on larger-scale datasets? I would like to know how this lightweight model performs under larger-scale conditions. Even if the performance isn't strong, can you provide the model's applicability boundary?\n\n5. Table 4: The performance on the CCD dataset seems close to overfitting. Has the lightweight version been applied to other methods? Also, are there additional evaluation metrics? Are the two metrics used sufficient?"}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OGrkFQA5kD", "forum": "8zzfTSVds2", "replyto": "8zzfTSVds2", "signatures": ["ICLR.cc/2026/Conference/Submission24015/Reviewer_bqJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24015/Reviewer_bqJU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787860368, "cdate": 1761787860368, "tmdate": 1762942899121, "mdate": 1762942899121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight dash-cam accident-anticipation model that distills future-aware temporal cues from a frozen MobileCLIP teacher using a windowed RWKV student and masked memory. It addresses the problem of anticipating accidents several seconds earlier than prior work while staying real time on edge devices with only RGB inputs. Experiments show that the porposed method obtains 75.3% mAP and 4.04s mTTA on DAD with 26M params, beating heavier SOTA (in Table 4)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The reviewer found the idea of distilling from an image-only MobileCLIP into a temporal RWKV student for accident anticipation to be interesting.\n- Interestingly, temporal-only distillation outperforms spatial-only (74.1% vs 71.2% mAP in Table 2), showing future supervision actually drives anticipation.\n- Experiments are comprehensive: Table 1 (RWKV depth), Table 2 (distillation parts), Table 3 (mask ratio), Figure 4 (qualitative attention) and writing is mostly clear and structured."}, "weaknesses": {"value": "- The temporally shifted distillation still does not show a direct comparison to distilling from a video teacher., so it is unclear how much is due to the shift itself.\n- Experiments miss an ablation on modern, purely temporal CNN/Transformer baselines that work on DAD/CCD datasets even though Table 4 already cites many frame-level systems."}, "questions": {"value": "- Please respond to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2oLXJdIwOM", "forum": "8zzfTSVds2", "replyto": "8zzfTSVds2", "signatures": ["ICLR.cc/2026/Conference/Submission24015/Reviewer_7qEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24015/Reviewer_7qEU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977549030, "cdate": 1761977549030, "tmdate": 1762942898761, "mdate": 1762942898761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a lightweight framework for accident anticipation using temporally shifted distillation (TSD).\nA student model learns to predict future cues by aligning its current features with a teacher’s representation from later frames.\nThe design combines a RepMixer backbone with an RWKV-based temporal block for efficiency.\nExperiments on DAD and CCD show improved early anticipation and real-time performance on edge devices."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe idea of learning temporal prediction through shifted distillation is original and intuitively appealing.\n\n2.\tThe method achieves a good balance between accuracy and efficiency, which makes it suitable for real-world deployment.\n\n3.\tThe paper is clearly written, and the model architecture and ablation design are well presented."}, "weaknesses": {"value": "1.\tThe analysis of the temporally shifted distillation is limited.\nTable 2 only reports overall performance, so it is difficult to tell whether the model actually learns to anticipate future events or only captures static correlations.\nAdditional experiments showing temporal behavior, such as feature alignment across time, would strengthen the claim.\n\n2.\tThe mechanism itself is not deeply analyzed.\nIt would be useful to evaluate how different time shifts affect learning or whether the model improves its understanding of future frames during training.\n\n3.\tSince the teacher model processes only single frames, it is unclear how much of the predictive ability comes from the distillation process compared with the student’s temporal module.\nMore explanation or comparative evidence would clarify this point."}, "questions": {"value": "Have the authors tried using a video-based teacher that already has temporal understanding? It would be interesting to see whether the proposed shift is still necessary in that setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oDiT9zxpU9", "forum": "8zzfTSVds2", "replyto": "8zzfTSVds2", "signatures": ["ICLR.cc/2026/Conference/Submission24015/Reviewer_rGQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24015/Reviewer_rGQA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097562275, "cdate": 1762097562275, "tmdate": 1762942898510, "mdate": 1762942898510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}