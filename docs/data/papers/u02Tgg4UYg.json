{"id": "u02Tgg4UYg", "number": 13807, "cdate": 1758222958553, "mdate": 1759897411339, "content": {"title": "Adaptive Logit Adjustment for Debiasing Multimodal Language Models", "abstract": "Vision-Language Models (VLMs) and Large Multimodal Models (LMMs) have significantly advanced image-to-text generation tasks such as image captioning and visual question answering (VQA). \nHowever, these models often exhibit biases, including attribute misalignment between the generated text and the input image, or the reinforcement of harmful stereotypes.\nExisting debiasing techniques primarily focus on modifying representations at the encoder or decoder level, which can degrade model performance and may be susceptible to bias reintroduction from external sources. In this work, we propose **Adaptive Logit Adjustment (ALA) for Bias Alignment and Neutralization**, a post-hoc debiasing method that operates directly on logits during autoregressive text generation. Unlike prior approaches that modify internal representations, ALA selectively adjusts token probabilities to mitigate biases without distorting essential model outputs. Our approach leverages external classifiers to measure bias misalignment between image and text, applies gradient-based importance analysis to identify bias-inducing tokens, and dynamically refines token probabilities to reduce undesired biases. \nWe evaluate ALA on image captioning and various VQA tasks, demonstrating its effectiveness in mitigating bias while maintaining contextual accuracy. Notably, our approach is applicable to various multimodal architectures in a model-agnostic manner, including VLMs and LMMs, across different tasks that involve autoregressive text generation. Our results show that logit-based debiasing offers a flexible and efficient alternative to existing encoder- and embedding-centric approaches, providing a more practical solution for building fairer multimodal AI systems.", "tldr": "", "keywords": ["Large Multimodal Model", "Fairness", "Image-to-Text", "Logit Adjustment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad26f69f6997ce163ea28006d79f5acb2a3609ef.pdf", "supplementary_material": "/attachment/9c07585d8351caafd8bd2a5393c48176960e37d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Adaptive Logit Adjustment (ALA), a post-hoc debiasing method for VLMs and LMMs. Instead of modifying encoder or decoder representations, ALA directly adjusts token-level logits during autoregressive generation. The method leverages external classifiers (for image and text) to measure bias misalignment (e.g., gender, toxicity) and performs logit correction guided by gradient-based importance scores. Experiments across image captioning and multiple VQA tasks show that ALA effectively mitigates bias while preserving model utility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper suggest post-hoc debiasing method for VLMs and LMMs, which does not require additional retraining or affect internal representation which may lead to large degradation.\n2. Proposed idea is simple but effectively mitigate the bias of VLMs and LMMs.\n3. The paper demonstrates consistent improvements in fairness scores with minimal degradation of utility across diverse datasets."}, "weaknesses": {"value": "1. Results that show the “prompt” baseline has bad fairness score is somewhat interesting. Including experiments with VLM backbones which have stronger instruction following capability, such as Qwen-2.5-VL (or Qwen-3-VL), could strengthen the proposed method.\n2. While the paper claims generality to “large multimodal models,”, the evaluated VQA tasks are limited to the captioning or keyword tagging. I suggest that the authors include experiments on more diverse VQA tasks or moderate their claim.\n3. Minor weaknesses about presentation\n    1. Several core components are deferred to the appendix (e.g., Algorithm 1, analysis about limitation regarding classifier, or definition of evaluation metric D_mean). A brief summary of these in the main text would aid comprehension.\n    2. “Baseline” in figure 4 is ambiguous. It seems to denote the original model. Explicitly stating this would help readers interpret the plots."}, "questions": {"value": "Please refer to weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "37aq6gg1bS", "forum": "u02Tgg4UYg", "replyto": "u02Tgg4UYg", "signatures": ["ICLR.cc/2026/Conference/Submission13807/Reviewer_Jum4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13807/Reviewer_Jum4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754073389, "cdate": 1761754073389, "tmdate": 1762924337241, "mdate": 1762924337241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method for adaptive logit adjustment such that the output generation of an LMM is less biased in the direction of protected attributes. Specifically, for each attribute, a pair of classifiers (one for image and text respectively) are trained to predict the amount of \"bias\" in the input image as well as the current generated text. Given a mismatch in bias between the image and text, a corrective factor is applied to the logits, and the given text is modified such that the generation aligns with the image. \n\nThis method is evaluated on LMMs like LLava and PaliGemma w.r.t multiple datasets including COCO-captions, FACET, and SocialCounterfactuals -- which are also used in previous relevant work.  \n\nResults are shown in terms of fairness-utility tradeoffs, i.e. \"does the model retain its capabilities\". Ablation studies are performed on the adjustment strength hyperparameter."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This work is well written and easy to follow\n\n* I believe this method is a sensible step in the context of previous work. All debiasing methods have an inherent tradeoff with the method they use to localize the bias (internal or logits). As long as the classifier training is reliable, this method seems reasonable. \n\n* I appreciated the comparison of resources needed to run each method examined here, this is important for an inference time method. \n\n* The datasets and models used are appropriate, but I would have appreciated more models for a method that only augments the logits e.g. qwen-vl and/or llama-3.2"}, "weaknesses": {"value": "* My main concern with this method is the reliance on an external classifier. Other previous inference-time methods, specifically model steering are able to more easily compute an adjustment to the target model's behavior no matter what the target attribute is. The need to train a binary classifier may be difficult depending on the deployment setting as well as the acquiring data about the attribute itself. These kinds of inference-time debiasing methods exist primarily because fine-tuning is prohibitively expensive to do well, so the requirement to train a classifier seems like a regression. \n\n* I think absolute counts or base rates are important here in the context of debiasing. Especially given that some models may have very different rates of producing biased text, its useful to know how much biased text is generated vs caught."}, "questions": {"value": "1) How could this approach scale to multiple attributes? Presumably we may want to debias the LMM away from a large set of protected attributes, are there any issues for this method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3wzJ6FWicy", "forum": "u02Tgg4UYg", "replyto": "u02Tgg4UYg", "signatures": ["ICLR.cc/2026/Conference/Submission13807/Reviewer_1bpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13807/Reviewer_1bpB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950522671, "cdate": 1761950522671, "tmdate": 1762924336854, "mdate": 1762924336854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a debiasing technique for VLMs and LMMs called Adaptive Logit Adjustment (ALA) which adjusts token probabilities rather than operating directly on internal model states. Bias misalignment between vision and text is evaluated using classifiers, with a gradient-based method used to identify and adjust probabilities for the most relevant bias-inducing tokens. The proposed ALA method is evaluated across three VQA tasks as well as image captioning. An ablation study on the effect of the hyperparamter introduced by ALA is also conducted."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall the paper is clear, well-written, and easy to follow\n2. To the best of my knowledge, the proposed ALA method is a novel approach to debiasing VLMs and MLLMs\n3. The approach is well-motivated in that it aims to address the pitfalls of existing debiasing techniques (i.e., general performance degradation) by avoiding manipulation of internal model representations.\n4. The experiments cover a decent range of tasks (image captioning + 3 VQA tasks) and datasets (MS-COCO, FACET, SocialCounterfactuals)."}, "weaknesses": {"value": "1. The experimental results are somewhat limited by the fact that only two models are evaluated for each task\n2. Some of the experimental results seem odd and unintuitive. Why does DeAR lead to such a large increase in bias for image captioning? Why does ALA lead to large debiasing effects relative to the baseline for CLIP-CAP but not for BLIP? Additional explanation of these results would be helpful.\n3. VQA-Task-3 aims to measure \"core utility\" of the model by asking directly for identification of gender. It seems like this task should also cover the identification of other attributes such as race, particularly because they are often described with words that have multiple ambiguous meanings (e.g., \"black\", \"white\"). It is important to ensure that probabilities for these words are not being lowered in other contexts where they do not refer to the social attribute."}, "questions": {"value": "1. Lines 327-329 state that the goal of VQA-task-2 is to ensure non-toxicity across all attributes. Shouldn't the goal rather be to ensure there are not differences in the level of toxicity across groups? A model can be toxic but not biased if it is equally toxic for all groups."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nCEYR9Ywh7", "forum": "u02Tgg4UYg", "replyto": "u02Tgg4UYg", "signatures": ["ICLR.cc/2026/Conference/Submission13807/Reviewer_WMmP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13807/Reviewer_WMmP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954564061, "cdate": 1761954564061, "tmdate": 1762924336208, "mdate": 1762924336208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}