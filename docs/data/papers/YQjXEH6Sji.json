{"id": "YQjXEH6Sji", "number": 18105, "cdate": 1758283878000, "mdate": 1759897133163, "content": {"title": "Efficient Generative Models Personalization via Optimal Experimental Design", "abstract": "Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization relaxation, and introduce a statistically and computationally efficient algorithm (ED-PBRL) that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.", "tldr": "ED-PBRL casts human preference query selection as optimal experimental design for general generative   models, giving a convex-relaxed, theory-backed algorithm that, in text-to-image personalization tests,   cuts queries below random exploration", "keywords": ["reinforcement learning", "personalization", "experimental design", "alignment", "rlhf", "generative models", "preference-based rl"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ceb74a59431477917256f74fe82a56339b30f3a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ED-PBRL, a theoretically grounded framework for reward modeling that uses Optimal Experimental Design (OED) to select the most informative human preference queries. It formulates the preference query selection problem as a convex optimization problem that maximizes information about the latent reward function underlying user preferences from the perspective of the Fisher Information Matrix. \n\nThe authors provide theoretical guarantees, including a self-concordant bound on the mean squared error via the Fisher information matrix and global convergence results for discrete generative models. Empirical experiments—on both synthetic data and human-in-the-loop text-to-image personalization—demonstrate that ED-PBRL achieves the same alignment performance with fewer preference queries than random selection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work proposes a new upper bound for MSE between the estimated latent reward model and the ground truth through the Fisher Information Matrix via a self-concordant analysis.\n2. The authors reformulate the intractable FIM maximization objective into a tractable one and use the Frank-Wolfe algorithm to obtain a solution with guaranteed optimality and convergence rate for exploration policies.\n3. Both the synthetic and real-world experiments demonstrate a significant performance boost with the proposed ED-PBRL-guided optimal human query selection."}, "weaknesses": {"value": "1. Policy extraction from state visitation measures is computationally inefficient. In the tabular setting, this is straightforward and computationally cheap. However, for token-level long-trajectory generative settings (which is the practical case for LLMs), the proposed methodology may be computationally intensive and impractical.\n2. The scale of the experiment is limited."}, "questions": {"value": "1. What is the time complexity of the proposed ED-PBRL algorithm using Convex-RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gkxZZ4LyQb", "forum": "YQjXEH6Sji", "replyto": "YQjXEH6Sji", "signatures": ["ICLR.cc/2026/Conference/Submission18105/Reviewer_GKit"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18105/Reviewer_GKit"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761165643431, "cdate": 1761165643431, "tmdate": 1762927876561, "mdate": 1762927876561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ED-PBRL, a framework for efficient personalization of generative models with minimal user feedback. The key idea is to model user preferences as an unknown linear reward function and to use Optimal Experimental Design principles to select the most informative queries for learning this reward. Experiments show that ED-PBRL personalizes text-to-image generation via prompt construction with fewer feedback rounds than random exploration."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper aims to address an important point about designing efficient and tractable experiments to learn preferences."}, "weaknesses": {"value": "- **Clarity and organization could be improved.** Some aspects of the method and experimental setup are difficult to follow from the main text. The setup in Appendix A.1 provides a clearer picture of the trajectory structure and bringing some of that material (perhaps with a small illustrative example or figure) into the main paper could be helpful. As it is currently written, one might initially think the user provides feedback after every token on a narrative description, which seems impractical—an explicit example trajectory could address this.\n- **Simple baseline.** It is not obvious why an MDP is necessary to learn the preferred attributes. E.g., a simple baseline could be to learn the distribution over the vocabulary. When a certain image is preferred, upweight the weights on all the attributes used to generate that image.\n- **Independence assumptions.** The formulation appears to assume that design attributes (e.g., ambient, style, etc.) contribute independently to the final trajectory. In general, such attributes can be correlated—for instance, ambience and lighting or style choices often co-vary in preferences. These are some biases that diffusion models or LLMs may already have, it might be useful to discuss whether the method could take advantage of such correlations.\n- **Human evaluation improvements appear modest.** The held-out accuracy in the human preference experiments seems close to that of random exploration. Some further discussion on potential reasons (e.g., noise in human feedback, reward parameterization) could help clarify how the method can be improved."}, "questions": {"value": "- How does a single trajectory of generation look like? Is there any grounding of the generation so adding new design tokens makes minimal changes to the image with the exception of the required new “design”?\n- Were other forms of rewards considered e.g., representing it with a network?\n- What is the size of the vocabulary? How were the image attributes determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zahJoDtIVA", "forum": "YQjXEH6Sji", "replyto": "YQjXEH6Sji", "signatures": ["ICLR.cc/2026/Conference/Submission18105/Reviewer_mNCP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18105/Reviewer_mNCP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761387910788, "cdate": 1761387910788, "tmdate": 1762927875450, "mdate": 1762927875450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an approach to personalizing text-to-image generative models with an RL based preference learning framework. The motivation is to use optimal experimental design to efficiently search the space of prompts that match user preferences. They present qualitative and quantitative results based on the Stable Diffusion architecture."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper over all is generally well communicated and theoretically grounded.\n- The motivation of efficiently personalizing a model from user feedback is a pressing one."}, "weaknesses": {"value": "- The authors don’t clearly outline information about the user study like how the participants were recruited and how many there were, this should be presented more front and center.\n- Opinion: I am not convinced that searching in the space of prompts is the best use of this kind of method. Preference learning seems like a great potential tool for identifying user preference information that is complementary to a prompt. Why couldn’t the user just write their own prompt rather than answering >50 queries? If the information from your search procedure were complementary to text that it would be more well motivated.\n- Minor weakness: the method does build upon an older text-to-image generation architecture.\n- Writing Opinion: the application setting should be presented earlier in the paper and more clearly. It is not until the last page that any qualitative results relevant to the application are presented, and there is only a single example in the main manuscript. I would advise making a more high-level qualitative figure that presents your method in the context of the application much earlier."}, "questions": {"value": "- Did the authors consider using a different value for K than 4? Are there practical limitations to increasing K? How does this impact the convergence of your method?\n- How many users did the authors use in their study? How were they recruited?\n- Do the collected preferences generalize to new base prompts? Or would a user need to answer queries like this for every prompt? I.e., does this capture a general sense of a user’s “style”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F9KNMzLBpg", "forum": "YQjXEH6Sji", "replyto": "YQjXEH6Sji", "signatures": ["ICLR.cc/2026/Conference/Submission18105/Reviewer_Eas7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18105/Reviewer_Eas7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941221381, "cdate": 1761941221381, "tmdate": 1762927873958, "mdate": 1762927873958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at the problem of query selection for preference learning. A novel approach rooted in optimal experimental design is presented in this paper. The main contributions are (1) upper bound on MSE as a function of Fisher Information Matrix and converting the query design problem into an optimization problem, (2) operationalizing the optimization problem by reformulating the objective to a tractable form and proposing a novel algorithm to optimize it, (3) experiments using a text-to-image generative model with both a synthetic and human-specified reward model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Query selection is an important problem, so the problem motivation is strong. The idea around using optimal experiment design for preference learning also seems interesting. The theoretical bounds for regularized Bradley-Terry model with a generalized linear reward model in terms of the Fisher Information Matrix could be of more general interest. I also appreciate the effort the authors put in conducting a study with human participants."}, "weaknesses": {"value": "My main concerns are regarding the practicality of the proposed approach. The objective function requires computing the state visitation measures, which is very data hungry especially for high-dimensional state spaces which are usually the norm for generative models like LLMs. Hence the statistical guarantees don't scale well with state space. In addition the objective function requires inverting a matrix, which is computationally very expensive for larger reward models. The experiments are somewhat simplistic with simple reward models, and doesn't provide strong evidence for establishing the practicality of the algorithm. I recommend the authors either address these issues or add a section on the practical limitations of the approach."}, "questions": {"value": "I'm not super familiar with optimal experimental design but a quick search for preference learning with optimal experimental design shows the following papers:\n1. Mukherjee, Subhojyoti, et al. \"Optimal design for human preference elicitation.\" Advances in Neural Information Processing Systems 37 (2024): 90132-90159.\n2. Schlaginhaufen, Andreas, Reda Ouhamma, and Maryam Kamgarpour. \"Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design.\" arXiv preprint arXiv:2506.09508 (2025).\n\nPlease include these and any other omitted citations if they are relevant to this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mK7iRWt3dl", "forum": "YQjXEH6Sji", "replyto": "YQjXEH6Sji", "signatures": ["ICLR.cc/2026/Conference/Submission18105/Reviewer_EPjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18105/Reviewer_EPjt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968470272, "cdate": 1761968470272, "tmdate": 1762927873308, "mdate": 1762927873308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}