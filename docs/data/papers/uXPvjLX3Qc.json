{"id": "uXPvjLX3Qc", "number": 433, "cdate": 1756739338130, "mdate": 1759898261145, "content": {"title": "BaryBind: Binding All Modalities via Multimodal Wasserstein Barycenter Space", "abstract": "Multimodal joint representation, which aligns multiple modalities in a shared latent space, has emerged as the foundation of recent multimodal understanding models. To scale beyond two modalities, existing models typically treat a specific modality (e.g., text) as the alignment center, or anchor, to bind other modalities via pairwise contrastive losses. However, the learned joint representation space tends to be sub-optimal and imbalanced, as it may insufficiently exploit the modality-agnostic semantics and holistic geometric structures within multimodal data. In this work, we are motivated by the intuition that multimodal representations arise from different shifts from an underlying modality-agnostic representation space. Based on this, we present **BaryBind**, a multimodal framework that aligns modalities in the multimodal Wasserstein barycenter (WB) space, which inherently models a modality-agnostic distribution by minimizing the average of Wasserstein distances to all modalities. We further construct a barycenter polytope, whose volume serves as a geometric metric for quantifying $n$-modality alignment.  This metric is integrated as a barycenter-anchored volumetric contrastive loss that contrasts the volumes of the $n$-dimensional polytopes, encouraging global alignment of non-anchor modalities to the barycenter while reducing inter-modality gaps. Extensive experiments show that BaryBind delivers more balanced zero-shot generalization performance in downstream tasks, e.g., cross-modal text/video retrieval and classification, highlighting its potential for general multimodal understanding.", "tldr": "We present BaryBind, which aligns all modalities to a Wasserstein barycenter with a volumetric loss for scalable and balanced multimodal learning.", "keywords": ["Multimodal representation learning; Inter-modality balance; Wasserstein barycenter; Video understanding."], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/598dee39c10a05092279d69da42a53d46e8c36c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to generate multimodal representations by aligning different modalities into a common latent space. The proposed method is evaluated on text-to-video (T-to-V), video-to-text (V-to-T), multimodal classification, and retrieval tasks, demonstrating promising performance gains over existing methods such as VAST."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors formulate the problem within a multimodal learning framework that aligns diverse modalities in the Wasserstein barycenter (WB) space and propose three complementary loss functions to effectively optimize the network.\n\n- The method is evaluated on multiple benchmark datasets, showing consistent and competitive performance.\n\n- Comprehensive ablation studies are conducted to assess the contribution of individual components (e.g., each loss term) and to analyze the effect of different experimental settings such as varying the anchor modality."}, "weaknesses": {"value": "- The authors claim that existing models typically fix a specific modality (e.g., text) as the alignment anchor to bind others via pairwise contrastive losses, which limits scalability beyond two modalities. Although BaryBind aims to align all modalities to a shared latent space via distribution matching based on the Wasserstein distance, it still appears to treat the text modality as the anchor through simple MLP mappings. Consequently, the proposed method may still be minimizing the discrepancy primarily between the text modality and others. It is unclear to what extent, BaryBind differs from the existing solutions to address the anchoring issue highlighted in the abstract.\n\n- Missing evaluation on system efficiency during training: The reported experiments were conducted on two NVIDIA A100 GPUs. It would be helpful to provide a direct comparison with existing model to illustrate the trade-off between efficiency and effectiveness.\n\n- The proposed method is not evaluated on text-to-audio (T-to-A) or audio captioning tasks. Since most experiments focus on T-to-V and V-to-T tasks, it would strengthen the paper to demonstrate that the approach generalizes across modalities by including results on text–audio datasets such as Clotho and AudioCaps. Otherwise, it remains unclear whether the generated representations are biased toward certain modalities.\n\n- Some important related works are missing and should be discussed for completeness:\n\nExplore the Limits of Omni-modal Pretraining at Scale, ICCV 2025.\n\nViT-Lens: Towards Omni-modal Representations, CVPR 2024."}, "questions": {"value": "Some additional questions are given below:\n\n- Is the dataset used for training the proposed method identical to that used in VAST? It appears that the current work employs VAST-150K, a subset of VAST-27M. Please clarify the rationale behind choosing this subset.\n\n- It would be beneficial to include a comparison against recent open-source multimodal large language models (e.g., Qwen, Qwen-Audio) to contextualize the proposed model’s performance relative to emerging large-scale multimodal baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AZXriy4tQU", "forum": "uXPvjLX3Qc", "replyto": "uXPvjLX3Qc", "signatures": ["ICLR.cc/2026/Conference/Submission433/Reviewer_kVFT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission433/Reviewer_kVFT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298932783, "cdate": 1761298932783, "tmdate": 1762915519473, "mdate": 1762915519473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Main Concerns and Ongoing Responses"}, "comment": {"value": "Dear Area Chair and Reviewers,  \n\nWe sincerely thank the Area Chair for handling our paper and all reviewers for their time, constructive feedback, and recognition of the novelty, presentation, and contribution of our work.  \n\nWe greatly appreciate the reviewers’ insightful comments, which help us clarify the motivation, enhance the transparency of implementation details, highlight the role of the Wasserstein barycenter, and further validate the effectiveness of our approach.  \n\nAfter carefully reviewing all comments, the main concerns can be summarized as follows:\n\n> **Concern regarding the motivation and the role of the Wasserstein barycenter.**  \nReviewers kVFT and oa7m raised questions about our claim of establishing a modality-agnostic representation space with the barycenter and asked how the barycenter differs from anchor-based methods.\n\n> **Request for clarification on pretraining details and dataset usage.**  \nReviewers kVFT and oa7m requested clearer descriptions of the pretraining procedure and the datasets used to eliminate confusion about the training process and ensure fairness in comparison.\n\n> **Request for additional experimental validation.**  \nReviewers suggested conducting additional experiments and analyses to further demonstrate the advantages and robustness of our method. We find these experimental ideas highly insightful and are actively incorporating them to strengthen our study.\n\nWe are diligently conducting additional experiments and preparing detailed point-by-point responses to address these concerns (e.g., extending evaluation to missing-modality cases, adding comparisons with recent baselines, and visualizing embeddings before and after alignment), which may take some time. We aim to provide comprehensive clarifications, updated results, and extended discussions throughout the ongoing rebuttal and revision process.\n\nFinally, we would like to thank the Area Chair and reviewers again for their time and effort in handling our paper. We will do our best to further strengthen the manuscript through these modifications and align it more closely with ICLR’s expectations.\n\nSincerely,  \nAuthors of the *BaryBind* paper"}}, "id": "cw2MtEMUaT", "forum": "uXPvjLX3Qc", "replyto": "uXPvjLX3Qc", "signatures": ["ICLR.cc/2026/Conference/Submission433/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission433/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission433/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763026071223, "cdate": 1763026071223, "tmdate": 1763026095913, "mdate": 1763026095913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates multimodal alignment by leveraging Wasserstein barycenter concept in Optimal transport theory. The main idea of the proposed method, BaryBind, is matching the Barycenter space derived from multimodal distributions to encourage each modality embedding being aligned to barycenter. Experiments show that the proposed method outperforms baseline methods, showing possible potential to enhance the multimodal alignment method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and organized. Leveraging Wasserstein barycenter space for aligning multimodal embeddings is interesting. The current experimental results are also interesting as only the proposed model with pretraining only one-epoch can surpass other baseline models."}, "weaknesses": {"value": "While the approach that leveraging OT theory looks interesting, I do not see a clear problem statement that the authors tackle and the rationale behind of using Wasserstein barycenter, which makes it seem incremental. Moreover, I am doubt with the experiment settings if it is fair comparison and if it properly shows the effect of the proposed loss functions. Below, I list some of my questions regarding my concerns."}, "questions": {"value": "1. First of all, what is the main challenge or limitations of existing methods? From my understanding, this works tries to make modality-agnostic multimodal alignment as the previous works use anchor. If so, the challenge needs to be formally analyzed.\n1. Constructing Wasserstein barycenter space needs barycenter, where the paper uses anchor-modality embedding for constructing it. This does not match the claim \"modality-agnostic\" as the method somehow biased to anchor modality when it align multimodal embeddings. \n2. Regarding the above, using Wasserstein barycenter space as a contrasting point seems just aligning pair-wise distance from barycenter (some embedding of anchor modality) and all other non-anchor modalities (from eq(5)). This should be clearly compared mathematically and empirically. \n3. I do not see the clear reasoning to use Data-anchor matching (DAM) loss. The paper only states \"To complement the barycenter-based alignment loss...\" Where does the problem behind of \"instance-level supervision that encourages the model to distinguish between matched and mismatched pairs\" come from and why do we need this? This requires clear motivation and analysis.\n4. It seems BaryBind requires full multimodal samples, meaning that we must have all samples for every modality, otherwise BaryBind does not work. This is very important in practice as such multimodal datasets are rare.\n5. Are the baseline methods (e.g., imageBind, languageBind, VAST, etc) trained on the same pretraining dataset with one epoch? Have the author compared baseline methods with the same backbone, the same hyperparameter, but only different in loss function? To fair compare and claim the superiority of the proposed loss, all the experimental setup should be identical. \n6. Time complexity is only studied between the similarity measure. As the BaryBind has other modules, total training time (e.g., batch completion time) should be analyzed too."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3kPbYgWr36", "forum": "uXPvjLX3Qc", "replyto": "uXPvjLX3Qc", "signatures": ["ICLR.cc/2026/Conference/Submission433/Reviewer_oa7m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission433/Reviewer_oa7m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761440929221, "cdate": 1761440929221, "tmdate": 1762915519341, "mdate": 1762915519341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach to multimodal representation learning—BaryBind, whose core idea lies in addressing the issue of imbalanced representation space caused by anchor modality bias in conventional methods. Unlike existing approaches that typically align modalities around a specific anchor modality, BaryBind unifies the representations of different modalities by aligning them into a shared Wasserstein barycenter space. This method innovatively leverages the Wasserstein barycenter as a modality-agnostic semantic center, thereby effectively capturing semantics common to all modalities and achieving more balanced and robust alignment of multimodal representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates significant strengths across several key dimensions:\n\n**Originality & Conceptual Innovation** - The core contribution lies in fundamentally rethinking multimodal alignment objectives. By proposing the Wasserstein barycenter as a modality-agnostic semantic center, it shifts the paradigm from point-based anchoring to distribution-centered alignment. This conceptual breakthrough is further enhanced by the introduction of the barycenter polytope volume as a geometric metric, which transforms abstract alignment notions into computable quantities that naturally capture higher-order interactions beyond pairwise similarities.\n\n**Theoretical Rigor & Technical Foundation** - The work establishes solid theoretical grounding through the dual formulation derivation of the MWB loss (Proposition 1), demonstrating careful mathematical development rather than heuristic design. \n\n**Experimental Validation** - The experimental design stands out for its comprehensive coverage and convincing demonstrations across multiple benchmarks. The evaluation strategy effectively substantiates the method's advantages while maintaining scientific rigor in comparisons with state-of-the-art approaches.\n\n**Presentation & Clarity** - Despite the conceptual complexity, the paper maintains logical coherence and accessibility through well-structured exposition. The introduction successfully frames the limitations of existing approaches and the paper's contributions, while Figures 1 and 3 provide exceptional visual intuition for understanding the core workflow and methodological distinctions from baselines."}, "weaknesses": {"value": "**Theoretical Limitations in Wasserstein Barycenter Approximation** - While theoretically grounded in optimal transport, the practical implementation relies on a lightweight MLP $T_\\theta$ to approximate the mapping to WB space through dual formulation. This parametric approximation raises questions about whether the method truly learns a distribution minimizing Wasserstein distances to all modalities, or merely converges to point estimates $b$ that optimize the specific loss function. The discrepancy between the theoretical WB (a distribution) and the implemented point estimate deserves further validation.\n\n**Geometric Metric Limitations** - The barycenter polytope volume $V$, though innovative as a global alignment measure, presents interpretability challenges. As a scalar quantity, it effectively indicates the degree of misalignment but cannot identify which specific modalities contribute to the problem. Furthermore, its dependence on the number of modalities $n$ limits cross-model comparability, undermining its potential as a universal metric. Normalization strategies such as $V^{1/n}$ could enhance its applicability across different modality configurations.\n\n**Insufficient Experimental Validation** - The experimental scope doesn't fully support the claims of modality-agnostic representation and improved inter-modal interactions. Critical tests for robustness under modality absence (e.g., missing video data during inference) are lacking, which would powerfully demonstrate advantages over anchor-based methods. Additionally, direct evidence for enhanced non-anchor modality interactions remains limited - probing tasks or mutual information analysis between modalities like audio and video under BVC constraints would provide more convincing validation."}, "questions": {"value": "**Comparative Analysis of WB Approximation Methods** - To address the theoretical concerns regarding Wasserstein barycenter approximation, future work should compare the current MLP-based approach with more advanced neural optimal transport mappings, such as those proposed in [Kolesov et al., 2024a] and [Tang et al., 2025]. This comparative evaluation would help validate whether different approximation techniques significantly impact final performance and provide insights into the trade-offs between computational efficiency and theoretical fidelity.\n\n**Normalization Strategies for Polytope Volume Metric** - For the barycenter polytope volume to serve as a universal alignment metric, investigation into normalization methods is essential. Exploring geometric normalization factors like $V^{1/n}$ could enable meaningful comparisons across models with varying numbers of modalities $n$. Developing a standardized normalization approach would enhance the metric's practicality and interpretability in diverse multimodal learning scenarios.\n\n**Enhanced Experimental Validation through Probing Tasks** - To substantiate claims of improved modality-agnostic representation and inter-modal interactions, future experiments should include targeted probing tasks. These could assess model robustness under modality conflicts (e.g., contradictory audio and video signals) or directly quantify cross-modal relationships through feature correlation analysis and mutual information measurements between modalities under BVC constraints.\n\n**References**  \n[1] Kolesov et al. Estimating barycenters of distributions with neural optimal transport. arXiv:2402.03828 (2024a)  \n[2] Tang et al. Baryir: Learning multi-source unified representation in continuous barycenter space for generalizable all-in-one image restoration. arXiv:2505.21637 (2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N2bmNIYSxt", "forum": "uXPvjLX3Qc", "replyto": "uXPvjLX3Qc", "signatures": ["ICLR.cc/2026/Conference/Submission433/Reviewer_EnWo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission433/Reviewer_EnWo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824669950, "cdate": 1761824669950, "tmdate": 1762915519206, "mdate": 1762915519206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BaryBind, a new framework used for aligning multimodalities to a Wasserstein barycenter (WB) space, say, a distribution in latent space that minimizes the average Wasserstein distance to each modality’s latent distribution. In addition, the paper introduces a volumetric metric among the embeddings of the modalities around the barycenter. and proposed a volumetric contrastive‐style loss for ensuring tighter alignment and reducing inter‐modality gaps. Experimental results demonstrate that BaryBind achieves significant performance improvement over baselines on cross-modal retrieval and classification tasks, highlighting the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and Well-Structured: The paper is well-organized, with detailed explanations of the preliminary, intuition, and methodology.\n\n- Interesting Method: The use of optimal transport / Wasserstein barycenters as a latent‐space tool is theoretically interesting.\n\n- Superiority in Alignment: The experimental results demonstrate that the proposed method achieves the best performance on the cross-modal retrieval and classification tasks compared to the baselines."}, "weaknesses": {"value": "- Currently, the MWB, BVC, and DAM loss objective functions are equally weighted in the combined loss, but it remains unclear whether assigning different weights could lead to better performance. A study of this trade-off would provide deeper insight into the relative importance of multimodalities' alignment preferences.\n\n- The paper does not include experimental comparisons with other recent multimodal alignment methods, such as TRIANGLE [1] and GRAM [2]. Including these baselines would provide a stronger empirical validation of BaryBind’s effectiveness.\n\n- The paper would benefit from a more in-depth ablation analysis. While the authors provide clear theoretical intuition and a validation experiment for each proposed component, the empirical section lacks a deeper discussion and interpretation of how these components individually and collectively contribute to the overall performance.\n\n- Figures showing toy embeddings (before/after alignment) would help in visualizing the effect of the volumetric loss. Such visualizations could help demonstrate how embeddings converge toward the Wasserstein barycenter, how inter-modality gaps are reduced, and whether the volumetric constraint indeed promotes tighter alignment.\n\n[1] A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity, NeurIPS 2025\n\n[2] Gramian multimodal representation learning and alignment, ICLR 2025"}, "questions": {"value": "How robust is it to missing modalities, which are common in realistic data? Does the barycenter degrade gracefully if one modality is missing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UBXFuxCcoP", "forum": "uXPvjLX3Qc", "replyto": "uXPvjLX3Qc", "signatures": ["ICLR.cc/2026/Conference/Submission433/Reviewer_XuQB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission433/Reviewer_XuQB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975590495, "cdate": 1761975590495, "tmdate": 1762915519011, "mdate": 1762915519011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}