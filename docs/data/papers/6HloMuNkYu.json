{"id": "6HloMuNkYu", "number": 21101, "cdate": 1758313744240, "mdate": 1759896941871, "content": {"title": "HC2D: Attention-Based Two-Phase Distillation for Transformer Continual Learning", "abstract": "Catastrophic forgetting refers to marked degradation in performance on previously learned tasks after training on new ones, and continual learning aims to mitigate this problem. Many existing works preserve past knowledge by constraining updates within the locally learned representations. However, such locality can hinder the discovery of genuinely novel discriminative cues, thereby intensifying the stability–plasticity dilemma. Inspired by hippocampal–cortical memory theory and the principle of introspection, we propose a novel training framework: Hippocampal-to-Cortical Two-Phase Distillation (HC2D). In Phase I (Pattern Separation), the introspective negative attention regularizer suppresses reuse of the original core peaks but preserves global directional consistency, guiding the student to discover novel, complementary discriminative cues and forming the hippocampal teacher. In Phase II (Cortical Consolidation), we selectively consolidate the hippocampal teacher’s most salient attentional patterns into the cortical backbone via asymmetric distillation without compromising the primary attention distribution. HC2D leverages Vision Transformer–based attention distillation to implement an ``aggressive exploration first, robust consolidation later'' strategy, with virtually no additional inference overhead. Experimental results show that HC2D consistently mitigates catastrophic forgetting and becomes increasingly effective over longer task sequences, offering a biologically inspired and engineering-efficient solution for transformer-based continual learning.", "tldr": "This paper introduces HC2D, a two-phase distillation framework inspired by the hippocampal–cortical memory mechanism.", "keywords": ["Continual Learning", "Vision Transformer", "Attention Distillation", "Pattern Separation", "Cortical Consolidation", "Introspection", "Asymmetric Distillation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45ec55b7220c489a5e712ea3e5130280467f7d4e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses catastrophic forgetting - the phenomenon where neural networks dramatically lose performance on previously learned tasks when trained on new ones. Existing approaches typically use local constraints around old decision boundaries, which limit the discovery of genuinely novel discriminative features. Therefore, this paper introduces HC2D (Hippocampal-to-Cortical Two-Phase Distillation), a novel framework for mitigating catastrophic forgetting in Vision Transformers during continual learning scenarios. The results show consistent improvements on CIFAR-100 and ImageNet-100 benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Biological Inspiration: Successfully translates hippocampal-cortical memory theory into a practical machine learning framework, providing theoretical grounding.\n2. Efficient inference: as mentioned in the paper, there are no additional parameters or computational overhead during inference - the hippocampal teacher is only used during training."}, "weaknesses": {"value": "1. Baseline Selection: The paper compares HC2D against a baseline (DyTox) published in 2021, and disables KL-based distillation without clarifying the reason. It needs to compare with more recent and stronger baselines to validate the effectiveness of the proposed method.\n2. Architecture Limitation: Specifically designed for Vision Transformers - unclear how well the approach would transfer to CNNs or other architectures.\n3. Dataset limitation: only discuss the CIFAR-100 and ImageNet-100 datasets. Why just discuss the ImageNet-100 instead of the full ImageNet dataset? Because of high training overhead or bad performance for a larger dataset? Scalability to larger datasets or more complex scenarios remains unclear.\n4. Hyperparameter Sensitivity: The authors acknowledge \"notable sensitivity to hyperparameter configurations,\" which could limit practical deployment without extensive tuning."}, "questions": {"value": "1. Why did you disable the KL-based distillation in the baseline DyTox? It is better to give more explanation.\n2. Please provide performance comparisons with more recent and stronger baselines (e.g, SAFE[1] and PROOF[2]) or clarify the difficulty for performing the comparison.\n3. How about the performance of the larger-scale dataset or other types of dataset tasks?\n4. It is better to show and discuss the different settings of hyperparameters.\n\n[1] SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning with Pre-Trained Models (NeurIPS 2024)\n[2] Learning without Forgetting for Vision-Language Models (TPAMI 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lFjZBZeYhn", "forum": "6HloMuNkYu", "replyto": "6HloMuNkYu", "signatures": ["ICLR.cc/2026/Conference/Submission21101/Reviewer_N5sz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21101/Reviewer_N5sz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410482663, "cdate": 1761410482663, "tmdate": 1762941260617, "mdate": 1762941260617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a continual learning framework for ViTs called HC2D, inspired by human memory systems. For each new task, it trains in two phases: A temporary Hippocampal Teacher model is trained.  It uses an Introspective Negative Distillation (IND) loss that \"punishes\" the model for using old attention patterns, forcing it to find new ones. Then, backbone model is trained. The authors claim this explore first, consolidate later strategy improves performance without adding inference cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear. Splitting training into exploration  and consolidation is a good analogy to human memory .\n\n2. The complex two-phase process is only for training. The final model used for inference has no extra parameters or speed cost compared to the baseline, which is a practical advantage."}, "weaknesses": {"value": "1. This method is impractical. It requires training twice for every new task. This doubles the total training cost. It needs very high training cost.\n\n2. In Phase II, the model is guided by two conflicting teachers.  The loss from the old teacher forces it to copy the old attention map, while the loss from the new teacher forces it to copy a different, new attention map. It is a conflict and damage the learning.\n\n3. The method adds so many new, sensitive hyperparameters, making the method almost hard to reproduce and use.\n\n4. The entire complex distillation process only applies to the attention maps in the final Transformer block. All earlier layers are frozen during this process. This ignores rich features in shallower layers and is a major bottleneck.\n\n5. The ablation only shows that removing parts of the model hurts performance.  This does not prove that this complex two-phase design is better than a simpler, single-phase method that just combines the same loss terms.\n\n6. Introspective Negative Distillation is just a loss term that penalizes old attention peaks rather than the major techniques that its complex name sounds."}, "questions": {"value": "See weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FN3BFKGRNg", "forum": "6HloMuNkYu", "replyto": "6HloMuNkYu", "signatures": ["ICLR.cc/2026/Conference/Submission21101/Reviewer_Qycd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21101/Reviewer_Qycd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637360902, "cdate": 1761637360902, "tmdate": 1762941259609, "mdate": 1762941259609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hippocampal-to-Cortical Two-Phase Distillation (HC2D), a continual learning strategy designed to address the critical challenge of balancing new knowledge exploration with the prevention of catastrophic forgetting. The proposed method employs a two-phase knowledge distillation process: Pattern Separation and Cortical Consolidation. In the initial Pattern Separation phase, HC2D promotes the exploration of new information through Introspective Negative Distillation (IND) while protecting previously acquired knowledge by freezing core features. The subsequent Cortical Consolidation phase focuses on integrating the newly learned information. This is achieved under the guidance of teacher model. Experimental results across various task-length settings demonstrate that HC2D outperforms baseline methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript provides a clear and compelling motivation for the proposed HC2D algorithm.\n- The concept of masking core features is interesting. It appears to strategically redirect the model's learning focus toward new information while protecting essential, previously learned knowledge.\n- The use of two distinct distillations in the second phase thoughtfully balances the dual objectives of preventing knowledge drift (maintaining stability) and ensuring the effective acquisition of new knowledge (maintaining plasticity)."}, "weaknesses": {"value": "- My primary concern is that the proposed method relies on a rehearsal-based strategy, which inherently increases memory storage costs. More importantly, this approach deviates from the strict continual learning constraint, where access to previous data is typically prohibited. The authors should provide a clear justification for adopting this memory-based design over existing rehearsal-free approaches (e.g., L2P, DualPrompt, SDLoRA). To ensure a fair evaluation of the method’s effectiveness, an additional experiment without the memory buffer should be included.\n\n- Another major issue is that the HC2D framework appears computationally expensive. During training, it requires forwarding each input through the ViT multiple times, which can be costly, especially for larger architectures. Moreover, the method involves cloning model parameters from recent states and includes a second training stage with both student and teacher models. This setup raises concerns about practical efficiency. The authors should provide a more objective evaluation, including metrics such as total training time and GFLOPs, compared to baseline methods during training.\n\n- The paper claims to conduct experiments on ImageNet-100; however, the detailed results are missing from both the main text and the appendix. These results should be explicitly included, as **the current experimental evaluation is limited to a single dataset**.\n\n- The results tables notably omit the forgetting rate metric. Reporting this measure is crucial, as it directly supports the claim that the Positive Distillation component mitigates catastrophic forgetting. Although the paper mentions the inclusion of forgetting analysis, no corresponding figures or results are provided.\n\n- Another point of confusion is that Table 1 reports different accuracies for Transf. Joint across steps, whereas joint training should produce consistent results throughout, as all data are available simultaneously."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nQnD9h2cSd", "forum": "6HloMuNkYu", "replyto": "6HloMuNkYu", "signatures": ["ICLR.cc/2026/Conference/Submission21101/Reviewer_2Cob"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21101/Reviewer_2Cob"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821148865, "cdate": 1761821148865, "tmdate": 1762941259050, "mdate": 1762941259050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes attention-based two-phase distillation for transformer continual learning. The 2 phases are patter separation and cortical consolidation. The paper proposes an interesting and unique idea. However, there are several fundamental issues that need to be addressed; please see the weaknesses."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "(1). The paper has an interesting and unique idea."}, "weaknesses": {"value": "Weakness:\n(1). From the perspective of methodology, the proposed HC2D has no solid fundamental theory. I can not follow how the formulated loss improves the model performance.  Derivations and theoretical analysis will help the readers on how the proposed method is formulated and guaranteed to improve the trained model.\n\n(2). Confusing term: what is teacher attention (a), novel-cue attention (b), asymmetric distillation processing (c), and fused attention (d). Are they feature-like vectors? Or functions? Or something else?\n\n(3). Technical issue: The experiment is conducted only on CIFAR100 dataset, which is relatively an easy problem in CL. I can not find the result on ImageNet-100 as mentioned in section 4.\n\n(4) Performance issue: In the 20-steps and 50-steps experiments, the HC2D makes only very small improvements, i.e,  < 1%.\n\n(5). There is no catastrophic forgetting measurement and discussion about it, while CL is motivated by CF. \n\n(6). Since the proposed method is for ViT-based CL, it should be applicable to any CL methods ViT backbone, including pre-trained ViT with parameter-efficient finetuning (Promp/Adapter/LoRA). However, the proposed method was only evaluated as a plugin for DyTox.\n\n(7). No pseudo-code to explain in detail how the proposed method works."}, "questions": {"value": "Please address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VBejJtEpOa", "forum": "6HloMuNkYu", "replyto": "6HloMuNkYu", "signatures": ["ICLR.cc/2026/Conference/Submission21101/Reviewer_Cw8u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21101/Reviewer_Cw8u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935696664, "cdate": 1761935696664, "tmdate": 1762941258024, "mdate": 1762941258024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}