{"id": "PjviszIZf1", "number": 879, "cdate": 1756821791420, "mdate": 1763731980643, "content": {"title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool", "abstract": "We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps.\nPrevious methods suffer from a trade-off between reconstruction quality and real-time performance.\nTo address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without introducing a large amount of extra computation.\nIn addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency.\nThese designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets.", "tldr": "WinT3R: a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps.", "keywords": ["computer vision", "3D reconstruction", "machine learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f2ce3f12104da96e9df0c798d8bcbda189b4961.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a feed-forward reconstruction model WinT3R, which can output precise camera poses and high-quality point maps.\nThe key design is a sliding window mechanism to ensure sufficient information exchange among adjacent frames, and a global camera token pool to enhance the reliability of camera pose estimation without sacrificing efficiency.\nExperimental results show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation is clear and easy to follow.\n2. The author conducted extensive experiments to support their statements. And results show that WinT3R outperforms other online baselines in some benchmarks."}, "weaknesses": {"value": "1. The architecture is highly similar to CUT3R and VGGT. Actually, it seems like a combination of these two methods (state tokens-CUT3R, interaction within window-VGGT). And the training costs are high (compared with CUT3R's 8GPUs).\n2. I want to know the relationship between the global and local tokens (image tokens and camera tokens) for each frame. From Sec.3.3, it seems that this work only supervise the point map in the local coordinate system and the relative poses between frames (following pi3). In this case, is it redundant to maintain both local and global tokens for each frame simultaneously?\n3. With the window based design, I want to know the robustness of WinT3R when faced with unordered inputs. Besides, please use very long sequences (200 frames and more) to compare the reconstruction quality, as it is important in this online settings.\n\nPlease answer the above questions carefully and provide more thorough discussion and comparison."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ek7h4rDy3u", "forum": "PjviszIZf1", "replyto": "PjviszIZf1", "signatures": ["ICLR.cc/2026/Conference/Submission879/Reviewer_6j5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission879/Reviewer_6j5J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760599986687, "cdate": 1760599986687, "tmdate": 1762915635252, "mdate": 1762915635252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents WinT3R, a feed-forward model for online 3D reconstruction from streaming images. It aims to solve the critical trade-off between reconstruction quality and real-time performance. The method introduces two key contributions: an online sliding window mechanism, which allows image tokens of adjacent frames to interact directly, improving geometric quality , and a compact camera token pool, which maintains global information from all historical frames to enhance the reliability of camera pose estimation.  Experiments demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper presents a novel framework, WinT3R, for online 3D reconstruction, which effectively tackles the critical trade-off between reconstruction quality and real-time performance. The two core contributions, the sliding window mechanism and the camera token pool, are technically sound and directly address clear limitations in prior streaming methods. \n2.\tExtensive experiments validate the effectiveness of the proposed method. \n3.\tThis paper is written and organized well."}, "weaknesses": {"value": "1.\tThe paper's entire motivation is to close the quality gap between \"suboptimal\" online methods and high-quality offline methods (like the cited VGGT or FLARE). However, all quantitative tables (Tables 1-3) only compare against other online methods. Without including at least one offline advanced baseline, it is impossible to quantify how much of the quality gap has actually been closed, which is essential for contextualizing the paper's contribution. It will be more convincing to add these comparisons.\n2.\tThe paper states the camera head queries the entire historical pool. This implies an attention mechanism with O(T^2) computational complexity, where T is the total number of frames. This quadratic complexity is not scalable, contradicts the real-time (17.2 FPS) claim for any reasonably long video, and is a fundamental design flaw for a streaming system. It will be interesting to discuss about it.\n3.\tThe paper states it uses the final pose from the subsequent window for overlapping frames（L209）. However, the model's token update rule adds the tokens from the initial window to the pool(Eq.5). This raises a problem that the final chosen pose does not match the token stored in the global pool. This problem undermines the pool's global consistency."}, "questions": {"value": "Refer to the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "orWFvwR3c9", "forum": "PjviszIZf1", "replyto": "PjviszIZf1", "signatures": ["ICLR.cc/2026/Conference/Submission879/Reviewer_K247"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission879/Reviewer_K247"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748384054, "cdate": 1761748384054, "tmdate": 1762915635073, "mdate": 1762915635073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new online streaming 3D reconstruction method called WinT3R that combines a sliding-window attention mechanism with a camera token pool memory to achieve real-time performance. It is an improvement based on the prior work CUT3R. The novelty mainly comes from the attention between image tokens within windows, and representing prior frames via compact tokens in a global camera token pool. The experimental results on standard benchmarks show strong improvements over existing online methods."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "It provides a way of balancing the usually high-cost transformer-based 3D reconstruction model and reconstruction quality and consistency over long sequences. This enables the model to operate at real-time with streaming input. The sliding window attention effectively avoids the full casual attention cost. It maintains the simplicity of feedforward neural reconstruction models without heavy keypoint feature matching in traditional SLAM pipelines."}, "weaknesses": {"value": "* It is unclear for long sequences how well the camera token pool can handle and preserve the global geometry for frames that are far apart. \n* For the efficiency and scalability claim, It lacks some experiments on demonstrating the ability of the model on long sequences and the corresponding runtime statistics. \n* The experiments on the selected scenes are mostly all indoor scenes (e.g. 7 Scenes, NRGBD). It lacks some experiments on more varying conditions such as outdoor scenes."}, "questions": {"value": "It seems like the selection of the window's length and stride is important here. Different video sequences will typically have non-uniform motions at every point. Will this affect the reconstruction quality a lot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9SexaV45Wy", "forum": "PjviszIZf1", "replyto": "PjviszIZf1", "signatures": ["ICLR.cc/2026/Conference/Submission879/Reviewer_i3GJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission879/Reviewer_i3GJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953049815, "cdate": 1761953049815, "tmdate": 1762915634765, "mdate": 1762915634765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper (“WinT3R”) presents a feed-forward architecture for predicting point maps from streaming RGB video. At a high level, it combines sliding-window attention with a global persistent state, conceptually related to CUT3R and the streaming variant of VGGT (StreamVGGT). The method uses overlapping windows to encourage direct interaction among neighboring frames while maintaining memory efficiency. In addition, a global camera token pool stores compact historical camera tokens to support future camera pose prediction. The overall idea is simple and well-motivated.\n\nOverall, the design choices are reasonable and the empirical results are strong. My concerns are primarily about figure presentation and some missing ablations; details follow."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach is simple, straightforward, and reasonable: combining sliding-window attention with a global state is a natural way to balance local interaction and long-range consistency.\n\n- Experimental results indicate state-of-the-art reconstruction quality among online/streaming methods.\n\n- The global camera token pool, while somewhat pragmatic, is an effective design specifically for camera pose estimation in a streaming setting."}, "weaknesses": {"value": "- Result visualization could be improved. For example, in Figure 4 it is difficult to tell which columns are better without clearer legends or more distinct visual cues.\n\n- Consider adding videos or point-cloud visualizations in the supplementary material to better convey qualitative improvements.\n\n- The performance improvement over StreamVGGT is relatively modest, which could lessen the perceived contribution of this paper."}, "questions": {"value": "- What are the training time and GPU resources used in the ablation study?\n\n- Which model size (and window size) is used for those ablations?\n\n- There is no ablation that removes the global persistent state entirely. How would performance change without it (e.g., sliding-window only, no global state)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mPx0gm8dlC", "forum": "PjviszIZf1", "replyto": "PjviszIZf1", "signatures": ["ICLR.cc/2026/Conference/Submission879/Reviewer_Qrbo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission879/Reviewer_Qrbo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966418902, "cdate": 1761966418902, "tmdate": 1762915634548, "mdate": 1762915634548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank the reviewers for their time and effort in reviewing our paper and making valuable suggestions.  We also appreciate your positive feedback on our results, particularly your recognition of novelty(K247), technical soundness(Qrbo, i3GJ), and strong experimental performance(Qrbo, K247, 6j5J).\n\nRegarding the reviewers' concerns about the model's long sequence performance, we have reported detailed experimental results and visualizations in the appendix. Additionally, we have included performance comparisons between our model and mainstream offline models.\n\nWe have uploaded a revised draft of our paper, with changes marked in blue. We summarize the revision here:\n\n- added offline models comparison in experiments;\n- added inference time and peak GPU memory usage statistics for long sequences in the appendix;\n- added reconstruction quality statistics for long sequences in the appendix;\n- added visualization results in the appendix;\n\nWe sincerely value your reviews and believe they have greatly contributed to making the new revision more comprehensive. Additionally, we have responded to your individual comments and questions by commenting on your reviews."}}, "id": "edWJVNyAlj", "forum": "PjviszIZf1", "replyto": "PjviszIZf1", "signatures": ["ICLR.cc/2026/Conference/Submission879/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission879/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission879/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763730260061, "cdate": 1763730260061, "tmdate": 1763730285859, "mdate": 1763730285859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}