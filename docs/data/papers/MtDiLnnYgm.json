{"id": "MtDiLnnYgm", "number": 24175, "cdate": 1758353679703, "mdate": 1759896778617, "content": {"title": "HippoTune: A Hippocampal Associative Loop–Inspired Fine-Tuning Method for Continual Learning", "abstract": "Studies have shown that catastrophic forgetting primarily stems from the difficulty of reactivating old memories; although parameter-efficient fine-tuning can mitigate forgetting while keeping most model parameters frozen, it still falls short in fully reawakening knowledge of prior tasks. In contrast, humans can efficiently retrieve and flexibly integrate existing experiences when learning new tasks, thereby maintaining stable performance on earlier ones. During cognition, the hippocampal EC–DG–CA3–CA1 circuit engages in multiple rounds of associative recall, and its pattern-separation and memory-completion mechanisms excel at activating historical information. Inspired by this mechanism, we propose HippoTune, a latent-space iterative retrieval strategy that embeds a query–retrieve–feedback loop within each Transformer layer. Starting from the hidden state as an initial query, the model performs a few rounds of soft key–value retrieval, projects the retrieved signals back into the query, and updates it iteratively until convergence or a preset iteration limit. Theoretically, we show this process implements a Krylov-style polynomial approximation, equivalent to a differentiable second-order preconditioner, thereby deepening retrieval in a principled way. Empirically, HippoTune outperforms classical buffer-free PEFT-CL methods by 5–8% in accuracy across three vision benchmarks, while reducing training FLOPs by 50%, effectively mitigating forgetting under tight compute constraints. Code is available at: https://anonymous.4open.science/r/HippoTune-1DF2.", "tldr": "HippoTune is a continual learning fine-tuning method that embeds hippocampal-inspired iterative retrieval loops into each Transformer layer, boosting accuracy by 5–8 pp while halving training FLOPs.", "keywords": ["Associative Memory", "Key–Value Memory", "Parameter-Efficient Fine-Tuning", "Continual Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/377661a5c1f192749ae5366c3d0a705bf7a2b43b.pdf", "supplementary_material": "/attachment/18a49bb3203c28f94a9835958563ae16e31ff65a.zip"}, "replies": [{"content": {"summary": {"value": "Recently, several continual learning (CL) methods have emerged that utilise pre-trained models through parameter-efficient fine-tuning. One category of these methods is based on soft prompts, which involve training a set of prompts to guide the representations of each input. Typically, these methods select prompts based on a feature vector of the input, computed via an additional forward pass through the model. In this work, the authors address the inefficiency of this process by proposing a new method for selecting prompts for each layer of the model, based on the representations from the previous layer. This approach reduces the number of FLOPs during training by eliminating the need for an initial forward pass to identify prompts. Inspired by the hippocampus, they introduce HippoTune, an iterative retrieval strategy designed for transformer layers. The paper includes results presented across three visual benchmarks, along with a comprehensive ablation analysis."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper effectively highlights a limitation of several CL methods that use soft prompts: they perform a double forward pass per example (prompt selection and classification). Using the representation of the previous layer for prompt selection is an interesting idea that has been used in other areas, but it is presented in a novel way in the CL scenario.\n- Performing multiple retrieval steps is an interesting idea, showing theoretically and empirically that it works.\n- A comprehensive ablation study of the components of the proposed method is conducted."}, "weaknesses": {"value": "- The paper discusses a framework for PEFT-CL methods, but it applies only to methods that use soft prompts. LoRA-based methods do not necessarily include a retrieval component, so they cannot be extended to the framework. It would be good to make it clear that the proposed framework applies only to methods that use a key-value selection of prompts.\n    - Also, it is not clear why the proposed framework is necessary. I understand that it helps to explain the method better, but it does not necessarily help to understand the other methods better.\n- There are many hyperparameters to search over (T_max, loss coefficients, T, plus the common ones for this type of method). This can increase the method's complexity."}, "questions": {"value": "- By concatenating the values of \"v\" from each iteration (Eq. 6), a better representation of what is retrieved is achieved. Although the proposal achieves fewer FLOPs, this concatenation increases the batch size, since each batch element adds several elements to the sequence, resulting in a larger batch size than previous methods.\n    - What percentage of the batch is used by these vectors? How much can this affect scenarios where GPU size may be an issue?\n- Despite showing better results with multiple iterations (T_max >1), can you provide insight into why this improves performance?\n    - Is it related to increasing the concatenated values to v?\n- One problem with prompt-based methods is the low generalisation of vectors to datasets outside the distribution of the pre-training set.\n    - Were experiments on datasets where these types of methods fail conducted?\n    - It would be interesting to see how the proposed method would perform."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hlbmLyRdLw", "forum": "MtDiLnnYgm", "replyto": "MtDiLnnYgm", "signatures": ["ICLR.cc/2026/Conference/Submission24175/Reviewer_r3mR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24175/Reviewer_r3mR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551132495, "cdate": 1761551132495, "tmdate": 1762942975956, "mdate": 1762942975956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a computationally efficient recurrent method for continual learning in the context of parameter-efficient fine-tuning. They draw an analogy between their method and a memory recall mechanism in the hippocampus. Proving properties of the recurrence enables the authors to provide practical guidelines for hyper-parameter selection. Although results are not state of the art in all settings, they are typically very strong vis-a-vis baselines, and the approach is computationally more efficient than the most competitive baselines. The focus here on the (exhaustive) recall of previously learned tasks is refreshing and a novel perspective in the literature."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, the theoretical analysis is insightful, and practical implications of the analysis are fleshed out. The evaluation is largely thorough. Results are reported on a variety of datasets with a number of baselines. Ablation studies are performed to illustrate the contributions of the (several) components of the method. Complete code appears to have been provided, although this reviewer was not able to review the code thoroughly."}, "weaknesses": {"value": "It appears to be the case that the evaluations are done using task-incremental learning [1]. Evaluations of the method in domain-incremental or class-incremental settings are not included. The provided code is perhaps too voluminous and is certainly too much for a reviewer to digest. A typical useful repository accompanying a paper enables a reviewer to inspect the implementation of the key details of the method. \n\n[1] https://www.nature.com/articles/s42256-022-00568-3"}, "questions": {"value": "* Is my impression that all evaluations are performed only in a task-incremental learning setting correct?\n* Can you report results in domain- or class-incremental settings in the appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IPXt7xeVvs", "forum": "MtDiLnnYgm", "replyto": "MtDiLnnYgm", "signatures": ["ICLR.cc/2026/Conference/Submission24175/Reviewer_RPFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24175/Reviewer_RPFD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958410962, "cdate": 1761958410962, "tmdate": 1762942974743, "mdate": 1762942974743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HippoTune, a continual learning approach inspired by the hippocampal memory circuit (EC–DG–CA3–CA1). Unlike traditional parameter-efficient fine-tuning (PEFT) methods that perform single-step prompt retrieval, HippoTune embeds a latent deliberation loop within each Transformer layer, enabling multi-round associative memory retrieval and feedback to alleviate catastrophic forgetting. The method integrates orthogonal and entropy regularization to stabilize learning, and theoretical analysis shows that the iterative retrieval approximates a second-order preconditioner through a Krylov subspace expansion. Experiments on Seq-CIFAR100, Seq-ImageNet-R, and Seq-CUB200 demonstrate that HippoTune achieves higher accuracy (≈80%) with roughly half the computational cost of prior PEFT-CL baselines, maintaining efficiency and strong memory retention without rehearsal buffers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper’s primary strength is its strong linkage between biological mechanisms and algorithmic design, modeling the hippocampal memory circuit (EC–DG–CA3–CA1) as a multi-round associative retrieval process within Transformer layers.\nThis biologically grounded framework makes the method both conceptually interpretable and practically effective in mitigating catastrophic forgetting.\n2. The authors formalize existing prompt-pool approaches as a single key–value retrieval framework and situate L2P, DualPrompt, CoDA-Prompt, and HiDe-Prompt as special cases, which clarifies trade-offs and motivates deeper, iterative retrieval.\n3. HippoTune  matches or outperforms leading buffer-free PEFT baselines on Seq-CIFAR100, Seq-ImageNet-R, and Seq-CUB200."}, "weaknesses": {"value": "1. While the paper includes ablation studies, it does not clearly isolate the individual contributions of the recurrent loop, orthogonal regularization, and entropy term to the overall performance gain.\n2. The model introduces several new hyperparameters (loop depth, temperature, $\\alpha$, Top-k), yet lacks an analysis of stability or sensitivity across different settings.\n3. The writing style feels overly formulaic and AI-generated, with excessive use of em dashes and uniform sentence patterns.\n4. All studies use a frozen ViT-B/16 with only PEFT modules trained, which limits adaptability and generalization; including experiments on other architectures such as Swin Transformer as well as partially unfrozen ViT variants, would better demonstrate the method’s robustness and general applicability.\n5. The paper only reports accuracy as the evaluation metric, but does not provide any quantitative measures of forgetting, such as BWT, FWT, or average forgetting over time."}, "questions": {"value": "1. What is the precise novelty over prior PEFT-CL prompt-pool methods beyond adding iterative retrieval; what can HippoTune do that they fundamentally cannot?\n2. What is the exact buffer size used for the classical continual learning baselines (e.g., LwF, DER++)?\n3. Could you clarify what the function $\\phi(x; \\theta)$ specifically represents in your formulation (line 188)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AGvxPfslFn", "forum": "MtDiLnnYgm", "replyto": "MtDiLnnYgm", "signatures": ["ICLR.cc/2026/Conference/Submission24175/Reviewer_m6em"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24175/Reviewer_m6em"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982251252, "cdate": 1761982251252, "tmdate": 1762942973817, "mdate": 1762942973817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a biologically inspired approach to mitigate catastrophic forgetting in continual learning. Traditional parameter-efficient fine-tuning (PEFT) methods, such as adapters and prompts, reduce computation and storage costs but still fail to retrieve and integrate previously learned knowledge effectively. Drawing inspiration from the hippocampal EC–DG–CA3–CA1 loop in the human brain, the authors propose HippoTune, a latent deliberation mechanism that iteratively reactivates past representations to enhance memory retention during fine-tuning. In this model, each transformer layer is augmented with a small associative retrieval loop: the input state initializes a query that undergoes pattern separation (DG), auto-associative completion (CA3), and integrative fusion (CA1) across multiple iterations. This process allows the network to refine representations through recursive interaction within a learned latent space, mimicking hippocampal recall dynamics. Theoretically, the iterative update of HippoTune is shown to approximate a second-order preconditioning step in the Krylov subspace, providing curvature-aware optimization without explicit Hessian computation. The method also maintains stability across varying task numbers, demonstrating its scalability and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This work propose HippoTune, a continual learning method that bridges biological insight and machine learning theory by translating hippocampal associative memory mechanisms into a PEFT framework, offering a novel perspective on memory consolidation and retrieval in neural models for continual learning.\n2. The paper presents a relatively novel and biologically inspired analogy that grounds its internal updating mechanism in the hippocampal associative loop. This connection gives the proposed architecture a plausible neurobiological motivation, and the accompanying theoretical analysis adds a layer of methodological soundness.\n3. In addition, the partial update mechanism (Eq. 5) effectively preserves previously acquired knowledge while allowing new information to be injected into multiple parameter layers, achieving a desirable balance between plasticity and stability. Empirically, the method demonstrates strong performance across continual learning benchmarks, surpassing existing PEFT-based baselines by a meaningful margin."}, "weaknesses": {"value": "However, despite the originality of its biological analogy, it should be acknowledged that the neuroscience of long-term memory formation remains far from fully understood. Thus, while the hippocampal metaphor adds interpretability, it does not constitute a rigorously biomimetic design in a scientific sense. Fundamentally, the proposed HippoTune module still functions as a multi-layer retrieval and partial parameter update process, consistent with mainstream continual learning paradigms rather than a radically new mechanism of memory consolidation. Consequently, the work’s strength lies more in its conceptual integration of biological inspiration and computational practicality than in demonstrating a genuinely new class of biologically faithful learning architecture."}, "questions": {"value": "In, eq5, you mentioned a layer-specific linear transforamtion fuction P, would you mind to elaborate it a littble more?\nHow is the process of eq5 can be seen as a minimal abstraction of the CA3 mechanism? Is there more previous work to enlight us on this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C8q7PKxiXC", "forum": "MtDiLnnYgm", "replyto": "MtDiLnnYgm", "signatures": ["ICLR.cc/2026/Conference/Submission24175/Reviewer_WqnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24175/Reviewer_WqnZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184522689, "cdate": 1762184522689, "tmdate": 1762942973495, "mdate": 1762942973495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}