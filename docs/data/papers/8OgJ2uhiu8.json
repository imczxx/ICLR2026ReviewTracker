{"id": "8OgJ2uhiu8", "number": 2422, "cdate": 1757081462338, "mdate": 1763719399190, "content": {"title": "Assembling the Mind's Mosaic: Towards EEG Semantic Intent Decoding", "abstract": "Enabling natural communication through brain–computer interfaces (BCIs) remains one of the most profound challenges in neuroscience and neurotechnology. While existing frameworks offer partial solutions, they are constrained by oversimplified semantic representations and a lack of interpretability. To overcome these limitations, we introduce **Semantic Intent Decoding(SID)**, a novel framework that translates neural activity into natural language by modeling meaning as a flexible set of compositional semantic units.\nSID is built on three core principles: semantic compositionality, continuity and expandability of semantic space, and fidelity in reconstruction.\nWe present **BrainMosaic**, a deep learning architecture implementing SID. BrainMosaic decodes multiple semantic units from EEG/SEEG signals using set matching and then reconstructs coherent sentences through semantic-guided reconstruction. \nThis approach moves beyond traditional pipelines that rely on fixed-class classification or unconstrained generation, enabling a more interpretable and expressive communication paradigm. Extensive experiments on multilingual EEG and clinical SEEG datasets demonstrate that SID and BrainMosaic offer substantial advantages over existing frameworks, paving the way for natural and effective BCI-mediated communication.", "tldr": "", "keywords": ["Electroencephalography (EEG)", "Brain-computer interface (BCI)", "Semantic Intent", "Neural decoding"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b1949a4cd254146138dd6007543236244b9e3a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Semantic Intent Decoding (SID) and a concrete model, BRAINMOSAIC, for translating EEG or SEEG into natural language by first decoding a variable set of semantic units, aligning them in a continuous text embedding space, and then reconstructing a sentence with an LLM constrained by those units. The pipeline comprises a set-matching Semantic Decomposer trained with a Hungarian loss, a Semantic Retriever that aligns slot embeddings to word and sentence embeddings and predicts global attributes, and a Semantic Decoder that prompts an LLM to generate text from retrieved units. The work targets semantic decoding during reading, listening, or imagined speech across Chinese and English datasets, plus a private SEEG dataset, and claims improvements over classification and unconstrained generation using embedding based metrics UMA, MUS, and SRS"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The three principles (compositionality, continuity, fidelity) are well-motivated by linguistic and neuroscience evidence.\n2. Interpretable pipeline: slots -> ranked retrieval -> prompted gen beats blackbox E2E.\n3. Consistent empirical gains across multiple datasets and baselines with both concept level and sentence level metrics.\n4. Comprehensive comparison with other relevant baselines.\n5. Extensive supplementary material includes dataset details, baseline descriptions, and sensitivity analyses."}, "weaknesses": {"value": "1. Lack of qualitative examples of reconstruction quality. Without concrete examples, readers cannot verify whether predicted semantic unit sets are genuinely interpretable or noisy/scattered, (b) how well do the quantitative metrics match real semantic correctness. This falls below the standards in neuro-decoding literature where it's common to show a few samples of the proposed model's input-output vs baseline\n2. Lack of comparability to standard text metrics. While semantics-first metrics are appropriate here, it's also important to have surface level metrics common in the literature such as BLEU score, WER for surface overlap, and BERTScore for semantic similarity."}, "questions": {"value": "1. For continuous corpora, how did you prevent leakage between train and test when the same long passage is segmented into sentences?\n2. Can you report error bars and conduct significance tests comparing BrainMosaic to baselines?\n3. How sensitive are results to the choice of embedding model? What happens with random embeddings as an ablation?\n4. For multi-subject datasets, what is cross subject variance? Can models trained on one subject decode another subject’s neural activity? How good is the generalizability\n5. Did you perform any electrode selection or analyze which channels/regions contribute most to semantic unit prediction?\n6. Table 5A shows declining UMA with vocabulary expansion. At what vocabulary size does performance approach random? Can you characterize this scaling law?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "as2N6WAbY6", "forum": "8OgJ2uhiu8", "replyto": "8OgJ2uhiu8", "signatures": ["ICLR.cc/2026/Conference/Submission2422/Reviewer_y3DJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2422/Reviewer_y3DJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802104634, "cdate": 1761802104634, "tmdate": 1762916231885, "mdate": 1762916231885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Semantic Intent Decoding (SID), a novel framework for brain-to-language translation that models communicative intent as a set of compositional semantic units rather than relying on fixed labels or unconstrained generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes an intriguing motivation and introduces a novel perspective for brain-to-text decoding. By modeling intent as an unordered and variable set of semantic units, it moves beyond traditional fixed-label or sequential decoding paradigms, potentially offering a more brain-plausible representation of semantic processing."}, "weaknesses": {"value": "1.Lack of robustness evaluation under input noise: the paper does not assess model performance under noisy or corrupted EEG inputs, which weakens the significance of the results reported in Tables 3, 4, and 5. Given the inherent noisiness of EEG signals, such evaluations are essential to validate the practical utility of the proposed method.\n\n2.Insufficient justification for LLM-based sentence generation: the use of LLMs for sentence reconstruction raises concerns about data contamination, especially if the test set sentences or similar phrasings were present in the LLM’s pretraining corpus. The authors provide no strong evidence (e.g., n-gram overlap analysis or controlled LLM ablation) to rule out this possibility, which undermines the credibility of the generation results.\n\n3.Missing comparison with standard generation metrics and SOTA methods: the paper does not compare with mainstream brain-to-text decoding methods using standard generation metrics such as WER, CER, BLEU, ROUGE, METEOR, or BERTScore. This omission makes it difficult to benchmark the proposed method against existing literature and assess its true advancement in the field."}, "questions": {"value": "1.How was K (number of semantic slots) chosen per dataset? Was it tuned? How does performance vary with K?\n\n2.Would the model scale to larger vocabularies? What are the computational bottlenecks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EVZzCFqkfd", "forum": "8OgJ2uhiu8", "replyto": "8OgJ2uhiu8", "signatures": ["ICLR.cc/2026/Conference/Submission2422/Reviewer_xT72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2422/Reviewer_xT72"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915986229, "cdate": 1761915986229, "tmdate": 1762916231657, "mdate": 1762916231657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to view EEG/SEEG-to-language decoding as predicting a variable-size, order-invariant set of semantic units (SID), rather than fixed-class decoding or fully free-form generation. It instantiates this with BRAINMOSAIC, including i) an EEG encoder + query slots, ii) a semantic retriever that aligns slots to an open-vocabulary unit bank, and iii) an LLM that does semantics-constrained generation from those units. Experiments on three public EEG datasets (Chinese + English) and one private SEEG dataset, using UMA, MUS, and sentence-level similarity, aim to show: (i) set-style decoding is reasonable, (ii) continuous semantic space helps scalability, (iii) constrained generation improves fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting formulation: treating an utterance as a set of semantic units is a neat and fairly novel way to handle variable-length EEG semantics.\n\n2. Well-structured method: the three design principles (compositionality, continuity/expandability, fidelity) map cleanly to three modules.\n\n3. Reasonable experiment designs: the same idea is run on Chinese imagined speech, Chinese naturalistic reading, English reading, and a clinical SEEG case, which supports the claim that the approach is not tied to a single dataset."}, "weaknesses": {"value": "1. **Train/validation split is underspecified:** the paper only mentions a unified 8:2 train–test split, but does not say whether this is by subject or by trial. Can the model guess which sentence it is by the sample length?\n\n2. **No true random / text-prior baselines**: the main metrics (UMA, MUS, sentence similarity) are not compared against (i) picking the same number of units at random or (ii) a text-only/corpus-frequency prior. This makes it hard to see how much of the score actually comes from EEG, especially since the method later calls an LLM. The paper reports `MUS_exp` but doesn’t clearly define it.\n\n3. Experiments don’t cleanly isolate the three research questions: the section is organized around “set is better,” “continuous space scales,” and “constrained generation is more faithful”. Several experiments change multiple components at once (set + LLM + thresholds), so it’s hard to attribute the improvements to the claimed factor."}, "questions": {"value": "1.Is the 8:2 split done by subject or by trial on each dataset? Is there possibility of data leakage?\n2.Please add complete random baselins for UMA/MUS/SRS. Please describe the MUS_exp more clearly.\n3.Please add ablations where only one of the three design choices (set, continuous space, LLM-constrained decoding) is changed at a time, so we can see which part actually drives the gains?\n4.Please distinguish clearly between ChineseEEG and ChineseEEG-2 as they are two distinct datasets.\nSome references are missing. For example, there should be references for this statement: “Alternatively, a more recent direction seeks to enhance expressive capacity by mapping neural signals directly into the latent representation space of large language models (LLMs).”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qc6Hz8fPBz", "forum": "8OgJ2uhiu8", "replyto": "8OgJ2uhiu8", "signatures": ["ICLR.cc/2026/Conference/Submission2422/Reviewer_MrKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2422/Reviewer_MrKn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971219757, "cdate": 1761971219757, "tmdate": 1762916231276, "mdate": 1762916231276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Comment (1/3)"}, "comment": {"value": "### 1. Random & Text-Prior Baselines\n\nSuggested by Reviewer MrKn, we implement two levels of baselines to decouple EEG contributions from corpus statistics (Appendix C.5). All random results represent the average of 3 independent runs over the entire test set.\n\n***1. Label-level***\nKeeps the prediction pipeline fixed but compares against random concept and sentence labels. \n\n***2. Prediction-level***\nTests statistical guessing against truth labels by bypassing the EEG encoder.\n- Random-prior: Uniform random selection.\n- TopK-prior: Deterministic Top-K frequent units.\n- Freq-prior: Frequency-weighted random selection.\n\nAs shown below, BrainMosaic significantly outperforms all random and text-prior baselines across all metrics. This consistent superiority confirms that the performance stems from valid neural decoding rather than statistical guessing.\n\n\n|               |  Clinical  |            |            |   Chisco   |            |            | ChineseEEG-2 |            |            |\n| ------------- | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :----------: | :--------: | :--------: |\n|               |    UMA     |    MUS     |    SRS     |    UMA     |    MUS     |    SRS     |     UMA      |    MUS     |    SRS     |\n| Label Random  |   0.2472   |   0.6878   |   0.5394   |   0.1390   |   0.6987   |   0.4997   |    0.1308    |   0.6955   |   0.4683   |\n| Random-prior  |   0.0563   |   0.6392   |   0.5056   |   0.0152   |   0.6185   |   0.4924   |    0.0112    |   0.5815   |   0.5129   |\n| Topk-prior    |   0.1510   |   0.6884   |   0.5418   |   0.1215   |   0.6793   |   0.5465   |    0.1159    |   0.6606   |   0.5282   |\n| Freq-prior    |   0.1008   |   0.6540   |   0.5321   |   0.0289   |   0.6365   |   0.5124   |    0.0332    |   0.6046   |   0.5170   |\n| BrainMosaic | **0.6596** | **0.8124** | **0.6651** | **0.5617** | **0.8009** | **0.6206** |  **0.3707**  | **0.7687** | **0.6163** |\n\n|               |   ZuCoSR   |            |            |   ZuCoNR   |            |            |  ZuCoTSR   |            |            |\n| ------------- | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: |\n|               |    UMA     |    MUS     |    SRS     |    UMA     |    MUS     |    SRS     |    UMA     |    MUS     |    SRS     |\n| Label Random  |   0.2557   |   0.7591   |   0.5499   |   0.2640   |   0.7339   |   0.5297   |   0.2588   |   0.7084   |   0.5009   |\n| Random-prior  |   0.0624   |   0.7198   |   0.5526   |   0.0788   |   0.7163   |   0.4879   |   0.0641   |   0.7049   |   0.4999   |\n| Topk-prior    |   0.1559   |   0.7407   |   0.5681   |   0.1467   |   0.7554   |   0.5227   |   0.1266   |   0.7137   |   0.5033   |\n| Freq-prior    |   0.1078   |   0.7311   |   0.5612   |   0.1003   |   0.7351   |   0.5033   |   0.0951   |   0.7190   |   0.4916   |\n| BrainMosaic | **0.7506** | **0.8586** | **0.6982** | **0.7144** | **0.8453** | **0.6094** | **0.5520** | **0.8198** | **0.5956** |\n\n\n### 2. Scalability & Finite Semantic Space\n\nSuggested by Reviewer y3DJ and xT72, we clarify that vocabulary expansion does not lead to performance degradation, due to the intrinsic properties of the continuous semantic space.\n\n**Semantic Density and Coverage**\nThe continuous embedding space is structurally dense. The core vocabulary already establishes the primary topological distribution of this space. As the vocabulary expands, new words (e.g., synonyms, derivatives) naturally map into existing high-density regions rather than creating distinct new conceptual dimensions. Consequently, the effective \"semantic volume\" the model needs to decode remains stable and bounded.\n\n**Cognitive Saturation: Theoretically, the number of distinguishable semantic concepts in cognition is finite.**\n\nTheoretically, the number of distinguishable semantic concepts in cognition is finite. Research indicates that approximately 12,000+ cue words are sufficient to cover both the core and long-tail regions of the human semantic space [1]. Our experimental configuration (30,000 units) already exceeds this threshold, ensuring that the semantic space is saturated and robust."}}, "id": "uzrwM1Ro0i", "forum": "8OgJ2uhiu8", "replyto": "8OgJ2uhiu8", "signatures": ["ICLR.cc/2026/Conference/Submission2422/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2422/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission2422/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763719772404, "cdate": 1763719772404, "tmdate": 1763719772404, "mdate": 1763719772404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Comment (3/3): Summary"}, "comment": {"value": "We sincerely thank all the reviewers for your time and the constructive feedback provided. \nWe found the comments extremely insightful and have used them to significantly strengthen the rigor and clarity of our work.\n\nIn response to your suggestions, we conduct a comprehensive revision of the manuscript. \nThe key improvements are summarized below:\n\n- **Enhanced Evaluation Rigor**: We update and reorganize the main table to include BERTScore-F1. Additionally, we incorporate standard deviations and statistical significance tests in Table 3, and standardize the text embedding configuration using 'Doubao' model to ensure a unified semantic space for more intuitive comparisons. The sensitivity analysis of other text embedding models is decoupled and detailed in Appendix E.\n- **Systematic Component Ablation**: We introduce a rigorous ablation study in Section 3.5, explicitly isolating and quantifying the distinct contribution of each core component within the BrainMosaic.\n- **Qualitative Analysis & Metric Validation**: We provide intuitive examples of output sentences in Appendix D, qualitatively demonstrating how our proposed metrics better capture semantic recovery.\n- **Extensive Supplementary Experiments**: We perform a series of new experiments inspired by reviewers: \n    - Appendix C: Random/Text-Prior Baselines\n    - Appendix D: Supplement of standard surface metrics (BLEU/WER)\n    - Appendix E: Analysis of Embedding Models.\n    - Appendix G: Hyper-parameter analysis for the number of semantic slots.\n    - Appendix I: Regional Contribution Analysis.\n- **Clarifications**: We refine the manuscript to address specific details, including:\n    - Adding missing citations (line 78) and theoretical explanations for Scalability (line 218).\n    - Formalizing the definition of $MUS_{exp}$ (line 345).\n    - Clarifying the data splitting protocol, repeated experiments (line 359).\n    - Fixing typo in dataset name (line 394).\n\nWe hope these revisions comprehensively address the concerns raised and firmly establish the validity and contribution of our work. A detailed point-by-point response follows."}}, "id": "z2cRf85uCw", "forum": "8OgJ2uhiu8", "replyto": "8OgJ2uhiu8", "signatures": ["ICLR.cc/2026/Conference/Submission2422/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2422/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission2422/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763719838243, "cdate": 1763719838243, "tmdate": 1763719838243, "mdate": 1763719838243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}