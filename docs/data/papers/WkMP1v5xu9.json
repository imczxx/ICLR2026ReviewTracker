{"id": "WkMP1v5xu9", "number": 18690, "cdate": 1758290157452, "mdate": 1759897087365, "content": {"title": "Preserving Representation In Continual Learning via Feature-Preserving Fine-Tuning", "abstract": "In real-world applications, deep learning models must continually adapt to sequentially arriving tasks without access to previous data. Although pre-trained foundation models show generalisation and zero-shot abilities, fine-tuning them in a continual learning setting often leads to representation degradation. In this study, we firstly systematically evaluate several recent feature-preserving fine-tuning methods (L2-SP, FTP, WiseFT and ImpReg) in continual learning scenario using a large scale pre-trained foundation model. We further explore the \neffectiveness of full fine-tuning (FullFT) versus parameter-efficient fine-tuning (PEFT) and propose a novel two-stage fine-tuning strategy, \nPEFT+Cons, designed to balance stability and plasticity by combining PEFT with task-specific knowledge consolidation. Extensive experiments on the CIFAR-100 and ImageNet-R benchmark datasets demonstrate that our proposed PEFT+Cons approach effectively prevents representation forgetting while enhancing task-specific knowledge retention.", "tldr": "", "keywords": ["continual learning", "representation learning", "fine-tuning", "foundation models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de7e26ddf9d3457da8c6ee01bb53adbbf9551402.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the challenge of representation degradation in continual learning (CL) for pre-trained foundation models. The authors systematically evaluate several feature-preserving fine-tuning methods in a class-incremental learning setting using a CLIP ResNet-50 model. It proposes a two-stage strategy, PEFT+Cons, which integrates PEFT with task-specific knowledge consolidation to better balance stability and plasticity."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper explored the challenges of representation degradation in continual learning with pre-trained foundation models. It proposed a two-stage fine tuning strategy that combines PEFT with task-specific knowledge consolidation, and provide insights into its effectiveness in mitigating representational forgetting."}, "weaknesses": {"value": "The innovation of this paper is limited. This paper primarily provides an empirical exploration of how combining FullFT or PEFT with different feature-preserving methods affects the maintenance of representational capacity in continual learning. The analysis of the underlying reasons is relatively empirical and lacks an investigation into the fundamental causes; the analysis is somewhat superficial and not sufficiently in-depth. The proposed method also appears to be a straightforward combination of FullFT or PEFT with various feature-preserving techniques, raising significant doubts regarding its novelty.\n\nFurthermore, the method proposed in this paper is not compared with other approaches based on pre-trained models. The datasets and continual learning scenarios used for validation are too limited, making the experimental support insufficient. In the Related Work section, a comprehensive analysis of current continual learning methods based on pre-trained models should be provided, but the discussion of such methods in this section is not thorough."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QMMpKVxxON", "forum": "WkMP1v5xu9", "replyto": "WkMP1v5xu9", "signatures": ["ICLR.cc/2026/Conference/Submission18690/Reviewer_6xnY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18690/Reviewer_6xnY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833253196, "cdate": 1761833253196, "tmdate": 1762928391450, "mdate": 1762928391450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the representation degradation problem in continual learning of large pre-trained models, focusing on how fine-tuning strategies affect the preservation of generalizable representations. They find that while feature-preserving methods mitigate catastrophic forgetting under FullFT, representation drift still occurs. In contrast, PEFT substantially reduces forgetting but at the cost of lower task-specific accuracy. To address this trade-off, the paper proposes PEFT+Cons, a novel two-stage fine-tuning strategy that first performs PEFT to stabilise pre-trained features, then applies feature-preserving FullFT for task-specific consolidation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of representation-level metrics (RF, UTA, FinalGLP) extends evaluation beyond accuracy, offering more interpretable and transferable measures for representation stability.\n2. The two-stage PEFT+Cons procedure is intuitive, reproducible, and well-motivated by observed limitations of both FullFT and PEFT.\n3. The authors’ interpretation of the attention pooling block as a naturally stable representation aggregator provides useful intuition that may inspire architectural research in continual learning."}, "weaknesses": {"value": "1. The study’s conclusions are based solely on CLIP-ResNet-50 backbones and vision-only tasks. It remains uncertain whether the same representational preservation trends would hold for other backbones and other tasks.\n2. While the empirical findings clearly demonstrate the stability benefits of feature-preserving fine-tuning, the paper provides no theoretical framework explaining why these methods maintain representational similarity in continual settings."}, "questions": {"value": "1. Can the consolidation stage be scheduled adaptively based on a forgetting metric rather than fixed intervals?\n2. Could combining PEFT+Cons with replay-based methods further improve the stability–plasticity trade-off?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hd4HsVxJql", "forum": "WkMP1v5xu9", "replyto": "WkMP1v5xu9", "signatures": ["ICLR.cc/2026/Conference/Submission18690/Reviewer_Avj8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18690/Reviewer_Avj8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975926365, "cdate": 1761975926365, "tmdate": 1762928390995, "mdate": 1762928390995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for preserving representations in continual learning, addressing the degradation of pre-trained foundation models when fine-tuned sequentially. The authors systematically evaluate several feature-preserving fine-tuning (FPFT) techniques (L2-SP, FTP, WiseFT, and ImpReg) in class-incremental learning with CLIP-ResNet-50. They highlight that naive full fine-tuning (FullFT) causes severe representational forgetting, while parameter-efficient fine-tuning (PEFT), which only updates the attention pooling block, greatly alleviates this issue but limits adaptability to new tasks. To balance stability and plasticity, the paper introduces PEFT+Cons, a two-stage fine-tuning strategy that first performs PEFT to stabilize pre-trained features, then applies feature-preserving FullFT to consolidate task-specific knowledge. This approach enables both robust representation retention and effective adaptation. Experiments on Split CIFAR-100 and Split ImageNet-R demonstrate that PEFT+Cons with FTP achieves the best trade-off, substantially improving final task accuracy and representational robustness while maintaining strong generalization to unseen tasks. Overall, the study provides new insights into how feature-preserving regularization and modular fine-tuning interact in foundation models. It underscores the importance of designing fine-tuning strategies that protect pre-trained representations while supporting continual adaptation, marking a step toward more stable and generalizable continual learning with vision-language foundations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is clearly written, and its overall structure flows logically from the underlying motivation and problem formulation to the methodological design and comprehensive experimental validation.\n\n2.The experimental design is rigorous and goes beyond conventional evaluations based solely on classification accuracy. Instead, it employs a comprehensive set of representation-level metrics—such as Representational Forgetting (RF), Unseen Task Accuracy (AvgUTA), and Global Linear Probe Accuracy (FinalGLP)—thereby providing a more nuanced and convincing assessment of the model’s effectiveness.\n\n3.Building on comprehensive experimental analyses, the paper introduces PEFT+Cons, a novel two-stage fine-tuning strategy designed to achieve a balanced trade-off between stability and plasticity. In this approach, Stage 1 employs PEFT to preserve robust representations and maintain generalization, while Stage 2 applies a constrained FullFT to consolidate task-specific knowledge. Empirical results demonstrate that FTP with this strategy consistently outperforms other fine-tuning methods across multiple evaluation metrics.\n\n4.The paper carefully characterizes the behaviors of both FullFT and PEFT strategies through well-designed experiments, offering valuable insights into their respective strengths and limitations. This analysis provides a clear rationale for why the proposed PEFT+Cons method achieves better performance under continual learning scenarios."}, "weaknesses": {"value": "1.The empirical evaluation uses only two datasets (SplitCIFAR-100 and SplitImageNet-R) and a single backbone (CLIP-ResNet-50). While these are reasonable starting points, the paper does not demonstrate whether conclusions generalize to (a) other backbone families (e.g., ViTs or larger CLIP variants), (b) longer task sequences or different class granularities, and (c) warm start setting[1].\n\n2.The work attributes PEFT’s resistance to forgetting to properties of the attention pooling block, yet does not examine alternative PEFT designs or architectural modifications that might further improve plasticity without FullFT (e.g., small adapter modules, layer-wise low-rank updates, or selective unfreezing). It would be beneficial for the authors to conduct additional experiments to investigate the effectiveness of PEFT+Cons with other modules which are commonly used in peft PTM methods[2,3].\n\n3.The paper reports that the combination of FTP with PEFT+Cons yields the best overall performance, yet provides limited mechanistic explanation for why FTP consistently outperforms other feature-preserving approaches within this two-stage framework. Moreover, the performance of other methods under the PEFT+Cons strategy is generally lower on the FTA, AvgUTA, and FinalGLP metrics compared to their results under the PEFT strategy without consolidation. A deeper analysis of these results would offer valuable insights and strengthen the authors’ conclusions.\n\n4.The experiments presented in this paper show that the FTP combined with the PEFT+Cons strategy achieves the best performance on two benchmark datasets compared with other fine-tuning methods. However, this evidence alone is not fully convincing. The applicability and generality of the proposed approach could be further strengthened by evaluating its performance with additional continual learning methods [4–7] (e.g., LwF, EWC, iCaRL, ZSCL). \n\n[1] Elastic feature consolidation for cold start exemplar-free incremental learning.\n[2] Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning.\n[3] LoRA Subtraction for Drift-Resistant Space in Exemplar-Free Continual Learning.\n[4] Learning without Forgetting\n[5] Overcoming catastrophic forgetting in neural networks\n[6] iCaRL: Incremental Classifier and Representation Learning\n[7] Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models"}, "questions": {"value": "1.Why was CLIP-ResNet50 pre-trained on ImageNet-1K chosen as the backbone? Given that CLIP models are already trained on large-scale and diverse datasets, the necessity of additional pretraining on ImageNet-1K is unclear. Furthermore, evaluating the proposed method with different backbone architectures would further validate its generality and strengthen the evidence for its effectiveness.\n\n2.All experiments appear to have been conducted using a single random seed, raising concerns that the chosen hyperparameter settings may be overfitted to this specific configuration. Evaluating the method across multiple random seeds would provide a more reliable assessment of its robustness and generalization.\n\n3.As shown in Table 1, the RF of the naive method combined with PEFT strategy is lower than that of L2-SP, FTP, and WiSE-FT on the benchmark datasets. What accounts for the reduced forgetting observed in the Naïve method? Moreover, the final GLP of the naive method is comparable to that of other approaches while requiring significantly less computational cost. What factors contribute to these results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UBqG8yDLJ3", "forum": "WkMP1v5xu9", "replyto": "WkMP1v5xu9", "signatures": ["ICLR.cc/2026/Conference/Submission18690/Reviewer_zz79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18690/Reviewer_zz79"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006956863, "cdate": 1762006956863, "tmdate": 1762928390388, "mdate": 1762928390388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of representation degradation in continual learning with pre-trained foundation models. The authors systematically evaluate four recent feature-preserving fine-tuning approaches (L2-SP, FTP, WiseFT, and ImpReg) under class-incremental continual learning using a CLIP-ResNet-50 backbone on Split CIFAR-100 and ImageNet-R. They compare full fine-tuning (FullFT) and parameter-efficient fine-tuning (PEFT) and introduce a two-stage method (PEFT+Cons) that combines PEFT with feature-preserving consolidation. Results indicate that PEFT+Cons, particularly when paired with FTP, provides improved balance between stability and plasticity, reducing representational forgetting without substantially constraining task-specific adaptation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The systematic comparison of FullFT and PEFT strategies reveals nuanced insights into when and why representational forgetting occurs, using well-chosen metrics.\n2. Demonstrates up to over FTA points over the next-best method \n3. Figures provide a clear illustration of the improvement induced by the proposed approach."}, "weaknesses": {"value": "Limited Novelty in Core Mechanisms: The PEFT+Cons procedure, while well-executed and carefully evaluated, essentially combines PEFT and existing feature-preserving FullFT approaches in a sequential pipeline. It does not introduce fundamentally new algorithms or theoretical insights into representation preservation. The novelty largely resides in the two-stage orchestration, not in a new method or principle."}, "questions": {"value": "Can the authors justify the novelty of the proposed approach over the simple recombination of existing approaches from the same domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U4bjrKTB91", "forum": "WkMP1v5xu9", "replyto": "WkMP1v5xu9", "signatures": ["ICLR.cc/2026/Conference/Submission18690/Reviewer_AvJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18690/Reviewer_AvJk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017941694, "cdate": 1762017941694, "tmdate": 1762928389766, "mdate": 1762928389766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}