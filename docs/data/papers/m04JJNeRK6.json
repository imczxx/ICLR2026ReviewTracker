{"id": "m04JJNeRK6", "number": 3889, "cdate": 1757561457781, "mdate": 1759898064400, "content": {"title": "Premise Selection for a Lean Hammer", "abstract": "Neural methods are transforming automated reasoning for proof assistants, yet integrating these advances into practical verification workflows remains challenging. A $\\textit{hammer}$ is a tool that integrates premise selection, translation to external automatic theorem provers, and proof reconstruction into one overarching tool to automate tedious reasoning steps. We present LeanPremise, a novel neural premise selection system, and we combine it with existing translation and proof reconstruction components to create LeanHammer, the first end-to-end domain general hammer for the Lean proof assistant. Unlike existing Lean premise selectors, LeanPremise is specifically trained for use with a hammer in dependent type theory. It also dynamically adapts to user-specific contexts, enabling it to effectively recommend premises from libraries outside LeanPremise's training data as well as lemmas defined by the user locally. With comprehensive evaluations, we show that LeanPremise enables LeanHammer to solve 21\\% more goals than existing premise selectors and generalizes well to diverse domains. Our work helps bridge the gap between neural retrieval and symbolic reasoning, making formal verification more accessible to researchers and practitioners.", "tldr": "LeanHammer integrates neural premise selection with symbolic reasoning to automate theorem proving in Lean.", "keywords": ["premise selection", "interactive theorem proving", "automated reasoning", "contrastive learning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d0ee6ceecb3de6dc34d34ce36be30f0363ca475.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "I think this is a well-written and impactful paper that makes a meaningful contribution to the growing intersection between neural methods and formal theorem proving. The authors present LeanHammer, an end-to-end system for automated reasoning in the Lean proof assistant that combines a new neural premise selection model (LeanPremise) with existing components for translation and proof reconstruction. The central innovation lies in designing LeanPremise specifically for dependent type theory, and in showing that it can dynamically adapt to user-specific contexts including both large existing libraries and locally defined lemmas. This is a significant advance over prior premise selection systems for Lean, which were often static or trained on limited corpora.\n\nEmpirically, the paper is strong. The authors demonstrate that LeanPremise enables LeanHammer to solve 21% more goals compared to existing premise selectors, and that it generalizes effectively across diverse mathematical domains. I found the experiments to be thorough and convincing, and the evaluation metrics appropriate for the problem setting."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n\nClear Motivation and Novelty: The paper is well-motivated in the broader context of neuro-symbolic reasoning. The idea of combining neural retrieval (premise selection) with symbolic reasoning (hammer backends) is well-established in Isabelle and Coq, but its extension to dependent type theory in Lean is non-trivial and timely.\n\nPractical Integration: I appreciate that the authors actually built an integrated hammer for Lean! Something the community has wanted for years. The engineering effort here is nontrivial and valuable.\n\nAdaptive Context Handling: The dynamic adaptation to local user contexts is, in my view, one of the paper’s most important contributions. It shows a genuine understanding of how theorem proving workflows operate in practice.\n\nStrong Empirical Results: The 21% improvement in goal-solving rate is not just statistically significant — it’s practically meaningful. The ablation and generalization studies make a solid case for the effectiveness of LeanPremise.\n\nClarity and Presentation: The writing is clear, structured, and accessible even to readers not deeply familiar with Lean. The paper situates itself well within the literature on neural premise selection and hammer systems."}, "weaknesses": {"value": "Some minor weaknesses that i thought\n\n1. Model Details and Reproducibility: While the authors provide a solid description of the LEANPREMISE training pipeline, the architecture details of the encoder model (layer sizes, tokenization, embedding pooling, etc.) are somewhat brief. The training section mentions that they fine-tune MiniLM and DistilRoBERTa models, but additional detail on input formatting or tokenizer alignment with Lean syntax would help others reproduce or extend this work.\n\n2. Limited Discussion of Failure Cases: The experiments are comprehensive, but it would strengthen the paper to include more qualitative analysis of where the hammer fails e.g., typical patterns of unsuccessful proofs, sensitivity to theorem complexity, or when external provers (like Zipperposition) fail to reconstruct. Understanding these limits could guide future improvements to both premise selection and proof reconstruction."}, "questions": {"value": "1. I really liked that LeanPremise can incorporate locally defined facts at runtime. Could the authors clarify how new lemmas are embedded on-the-fly? For example, are these signatures tokenized and passed through the encoder server dynamically, or is there a cached retriever that updates incrementally?\n\n2. Did the authors explore hybrid ranking (e.g., mixing similarity-based and logic-based retrieval) or reinforcement-based refinement of premise scores?\n\n3. The paper describes four hammer variants (aesop, auto, aesop+auto, full). In practice, how sensitive is LeanHammer’s success rate to the choice of variant and to timeout parameters (e.g., 10 s for Zipperposition)? Would adaptive scheduling of these variants yield further gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6i7bYedYdZ", "forum": "m04JJNeRK6", "replyto": "m04JJNeRK6", "signatures": ["ICLR.cc/2026/Conference/Submission3889/Reviewer_XPxN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3889/Reviewer_XPxN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761192903521, "cdate": 1761192903521, "tmdate": 1762917084480, "mdate": 1762917084480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We are grateful to the referees for their thoughtful reviews. We are pleased that on the whole, the reviewers found our work to be well-motivated and practical. Several reviewers raised questions about details of our methodology (e.g. hardware used, hyperparameters, blacklists, etc.) and expressed a desire to see more analysis of where and how the hammer fails.\n \nConcerning the methodology, we are committed to making all aspects of LeanHammer and LeanPremise open source and as transparent as possible. We are optimistic that, should the paper be accepted, many questions can be simultaneously addressed by including links to:\n- Our data extraction scripts\n- Our model training scripts\n- The code that implements the LeanPremise server\n- The implementation of LeanHammer itself\n- Our exact evaluation splits\n \nIn addition to the above, we will modify the paper to address and clarify particular points mentioned by the reviewers (e.g. we will fix the typo discovered by u2zP, we will include hardware details requested by rKRM, and we will expand on the architecture details of the encoder model as requested by XPxN).\n \nConcerning the failure analysis, we agree that some additional comments in the main text would be beneficial. We do discuss some details related to failure analysis in our appendices. Appendix D.2 comments on the Carleson split, Appendix D.4 provides some statistics on the lengths of proofs that LeanHammer is (and isn’t) able to replace, and Appendix D.5 gives statistics on which components tend to be responsible for LeanHammer failures. But we agree that there should be more discussion of this in the main text, so in the next couple of days, we will modify the paper accordingly."}}, "id": "Z1Wv83lNdE", "forum": "m04JJNeRK6", "replyto": "m04JJNeRK6", "signatures": ["ICLR.cc/2026/Conference/Submission3889/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3889/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3889/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763658093005, "cdate": 1763658093005, "tmdate": 1763658093005, "mdate": 1763658093005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LEANPREMISE, a learned premise retriever for Lean, and integrates it into LEANHAMMER, an automated “hammer” tactic for Lean. The system retrieves relevant lemmas, translates to higher-order logic, calls ATPs, and reconstructs proofs back in Lean. The authors claim this is the first practical, general-purpose Lean hammer that normal Lean users can call. They train LEANPREMISE on hammer-style data (including implicit premises and both tactic/term proofs), and show strong gains on Mathlib and an out-of-distribution benchmark (miniCTX-v2), with substantially higher recall and proof rates than baselines like MePo or ReProver."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The pipeline design is careful and realistic for Lean’s dependent type theory. The data extraction method (capturing all premises truly used in a proof state, not just the next tactic) is well motivated, and ablations support it. The system also supports dynamically including a user’s local lemmas at inference time via FAISS indexing.\n\n2. LEANPREMISE achieves much higher recall@32 (≈70%+) than MePo and ReProver, and when plugged into LEANHAMMER, proof rates jump from ~20–27% (baselines) toward ~33%+ for Mathlib-test, and near 40% with ensembles. It also transfers to miniCTX-v2, suggesting generalization beyond Mathlib.\n\n3. A turnkey hammer for Lean is genuinely impactful for interactive theorem proving and AI-for-math. The work is clearly written, includes ablations, and promises to release code, data, and models."}, "weaknesses": {"value": "1. The paper does not compare against a simple LLM as premise suggester baseline: prompt a modern LLM with the Lean goal and ask it to list ~32 relevant lemmas from Mathlib, then feed those to the same hammer. Because the hammer mainly needs high recall and tolerates many irrelevant premises, this is an important baseline. It would clarify how much LEANPREMISE beats “just ask an LLM.”\n\n2. Some miniCTX-v2 domains are still barely solved, and it would help to briefly analyze where the pipeline fails (missing premises vs. reconstruction failures)."}, "questions": {"value": "How robust is LEANHAMMER to hallucinated lemmas (names that don’t exist)? Do they just get ignored like harmless false positives?\n\nWhat exact hardware produced the reported latencies, and how does latency scale with many user-defined lemmas?\n\nFor hard miniCTX-v2 splits, what is the main blocker: retrieval, ATP search, or Lean reconstruction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EyoBFzoubG", "forum": "m04JJNeRK6", "replyto": "m04JJNeRK6", "signatures": ["ICLR.cc/2026/Conference/Submission3889/Reviewer_rKRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3889/Reviewer_rKRM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739099215, "cdate": 1761739099215, "tmdate": 1762917084271, "mdate": 1762917084271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel premise selector, LeanPremise, tailored for premise selection for hammer use.\nThe authors fine-tune a textual encoder on carefully extracted (state, premise) data pairs with an InfoNCE-like loss, and rank premises based on embedding similarity. \nBy integrating LeanPremise with Aesop, Lean-auto and Duper, they also build a LeanHammer, which improves the accumulate proof rate from 27.5% (baseline, using MePo as the premise selector) to 33.3% on Mathlib-test. Notably, LeanHammer is currently accessible by simply using the corresponding tactic in Lean."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Significance: This work fills the blank that there is no hammer tool (premise selection + translate to external language for ATP/SMT solver application + translate back) in Lean. The proposed LeanHammer tool is practical. It has been built as a tactic in Lean, and I personally have been using it for daily hands-on proofs. The effort is quite non-trivial.\n\nClarity: The pipelines of LeanHammer and LeanPremise are clearly stated. Details such as how to handle the signature are enough covered. \n\nQuality: The performance of LeanPremise is verified extensively on multiple datasets and various experimental settings, compared with many previous methods as baselines.\n\nOriginality: (1)The data extraction pipeline is tailored for hammer use in many details, such as using all premises for the current goal rather than only for the next tactic. (2)  The use of InfoNCE loss for training the LeanPremise encoder, instead of binary-class MSE loss used by ReProver, is quite reasonable. (3) The LeanHammer pipeline not only makes use of current strong tools such as Aesop and external theorem provers like Zipperposition, and also alllows for adding additional automation tactics in the future as a rule of Aesop."}, "weaknesses": {"value": "1. The conceptual novelty of this work is limited. The overall design of LeanHammer mainly follows that of the Sledgehammer in Isabelle, and the LeanPremise is a standard embedding similarity based retrieval method, similar to what ReProver does. The contribution is mainly engineering rather than methodological. Nevertheless, the engineering effort is huge.\n2. The writing of the paper should be further checked. There are typos and confusing statements that could harm the understanding of the paper. For example, on page 6, $p_{i}^{+}$ should be changed to $p_{i}^{-}$ in the sum for negative premises in the denominator in the loss function expression, and the definition $$\\mathcal{N}_{i}=\\{p_{i}^{+}\\}_{i}\\cup\\{p_{ij}^{-}\\}_{ij}\\setminus\\mathcal{P}_{s_{i}}^{+}$$ should be simplified to $\\{p_{ij}^-\\}_{j=1}^{B^-}$ directly. \n3. The final performance achieved by LeanHammer is not that impressible compared to the performance of today's LLM theorem provers. On miniCTX-v2, the current SOTA Seed-Prover(light mode) can prove 81.8% of the theorems without accessing any external ATP/SMT solvers. In contrast, LeanHammer could only achieve 26.1% even with ground truth premises, let alone the 20.7% using LeanPremise."}, "questions": {"value": "1. I am most curious about whether implementing such a hammer to make use of external ATP/SMT solvers can help with LLM theorem provers. I will appreciate if the authors can provide some examples where the current open-weights SOTA models such as Goedel-Prover-V2-32B fail in pass@k(k=32 or so), but LeanHammer can help the model find a proof.\n\n2. Have you tried to add automation tactics other than simp_all, such as linarith, omega, native_decide, etc. into the pipeline? My experience is that these tactics can usually help solve goals that Aesop, simp_all and external ATPs cannot solve. I agree with that simp_all can not help improve further from Aesop."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C9eKODLkY1", "forum": "m04JJNeRK6", "replyto": "m04JJNeRK6", "signatures": ["ICLR.cc/2026/Conference/Submission3889/Reviewer_u2zP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3889/Reviewer_u2zP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757648968, "cdate": 1761757648968, "tmdate": 1762960336379, "mdate": 1762960336379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LEANPREMISE, an LM-based premise selector tailored for use with a hammer in Lean’s dependent type theory, and integrates it with Aesop, Lean-auto (DTT to HOL translation to external ATPs), and Duper to build LEANHAMMER, \"the first end-to-end domain-general hammer for Lean.\" The selector standardizes premise/state serialization, trains with a masked contrastive objective, and dynamically incorporates user/local premises. On Mathlib and miniCTX-v2, the system improves proof rates over prior premise selectors and shows complementary behavior with MePo. The authors provide ablations and claim open-sourced code, data, and trained models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Positions premise selection explicitly for a Lean hammer, not just for next-tactic generation, with hammer-aware data extraction (term + tactic proofs; collecting implicit simp/rw premises; signature normalization that avoids notation brittleness). This is a concrete, useful framing that, as far as I know, prior Lean premise selectors did not target.\n\n2. Dynamic augmentation with project-local premises and server-side caching for low-latency retrieval is practically novel for Lean users.\n\n3. Clear methodology for training: masked InfoNCE with explicit negatives; ablation shows each training choice (negatives, loss mask, data extraction) matters.\n\n4. If robust, first domain-general hammer for Lean is a meaningful community contribution, potentially changing day-to-day Lean workflows and enabling downstream NTP systems to \"call a hammer\" in Lean."}, "weaknesses": {"value": "1. The paper retrains/ports baselines (e.g., MePo to Lean; ReProver on the authors’ splits) and changes data extraction relative to next-tactic systems; while understandable, this complicates strict novelty/efficacy attribution. It would be good to expand baseline re-implementation details and release scripts to reproduce exact comparison settings (hyperparameters, filters, blacklists) and the exact evaluation splits.\n\n2. Success depends on Lean-auto translation quality and Duper reconstruction once Zipperposition returns premises. It would be good to have more failure analyses, for example to provide breakdowns of (a) external ATP success vs. Duper reconstruction failure, (b) cases where Aesop premise applications alone succeed vs. Lean-auto is essential, and (c) how often missing implicit vs. explicit premises is the root cause.\n\n3. Filtering 479 \"basic logic\" theorems and aggressive fully-qualified, notation-free signatures are sensible, but please quantify how each filter/format affects recall, ATP success, and end-to-end proofs on a held-out set (beyond the aggregate ablation), and whether performance is brittle to Mathlib version changes.\n\n4. The reported metrics could be richer. For example, consider reporting proof length, number of premises used, and time-to-first-proof distributions, to understand proof mechanics, as well as the effect of k (and of per-step dynamic k) across problem difficulty buckets, etc."}, "questions": {"value": "1. While “domain-general” within Lean, it’s unclear how easily the approach ports to other proof assistants like Rocq or Isabelle. A cross-assistant pilot (even small-scale or case-by-case) would strengthen the generality claim and offer insights for future work to extend the approach.\n\n2. In the “ground-truth premises” oracle runs, how many of those premises are actually required for the found proof vs. incidental? Could a minimal-premise oracle raise the theoretical ceiling?\n\n3. When users work in private libraries with heavy custom notations (even if you print fully-qualified constants), how often do premise embeddings drift enough to harm retrieval? Any quantitative OOD tests beyond miniCTX-v2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SC32Q9pDXl", "forum": "m04JJNeRK6", "replyto": "m04JJNeRK6", "signatures": ["ICLR.cc/2026/Conference/Submission3889/Reviewer_cvrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3889/Reviewer_cvrg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999931768, "cdate": 1761999931768, "tmdate": 1762917083424, "mdate": 1762917083424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}