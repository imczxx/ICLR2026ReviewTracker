{"id": "lXDg6quPKT", "number": 13161, "cdate": 1758214376239, "mdate": 1763575065732, "content": {"title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as the leading approach for enhancing reasoning capabilities in large language models. However, it faces a fundamental compute and memory asymmetry: rollout generation is embarrassingly parallel and memory-light, whereas policy updates are communication-heavy and memory-intensive. To address this, we introduce **PODS** (**P**olicy **O**ptimization with **D**own-**S**ampling), which decouples rollout generation from policy updates by training only on a strategically selected subset of rollouts, maintaining learning quality while dramatically reducing update costs. We propose a principled subset selection criterion‚Äî*max-variance down-sampling*‚Äîthat maximizes reward diversity, and provide an efficient $O(n\\log n)$ implementation. Empirically, Group Relative Policy Optimization (GRPO) with PODS achieves the peak test accuracy of vanilla GRPO at least $\\mathbf{1.7\\times}$ **faster** across the different reasoning benchmarks and hardware configurations we tested.", "tldr": "We propose a method that makes GRPO training significantly faster by generating many rollouts in parallel but only training on the most informative subset.", "keywords": ["GRPO", "down-sampling", "reinforcement learning", "large language models", "RLVR"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1ba7227d9f1405c08bf8c11b423cb5bd8075d28d.pdf", "supplementary_material": "/attachment/6f7313be777115fbd7ae6b082ae873d5a53303ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces PODS, a data selection framework designed to improve the efficiency of GRPO training. The key idea is that rollout generation can be efficiently parallelized via batching, whereas policy updates become costly with large rollout sizes. To address this, PODS downsamples generated rollouts by selecting those with the lowest and highest rewards, thereby maximizing reward variance within each training batch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "+ GRPO efficiency is an important and timely problem in RLHF.\n+ The paper is overall well-written and well-structured.\n+ Empirical results show encouraging performance improvements."}, "weaknesses": {"value": "- The contribution is relatively incremental, both in the data selection strategy and in algorithmic design.\n- The evaluation is limited in scope: benchmarks focus only on GSM8K and MATH, and baselines are restricted to the naive GRPO implementation. Including code generation or reasoning benchmarks would strengthen the results. Also, comparative analysis could be more comprehensive using relevant baselines, such as GRESO [1]."}, "questions": {"value": "Thank you for submitting this work. The paper tackles an important problem in RLHF training efficiency, but I am concerned that the novelty is limited and that key design choices are not sufficiently justified. Below are specific comments and questions:\n\n- Q1: he complexity of Algorithm 1 should be O(mlogn), since maintaining the m lowest and highest rollouts in a priority queue avoids sorting the entire list. Could the authors clarify?\n\n- Q2: How does PODS perform on more diverse tasks and benchmarks, such as code generation or reasoning datasets? Including stronger baselines (e.g., GRESO [1]) would make the results more convincing.\n\n- Q3: Could the authors elaborate on why PODS leads to better final model performance? This improvement and claim seem highly sensitive to the choice of the hyperparameter m.\n\nReference:\n\n[1] Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts, NeurIPS 2025 / arXiv:2506.02177."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dGk2Rf1tkx", "forum": "lXDg6quPKT", "replyto": "lXDg6quPKT", "signatures": ["ICLR.cc/2026/Conference/Submission13161/Reviewer_ddiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13161/Reviewer_ddiB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760844873170, "cdate": 1760844873170, "tmdate": 1762923873488, "mdate": 1762923873488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear Area Chairs and Reviewers,\n\nThank you for your time reviewing our submission, \"Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning.\"\n\nAfter careful consideration of the reviews, we have decided to withdraw our paper and substantially revise it for resubmission to another venue. We believe there are several core misalignments between the reviews and our paper's stated scope and contributions that would require significant clarification.\n\nSpecifically:\n* Our explicit focus on RLVR (Reinforcement Learning with Verifiable Rewards) was not consistently recognized. Additionally, our experiments were characterized as using \"binary rewards,\" despite our use of multifaceted, noisy, non-binary rewards that evaluate both response structure and final answers\n* The systems-level motivation presented in Figure 1, which forms the basis of our method, appears to have been overlooked\n* Some comparisons drawn were to contemporaneous work that post-dates and cites our preprint\n\nWe have provided detailed individual rebuttals for the record. However, we believe the gap between our framing and the reviews' interpretation indicates we need to substantially improve our exposition before resubmission.\n\nWe appreciate the time invested in reviewing our work and will use this feedback to strengthen the paper.\n\nSincerely,\n\nThe Authors"}}, "id": "qKzoo4czpD", "forum": "lXDg6quPKT", "replyto": "lXDg6quPKT", "signatures": ["ICLR.cc/2026/Conference/Submission13161/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13161/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763575064778, "cdate": 1763575064778, "tmdate": 1763575064778, "mdate": 1763575064778, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces PODS (Parallel Optimized Down-Sampling), a lightweight and algorithm-agnostic framework designed to tackle a core inefficiency in modern Reinforcement Learning with Verifiable Rewards (RLVR): the mismatch between highly parallelizable rollout generation and memory-constrained, sequential policy updates. PODS addresses this by generating large batches of rollouts in parallel and selectively updating the policy on a small, informative subset chosen via a max-variance selection rule. \n\nDespite its simplicity, PODS delivers consistent improvements: under identical wall-clock time budgets, it outperforms standard GRPO, achieves at least a 1.7√ó training speedup, and attains higher final accuracy across diverse model architectures, scales, and deployment settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Well-written\n(2) Detailed experiment\n(3) The problems related to training efficiency that have been solved are distinctive and seem valuable to the industrial sector"}, "weaknesses": {"value": "N/A"}, "questions": {"value": "I do not know this specialized research field very well. I will adjust my score and optimize my review document based on the evaluations of other expert reviewers and my performance during the rebuttal period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "jbA5E7dxMc", "forum": "lXDg6quPKT", "replyto": "lXDg6quPKT", "signatures": ["ICLR.cc/2026/Conference/Submission13161/Reviewer_K7yh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13161/Reviewer_K7yh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479367976, "cdate": 1761479367976, "tmdate": 1762923873249, "mdate": 1762923873249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims at the computational inefficency RLVR for large languange models. This paper identify a key asymmetry between the paralleizable rollout generation phase and memory-intensive policy update phase. To mitigate this, this paper proposes PODS, a general framework that generates rollouts but selective update the policy with a subset of the rollouts. Meanwhile, thie paper introduces a max-variance down-sampling method to select rollout with maximum reward diversity. Experiments with GRPO on GSM8k and MATH shows that PODS maintains comparable accuracy performance and more than 1.7x faster convergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: This paper tackles an underexplored issue in LLM RL. The proposed PODS framework and max-variance down-sampling criterion represent an interesting aspect of efficiency optimization. This work distincts from existing prompt-selection or gradient-accumulation approaches. The idea of selective subset section is conceptually simple and original.\n2. Quality: The methodology is clearly formalized with simple mathematical justification. Theoretical analysis provides clarity and computational guarantees for the proposed down-sampling rule. The experiments are comprehensive, spanning different model sizes, datasets, and hardware setups.\n3. Clarity: The paper is well-written and easy to follow. The motivation for the problem is clearly illustrated. The presentation of algorithms and visual comparisons are well-structured. \n4. Significance: The work addresses a bottleneck in scaling reinforcement learning for LLMs, reducing memory and communication overhead during policy updates. Given the rising computational costs in reasoning-focused RL for LLMs, the proposed method is timely and practically impactful. This framework can be integrated with other RL variants, increasing its potential influence and relevance for large-scale LLM training."}, "weaknesses": {"value": "1. LImited scope empirical validation. As mentioned in the limitation section, the evaluation is conducted on mathematical reasoning tasks (GSM8K, MATH) with rule-based reward models. These tasks have well-defined, verifiable rewards, which may overstate the benefits of the method. The paper would be significantly strengthened by including results on other multiple tasks where reward distributions are noisier and less binary.\n2. Dependence on single baseline RLVR algorithm. Although the method is claimed to be algorithm-agnostic, all experiments and analyses are built around GRPO. It remains unclear whether the same variance-based selection criterion would hold for other RL frameworks.  Demonstrating adaptability across multiple RL algorithms would make the contribution more convincing.\n3. Lack of deeper theoretical justification of learning dynamics. The paper provides a sound combinatorial analysis for max-variance selection but does not connect this formally to expected policy improvement or gradient variance reduction. Without a theoretical link between rollout diversity and learning efficiency, the variance criterion, while intuitive, remains heuristic.\n4. Limited discussion of potential trade-offs: While the paper reports 1.7√ó‚Äì3√ó speedups, it provides little detail about wall-clock breakdowns (e.g., inference vs. update time) or the communication cost savings. Furthermore, potential drawbacks such as off-policy bias or degradation of gradient fidelity with extreme down-sampling ratios are acknowledged but not empirically investigated.\n5. Meanwhile, the LLM used in this paper is simply 3B and 7B, of which the inference time consumption and training time is fair. However for larger LLMs, the inference time and training time increase significantly. The time complexity of subset selection, nlogn, is not that important, especially for rollout number of 64 and subset size 16. The reduction of average training seconds per training step is mainly caused by the reduction of training data. Therefore, the time complexity of this subset selection is fair theoretically but limited in actual training."}, "questions": {"value": "1. The evaluation focuses exclusively on GSM8K and MATH which have verifiable and relatively noise-free rewards. Can the authors discuss any preliminary evidence or theoretical reasoning suggesting that the max-variance down-sampling rule remains effective under noisy or sparse reward distributions?\n2. While PODS is described as algorithm-agnostic, could the authors explain what modifications, if any, would be required to adapt PODS to reinforce++ or to reinforcement learning from human feedback (RLHF) setups? Please provide one of the demonstration results.\n3. Since PODS alters the effective training distribution by selective sampling, does this introduce off-policy bias relative to standard on-policy GRPO?\n4. Have the authors considered or tested any correction mechanisms (e.g., importance weighting) to mitigate this potential bias? A short theoretical justification or empirical comparison could help assess the trade-off between efficiency and policy fidelity.\n5. The proposed max-variance rule is intuitive and empirically effective, but the paper does not formally connect it to expected policy improvement. Could the authors provide analytical or empirical arguments showing that higher reward variance indeed correlates with more informative gradients or faster convergence in GRPO-like updates?\n6. The paper reports wall-clock speedups, but it would be helpful to see a breakdown of inference time, policy-update time, subset data selection time, especially in distributed setups.\n7. How does max-variance down-sampling compare to other principled criteria such as entropy-based selection, uncertainty sampling, or advantage-based weighting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wa87JpmfKi", "forum": "lXDg6quPKT", "replyto": "lXDg6quPKT", "signatures": ["ICLR.cc/2026/Conference/Submission13161/Reviewer_XkuY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13161/Reviewer_XkuY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790034296, "cdate": 1761790034296, "tmdate": 1762923872910, "mdate": 1762923872910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets a very concrete bottleneck in reinforcement learning for LLM reasoning with verifiable rewards: rollout generation is cheap and highly parallelizable, but policy updates (GRPO/PPO-style) are memory- and communication-bound, so we can‚Äôt just ‚Äúgenerate more rollouts‚Äù to improve sample quality. The authors propose PODS (Policy Optimization with Down-Sampling): for each prompt, generate a large pool of rollouts, score them with the verifiable reward, and then select only a small, most informative subset for the policy update. The key technical piece is a max-variance selection rule: choose the size-ùëö subset whose rewards have the largest variance, which they show always corresponds to ‚Äútake some of the lowest and some of the highest‚Äù rollouts; this yields an efficient $O(nlogn)$ algorithm. Plugged into GRPO, PODS achieves 1.7√ó‚Äì3√ó faster wall-clock to the same or better accuracy on GSM8K/MATH and across Qwen2.5 and Llama 3.2 models, on both single-GPU (LoRA) and multi-GPU (full-param) setups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Very well-motivated problem. The paper identifies a real training-systems bottleneck: inference scales, but updates don‚Äôt.\n2. Simple, plug-in idea. PODS is architecturally lightweight: keep your GRPO pipeline, just over-generate and then down-sample. This makes adoption easy.\n3. Principled selection rule. Instead of just ‚Äúpick top-k,‚Äù they argue that we want reward diversity. The max-variance formulation + structure theorem is a nice, clean piece of theory that justifies the heuristic.\n4. Strong empirical evidence. Multiple models, multiple hardware regimes , and realistic tasks. The improvement is in wall-clock, not just final accuracy, which matters in practice.\n5. Clear ablations. They study both the number of generated rollouts n and the number of kept rollouts m, and compare to random and max-reward selection. The proposed max-variance rule consistently wins or ties."}, "weaknesses": {"value": "1. Task scope is narrow. All experiments are on verifiable math-style rewards. These are low-noise, almost binary rewards. It‚Äôs not fully clear that the same max-variance rule is optimal when rewards are noisy, delayed, or dense (e.g., coding with partial credit, preference RL, or tool-use tasks).\n2. Mild off-policy effect not deeply analyzed. By selecting only a subset of generated samples, the method introduces some off-policy bias relative to pure on-policy GRPO. In practice it seems fine (results look good), but the paper could say more about stability under very aggressive down-sampling (e.g. keeping 2 out of 64).\n3. Assumes you can cheaply over-generate. The whole story relies on the common RLVR setup where generation is the easy part. In settings where inference is also bottlenecked (long contexts, tools, multi-turn), the benefit may shrink.\n4. No non-verifiable / preference benchmark. Even one experiment on a noisier or non-binary task would make the generality claim stronger."}, "questions": {"value": "1. Reward noise: How sensitive is max-variance selection when reward signals have small stochastic noise (e.g., randomized unit tests for code)? Does the bottom-and-top structure still hold in practice?\n2. Extremely unbalanced batches: If almost all rollouts succeed (or all fail), does the algorithm degrade gracefully to a reasonable selection (e.g. pick the rare failures/successes)?\n3. Generalization to other objectives: You show PODS with GRPO. Would you expect the same variance criterion to work for PPO-like preference RL where the ‚Äúcontrast‚Äù is not purely on reward but on pairwise preferences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "59D8c0bD4H", "forum": "lXDg6quPKT", "replyto": "lXDg6quPKT", "signatures": ["ICLR.cc/2026/Conference/Submission13161/Reviewer_ByiU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13161/Reviewer_ByiU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985695052, "cdate": 1761985695052, "tmdate": 1762923872640, "mdate": 1762923872640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}