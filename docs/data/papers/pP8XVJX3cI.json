{"id": "pP8XVJX3cI", "number": 12393, "cdate": 1758207524250, "mdate": 1763602178257, "content": {"title": "Deep-ICE: The first globally optimal algorithm for empirical risk minimization of two-layer maxout and ReLU networks", "abstract": "This paper introduces the first globally optimal algorithm for the\nempirical risk minimization problem of two-layer maxout and ReLU networks,\ni.e., minimizing the number of misclassifications. The algorithm has\na worst-case time complexity of $O\\left(N^{DK+1}\\right)$, where $K$\ndenotes the number of hidden neurons and $D$ represents the number\nof features. It can be can be generalized to accommodate arbitrary\ncomputable loss functions without affecting its computational complexity.\nOur experiments demonstrate that the proposed algorithm provides provably\nexact solutions for small-scale datasets. To handle larger datasets,\nwe introduce a heuristic method that reduces the data size to a manageable\nscale, making it feasible for our algorithm. This extension enables\nefficient processing of large-scale datasets and achieves significantly\nimproved performance in both training and prediction, compared to state-of-the-art approaches\n(neural networks trained using gradient descent and support vector\nmachines), when applied to the same models (two-layer networks with\nfixed hidden nodes and linear models).", "tldr": "The first globally optimal algorithm for empirical risk minimization of two-layer maxout and ReLU networks", "keywords": ["Neural network", "Global optimal", "Algorithm design", "Combinatorial optimization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/282c2be0fc35262503bb6dc2ee16eb336a7a057f.pdf", "supplementary_material": "/attachment/c434887d7ad5eac6a5bd2d5099f1800eb8ff3902.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a globally optimal algorithm for minimizing 0-1 loss in two-layer ReLU and Maxout networks and shows improved computational complexity. Also, for larger dataset, this paper introduces a heuristic method such that the dataset is feasible for the proposed algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well structured and clearly written.\n2. The problem is well motivated, and the proposed approach is methodologically sound.\n3. The discussion of related work is comprehensive and demonstrates a strong understanding of the existing literature."}, "weaknesses": {"value": "1. The presentation of the Table 1 is not clear enough.\n2. The experimental validation is not fully convincing. The experiments show that the algorithm proposed in this paper has better performance compared with baselines, but I am also curious of the running time of different approaches. Also, the proposed algorithm has improved computational complexity compared with literature. I am wondering if the improved computational complexity can be validated experimentally.\n3. The evaluation could be strengthened by including additional large datasets with more features (large $N$ and $D$)."}, "questions": {"value": "Please see above.\nThe meaning of numbers in Table 1 is not clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GmyX91PqZV", "forum": "pP8XVJX3cI", "replyto": "pP8XVJX3cI", "signatures": ["ICLR.cc/2026/Conference/Submission12393/Reviewer_fiaE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12393/Reviewer_fiaE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619680116, "cdate": 1761619680116, "tmdate": 1762923293786, "mdate": 1762923293786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Deep-ICE, a globally optimal algorithm for minimizing 0–1 loss in two-layer ReLU and maxout networks. The method exhaustively and efficiently searches over feature splits via a recursive nested-combination generator with CUDA acceleration, enabling exact training on small datasets and heuristic coreset-based extensions for larger problems. The authors provide formal correctness claims, complexity analysis, and empirical comparisons to SVMs and MLPs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The algorithmic description and theoretical details are well-structured and readable.\n- Constructing an efficient search over all feature splits is nontrivial and technically interesting.\n- CUDA implementation and memory optimization increase the practical relevance.\n- Addresses an important research direction: global optimization of neural networks under 0–1 loss."}, "weaknesses": {"value": "- The claim that two-layer networks are interpretable (unlike linear models) needs stronger justification, especially given nonlinear thresholds.\n- Several relevant exact [1] or gradient-based [2, 3] 0–1 loss optimization methods are not cited nor discussed.\n- For example, on the dataset from Figure 1, EXACT (with Tanh activation) achieves 18 errors with 2 hiddens and 16 errors with 5 hiddens, substantially outperforming MLP’s 25 errors.\n- A Python interface to the CUDA implementation would be beneficial."}, "questions": {"value": "- What exactly makes a 2-layer ReLU/maxout model “interpretable”? Can the authors provide interpretability examples or a measure?\n- How does Deep-ICE compare to global optimization methods like [1] and EXACT [3]? Can runtime and performance comparisons be added?\n- Do the authors have plans to release a Python library interface for CUDA to improve usability?\n\nThe score can be adjusted based on the responses (especially related work).\n\n[1] Efficient global optimization of two-layer relu networks: Quadratic-time algorithms and adversarial training (2022)\n\n[2] Algorithms for direct 0–1 loss optimization in binary classification (2013)\n\n[3] EXACT: How to train your accuracy (2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5707pkgdfl", "forum": "pP8XVJX3cI", "replyto": "pP8XVJX3cI", "signatures": ["ICLR.cc/2026/Conference/Submission12393/Reviewer_8CaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12393/Reviewer_8CaP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733981806, "cdate": 1761733981806, "tmdate": 1762923293483, "mdate": 1762923293483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first algorithm for finding the globally minimal empirical risk of two layer neural networks under 0–1 loss. The algorithm achieves polynomial complexity for fixed input feature size D and hidden feature size K, i.e. $O(2^{K-1}N^{DK+1})$ compared with previous $2^KC_1N^{DK+C_2}$. When combined with heuristics for large-scale problems, such as coreset selection, the proposed algorithm demonstrates strong out-of-sample performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper introduces the first globally optimal algorithm for the empirical risk minimization problem of two-layer maxout and ReLU networks, i.e., minimizing the 0-1 loss.\n\n2.\tExperiments demonstrate better performance than those of SVMs and the same maxout network trained with gradient descent.\n\n3.\tThe paper develops an efficient recursive nested combination generator for GPU execution."}, "weaknesses": {"value": "1.\tThere are confusing statements in the paper. In line 105, the paper says ”our algorithm demonstrates strong out-of-sample performance, even when **training accuracy is lower than** that of SVMs or DNNs trained with gradient descent” and the in line 483 the paper claims that the proposed method “achieves significantly **higher training accuracy** than SVMs or two-layer neural networks, still perform well on unseen data when model complexity is properly controlled”. The two claims appear to be in conflict. Besides, there is no clear evidence or discussion in the paper to support either of them.\n\n2.\tAnother concern is about computation efficiency as the method needs to enumerate data points. Is it possible to have computation time comparison?"}, "questions": {"value": "1.\tIn table 1, there are two numbers delimited by ‘/’. What do the two numbers denote? What is the difference?\n\n2.\tThe paper argues that study of two layer of neural network will benefit model interpretability. However, the model output is a linear combination of hidden units, which is hard to interpret. If possible, please explain more about why two layer of neural network benefit interpretability.\n\n3.\tTypo in 69,70, $C_1, C_2$ should be switched. What is $K_-$ in line 383?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "blKalQcC9U", "forum": "pP8XVJX3cI", "replyto": "pP8XVJX3cI", "signatures": ["ICLR.cc/2026/Conference/Submission12393/Reviewer_Dei1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12393/Reviewer_Dei1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930333745, "cdate": 1761930333745, "tmdate": 1762923293184, "mdate": 1762923293184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}