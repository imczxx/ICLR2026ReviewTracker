{"id": "sa0udzVW4M", "number": 2164, "cdate": 1757005857912, "mdate": 1762964578128, "content": {"title": "ShadowDraw: From Any Object to Shadow–Drawing Compositional Art", "abstract": "We introduce *ShadowDraw*, a framework that transforms ordinary 3D objects into shadow–drawing compositional art. Given a 3D object, our system predicts scene parameters---including object pose and lighting---together with an incomplete line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that *ShadowDraw* produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow–drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our anonymous [project page](https://anonymous.4open.science/w/ShadowDraw-anon-E584/) for more results.", "tldr": "We present \\textsc{ShadowDraw}, a framework that transforms arbitrary 3D objects into shadow–drawing compositional art, where the cast shadow seamlessly completes a generated incomplete line drawing to form a recognizable image.", "keywords": ["Computational visual art", "Computation art design", "Shadow art"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/306622b54e560d50e1d33eb39ed5f1af770c2c6d.pdf", "supplementary_material": "/attachment/650473267021ed403fa1a050dd624cd8ed142dc1.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a framework for generating compositional art using shadows. Shadows cast by 3D objects are used to complete partial line drawings and represent a concept as a whole. Parameters like lighting, pose are optimized and line drawings are generated conditioned on contours of shadows."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel problem statement, finding novel concepts from shadows and drawing them out. \n- Using boundary contours rather than raw shadows as conditioning provides stronger geometric guidance"}, "weaknesses": {"value": "- The paper lacks significant novelty. The paper chains different networks from data generation to training with limited novelty. While the shadow contouring is effective, as a primary contribution it is not significant. The idea to draw lines on shadows from real objects is quite niche and from a storytelling point of view. \n- The paper lacks clarity in many places. Ranking section in 3.3 can be further explained and user studies lack sufficient explanation in the main paper. The contents of the paper are also scattered in appendix and main paper with no proper structure. (eg. user studies referenced in results and analyses but described in appendix)\n- The time sink of the entire pipeline from object to generated drawing is 40 mins (important detail, should be mentioned in main paper and not appendix). This is a prohibitively large duration for a relatively simple task. While there might be a weak question on whether this kind of automated shadow drawing is required, the time spent computing does not seem to justify the output."}, "questions": {"value": "- Why did the authors choose Flux-Canny ? It seems a particularly complex network for simple line drawings. \n- What do the authors mean by a \"greedy merging algorithm\"? \n- Is there any difference between S and c_i in Eq. 3 and 1 respectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dyut0OXRmF", "forum": "sa0udzVW4M", "replyto": "sa0udzVW4M", "signatures": ["ICLR.cc/2026/Conference/Submission2164/Reviewer_Cdhm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2164/Reviewer_Cdhm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959530410, "cdate": 1761959530410, "tmdate": 1762916075112, "mdate": 1762916075112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "X83Yq4hRxy", "forum": "sa0udzVW4M", "replyto": "sa0udzVW4M", "signatures": ["ICLR.cc/2026/Conference/Submission2164/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2164/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762964577304, "cdate": 1762964577304, "tmdate": 1762964577304, "mdate": 1762964577304, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to create line drawings with an effect called \"shadow drawing\". The shadow is cast by an 3D object and becomes a part of the drawing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis is an interesting problem. It needs some efforts in considering the optical modeling to achieve such effects.\n2.\tThe  application looks satisfying and I will like it when I have a “shadow drawing” in my office.\n3.\tThe technical design looks rational and  those components like line drawing processing and image/lineart completion are explainable and will really achieve the wanted effects."}, "weaknesses": {"value": "1.\tI am not sure if this work fits ICLR. From my perspective this paper looks typical for SIGGRAPH/ASIA/TOG. Nevertheless, if other reviewers think it is okay for ICLR scope then we can see this as a kind of “learning representation”.\n2.\tFrom the technical standard of ICLR, the main uniqueness of this problem is likely an inpainting task to find where and what to inpaint a line drawing. Although the shape needs to match an 3D object shadow, the pure technical aspect seems very narrow considering the image editing capability of current SOTA image models. \n3.\tHowever I can still understand the problem value and some HCI value in the evaluation. But I am not sure the level of contribution in machine learning audiences. I will wait for other reviews and inputs."}, "questions": {"value": "1. What CLIP model is used for CLIP score? There are many better CLIPs like SigLIP2 that can replace the CLIP score for better evaluation.\n2. I would see more details about the \"Human Preference Score\", like the user instructions and user grouping/counting..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cjrMRDjsCt", "forum": "sa0udzVW4M", "replyto": "sa0udzVW4M", "signatures": ["ICLR.cc/2026/Conference/Submission2164/Reviewer_7MHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2164/Reviewer_7MHK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960773151, "cdate": 1761960773151, "tmdate": 1762916074919, "mdate": 1762916074919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an interesting framework that creates shadow–drawing art from everyday 3D objects. Given any real-world object, the system jointly predicts the object pose, light source configuration, and a partial sketch that completes the composition, such that the cast shadow naturally integrates with the drawing. To enhance the generative model’s ability to produce coherent sketches, the authors propose conditioning on the shadow contour—the binary outline of the shadow—instead of using raw shadow images or object–shadow composites. The methodology is clearly described, combining scene parameter optimization, line drawing generation, and automatic evaluation to ensure visual and semantic coherence. For evaluation, the authors compare their approach with Gemini Flash 2.5 under different conditioning settings, demonstrating a significantly higher human preference score and better visual quality across various objects."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and well structured.\n- The paper introduces a novel and well-motivated problem—generating shadow–drawing compositional art from arbitrary 3D objects—which extends the frontier of computational visual art and connects physical rendering with generative modeling in an elegant way.\n- The proposed framework is technically sound and well engineered, combining shadow contour conditioning, differentiable scene parameter optimization, and automatic evaluation in a coherent pipeline. Each component is justified and shown to contribute meaningfully through ablations.\n- The use of shadow contours as conditioning improves geometric alignment and data scalability, addressing a core limitation of prior image- or composite-based conditioning approaches.\n- The experimental results show a predominant user preference compared to the baseline methods."}, "weaknesses": {"value": "- While the paper presents an elegant and creative system, the core contributions lie primarily in system design and integration for artistic creation, rather than introducing new scientific, algorithmic, or engineering advances.\n- The method relies on having accurate 3D object models (digital twins) in the Scene Configuration Selection stage, which may limit its applicability in casual or unstructured real-world settings where high-quality geometry is unavailable.\n- The runtime is relatively long, as generating one final composition requires substantial computation per object, reducing scalability for practical or interactive applications."}, "questions": {"value": "- How practical is the method for real-world scenarios involving everyday objects? Does it require a full and accurate 3D reconstruction of each object before generating the shadow–drawing composition, or could it generalize to partial scans or single-view captures?\n\n- In the scene parameter optimization stage, how is the search space of lighting and object configurations defined and constrained? Specifically, how do the authors ensure that the optimized light positions remain physically plausible and easily reproducible in real-world setups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9nNnOj7YjD", "forum": "sa0udzVW4M", "replyto": "sa0udzVW4M", "signatures": ["ICLR.cc/2026/Conference/Submission2164/Reviewer_AGLE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2164/Reviewer_AGLE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976660977, "cdate": 1761976660977, "tmdate": 1762916073314, "mdate": 1762916073314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a system that automatically creates shadow-drawing compositions — line drawings whose meaning is completed by the cast shadow of an arbitrary 3D object. The pipeline jointly predicts (i) a stylized line drawing and (ii) scene parameters (object pose and lighting) such that the resulting shadow aligns with and completes the drawing.\nThe method integrates:\n* a shadow-contour–conditioned diffusion model (LoRA on FLUX) trained on synthetic contour→drawing pairs,\n* a differentiable scene optimizer maximizing fractal dimension of the shadow contour to encourage visually rich silhouettes,\n* a VLM-based prompt generator that proposes textual concepts matching each contour,\n* and automated ranking/filtering using CLIP, ImageReward, Human Preference Score, and a concealment metric.\nQualitative results show creative and often convincing “shadow-drawing” compositions, including multi-object and animated scenes, and physical prototypes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Novel problem formulation. Casting “shadow-drawing art” as a computational generation task is original and visually compelling.\n* End-to-end system design. Integrates differentiable rendering, optimization, text-vision prompting, and diffusion-based generation into a cohesive automated pipeline.\n* Empirical insight: Demonstrates that closed shadow contours yield stronger conditioning than grayscale shadows or composites for contour-to-drawing generation.\n* Synthetic data pipeline. Clever use of LLMs and a LoRA diffusion model to synthesize large contour–drawing pairs when artist data is scarce.\n* Differentiable fractal-dimension objective. Creative heuristic to search lighting/pose configurations that produce visually interesting shadows.\n* Automated evaluation and ranking. Uses learned reward metrics (ImageReward, HPS) to filter appealing results; this produces consistent quality.\n* Solid visuals. Includes single/multi-object, animation, and real-world setups demonstrating generality and practical creativity."}, "weaknesses": {"value": "* I’m unsure if there is sufficient new learning for a venue like ICLR. Core algorithmic components (diffusion, LoRA, VLM prompting, differentiable rendering) are standard; main contribution lies in engineering and system integration rather than a new learning principle or model.\n\n* What is the relationship between the number of sampled views (I believe 48 is default) and success rate? Can users supply a target view/light source and get line drawings that would be a good fit?\n\n* The lack of control is a concern for an artist expression tool like this. The user can pick the 3D model, but it does’t appear they can pick the contour drawing of interest or the text prompt - several text prompts are sampled and an automated VQA scoring pipeline filters candidates. Thus there is limited controllability or guarantee of semantic adherence.\n\n* The reported inference costs are quite slow to be useful in practice, requiring 20-40 mins per input object without guarantee that the synthesized results are acceptable to the user. \n\n* This is primarily a creative-system contribution; I'm unsure if there is a theoretical or generalizable ML insight expected for this venue.\n\n* Analysis of the text prompt: what is the sensitivity of the line art generator to the level of details in the text prompt? The authors could consider evaluating this generative model’s performance independent of the full system (e.g., conditioning accuracy, FID, CLIP alignment, prompt fidelity, or diversity).\n\n* In addition, there is an opportunity for learning-based scene optimization, which could elevate the contribution further. Instead of heuristic sampling, a model could be trained to predict the optimal view/light configuration given a 3D object and (optionally) a target drawing or contour. This could accelerate the inference process and add more user control."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jdqu0EKHQr", "forum": "sa0udzVW4M", "replyto": "sa0udzVW4M", "signatures": ["ICLR.cc/2026/Conference/Submission2164/Reviewer_f574"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2164/Reviewer_f574"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762396291163, "cdate": 1762396291163, "tmdate": 1762916067850, "mdate": 1762916067850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}