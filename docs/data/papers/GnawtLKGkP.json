{"id": "GnawtLKGkP", "number": 25236, "cdate": 1758365569601, "mdate": 1759896728429, "content": {"title": "Any-step Generation via N-th Order Recursive Consistent Velocity Field Estimation", "abstract": "Recent advances in few-step generative models (typically $1$-$8$ steps), such as consistency models, have yielded impressive performance. However, their broader adoption is hindered by significant challenges, including substantial computational overhead, the reliance on complex multi-component loss functions, and intricate multi-stage training strategies that lack end-to-end simplicity. These limitations impede their scalability and stability, especially when applied to large-scale models.\n\nTo address these issues, we introduce **$N$-th order Recursive Consistent velocity field estimation for Generative Modeling (RCGM)**, a novel framework that unifies many existing approaches. Within this framework, we reveal that conventional one-step methods, such as consistency and MeanFlow models, are special cases of 1st-order RCGM. This insight enables a natural extension to higher-order scenarios ($N \\geq 2$), which exhibit markedly improved training stability and achieve state-of-the-art (SOTA) performance.\n\nFor instance, on ImageNet $256\\times256$, RCGM enables a $675\\text{M}$ parameter diffusion transformer to achieve a $1.48$ FID score in just $2$ sampling steps. Crucially, RCGM facilitates the stable full-parameter training of a large-scale ($3.6\\textrm{B}$) unified multi-modal model, attaining a $0.85$ GenEval score in $2$ steps. In contrast, conventional 1st-order approaches, such as consistency and MeanFlow models, typically suffer from training instability, model collapse, or memory constraints under comparable settings.\n\n*Code will be publicly available.*", "tldr": "", "keywords": ["Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15ab6b71b1d024e9934411c9d3377a01ee4edc77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes RCGM (Recursive Consistent velocity field estimation for Generative Modeling), a framework that extends consistency models to N-th order formulations. The authors claim their method unifies existing few-step generative models and achieves improved training stability and SOTA performance on ImageNet generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong empirical results: The method achieves competitive FID scores on ImageNet (1.48 FID at 256×256 with 2 steps, 1.79 FID at 512×512 with DC-AE).\n- Comprehensive experimental coverage: The paper includes experiments on class-conditional generation, text-to-image synthesis, and unified multimodal models."}, "weaknesses": {"value": "**Limited Technical Novelty**\n\nThe core contribution is an incremental extension of existing consistency models using standard numerical ODE techniques. The segmented integration in Equations (2)-(5) is a straightforward application of multi-step numerical integration (similar to predictor-corrector methods). The claim of \"unification\" is overstated—showing that N=0 recovers diffusion models and N=1 recovers consistency models is mathematical verification rather than novel insight.\n\n**Table 2 concerns:**\n\n- Compares methods with vastly different training budgets (RCGM: 424 epochs vs. IMM: 3840 epochs), making direct comparison problematic\n- Missing recent baselines: FLUX-Schnell, SD3-Turbo are mentioned in Table 3 but absent from Table 2\n- No ablation on the choice of autoencoder (SD-VAE vs. DC-AE vs. VA-VAE)\n\n**Missing ablations on:**\n\n- Time point sampling strategies for {t_i}\n- Sensitivity to N across different model scales and datasets\n\n**Weak Theoretical Contribution**\n\n- Appendix D provides standard numerical analysis (Taylor expansion error bounds)\n- Theorem 1 gives O(Δt²) truncation error + O((N+1)²ε²) approximation error—this is expected from numerical ODE theory\n- No theoretical explanation for why higher-order methods improve training stability (only convergence rate is analyzed)\n- The \"optimal N\" in Corollary 1 depends on unknown problem-dependent constants, providing limited practical guidance\n- Figure 3b shows N=2 still requires very large EMA (κ=0.999) for best performance, which contradicts claims of resolving the stability-performance trade-off\n- The stability comparison is confounded by EMA rate—N=1 with better hyperparameter tuning might achieve similar results\n\n\n## Minor Weaknesses\n\n - Code not available for review (promised \"upon acceptance\")\n- Many implementation details missing (hierarchical time sampling procedure, exact schedule)\n- Table 3 includes \"†Our evaluation\" for baselines—without code, these cannot be verified\n- No error bars or multiple runs reported for any experiments"}, "questions": {"value": "- Can you provide training curves, loss plots, or other evidence for the claimed \"model collapse\" of baseline methods in Table 4?\n- What is the actual wall-clock training time comparison between N=1 and N=2, accounting for the additional forward passes?\n- How does the method perform with error bars over multiple random seeds? Generative models are known to be sensitive to initialization.\n- Can you provide ablation studies showing N>1 helps across different model scales (not just 675M) and datasets (not just ImageNet)?\n- Why not compare against adversarial methods (DMD2, LADD) experimentally rather than only dismissing them in the related work?\n- In Equation (8), what is the theoretical justification for the asymmetric stop-gradient operation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HgBbrJygUt", "forum": "GnawtLKGkP", "replyto": "GnawtLKGkP", "signatures": ["ICLR.cc/2026/Conference/Submission25236/Reviewer_948b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25236/Reviewer_948b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761518393364, "cdate": 1761518393364, "tmdate": 1762943376175, "mdate": 1762943376175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel method for training one or few-step generative models by using recurrent consecutive flow maps as target. The argument is that increasing the number of these steps N, with respect to other few steps methods such as consistency models and MeanFlow where N=1, increases training stability. The method results in SOTA generative performance on common benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is conceptually simple, and is effective on the proposed benchmarks. It does not rely on JVP, which can simplify implementation and usage with different frameworks and architectures."}, "weaknesses": {"value": "While the results are encouraging, I think the method section of the paper is a bit rushed, which makes it hard to understand exactly the training procedure. There is no pseudocode or code, and I could not properly grasp how the training works (see questions). Furthermore, there are important details missing, such as batch size used and other hyperparameters. Some choices are not well motivated, such as how the authors arrived to Eq.9.\n\nnother thing that seems unusual is the 1-st order model not working well without EMA target. This is somewhat in disagreement with the literature. It would have been great to see the effect of EMA rate on the target network with other 1st order methods such as MeanFlow or Shortcut Models, to verify the claim that using EMA weights is helpful.\n\nOverall, the fact that using a higher order N improves performance seems counterintuitive, as I would expect more error from the target model to accumulate. While the great results obtained prove my intuition wrong, I would appreciate a clearer exposition of the why this is the case, as well as a way to verify what is actually done in practice (e.g. sharing the code)."}, "questions": {"value": "- 1) From my understanding, in the training loss there are mostly three terms: the prediction term for the displacement, which will receive gradients; the ground truth velocity, which is the montecarlo estimate (x1 - x0) commonly used in rectified flow; the N step predictions for the small consecutive displacements, which are also used as a target from t_1 to t_N. If that's the case, I wonder how one can ensure stability, as the smaller jumps and the flow matching style 0 order term receive less or no training.\n- 2) If the argument holds for N=2, then is it correct to expect the results to improve also with N=3? Have the authors experimented with this? It would be interesting to see an ablation for different N."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vo8XbLhWLB", "forum": "GnawtLKGkP", "replyto": "GnawtLKGkP", "signatures": ["ICLR.cc/2026/Conference/Submission25236/Reviewer_6g9s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25236/Reviewer_6g9s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533945922, "cdate": 1761533945922, "tmdate": 1762943375951, "mdate": 1762943375951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a unified framework for few-step generative modeling that generalizes existing methods like Consistency Models and MeanFlow to an N-th order recursive formulation. By leveraging higher-order trajectory information from the probability flow ODE, RCGM improves training stability, scalability, and sample quality without requiring jacobian-vector products or auxiliary networks. The method achieves strong results: FID of 1.48 on ImageNet 256×256 in two sampling steps, demonstrating effectiveness in few-step generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Clear motivation: few‐step generative models are valuable but face obstacles (instability, complexity). RCGM directly addresses this.\n\n2) Good theoretical framing: the segmentation of the PF-ODE trajectory, the definition of N^th order recursive velocity field estimator gives a compelling narrative and unifies prior methods.\n\n3) Reported FID of ~1.48 on ImageNet 256×256 with 2 steps using a 675 M parameter model is highly competitive. The ablation that shows N > 1 improves stability under large EMA decay rates is particularly valuable (shows the method has practical benefits). \n\n4) The fact that the method does not require Jacobian-vector products (JVP) is a good plus for memory efficiency."}, "weaknesses": {"value": "1) While the theoretical derivation is sound, the paper does not fully characterise the conditions/assumptions under which the higher‐order recursion yields gains (e.g., error bounds, effect of Δt, effect of segmentation size, how approximation error accumulates).\n\n2) The paper claims simplicity and memory‐efficiency, but limited quantitative reporting is provided on computational cost, memory usage, training time, inference time, etc., especially compared to baseline few‐step methods.\n\n3) The title says “Any-step Generation” but experiments for wide range of steps beyond 2 are less emphasized.\n\n4) The paper shows that for N > 4 the performance reversed (due to approximation error) which is good, but the discussion of why this happens (and what one should do) is limited. Also does the method’s advantage hold in ultra low‐step regimes (e.g., 1 step) or does it degrade rapidly?\n\n5) How sensitive is RCGM to the choice of the time segmentation sequence {t₀, t₁, …, tₙ₊₁}? For instance, how does performance vary if the intermediate times are uneven, or chosen adaptively rather than uniform?\n\n6) You note that for N > 4 the performance trend reverses due to “accumulation of approximation errors” (Sec 4.2). Could you provide more quantitative analysis of how approximation error behaves as N increases? Are there theoretical bounds or heuristics for choosing N?\n\n7) The theoretical formulation considers the PF‐ODE velocity field and segments the integral (Eq 2, (3) etc). Does the method assume that the underlying forward noising schedule and PF‐ODE satisfy certain smoothness or Lipschitz conditions? If so, it would be good to state them explicitly."}, "questions": {"value": "Please consider addressing the weaknesses noted above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uj9JH9Rn9j", "forum": "GnawtLKGkP", "replyto": "GnawtLKGkP", "signatures": ["ICLR.cc/2026/Conference/Submission25236/Reviewer_jXWE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25236/Reviewer_jXWE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983668729, "cdate": 1761983668729, "tmdate": 1762943375720, "mdate": 1762943375720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RCGM (N-th order Recursive Consistent velocity field estimation for Generative Modeling), a unified framework for any-step generation that generalizes diffusion/flow-matching (0-th order) and one-step consistency/MeanFlow models (1-st order) to higher orders (N≥2). The key idea is to derive a multi-step target for the PF-ODE velocity via segmented integration and use it to train a displacement predictor, with a practical loss that avoids JVP and scales to large models. Empirical results indicates that higher-order training improves stability and quality in the few-step regime, achieving strong ImageNet FID at 2 NFEs and competitive multimodal text-to-image results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The segmented-integration derivation for the recursive estimator is neat and connects cleanly to known objectives; the displacement parameterization under a linear transport path is well-motivated.\n- Systematic experiments demonstrate RCGM’s performance for different orders N, including an instability analysis across EMA decay factors and thorough comparisons against alternative methods.\n- The presentation is clear and easy to follow, and RCGM is introduced with well-motivated explanations and helpful illustrations."}, "weaknesses": {"value": "- It would be useful to include experiments on additional datasets beyond ImageNet.\n- Figure 3(c) only compares FID scores for different N under a fixed k. It would be interesting to see whether the current results hold for varying k.\n- It would be helpful to report and compare the computational cost of the method for different N."}, "questions": {"value": "- The experiments on RCGM rely on specific autoencoders (VA-VAE and DC-AE). Can the method be adapted to other, possibly weaker, encoders?\n- Corollary 1 shows that as N increases, the uniform approximation error decreases. Could the authors provide some intuition behind this result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aRWlHmFF9P", "forum": "GnawtLKGkP", "replyto": "GnawtLKGkP", "signatures": ["ICLR.cc/2026/Conference/Submission25236/Reviewer_91YB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25236/Reviewer_91YB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216522598, "cdate": 1762216522598, "tmdate": 1762943374916, "mdate": 1762943374916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}