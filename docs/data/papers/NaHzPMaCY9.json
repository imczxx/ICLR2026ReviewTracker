{"id": "NaHzPMaCY9", "number": 22703, "cdate": 1758334561708, "mdate": 1763686190919, "content": {"title": "Steering Autoregressive Music Generation with Recursive Feature Machines", "abstract": "Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model's internal gradients to produce interpretable \"concept directions\", or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within MusicGen's hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity.", "tldr": "We adapt Recursive Feature Machines to steer pre-trained music models in real time, enabling fine-grained, interpretable control of musical attributes without retraining.", "keywords": ["music generation", "probing", "interpretability", "music ai", "multimodal LLMs", "steering", "inference"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afb3418cb5287cc6bf42f346c46d8c707137b2b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MusicRFM, a framework for activation-level steering of pre-trained autoregressive music models. Building on RFMs, the authors identify interpretable “concept directions” in MusicGen’s hidden states that correlate with musical attributes such as notes, chords, or tempo. These directions are injected into the model’s residual stream during inference, allowing fine-grained control without retraining or step-wise optimization. Experiments on synthetic datasets (SYNTHEORY) and real-music benchmarks (MUSICBENCH) evaluate classification accuracy, FAD/MMD/CLAP metrics, and small-scale listening tests."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach enables interpretable, fine-grained control over musical attributes such as notes, chords, and tempo without retraining or per-step optimization. The introduction of layer-aware steering, time-varying schedules, and multi-directional control makes the framework flexible and musically relevant. Empirical results demonstrate effective controllability with minimal loss of fidelity, showing a clear advancement toward interpretable and lightweight control in music generation systems."}, "weaknesses": {"value": "The paper lacks essential experimental details, including the datasets, prompts, and number of samples used for evaluation. While the experiments appear to partially rely on the SynTheory dataset, this is never explicitly stated or described in detail, making it difficult to assess reproducibility. In addition, the absence of a direct baseline would be the critical issue of the paper. Most naively, authors can compare the proposed steering approach to a simple text-conditioning (e.g., simply adding “fast music” to the input prompt) limits the clarity of the paper’s contribution. Such a comparison is crucial to demonstrate the unique benefits of activation-space control over standard prompt-based methods.\n\n**missing references**\n\n- citation for metrics? (FD, MMD, CLAP) it’s not mentioned of which CLAP model was used for evaluation\n\n**minor**\n\n- Table 1: correct the caption (”We train using 7 We report…”) and place the proposed method at the most bottom row\n- reporting FD and MMD together is a bit redundant\n- Table 2 caption: “higher better for” → “higher is better for”\n- line 315: “were randomly chosen base model …” → “were randomly chosen from base model ...”\n- couldn’t find listening examples for time-based schedules from the demo page upon time of reviewing"}, "questions": {"value": "- Table 1: what is the implication of performance difference between proposed and RFM (last token)?\n- Table 2: mae for Tempos?\n- Table 2: Classification results were very high from Table 1. Doesn’t this mean it’s a bit reliable to trust the metric?\n    1. If the authors claim it’s not reliable for real music (according to Table 2 caption), then correlation between human evaluation should be included to observe how different it is and find the best way for evaluation (e.g., models specific for each downstream task)\n    2. If this Probe Acc is reliable, then the successful controllability is below chance level for all tasks except for Notes. Is this acceptable? Especially since there’re no other baselines to compare.\n- details of participants of listening test? and 3 questionnaire seems too small\n- what’s the benefit compared to diffusion-based controls\n- so what is the best $\\eta_0$? and any way to automate this for each steering direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pEbVe6qkmi", "forum": "NaHzPMaCY9", "replyto": "NaHzPMaCY9", "signatures": ["ICLR.cc/2026/Conference/Submission22703/Reviewer_6iX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22703/Reviewer_6iX2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827892456, "cdate": 1761827892456, "tmdate": 1762942348161, "mdate": 1762942348161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) to enable fine-grained control over frozen autoregressive music generation models like MUSICGEN. By training lightweight RFM probes on the SYNTHEORY dataset, the authors extract interpretable \"concept directions\" corresponding to musical attributes (e.g., notes, chords, tempos). These directions are injected into the model's activations during inference to steer generation without retraining or per-step optimization. Key innovations include layer-based pruning (top-K and exponential weighting), time-varying schedules (e.g., linear fades, sinusoidal modulation), and multi-direction steering for simultaneous control of multiple attributes. Experiments demonstrate improved control accuracy (e.g., note classification from 0.23 to 0.82) with minimal impact on text prompt fidelity (CLAP score within ~0.02 of baseline), supported by quantitative metrics (FD, MMD, CLAP) and a small listening test."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a solid paper that explores the application of Recursive Feature Machines (RFMs) to music generation, specifically for steering codec-based autoregressive models like MusicGen. \n\nThe work addresses an important challenge in controllable music generation by enabling fine-grained, interpretable control over musical attributes such as notes, chords, intervals, scales, progressions, tempos, and time signatures, without requiring model retraining or heavy optimization. \n\nThe use of the SYNTHEORY dataset for probe training is well-motivated, as it provides clean, music-theoretic labels. \n\nThe proposed extensions—layer pruning strategies (top-K and exponential weighting), dynamic time schedules, and multi-direction steering—are novel and practical, allowing for more robust and flexible control. \n\nExperiments are comprehensive, including classification results showing RFMs outperforming baselines (e.g., average score 0.942 vs. 0.929 for SYNTHEORY FFNs), quantitative metrics on steering trade-offs (FD, MMD, CLAP, probe accuracy), and a listening test demonstrating perceptual improvements. \n\nOverall, the paper makes a meaningful contribution to activation-level steering in generative audio models, with potential for broader applicability."}, "weaknesses": {"value": "First, the related work section underestimates prior methods by claiming they require \"intense finetuning runs,\" when many actually use parameter-efficient fine-tuning (PEFT) techniques, which are computationally lighter than full finetuning (though still more than RFMs). This portrayal lacks objectivity and overlooks the fact that some methods already achieve control over notes and chords without architectural gaps. \n\nAdditionally, relevant papers are missing, such as Zhu et al. (2025) on efficient fine-grained guidance for diffusion-based symbolic music generation via inference-time control, and Zhang et al. (2024) on zero-shot text-to-music editing with diffusion models, which also focuses on inference-time interventions. Second, although the writing is clear overall, the methods section can be dense and challenging to follow, particularly the details on RFM adaptations, layer pruning, and schedule formulations; more explanatory text or examples would help. \n\nThird, objective experiments show that increasing steering strength degrades FD and MMD scores, which seem to imply a degradation in musicality as well—objective metrics like FD and MMD are designed to capture distributional shifts that often correlate with perceptual quality, suggesting that stronger steering may introduce artifacts or incoherence that harm the overall musical experience. This is my biggest concern, as it raises questions about the method's ability to maintain high-fidelity generations under aggressive control, yet the paper lacks subjective evaluations of fidelity and musicality beyond the small listening test—relying on proxies like CLAP may not fully capture these aspects, potentially limiting the method's practical value. \n\nFinally, the listening test protocol uses only 12 participants, which is insufficient for robust conclusions, and omits details on participant selection criteria, demographics, or distribution.\n\n[1] Zhu, T., Liu, H., Wang, Z., Jiang, Z., & Zheng, Z. Efficient Fine-Grained Guidance for Diffusion Model Based Symbolic Music Generation. In Forty-second International Conference on Machine Learning.\n\n[2] Zhang, Y., Ikemiya, Y., Xia, G., Murata, N., Martínez-Ramírez, M. A., Liao, W. H., ... & Dixon, S. (2024). Musicmagus: Zero-shot text-to-music editing via diffusion models. arXiv preprint arXiv:2402.06178."}, "questions": {"value": "1. Could the authors address the underestimation of related work? For instance, how does MusicRFM compare directly to PEFT-based methods in terms of computational cost and control granularity? Also, why were papers like Zhu et al. (2025) and Zhang et al. (2024) not discussed, given their focus on inference-time control in music generation?\n\n2. To improve readability, could the authors suggest additions to the methods section, such as pseudocode for the steering injection process or a simple worked example of a time schedule (e.g., linear rise) applied to a generation?\n\n3. Regarding the trade-off between control strength and metrics like FD/MMD, do the authors have plans for larger-scale subjective evaluations of audio fidelity and musicality? How might this affect the method's usability in real-world applications, and are there mitigation strategies?\n\n4. For the listening test, could the authors provide more details on the protocol? Specifically, what were the participant demographics, expertise levels (e.g., musicians vs. general listeners), and how were samples randomized?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "This paper comes with a subjective experiments, usually needs an ethics approval."}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9pFDy44H7N", "forum": "NaHzPMaCY9", "replyto": "NaHzPMaCY9", "signatures": ["ICLR.cc/2026/Conference/Submission22703/Reviewer_h1Zj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22703/Reviewer_h1Zj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971983990, "cdate": 1761971983990, "tmdate": 1762942346863, "mdate": 1762942346863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a training-free method to steer MusicGen with RFMs. The controllable features include tempo, chords, notes, time signatures etc."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The idea of fine tuning-free steering of multiple music concepts in a universal way is intriguing, and the time-variant control is a useful direction. \n\n2. In the demo page it seems that some controls (e.g., augmented chords; interval 6) are effective. But it is strange why the probing results are low (see questions 2)."}, "weaknesses": {"value": "1. The model focuses on global controls that are described by labels. This is not a novel task and can be done by many fine-tuning methods, using either text-based or other controls. The methodology itself is not novel either.\n\n2. The performance of the model seems to be low. Previous works seem to provide better controllability and musicality as in Wu et al (2024) & Lin et al (2023). There is no comparative experiments against previous fine-tuning methods either.\n\n3. Sec 5.1: For some tasks like notes, chord etc., a pretrained model (e.g., audio chord estimator) provides a better evaluation metrics compared to subjective and objective (by reusing the prober) test.\n\n4. Line 243: incomplete sentence."}, "questions": {"value": "1. Line 257: I cannot quite understand the tempo issue. what happened to the tempo category and what specific methods did you used for tempo?\n\n2. I do not quite understand the correspondence between table 2 and the demo page. From the demo page it seems that the controllability is relatively good but the audio quality/musicality is harmed. However, table 2 shows that the quality is relatively ok but the controllability is low. Why?\n\n3. For time varying controls (changing $\\phi(t)$), do you have any demos that can produce i.e., tempo changes or time signature changes within a song? As far as I know these are very difficult for current controllable generation models. Currently, I see no time varying control in the demo page."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n7zxB1vW3J", "forum": "NaHzPMaCY9", "replyto": "NaHzPMaCY9", "signatures": ["ICLR.cc/2026/Conference/Submission22703/Reviewer_UgTQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22703/Reviewer_UgTQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004563895, "cdate": 1762004563895, "tmdate": 1762942346397, "mdate": 1762942346397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Rebuttal Response"}, "comment": {"value": "We thank the reviewers for their insightful comments, and are glad to see that the majority recommend acceptance, highlighting the breadth of our experimental setup (reviewers h1Zj, 6iX2), lightweightness of approach (UgTQ, h1Zj, 6iX2), and overall novelty (h1Zj). Below, we highlight our response to shared useful comments and concerns brought up by the reviewers, specifically the 2 main additions of prompt-conditioning baseline experiments and external evaluations that are not dependent on multiclass RFM-probes.\n\n## Prompt-Based Baseline\nFirst, we add a prompt-only conditioning baseline to directly address concerns about the absence of a simple text-conditioning comparison. For each concept, we append explicit control hints (e.g., “Note: C#”, “Slow tempo”) and evaluate MusicGen without any RFM steering. Across all settings, we find that prompt-only control is consistently outperformed by RFM-based steering. Additionally, we conduct experiments on combining prompt conditioning and RFM steering. We see that for most categories, this experimental setup yields the best accuracy results, and most noticeably for notes, where probe accuracy reaches 95%. By providing these new experimental results, we can concretely show that activation-space steering enables forms of fine-grained control that prompting cannot achieve.\n\n## External Control Accuracy Metrics\nSecond, we introduce external evaluators that operate directly on the waveform, addressing reviewer concerns regarding reliance on probe-based evaluation. We now report: (i) note accuracy via chromagram energy, (ii) chord accuracy using Essentia’s chord estimator, and (iii) tempo control via onset-based event rates. Overall, we find that RFMs perform much better than simple prompt conditioning, and that combining the two methods yields better results as well. We believe that the correlation between probe accuracy and our external evaluators is solid evidence that RFM steering indeed works well and thus can be extrapolated to the other untested categories if such evaluation methods existed.\n\nFinally, we have clarified all experimental details (dataset usage, prompt sources, sample counts, and evaluation procedures) and corrected the typos within the paper to ensure full reproducibility, addressing the other concerns raised by the reviewers. We have also updated the website with new audio samples (time-controlled, multi-direction) and controls on a variety of prompts."}}, "id": "Rxtdlcgn61", "forum": "NaHzPMaCY9", "replyto": "NaHzPMaCY9", "signatures": ["ICLR.cc/2026/Conference/Submission22703/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22703/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission22703/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763687918615, "cdate": 1763687918615, "tmdate": 1763687918615, "mdate": 1763687918615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}