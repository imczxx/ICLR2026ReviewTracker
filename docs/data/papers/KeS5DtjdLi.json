{"id": "KeS5DtjdLi", "number": 10583, "cdate": 1758176597184, "mdate": 1763499025792, "content": {"title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "abstract": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents--\\textit{Coordinator, Explainer, Coder, Evaluator, and Debugger}-- enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and evaluator–debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.", "tldr": "We expand the explainability of RL agents by augmenting existing XRL techniques and bridging them with natural language queries.", "keywords": ["Explainable Reinforcement Learning", "Large Language Models", "Counterfactual Explanations", "Natural Language Explanations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f41f893df8f1c8e3ba25faa34e05ca41d3ce273c.pdf", "supplementary_material": "/attachment/7576518691cf6b471c2ae434239eebde7f53613f.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce TalkToAgent, a multi-agent LLM system capable of producing different explanations for a reinforcement learning agent’s behavior. This system includes agents for coding (the “coder”, “debugger”, and “evaluator”), choosing an explanation modality (the “coordinator”), and writing an explanation back to the user (the “explainer”). By using pre-built explanation tools in conjunction with on-demand coding for counterfactual policies, the multi-agent system can take in a user question about a policy and return an appropriate explanation. \n\nThe agents are built on (and tested with) GPT 4.1, and the RL agent is learned via soft-actor critic in a quadruple-tank system environment. In this environment, the agent has access to 2 continuous actions, and state features for: 4 tanks (1D continuous values), error levels between the lower tanks, and set points. In total, the RL evaluation environment is quite simplistic, although the function that must be learned is tricky.\n\nTo evaluate the TalkToAgent system, the authors propose to measure the system on:\n1. How effectively can it classify user queries into the right tool call?\n2. How reliably can it generate counterfactual policies that answer the user’s query?\n3. Can the explainer properly generate an explanation that summarizes the explainable tool’s output?\nThe authors find that the system can classify queries appropriately, that it can generate code effectively (thanks to the multiple agents, particularly the debugger agent), and that it generates plausible explanations that reflect the input query."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Strengths:\n+ The TalkToAgent architecture is clearly explained and the individual agents are well-motivated. In particular, the coding agents are ablated effectively and shown to perform well.\n+ The approach combines several different approaches to explainability, overcoming a shortcoming in prior works that focused on a single modality of explanation. By using TalkToAgent, users gain access to multiple types of explanations and can therefore better understand the RL policy."}, "weaknesses": {"value": "Weaknesses:\n- A key weakness of this work is that there is no evaluation of how faithful or effective the explanations actually are. There is no user study, there are no human evaluators, and we do not know if the provided explanations are actually (1) faithful to the underlying policy, (2) intelligible by the user, or (3) general across more complex domains, state/action spaces, or policies.\n- The evaluation queries are generated by GPT 4.1 and then they are evaluated by GPT 4.1. It is unsurprising that GPT 4.1 does well on this task, as it is classifying its own generated data, so queries should be exactly what that model expects (depending on how the query-generator was prompted, and this information is missing in the paper). However, this is not a particularly strong experiment, and the generated queries should ideally come from (1) human users or (2) an LLM from a different family.\n- As the authors rightly point out, some of the explanations require a simulator to rollout a counterfactual policy. Could we not simply rollout the current policy, if we had access to a simulator? This would be quite a compelling explanation. While the authors do raise this point themselves, it is a weakness in the approach that limits generality.\n- Counterfactual explanations play a key role in the paper, yet several key related works are missing ([1, 2])\n- The particular choice of explanations is not well motivated, and feels somewhat arbitrary. Why not consider decision trees [3, 4] or case based reasoning [5]\nOverall, while I like the general idea and direction of the paper, the evaluation is too weak to be accepted in its current state. The paper must at least include a human evaluation of the explanations, or a generalization experiment to new domains. Without either of these two experiments, the work is too preliminary and is not ready for publication at a top-tier venue.\n\n[1] Verma, Sahil, et al. \"Counterfactual explanations and algorithmic recourses for machine learning: A review.\" ACM Computing Surveys 56.12 (2024): 1-42.\n\n[2] Karimi, Amir-Hossein, et al. \"A survey of algorithmic recourse: contrastive explanations and consequential recommendations.\" ACM Computing Surveys 55.5 (2022): 1-29.\n\n[3] Wu, Mike, et al. \"Optimizing for interpretability in deep neural networks with tree regularization.\" Journal of Artificial Intelligence Research 72 (2021): 1-37.\n\n[4] Silva, Andrew, et al. \"Optimization methods for interpretable differentiable decision trees applied to reinforcement learning.\" International conference on artificial intelligence and statistics. PMLR, 2020.\n\n[5] Caruana, Rich, et al. \"Case-based explanation of non-case-based learning methods.\" Proceedings of the AMIA Symposium. 1999."}, "questions": {"value": "* How were the user queries in the evaluation generated? Was the model prompted to produce questions focusing on feature importance, or long term planning, or a specific action, or was it simply told to generate 90 queries about an RL policy?\n* How effectively would TalkToAgent generalize to new domains? Would the coder be able to handle unstructured data, such as images? Or is it constrained to fairly linear systems with known parameters?\n* How is the appropriateness of the user-query/evaluator matchup scored? It seems like the authors looked at a few examples and decided that RQ3 was satisfied, but I do not see any mention of an experiment or analysis on the data here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fAVDz2ibDM", "forum": "KeS5DtjdLi", "replyto": "KeS5DtjdLi", "signatures": ["ICLR.cc/2026/Conference/Submission10583/Reviewer_SPFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10583/Reviewer_SPFN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761076288669, "cdate": 1761076288669, "tmdate": 1762921851762, "mdate": 1762921851762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "O7R7uFW4lB", "forum": "KeS5DtjdLi", "replyto": "KeS5DtjdLi", "signatures": ["ICLR.cc/2026/Conference/Submission10583/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10583/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763499025079, "cdate": 1763499025079, "tmdate": 1763499025079, "mdate": 1763499025079, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposes an intriguing way to employ LLMs to explain deep reinforcement learning (DRL) agents. Specifically, the authors trained chatGPT4 to play 5 different roles, which means 5 different agents are derived from chatGPT4. The 5 agents would 1) parse user queries, 2) select an optimal tool from 5 XAI tools, 3) create python scripts that can generate new reward functions and decompose existing Q functions and 5) generate final explanation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "‘TalktoAgent’ was tested against a control problem, in which inputs to two pumps are optimized to maintain the proper water levels in 4 tanks. The authors used three research questions (RQs 1-3), and their empirical evaluations suggest that TalkToAgent can choose a good XAI algorithm (RQ1)."}, "weaknesses": {"value": "Evaluations of RQ2 and RQ3 seem very limited to me. \n\nIn line 414, the manuscript stated “we evaluated the fidelity of contrastive trajectory generations.”, but I could not find the corresponding results (especially for CE-A and CE-B). \n\nIn section 4.5, the authors summarize the explanation obtained for FI, EO and CE queries. But the authors do not provide how accurate they are. \n\nTalktoAgent is tested with a single control task, which raises the question if it can be generalized to other tasks, and it was not compared with any other baseline algorithms."}, "questions": {"value": "1. In line 415, the authors state that “the intended trajectory is directly generated by the predefined functions\". Where did the authors define the predefined functions? Does it refer to Q-function?\n\n2. What criteria are used to determine “successful” creation of proper codes from the code generator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NkVSd2iGqj", "forum": "KeS5DtjdLi", "replyto": "KeS5DtjdLi", "signatures": ["ICLR.cc/2026/Conference/Submission10583/Reviewer_2Qvs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10583/Reviewer_2Qvs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713582185, "cdate": 1761713582185, "tmdate": 1762921851452, "mdate": 1762921851452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multi-agent LLM framework designed to provide interactive, natural language explanations for deep RL agents. The system uses five collaborating LLM agents to map user queries into XRL techniques, automatically invoke XRL tools and generate textual and visual multimodal explanations. Evaluation is performed on a quadruple-tank process control environment, demonstrating accuracy in mapping queries to the right XRL method"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and well-motivated framework combining natural-language understanding, code synthesis, and explanation verification.\n- Promotes reproducibility via detailed prompts and modular architecture.\n- Advances accessibility of RL interpretability\n- Uses and adapts existing tools to XRL and goes beyond prior work by introducing active code generation and policy simulation\n- High accuracy in the examined domain\n- Writing is clear and well-structured (but I recommend introducing the MARL formulation earlier)"}, "weaknesses": {"value": "- While the title indicates a 'human-centric' framework, there is no user study to measure interpretability gains.\n- Methodologically incremental in reusing existing XRL techniques and coupling it with LLMs without significant algorithmic innovation.\n- Evaluation is done on a single domain, which is 'narrow'; real-world generalization is not demonstrated.\n- Limited quantitative evaluation of the quality or faithfulness of generated explanations, only classification accuracy and qualitative examples are reported, without specifying how the labels were generated. \n- Reward decomposition and qualitative trajectory explanations rely on manually designed heuristics; a robustness analysis is missing."}, "questions": {"value": "- \"We compared task classification accuracy of four different\" - how was accuracy measured? How were the ground truth labels generated? \n- How generalizable is TalkToAgent beyond the quadruple-tank process control? \n- Can the framework handle temporal queries ?\n- How are ambiguous or multi-intent queries (mixing “why” and “what-if”) resolved by the coordinator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rbDUjTGmQm", "forum": "KeS5DtjdLi", "replyto": "KeS5DtjdLi", "signatures": ["ICLR.cc/2026/Conference/Submission10583/Reviewer_p4ig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10583/Reviewer_p4ig"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824306508, "cdate": 1761824306508, "tmdate": 1762921850000, "mdate": 1762921850000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework to generate different types of explanations for a reinforcement learning agent. TalkToAgent utilizes an LLM, to interpret user-intent and route to the correct explanation mechanism. They provide three types of explanations; Feature Importance, Expected Outcome, and Contrastive Explanations. The contrastive explanations are further sub-divided into three types; (1) action, (2) behavior, and (3) policy explanations. The TalkToAgent mechanism begins by interpreting user-intent through the coordinator module to trigger the appropriate type of explanation. Next, an optional coder module is triggered to generate and execute code to procude an explanation (if necessary). Finally, the explainer module parses the generated explanation and converts them into intelligible natural language statements, as well as any figures generated by the explanation. The authors test their approach on a quadruple tank RL environment, and showcase the utility of the various modules used in the approach and provide a qualitative analysis of the explanations generated."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I appreciated the efforts of this paper to produce intention aligned explanations as explainable-AI is not a “one-size-fits-all” solution.\n- This paper explores a wide-breadth of explanations, particularly under the contrastive explanation category. Particularly, the inclusion of behavior- and policy-based contrasts was key as they are often under-explored in work on contrastive explanations.\n- This paper effectively reviews and defines the categories of explainable reinforcement learning in prior work."}, "weaknesses": {"value": "- The technical approach in this paper seems to have limited novelty.\n    - This paper seems to put together a set of components from prior work, either in explainable reinforcement learning, or LLM-research.\n        - Language models as reward engineers - [1,2]\n        - Debugging using code feedback - [3,4]\n        - Dynamic routing to response strategies - [5]\n- A system comprised of existing approaches can still comprise a novel contribution to the research community, however, this paper is also lacking in sufficient experimentation to highlight the utility of their system.\n    - Section 4.1 focused on the task-classification accuracy across base-models utilized in their system. The authors show that GPT-4.1 performs the best compared to it’s weaker predecessors or variants. It is unclear to me what this tells me about the utility of TalkToAgent.\n    - Similarly, Section 4.2 focuses on evaluating the utility of the debugger components by ablating the debugger. They find that removing the debugger provides a distinct drop in performance. However, this has already been shown in prior work [4]. Similar to the previous section, while this is useful to know, it does not provide readers with an understanding of the holistic performance of TalkToAgent.\n    - The qualitative analysis is surface-level. The authors provide some observations about the explanations generated by their system, however, these are anecdotal. If the authors want to include a qualitative analysis, I would recommend conducting a thematic analysis, or an analogous approach, to more methodologically analyze the explanations.\n    - I understand that there isn’t an easy metric for correctness of this system, as there is not a 1-to-1 ground truth for a perfect explanation for each case. However, I believe since the goal of TalkToAgent was to interpret human-intent, the authors should have conducted a human-evaluation which sought to understand whether their explanations are satisfactory to actual users.\n\n[1] - Ma, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A. Eureka: Human-level reward design via coding large language models. ArXiv preprint, abs/2310.12931, 2023. URL https://arxiv.org/abs/2310.12931.\n[2] - Chen, Letian, Nina Moorman, and Matthew Gombolay. \"ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics.\" arXiv preprint arXiv:2411.18825 (2024).\n[3] - Alrashedy, Kamel, et al. \"Generating cad code with vision-language models for 3d designs.\" *arXiv preprint arXiv:2410.05340* (2024).\n[4] - Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. 2023b.\n[5] - Pan, Zhihong, et al. \"Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection.\" arXiv preprint arXiv:2505.19435 (2025)."}, "questions": {"value": "Listed in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2MZURustZg", "forum": "KeS5DtjdLi", "replyto": "KeS5DtjdLi", "signatures": ["ICLR.cc/2026/Conference/Submission10583/Reviewer_tAdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10583/Reviewer_tAdp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975244284, "cdate": 1761975244284, "tmdate": 1762921849464, "mdate": 1762921849464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}