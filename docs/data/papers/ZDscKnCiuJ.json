{"id": "ZDscKnCiuJ", "number": 4599, "cdate": 1757722874568, "mdate": 1759898024026, "content": {"title": "Reward Model Boosting for RLHF", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a powerful technique for aligning large language models (LLMs) with human preference. However, it often suffers from the reward hacking issue, where policy optimization improves the proxy reward model while actually degrading performance with respect to the true human preference, due to the imperfection of the proxy. To address this, we propose Reward Model Boosting (RMB), a novel approach that enhances the robustness and reliability of the reward signal for RLHF. RMB first trains a set of reward models with a diverse-promoting regularizer. This encourages each model to learn complementary aspects of the reward landscape. Then, RMB learns a lightweight aggregator in the principle of boosting to aggregate the outputs of the diverse reward models into a more accurate and robust reward signal. Our extensive experiments demonstrate that RMB significantly improves reward accuracy on both in-distribution and out-of-distribution datasets, substantially mitigating the reward hacking issue and ultimately improving RLHF performance.", "tldr": "We propose Reward Model Boosting (RMB), which uses advanced boosting to construct a reliable reward signal for RLHF, effectively mitigating reward hacking and significantly improving policy learning.", "keywords": ["Reinforcement Learning", "Machine Learning", "RLHF", "Reward Overoptimization", "Reward Hacking"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/195053a66655ff747113fb7481141eb7dced3767.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present Reward Model Boosting, a novel two-stage approach to improve the robustness of reward models in RLHF and mitigate the reward hacking problem. The method first trains a set of reward models with an HSIC-based diversity-promoting regularizer to encourage learning complementary reward landscape. Subsequently, it learns a gradient-boosted decision tree aggregator to combine the outputs of these diverse RMs into a more accurate and robust final reward signal. The experiments show that RMB outperforms existing methods on both in-distribution and out-of-distribution datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper aims to address reward hacking, a highly relevant problem in RLHF. The motivation is strong and valuable to the community.\n\n2. The experiments are well-designed to specifically validate the mitigation of reward hacking. The results show that RMB maintains consistent improvement, even under noisy conditions.\n\n3. The proposed method consistently outperforms a set of baselines across both in-distribution and out-of-distribution datasets."}, "weaknesses": {"value": "1. It seems that the proposed method introduces substantial training costs from three aspects:\n- Training multiple reward models, each potentially as large as the actor model; \n- Computing pairwise HSIC values during training\n- An additional boosting stage post-training. \n2. The core components—ensembling and boosting-based aggregation—are well-established techniques. The paper’s novelty primarily stems from combining existing methods in an engineering-oriented way, rather than introducing a fundamentally new approach, which makes the overall contribution incremental.\n3. Lack of sensitivity analysis with respect to hyperparameters or model structure."}, "questions": {"value": "1. The RMB approach significantly increases training costs. Under a fixed computational budget, how does RMB compare to training a single, larger reward model? Does RMB's advantage hold in a budget-constrained scenario?\n\n2. The current experiments are conducted with a 2B-parameter model. Given that the training cost grows super-linearly with model size, how well does the method scale to larger backbones (e.g., 7B or 13B)?\n\n3. While the introduction of HSIC regularization is interesting, could a large value of $\\lambda_{HSIC}$ lead to \"over-diversification\" and thus reduce the accuracy of individual models? It would be helpful to include a sensitivity analysis with respect to $\\lambda$.\nIn addition, is there any specific reasons for the current design of aggregator?\n\n4. The paper reports that performance peaks when the number of reward models is around 8–10. Is there a principled or data-driven method for determining the optimal ensemble size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n08mPhvuTO", "forum": "ZDscKnCiuJ", "replyto": "ZDscKnCiuJ", "signatures": ["ICLR.cc/2026/Conference/Submission4599/Reviewer_urA4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4599/Reviewer_urA4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470770763, "cdate": 1761470770763, "tmdate": 1762917463464, "mdate": 1762917463464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reward Model Boosting (RMB), a novel approach to improve the robustness and reliability of reward models used in Reinforcement Learning from Human Feedback (RLHF). The authors identify that traditional reward models are prone to reward hacking—where optimizing the proxy reward degrades alignment with true human preference—due to imperfect supervision. RMB mitigates this by (1) training multiple diverse reward models using a Hilbert–Schmidt Independence Criterion (HSIC)-based regularizer (§2.3) to reduce correlation among models, and (2) aggregating them with a boosted decision tree trained in the spirit of XGBoost (§2.2). Extensive experiments across four datasets (Unified-Feedback, HHH-Alignment, MT-Bench, and RewardBench) demonstrate that RMB significantly improves both in-distribution and out-of-distribution (OOD) reward accuracy (§3.2), reduces reward hacking under Best-of-N sampling and PPO training (§3.3), and shows robustness against label noise and text perturbations (§3.5). The results suggest that RMB yields a more trustworthy reward signal for downstream RLHF optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The idea of applying boosting principles to ensemble reward models in RLHF is original and well-motivated. While prior work has explored ensembles for robustness (e.g., Coste et al., 2024), this paper’s use of a gradient boosting aggregator to iteratively minimize residual reward errors (Eqn. (3)) and the addition of HSIC-based diversity regularization (Eqn. (8)) represent a novel synthesis of ideas. The focus on margin diversity rather than raw score diversity (§2.3) is also conceptually sound and innovative.\n\nQuality: The empirical evaluation is sound. Table 1 shows that RMB (N = 3–10) consistently outperforms GRM, Label Smooth, Margin Loss, and Avg Ensemble baselines in preference prediction accuracy. Table 2 and Figure 1–2 further validate robustness in both synthetic and RLHF settings. The ablation in Table 3 and correlation heatmaps in Figure 3 provide convincing evidence that HSIC-induced diversity is key to boosting performance.\n\nClarity: The paper is clearly structured. Mathematical formulations are precise—especially the detailed boosting objective (Eqns. (3)–(5)) and HSIC margin definition (Eqn. (7)). Figures and tables are well-integrated and support the claims effectively.\n\nSignificance: The contributions are significant for the RLHF community. The approach is general and could be integrated into diverse RLHF pipelines. Moreover, the demonstrated OOD generalization and robustness to label noise suggest practical benefits for real-world LLM alignment."}, "weaknesses": {"value": "Dependence on tree-based aggregation: The reliance on decision-tree aggregators (XGBoost) may limit scalability or differentiability for large-scale LLM training. A brief comparison to neural aggregators or soft ensemble weighting would be beneficial.\n\nDiversity regularization tuning: The choice of the HSIC coefficient λ_HSIC is mentioned but not systematically analyzed. Since Table 3 shows performance sensitivity to diversity, an ablation on λ_HSIC or kernel choice would make the results more complete.\n\nLimited exploration of scalability and efficiency: The paper does not discuss training cost or inference latency when increasing N from 3 to 10 reward models (Table 1). Given the growing size of LLM-based reward models, it would be valuable to evaluate computational overhead and potential efficiency trade-offs.\n\nEvaluation scope: The experiments rely on relatively small models (Gemma-2B-IT, Mistral-7B). While this is acceptable for a proof of concept, the practical significance for frontier-scale RLHF systems (tens to hundreds of billions of parameters) remains uncertain."}, "questions": {"value": "Hyperparameter sensitivity: How sensitive is the performance to λ_HSIC and the number of boosting trees K?\n\nScalability: Could the boosting aggregator be replaced with a neural network trained end-to-end for differentiability within the RLHF loop?\n\nComputational cost: What is the runtime overhead of training N = 10 RMs plus the aggregator compared to a single RM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I6XVeD2OFx", "forum": "ZDscKnCiuJ", "replyto": "ZDscKnCiuJ", "signatures": ["ICLR.cc/2026/Conference/Submission4599/Reviewer_DuEg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4599/Reviewer_DuEg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734979920, "cdate": 1761734979920, "tmdate": 1762917463250, "mdate": 1762917463250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method called Reward Model Boosting (RMB) to improve the reliability of reward signals in RLHF, in an attempt to address the reward hacking issue. The model itself is a collection of several traditional language reward models, optimized so that their pairwise distance defined from Hilbert–Schmidt Independence Criterion (HSIC) is large. Experiments show the effectiveness of both: using HSIC based distance in training diverse reward models; and using decision tree boosting to improve the final prediction performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. It's interesting to incorporate boosting methods in RLHF. Using ensembles has been widely explored before, but boosting looks like a technique that hasn't been used so far.\n\n2. Using HSIC as diversity regularization has strong motivation and clear explanation.\n\n3. Experiments indicate the RMB method is capable of more robust reward modeling than single models / average ensembles."}, "weaknesses": {"value": "1. The paper lacks explanation on how HSIC is superior to other diversity regularization methods. I think there should be ablation on simpler diversity regularizations and HSIC.\n\n2. Some experimental results are not clearly explained. In Table 3, both individual RM and average ensemble benefits from using HSIC than random, which may contain other more deeper reasons why the regularization works better instead of just introducing diversity.\n\n3. This seems an interesting topic overall of applying higher level learning tricks to reward modeling, but the novelty is somewhat compromised by DPO partially replacing RLHF in practice."}, "questions": {"value": "1. Why did you choose HSIC as the diversity regularization? Did you try other methods?\n\n2. Can you provide more explanation on Weakness 2?\n\n3. On Table 1, what is special about reasoning that makes RMB perform worse than marginal loss?\n\n4. With later DPO methods instead of RLHF, improving on general reward modeling / solving reward hacking seems a little marginal overall. This method seems more suitable to be used in digging more specific domains, e.g. security, etc. Do you have any idea how your method can tackle these domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2tk67YWbh1", "forum": "ZDscKnCiuJ", "replyto": "ZDscKnCiuJ", "signatures": ["ICLR.cc/2026/Conference/Submission4599/Reviewer_mFLE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4599/Reviewer_mFLE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968763626, "cdate": 1761968763626, "tmdate": 1762917462974, "mdate": 1762917462974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reward Model Boosting (RMB), a novel approach designed to mitigate reward hacking in RLHF. The core components of RMB include a lightweight aggregator based on XGBoost and a diversity-promoting regularizer using the Hilbert-Schmidt Independence Criterion (HSIC). The aggregator operates on the principle of boosting, iteratively learning to combine the outputs of multiple reward models to produce a more accurate and robust final reward signal. Concurrently, RMB trains an ensemble of reward models, encouraging them to evaluate rewards from diverse perspectives. Through extensive experiments, the paper demonstrates that RMB significantly outperforms existing methods in reward accuracy on both in-distribution and out-of-distribution datasets. It also effectively alleviates reward hacking, ultimately enhancing the overall performance of RLHF."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem addressed in this paper—reward hacking—is a well-recognized and critical challenge in the RLHF domain. Any research that can effectively mitigate this issue is of significant value. The paper clearly identifies the root cause of the problem (the imperfection of proxy models) and precisely situates its research direction.\n\n2. The paper's evaluation is comprehensive, conducted across multiple standard datasets covering both in-distribution and OOD scenarios. By comparing against several strong baselines, the authors not only validate the accuracy of their proposed method but also provide robust evidence of its effectiveness in mitigating reward hacking and its strong robustness through a variety of experiments."}, "weaknesses": {"value": "1. Besides HSIC, there are other methods in the literature for promoting model diversity. The paper lacks a comparison with some of these common diversity-promoting techniques. The work would be significantly strengthened by including a discussion on the theoretical advantages of choosing HSIC, supplemented by experimental comparisons with alternative approaches.\n\n2. The RMB method requires the simultaneous training of N reward models, along with an additional HSIC regularization term, which undoubtedly increases training complexity and computational overhead. Although the appendix mentions the required computational resources, the total cost is multiplied compared to training a single RM. It is recommended that the authors provide a more explicit discussion of this overhead in the main text and analyze the trade-off between performance gains and computational costs.\n\n3. As shown in Table 1, the performance of RMB improves as N increases from 3 to 8, but it appears to saturate or slightly decline for some metrics when N=10. This suggests that the choice of N is a critical factor. I would suggest that the authors conduct a more in-depth analysis of the combined impact of N on performance, diversity, and computational cost, and provide practical guidance for selecting an optimal N."}, "questions": {"value": "1. In the experiments, noise is simulated by randomly flipping 25% of the labels, which validates the model's robustness to random errors. However, real-world human feedback often contains systematic biases rather than purely random noise (e.g., a preference for longer, more verbose responses, regardless of information density). How does RMB perform in the presence of such systematic biases? If the training data contains a \"length bias\" or a \"sycophancy bias,\" can RMB's diversity mechanism identify and mitigate these undesirable tendencies, or would it be misled in the same way as a baseline model?\n\n\n2. As shown in Table 2, the paper uses DeepSeek-R1 as the judge to evaluate the quality of policies trained with different reward models. While this is a common evaluation paradigm, the judge itself is an LLM with its own inherent biases. Is it possible that the preferences of DeepSeek-R1 happen to align more closely with the style of the model trained via RMB, thus leading to its higher win rate? Have the authors considered validating these results with other judges or a small-scale human evaluation to ensure the conclusion is robust?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TfklVDNhIE", "forum": "ZDscKnCiuJ", "replyto": "ZDscKnCiuJ", "signatures": ["ICLR.cc/2026/Conference/Submission4599/Reviewer_c3kk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4599/Reviewer_c3kk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188753398, "cdate": 1762188753398, "tmdate": 1762917462778, "mdate": 1762917462778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}