{"id": "kuaZXtReJ0", "number": 25477, "cdate": 1758368448492, "mdate": 1759896719607, "content": {"title": "Self-Organizing Resonant Network", "abstract": "We introduce the Self-Organizing Resonant Network (SORN), a novel learning paradigm that operates without backpropagation. To address core challenges in representation quality, learning stability, and adaptability faced by existing continual learning models, SORN operates within a robust feature space encoded online. Its learning process is driven by two tightly coupled, biologically-inspired plasticity principles: (1) Novelty-Gated Structural Plasticity: The system dynamically creates a new neural prototype only when an input cannot be adequately represented by existing knowledge (resonators), a mechanism analogous to a self-growing vector-quantized codebook. (2) Stable Hebbian Synaptic Plasticity: By incorporating Hebbian variants with normalization and homeostatic mechanisms, the network's association matrix stably learns sparse inter-concept correlations, effectively circumventing weight explosion and saturation issues. We theoretically demonstrate the framework's computational efficiency and convergence. Extensive experiments on standard continual learning benchmarks and unbounded data streams show that SORN not only surpasses mainstream methods in catastrophic forgetting resistance and accuracy, but also exhibits superior autonomous concept formation and stable adaptation when handling continuous, non-stationary environments.", "tldr": "A novel, non-backpropagation learning paradigm where a network self-organizes by dynamically creating neurons for novel concepts and learning their associations via local rules.", "keywords": ["Continual Learning", "Self-Organizing Networks", "Hebbian Learning", "Structural Plasticity", "Online Learning", "Representation Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fdb3fde8af71495cb28af9ce80bc3033434f999.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a biologically inspired continual learning framework (SORN). Instead of backpropagation use two complementary mechanisms:\n(1) Novelty-Gated Structural Plasticity, which creates or prunes “resonator” neurons as new concepts appear,\n(2) Stable Hebbian Synaptic Plasticity, which learns sparse inter-concept associations under homeostatic constraints.\n\nSORN operates in a fixed feature space (e.g., DINOv2 embeddings). The paper provides theoretical analysis. The evaluation is performed on several continual learning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper revisits biologically motivated, non-gradient-based learning — an underexplored but important area. \n2. The authors provide an analysis of stability and convergence."}, "weaknesses": {"value": "1. SORN’s stability crucially depends on pre-trained DINOv2 embeddings. This outsources representation learning to an external model, weakening the claim that it solves continual learning “end-to-end.” The system’s success may largely stem from DINOv2’s high-quality representations rather than from SORN’s mechanisms.\n2. The backprop-based baselines are trained end-to-end from scratch, whereas SORN uses a fixed high-capacity pretrained model. This unfairly advantages SORN in stability and efficiency metrics. An alternative comparison would freeze the encoder for baselines as well.\n3. SORN is compared to outdated baselines. Since the authors use pretrained model, they should compare to methods of similar spirit like FeTrIL [1], FeCAM [2], or H-Prompts [3].\n4. The paper is quite hard to read clearly. \n\n[29] Petit, G., Popescu, A., Schindler, H., Picard, D., Delezoide, B.: Fetril: Feature translation for exemplar-free class-incremental learning. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision. pp. 3911–3920 (2023)\n[10] Goswami, D., Liu, Y., Twardowski, B., van de Weijer, J.: Fecam: Exploiting the heterogeneity of class distributions in exemplar-free continual learning. In: Advances in Neural Information Processing Systems (NeurIPS) (2023)\n[42] Zhanxin Gao, Jun CEN, X.C.: Consistent prompting for rehearsal-free continual learning (2024)"}, "questions": {"value": "Can the model operate effectively when the encoder is fine-tuned jointly (e.g., partially trainable features)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WVRXVHuk6M", "forum": "kuaZXtReJ0", "replyto": "kuaZXtReJ0", "signatures": ["ICLR.cc/2026/Conference/Submission25477/Reviewer_LrGA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25477/Reviewer_LrGA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761282347524, "cdate": 1761282347524, "tmdate": 1762943447857, "mdate": 1762943447857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Self-Organizing Resonant Network (SORN) for continual learning.\nThe approach first uses a fixed, pre-trained encoder to extract features, and then applies a non-backpropagation learning mechanism that self-organizes. The network dynamically adds or prunes neurons based on a utility-driven novelty threshold and updates connections using a stabilized hebbian rule.\nExperiments on standard continual learning benchmarks compare SORN against regularization-based, replay-based, and dynamic architecture methods, and show less forgetting, and better performance. The paper also performs ablations to examine the effect of the components of their proposed method, the fixed encoder, and the update rule."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated and well-written. The method is explained thoroughly, and the notations are easy to follow.\n\n2. The paper shows strong experimental rigor, including careful hyperparameter tuning for both the proposed method and the baselines, an ablation study of key components, and sensitivity analyses for the new hyperparameters introduced by the approach.\n\n3. The paper covers the related literature well, and includes a wide range of baselines, both classical and recent,i n the experiments to compare with their method"}, "weaknesses": {"value": "1. The use of the encoder is a fundamental part of the proposed method in the paper. However, a key challenge in continual learning is actually for the model to learn the representations over time. The paper needs to disentangle the effect of the use of the encoder and the power of the design and structure of the network. For instance, how does a backpropagation-based network work in comparison to SORN, if its input is the encodings derived by the encoder? Furthermore, more investigation into the role of the encoder would be helpful. For instance, how does the performance change if we use other encoders/foundation models to extract features?\n\n\n2. Performance metrics should be reported with the number of seeds run for each method, and confidence intervals  and preferably statistical tests to show significant differenc,e especially between the methods that have close performance values (e.g., close forgetting values) with SORN. Since confidence intervals are not specified, this is unclear.\n\n\n3. The theoretical prepositions assume stationary input. While it is very valuable to examine the properties of SORN, like network size, boundedness, and convergence in a stationary setting, the fundamental difference in CL is that the input can be non-stationary. How do you expect these properties to change under non-stationarity?\n\n\n4. The paper rightly emphasizes task-agnostic continual learning as one of SORN’s main advantages. However, the reported metrics still depend on predefined task boundaries. Although identifying more suitable metrics for task-agnostic evaluation remains an open challenge, including additional measures such as online learning curves would make the evaluation more aligned with the paper’s stated goals.\n\nMinor comments:\n\nAdd a reference for some of the keywords, like loss of plasticity\n\nAdd the reference for CHNN in line 332\n\nAdd a reference to the appendix in the paragraph around line 297, referring to the hyperparameter sensitivity analysis section\n\nAdd the reference in the text to tables 1 and 2 when explaining the results"}, "questions": {"value": "1. How do you decouple the power of the encoder from your network structure? How does SORN compare to methods that use the same feature extraction method, but, e.g., backpropagation-based networks?\n\n2.  Why did you choose the DINOV2 encoder? How do the experiment results change in the case of the use of other feature extractors?\n\n3. What is the number of seeds run, and what are the confidence intervals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5IwxhzJxwr", "forum": "kuaZXtReJ0", "replyto": "kuaZXtReJ0", "signatures": ["ICLR.cc/2026/Conference/Submission25477/Reviewer_DasP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25477/Reviewer_DasP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514743334, "cdate": 1761514743334, "tmdate": 1762943447544, "mdate": 1762943447544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Self-Organizing Resonant Network, a novel paradigm for continual learning that operates without backpropagation except for the pre-trained feature extractor. SORN addresses catastrophic forgetting through two biologically-inspired principles: Novelty-Gated Structural Plasticity to dynamically create new \"resonators\" for novel concepts, and Stable Hebbian Synaptic Plasticity to learn sparse associations. This approach enables the network to achieve state-of-the-art resistance to catastrophic forgetting in various continual learning benchmarks, especially in non-stationary, task-agnostic environments."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very clearly presented and the proposed method is easy to understand. The approach is significant because of its non-backpropagation, bio-inspired update rule rooted in Hebbian learning. It thus offers potential contributions to both the neuroscience and continual learning communities. This novel architecture delivers strong empirical performance, demonstrating state-of-the-art resistance to catastrophic forgetting against leading baselines. Crucially, the work is supported by a robust theoretical analysis that formally clarifies the network’s convergence properties and capacity scaling, moving the contribution beyond a purely empirical result."}, "weaknesses": {"value": "1. My main concern about this work is whether the comparison to other CL methods in Table 1 is fair, given that the current method uses an advanced pre-trained visual encoder (DINOv2). See questions below.\n\n2. If possible, I would suggest adding a brief discussion of the limitations of the current method and potential directions for future work.\n\n3. The proposed method involves a range of different hyperparameters, and the settings vary considerably across datasets. This raises concerns about whether the method could scale to real-world scenarios with continuous distributional shifts."}, "questions": {"value": "1. Could the authors clarify which baselines also use a fixed pre-trained encoder and whether they use the same vision encoder? Is it possible to compare to these baselines after replacing their visual encoders with a frozen pre-trained DINOv2?\n\n2. The hyperparameter settings appear to differ substantially across datasets (Tables 5–8). How are these hyperparameters tuned? The authors discuss the sensitivity of performance to two key parameters, k and α, but it would be helpful to extend this analysis to different datasets and to other parameters that are assigned different values across datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ICdHe9ZMaW", "forum": "kuaZXtReJ0", "replyto": "kuaZXtReJ0", "signatures": ["ICLR.cc/2026/Conference/Submission25477/Reviewer_ZQ9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25477/Reviewer_ZQ9h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970344227, "cdate": 1761970344227, "tmdate": 1762943447317, "mdate": 1762943447317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-organizing, dynamically growing and shrinking networks architecture for continual learning that is validated on several common CL benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper gives a very detailed and well-reasoned account of the SORN model, taking great care to explain the reasonings behind design choices. The model itself is interesting and novel, although the merit of the individual concepts is hard to verify. Comparisons on common benchmarks are given w.r.t. to a wide number of baseline algorithms using a public reference library."}, "weaknesses": {"value": "I have grave concerns concerning a number of points that often come up in CL research. I have formulated questions w.r.t. each of these, and I may adapt my score depending on the answers:\n- fairness of comparison: if you compare SORN to other algorithms, it is unfair to use feature encoding for SORN only\n- hyper-parameter selection: SORN critically depends on a rather large set of hyper-parameters. It looks like they are selected by grid-search  based on a whole CL experiment with all tasks. That is strongly illegal in CL, since you would use knowledge of *all* tasks of the benchmark to select hyper-parameters. The point of CL is *not* having access to future data. So if you select hyper-parameters, you can do it on task 1 data but not more.\n- hyper-parameter selection for baselines: it is highly questionable to carefully tune hyper-parameters for the own algorithm while using default parameters for the baselines. We know that, e.g., \\lambda in EWC play are crucial role, and so do buffer sizes etc. in GEM or iCarL."}, "questions": {"value": "- when comparing to other CL baselines: do the other algorithms work on the pre-encoded feature space or on the raw data? Because if the latter is true, how is this comparison fair?\n- how are the hyper-parameters for SORN fixed? In the appendix you mention grid search for (k, \\alpha), what about the others? Please explain this process in more detail? What validation data are you using for this grid-search, is it the whole dataset that is tested after training on all tasks, or just a subset, e.g., task 1?\n-  is the fixed feature extractor (dinoV2) adapted to a particular dataset? Or do you just re-scale all images to the encoders' fixed input size and always use the same one?\n- please comment on the following statement: the fixed feature encoder makes each problem essentially linearly separable, and therefore the inherent difficulty of the different benchmarks is irrelevant\n- can you show the performance of a simple linear classifier on task 1 from each benchmark as preprocessed by the feature encoder? Just to demonstrate the inherent difficulty of the benchmarks?\n- how are parameters to baseline algorithms chosen? Do you use default parameters or do you optimize those as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qNkXblweeH", "forum": "kuaZXtReJ0", "replyto": "kuaZXtReJ0", "signatures": ["ICLR.cc/2026/Conference/Submission25477/Reviewer_8Gek"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25477/Reviewer_8Gek"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981992918, "cdate": 1761981992918, "tmdate": 1762943447078, "mdate": 1762943447078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}