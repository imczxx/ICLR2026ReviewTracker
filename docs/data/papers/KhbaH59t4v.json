{"id": "KhbaH59t4v", "number": 1687, "cdate": 1756906035318, "mdate": 1759898194719, "content": {"title": "Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach", "abstract": "In deep Reinforcement Learning (RL), the learning rate critically influences both stability and performance, yet its optimal value shifts during training as the environment and policy evolve. Standard decay schedulers assume monotonic convergence and often misalign with these dynamics, leading to premature or delayed adjustments. We introduce LRRL, a meta-learning approach that dynamically selects the learning rate based on policy performance rather than training steps. LRRL adaptively favors rates that improve returns, remaining robust even when the candidate set includes values that individually cause divergence. Across Atari and MuJoCo benchmarks, LRRL achieves performance competitive with or superior to tuned baselines and standard schedulers. Our findings position LRRL as a practical solution for adapting to non-stationary objectives in deep RL.", "tldr": "We introduce LRRL, a bandit-based meta-optimizer that adapts learning rates on the fly in deep RL, tackling the challenge of non-stationary objectives.", "keywords": ["Meta-learning", "Reinforcement Learning", "Continual Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17d9591e9b22fad9634e1ac2b9215134df25fbe8.pdf", "supplementary_material": "/attachment/e1d94bb723e940709775248dbba815278c6d68e8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes LRRL, a bandit-based method, to dynamically adapt the learning rate for deep reinforcement learning.\n\nThe set of learning rates are predefined in advance by users. LRRL will consider each learning rate choice as an arm in the multi-arm bandit setting.\n\nThe experiments show that the LRRL can gain benefit against the fixed rate schedule.\n\nThe paper is generally well written and easy to follow.\n\nHowever, the reviewer has a strong concern about the novelty, significant and lack of key baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea is simple and effective\n\nThe experiment is better than the fixed rate schedule."}, "weaknesses": {"value": "There are existing works, such as PB2 [1,2] have already solved the above problem in a much scalable and principled way.\n\n* While this paper defines the fixed and discrete schedule for learning rate, PB2 [1] considers a continuous range [min, max] that allows the search operate flexibly. At the end, PB2 can return the optimal learning rate.\n\n* While this paper only considers optimizing a single parameter, a learning rate, PB2 [1] can jointly optimize and learn the schedule for multiple parameters.\n\n* The idea of using EXP3 algorithm as multi-arm bandit for selection has also been explored in PB2-Mix [2] in which the authors of PB2-Mix consider optimizing the mixed space of continuous and discrete variables.\n\n* Both PB2 [1] and PB2-Mix [2] come with theoretical guarantee on the performance of the algorithm, while this paper LRRL does not have any guarantee.\n\n* This paper doesnot compare with PB2.\n\n\n[1] Parker-Holder, J., et al. (2020). Provably efficient online hyperparameter optimization with population-based bandits. NeurIPS 2020\n[2] Parker-Holder, J , et al. \"Tuning mixed input hyperparameters on the fly for efficient population based autorl.\" NeurIPS 2021"}, "questions": {"value": "Please see the weakness.\nConsider a compare with PB2 and explain why LRRL is better than PB2?\n\nCode from the RayTune library https://docs.ray.io/en/latest/tune/api/doc/ray.tune.schedulers.pb2.PB2.html\nCode from the author https://github.com/jparkerholder/PB2"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k73lURP2l4", "forum": "KhbaH59t4v", "replyto": "KhbaH59t4v", "signatures": ["ICLR.cc/2026/Conference/Submission1687/Reviewer_CaoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1687/Reviewer_CaoU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761090345813, "cdate": 1761090345813, "tmdate": 1762915856535, "mdate": 1762915856535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LRRL, a bandit-based meta-learning approach that dynamically selects the learning rate during deep RL training based on policy performance feedback. Experiments are conducted using Atari, Mujoco, and stationary non-convex optimization problems, demonstrating promising performance compared to fixed learning rates and standard decay schedulers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- In summary, this paper investigates the hyperparameter optimization (HPO) problem in deep RL. This is a promising area, and I really appreciate it. However, the whole paper focuses on optimizing the learning rate solely, which significantly restricts its landscape and scalability. Moreover, it seems that LRRL can only handle categorical HP values rather than continuous HP space.\n\n- Bandit-based HPO approaches have been widely studied, such as Hyperband [1] and ULTHO [2]. Could the authors clarify the key advantage of LRRL compared to the previous work?\n\n- Very few comparison experiment results are provided, which raises concerns about the superiority of LRRL. I recommend that the authors use ARLBench [3], a benchmark of HPO methods in RL, to conduct more comprehensive experiments.\n\n\n[1] Li L, Jamieson K, DeSalvo G, et al. Hyperband: A novel bandit-based approach to hyperparameter optimization[J]. Journal of Machine Learning Research, 2018, 18(185): 1-52.\n\n[2] Yuan M, Li B, Jin X, et al. ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning[J]. arXiv preprint arXiv:2503.06101, 2025.\n\n[3] Becktepe J, Dierkes J, Benjamins C, et al. ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning[J]. arXiv preprint arXiv:2409.18827, 2024."}, "weaknesses": {"value": "See the comments above."}, "questions": {"value": "See the comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JpbUornMbd", "forum": "KhbaH59t4v", "replyto": "KhbaH59t4v", "signatures": ["ICLR.cc/2026/Conference/Submission1687/Reviewer_mGzu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1687/Reviewer_mGzu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641336755, "cdate": 1761641336755, "tmdate": 1762915856410, "mdate": 1762915856410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using a multi-armed bandit algorithm to dynamically adjust an optimizer's learning rate in reinforcement learning. Concretely, the bandit selects from a finite set of learning rates (its 'arms') with the objective of maximizing \"performance gains\"."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed algorithm boosts the performance of a baseline RL algorithm without a significant computational overhead. Since the proposed algorithm is based on Exp3, an algorithm for a finite arm bandit, the computational overhead is significantly low compared to other meta-gradient methods."}, "weaknesses": {"value": "- The goal is unclear.\n- A heuristic definition of reward (Line 206) for learning rate tuning is not theoretically motivated well.\n- Provided empirical results do not seem to be convincing.\n\n# The Goal is Unclear\n\nThe research question asked in Introduction is the following: Can we adapt the learning rate dynamically based on the agent’s performance, rather than relying on training progress or gradient-based heuristics?\n\nThere would be many possible ways to adapt the learning rate dynamically based on the agent’s performance. Is the goal of the paper just finding one of them? I guess the authors actually want to find a better algorithm rather than just finding one of them. Then, a natural question is what metric to use. It seems the paper does not clearly mention which metric the paper cares about. Maybe regret?\n\nFurthermore, the research question and empirical results do not seem to align well, if I do not misunderstand the paper. In Introduction, the authors wrote\n\n> Because these methods rely on step counts or gradient norms rather than environment feedback, they often make premature or delayed adjustments, limiting their ability to cope with RL’s non-stationarity.\n\nand asked the research question above. Therefore, I expected that the authors would have compared the combination of the proposed method with SGD (optimizer WITHOUT learning rate tuning based on training progress or gradient-based heuristics) to Adam/RMSProp (optimizer WITH learning rate tuning based on training progress or gradient-based heuristics). However, compared the combination of the proposed method with Adam/RMSProp to Adam/RMSProp.\n\n# A Heuristic Definition of Reward\n\nThe reward definition (Line 206) for learning rate tuning is intuitive, but it is unclear whether it leads to any performance boost of a base RL algorithm. It would be nice if there exists any theory to justify it.\n\n# Empirical Results not Convincing\n\nLooking at figures, the performance gain by the proposed algorithm seems marginal.\n\nIn Figure 1, the authors might argue that their proposed method is on par with or superior to DQN with a BEST learning rate (Appendix B1). However, the fixed learning rate used there ($6.25 \\times 10^{-5}$) is a default learning rate of DQN in Dopamine. Therefore, it is possible that DQN with Adam might stably work well with the learning rate $6.25 \\times 10^{-5}$ across various environments. Then, we can simply use the learning rate $6.25 \\times 10^{-5}$, and there is no need to tune the learning rate.\n\nIn Figure 3, the proposed method seem to clearly outperform other learning rate decays only in Breakout and Seaquest. Also in Breakout, DQN with a fixed learning rate (in Figure 1) seems to perform almost on par with the proposed algorithm. (Is the black dotted line calculated correctly?)\n\nIn Figure 5, the authors wrote that the proposed algorithm achieves higher returns on two of five MuJoCo task, but it is unclear if the proposed algorithm is statistically better."}, "questions": {"value": "# Theoretical motivation for the choice of rewards for Exp3\n\nIs it possible to theoretically motivate the choice of rewards for Exp3? For example, can we guarantee a higher return of a base RL algorithm under some ideal situation?\n\n# Game where DQN with a default learning rate fails\n\nIs there a game where DQN with a fixed learning rate of $6.25 \\times 10^{-5}$ fails but LRRL performs better?\n\n# Typo?\n\nThere seem to be some typos.\n\nFirstly, in the definition of the pseudo-regret, I think $\\max$ must be in front of $\\sum$. See Section 11 of Lattimore & Szepesvari (2020) as well as Excercise 11.4 for an issue of using the regret defined as in the paper.\n\nSecondly, in Equation 3, I think the denominator of $f'_n$ must be $p_n(k)$. The current one seems to result in a biased loss estimate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lr6uH77Ctj", "forum": "KhbaH59t4v", "replyto": "KhbaH59t4v", "signatures": ["ICLR.cc/2026/Conference/Submission1687/Reviewer_9a3E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1687/Reviewer_9a3E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925347998, "cdate": 1761925347998, "tmdate": 1762915856197, "mdate": 1762915856197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* A modified version of Exp3 (the adversarial multi-armed bandit algorithm) for online adaptation (meta-learning) of the learning rate\n* Tested on reinforcement learning (to maximize returns), and also on non-convex optimization problems"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tests two codebases, two environment suites (Atari and MuJoCo), three base RL algorithms (DQN, IQN, and PPO), and three base optimizers (SGD, Adam, and RMSprop)\n* Also tests six stationary, non-convex optimization problems\n* To my limited knowledge, the approach is somewhat novel"}, "weaknesses": {"value": "* I think it overclaims the practical benefits. For example, \"LRRL ... [reduces] tuning effort while remaining competitive or superior to the best fixed choice\", but I think the empirical evidence for this is not that convincing. I think this overclaiming is a big weakness. For example, Fig. 8 shows the Exp3 learning rate is sensitive (which the paper itself notes).\n* Few seeds (5-10)\n* Adds 4 hyperparameters (alpha, delta, j, kappa)\n* I am pretty sure \"half of a standard deviation\" is a nonstandard amount of variation, and I do not think it is justified in the paper\n* The paper is framed around RL, but my understanding is the algorithm is not specialized to meta-learning the LR for RL, beyond the fact that it can handle nonstationarity and a non-differentiable final objective\n* Compares against cyclical LR and AdamRel, but does not experimentally compare against other meta-learning algorithms for tuning the LR"}, "questions": {"value": "* \"Similarly, adaptive optimizers such as RMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015) adjust effective learning rates (see Lyle et al., 2024)\"\n    * This confuses me. The \"effective learning rate\" in Lyle et al. is about neural networks that use normalization layers, which seems only tangentially related to adaptive optimizers. Lyle et al. do discuss the interaction of Adam with normalization layers, but still, the \"effective learning rate\" analysis there requires the normalization layers (or some similar, additional mechanism).\n* \"Because these methods rely on step counts or gradient norms rather than environment feedback, they often make premature or delayed adjustments, limiting their ability to cope with RL’s non-stationarity.\"\n    * I think this should either be noted in the paper as speculation, or a reference should be provided for it\n* \"a dual objective\"\n    * I think the paper should avoid the term \"dual\" here, because \"dual\" often has a precise meaning in math that is not meant here\n* \"Unlike fixed schedules or gradient-based meta-learning heuristics, LRRL is algorithm-agnostic, seamlessly integrates with optimizers such as Adam, RMSProp, and SGD, and enables dynamic adaptation across different phases of training.\"\n    * I'm unsure if I understand any of the three claims here, particularly for gradient-based meta-learning vs LRRL.\n    * \"algorithm-agnostic\": what does this mean here? I could imagine some meanings where \"algorithm-aware\" is actually better than \"algorithm-agnostic\". Is it supposed to mean differentiable vs. not differentiable?\n    * \"seamlessly integrates with optimizers such as [...]\": again, what does this mean here? PyTorch's autodiff can differentiate through all three of those optimizers.\n    * \"enables dynamic adaptation across different phases of training\": how does gradient-based meta-learning not do this?\n* \"which deep RL mitigates by employing a large replay memory and calculating the target using a frozen network\"\n    * many deep RL algorithms do not use a replay memory nor a target network\n* \"MAB can be viewed as a special case of RL [...]\"\n    * \"MAB is a special case of RL [...]\"\n* \"single best arm, and [...] is the accumulated reward\"\n    * the inline equation there overlaps the line above it\n* \"updates —specifically\"\n    * typo? (nonstandard to have a space on only one side of a single em dash in a sentence)\n* \"updated recursively\"\n    * Would it not be equally valid (and possibly simpler and thus clearer) to just say, for example, \"updated iteratively\"?\n* \"providing a controlled setting where performance can be attributed solely to learning rate adaptation\"\n    * Although I think it's great to test on stationary non-convex optimization problems like this, I'm confused by this description. The paper might instead say \"providing a simpler setting [...]\"?\n        * (To elaborate: in both these stationary optimization problems and the RL problems, the only thing the paper changes between the baseline and the proposed algorithm is the learning rate adaptation, right? Another way to put my confusion: if the paper also tested on, for example, stationary _convex_ problems, they would be even simpler settings than merely stationary _non-convex_ problems, and yet in all cases I would say \"the performance can be attributed solely to learning rate adaptation\".)\n* \"We consider five possible learning rates,\"\n    * To me, that subjectively feels like a very narrow range of learning rates. It's narrower than 1e-5 to 3e-4.\n* Figure 6 looks interesting, but is hard to interpret, in part because there is so much visual overlap between trajectories. A table of AUC or final loss or something like that might help a lot.\n* \"(e.g., convexity, [...]\"\n    * This \"e.g.\" is not italicized, whereas the other \"e.g.\" instances (at least in the main text) are italicized"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Jb8xcHejpq", "forum": "KhbaH59t4v", "replyto": "KhbaH59t4v", "signatures": ["ICLR.cc/2026/Conference/Submission1687/Reviewer_owuz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1687/Reviewer_owuz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080814597, "cdate": 1762080814597, "tmdate": 1762915856012, "mdate": 1762915856012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}