{"id": "OXPKjZ6iB8", "number": 7565, "cdate": 1758027756302, "mdate": 1759897846001, "content": {"title": "Revisiting Visual Understanding in Multimodal Reasoning through a Lens of Image Perturbation", "abstract": "Despite the rapid progress of multimodal large language models (MLLMs), the role of visual processing in multimodal reasoning remains underexplored. In a simple yet revealing experiment, we find that language-only models, when augmented with image captions, can sometimes outperform multimodal counterparts consuming raw visual inputs. This indicates that current MLLMs may perceive visual content but fail to effectively integrate it during reasoning. Moreover, even minimal visual perturbations such as small rotations lead to severe performance drops, exposing a fragility in their visual understanding. To address this overlooked bottleneck, we propose a lightweight visual perturbation (VP) framework that strengthens perceptual robustness without architectural changes or additional data. VP introduces three targeted strategies—distractor concatenation, dominance-preserving mixup, and random rotation—that can be seamlessly integrated into post-training pipelines including SFT, DPO, and GRPO. Extensive experiments across four multimodal reasoning benchmarks show consistent absolute gains of 1–2 points, with improvements holding across datasets, training pipelines, and even advanced RL-tuned models. Ablation and task-level analyses further reveal how different perturbations uniquely benefit geometry, algebra, OCR, and chart reasoning. These findings underscore a central insight: better reasoning begins with better seeing.", "tldr": "We show that MLLMs often fail to leverage visual inputs effectively. We propose a lightweight VP framework that boosts reasoning robustness across training methods and benchmarks, demonstrating that better reasoning begins with better seeing.", "keywords": ["Multimodal LLM", "Multimodal reasoning", "Image perturbation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ac2d6405a7c54e8e541cd4ba01cea1ee7ac6457.pdf", "supplementary_material": "/attachment/909fb3b29e516e09c74a8acfc402b7ac69cf58d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper combines data augmentation techniques (rotation / mixup / concatenation) with VLM training (SFT/DPO/GRPO). The authors claim a uniform performance gain across training datasets / types on mathematical reasoning benchmarks. They also run ablations / qualitative analyses to analyze the effect of their perturbation method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and the idea is simple. It is easy to understand the position of the paper.\n2. The experimental setup and methodology is very cleanly written. This enhances reproducibility of the results.\n3. The paper reports mean, stddev across 3 runs in Table 2. This helps ensure statistical significance of the results.\n4. Results are clearly annotated with the performance difference with the baseline in Tables 6, 7."}, "weaknesses": {"value": "# Feedback on Experiment Setup / Results Analysis\n- One of the biggest concerns I have about this paper is that Tables 2 and 3 do not seem to compare against a fair baseline. Based on the 54.9 number from Table 5, I can only guess that the numbers in Tables 2 and 3 are all from Clean + 1x VP. However, as the authors note, this is a number from when trained with a 2x compute. A fair comparison would be 1) 2x Clean vs. Clean + VP; or 2) Clean vs. half-Clean + half-VP. While I can believe that the proposed method will improve results on a fairly constructed experiment, I would suggest swapping out the main results with fair baselines, if the authors want to clearly argue the effectiveness of their method. At this point, it is unclear whether the numbers in Tables 2 and 3 are indeed from the proposed method (VP) or simply from more compute. \n- I do not fully agree with the points in the paragraph named “Complementary to Advanced Models” (lines 350 - 366). First, I think the paragraph is confusing because it is simultaneously trying to convey two different messages: 1) training existing checkpoints with VP datasets further enhances performance; 2) Geometry3K+VP leads to “comparable” performance as SOTA models. For the point 1), it is unclear whether the performance boost comes from the proposed method VP or from more compute (or the quality of the base dataset). To correctly claim the performance gain as deriving from the proposed method, you should set the baseline to be existing checkpoints + GRPO on existing dataset (e.g., MM-eureka-Qwen-7B + GRPO on MMMK12-16K). For the point 2), first note that the Geometry-3K results are not from the same two-stage pipeline (GRPO on existing dataset -> GRPO on VP dataset), but from a one-stage pipeline (GRPO on Clean + VP). Therefore, the entry for Geometry-3K in Table 4 (lines 333-334) should be marked separately and the caption should also clarify that all other entries correspond to a two-stage pipeline of Clean vs. Clean->VP, whereas the Geometry-3K results correspond to Clean vs. Clean+VP. Also, it is unclear if you can claim Geometry3K+VP as being “comparable” to SOTA models. I believe you want to claim that Geometry3K+VP is comparable to ThinkLite-VL-7B or VL-Rethinker-7B from the numbers 54.2 < 54.9 <= 55.2. However, ThinkLite-VL-7B and VL-Rethinker-7B claim SOTA from a wider range of tasks (including non-math datasets like MMMU). If you train on a strictly math dataset, you will of course need a smaller dataset than training on a more comprehensive dataset to get the same performance gain on math benchmarks. Even then, I think you might be cherry-picking results to claim SOTA status. The results on MathVista, MathVerse, MathVision (removing We-Math which are not reported for ThinkLite-VL-7B and VL-Rethinker-7B) are 72.6, 48.5, 28.4 (avg 49.8). These numbers are all significantly lower than ThinkLite-VL-7B’s 75.1, 52.1, 32.9 (avg 53.4) or  VL-Rethinker-7B’s 74.9, 54.2, 32.2 (avg 53.8). The numbers are taken from the respective papers. So I can only guess that either the evaluation protocol was different from the papers or that either the Geometry3K dataset or the proposed VP method is particularly useful for We-Math. Unless you report on the detailed breakdown of the results (as you promise in line 344), I cannot make a judgment on this yet.\n- I don’t necessarily agree with Line 177 (“assumption that both models possess comparable language understanding capabilities”). While the Qwen2.5-VL uses Qwen2.5 as the base LLM, the LLM weights have been further trained during the VLM integration, which may have changed certain benchmark performance (similar to Base vs. Instruct models on NLP / math benchmarks). It would be cleaner to compare against would be to use the exact same VLM (e.g., Qwen2.5-VL-7B) but ask the same question without the image\n\n\n# Missing Citations to / Discussion on Previous Works\n## Section 3 Observation 1 (LLM+caption > VLM)\nThe main argument here is that VLMs are able to extract enough information from an image to solve a relevant mathematical question even when it cannot directly solve it from the given image. A similar observation has already been made in previous literature. I recommend citing such results as relevant. Zhang et al. (2023) report that 1) VLMs can extract information from image even when it cannot solve the question in the image but can solve it in text format (Section 5.2); 2) prompting a VLM to extract relevant information from the image before solving a mathematical question boosts performance (Section 6). Park et al. (2025) also report that their finetuned models perform better when explicitly asked to convert an image to text first (Appendix I.6).\n\n[1] Zhang et al., “Lost in Translation: When GPT-4V(ision) Can’t See Eye to Eye with Text\nA Vision-Language-Consistency Analysis of VLLMs and Beyond,” arXiv preprint, 2023. \n[2] Park et al., “Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?,” ICML 2025, 2025. \n\n## Section 3 Observation 2 (benchmark performance drops when images are rotated)\nVLMs underperforming when the input images are perturbed have also been explored by Verma et al. (2024). I would suggest citing the paper.\n\n[3] Verma et al., “Evaluating Multimodal Large Language Models across Distribution Shifts and\nAugmentations,” CVPR 2024, 2024. \n\n## Section 5: Experimental Setup\nWu et al. (2024) also consider adding Gaussian noise to images when fine-tuning VLMs. While they only report evaluations for hallucinations, their result on resolving over-reliance on text tokens seems relevant to the work.\n\n[4] Wu et al., “NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models,” arXiv preprint, 2024. \n\n\n# Nit-picky details\n1. There seems to be a typo for the MathVista numbers (either the original version or the perturbed version) for Qwen2.5-VL-7B in Table 1. The difference between 66.2 and 49.1 is 17.1, not 16.3.\n2. On line 172, is there a reason you only point out MathVerse as the source of consistent results? Table 1 shows consistent results for MathVista and We-Math as well. \n4. Line 212: just to clarify, it was chosen uniformly at random?\n5. Line 240: how is the $lambda$ value chosen? What distribution is it drawn from?\n6. There are multiple references where the first and last names are reversed (e.g., Zheng Yaowei in line 639)\n7. Where is the table for detailed results that you promise in Line 344?"}, "questions": {"value": "- Can you elaborate on “higher-quality datasets such as Geometry3K and GeoQA-8K show more pronounced improvements” (line 311)? How do you define the quality here? Why do you consider Geometry3K and GeoQA-8K to be of higher quality than MMR1-6K or TQA-7K? Based on Tables 2 and 3, training on MMR1-6K leads to the best average accuracy across all training types (SFT/DPO/GRPO). \n- I don’t know if I agree with line 185 (“This transformation [rotation] preserves all semantic\ncontent”) Did you check if there are any explicit references to spatial relations between objects in the image (e.g., “left/right/up/right”) in the questions in the existing benchmarks? If so, the question might no longer be correct after the images are rotated.\n- Line 214: How did you determine the distractor is semantically irrelevant? Did you use an external VLM? Did you hand-curate such data points? Is there any reference (e.g., “point A”) that appears in both images? \n- What is the rationale behind choosing the longest/shortest response for SFT/DPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kckLyKPMZc", "forum": "OXPKjZ6iB8", "replyto": "OXPKjZ6iB8", "signatures": ["ICLR.cc/2026/Conference/Submission7565/Reviewer_EuC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7565/Reviewer_EuC5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761316522951, "cdate": 1761316522951, "tmdate": 1762919659670, "mdate": 1762919659670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduces a fix that targets the fragility in MLLM-based visual reasoning. They introduce and incorporate Visual Perturbation (VP) during the post-training process e.g. GPRO. The results show that VP contributes to consistent improvements over multiple multi-modal visual reasoning benchmarks, such as MathVision and MathVerse."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I find the following aspects of this work remarkable\n\n1. The authors have clearly demonstrated their motivation. The lack-of-robustness issue of existing reasoning MLLMs is well explained through clear examples such as Table 1. \n2. The design of the experiments, along with all the verifications to demonstrate the effectiveness of VP, are comprehensive. I appreciate the authors’ effort to cover all the corners for as much as possible."}, "weaknesses": {"value": "Still, I find several design flaws/loopholes with regard to VP. Out of the following two concerns, the first one is a major severe flaw that makes me question if the contribution of VP is genuine enough, especially if left unresolved. \n\n1. **VP seems to introduce new problems, by rendering the original image unsolvable after perturbation.** Several more drastic perturbation strategies from VP, such as Random Crop 45%, may simply make original task unanswerable. For example, in Figure 3, after the applying the 45% Random Crop, the diagram in question no longer provides sufficient visual cues to solve x, for anyone (including human beings) that is concerned to start with. I believe the motivation of VP is to improve the robustness of MLLMs, but using unsolvable examples during post-training would achieve quite the opposite effect - the model would be driven to make ungrounded judgements, since now it’s fed with insufficient contexts to be based on.\n2. **VP may seem to offer some improvements, but only up to a certain point.** From Table 2 & 3, it seems to me that the introduction of VP can only help improve the overall benchmarking performance up to a limited extent, all capped at 55%, suspiciously. To truly verify if VP can consistently improve benchmarking performances, as a suggestion, I would recommend the authors provide additional evidences, with incremental sizes in post training data, e.g. GRPO+VP with 2K, 4K, and 6K randomly sampled instances from GeoQA-8K."}, "questions": {"value": "Please find my main concerns in the Weaknesses section. Other than that, I have one more minor concern.\n\n1. It seems the authors have only tested with Qwen2.5-VL-7B as the only baseline/backbone. Have they tried deploying alternative baseline models, such as Qwen2.5VL-72B or variants of InternVL2/LLaVA? It would bring more contributive if we could see VP has generalizable deployability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Vqio0f1hM3", "forum": "OXPKjZ6iB8", "replyto": "OXPKjZ6iB8", "signatures": ["ICLR.cc/2026/Conference/Submission7565/Reviewer_KowY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7565/Reviewer_KowY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861477811, "cdate": 1761861477811, "tmdate": 1762919658519, "mdate": 1762919658519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study how well multimodal LLMs actually use visual information in math-reasoning tasks. Their experiments build on 2 primary findings: (a) On vision tasks, a language-only model given generated image captions can improve over the multimodal model that produced the captions. (b) Multimodal models are susceptible to image rotation. To mitigate this issue, the authors propose a data augmentation based training, that improves model performance across all the tasks. The"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper lies in its motivating experiment that shows the key weaknesses they are targeting to. By showing strong evidence that current MLLMs don't reliably use visual information, they motivate the necessity of augmentation based training. The proposed algorithm uses conventional augmentation methods that visual perturbation and improve the model's performance across $4$ benchmarks. Furthermore, the authors conduct extensive study on which perturbation method affects each benchmark the most. Overall, the authors provide a comprehensive empirical study showing the strengths of their proposed training algorithm."}, "weaknesses": {"value": "There are few questions regarding the experiment setting that I would like the authors to address.\n\n\n\na) How does the fine-tuning of the vision tower affect the performance? Is the performance gain primarily because of the weakness in the vision tower? What happens if you freeze the vision tower or the language model during training?\n\nb) Can perturbations be applied at evaluation time to improve the model performance further? That is, one could apply different kinds of perturbations to the image and then take a majority vote among all responses of the model to the different perturbed images. \n\nc) How do the different perturbation types affect the model performance when the same (or other) perturbations are applied to the images during evaluation? And can the authors provide more quantitative arguments on why such perturbations helped the model after training?"}, "questions": {"value": "Please check my questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "69xVJMKEMB", "forum": "OXPKjZ6iB8", "replyto": "OXPKjZ6iB8", "signatures": ["ICLR.cc/2026/Conference/Submission7565/Reviewer_ixh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7565/Reviewer_ixh4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875632920, "cdate": 1761875632920, "tmdate": 1762919657845, "mdate": 1762919657845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how MLLM models utilize its visual inputs and how robust these models are to input perturbations. It proposes to use different input perturbation methods to input images when training MLLMs. Experiment results show modest improvements in performances."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally clearly written but with some logical jump (please see Q2 in weakness).\n\n2. I appreciate the authors' motivation in conducting analysis experiments in Section 3. Such experiments can help us understand where current MLLMs fall short, e.g., whether they cannot perceive the visual inputs well enough or it's their lack of reasoning capability, or even that their reasoning is not well grounded on the inputs (more in weakness Q1)."}, "weaknesses": {"value": "1. The motivation in the experiments conducted in Figure 1 is interesting, but I feel the conclusion that \"MLLMs may generate\naccurate visual descriptions but fail to effectively integrate them during reasoning\" is not very well supported by the experiment setup. Specifically, Answer C has better performance than Answer B does not necessary imply MLLMs fail to integrate visual description during reasoning. It might be from the fact that the explicitly produced the caption may help ground models to the input image in answering the question. To support the claim, shouldn't the setup be replacing QwenLM with Qwen-VL in the Answer C setup? This may help us understand where MLLMs fall short.\n\n2. In the paper, there is logical jump from MLLM's reasoning capability based on visual inputs to their robustness to visual input perturbations. It's not clear to me where the connection is, and how improving robustness to input perturbation helps reasoning. These seems to me as orthogonal axes.\n\n3. The main method proposed in the paper appears marginal, which applies existing input perturbation (data augmentation) methods to input images in model training pipeline. It is thus not surprising to see that augmentation helps improves model performance, but also only with relatively small gains (as seen in Table 2)."}, "questions": {"value": "Could the authors clarify the main contribution of the paper and how that relates to existing data augmentation literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LKQVSSfk6V", "forum": "OXPKjZ6iB8", "replyto": "OXPKjZ6iB8", "signatures": ["ICLR.cc/2026/Conference/Submission7565/Reviewer_oSMF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7565/Reviewer_oSMF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136192323, "cdate": 1762136192323, "tmdate": 1762919656710, "mdate": 1762919656710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}