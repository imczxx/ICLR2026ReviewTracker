{"id": "xZDoGrMTGI", "number": 19782, "cdate": 1758299281509, "mdate": 1759897019842, "content": {"title": "STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing", "abstract": "Large language models (LLMs) can parse natural language into SQL or Cypher, but remain fragmented—lacking a unified ability to reason across both relational and graph-structured data. We present STRuCT-LLM, a reinforcement learning framework for cross-domain query understanding. Our approach integrates supervised chain-of-thought traces with topology-aware execution rewards, enabling models to acquire complementary reasoning skills: computational and inter-column analysis from SQL and graph traversal from Cypher. On noisy real-world datasets (e.g., SEDE), STRuCT-LLM achieves consistent gains over supervised fine-tuning baselines, including ~17\\% fewer logical errors and ~20\\% fewer data-reference errors, while maintaining robustness under perturbations. Beyond benchmark improvements, we provide a structural analysis of SQL–Cypher equivalence and qualitative case studies showing how unified training resolves errors that single-domain models cannot. These results establish reinforcement learning as a driver of structure-aware generalization across heterogeneous data modalities, paving the way for natural language interfaces to more diverse and unified database systems.", "tldr": "Our research shows that joint training of language models on Text-to-SQL and Text-to-Cypher tasks with reinforcement learning improves performance through structural transfer, enhancing accuracy in both tasks and related structured data reasoning.", "keywords": ["Reinforcement Learning", "LLMs", "Text-to-SQL", "Text-to-Cypher", "Structural Transfer"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13b86a7fbdf2256d78d731c1005f43844b8c117b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces STRUCT-LLM, a reinforcement learning (RL) framework that unifies tabular (SQL) and graph (Cypher) reasoning for large language models (LLMs). The core idea is that jointly training on both SQL and Cypher teaches complementary skills: SQL imparts computational reasoning, while Cypher imparts graph traversal logic.\nThe method uses a novel \"topology-aware reward\" for Cypher, which provides more granular feedback than simple execution success. This joint training approach significantly reduces logical errors (by 17%) and data-reference errors (by 20%) compared to single-domain training. The framework also enables bidirectional skill transfer—SQL training improves complex Cypher queries, and Cypher training improves complex SQL queries —and generalizes to unseen QA tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a clear and important gap in current LLM capabilities. While Text-to-SQL and Text-to-Cypher are often treated as separate tasks, real-world data is increasingly heterogeneous, blending relational and graph structures. The paper makes a compelling case that a unified framework is necessary.\n2. The paper goes beyond simple accuracy metrics by providing an error-type analysis (Table 2). It shows that joint training specifically reduces logical errors (by ~17%) and data-reference errors (by ~20%), offering deeper insight into the model's improvements. The qualitative case studies in Figure 2 are a key strength. They provide concrete, easy-to-understand examples of the claimed \"bidirectional transfer\": Case A shows SQL's computational logic (median/IQR) transferring to Cypher, and Case B shows Cypher's traversal logic (distinct counting) transferring to SQL."}, "weaknesses": {"value": "1. The paper lacks significant technical novelty (only combination of different tricks) and fresh insights, making it less suitable for the ICLR conference.\n2. The supervised fine-tuning (SFT) stage relies on synthetically generated Chain-of-Thought (CoT) traces. These traces are \"produced and verified by LLMs for coherence\". The quality and reliability of this LLM-as-verifier pipeline are crucial for the model's foundational reasoning ability, but this process could introduce noise or biases from the generator/verifier model.\n3. The paper’s writing quality leaves room for improvement. For instance, the reference formatting is inconsistent, and there are missing spaces in key places—such as in the example: \"(Li et al., 2023; Pourreza & Rafiei, 2023).Executable\" (where a space is needed between the closing parenthesis and \"Executable\")."}, "questions": {"value": "The paper's two-stage (SFT + RL) training approach is computationally expensive. The authors note that training the 32B model required 32 H100 GPUs for 20 hours. This high resource requirement could be a significant barrier to reproducibility and practical adoption for many researchers and organizations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qM8ShxJKji", "forum": "xZDoGrMTGI", "replyto": "xZDoGrMTGI", "signatures": ["ICLR.cc/2026/Conference/Submission19782/Reviewer_E8re"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19782/Reviewer_E8re"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552339179, "cdate": 1761552339179, "tmdate": 1762931631581, "mdate": 1762931631581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STRuCT-LLM, a unified framework for semantic parsing across relational and graph-structured data. The approach leverages reinforcement learning to jointly optimize models on Text-to-SQL and Text-to-Cypher tasks, combining chain-of-thought supervision with instruction-tuned large language models and Group Relative Policy Optimization (GRPO) conditioned on execution feedback.\n\nA key contribution is the design of task-specific reward functions even trivial: component-level matching rewards for SQL query generation and graph edit distance-based rewards for Cypher query generation. Through joint training, the model achieves competitive performance on both semantic parsing tasks while exhibiting improved generalization and zero-shot transfer capabilities compared to separately trained baselines. Furthermore, evaluation on downstream benchmarks for tabular and knowledge graph question answering demonstrates that the model outperforms its base LLM, despite lacking direct training on these tasks. This suggests the joint training regime enables the model to capture latent structural knowledge transferable across domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Authors first focus on Cypher, a graph-abased semantic parsing domains with reinforcement learning.\n2. Experiments are sound."}, "weaknesses": {"value": "1. The whole paper is trivial, is just like implement GRPO algorithm in semantic parsing domains and run experiments. It seems that authors only refine reward function with other parts not different with GRPO-based papers in other domains. By the way, the are many GRPO-based papers for text-to-SQL training such as SQL-R1, Reasoning-SQL, Arctic-SQL, author didn't compare results with those relevant methods. From my perspective, the only difference is different reward function. Therefore, i think the the contribution is not significant enough.\n2. The motivation of unified training is not obvious, even if there are some logics shared across SQL and Cypher, this may also sacrifice preciesness of each structure. Also, authors didn't show up difference between unified training and separate training or continuous training to illustrate the advantages of unified training.\n3. The method only has been verified on one kind of backbone of llms, which cannot prove that the method is effective instead of Qwen pre-trained relational knowledge can be elicited by RL."}, "questions": {"value": "1. Have the authors adequately compared against recent GRPO-based semantic parsing methods (SQL-R1, Reasoning-SQL, Arctic-SQL) that also optimize SQL generation? Compared to usage of GROP of these methods for SQLs or relational data, except just modifying reward functions, what are other benefits.\n2. What empirical evidence demonstrates that unified Text-to-SQL and Text-to-Cypher training provides tangible advantages over separate or sequential training? Does joint training risk compromising the precision of structure-specific optimization?\n3. Can the reported improvements be confidently attributed to the proposed method, or do they reflect model-specific capabilities of the Qwen backbone? Would results generalize to other LLM architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NaLdbUuyEe", "forum": "xZDoGrMTGI", "replyto": "xZDoGrMTGI", "signatures": ["ICLR.cc/2026/Conference/Submission19782/Reviewer_cZdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19782/Reviewer_cZdx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761688708811, "cdate": 1761688708811, "tmdate": 1762931630316, "mdate": 1762931630316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper deals with training large language models to parse natural language into both SQL and Cypher. The proposed framework STRuCT-LLM uses supervised chain-of-thought traces and Group Relative Policy Optimization to fulfill this goal. By utilizing graph edit distance, topology-aware structural reward is achieved. Compared to other models, the one proposed can understand cross-domain query and transfer complementary skills from SQL and multi-hop traversal from Cypher. The experiments are conducted across Spider,EHRSQL, BIRD and Text2Cypher, with additional zero-shot transfer to table and KG QA (CRT-QA, TableBench, CR-LT-KGQA) and to an unseen query language (MQL). The experiment proves that the proposed method outperforms the basic method and reports consistent gains, including ~17% fewer logical errors and ~20% fewer data-reference errors versus single-domain baselines. Also, it maintains good execution accuracy. However, limitations like computational cost, unexplored extensions to interactive or schema-free parsing, and potential indirect leakage for this method still exist, which may need to be addressed in the future work.\n\nFor the contribution, this method’s joint training reduces logical errors by ~17% and data-reference errors by ~20% successfully, which proves its effectiveness in reasoning across both relational and graph-structured data. Also, it uses bread benchmarks across Spider, EHRSQL, and BIRD; there’s also zero-shot transfer to Table/KG QA and unseen language (MQL). This method achieves a better result in many scenarios compared to the basic method. Additionally, it shows bidirectional transfer. The contribution is substantive and broad."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The reward design is of novelty. It combines multi-signal rewards with a topology-aware structural reward for Cypher, going beyond binary execution checks.\n- It has Bidirectional transfer across formalisms, which reduces both logical and data-reference errors.\n- It is with credibility via analyses and ablations. Error type breakdown and ablation studies make improvements interpretable and trustworthy.\n- The CoTplus GRPO pipeline, reward mixing, and cross-formalism setup can be reused to new datasets, schemas, and even other query languages."}, "weaknesses": {"value": "- The figures are overly concise. For example, Figure 1 shows a unified pipeline but does not explain how the same natural-language question is systematically mapped into SQL and Cypher. The figure also doesn’t describe where they diverge and converge, which makes the figure more difficult to understand.\n- RL uses a modest sample size (~3.5k per language); Cypher evaluation shares a lineage with training data, which is limited.\n- There is no dedicated hallucination metric.\n- The evaluation relies on BLEU/EXE, some other evaluation metrics like Component-level F1, Efficiency/executability assessments should also be considered\n- Training/fine-tuning large models is costly. This may be a challenge when this method is applied in the practical world.\n- It does not provide variance across random seeds, statistical significance tests, or sensitivity analyses.\n- For generalisation beyond SQL/Cypher, it still needs to be improved. It doesn’t discuss how well the method scales/extends to other graph/query languages other than MQL."}, "questions": {"value": "- For the figure 1, a side-by-side mapping can be added to illustrate how the same NL question can be converted to SQLquery and Cypher query. It should also show where they converge/diverge and include a schema-linking flow.\n- Expand RL data with schema-preserving augmentation and new DB/KG domains.\n- For hallucination metric, add some other metrics like schema-violation rate, fabricated attributes/labels, and SQL–Cypher answer consistency.\n- Add component-level F1 and canonical IR EM, canonicalized pattern matching, and efficiency metrics.\n- The cost is still a challenge that needs to be addressed or optimized.\n- Run some seeds and report mean±std with CIs; do sensitivity tests on GRPO and reward weights.\n- Still need to add some content to illustrate how this method can be applied to other graph/query languages to prove its generalization and effectiveness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AwmOqlH6JA", "forum": "xZDoGrMTGI", "replyto": "xZDoGrMTGI", "signatures": ["ICLR.cc/2026/Conference/Submission19782/Reviewer_6rVQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19782/Reviewer_6rVQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836109267, "cdate": 1761836109267, "tmdate": 1762931629669, "mdate": 1762931629669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes   a reinforcement-learning framework that trains large language models to reason across tabular (SQL) and graph databases within a single model. Its main claim is that structured reasoning across these symbolic modalities can be unified through Group Relative Policy Optimization (GRPO), guided by execution-aware, syntax-aware, and topology-aware rewards. \n\nThe study reports empirical improvements:they report 13.5% relative(?) gain on the Spider benchmark and 73.1% on text-to-Cypher tasks. They also suggest observing cross-formalism transfer: SQL’s arithmetic logic strengthening Cypher’s traversal reasoning, and vice versa. They say this hints at a deeper structural equivalence among symbolic data representations. Conceptually, this is an ambitious and appealing direction, which is believable. \n\nYet, the paper’s significance is difficult to judge. The reported gains are not compared against the state-of-the-art baselines listed on public leaderboards like Spider or BIRD. Without those metrics or direct SOTA comparison, I have a hard time deciding whether STRuCT-LLM advances the field or merely improves upon its own controlled baseline. Moreover, I am worried about the methodology. I seems to me that it us computationally heavy and brittle. \n \nI will be willing to increase my score if the authors can address these two issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's methodology is interesting and the writing is clear. The main questions is the significance."}, "weaknesses": {"value": "The main question for me is the significance of the result, and whether this approach actually moves the needle."}, "questions": {"value": "see the review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vadH6j4bqX", "forum": "xZDoGrMTGI", "replyto": "xZDoGrMTGI", "signatures": ["ICLR.cc/2026/Conference/Submission19782/Reviewer_L3bt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19782/Reviewer_L3bt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947907412, "cdate": 1761947907412, "tmdate": 1762931628574, "mdate": 1762931628574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}