{"id": "27fc8hXB5N", "number": 25540, "cdate": 1758369009966, "mdate": 1759896716282, "content": {"title": "Geometric Compression in Grokking: The Three-Stage Modular Dynamics of Transformers", "abstract": "A central mystery in deep learning is how generalizable algorithms emerge from the complex dynamics of training. The phenomenon of grokking serves as a canonical example of this puzzle. While mechanistic reverse engineering has successfully identified the final algorithms networks discover, the dynamic process of their formation remains largely uncharacterized. Progress in understanding these dynamics is hindered by complexity metrics that are either monolithic or presuppose non-universal architectural properties like piecewise linearity. We approach this challenge from a geometric perspective, introducing the Geometric Coherence Score (GCS), which quantifies how consistently networks transform local geometric structures across inputs, revealing the hidden geometric evolution of algorithmic learning. Applying GCS to Transformer grokking on modular arithmetic, we discover a universal three-stage construct-then-compress dynamic with precise modular division of labor: (I) Early Geometric Convergence attention achieves high geometric coherence through simple memorization patterns; (II) Geometric Restructuring attention actively decreases coherence by constructing complex structured representations necessary for generalization; (III) System wide Consolidation all computational flows coordinately increase coherence, stabilizing the generalizable algorithm. We substantiate this discovery through multiple lines of evidence: the dynamic persists across various activation functions (ReLU, GeLU, SiLU); it distinguishes successful grokking from overfitting (where geometric restructuring fails); and GCS dynamics directly correspond to the evolution of attention patterns from uniform simplicity to algorithmic sophistication. Our work reframes grokking as sophisticated modular geometric reorganization, providing the first direct geometric evidence for construct then compress mechanisms in neural networks and offering a principled diagnostic tool for interpreting emergent algorithms.", "tldr": "We show that grokking in Transformers is not monotonic simplification, but a \"construct-then-compress\" algorithm where the Self-Attention module must first increase its geometric complexity to enable a subsequent, rapid compression in the FFN.", "keywords": ["Grokking", "Geometric Deep Learning", "Transformers"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0b2ed6db55030eed8cb1e7520d8453a47c3de74.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work offers a mechanistic perspective on grokking and introduces a principled geometric framework to analyze the dynamic interplay of complexities that drive learning in neural networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The proposed method is novel and elegant, offering a new way to study neural networks.\n\n- geometric coherence is a new way for understanding grokking dynamics, going beyond static complexity metrics\n- modular method allowing to study separately the attention and the feed-forward layer with the same method\n- clear experimental setup"}, "weaknesses": {"value": "- missing scalability discussion for larger networks\n\n- the experiments are currently confined to a single, clean algorithmic task (modular addition). While this is a standard and valid testbed for grokking, the paper would be significantly strengthened by a demonstration on another task (e.g., a different modular operation or a simple symbolic regression task) to show the generality of the dynamic beyond a single function."}, "questions": {"value": "- Does the three-stage dynamic appear for other grokking tasks? This is critical for claiming a \"universal\" mechanism.\n\n- Liu et al. 2024 also measured compression during grokking. How do GCS and LMN compare? Do they capture different aspects or the same phenomenon?\n\n- You show correlation between GCS and generalization, but is there evidence that GCS causes or predicts grokking before it happens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pf4zWuCE7g", "forum": "27fc8hXB5N", "replyto": "27fc8hXB5N", "signatures": ["ICLR.cc/2026/Conference/Submission25540/Reviewer_EySh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25540/Reviewer_EySh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875484752, "cdate": 1761875484752, "tmdate": 1762943467386, "mdate": 1762943467386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel metric for analyzing the way in which a network changes the local structure of the representational space (which they assume is captured by a low-dimensional manifold) using a metric (\"Geometric Coherence Score\") that measures how strongly the model locally compresses this space. They then use this score to measure changes in GCS in Transformers trained on modular addition (a task on which Transformers show grokking behavior). They connect the grokking behavior to three distinct phases indicated by different behaviors of the GCS."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-written.\n- Understanding the representational dynamics of Transformers over training is an important challenge and introducing new tools for this purpose can be very valuable.\n- The expand-then-compress mechanism the authors identify in the grokking Transformer is interesting."}, "weaknesses": {"value": "1. Primarily, I think the paper currently insufficiently shows why the phenomena the authors measure requires the introduction of a novel method. It seems to me that the phenomenon of the expansion and compression could also be captured by general measures of dimensionality (such as the participation ratio). While I think the introduction of novel measures is certainly valuable, I think the paper would benefit substantially from demonstrating why this novel measure is useful and provides insight beyond existing measures.\n\n2. Further, I had trouble understanding how your metric is defined. You characterize the local geometry by using the k nearest neighbors of each data point $x_i$. That makes me think that the resulting singular vectors $v_{i,k}$ are separately defined for each data point. But then in constructing $G$, you're again considering a k-NN neighborhood? Is this one different from the first one? Moreover, in equation 3, you're measuring the coherence between those neighborhoods by measuring the correlation between the different pairs of the transformed singular vectors $v_{i,k}'$. Doesn't that make the assumption that the singular vectors $v_{i,k},v_{j,k}$ are aligned? E.g. if two data points' neighborhoods have literally the same mapping $f$ and the same singular vectors, but in a different order, wouldn't $G_{ij}=0$? If that's true, that seems problematic. Am I missing something?\n\n3. Relatedly, I also think it would be important to provide an intuition for what each step is doing and why the equations are defined as they are defined. E.g. it would be great to explain what equation (3) is measuring --- my understanding is that similar mappings should have similar transformed singular vectors $v'$ and so by measuring the average correlation between those vectors, we measure how similarly those mappings are in different parts of the space. Is that correct? One suggestion for how you could provide this intuition would be to give a couple of different examples of different local geometries in the input and different mappings and explain how that affects the overall mapping. E.g. what happens when the local geometries in the input are very different but the mappings are very similar? What happens in the contrary case?\n\n4. Finally, I think it would be helpful to discuss how this metric relates to established methods in the field. E.g. have people previously used entropy to characterize coherence in this kind of context?\n\nIn its current form, these concerns prevent me from recommending acceptance. However, I think that the authors address an important problem with a potentially promising approach. I am looking forward to the rebuttal and am certainly open to improving my score."}, "questions": {"value": "- Could you explain whether my understanding in Weaknesses, paragraph 2, is correct? If so, why is the situation I'm describing not problematic?\n- Would other measures of representational geometry (e.g. the participation ratio) also pick up on the transitions in network training you're identifying?\n- Why is setting $G_{ij}=0$ for non-neighboring pairs reasonable? Couldn't the tangents still be highly correlated?\n- Can you elaborate on the \"competing circuits\" hypothesis and how it relates to your findings?\n- Can't you see the initial stages of Phase II at the end of Figure 4? You can see the attention path GCS decreasing and the MLP Path GCS increasing.\n- Figure 5 is really difficult to see right now due to its size. Further, is it possible to provide a more quantitative measure for how the attention scores at these different phass are different? While I can see certain visual differences, it is difficult to get a sense of what they mean in particular comparing \"peak state\" and \"valley state\". You're also introducing this terminology for the first time here, I would suggest keeping your terminology consistent with the one you're using earlier.\n\n**Minor suggestions**\n- It might be useful to visually indicate the epoch where grokking starts setting in in the flow scores in Fig. 3\n- It would be helpful to visually indicate the phases in Fig. 2"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1xhV9vEsf2", "forum": "27fc8hXB5N", "replyto": "27fc8hXB5N", "signatures": ["ICLR.cc/2026/Conference/Submission25540/Reviewer_uodt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25540/Reviewer_uodt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949179409, "cdate": 1761949179409, "tmdate": 1762943467172, "mdate": 1762943467172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces a new method for measuring the dynamics of grokking called the Geometric Coherence Score (GCS) and applies it to modular addition, a well-studied task exhibiting grokking. Training 1-layer transformers and tracking GCS the authors describe a three-stage construct-then-compress dynamic. GCS also appears to distinguish between overfitting and generalization in this particular setting."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Potentially novel perspective on the dynamics of grokking.\n2. The use of multiple activation functions."}, "weaknesses": {"value": "**Some missing citations**:\n\n*Lazy-rich training dynamics* (mentioned on lines 315, 431, 465):\n\n[1] \"Grokking as the Transition from Lazy to Rich Training Dynamics\" Kumar et al. ICLR 2024\n\n[2] \"Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry\" Chou et al. ICML 2025\n\n*Connection to double descent* (line 465):\n\n[3] \"Unifying Grokking and Double Descent\" Davies et al. 2023\n\n[4] \"Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition\" Huang et al. COLM 2024\n\n*GELU*:\n\n[5] \"Gaussian Error Linear Units (GELUs)\" Hendrycks et al. 2016\n\n*Related complexity measures*:\n\n[6] \"Deep Networks Always Grok and Here is Why\"  Imtiaz Humayun et al. ICML 2024\n\n*Previous works studying modular addition, features/representations*:\n\n[7] \"Grokking modular arithmetic\" Gromov. 2023.\n\n[8] \"The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks\" Zhong et al. NeurIPS 2023.\n\n[9] \"Feature emergence via margin maximization: case studies in algebraic tasks\" Morwani et al. ICLR 2024\n\n[10] \"Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks\" McCracken et al. NeurIPS 2025\n\n\n**Narrow experimental scope**: only study one architecture that is a single layer. This is problematic because large changes in what's learned have been observed by different papers studying different experimental settings and/or architectures in modular addition. It's worth noting this paper doesn't cite any of these works, being: [8], found that uniform attention models learn ``pizza circuits'' instead of the clock circuits described by Nanda. [9], found that 1 layer networks learn all n-1/2 frequencies (mod n), importantly explained that the generalizing features emerged due to margin maximization and tracked it throughout training. [10], empirically found that 2, 3, 4 layer networks learn O(log(n)) frequencies and proved it. It remains an open problem [9, 10] why there's such a stark difference in the number of features that emerge between 1-layer vs multilayer networks, and it's conjectured it has to do with training dynamics, which your paper studies.\n\nIn light of the above, for this paper to be convincing, it's necessary to see experiments on MLPs of 1-3 layers and transformers of 1-3 layers, and I would hope to see that the geometric coherence score works over all experimental conditions consistently. I'd also like to see quadratic activations studied, since [7] proved an exact solution exists in networks using them, and many papers followed this up contrasting ReLU with quadratic activations.\n\n**Poor plot quality**: the plots are hard to read with tiny fonts and are pixelated. They're also oversized and taking up much more space than necessary, and this space should be used to include other experiments.\n\nOverall, this work seems to be preliminary, and were it reorganized with additional experimental results, it could be an interesting piece of the modular addition story (were it to answer any of the remaining open problems on modular addition, e.g. what changes in the training dynamics between 1 and 2 layer networks. However, if instead its goal is to serve as a work aiming to give insights into grokking, it's far too limited in scope---grokking was originally studied on modular addition but has since been studied on many other datasets including natural data (e.g. MNIST, CIFAR-10, etc). Thus, I would like to see this framework working on these datasets as well (unless this framework resolves the aforementioned open question, or any other ones on modular addition)."}, "questions": {"value": "Q1. In the main paper why are you using d=8 if the appendix states that d=2 is the best? \n\nQ2. If you were looking at models that weren't a transformer (e.g. MLP without attention), what aspect of those networks would correspond to the attention flow you saw in the transformer? And what would be the corresponding version of Figure 5? Would neuron activation plots capture this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3G4GBp06ZL", "forum": "27fc8hXB5N", "replyto": "27fc8hXB5N", "signatures": ["ICLR.cc/2026/Conference/Submission25540/Reviewer_bJwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25540/Reviewer_bJwk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981753227, "cdate": 1761981753227, "tmdate": 1762943466834, "mdate": 1762943466834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}