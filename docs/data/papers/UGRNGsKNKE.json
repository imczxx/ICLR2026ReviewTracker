{"id": "UGRNGsKNKE", "number": 1452, "cdate": 1756883598069, "mdate": 1763360953134, "content": {"title": "CAMoE: Cost-Aware Communication Optimization for Mixture-of-Experts Inference", "abstract": "Mixture-of-Experts (MoE) is currently the most promising method for scaling the parameters of large language models. Its architecture consists of different experts at different layers, with a fixed number of top experts selected dynamically for each token based on the token’s information during inference. Ideally, if all experts could be placed on the same device, token routing would not be impeded by communication overhead. However, as the parameters of MoE models grow toward trillion-scale, experts cannot be accommodated on a single device or even a single node, leading to significantly increased tail latency during all-to-all communications—the tokens with the highest communication cost slow down the inference process.\n\nIn this paper, we thoroughly analyze the patterns of all-to-all communications during inference in MoE models and develop a profiler to measure heterogeneity between devices. Using parameters obtained from profiler runs, we implement a SystemC-based simulator to model communication times during all-to-all communications. Based on detailed information about transmitted data, we propose a cost-aware method designed to reduce tail latency during model inference. Experimental results demonstrate that this method does not affect model accuracy on downstream tasks and effectively reduces all-to-all communication time during inference.", "tldr": "We propose CAMoE, a cost-aware method for reducing all-to-all communication latency during inference of large-scale Mixture-of-Experts (MoE) models.", "keywords": ["LLM Inference", "Mixture-of-Experts"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6ea3fd5c40fc4b37687579d42a4e9df6b34c0448.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses inference-time latency in Mixture-of-Experts models that employ expert parallelism (each expert MLP slice is hosted on a separate device). This parallelism strategy introduces significant communication overhead: each MoE layer requires an All-to-All dispatch (to send token embeddings to their selected experts) and an All-to-All combine (to gather processed outputs back). The dispatch phase, in particular, contributes to high latency. Since the total all-to-all latency depends on the slowest pair, specifically To mitigate this, the authors propose a profiler-based approach that measures per-link communication characteristics (, ) across GPUs and incorporates these estimates into the routing bias—so that the gate function implicitly prefers experts located on faster communication paths. In essence, the router becomes communication-aware, balancing accuracy against network cost. Experiments on the Qwen-30B MoE model show modest latency reductions without noticeable accuracy degradation.\n\nDespite that I very much like the idea of making router communication aware, I think the paper needs more work (positioning, writing, experiments and baselines) to be accepted. The paper is very difficult to follow and the experiment section is very limited (see weakness). I also have some question regarding the evaluation setup (see questions).\n\nI would encourage authors to work in this direction, however I think the paper is not ready yet to be published."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The problem the paper addresses is real and well-documented in prior literature. I also find the core idea of making the routing mechanism communication-aware interesting and conceptually valuable."}, "weaknesses": {"value": "The overall positioning and writing style of the paper raise several disagreements with how the authors frame and present their findings:\n\nFor example, the authors state (Section 2):\n\n> However, a critical and often-overlooked challenge in the real-world deployment of modern multiGPU servers is the extreme heterogeneity of intra-node communication. While it is commonly assumed that inter-GPU communication within the same server is homogeneous and efficient, our empirical measurements reveal a starkly different reality. \n\nI don't think this is a correct statement, people who operate large-scale GPU clusters do not actually assume that intra-node communication is homogeneous, unless the system indeed uses a uniform interconnect. Calling this an “overlooked challenge” is therefore misleading. If the authors believe otherwise, they should support this claim with references or survey evidence showing that this misconception exists in practice.\n\n> As shown in Figure3, depending on the physical interconnect topology between a pair of GPUs, their pointto-point write latency can differ by orders of magnitude. \n\nThis is not a surprising empirical discovery, it reproduces well-known and well-documented hardware behavior. A simple reference to NCCL documentation or NVIDIA’s public materials would be sufficient. The same comment applies to Figure 5.\n\nSubsection 2.2 spends considerable space re-stating a well-known hardware fact: that latency depends on the type of interconnect and that All-to-All communication is limited by the slowest peer. This discussion feels redundant—particularly since the main Algorithm 1 is already placed in the Appendix. I would recommend that the authors either remove this paragraph entirely or condense it significantly. Given that the proposed algorithm itself does not explicitly differentiate between intra- and inter-node communication, the space would be better used to provide a clearer and more detailed description of the proposed mathematical communication model.\n\n2. Lack of baselines and experiments\n\nAfter reading the paper, I understand that the main contribution is its applicability to heterogeneous hardware setups. In the Related Work section (Appendix), the authors state:\n>However, these systems still assume relatively stable or offline-optimized topologies. Our work differs by profiling heterogeneous links at inference time and incorporating a lightweight, cost-aware gating bias without retraining.\n\nHowever, I found multiple prior works that also optimize for heterogeneous clusters, which are not mentioned or compared against in this paper. For example, https://arxiv.org/abs/2503.04398 (see Subsection 5.1(B)) presents a relevant approach. I would strongly recommend that the authors include comparisons to such works or clearly justify why they are not directly relevant.\n\nIn the introduction, the authors emphasize the importance of handling heterogeneous intra-node communication, so I expected the experiments to demonstrate that the proposed profiler is effective under a range of connectivity types. However, in Appendix A.3 the provided configuration appears to focus on heterogeneous inter-node communication instead. It is not clear to me why this particular setup was chosen.\n\nMore specifically, the reported latencies are confusing: for example, the latency from node 3 to node 4 is given as ~70 microseconds, while the latency from node 1 to node 2 is given as ~65 milliseconds. This is a difference of roughly three orders of magnitude, and the paper never explains why this disparity exists, whether it is intentional, or whether it reflects real hardware. I think the paper would benefit from a clearer description of the hardware topology, how these latency numbers were obtained, and why these particular node pairs were chosen.\n\nRelated to that, it would also strengthen the work to vary the communication latency across nodes in a controlled way. Right now, having one link that is ~100× faster than the others feels a bit ad hoc; if this is actually representative of real clusters, a citation or justification would make that much more convincing.\n\n---\nSmall corrections:\n\n* line 33, 223: \\cite instead of \\citep? \n* line 205: space is missing\n* Figure 4 takes considerable amount of space, but mentioned only one time in the paper and looks redundant"}, "questions": {"value": "1. > By reusing the parallel state abstraction provided by Megatron-core, we can invoke built-in alltoall primitives without modifying the core framework code.\n\nCould you elaborate on what was actually changed or configured?\n\n2. It is not clear how much overhead running the profiler contributed to the overall latency. Can authors provide details? \n3. No warmups/repetitions:\n> We estimate (α_{uv}, β_{uv}) with no warmups or repeated trials.\n\nDid the authors observe differences when warmups or repetitions were included?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5IUVlZlFGW", "forum": "UGRNGsKNKE", "replyto": "UGRNGsKNKE", "signatures": ["ICLR.cc/2026/Conference/Submission1452/Reviewer_w3MD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1452/Reviewer_w3MD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760654324083, "cdate": 1760654324083, "tmdate": 1762915773473, "mdate": 1762915773473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you for all reviewers‘ comments"}}, "id": "BBBgdzYxNl", "forum": "UGRNGsKNKE", "replyto": "UGRNGsKNKE", "signatures": ["ICLR.cc/2026/Conference/Submission1452/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1452/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763360952345, "cdate": 1763360952345, "tmdate": 1763360952345, "mdate": 1763360952345, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a peculiar but crucial topic, i.e, how to implement efficient all-to-all communication considering physical configurations during MoE model inference. The communication bandwidth across different GPUs or nodes is typically the main bottleneck in the serving of MoE-based LLMs with expert parallelism, and CAMoE tries to alleviate this issue by injecting a lightweight, topology-aware bias into the router’s gating decisions during inference, steering tokens away from high-cost communication paths without retraining. The key contributions of this paper include:\n1. CAMoE Method: A drop-in, inference-time routing bias that incorporates communication cost into expert selection, reducing both mean and tail latency of All-to-All communication.\n2. System Modeling and Profiling: A two-phase profiler and $\\alpha-\\beta$ communication model to estimate per-link latency and congestion, enabling accurate cost-aware routing.\n3. Practical Integration: Seamlessly integrates with existing MoE systems (e.g., Megatron-LM), supporting various parallelism strategies without modifying model parameters or training.\n4. Empirical Results: On the Qwen3-30B-A3B model, CAMoE reduces mean All-to-All latency by up to 15.8% and p95 latency by up to 19.1%, with minimal impact on downstream task accuracy.\n5. Open-Source Toolkit: Provides a lightweight profiling and simulation toolkit for cost-aware MoE routing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper develops a cost-aware method designed to reduce tail latency during model inference, which does not affect model accuracy on downstream tasks and effectively reduces all-to-all communication time during inference.\n1. The topic of your work is quite interesting and practical, targeting a bottleneck problem in LLM serving.\n2. CAMoE is training-free and does not significantly harm the model performance on downstream tasks.\n3. The core design of CAMoE is simple and effective, which can be easily integrated into Megatron or other frameworks."}, "weaknesses": {"value": "There are some key drawbacks in this submission, including:\n1. Figure 1 may not be accurate and precise because\n    - You did not provide the detailed configuration of this profiling figure. Since this paper is targeted at inference, you should state whether this figure is derived from the prefilling or decoding stage, because in the prefilling stage, the communication burden of the MoE layer can be significantly higher than in the decoding stage. \n    - You should specify (1) which model you are examining, (2) which stage you are testing, (3) the batch size, and (4) the context length.\n2. In Figure 1, the all-to-all dispatch time is significantly higher than the all-to-all combine. However, this can be confusing, as the computation, memory movement, and communication patterns of these two operations are symmetric, and the computation burden here is very small, allowing for well-parallelized execution with specialized CUDA (in Tutel) or Triton (in MegaBlocks) kernels.\n3. You forgot to cite some papers with a very similar topic or method, including Auxiliary-loss free load balancing (DeepSeekAI), Occult (ICML2025), and MegaBlocks (MLSys 2023).\n4. Some of the concepts in this paper are not clarified, e.g, row-wise z-score and direction-specific $\\alpha-\\beta$ model. The explanation of them should also be provided.\n5. More MoE-based LLMs should be investigated because they typically adopt different MoE strategies. For example, Mixtral-8x7B uses heavy individual experts and top-2 routing among 8 total experts. DeepSeek-V2-Lite and Moonlight-16B-A3B utilize lightweight individual experts and employ top-6 routing among 64 total experts, with shared experts also introduced. These models have different MoE patterns, and the inference strategies are slightly different from the Qwen3-30B-A3B model you used in this paper.\n6. Just a reminder: Megatron is an LLM training framework. In industrial applications such as LLM serving, it is vLLM and SGLang that are typically preferred as the inference engine. But considering vLLM or SGLang are not as customizable as Megatron, it is also reasonable to use them in your paper."}, "questions": {"value": "You should distinguish between the prefilling and decoding stages in the efficiency evaluation experiments, along with other detailed configurations on workload, such as batch size and context length."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8hoLCPxSua", "forum": "UGRNGsKNKE", "replyto": "UGRNGsKNKE", "signatures": ["ICLR.cc/2026/Conference/Submission1452/Reviewer_y8qw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1452/Reviewer_y8qw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919837976, "cdate": 1761919837976, "tmdate": 1762915773150, "mdate": 1762915773150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAMoE, a training-free framework designed to mitigate the All-to-All communication bottleneck in large, multi-device MoE model inference. The core problem is that standard MoE routers are oblivious to the underlying hardware topology, often routing tokens over slow, high-latency network paths, which creates performance stragglers and increases tail latency. The proposed method addresses this by first profiling the communication costs between devices to create a cost model. It then injects a simple, cost-aware bias directly into the router's logits at inference time, discouraging the selection of experts located on high-cost paths. The authors evaluate this on the Qwen3-30B-A3B MoE model, showing that it can reduce All-to-All latency by up to 19.1% with only a minor impact on downstream task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a critical and practical bottleneck in the deployment of very large MoE models.\n- The authors provide a clear and compelling motivation for their work, including an excellent empirical analysis of the heterogeneity of communication costs in modern multi-GPU servers.\n- The proposed method is simple to implement at inference time, boiling down to a single gather-and-add operation before the top-k selection."}, "weaknesses": {"value": "**Critically Limited Experimental Scope:** The paper's biggest weakness is its empirical evaluation. The method is evaluated on only a single MoE model (Qwen3-30B-A3B) and at a small scale (4 GPUs). Furthermore, there are no comparisons to any alternative methods. The baseline is simply CAMoE with the cost parameter set to zero. This is insufficient to demonstrate the general applicability or competitiveness of the approach. The authors must expand their evaluation. This should include: \n1) At least one other MoE model to show the method is not specific to one architecture. \n2) Comparisons to relevant baselines, such as other topology-aware routing or expert placement methods (e.g., NetMoE, MoETuner), to properly contextualize the performance. Even a comparison to a simple, non-trained heuristic (e.g., prefer experts on the same node) would be informative. \n3) Evaluation at a larger scale (e.g., 8 or 16 GPUs across multiple nodes) to validate the approach in a more realistic deployment scenario.\n\n\n**Lack of Clarity and Readability:** The paper is not easy to follow. The overall pipeline connecting the profiler, simulator, and the final inference-time biasing is not explained cohesively. In addition, some of the terms are not defined in the main text and moved to Appendix without a reference in the main sections. E.g., cv in Section 4.5, which is a Coefficient of Variation, is mentioned and described only in Appendix A.4 without a link in the text. It would be better if it the main text would refer a reader to the corresponding section. \n\n\nIn its current form, the paper presents an interesting proof-of-concept but lacks the rigorous and comprehensive evaluation required for ICLR. It would need to significantly expand its experimental evaluation to be a reasonable candidate."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XCDuT6RSOy", "forum": "UGRNGsKNKE", "replyto": "UGRNGsKNKE", "signatures": ["ICLR.cc/2026/Conference/Submission1452/Reviewer_RLFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1452/Reviewer_RLFY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943013364, "cdate": 1761943013364, "tmdate": 1762915773026, "mdate": 1762915773026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CAMoE is a drop-in router-side tweak that aims squarely at tail latency, which dominates MoE inference and is bottlenecked by the slowest receiver. The aim is to cut MoE All-to-All communication latency by steering the router’s top-k expert choices toward lower-cost links. Concretely: \n\n1. A lightweight two-phase profiler measures per-direction α–β link parameters over heterogeneous paths. \n\n2. Using measured (α,β), the authors build a traffic-weighted time proxy for each source-endpoint - expert-endpoint pair and row-wise z-score it, producing a topology-aware bias that is added to the router logits before normal top-k selection. \n\n3. The communication model and All-to-All timing are formalized, and the α–β fits are validated against microbenchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a training-free generalizable method; no changes to collectives or retraining.\n\n2. The detailed method is lightweight profiling and direction-specific α–β modeling with a neat two-phase procedure. The code integration is lightweight, with minimal code change (about 1.5k lines of Python code).\n\n3. The results demonstrate clear monotonic latency reductions and good performance."}, "weaknesses": {"value": "1. Experiments only run on 4× Ada 6000 with emulated network shaping. The validity of production environments is not demonstrated.\n\n2.  No full E2E latency/throughput vs. batch/seq-len or cost-per-token is collected. Lack baseline with placement or topology-aware systems (e.g., NetMoE ICLR'25) and a simpler heuristics-based baseline. \n\n3. Some tasks' performance is degraded in Table 1. The author should provide detailed reasons and analysis.\n\n4. It's not clear how to online adapt \\lambda_{\\text{cost}} and refreshing (α,β).\n\n5. Abstract references a SystemC-based simulator, but the main body of paper emphasizes Megatron-LM integration and α–β validation. The clarify the simulator’s role."}, "questions": {"value": "1. How do improvements in per-layer A2A translate to full-model latency/throughput across realistic batch sizes and sequence lengths? Can you report tokens/s and time-to-first-token?  \n2. How does CAMoE perform on multi-node, NVSwitch/NVLink clusters with dozens to hundreds of GPUs and real spine/leaf congestion? Any results on p99? \n3. Please compare against (a) placement-aware methods (Tutel, NetMoE, MoETuner) and (b) simpler biases (e.g., hop count / static bandwidth map) to isolate where α–β modeling adds value.   For tasks that regress at high \\lambda_{\\text{cost}} (PIQA, HellaSwag, GSM8K), do you observe changes in expert utilization/specialization (e.g., entropy of expert assignments, cross-layer affinity)? Can a per-layer \\lambda_{\\text{cost}} mitigate this? \n4. What is the overhead and stability of an online controller that tunes \\lambda_{\\text{cost}} and refreshes a subset of (α,β) under drifting congestion? Any risk of oscillation or thrashing? How does the bias interact with capacity factors, token-drop, top-k choices (k=1 vs 2), and expert parallel group size?\n5. Simulator vs. measurements: The abstract mentions a SystemC-based simulator; how is it used relative to the Megatron-based profiling/validation reported in Section 4? Can you reconcile and release both? What is the runtime overhead of the gather-and-add bias table lookup and the one-shot accounting pass? Can the profiler run in the background without disturbing SLA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P7j2aCBOMb", "forum": "UGRNGsKNKE", "replyto": "UGRNGsKNKE", "signatures": ["ICLR.cc/2026/Conference/Submission1452/Reviewer_mYre"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1452/Reviewer_mYre"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048053903, "cdate": 1762048053903, "tmdate": 1762915772850, "mdate": 1762915772850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}