{"id": "KR1ktPNJkC", "number": 13542, "cdate": 1758219076960, "mdate": 1762958433262, "content": {"title": "ReCLLaMA: A Reasoning-Centered LLM Agent for Medical Diagnosis", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language understanding, yet their application to clinical diagnosis remains constrained by hallucinations, limited interpretability, and the absence of formal reasoning mechanisms. To address these limitations, we propose ReCLLaMA, a Reasoning-Centered LLM Agent for Medical Diagnosis, which integrates statistical language models with symbolic inference over structured medical knowledge. ReCLLaMA aligns free-text symptom descriptions with standardized ontologies using pretrained biomedical encoders and performs logical reasoning over heterogeneous knowledge graphs constructed from EHR and pharmacological data. To reconcile representational mismatches across sources, we introduce a statistical entity alignment module based on random forest classification. This enables the construction of a unified knowledge space in which ReCLLaMA applies both deductive and abductive reasoning to derive interpretable diagnostic pathways. Our framework advances the theoretical integration of subsymbolic and symbolic AI in clinical contexts, offering a principled approach to traceable, knowledge-grounded decision-making. Empirical results on real-world datasets validate its superiority over black-box LLMs and rule-based systems in both accuracy and explainability.", "tldr": "", "keywords": ["Large Language Model", "Explainable AI Agent", "Knowledge Graph Reasoning", "Deduction/Abduction Inference", "Medical Diagnosis"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7e5c79960a4f196ee2a30167d5a9bdd06c8d8a50.pdf", "supplementary_material": "/attachment/2fb24268141cc2608a849585b87d3bd14811986b.zip"}, "replies": [{"content": {"summary": {"value": "This paper propose ReCLLaMA, which is a clinical agent designed to improve medical diagnosis by combining LLMs with symbolic reasoning. It first uses an LLM to understand a free-text medical descriptions and convert them into standardized medical elements. Then, its core innovation lies in using a symbolic reasoning engine (NARS) over biomedical knowledge graphs to logically infer diagnoses. This hybrid approach aims to provide accurate, interpretable, and evidence-based diagnostic hypotheses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is easy to follow, with the components of the agent pipeline clearly described. This modular breakdown enhances reproducibility and understanding of the system's workflow.\n\n2. The work addresses a critically important task: providing interpretable medical reasoning and diagnosis. This focus on explainability is essential for building trust and facilitating the adoption of AI in clinical decision-making."}, "weaknesses": {"value": "1. Lack of Novelty. The core premise of the paper is to combine LLMs with symbolic reasoning over knowledge graphs (KGs). However, the architectural design assembles existing components, and demonstrates a lack of innovation. \n\n- LLM as a Mere Front-End: The LLM is used as a simple pre-processing role, i.e., parsing natural language queries. This usage of LLMs as an \"interface\" to structured databases is now a standard and well-established practice and does not constitute a novel contribution. \n\n- Conventional Knowledge Graph Reasoning: The \"reasoning\" part of the system is handled by a traditional symbolic reasoning engine (i.e., the Non-Axiomatic Reasoning System, NARS 2013) operating on a pre-existing knowledge graph. The NARS is based on work from 2013, and KG-based reasoning for medical diagnosis is a mature field with extensive prior works. \n\n- A \"Pipelined\" Assembly, rather than an Integrated Solution: The work essentially chains together an LLM front-end with an reasoning backend. The primary contribution appears to be the engineering of this pipeline. The combination itself does not seem to produce emergent capabilities that neither component could achieve independently. I would expect more innovative integration than improves the reasoning capability of LLMs with KG.  \n\n2. About the Experimental Design and Results \n- Insufficient Test Set Size: The experiments are conducts on mimic 3, while mimic 4 is released. Furthermore, the key diagnostic accuracy are conducting on only 100 held-out cases (e.g., Table 2). Such a small sample size cannot reliably measure performance, generalize findings, or detect improvements over baselines. \n\n- Unexplained Data Selection: The paper fails to detail how these 100 test cases were selected. \n\n- Low Performance Metrics: The results reported in Table 2 are low. Key metrics like Precision, Recall, and F1-score are near zero (0.0100, 0.0033, and 0.0050, respectively). Furthermore, several ablated model variants in Table 3 report scores of 0.0000. This may indicate that the system performs very poorly. \n\n- Weakened Baselines: The choice of baselines (e.g., \"MDAgent,\" \"KG-CoT\") are not represent the state-of-the-art. A more compelling comparison would be against more strong baselines, fine-tuned LLM or other well-established clinical decision support system."}, "questions": {"value": "Please refer to the questions raised in the \"Experimental Design\" part in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rAyEFSSy3b", "forum": "KR1ktPNJkC", "replyto": "KR1ktPNJkC", "signatures": ["ICLR.cc/2026/Conference/Submission13542/Reviewer_yuwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13542/Reviewer_yuwe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701579731, "cdate": 1761701579731, "tmdate": 1762924141800, "mdate": 1762924141800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "pn7Hov7KB7", "forum": "KR1ktPNJkC", "replyto": "KR1ktPNJkC", "signatures": ["ICLR.cc/2026/Conference/Submission13542/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13542/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762958431993, "cdate": 1762958431993, "tmdate": 1762958431993, "mdate": 1762958431993, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission introduces ReCLLaMA, a modular, Reasoning-Centered LLM Agent designed for interpretable and accurate medical diagnosis by explicitly integrating subsymbolic and symbolic AI techniques. This agent processes free-text symptom queries using a BioBERT-LLM hybrid for entity extraction, followed by a statistical alignment step using Random Forest over embedded entities to unify heterogeneous biomedical knowledge graphs (EHR and Drug KGs). Central to the framework is a symbolic reasoning engine built upon Non-Axiomatic Logic (NAL), which performs traceable deductive and abductive inference under uncertainty to derive ICD-9 diagnostic hypotheses, culminating in LLM-generated, patient-friendly explanations grounded in KG traces. Empirical results on MIMIC-III and Oregano KG data demonstrate that the system achieves high diagnostic accuracy and calibrated confidence superior to comparable LLM and KG-CoT baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework offers an original, theoretically grounded integration of subsymbolic LLMs and symbolic Non-Axiomatic Logic for clinical decision support.\n2. Modularity in the design, encompassing extraction, alignment, reasoning, and translation, enhances both clarity and engineering robustness.\n3. The reasoning component successfully mitigates LLM hallucination by explicitly grounding diagnostic pathways in structured knowledge graph traces.\n4. Empirical evaluation demonstrates superior performance in structured diagnostic accuracy and confidence calibration compared to strong baselines."}, "weaknesses": {"value": "1. The statistical knowledge alignment backbone, relying on Word2Vec embeddings and Random Forest, appears relatively simple compared to modern Graph Neural Network alignment methods.\n2. The claimed full integration of the abductive, deductive, and inductive cycle lacks explicit demonstration of the iterative rule discovery or hypothesis revision stage.\n3. Quantitative results demonstrating diagnostic superiority are based on a relatively small cohort of 100 test cases, potentially limiting the statistical significance of the claims.\n4. The reasoning system's reliance on fixed background rules K1 and K2 suggests limited ability for autonomous knowledge discovery or adaptation within the NAL framework.\n5. Detailed results for the knowledge alignment module are reported on a synthetic pairwise symptom-protein dataset which may not accurately reflect real-world KG linking difficulty.\n6. Specific ICD-9 codes and the choice of the Oregano KG may constrain the immediate applicability and update burden of the system in evolving clinical environments.\n7. The ablation study (Table 3) shows that removing the Random Forest alignment backbone drastically reduces performance to zero, suggesting this statistical component may be overly relied upon."}, "questions": {"value": "1. Authors should clarify exactly how the framework implements the inductive phase of the Peirceian reasoning cycle, specifically addressing how rules (like K1 or K2) are actually discovered or revised based on observations.\n2. Could the authors provide latency benchmarks for the Module IV symbolic reasoning component, especially for complex multi-step deductions, and discuss its scalability for real-time deployment?\n3. Beyond the concatenated Word2Vec embeddings, please list and discuss any other features used by the Random Forest classifier in Module III to predict the validity of biomedical relations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fdla0aJopm", "forum": "KR1ktPNJkC", "replyto": "KR1ktPNJkC", "signatures": ["ICLR.cc/2026/Conference/Submission13542/Reviewer_cEkJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13542/Reviewer_cEkJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872791737, "cdate": 1761872791737, "tmdate": 1762924141503, "mdate": 1762924141503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReCLLaMA, a \"Reasoning-Centered LLM Agent for Medical Diagnosis.\" The authors aim to address the limitations of Large Language Models (LLMs) in clinical diagnosis, specifically their tendency for hallucination and lack of interpretable reasoning. The proposed framework consist of 5 modules: user interface, knowledge extraction, knowledge alignment, knowledge reasoning, and knowledge translation. The authors conduct experiments on each module to evaluate the effectiveness of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is easy to follow and understand.\n2. The paper attempts to solve the correct and critical problems in medical AI: hallucination, lack of interpretability, and the need for formal, grounded reasoning.\n3. The 5-module design (Extraction, Alignment, Reasoning, etc.) is a logical way to separate concerns."}, "weaknesses": {"value": "1. The complete evaluation system in the paper is very confusing. There are no detailed descriptions of how the evaluations were done and what the meanings of the evaluation results are. The results in the Table 2 and 3 are basically all 0 or near-zero. I don't see any reliable results can be drawn from the results like these. And the same for the results in table 4, where all numbers are almost 100 for both the proposed method and compared baselines.\n2. The use of Word2Vec and Random Forest for a critical alignment task is a weak methodological choice, where these 2 methods are very out-dated and there are no clear indications of why no more recent methods are used or compared.\n3. The \"reasoning-centered\" part of the agent relies entirely on applying NARS, an existing external logic system. The novelty of the framework is very limited."}, "questions": {"value": "1. How can the paper claim 92.8% accuracy for extraction, ~99% for alignment, and 81% for reasoning, yet the final end-to-end system in Table 2 achieves only 5% \"Any-Hit\" accuracy? This implies a catastrophic failure at the interfaces between modules. Where exactly is the system breaking down?\n2. he ablation in Table 3 shows that removing any component (CE, RF, or Reasoner) results in a score of 0.0000 across all metrics. This seems statistically extreme. Does this mean not a single test case could be correctly processed if even one module was changed? Can you explain this total pipeline collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MpBEXqAlgR", "forum": "KR1ktPNJkC", "replyto": "KR1ktPNJkC", "signatures": ["ICLR.cc/2026/Conference/Submission13542/Reviewer_xHmv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13542/Reviewer_xHmv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152873421, "cdate": 1762152873421, "tmdate": 1762924141023, "mdate": 1762924141023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}