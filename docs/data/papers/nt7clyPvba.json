{"id": "nt7clyPvba", "number": 11873, "cdate": 1758204411635, "mdate": 1759897549523, "content": {"title": "Moment Matters: Mean and Variance Causal Graph Discovery from Heteroscedastic Data", "abstract": "This paper proposes a Bayesian causal discovery approach \n  to uncover the causal mechanisms \n  underlying heteroscedasticity, \n  where the variance of one variable is influenced \n  by the values of the others.\n  To distinguish between the causes that affect the mean\n  and those that influence the variance,\n  we infer the posterior distribution over \n*mean* and *variance causal graphs*,\n  whose structures can be different, \n  depending on the moment information.\n  We establish identifiability conditions for these causal graphs \n  by extending the results on heteroscedastic noise models (HNMs).\n  Building on these conditions,\n  we develop a variational inference framework that can \n  incorporate prior knowledge about \n  the node orderings of the underlying graphs.\n  We experimentally show that our method can successfully infer both mean and variance causal graphs,\n  outperforming the state-of-the-art baselines.", "tldr": "We propose a moment-driven causal discovery framework that can identify the cause variables that affect the mean and those that influence the variance, leading to a better understanding of complex real-world phenomena.", "keywords": ["causal discovery; structural causal models; heteroscedastic noise models"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e3c89d638e670d5becf88e1f1d220405d8675fac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new causal discovery framework that distinguishes between causes affecting the (conditional) mean and those affecting the (conditional) variance of a variable. It introduces the concept of mean and variance causal graphs, extending existing heteroscedastic noise models (HNMs). The authors prove identifiability conditions for these graphs and develop a Bayesian variational inference method that infers their posterior distributions from observational data. Empirical results on synthetic, semi-synthetic, and real datasets show competitive performance compared to prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Bayesian inference with uncertainty quantification is very important, not only for this specific case of discovery in heteroscedastic noise models, but generally to all causal discovery methods. Unlike point-estimation methods, such full posterior estimation gives more robust results especially in small data regimes.\n\n2. Theoretical results and the whole paper progression is generally well structured. The assumptions are stated clearly and used transparently in the derivation of identifiability conditions."}, "weaknesses": {"value": "1. The motivation for separating the mean graph and the variance graph is still unclear to me. While the authors argue for the importance of distinguishing between causes of mean and variance (e.g., in drug design or economic variability), the practical necessity of _explicitly modeling two separate causal graphs_ remains somewhat unclear. Intuitively, one could always first estimate a moment-agnostic causal graph and then use flexible (e.g., nonparametric) regression to analyze how each parent affects the target variable (referred to as \"double dipping\" in this paper). The necessity could be made more precise and convincing.\n\n2. More literature review and comparison to other HNM models and methods are needed, especially since this is a relatively new area. The authors show two main advantages of their model comparing to others (separating, and Bayesian estimation). But for a balanced comparison, what are the other aspects that other methods may be able to address but not this one? In particular,\n\n   - Are there existing models (e.g., beyond HNM) that allow more general functional forms, such as multiplicative or non-additive noise structures (instead of multiplier only posed on the exogenous noise)?\n\n   - Do any existing methods relax the Gaussian noise assumption?\n\n\n3. More elaboration about the identifiability conditions are needed. Specifically,\n\n   - Are the stated conditions only sufficient, or are they also necessary for identifiability?\n\n   - What happens when the conditions are violated? For instance, could the authors give simple concrete examples where identifiability fails (e.g., two models with different graphs but same distribution), to illustrate the role of nonlinearity or piecewise variance functions?\n\n4. Minors:\n\n   - The title “Moment Matters” may be slightly misleading to readers expecting techniques involving higher-order moments (e.g., skewness, kurtosis), when in fact the method focuses on first and second moments only (mean and variance). May consider rephrasing or clarifying this early in the introduction.\n\n   - Assumption 3.2 (causal minimality) can be explained more in details in the main body, especially to emphasize that it is weaker than the standard faithfulness assumption."}, "questions": {"value": "see \"weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f3DvLuJumd", "forum": "nt7clyPvba", "replyto": "nt7clyPvba", "signatures": ["ICLR.cc/2026/Conference/Submission11873/Reviewer_LbVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11873/Reviewer_LbVq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760672166245, "cdate": 1760672166245, "tmdate": 1762922891829, "mdate": 1762922891829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Bayesian causal discovery approach that explicitly disentangles and recovers mean and variance causal graphs. Building on heteroscedastic noise models (HNMs), the authors establish identifiability conditions for mean and variance causal graphs and propose a variational inference approach that enables uncertainty quantification. The method is evaluated on synthetic, semi-synthetic, and real-world biological datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach can qualify uncertainty, which is an improvement over most prior point estimation methods.\n\n2. The method allows domain knowledge to be flexibly incorporated into the learning process via a differentiable relaxation of the permutation matrix. This is important for practical small-sample applications.\n\n3. The authors provide comprehensive experimental results to validate the effectiveness of the proposed appoach."}, "weaknesses": {"value": "My main concern is that the novelty of this paper is somehow limited.\n\n- Theorem 1 in (Yin et al., 2024) has already established identifiability of HNM under three conditions. Theorem 3.5 in this paper is very similar to Theorem 1 in (Yin et al., 2024) and the proof of the former relies heavily on the latter.\n- While the exploration on mean/variance graph separation is well-motivated, the variational Bayesian treatment, use of Gumbel-Softmax relaxations, and exploitation of domain knowledge via permutation matrix regularization are closely related to existing Bayesian DAG learning frameworks such as DDS (Charpentier et al., 2022), MC3 (Giudici and Castelo, 2003), BayesIMP [1], and BayesDAG [2]. The transition to mean/variance-specific graphs is meaningful, but primarily an extension rather than a conceptual breakthrough.\n\n[1] Bayesimp: Uncertainty quantification for causal data fusion. NeurIPS 2021\n\n[2] Bayesdag: Gradient-based posterior inference for causal discovery. NeurIPS 2023."}, "questions": {"value": "There is a typo in line 179: \"if $\\pi (i) < \\pi (j)$, then $X_{\\pi (j)}$ cannot have a directed path to $X_{\\pi (i)}$\", it seems that $X_{\\pi (j)}$ should be \"$X_ j$ cannot have a directed path to $X_ i$\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EhHspTb0E1", "forum": "nt7clyPvba", "replyto": "nt7clyPvba", "signatures": ["ICLR.cc/2026/Conference/Submission11873/Reviewer_gKQK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11873/Reviewer_gKQK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485222297, "cdate": 1761485222297, "tmdate": 1762922891420, "mdate": 1762922891420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a mean-variance heteroscedastic noise model (HNM) that separates causal influences on a variable’s mean and variance via two graphs, $G^M$ and $G^V$, assumed to share a topological order. This model is a reparameterization of the original HNM that allows for more interpretable causal discovery. The authors prove identifiability of both graphs under standard HNM assumptions plus the shared ordering constraint, and develop a variational inference method using Gumbel-Softmax and SoftSort to learn both graphs jointly. Experiments on synthetic and real datasets show the method can uncover causal links missed by standard approaches, especially variance-only effects."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "a. The paper establishes identifiability of dual causal graphs ($G_M$ and $G_V$) under clear conditions. The proof extends existing heteroscedastic causal discovery theory (e.g. Yin et al., 2024) to show that one can recover not just the overall DAG structure but also which edges belong to mean vs. variance relationships, given a shared ordering.\n\nb. It proposes a Bayesian variational approach to infer two linked DAGs simultaneously. The formulation cleverly uses a shared permutation (ordering) for both graphs and generalizes differentiable DAG sampling (DDS) to handle two adjacency matrices. Techniques like Gumbel-Softmax and SoftSort are employed to maintain differentiability, which is an innovative extension of prior continuous DAG optimization methods.\n\nc. The framework naturally incorporates prior knowledge (e.g. known partial ordering of nodes) into the inference procedure. This is valuable for real applications where domain knowledge about causal ordering exists and can guide the search."}, "weaknesses": {"value": "a. The identifiability and method rely on the assumption that the mean and variance causal graphs share a single topological ordering. In practice, this means no cause-effect relationship flips between mean and variance graphs (an edge present in one cannot appear reversed in the other). It might be too restrictive in some real systems. If the true causal mechanism violates this shared order (e.g. a variable influences another’s variance but is downstream in mean effects), the current approach may struggle or require the user to know and enforce the correct ordering upfront.\n\nb. Like standard HNM, the theory requires Gaussian noise and specific nonlinear forms (mean functions must be nonlinear; variance functions non-constant piecewise). These assumptions are crucial for identifiability but limit generality.\n\nc. It seems that most distributions under this two-graph model could also be captured by a standard single-graph HNM, with appropriate functional form (mask). The mean-variance HNM, while more interpretable, does not expand the class of distributions that can be represented compared to the original HNM."}, "questions": {"value": "a. How critical is the Gaussian noise assumption in practice? Could the method be adapted for non-Gaussian heteroscedastic noise in a way that still yields at least partial identifiability?\n\nb. The approach assumes a shared causal order. If this assumption is mildly violated in reality (for instance, one edge slightly conflicts between mean and variance ordering), how robust is the inference?\n\nc. Given the two-graph sampling scheme, what are the practical limits on the number of nodes $d$ the method can handle? Have the authors considered any heuristics or structure in the permutation search (besides simple priors) to improve scalability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PD6GHbzoob", "forum": "nt7clyPvba", "replyto": "nt7clyPvba", "signatures": ["ICLR.cc/2026/Conference/Submission11873/Reviewer_eWXa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11873/Reviewer_eWXa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548023415, "cdate": 1761548023415, "tmdate": 1762922891083, "mdate": 1762922891083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Bayesian causal discovery method for mean-variance hierarchical noise model causal graphs: where the mean and variance of a node depends on its parents. Crucially, the mean and variance graphs can be different.\n\nThe authors provide an identifiability result and provide a variational method for recovering an approximate posterior using neural networks to approximate the mean and variance functions. Some improvements in the implementation are introduced to make the optimization more tractable, and the method is compared against several competing Bayesian methods on a variety of data sets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The identifiability result is, as far as I can tell, correct and nicely motivated.\n+ The parameterization is well-structured and clearly chosen in a way that makes the variational inference tractable.\n+ The implementation tricks such as the two-phase optimization and prior knowledge incorporation are novel applications to improve the performance of the approach.\n+ The empirical results are quite strong on these small datasets."}, "weaknesses": {"value": "+ I am not sure how effectively the proposed probabilistic model can actually capture the posterior of the mean-variance HNM. As I understand it, the mean and variance functions are represented by an MLP which is independent of $A^M$ and $A^V$ (except insofar as the edges are masked out with the adjacency matrices when applying the MLP). Therefore, the MLP would have to learn mean and variance functions that would be applicable when the underlying inputs correspond to different nodes. As a somewhat trivial example, if we had some posterior density on $X_1 \\rightarrow X_2$ and also on $X_1 \\leftarrow X_2$, then the MLP would have to be able to represent the mean function from $X_1$ to $X_2$, but also from $X_2$ to $X_1$. It's possible I have misunderstood this point, but it seems like this independence assumption could be quite limiting.\n\n+ The contribution of the prior knowledge incorporation is a bit unclear, since there's no baseline for the other methods. It's hard to tell if the improvement is just because of the fact that some of the edges are being specified to correctly exist/not exist."}, "questions": {"value": "+ Is it possible to adjust your method to condition the MLP on the permutation or adjacency matrix?\n+ Could you provide some small-dimensional (2,3,4)-node cases where the posterior can be calculated exactly or via direct sampling, and compare this approach to the exact posterior?\n+ Could you provide a comparison to the other methods when they are given the prior knowledge incorporation? You could for instance take the methods' solution and specify that an edge must exist in the solution when given the prior information.\n+ Did you experiment with different values of the temperature parameters $\\tau$? How does the solution accuracy/optimization smoothness trade-off?\n\ntypos: 'VARINACE' in table 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bLXEe7pDC0", "forum": "nt7clyPvba", "replyto": "nt7clyPvba", "signatures": ["ICLR.cc/2026/Conference/Submission11873/Reviewer_YN5K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11873/Reviewer_YN5K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941433770, "cdate": 1761941433770, "tmdate": 1762922890609, "mdate": 1762922890609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}