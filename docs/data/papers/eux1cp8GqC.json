{"id": "eux1cp8GqC", "number": 14029, "cdate": 1758227242564, "mdate": 1759897395019, "content": {"title": "Latent Veracity Inference for Identifying Errors in Stepwise Reasoning", "abstract": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. \nTo address this, we propose to augment each reasoning step in a CoT with a latent veracity (or correctness) variable.\nTo efficiently explore this expanded space, we introduce Veracity Search (VS), a discrete search algorithm over veracity assignments. It performs otherwise intractable inference in the posterior distribution over latent veracity values by leveraging the LM's joint likelihood over veracity and the final answer as a proxy reward.\nThis efficient inference-time verification method facilitates supervised fine-tuning of an Amortized Veracity Inference (AVI) machine by providing pseudo-labels for veracity. AVI generalizes VS, enabling accurate zero-shot veracity inference in novel contexts. Empirical results demonstrate that VS reliably identifies errors in logical (ProntoQA), mathematical (GSM8K), and commonsense (CommonsenseQA) reasoning benchmarks, with AVI achieving comparable zero-shot accuracy. Finally, we demonstrate the utility of latent veracity inference for providing feedback during self-correction and self-improvement.", "tldr": "We propose a search-based latent-variable inference method for identifying errors in the reasoning chains of language models.", "keywords": ["Latent variable models", "Language models", "Probabilistic inference", "Veracity", "Chain-of-thought"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a19ad151d1325ac5f10379d5f56e0fb27a33b9d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on identifying stepwise error in Chain of Thoughts (CoT) of language models. To address this problem, the paper formulates the problem as a latent variable model, and proposes two methods: veracity search (VS) and Amortized Veracity Inference (AVI) to solve the problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method provides significant improvement over the stepwise error detection performance. \n2. The inference time cost of the proposed AVI method is relatively low."}, "weaknesses": {"value": "1. The experiments are done with synthetic errors. I'm curious about how the proposed methods work for \"real\" errors in model's CoT. Can the AVI method detect and correct real CoT errors thus improve the reasoning ability?"}, "questions": {"value": "1. Can the empirical study in appendix C.6 be extended to other models/datasets? I think the hypothesis here is actually key reason why the proposed methods work: the model actually \"knows\" the error and assign higher probability to the joint distribution of answer and veracity, if it's closer to ground truth. Providing more empirical evidence on this will strongthen the conclusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "spxb12osiX", "forum": "eux1cp8GqC", "replyto": "eux1cp8GqC", "signatures": ["ICLR.cc/2026/Conference/Submission14029/Reviewer_xAHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14029/Reviewer_xAHw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776758770, "cdate": 1761776758770, "tmdate": 1762924518039, "mdate": 1762924518039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach to disentangle the content of a chain-of-thought (CoT) from its veracity/correctness. The authors introduce veracity search (VS), which is a metropolis search methodthat samples veracity assignments for reasoning steps using the LM’s joint likelihood as a reward, and amortized veracity inference (AVI), which fine-tunes an LM on pseudo-labels from VS to enable zero-shot veracity prediction. The idea of treating correctness as a latent variable is both intuitive and novel, and the experiments convincingly demonstrate gains over prompting-based verification baselines. The paper is well written and includes extensive experiments across logical, math, and commonsense reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Conceptual clarity and originality**: The paper disentangles reasoning content and correctness via a latent variable formulation. It provides a neat probabilistic framing of step-wise error identification in chain of thoughts.\n2. **veracity search is intuitive**: and effective inference-time algorithm. It  outperforms simple prompting-based verifiers across different benchmarks. \n3. **Comprehensive evaluation.** The paper is well written, and the experiments are well-organized, and includes detailed ablations (e.g., simulated annealing, greedy initialization, scalability with reasoning hops on prontoQA).."}, "weaknesses": {"value": "1. **Reliance on prompting LMs for veracity scoring:** The entire framework assumes that the LM can reliably evaluate joint likelihoods of veracity assignments, yet prior studies (e.g., Huang et al., 2023; Zhang et al., 2024) show that LMs are often poor self-verifiers, especially on real-world reasoning where correctness is tricky/subtle. Some discussion or empirical evidence of robustness on naturally occurring reasoning errors would strengthen the claims.\n\n2. **Lack of comparison to process reward models (PRMs).** There exist strong baselines such as PRMs (Lightman et al., 2024; Cobbe et al., 2021) that explicitly model step-level correctness and can perform veracity inference without search. This begs the question of why should VS/AVI be preferred over training or using a PRM?\nPRMs would likely achieve comparable or better results with far fewer LM forward passes and a clearer training signal.\n\n3. The search process requires up to 200 LM forward passes per example, making it orders of magnitude more expensive than single-pass prompting baselines so the comparison to these is unfair in my opinion. While AVI mitigates this at inference, the paper should discuss the practical feasibility and compute trade-offs, especially when compared to PRMs or trained verifiers.\n\n4. **Limited practical applicability:** Many of the ablations/analysis focus on synthetic or structured boolean reasoning over prontoQA. It remains unclear whether the method can handle open-ended or natural reasoning errors (e.g., in mathematical or real-world proofs). The negation-based correction strategy also seems tied to synthetic logic tasks."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1NiuQ4n4Wy", "forum": "eux1cp8GqC", "replyto": "eux1cp8GqC", "signatures": ["ICLR.cc/2026/Conference/Submission14029/Reviewer_zvRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14029/Reviewer_zvRp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939757234, "cdate": 1761939757234, "tmdate": 1762924517637, "mdate": 1762924517637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed augmenting each CoT step with a latent correctness variable and a search method, Veracity Search to infers step correctness by maximizing the language model’s joint likelihood of veracity and the final answer"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. strong performance gain\n2. label-efficient step verification, the proposed veracity search use LLM's joint likelihood, avoiding expensive step-level supervision."}, "weaknesses": {"value": "1. Most tests use artificially corrupted chains; evidence on naturally occurring errors is limited and needs broader experiments.\n2. The joint-likelihood reward correlates with true veracity but not perfectly (Pearson 0.56–0.74), so misrankings can occur.\n3. the computation efficiency can be improved, strong VS performance often use tens to 100 samples, this add-on computation cost compared with single-pass verifiers."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A1A5wLkwdN", "forum": "eux1cp8GqC", "replyto": "eux1cp8GqC", "signatures": ["ICLR.cc/2026/Conference/Submission14029/Reviewer_Qqpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14029/Reviewer_Qqpf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944605886, "cdate": 1761944605886, "tmdate": 1762924517093, "mdate": 1762924517093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem that chain-of-thought (CoT) reasoning in LLMs often contains flawed intermediate steps, which undermines both accuracy and interpretability. The authors propose to augment each reasoning step with a latent veracity variable indicating correctness. They introduce Veracity Search (VS), a discrete search algorithm that leverages the joint likelihood of veracity assignments and final answers to approximate posterior inference. To avoid reliance on ground-truth answers at test time, they further propose Amortized Veracity Inference (AVI), which is trained on pseudo-labels generated by VS and enables zero-shot veracity prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Originality: The methods proposed for improving the CoT are innovative.\n\n(2) Quality: The combination of VS and AVI is well-motivated, and experiments are carefully designed across logical, mathematical, and commonsense reasoning tasks.\n\n(3) Clarity: The paper provides clear definitions, and comprehensive experiments.\n\n(4) Significance: Identifying and correcting reasoning errors is an important challenge for improving the reliability of LMs, and this work provides a promising direction."}, "weaknesses": {"value": "(1) Many experiments rely on artificially corrupted reasoning chains. It would be valuable to see more extensive evaluation on naturally generated CoTs (this is the real-world use case).\n\n(2) The research on the impact of reasoning time is somewhat lacking.\n\n(3) The AVI is dependent on VS, but VS itself may not able to guarantee accuracy. This influence is not analyzed."}, "questions": {"value": "(1) How well does the method generalize to naturally occurring errors in CoTs, beyond the controlled corruption schemes?\n\n(2) Could the authors provide more discussion on the computational cost compared to baselines, especially for longer reasoning chains? The influence on reasoning time is not known.\n\n(3) How do you think your proposed method will perform on larger or smaller LLMs? Your experiment only involves LLMs of approximately 4B or 8B."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dTkfMWfVXw", "forum": "eux1cp8GqC", "replyto": "eux1cp8GqC", "signatures": ["ICLR.cc/2026/Conference/Submission14029/Reviewer_knXB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14029/Reviewer_knXB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973933008, "cdate": 1761973933008, "tmdate": 1762924516710, "mdate": 1762924516710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}