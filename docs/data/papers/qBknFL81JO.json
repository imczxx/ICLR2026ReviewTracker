{"id": "qBknFL81JO", "number": 12749, "cdate": 1758210037518, "mdate": 1763753508620, "content": {"title": "Random Label Prediction Heads for Studying and Controlling Memorization in Deep Neural Networks", "abstract": "We introduce a straightforward yet effective method to empirically measure and regularize memorization in deep neural networks for classification tasks.\nOur approach augments each training sample with auxiliary random labels, which are then predicted by a random label prediction head (RLP-head).  \nRLP-heads can be attached at arbitrary depths of a network, predicting random labels from the corresponding intermediate representation and thereby enabling analysis of how memorization capacity evolves across layers.\nBy interpreting the RLP-head performance as an empirical estimate of Rademacher complexity, we obtain a direct measure of both sample-level memorization and model capacity.\nWe leverage this random label accuracy metric to analyze generalization and overfitting in different models and datasets.\nBuilding on this approach, we further propose a novel regularization technique based on the output of the RLP-head, which demonstrably reduces memorization.\nInterestingly, our experiments reveal that reducing memorization can either improve or impair generalization, depending on the dataset and training setup.\nThese findings challenge the traditional assumption that overfitting is equivalent to memorization and suggest new hypotheses to reconcile these seemingly contradictory results.", "tldr": "", "keywords": ["Memorization", "Random Labels", "Overfitting", "Generalization", "Regularization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51d6f95cf61ed7e983b72c9f5888a3ebb6153a4f.pdf", "supplementary_material": "/attachment/34d390546a569c54f763a395e511754a46ccf886.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a new mechanism that both measures and regularizes memorization in ML models. The proposes mechanism introduces a loss term that measures how well a learned representation can be used to predict randomly assigned labels. Smaller loss is correlated with stronger memorization in the representation. The loss then can become a regularizer that prevents memorization. Empirical study shows that the  reduced memorization may have different effects on model utility depending on the sample size and the nature of information being memorized."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes an interesting and effective mechanism based on simple principles to detect and control memorization. The design is interesting and the intuition is clear. Well done. The work is technically sound."}, "weaknesses": {"value": "Despite the smart design of RLP-head and the fairly comprehensive experiments, there seem to be a few missing links in the argument of the paper. Specifically:\n\n1) The paper proposes the loss on random label prediction as **both** the regularizer **and** the measure of memorization. Notice that the RLP loss serves as a proxy of empirical Rademacher complexity, which measures the model's capacity to memorize instead of the amount of memorization. Does a low RLP score necessarily mean less memorization?\n\n2) Given the plethora of work on the pro and cons of memorization, I wonder if the empirical evaluation results and their implications are different from previous work. (See questions.)\n\nIn addition, the pdf file seems not fully follow the template. Some margins between the paragraphs are too small. Hope this can be fixed in future versions."}, "questions": {"value": "1) Could you use a different metric for memorization, say influence-based heuristic in [1] or a method of your choice, to show that the model has less memorization when regularized more heavily with RLP?\n\n2) Has empirical Rademacher complexity ever been used as regularizer before?\n\n3) There has been literatures showing memorization could be beneficial (long-tail) [2] and detrimental (wrong-label) [3]. What are the key insights in this work's experiment that are different from the previous work?\n\n[1] Feldman, Vitaly, and Chiyuan Zhang. \"What neural networks memorize and why: Discovering the long tail via influence estimation.\" Advances in Neural Information Processing Systems 33 (2020): 2881-2891.\n\n[2] Feldman, Vitaly. \"Does learning require memorization? a short tale about a long tail.\" Proceedings of the 52nd annual ACM SIGACT symposium on theory of computing. 2020.\n\n[3] Liu, Sheng, et al. \"Early-learning regularization prevents memorization of noisy labels.\" Advances in neural information processing systems 33 (2020): 20331-20342."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WJdSkO73D1", "forum": "qBknFL81JO", "replyto": "qBknFL81JO", "signatures": ["ICLR.cc/2026/Conference/Submission12749/Reviewer_Lg7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12749/Reviewer_Lg7F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724474995, "cdate": 1761724474995, "tmdate": 1762923567462, "mdate": 1762923567462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal Summary"}, "comment": {"value": "We thank the reviewers for their detailed and constructive feedback, which has helped us substantially strengthen the manuscript.\\\nWe have revised the main text to improve clarity and comprehensibility in response to their comments. In particular, we now state more explicitly that Rademacher complexity serves as a conceptual motivation for our method and we do not claim any theoretical guarantee linking the proposed RLP- head’s random label accuracy to Rademacher complexity. Nevertheless, we validate the conceptual similarities through a wide range of empirical investigations.\\\nWe also clarify that the RLP-regularizer is intended primarily as a tool for studying and controlling memorization in order to better understand its underlying mechanisms, and not as a replacement for established regularization techniques aimed at improving generalization.\n\n\nWe revised the following figures in the main manuscript:\n- Figure 4: We added multiple iterations per data point showing low variance of the random label and test accuracy.\n- Figure 7C: We identified and corrected a minor bug in our label noise implementation for distributed training. After fixing it, the overall conclusion remains unchanged: the\nregularizer provides substantial generalization improvements under noisy labels. We also added additional data points.\n- Figure 9C: We added multiple iterations per data point.\n\n\nFurthermore, to fully address the questions of the reviewers, we added the following sections to the appendix:\n- A.11: Demonstrates that reducing memorization with the RLP-regularizer is less harmful when duplicate images between the train and test sets are removed using the ciFAIR100 dataset.\n- A.12: Provides a detailed comparison between our method and the approach of Feldman & Zhang (2020).\n- A.13: Shows strong similarities between our proposed random label memorization measure and the training accuracy on noisy labeled images, which act as canary points for memorization.\n- A.14: Evaluates other regularizers (dropout, label smoothing, and weight decay) in terms of random label accuracy, performance on noisy-label canary points, and generalization, and compares them to the RLP-regularizer.\n- A.15: Provides empirical evidence strengthening the link between Rademacher complexity and the RLP-head’s random-label accuracy by showing strong correlation with the accuracy of networks trained on fully random labels.\n- A.16: Investigates memorization in models trained with mixup using random label accuracy.\n- A.17: Demonstrates improved adversarial robustness in models trained with RLP-regularization.\n- A.18: Demonstrates improved robustness against membership inference attacks in models trained with RLP-regularization.\n- A.19: Shows that the memorization reduction induced by RLP-regularization, decreases performance particularly on long-tail classes.\n- A.20: Validates our hypothesis that applying regularization to each transformer block’s output shifts memorization into the transformer blocks.\n- A.21: Provides evidence for the memorization-mitigation effect of the RLP regularizer by showing that, for models in which memorization is suppressed by the regularizer, a newly initialized RLP-head is unable to recover the random labels when trained on the model’s frozen feature extractor.\n- A.22: Demonstrates that RLP-heads can be used to study memorization effects beyond vision tasks by investigating memorization in text classification."}}, "id": "M1KmFZhn1u", "forum": "qBknFL81JO", "replyto": "qBknFL81JO", "signatures": ["ICLR.cc/2026/Conference/Submission12749/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12749/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12749/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763753526768, "cdate": 1763753526768, "tmdate": 1763753526768, "mdate": 1763753526768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to measure memorization in deep models. The method involves a trainable RLP heads that can be attached to any layer of the network. The performance of RLP heads serves as a proxy for sample-level memorization. The authors also propose an RLP-based regularizer that reduces memorization by penalizing confident random-label predictions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think this is a really important problem, especially given the fact that sample-level memorization is expensive. The authors present a light weight solution, that can not only be run with relatively low latency, but can also be adapted to different layers of the model. I think this paper has a lot of potential if the authors can address the concerns below."}, "weaknesses": {"value": "1. Lack of Suitable Baseline:\n\nRLP approximates sample-level memorization. However, the paper lacks a clear baseline to verify whether the points identified or regularized by RLP are indeed the truly memorized samples. This is a major concern. The authors should validate this by comparing RLP’s behavior against established memorization benchmarks. This can be using Feldman et al.’s methodology or by introducing random noise images or “canary” points into the dataset. This would help determine whether 1) RLP selectively targets memorized data or unintentionally affects well-learned samples 2) It will also help understand how RLP behaves when base points are learned vs memorized. At this point, it is hard to gauge how well this technique performs. However, a good baseline can alleviate those concerns.\n\n2. Limited Comparison with Existing Regularization Methods:\n\nAlthough the authors present preliminary results suggesting that RLP-based regularization reduces memorization, the study lacks *direct* comparisons to standard regularizers (e.g., dropout, weight decay, or label smoothing). Such comparisons are critical for understanding whether RLP provides a distinct benefit beyond existing techniques. Ideally, these evaluations should be performed along two axes: (a) classification accuracy on intentionally mislabeled or noisy points (refer to point 1) to measure memorization control and (b) test accuracy on clean data (to assess generalization). Without these baselines, it is difficult to fully gauge the contribution and novelty of RLP as a regularizer.\n\n3. Narrow Experimental Scope:\n\nThe current experiments are restricted to vision models trained on image datasets. Extending the analysis to text classification models and even LLMs would significantly strengthen the work, demonstrating the generality of RLP as a tool for studying memorization across architectures and domains."}, "questions": {"value": "Refer to points above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rvjPCTUN6b", "forum": "qBknFL81JO", "replyto": "qBknFL81JO", "signatures": ["ICLR.cc/2026/Conference/Submission12749/Reviewer_hHXA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12749/Reviewer_hHXA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758042662, "cdate": 1761758042662, "tmdate": 1762923567142, "mdate": 1762923567142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Random Label Prediction heads (RLP-heads) as a simple mechanism to measure and control memorization in deep nets during standard supervised training. Each training sample is given an auxiliary random label. A small prediction head attached to an intermediate activation is trained to predict these random labels in parallel with the main task. The authors interpret the RLP accuracy as an empirical proxy for Rademacher complexity and use it to study how memorization evolves over time and across layers. They also introduce a regularizer that penalizes correct random-label predictions to suppress memorization while keeping the task head untouched. Formally, the losses are L_{\\text{class}}=-\\log p_y, L_{\\text{rnd}}=-\\log \\hat p_{\\hat y}, and the regularizer L_{\\text{reg}}=\\log(1-\\hat p_{\\hat y}) scaled by \\lambda. \n\nEmpirically, the paper shows:\n1.\tRLP accuracy tracks capacity and overfitting dynamics, rising to about 70 percent when training ViT-B/32 on ImageNet as test and train accuracies begin to separate. \n2.\tStandard regularizers like dropout, weight decay, and label smoothing reduce RLP accuracy, supporting the complexity interpretation. \n3.\tUsing L_{\\text{reg}} can reduce overfitting and improve test accuracy on ImageNet with ViT (about +1.5 points), but on CIFAR-100 with WRN it reduces memorization without helping test accuracy.\n4.\tLayer-wise probes reveal memorization grows with depth and that regularizing the final layer shifts memorization earlier rather than eliminating it. \n5.\tDataset size and noise matter. Suppressing memorization helps when data are well sampled, but can hurt on undersampled datasets. Adding label noise makes the regularizer beneficial, as predicted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tOriginality: using online random-label heads to continuously estimate per-layer memorization without retraining on random labels is clever and practical\n•\tQuality: solid empirical study across architectures, datasets, head designs, and hyperparameters, including offline sanity checks and dataset subsampling\n•\tClarity: method, losses, and training protocols are well specified, figures communicate dynamics and layer-wise trends\n•\tSignificance: provides a low-overhead diagnostic for memorization and a tunable knob to reduce it, yielding nuanced insights on when memorization helps or hurts generalization; the layer-wise shift phenomenon is especially interesting"}, "weaknesses": {"value": "•\tThe mapping from RLP accuracy to Rademacher complexity is argued empirically; a theoretical bridge or formal bound would strengthen the claim beyond correlation\n•\tImprovements on ImageNet are relatively small and sensitive to \\lambda; practical guidance on choosing \\lambda beyond grid search is limited\n•\tBaseline comparisons are missing to targeted anti memorization methods such as mixup, manifold mixup, early stopping with sharpness awareness, or confidence based noise filtering; the paper mostly compares to generic capacity regularizers\n•\tThe regularizer may alter features in ways that indirectly affect the main head; while gradients are restricted, feature extractor changes can still trade off task signal vs sample specificity; stronger checks isolating collateral effects would help\n•\tThe undersampling hypothesis is compelling but remains somewhat post hoc; additional controlled long tail benchmarks or per class sampling analyses would solidify it\n•\tCompute overhead and wall clock costs from extra heads, multi head variants, and per layer probes are not quantified in detail; practicality at large scale is uncertain\n•\tPrivacy claims are hinted at in motivation but not evaluated; no extraction or canary tests are provided"}, "questions": {"value": "•\tCan the authors formalize the connection between RLP accuracy and empirical Rademacher complexity for multiclass settings, perhaps via margin based surrogates or a bound that depends on head capacity and n?\n•\tHow sensitive are results to the random label assignment itself; do multiple seeds produce similar \\lambda optima and layer profiles, and what is the variance?\n•\tCould you compare RLP regularization against mixup or manifold mixup at matched hyperparameter tuning budgets, and report both accuracy and calibration?\n•\tDo RLP metrics predict robustness or privacy leakage; for example, do higher RLP accuracies correlate with membership inference or canary extraction rates?\n•\tOn long tail datasets like iNaturalist or ImageNet LT, does suppressing memorization reduce performance on rare classes; per tail analysis would test the hypothesis directly\n•\tFor the observed shift of memorization to earlier layers, can you probe within blocks to rule out within block hiding; e.g., multiple taps inside the same transformer block\n•\tWhat is the runtime and memory overhead of single head vs multi head vs all layers, and how does this scale with n and hidden size\n•\tDoes RLP regularization interact with self supervised pretraining; does it help fine tuning stability or hurt transfer"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BTfwlHSimT", "forum": "qBknFL81JO", "replyto": "qBknFL81JO", "signatures": ["ICLR.cc/2026/Conference/Submission12749/Reviewer_xMP4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12749/Reviewer_xMP4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862553372, "cdate": 1761862553372, "tmdate": 1762923566859, "mdate": 1762923566859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Random Label Prediction (RLP) heads: auxiliary prediction heads attached at various depths that are trained to predict fixed, per-example random labels during standard supervised training. The accuracy of these heads is used as a proxy for memorization/capacity. The authors also propose an RLP regularizer that discourages the feature extractor from confidently fitting the random labels, aiming to limit memorization without changing the task head. Experiments on ViT-B/32 with ImageNet and WRN-16-4 with CIFAR-100 show: (i) RLP accuracy rises early in training and can reach high levels on ImageNet; (ii) standard regularizers like dropout, weight decay, and label smoothing tend to reduce RLP accuracy; (iii) the RLP regularizer can improve ImageNet generalization but can hurt on CIFAR-100; (iv) RLP attached at intermediate layers suggests memorization increases with depth, and regularizing only the last layer can shift memorization earlier."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple, general mechanism that is easy to add and interpret: higher random-label accuracy indicates more sample-specific information in the features.\n\n2. Clear empirical phenomena: early rise of RLP accuracy and monotonic increase with depth.\n\n3. Useful bridge from measurement to control: correlation with common capacity controls and a targeted regularizer derived from the same signal.\n\n4. Layerwise analysis reveals memorization shifting under last-layer regularization and provides a way to localize where class information emerges."}, "weaknesses": {"value": "1. Theoretical grounding is informal. The connection to Rademacher complexity is motivational rather than a formal result; no theorem guarantees RLP accuracy is a calibrated surrogate for capacity in deep nets.\n\n2. Attribution is ambiguous. The metric may conflate memorization in the feature extractor with the auxiliary head’s own capacity, the number of random labels, and optimization budget. Appendix ablations help but a clearer identifiability story would be better.\n\nMinor comments\n1. Please switch to the official ICLR template and fonts; the current submission uses a nonstandard font."}, "questions": {"value": "1. CIFAR100 is known to have near duplicates in train and test sets, this could be causing the contrary results on CIFAR100, would de-duplicated CIFAR100 be a better fit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N9b6ekgF5q", "forum": "qBknFL81JO", "replyto": "qBknFL81JO", "signatures": ["ICLR.cc/2026/Conference/Submission12749/Reviewer_BWFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12749/Reviewer_BWFD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974358924, "cdate": 1761974358924, "tmdate": 1762923566531, "mdate": 1762923566531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}