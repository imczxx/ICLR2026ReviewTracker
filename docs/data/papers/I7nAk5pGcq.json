{"id": "I7nAk5pGcq", "number": 17029, "cdate": 1758271352423, "mdate": 1759897203616, "content": {"title": "DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models", "abstract": "Vision-Language-Action (VLA) models have advanced autonomous driving, but existing benchmarks still lack scenario diversity, reliable action-level annotation, and evaluation protocols aligned with human preferences. To address these limitations, we introduce DriveAction, the first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios. DriveAction leverages real-world driving data proactively collected by drivers of autonomous vehicles to ensure broad and representative scenario coverage, offers high-level discrete action labels collected directly from drivers’ actual driving operations, and implements an action-rooted tree-structured evaluation framework that explicitly links vision, language, and action tasks, supporting both comprehensive and task-specific assessment. Our experiments demonstrate that state-of-the-art vision-language models (VLMs) require both vision and language guidance for accurate action prediction: on average, accuracy drops by 3.3% without vision input, by 4.1% without language input, and by 8.0% without either. Our evaluation supports precise identification of model bottlenecks with robust and consistent results, thus providing new insights and a rigorous foundation for advancing human-like decisions in autonomous driving.", "tldr": "", "keywords": ["Vision-Language-Action (VLA)", "Autonomous Driving", "Benchmark", "Human Driving Preferences"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ead02f4b4df28be5797fbe3b4ad1082f71ebef25.pdf", "supplementary_material": "/attachment/76c946fc79fd253e2f46cc1ec2915e15a20f3aec.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DriveAction, a new VLA benchmark built from real-world driving data. The dataset contains QA pairs — including action questions — collected from deployed autonomous vehicles, and uses driver-labeled discrete action decisions. The benchmark covers a wide range of scenarios such as intersections, lane changes, and ramp merges that are useful for evaluating corner-case decision making. The authors propose an action-rooted evaluation framework and show results across various VLMs to demonstrate benchmark sensitivity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The dataset comes from actual self-driving deployments, not synthetic or open-source simulator data. This includes real corner cases, high-value for both research and industry.\n\nHuman-validated action labels ensure the dataset has clean supervision and avoids spurious driving behaviors.\n\nThe scenarios selected are indeed realistic and relevant — these are good assets for the community to study."}, "weaknesses": {"value": "I’m really unsure about the usefulness and motivation of this benchmark for VLA-based driving.\nThe actions are framed as multiple-choice. In real driving, there is no predefined set of choices popping up like a test. So I’m not convinced how this maps to actual autonomous driving:\n\nWhere would these choices come from at runtime?\n\nGenerated from another model? If so, that introduces another huge source of error.\n\nMany examples feel tailor-crafted to match the scenario — unlikely to generalize.\n\nModern VLAs are being used to directly produce actions (i.e., trajectories or control tokens). In that case, direct prediction is simpler and more aligned with real-time operation than asking them to pick from provided abstract options.\n\nTo truly show real-world relevance, the authors need to demonstrate that better DriveAction performance → better driving.\nFor example:\n1. improvements in collision rate\n2. lower displacement error\n3. higher success rate in closed-loop evaluation\nNone of that is measured here.\n\nIf the intention is instead to evaluate general reasoning, then the benchmark is too narrow — it only includes driving scenarios. In that framing, its impact would be limited because it can’t tell you anything about general VLM robustness across domains.\n\nSo either:\n• The benchmark evaluates driving performance — then connect results to actual driving metrics.\n• The benchmark evaluates reasoning — then it’s too domain-specific.\nRight now it’s stuck in the middle — neither fully useful for driving systems nor for reasoning more broadly."}, "questions": {"value": "Can the authors show that performance on DriveAction correlates with real driving outcomes (DE, CR, etc.)?\n\nCan the authors justify how these multiple-choice actions would exist in an actual autonomous vehicle system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F84Obgh5ng", "forum": "I7nAk5pGcq", "replyto": "I7nAk5pGcq", "signatures": ["ICLR.cc/2026/Conference/Submission17029/Reviewer_5d5K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17029/Reviewer_5d5K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592629600, "cdate": 1761592629600, "tmdate": 1762927051504, "mdate": 1762927051504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DriveAction, the first action-driven benchmark specifically designed for Vision-Language-Action (VLA) models in autonomous driving. DriveAction focuses on human-like decision-making. Extensive evaluations on 12 general VLMs and two domain-specific models (Non-MoE vs MoE) reveal how vision and language inputs affect final decisions and expose task-specific bottlenecks (e.g., navigation and traffic-light understanding)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Collecting 16k QA pairs from 2,610 real-world driving scenarios contributed by professional drivers.\n2. Using real-time driver actions as ground-truth labels to capture authentic human decision intent.\n3. Proposing an action-rooted tree-structured evaluation framework that connects vision, language, and action layers."}, "weaknesses": {"value": "As we all know, VLA models are inherently action-centric, and thus the action dimension should play a more decisive role in evaluation. However, DriveAction primarily emphasizes open-loop QA assessments on Dynamic, Static, Navigation, and Efficiency tasks, rather than measuring closed-loop driving behavior that reflects real-time control and long-horizon decision consistency. So it makes me confused. I think the author needs to discuss more about the importance of this benchmark in the community."}, "questions": {"value": "Same to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5LsjblRS5r", "forum": "I7nAk5pGcq", "replyto": "I7nAk5pGcq", "signatures": ["ICLR.cc/2026/Conference/Submission17029/Reviewer_MFt4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17029/Reviewer_MFt4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645876744, "cdate": 1761645876744, "tmdate": 1762927051199, "mdate": 1762927051199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DriveAction, a benchmark specifically designed for Vision-Language-Action (VLA) models in autonomous driving. It aims to fill gaps in scenario diversity, action-level annotation, and human-aligned evaluation. DriveAction includes 16,185 QA pairs across 2,610 driving scenarios, derived from driver-contributed real-world data, featuring action-rooted, tree-structured evaluation connecting vision, language, and action tasks. Experiments with 12 VLMs (e.g., GPT-4o, Claude 3.7, Gemini 2.5 Pro) reveal performance drops when either vision or language modalities are removed, highlighting multimodal dependence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes DriveAction, the first benchmark explicitly designed for Vision-Language-Action (VLA) evaluation in autonomous driving, addressing missing links between vision, language, and action reasoning.\n\n2. Action labels are collected directly from real-time driver operations, faithfully capturing human decision intent rather than post-hoc annotations.\n\n3. The action-rooted, tree-structured framework enables interpretable, modular analysis across V-L-A components, offering fine-grained evaluation flexibility.\n\n4. Evaluates 12 state-of-the-art VLMs under four modality configurations (V-L-A / V-A / L-A / A), systematically showing modality dependencies."}, "weaknesses": {"value": "1. While the benchmark is well-structured, its main finding (that models need both vision and language inputs) is intuitive and not conceptually groundbreaking.\n\n2. Previous works like DriveLM (Sima et al., 2024) and Reason2Drive (Nie et al., 2024) already explore end-to-end reasoning or goal-driven evaluation, weakening the “first action-driven” claim.\n\n3. Evaluation focuses on accuracy without deeper breakdowns (e.g., statistical variance, error typology, or causal reasoning analysis)."}, "questions": {"value": "1. How is inter-driver variability handled in “driver-contributed” data to ensure label consistency?\n\n2. Could authors clarify whether the action labels are categorical only or also include continuous control values?\n\n3. How are QA pairs validated for bias or ambiguity given LLM assistance in generation?\n\n4. Do the results generalize to unseen city environments, or is there domain overfitting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1cVqOL5MyL", "forum": "I7nAk5pGcq", "replyto": "I7nAk5pGcq", "signatures": ["ICLR.cc/2026/Conference/Submission17029/Reviewer_HC2D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17029/Reviewer_HC2D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042003948, "cdate": 1762042003948, "tmdate": 1762927050851, "mdate": 1762927050851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DriveAction, an action-driven benchmark for Vision-Language-Action (VLA) models in autonomous driving. It includes over 16K QA pairs across diverse real-world scenarios with human-annotated action labels and a tree-structured evaluation framework, enabling comprehensive assessment of vision, language, and action reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces DriveAction, a well-structured benchmark.\n\n2. Dataset quality is high, with real-world, driver-contributed data with diverse scenarios."}, "weaknesses": {"value": "1. I suggest that the authors include a discussion of recent studies on VLM-generated datasets for autonomous driving that are built on different foundations. For example, some works such as [1][2] generate data based on existing datasets like nuScenes or nuPlan, while others use internal datasets. Highlighting these distinctions would help the community better understand the overall differences and positioning of this work.\n\n[1] Y. Xu et al., “VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision,” CoRL, 2025\n\n[2] Z. Zhou et al., “AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning,” NeurIPS, 2025.\n\n2. Another concern is dataset quality. Although human verification is mentioned, the annotation and quality-control process could be described in greater detail to improve transparency and reproducibility.\n\n3. While the benchmark design is strong, the paper mainly focuses on dataset construction and evaluation, with limited methodological novelty. I am not sure whether it fits better under a benchmark or dataset track, if such a category exists."}, "questions": {"value": "1. How is DriveAction’s action taxonomy defined and maintained to prevent overlap or ambiguity between tasks (e.g., “navigation lane change” vs. “efficiency lane change”)?\n\n\n2. Were there any efforts to balance action categories, given the natural bias toward simple maneuvers (e.g., going straight)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9LSjPkoKaL", "forum": "I7nAk5pGcq", "replyto": "I7nAk5pGcq", "signatures": ["ICLR.cc/2026/Conference/Submission17029/Reviewer_KDFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17029/Reviewer_KDFH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236988572, "cdate": 1762236988572, "tmdate": 1762927050373, "mdate": 1762927050373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}