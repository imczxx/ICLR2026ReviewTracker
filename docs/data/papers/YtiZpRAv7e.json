{"id": "YtiZpRAv7e", "number": 12560, "cdate": 1758208571620, "mdate": 1763120343192, "content": {"title": "An Unlearning-Enhanced General Framework for Test-Time Adaptation", "abstract": "Test-time Adaptation (TTA) aims to mitigate performance degradation caused by distribution shifts during testing time. While various TTA approaches exist, such as entropy minimization, pseudo-labeling, weight-space regularization and Bayesian methods, a generalized optimization framework for TTA is currently absent. To address this gap, we present a general framework for TTA. This framework provides a conceptual basis for understanding existing methods as specific instances within a broader optimization framework, and facilitates the development of new TTA methods. Additionally, our proposed framework brings attention to limitations in existing approaches by unveiling an implicit assumption that all source domain knowledge is universally beneficial for adapting to the target domain. In reality, only a portion of the source domain knowledge is useful due to potential large distribution discrepancies between the source and target domains. Based on this insight, we build upon our general framework and derive a novel method named Unlearning-enhanced test-time adaptation (Lana). Specifically, it adaptively unlearns irrelevant source domain knowledge and then adapts to the target test domain. Through thorough theoretical analysis and empirical results, we showcase the effectiveness of our proposed method in enhancing TTA performance. This work contributes not only a broader understanding of TTA through a general framework but also a novel practical solution, Lana, derived from our general framework, offering a foundation for further advancements in addressing distribution shifts during testing in machine learning models.", "tldr": "", "keywords": ["Test time adaptation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2da3a14450ac01dc65343d7abcc113e167d251fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a general optimization framework for test-time adaptation (TTA) that unifies existing methods under a Bregman divergence formulation. Building on this framework, the authors introduce Lana, a two-stage method that first \"unlearns\" source-specific knowledge using the Fisher Information Matrix (FIM) and then adapts to the target domain via entropy minimization. The method is designed to be plug-and-play and is evaluated on ImageNet-C and DomainNet-126 under various distribution shift scenarios. Experimental results show consistent improvements over strong baselines such as Tent, SAR, and EATA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method achieves noticeable and consistent accuracy gains across multiple challenging TTA scenarios, including label imbalance, small batch sizes, and mixed corruption types.\nIntroducing 'unlearning' into TTA and proposing a two-stage adaptation process (forgetting first, then adapting) has certain biological inspiration and novelty.\nLana can be integrated into existing TTA methods without modifying their original structure, which enhances its practical appeal.\nThe general framework based on Bregman divergences offers a unified view of diverse TTA objectives, which may facilitate future theoretical analysis and method development."}, "weaknesses": {"value": "1. The proposed Lana method essentially concatenates two well-studied components: (i) Fisher-weighted parameter updating (borrowed directly from EATA) and (ii) standard entropy minimization (Tent). The resulting pipeline therefore feels like a \"stitched\" baseline rather than a principled advance, and the observed gains, are better viewed as incremental ablations than conceptual breakthroughs.\n\n2. The combination of different TTA method seem to be exaggerate. The paper simultaneously advertises a \"general framework\", a \"biologically-inspired forgetting mechanism\", and a \"plug-and-play enhancement\". These multiple, partially conflicting narratives obscure the true contribution: a two-stage heuristic that re-weights gradients by the inverse Fisher diagonal. A crisper focus on this practical detail would have been preferable to the sweeping claims of universality.\n\n3. Your comparison methods are outdated and insufficient, and more recent SOTA methods need to be compared. Moreover, most of the methods used in your approach are existing and seem to have not undergone any changes, just being directly applied.\n\n4. Dozens of equations (Taylor expansions, matrix inverses, PAC-Bayesian bounds) are devoted to a one-line update rule. The dense formula load therefore appears ornamental and hinders rapid comprehension and make the description very confusing.\n\n5. The manuscript provides no visual evidence (e.g., t-SNE, and only one figure through the whole article) indicating what knowledge is actually erased or what."}, "questions": {"value": "See the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mjm6epecJR", "forum": "YtiZpRAv7e", "replyto": "YtiZpRAv7e", "signatures": ["ICLR.cc/2026/Conference/Submission12560/Reviewer_Csov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12560/Reviewer_Csov"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378164531, "cdate": 1761378164531, "tmdate": 1762923417377, "mdate": 1762923417377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "G8HGMjEqwB", "forum": "YtiZpRAv7e", "replyto": "YtiZpRAv7e", "signatures": ["ICLR.cc/2026/Conference/Submission12560/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12560/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120342422, "cdate": 1763120342422, "tmdate": 1763120342422, "mdate": 1763120342422, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a general framework for test-time adaptation (TTA) that establishes a unified conceptual foundation for understanding existing approaches as specific instances within a broader optimization perspective. This framework also offers practical guidance for designing new TTA algorithms. Building upon it, the authors introduce a novel method termed Unlearning-Enhanced Test-Time Adaptation (Lana), which adaptively unlearns irrelevant source-domain knowledge before adapting to the target test domain. Comprehensive theoretical analysis and extensive empirical evaluations demonstrate the effectiveness of the proposed method in improving TTA performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents a general optimization framework for test-time adaptation (TTA), which provides a unified guideline for developing new TTA methods.\n2. Building upon this framework, the paper proposes an unlearning-enhanced TTA approach that improves adaptability to target test data distributions."}, "weaknesses": {"value": "1. The authors claim to compare their method with Tent, EATA, AdaContrast, SAR, DeYO, and TEA in the baseline section. However, in many result tables (e.g., Table 2, Table 3, and others), these baselines are not consistently included across all experimental settings. Could the authors clarify the reason for this inconsistency?\n2. In the section \"Evaluation of different methods under a standard TTA setting,\" the authors only state that \"we follow the setup outlined in (Yuan et al., 2024)\". However, the paper does not provide sufficient details about this setup. The results shown in Table 11 are limited and do not convincingly demonstrate Lana’s effectiveness under the standard TTA setting. I suggest the authors include more experiments to support this claim.\n3. In Table 3, the bold numbers are presumably meant to highlight specific results. Could the authors clarify what the bold formatting represents? In particular, why are no numbers bolded under the ResNet-50 (BN) setting?\n4. In Table 9, the authors only report the runtime of Tent + Lana. I recommend also reporting the runtime when combining Lana with other baselines for consistency. In addition, could the authors provide an analysis of the additional memory overhead introduced by Lana?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DJfvhPx2hc", "forum": "YtiZpRAv7e", "replyto": "YtiZpRAv7e", "signatures": ["ICLR.cc/2026/Conference/Submission12560/Reviewer_1wNH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12560/Reviewer_1wNH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460543679, "cdate": 1761460543679, "tmdate": 1762923417011, "mdate": 1762923417011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper first introduces a unified Test-Time Adaptation (TTA) optimization framework based on Bregman divergence, which integrates multiple existing paradigms including entropy minimization, pseudo-labeling, weight/output regularization, and Bayesian approaches into one general formulation.Building on this, it proposes Lana, a two-step TTA method: first, it performs selective unlearning; second, it conducts standard target-domain adaptation.\nMain contribution:This paper presents a unified theoretical framework for Test-Time Adaptation (TTA) and introduces a new paradigm of “unlearning first, then adapting” (Lana). It provides an update rule that can be easily combined with existing TTA methods, and demonstrates superior performance over multiple state-of-the-art (SOTA) approaches on benchmarks such as ImageNet-C."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unifying perspective: The Bregman-based objective offers a tidy lens to rewrite several existing methods.\n2. Method simplicity & implementability: Lana is easy to plug in (one-shot, diagonal FIM; no source labels), and can be combined with common baselines such as Tent with minimal code changes.Estimating F once and using a diagonal approximation keeps the overhead low.\n3. Experimental stress tests: Includes difficult ImageNet-C regimes (severity 5, label imbalance, bs=1, mixed corruptions), which are relevant stressors for TTA.\n4. Clarity of presentation: Key equations are explicit; the special-case reductions and algorithmic flow are reasonably easy to follow."}, "weaknesses": {"value": "1. The current Bregman-divergence-based unification appears mostly formal, lacking justification for why such unification is necessary and what concrete benefits it brings. The paper should present new variants systematically derived from the framework that show empirical advantages, and offer unified theoretical insights (e.g., on stability, flatness, or convergence) to demonstrate added value rather than mere reformulation.\n2. The “unlearning-then-adaptation” idea is appealing, yet the implementation essentially reduces to a Fisher-preconditioned entropy gradient step. This is closely related to EWC, EATA, natural gradient, or generalized Gauss–Newton methods, making the innovation limited. The link between the conceptual motivation (“selective unlearning”) and the concrete method is weak. The authors should clarify why this specific step embodies unlearning and how it fundamentally differs from existing Fisher-based approaches; otherwise, the novelty risks being seen as a repackaging of known techniques.\n3. The comparisons focus mainly on classical methods, lacking evaluation against more recent and stronger baselines. Without up-to-date SOTA results under consistent experimental protocols, the claimed improvements remain unconvincing.\n4. Theoretical approximation validity and error bounds unclear. The method relies on first- and second-order Taylor expansions, yet the paper does not specify the magnitude of the neglected higher-order terms or the radius of validity of these approximations."}, "questions": {"value": "1. The proposed general TTA optimization framework is based on Bregman divergence.  Please clarify the unique theoretical or practical advantages of this formulation. For example, can it systematically inspire new algorithmic designs or provide novel insights into stability or generalization?\n2. The core of the proposed method appears nearly equivalent to a single Fisher-preconditioned update. Please explain how this process mechanistically achieves “selective unlearning,” and in what essential ways Lana differs from other Fisher-based approaches such as EWC, EATA, or natural gradient methods.\n3. In the paper, the Fisher Information Matrix F is computed only once and approximated diagonally. If F changes over time, would the algorithm’s performance degrade? Have you considered dynamic re-estimation or damping mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "skmwG6OdyA", "forum": "YtiZpRAv7e", "replyto": "YtiZpRAv7e", "signatures": ["ICLR.cc/2026/Conference/Submission12560/Reviewer_uE2V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12560/Reviewer_uE2V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468056743, "cdate": 1761468056743, "tmdate": 1762923416669, "mdate": 1762923416669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a general optimization framework for Test-Time Adaptation (TTA) using Bregman divergences to unify entropy minimization, pseudo-labeling, weight/output regularization, and Bayesian methods. It reveals a key limitation: existing TTA assumes all source knowledge is beneficial. The authors introduce Lana, an unlearning-enhanced method that first adaptively removes irrelevant source knowledge via Fisher-based parameter importance, then adapts to the target domain."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Lana is a principled extension: unlearning irrelevant source knowledge via Fisher inverse addresses negative transfer — biologically and empirically motivated.\n\n2. Strong empirical gains: +13.8% over Tent on ImageNet-C (ViT), +15.4% on ResNet50; consistent across corruptions and natural shifts.\n\n3. Thorough ablations validate unlearning threshold, integration with multiple TTA baselines, and robustness to batch size/class imbalance."}, "weaknesses": {"value": "1. The proposed unified framework is more of a summary and categorization of prior work, grouping loss functions via hyperparameters and Bregman divergences — serves only a preliminary role, not a core contribution. Authors should invest more in deeply exploring machine unlearning’s role in TTA with theory and experiments.\n\n2. How is F (Fisher matrix) computed? Diagonal approximation is used, but no details on empirical Fisher vs. true Fisher, stability, or computational cost.\n\n3. Equations (9)–(11) are vaguely explained — purpose of Taylor expansions and their role in unlearning/adaptation is not clearly introduced.\n\n4. Why does $ F^{-1} $ indicate parameter importance for source domain knowledge? Intuition is stated but not rigorously justified; connection to forgetting is weak."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ivwSAbaxd7", "forum": "YtiZpRAv7e", "replyto": "YtiZpRAv7e", "signatures": ["ICLR.cc/2026/Conference/Submission12560/Reviewer_VVW5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12560/Reviewer_VVW5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996512681, "cdate": 1761996512681, "tmdate": 1762923416218, "mdate": 1762923416218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}