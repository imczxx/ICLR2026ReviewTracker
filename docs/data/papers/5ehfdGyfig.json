{"id": "5ehfdGyfig", "number": 6889, "cdate": 1758000128666, "mdate": 1763688120458, "content": {"title": "AGE: Adaptive-masking for Graph Embedding in Graph Retrieval-Augmented Generation", "abstract": "GraphRAG is an extension of retrieval-augmented generation (RAG) that supports large language models (LLMs) by referring to graph-structured data as external knowledge. While this technique ideally captures intricate relationships, it often struggles with graph representations for LLMs, particularly for frozen LLMs, due to the misalignment between graph-based and text-based latent features. We tackle this issue by introducing the Adaptive-masking for Graph Embedding (AGE). AGE employs a Transformer in a mask-based self-supervised learning (SSL) approach. We designed the architecture similar to text embedding encoders, addressing the latent feature misalignment. In contrast to natural language texts, graphs are concise representations, and there exist key nodes that hold dominant contextual information, which are challenging to predict from their surroundings. Masking such key nodes leads to inefficiency in the SSL process. Therefore, AGE focuses on predicting nodes apart from key nodes, utilizing a learnable node sampler. Our experimental results indicate that AGE significantly improves approaches using non-parametric search component in GraphQA tasks, achieving superior accuracy across three benchmark datasets with distinct characteristics.", "tldr": "", "keywords": ["Self Supervised Learning", "Large Language Models", "Graph Retrieval-Augmented Generation", "Reinforcement Learning", "Knowledge Graphs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9941ebb1781c5221a700c359ee83f9abb0075c8.pdf", "supplementary_material": "/attachment/ad56d296d7da22b1e40f51f56e0919e8956c66ac.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes GraphDistill, a framework where large language models (LLMs) are used as teachers to improve the reasoning ability of graph neural networks (GNNs). The general idea is to query an LLM with graph-related questions (e.g., about connectivity, motifs, or reasoning over structures), collect its natural-language reasoning traces, and then distill that information into a smaller GNN through a combination of supervised and contrastive training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using LLMs as reasoning teachers rather than solvers is appealing. It’s an interesting shift from “let’s replace GNNs with LLMs” to “let’s make GNNs learn from LLMs.” That framing could inspire further work on cross-modality supervision and model interpretability.\n2. The experiments show consistent gains across multiple datasets. While the improvements are small, they’re at least consistent, which suggests there’s something meaningful in the approach. The inclusion of ablations with different LLMs adds a bit more credibility."}, "weaknesses": {"value": "1. The paper claims that reasoning traces help GNNs “reason better,” but there’s no strong evidence for that. The analysis doesn’t clearly show that the GNN learned reasoning-like behaviors, only that its accuracy improved slightly. Without causal or behavioral evaluation (e.g., testing generalization to unseen reasoning patterns), it’s hard to believe that genuine reasoning was distilled.\n2. While improvements are reported, they’re small and sometimes within the margin of noise (1–3%). On larger or more realistic benchmarks, the method offers little advantage. There’s also no discussion of computational cost — querying GPT-4 for thousands of samples is non-trivial, and the cost-benefit tradeoff is unclear.\n3. The experiments mostly compare against simple GNNs. There are no comparisons to more recent works like GraphLLM, Graph Neural Prompting, or GNN-LM hybrid models. Without these, it’s difficult to position the novelty or significance of the approach in the broader literature"}, "questions": {"value": "1. What exactly is the GNN learning from the reasoning traces — structure, text semantics, or something else?\n2. Can you show any qualitative examples where the distilled GNN performs a reasoning-like behavior the baseline cannot?\n3. How robust is the approach to noisy or low-quality reasoning outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SKZaWQWXmO", "forum": "5ehfdGyfig", "replyto": "5ehfdGyfig", "signatures": ["ICLR.cc/2026/Conference/Submission6889/Reviewer_J96N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6889/Reviewer_J96N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644184286, "cdate": 1761644184286, "tmdate": 1762919134516, "mdate": 1762919134516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the issue of misaligned graph representations for frozen LLMs. It proposes Adaptive-masking for Graph Embedding (AGE), a framework that employs a mask-based self-supervised learning (SSL) process incorporating the Joint-Embedding Predictive Architecture (JEPA) and introducing a reinforcement learning–trained node sampler. AGE demonstrates its effectiveness on graph QA tasks, including ExplaGraphs, SceneGraphs, WebQSP, and CWQ. The main contributions are: (i) proposing a new graph representation framework, AGE; (ii) replacing random masking with a node sampler to distinguish key nodes from auxiliary nodes; and (iii) conducting benchmark experiments, achieving state-of-the-art accuracy or Hit@1 results on three datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The AGE framework addresses the problem of graph representation in LLMs by leveraging the effective SSL paradigm JEPA and introducing a new node sampler to replace random masking, thereby improving performance.\n\n2. The experimental results overall demonstrate the effectiveness of the proposed framework across four benchmarks, achieving SOTA performance on three of them.\n\n3. The structure of the paper is clear and well supported by figures and tables.\n\n4. While the improvements are modest in certain cases, the results consistently surpass strong baselines, indicating that the approach has potential significance for future research in graph–LLM integration."}, "weaknesses": {"value": "1. The framework primarily combines JEPA with a Multi-Head Attention–based node sampler, which appears somewhat ad hoc and lacks sufficient theoretical justification for why this combination effectively addresses graph misalignment. In addition, the designed modules are not adequately demonstrated, leaving readers uncertain about the necessity of each module like Concept Encoder-Decoder in supporting graph representation.\n\n2. In terms of experiments, the evaluation is restricted to relatively small frozen LLMs (1B–13B), leaving it unclear whether the approach would remain effective or scalable for larger models, or if it is only effective for specialized small-scale models.\n\n3. The evaluation metrics are rather limited, focusing solely on Accuracy and Hit@1. Incorporating more fine-grained measures (e.g., F1, Recall@k, Hit@k, or efficiency-related metrics) would enable a more comprehensive assessment of the proposed approach.\n\n4. Furthermore, the readability of the paper could be improved, as the excessive use of subscript abbreviations makes the technical sections difficult to follow. In addition, there is a typo in the explanation of Equation (1), where the symbol for the text-modal knowledge T is missing. Also, the explanation of Table omits discussion of the CWQ dataset."}, "questions": {"value": "Could the authors clarify the role of each module in the AGE framework, such as the Concept Encoder-Decoder, and explain how they contribute to the overall graph representations through additional descriptions or supporting experiments?\n\nHow is the training process in terms of complexity and duration? After training, is the inference time comparable to that of non-LLM retrieval methods?\n\nCould the authors comment on whether the method would remain effective when applied to larger-scale LLMs, or if it is more suitable for small-scale specialized models?\n\nCould the authors provide additional metrics to better demonstrate the effectiveness of the experiments, and include a deeper analysis of these metrics to reveal the intrinsic mechanisms of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R7yKRAOc4w", "forum": "5ehfdGyfig", "replyto": "5ehfdGyfig", "signatures": ["ICLR.cc/2026/Conference/Submission6889/Reviewer_r4Hw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6889/Reviewer_r4Hw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782988656, "cdate": 1761782988656, "tmdate": 1762919134181, "mdate": 1762919134181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AGE (Adaptive-masking for Graph Embedding), a novel node-masking self-supervised learning strategy that adapts masking patterns for improved graph representation, specifically designed for integration with GraphRAG (Graph Retrieval-Augmented Generation) systems that interface with large language models (LLMs). AGE replaces random masking with a reinforcement learning-based node sampler targeting key nodes in the graph, aiming to improve the alignment between graph structure and textual representations, particularly for frozen LLMs with non-parametric retrievers. The methodology integrates a JEPA-inspired encoder-predictor-target architecture and demonstrates consistent accuracy gains on several GraphQA benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear Motivation and Problem Statement: The paper clearly states the challenge of misalignment between graph and text embeddings for LLMs, especially when using frozen LLMs and non-parametric retrievers. The rationale for focusing on masking non-key nodes is convincing.\n\n\n- Novelty in Masking Strategy: Introducing a reinforcement learning-based node sampler to select key nodes for masking represents a creative adaptation over random masking, and is conceptually well-motivated by the concise, non-redundant nature of graphs.\n\n\n- Comprehensive Experimental Results: The experimental results and ablation studies offer multi-perspective evaluation across multiple datasets and model settings. The experimental design is careful, and performance gains are significant; in particular, AGE shows 26.7 percentage point improvement on ExplaGraphs compared to the G-Retriever baseline with Llama3.2-1B."}, "weaknesses": {"value": "+ Inadequate Ablation on the Node Sampler's Effectiveness on Other Tasks: The paper only tests AGE on GraphRAG (GraphQA) tasks. As stated in the limitations, it is unclear whether the RL-based node sampler for adaptive masking would retain its efficacy for other graph embedding applications (e.g., node classification, link prediction, or other modalities). Targeted experiments on at least one such task would clarify broader utility.\n+ Lack of Theoretical Analysis for Node Sampler: While the reinforcement learning/REINFORCE formulation is presented, there is little theoretical or empirical explanation as to why the learned node saliency effectively distinguishes 'key' from 'auxiliary' nodes in a way that generalizes or improves graph representation universally. Are there common node properties (e.g., centrality, clustering, bottlenecks) that the sampler converges to in practice? The color-coded qualitative t-SNE in Figure 5 is informative but anecdotal.\n+ Lack of comparison with strong heuristic baselines for key-node selection: The paper does not compare the learnable node sampler with several simple yet reasonable heuristic strategies (e.g., degree-based, PageRank, betweenness centrality, TF-IDF for textual importance, or top-k nodes ranked by retrieval scores). If such heuristic methods already provide similar improvements, the additional complexity of a learnable sampler may not be sufficiently justified. Currently, the paper only contrasts random masking with the learnable sampler, without including these stronger heuristic baselines, which weakens the empirical claim of the proposed module’s necessity.\n+ Result Presentation and Statistical Robustness: In Table 1 and 2, no variances or confidence intervals are reported, nor is there discussion of statistical significance or stability across multiple experimental runs. This raises uncertainty regarding the robustness of claimed gains, especially since improvements on some datasets (e.g., WebQSP) are much smaller."}, "questions": {"value": "1. Could you please provide more rigorous analysis (theoretical or empirical) of how the RL-based node sampler distinguishes key nodes from non-key nodes? For example, are there emergent properties, metrics, or correlations with known graph-theoretic quantities?\n\n2. Have you run any of the ablation tests or main experiments on graph-centric tasks outside the GraphQA/GraphRAG scope (e.g., node classification, link prediction) using public benchmarks? If so, please provide results or commentary.\n\n3. Could you systematically report variance or statistical significance (multiple seeds/runs) for key results, especially those with less dramatic improvements?\n\n4. Is the node sampler robust to varying graph sizes/densities, or is model performance highly sensitive to the fixed sampling rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PyTPlT3q37", "forum": "5ehfdGyfig", "replyto": "5ehfdGyfig", "signatures": ["ICLR.cc/2026/Conference/Submission6889/Reviewer_mUCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6889/Reviewer_mUCY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923165867, "cdate": 1761923165867, "tmdate": 1762919133386, "mdate": 1762919133386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AGE, an adaptive-masking embedding strategy for graph designed to enhance Graph Retrieval-Augmented Generation (GraphRAG) for Large Language Model (LLM) by improving the alignment and quality of graph-structured embeddings for frozen LLM through a self-supervised learning approach that uses a reinforcement learning-guided node sampler to strategically mask auxiliary nodes instead of critical key nodes."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The central concept of adaptively masking nodes in a graph, rather than using random masking, is well-motivated for graph structure data. By masking auxiliary nodes during SSL process for graph embedding module, LLMs can identify redundant information within retrieved graph.\n2. The overall architecture is well designed. It integrates the Joint-Embedding Predictive Architecture into the GraphRAG context, moving beyond simple generative or contrastive approaches, to alleviating the reconstruction noise and collapse risk of generative SSL. And the use of a Reinforcement Learning based node sampler to solve the non-differentiable selection problem is a clever and appropriate technical choice.\n3. AGE is a practical method for improving GraphRAG performance. It update graph embedding module on non-parametric retrievers with frozen LLMs, which avoids end‑to‑end LLM fine‑tuning, and keeps compute within reasonable bounds for real deployments."}, "weaknesses": {"value": "1. How well the graph embedding module can be under AGE framework depends largely on whether the node sampler can find out key nodes within a graph. Currently, AGE use a trainable MHA network to predict key nodes. And it's better to analyze the performance between different node sample strategy and see whether it would affect the model performance, which can provide more compelling evidence for the core insight about key nodes of this paper.\n2. The training pipeline is too complex under three different loss function and lots of modules, which may leads to unstable training process for AGE. And the authors do not provide more detail about the experiments setup and implementation details about the training process. How to make so many modules converge as quickly as possible during the training process? \n3. The work claims that AGE can be used for arbitrary LLMs, but experiments are all conducted under Llama series model. It would be better to incorporate other LLMs such as Qwen in the experiments to further confirm this claim."}, "questions": {"value": "1. Although there is no overlap between the three losses, is it really feasible to directly sum the three loss functions? This is somewhat counterintuitive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8YBKVddqKH", "forum": "5ehfdGyfig", "replyto": "5ehfdGyfig", "signatures": ["ICLR.cc/2026/Conference/Submission6889/Reviewer_wmUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6889/Reviewer_wmUU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989948954, "cdate": 1761989948954, "tmdate": 1762919132895, "mdate": 1762919132895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}