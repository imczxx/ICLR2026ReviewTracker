{"id": "Vc9yt0YwPw", "number": 12480, "cdate": 1758208099449, "mdate": 1759897507121, "content": {"title": "Benchmarking Machine Unlearning for Vision Transformers", "abstract": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. \nIn parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. \nYet, MU research for vision tasks has largely centered on CNNs, not VTs. \nWhile benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs.\nThis work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities.\nThe work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; \n(ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. \nAdditionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. \nThe benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing the current state of affairs.", "tldr": "", "keywords": ["machine unlearning", "vision transformers", "memorization", "continual and one-shot unlearning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/21c27a2d45288d99ac80d28719ef2e55a9274e0e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the first systematic benchmark for machine unlearning (MU) in Vision Transformers (VTs), addressing a gap in the literature where prior work focused primarily on CNNs, LLMs, and diffusion models. The authors evaluate a set of MU algorithms (Fine-tune, NegGrad+, and SalUn with RUM meta-framework) across multiple VT architectures (ViT, Swin-T), varying model capacities, and datasets (CIFAR-10/100, SVHN, and ImageNet). The study also explores memorization dynamics, proxy validity, and continual unlearning performance, proposing a unified evaluation framework using ToW and ToW-MIA metrics. The benchmark reveals several insights: VTs exhibit memorization patterns similar to CNNs; CNN-derived memorization proxies (notably Confidence and Holdout Retraining) remain effective for VTs; NegGrad+ generalizes best across settings, and pretraining enhnces unlearning on simpler tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Novelty and importance:** The work tackles a clear and timely gap in machine unlearning research as ViTs are increasingly central to modern vision systems. The inclusion of multiple VT families (ViT, Swin-T) and capacity scales demonstrates thoughtful coverage of architectural diversity.\n- **Comprehensive experimental design:** Evaluates across four datasets of varying complexity and multiple model capacities, providing a nuanced understanding of how data scale and architecture interact with unlearning. Inclusion of both single-shot and continual unlearning is commendable, as continual settings reflect realistic deployment conditions.\n- **Methodology:** The ToW and ToW-MIA metrics offer a unified and interpretable framework balancing forget quality and performance retention, building on and extending recent unlearning evaluation practices. Detailed hyperparameter configurations, reproducibility links, and confidence intervals enhance the transparency of the paper.\n- **Insightful findings:** The paper provides actionable empirical insights: (a) pretraining mitigates over-memorization (b) Swin-T is more amenable to gradient-based unlearning and (c) proxies like Holdout Retraining scale well computationally. It also identifies non-trivial architecture–algorithm compatibilities, such as fine-tuning working better for ViT and NegGrad+ excelling for Swin-T.\n- **Benchmarking Contribution:** Offers a publicly available benchmark and codebase that can serve as a foundation for future research, similar to TOFU or MU-Bench in other modalities."}, "weaknesses": {"value": "- **Limited algorithmic diversity:** While the selected algorithms are representative, the benchmark excludes newer VT-specific unlearning methods (e.g., NOVO [1]), even if contemporaneous. Including at least a comparative baseline or partial replication would strengthen relevance.\n- **Missed baselines from VT-specific continual learning (CL).** The study benchmarks MU algorithms but does not adapt or compare against strong CL methods purpose-built for Vision Transformers, such as lifelong/continual ViTs and exemplar-free approaches that rely on attention-, functional-, or weight-regularization [2,3]. Many of these techniques are replay-free (matching the data-deletion spirit) and could be repurposed as unlearning operators (e.g., using regularizers to suppress influence of $D_f$ while retaining $D_r$ ). Without these baselines, it’s hard to tell whether MU-specific methods truly outperform CL-style regularizers in VT settings. This gap is especially relevant given the paper’s emphasis on continual unlearning scenarios.\n- **Focus on memorization-based methods** Along a similar note, the study’s restriction to memorization-leveraging algorithms provides depth but limits generalizability. It would be infurmative to see how non-memorization-based or regularization-based MU methods perform in VTs.\n- **Metric Interpretability and Sensitivity:** ToW and ToW-MIA, though unified, may be opaque to practitioners: the product-form metric can conflate tradeoffs between retention and forgetting. Ablating or weighting terms could clarify sensitivity.\n- **Limited Theoretical Framing:** While I understand that the paper is primarily meant to be empirical, a theoretical discussion of why transformer attention patterns might affect unlearning differently (e.g., entanglement across heads/layers) would be extremely beneficial to readers.\n- **Scalability:** Although the study includes ImageNet-scale data, it remains confined to relatively small VT variants (≤50M parameters). Real-world ViT-L/16 or Swin-B models might exhibit different behaviors, particularly for continual unlearning. Can the authors comment on this? \n\n\n**References**\n\n[1] Roy, Soumya et al. “NOVO: Unlearning-Compliant Vision Transformers.” (2025)\n\n[2] Pelosin, Francesco et al. “Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization.” CVPRW (2022).\n\n[3] Wang, Zhen et al. “Continual Learning with Lifelong Vision Transformer.” CVPR (2022)."}, "questions": {"value": "- **Proxy Generality:** Have you tested whether confidence or holdout retraining proxies remain stable across fine-tuning stages or different pretraining corpora (e.g., ImageNet-21K vs. ImageNet-1K)?\n- **Algorithm Interactions:** How does RUM partitioning (low/medium/high memorization) affect convergence stability for transformers with high inter-layer attention mixing? Does partition ordering matter?\n- **Continual Unlearning Robustness:** In continual unlearning, would retraining intermediate models periodically (vs. sequential finetuning) improve long-term stability, or is catastrophic forgetting already minimal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uz29LBnxVS", "forum": "Vc9yt0YwPw", "replyto": "Vc9yt0YwPw", "signatures": ["ICLR.cc/2026/Conference/Submission12480/Reviewer_FuaG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12480/Reviewer_FuaG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760805810686, "cdate": 1760805810686, "tmdate": 1762923357035, "mdate": 1762923357035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the first comprehensive benchmark of vision transformers (VT) in machine unlearning, which is commonly dominated by CNNs and LLMs. The work evaluates two vision transformer architectures (ViT and Swint-T) with two parameter sizes on established unlearning benchmarks (i.e., CIFAR-10/100, SVHN, and an ImageNet subset). Additionally, this work compares CNN and VT memorization capabilities, the influence of pretraining in model unlearning, and the performance stabilities under continual unlearning. Results highlight fundamental similarities between CNNs and VTs' unlearning behaviors and memorization patterns, while showing discrepancies for some unlearning methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to understand.\n2. The idea is interesting, and I believe the research question is well-justified.\n3. The paper also explores memorization and continual unlearning, which are important aspects for the narrative.\n4. I appreciated that the source code was attached to the submission."}, "weaknesses": {"value": "**Major weaknesses**\n1. **Limited findings.** While the idea of benchmarking vision transformers in machine unlearning is an interesting research direction, I believe the major finding of this paper is that VTs closely resemble CNN when unlearning, with some exceptions that I believe end up in the background. For instance, the usefulness of pretraining for VTs unlearning on small datasets is of low interest, considering that retraining is feasible for such small datasets. Therefore, I am unsure whether this paper's findings are sufficiently interesting for the ICLR community, considering that ViTs were already employed by a few unlearning papers [1, 2, 3, 4].\n2. **Odd unlearning scheme.** This paper follows the unlearning scheme of Zhao et al. (2024). In a nutshell, it uses proxy measures to estimate sample memorization. Then, it exploits the estimated memorization to partition forget set samples into three subsets, i.e., low-, medium-, and highly-memorized samples. Unlearning is performed by sequentially unlearning these three subsets from low- to highly-memorized. Although this is a valid unlearning procedure, it is not widely adopted in the literature, to the best of my knowledge. Furthermore, it conditions unlearning on an arbitrary proxy measure for estimating memorization, partially hiding the real unlearning contribution. Therefore, I believe unlearning in a more \"standardized\" way would have at least strengthened this paper's claims, which appear limited to this specific scenario in my opinion.\n3. **Limited number of models, model sizes, and datasets.** As this paper proposes to benchmark vision transformers in machine unlearning, I expect more architectures (e.g., segmentation models, object detection models, self-supervised models, or just more classification models), bigger model sizes (i.e., not limited to 50M parameters), and larger datasets. With such small models and datasets, this paper's findings have a limited impact, as it is unclear whether they can also be observed across multiple models, model sizes, and large datasets. \n\n**Minor weaknesses**\n1. It is unclear what $A$ represents in Eq. (3).\n2. The \"Memorization Proxy Metric\" paragraph requires more context. For instance, it is unclear how ranking is performed, and the meaning of proxy could be better explained.\n3. Pretraining advantage of VTs is not investigated on CNNs; therefore, the overall finding is inconclusive (i.e., this could also be true for CNNs).\n4. It is unclear from Figure 2 whether the unlearning performance is good or bad without showing the original model ToW. Also, in this case, it would be interesting to compare with CNNs; otherwise findings are inconclusive.\n5. I suggest adding TOW, TOW-MIA, and the original model to Table 13.\n6. The paper references multiple times results that are in the appendix. This makes everything hard to follow as it requires the reader to constantly jump from the main paper to the appendix. If a figure is necessary for the main paper's narrative, I suggest including it in the main paper.\n\n[1] Foster, Jack, Stefan Schoepf, and Alexandra Brintrup. \"Fast machine unlearning without retraining through selective synaptic dampening.\" AAAI, 2024.\\\n[2] Chundawat, Vikram S., et al. \"Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher.\" AAAI, 2023.\\\n[3] De Min, Thomas, et al. \"Unlearning personal data from a single image.\" TMLR, 2025.\\\n[4] He, Zhengbao, et al. \"Towards natural machine unlearning.\" TPAMI, 2025."}, "questions": {"value": "1. Are the results presented in Figures 3 and 4 averaged over multiple datasets? If not, then the claim in L.257-261 is not supported.\n2. What is the rationale behind using the RUM unlearning scheme?\n3. Could not the L.451-452 key takeaway be imputed to the retaining steps in NegGrad+?\n4. Table 8 experiments show that for low and medium-memorized samples, the authors used 5 unlearning epochs, while for highly-memorized ones, they used 10. Does an epoch imply an entire iteration of the subset of the forget set and the entire retain set? If so, then the unlearning cost is just half of the original training cost (see Table 7).\n\n**Motivations for my score**\\\nDespite the limited breadth of the benchmark, I believe the main findings are not surprising, nor do I think they are sufficiently interesting to be published on ICLR. Nonetheless, I am willing to reconsider my score based on the opinions of the other reviewers and the outcome of the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S7ZgpMpWfv", "forum": "Vc9yt0YwPw", "replyto": "Vc9yt0YwPw", "signatures": ["ICLR.cc/2026/Conference/Submission12480/Reviewer_ZpV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12480/Reviewer_ZpV7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761055046001, "cdate": 1761055046001, "tmdate": 1762923356769, "mdate": 1762923356769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first comprehensive benchmark of machine unlearning (MU) methods applied to Vision Transformers (ViTs), focusing on how well existing CNN-based unlearning algorithms perform in this new setting. The authors evaluate several representative methods (Fine-tune, NegGrad+, SalUn) across different ViT architectures (ViT, Swin-T), model capacities, datasets, and both single-shot and continual unlearning protocols. They found that ViTs and CNNs exhibit similar memorization behaviors, and that simple measures such as Confidence and Holdout Retraining can effectively assess memorization strength to guide the unlearning process. Among the tested methods, NegGrad+ consistently delivers the most robust unlearning performance, while Swin-T generally outperforms ViT on more complex datasets. Overall, the study provides a reproducible framework for evaluating MU in ViTs and highlights key factors, such as architecture, capacity, and memorization, that shape unlearning effectiveness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper has the following strengths:\n- It is the first systematic benchmark of machine unlearning methods applied to ViT, addressing a clear research gap. Furthermore, it offers a reproducible framework for this purpose\n- the study uses multiple architectures (ViT, Swin-T), datasets of varying complexity, and both single-shot and continual unlearning setups, ensuring robust and generalizable findings.\n- it provides valuable insights into how memorization patterns, model capacity, and architectural design influence unlearning effectiveness"}, "weaknesses": {"value": "The paper has the following weaknesses:\n- While datasets vary in complexity, they remain classification-focused. Extending to other vision tasks (e.g., segmentation, detection) could have broadened the paper's impact.\n- The paper relies heavily on quantitative evaluation. However, more qualitative examples or visual explanations could have made the findings more intuitive."}, "questions": {"value": "Besides the weaknesses mentioned above, here are my other concerns:\n- Extend the continual unlearning experiments beyond five steps to better assess long-term stability and cumulative degradation.\n- Discuss possible causes of degradation (e.g., parameter drift, overfitting) and how they might be mitigated.\n- Incorporate qualitative examples or visualizations showing how unlearning affects image representations or attention maps in ViT/Swin-T\n- In the 'Limitations' section, discuss potential directions for extending the benchmark, e.g unlearning in self-supervised or multi-modal transformer models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ChJqKfaYJ6", "forum": "Vc9yt0YwPw", "replyto": "Vc9yt0YwPw", "signatures": ["ICLR.cc/2026/Conference/Submission12480/Reviewer_wENs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12480/Reviewer_wENs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761474283888, "cdate": 1761474283888, "tmdate": 1762923356306, "mdate": 1762923356306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic benchmark for machine unlearning in Vision Transformers, which extend previous researches in CNNs, LLMs, and diffusion models. The authors benchmark three representative MU algorithms — Fine-tune (FT), NegGrad+, and SalUn — all within the RUM meta-framework that leverages memorization-based partitioning of the forget set. They evaluate these methods across two VT families with different model capacities on multiple dataset. Its insights that CNN-derived algorithms remain effective for VTs and that memorization proxies are transferable is novel"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel benchmark addressing an unstudied area on the machine unlearning for Vision Transformers.\n2. Comprehensive experimental validation across architectures, datasets, algorithms, and proxy metrics.\n3. Novel findings that  CNN-based MU methods can transfer effectively to VTs.\n4. Novel findings that highlighting the distinct unlearning behaviors of ViT and Swin-T."}, "weaknesses": {"value": "1. The study relies on CNN-derived algorithms, with limited exploration of VT-specific unlearning methods. This might weaken the findings that \"CNN-based MU methods can transfer effectively to VTs\".\n2. The downstream tasks are simply classification task, cannot provide these claims also works on segmentation tasks or deepth estimation tasks."}, "questions": {"value": "1. If downstream tasks become more complex like segmentation or deepth estimation, will the findings in paper still work?\n2. Can the observed memorization patterns extend to self-supervised pretrained VTs?\n3. How the self-attention framework in transformers influence the MU and data memorization?\n4. what are the insights behind the findings that ViT achieves stronger results with fine-tuning–based methods while Swin-T performs better with gradient-based unlearning approaches like NegGrad+."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vfIZFvf2c0", "forum": "Vc9yt0YwPw", "replyto": "Vc9yt0YwPw", "signatures": ["ICLR.cc/2026/Conference/Submission12480/Reviewer_X2HB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12480/Reviewer_X2HB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571055020, "cdate": 1761571055020, "tmdate": 1762923355815, "mdate": 1762923355815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}