{"id": "zkAAVuTbwY", "number": 14547, "cdate": 1758238641565, "mdate": 1759897363327, "content": {"title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions", "abstract": "Current large language model (LLM) evaluations primarily focus on single-answer tasks, whereas many real-world applications require identifying multiple correct answers. This capability remains underexplored due to the lack of dedicated evaluation frameworks. We introduce \\method, a benchmark for evaluating LLMs on Select All That Apply (SATA) questions spanning six domains, including reading comprehension, legal reasoning, and biomedicine. Our evaluation of 32 models demonstrates substantial limitations: the strongest model achieves only 75.3% Jaccard Index and 41.8% exact match accuracy. We identify three systematic biases underlying these failures: (i) unselection bias: models systematically avoid certain correct answer choices; (ii) speculation bias: models include incorrect answers when uncertain; and (iii) count bias: models consistently underpredict the number of correct answers. To address these limitations, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding and abstention handling to guide models toward complete and accurate multi-answer selections. Choice funnel improves the accuracy of the exact match by up to 29% while reducing the inference cost by more than 64% compared to the existing approaches. We release \\method and Choice Funnel to encourage the development of LLMs capable of robust decision-making in realistic multi-answer scenarios.", "tldr": "Select All That Apply Benchmark for Multiple Choice Questions for large language models", "keywords": ["LLM Bias", "LLM Evaluation", "SATA", "Debiasing", "Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/896681515f548e9c6d34e59c0a138b40e588290d.pdf", "supplementary_material": "/attachment/8be0f150d19c7fed4cbf145aefa2abfcc1fb4e91.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores multiple-choice question answering in settings where there is more than one correct answer. The authors introduce SATA-BENCH, a select-all-that-apply benchmark for evaluating model performance across domains. Across 32 models and 6 domains, they find that models tend to struggle when tasked with selecting multiple correct answers. Specifically, they find that models tend to (1) select too few options, (2) guess (or, \"speculate\") rather than abstaining when uncertain, and (3) consistently prefer/avoid certain option positions. To counter these three challenges, they propose \"Choice Funnel\", an algorithm that combines three relatively simple approaches (token debiasing methods, a none-of-the-above option, and iteratively selecting the highest probability option) to improve performance on SATA-BENCH, demonstrating positive results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper studies an important problem with real-world implications (e.g., medical diagnosis where multiple conditions could be valid options).\n* The authors make a thorough, end-to-end contribution, spanning benchmark construction, evaluation and a deep-dive on failure modes, and proposing a candidate algorithm to mitigate some observed shortcomings.\n* SATA-BENCH clearly had a lot of time put into producing a high-quality resource, with the authors taking numerous steps to remove too short or trivial samples, reduce contamination, eliminate redundancy, and filter out vague or ambiguous prompts. The approach for balancing difficulty using confusion scores (i.e., similarity between correct and incorrect answer options) is a particularly nice detail.\n* The detailed evaluation of the three biases is informative, and in particular the tension between models that tend to select too few options while also being biased towards guessing. This result is a useful finding that can motivate future research.\n* The authors also find certain counter-intuitive results, such as changing the symbol used to delimit options does not appear to help with SATA-BENCH (in contrast with previous work finding strong sensitivity to option symbol), and that scaling up the number of few-shot examples is of limited utility."}, "weaknesses": {"value": "* Given the presentation of results in Table 2, it is hard to draw any meaningful conclusions about differences between models. In particular, there is no clear winner, and very little consistency in rank ordering of models between the different metrics. Perhaps the authors consider a clearer visualization of what this table is intended to convey.\n* Relatedly, there are an awful lot of metrics deployed, and it's often unclear why multiple are required, given they appear to be different implementations for measuring the same underlying quality. For example, what do we learn by the inclusion of MAP, when we already have FPR? Or, similarly, CtDif and CtDifAbs? It would significantly improve the presentation of the results if the authors were to select and focus on the most appropriate metric, and perhaps include others as ablations in the appendices if necessary.\n* Some of the contributions of the Choice Funnel are a little oversold. Describing adding a NOTA option as \"abstention handling\" (line 74), or taking the highest probability option until hitting a threshold as \"adaptive\" (line 74), obfuscates the simplicity of the algorithm itself. Similarly, the claim that Choice Funnel represents a reduction in inference cost isn't backed up by the data presented in Table 4, where Choice Funnel is the second worst approach in terms of inference cost.\n* The paper is unclear on whether SATA-BENCH comprises 10k samples (as stated in the introduction) or 1.5K evaluation samples (line 166). If it's the former, if only 1.5k samples are used for evaluation, what are the other samples for?\n* The text in Figures 1 and 2 is illegibly small without zooming in.\n\n### Minor\n* Statistical tests are improperly specified. If a t-test is used, the authors should declare whether it was paired or independent, one- or two-tailed, and report both the t statistic and degrees of freedom. This information is frequently missing throughout each mention of a t-test in the paper.\n* Several inline text citations are accidentally in parentheses (e.g. line 150, line 155).\n* Several typos in section 4. Line 431 might be missing some words \"current best-performed method\"; Line 446 is missing an \"a\" before \"t-test\"; etc."}, "questions": {"value": "1. What is the k parameter on line 105? It looks like it's set to c, which would appear to lead to k - c = 0 distractors. Could the authors clarify?\n2. Could the authors provide further detail on the confusion score implementation in lines 153-158? Given the sentence embeddings from ST5-XXL, then what?\n3. How is \"hallucination rate\" (line 297) defined and calculated?\n4. Why is the option set unioned with {NOTA} in the final condition of Algorithm 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LxUn6b1L1d", "forum": "zkAAVuTbwY", "replyto": "zkAAVuTbwY", "signatures": ["ICLR.cc/2026/Conference/Submission14547/Reviewer_iS7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14547/Reviewer_iS7F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761043406282, "cdate": 1761043406282, "tmdate": 1762924936425, "mdate": 1762924936425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SATA-BENCH, a new LLM benchmark for evaluating MCQ problems that can better reflect real-world applications of  multiple choice tasks.\n\nThis benchmark covers 10000 human validation questions across six domains, measuring not only correctness but also three systemic biases observed in LLM: speculation bias, unselection bias and count bias.\n\nThis paper further proposed a decoding method that combines a selection funnel with token debiasing, adaptive thresholding, and abstention handling. Experiments on 32 LLMs showed that compared to strong baselines, Choice Funnel improved accurate matching by 29% and reduced inference costs by 64%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The benchmark is novel in its approach to evaluating LLMs in SATA tasks. The analysis and classification of bias (speculation bias, unselection bias and count bias)  in the execution of SATA tasks by LLM are clear and well-founded.\n\nThe process of constructing the dataset is detailed, covering data filtering, readability scoring, and multiple rounds of manual validation. The experimental design covers a wide range, including 18 closed source models and 14 open source models, all of which prove that this work is solid.\n\nThe writing is relatively clear and accompanied by very detailed and clear appendices.\n\nThe design of the core method Choice Funnel is effective, improving accuracy and inference efficiency. This benchmark has completed the evaluation task of MCQ, providing a new standard and approach for interpretable evaluation of LLM multi answer reasoning, and has a certain academic influence."}, "weaknesses": {"value": "1. The Choice Funnel method relies on token probability and is therefore only applicable to open-source models with accessible probability distributions.\n2. The method is similar to traditional greedy selection and early stopping mechanisms, with high similarity to existing selective prediction or multi label output strategies such as probability thresholding and top-k selection.\n3. There may be significant differences in the threshold parameter τ between different models and tasks.\n4. This paper claims a 64% reduction in inference costs (Table 4), but compared to inefficient yes/no methods that require generating independent responses for each option. Is the cost significantly reduced compared to standard generative reasoning?"}, "questions": {"value": "1. This paper uses metrics such as JI, FPR, EM and Precision to evaluate the performance of the model, but I am curious whether selecting multiple or fewer options incorrectly in certain SATA tasks in specific fields may result in different costs?\n2. Would using more diverse languages to evaluate SATA tasks be more comprehensive?\n3. In the task settings of the \"NON-EXPERT HUMAN BENCHMARK\" mentioned in the paper, it is stated that \"Each question was presented with the original answer options plus decoys (e.g. ABCD→ABCDEFGHIJK) to identify inattentive workers.\" Is the setting of distracting options based on any principle? Is the number reasonable? Based solely on the number of distracting options in this example, I wonder if it will affect the validity of the response data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6ZAAkZY7pW", "forum": "zkAAVuTbwY", "replyto": "zkAAVuTbwY", "signatures": ["ICLR.cc/2026/Conference/Submission14547/Reviewer_Evq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14547/Reviewer_Evq3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477348347, "cdate": 1761477348347, "tmdate": 1762924935493, "mdate": 1762924935493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper make three primary contributions: first, it introduces SATA-BENCH, a \"select-all-that-apply\" benchmark that is harvested from previous benchmarks. Second, it contributes the \"ChoiceFunnel\" algorithm, an algorithm that iteratively selects and eliminates options until a stopping criterion is met.  A reasonably extensive empirical evaluation of the benchmark shows a wide performance spread across models; as a third contribution, the benchmark illuminates several biases in LLMs that negatively impact performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-written and easy to understand. It is easy to see the motivation for the work, and the benchmark itself seems thoughtfully constructed.\n\n* The authors have considered a variety of evaluative methods. It feels comprehensive.\n\n* The authors test against a number of natural baselines.\n\n* The benchmark has two qualities that I think are important for a benchmark: (1) there is room for improvement (but it's not too hard), and (2) it provides discriminative insight across models.  The spread for this benchmark seems good, with the best performers topping out at 75% accuracy, and the lowest performers scoring around 30%.\n\n* The insights about cardinality bias seem fresh.\n\n* The ChoiceFunnel algorithm seems reasonable, although a bit obvious."}, "weaknesses": {"value": "While this is a well-done evaluation of a simple idea, I have two main concerns. The first is just the significance of the work. While I appreciate the careful attention to detail and the reasonably well done empirical work, it seems like a fairly niche question.  Frankly, I don't think the authors do a good job of building the case for the need for this benchmark.\n\nThe second is that this paper seems to be missing an important set of related work, where this problem is termed \"multi-label classification\".  A quick google search turned up these two papers:\n\nLarge Language Models Do Multi-Label Classification Differently\nMarcus Ma*, Georgios Chochlakis*, Niyantha Maruthu Pandiyan, Jesse Thomason, Shrikanth Narayanan\n\nValidate Your Authority: Benchmarking LLMs on Multi-Label Precedent Treatment Classification\nM. Mikail Demir and M. Abdullah Canbaz\n\nwhich both cite many papers that don't seem to appear in the present paper.  I worry therefore, that this paper's claim to novelty isn't a strong as it appears.\n\n--- Additional concerns\n\nI am also worried about the performance of Choice Funnel in the case where the number of correct options is 1.  That is: in the \"real world\", it may not be guaranteed that there is more than one correct answer, and yet as far as I can tell, all of the benchmark questions have at least 2 correct choices.  The ideal algorithm would tolerate both single-answer and multi-answer questions.\n\nI wish the text were more balanced on discussing drawbacks of the proposed ChoiceFunnel algorithm. For example, in the \"Key Observations\" section, the authors extol the positive effects that their algorithm has, without a balanced discussion of negative aspects of the algorithm.  (In other conferences, perhaps these would be included in a \"Limitations\" section).  For example, in my reading of Table 4, the CF algorithm often improves EM accuracy (good) but often reduces precision (bad).  It's not uniformly positive, but the authors don't transparently acknowledge that.\n\n(Along those lines, it feels fair to point out that the yes/no algorithm has the advantage that it can assign independent confidences to each answer, while the CF algorithm cannot).\n\nI'm also not sure how I feel about this sentence: \"Existing SATA datasets, such as (Lewis et al., 2004; Kowsari et al., 2017; Aly et al., 2019; Katakis et al., 2008; Charte et al., 2015), often include over 30 labels per question, making exhaustive prediction impractical for LLMs.\"  I think it's unfair to dismiss this because of computational limitations -- in reality, all of these benchmarks are *tiny* and run very quickly, and I think the point of benchmarks is to assess capabilities and limitations of models, and rarely truly compute bound.  I think it's a bit presumptuous to judge whether the computational disadvantages of an algorithm outweigh its other advantages; that decision should be to the person who actually cares about the tradeoff."}, "questions": {"value": "Can you more strongly distinguish this work from previous work on multi-label classification?\n\nHow does choice funnel perform when there is only one right answer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "siRyVoczCx", "forum": "zkAAVuTbwY", "replyto": "zkAAVuTbwY", "signatures": ["ICLR.cc/2026/Conference/Submission14547/Reviewer_uLiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14547/Reviewer_uLiS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042965265, "cdate": 1762042965265, "tmdate": 1762924935014, "mdate": 1762924935014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce SATA-BENCH, a new benchmark for evaluating Large Language Models (LLMs) on \"Select All That Apply\" (SATA) questions, where multiple correct answers are possible. This work addresses a gap in current evaluations, which primarily focus on single-answer tasks. The benchmark includes over 10,000 human-validated questions (but only 1.47K where annotators consensus is reached) across six domains.\nTheir evaluation of 32 models reveals that even the best-performing models struggle, with the top model achieving only 41.8% exact match accuracy. The paper diagnoses three systematic failure modes: unselection bias (avoiding certain correct choices), speculation bias (including incorrect choices), and count bias (underpredicting the number of correct answers).\nTo address these issues, the authors propose Choice Funnel, a decoding algorithm for open-source models that combines token debiasing, adaptive thresholding, and abstention handling. This method is shown to improve exact match accuracy by up to 29 percentage points and reduce inference costs by 64% compared to a \"yes/no\" baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1)\tThe paper addresses a valid or a good gap. Authors point out that most real-world applications (e.g., content moderation, medical diagnosis, legal research) involve identifying multiple valid \"answers\". Current single-answer benchmarks fail to capture this capability. The creation of a large-scale, human-validated benchmark for this task could be a good resource\n2)\tThe curation of SATA-BENCH is explained clearly, involving a multi-stage process of transformation and filtering based on readability, similarity, confusion score, and human validation . The proposed evaluations of the three biases (unselection, speculation, and count) is also informative. \n3)\tPapaer proposes choice funnel, a decoding strategy that directly targets the identified biases. The ablation studies are insightful, such as the finding that explicitly providing the correct number of answers dramatically improves performance (20.95 point EM increase), which supports their focus on \"count bias.\""}, "weaknesses": {"value": "•  SATA questions can have multiple answers, but it is debatable whether a ranking could exist among the correct answers. A binary prediction of whether a candidate option is correct or not doesn't capture this. This raises a slight concern about the positioning of the multiple-answers task within SATA.\n\n\n•  Further, one of the other important concerns regarding the curation of this dataset is that very little information is given on the annotators. What was their background? Did they have expertise in the domains considered within SATA?\n\n\n• Following up on shortcomings within the annotation process, disagreement issues between annotators are not discussed. It would be important to know what happened in cases where two out of three annotators agreed and the third disagreed. Lastly, in the annotation process, key details (e.g., distractor sampling, domain balancing, and overlap with training data) are not fully explained.\n\n• The non-expert human benchmark is a valuable inclusion, but the results are quite low: 17.9% Exact Match (EM) and 45.0% Jaccard Index (JI). This performance is not dramatically better than that of many of the open-source models. This low score could suggest that many questions in the benchmark are inherently ambiguous or extremely difficult even for humans, which complicates the paper's narrative that the models have \"substantial limitations.\"\n\n• System prompts are not discussed in detail, and there are intuitively several possibilities. For example, instead of “The system prompt specifies that each question has at least two correct answers,” the system prompt could be more generic, instructing LLMs to pick any candidate answer that is correct. A similar gap exists for the CoT prompt."}, "questions": {"value": "Please see the questions in weakness points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5F9DJIQ7Bx", "forum": "zkAAVuTbwY", "replyto": "zkAAVuTbwY", "signatures": ["ICLR.cc/2026/Conference/Submission14547/Reviewer_1sgr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14547/Reviewer_1sgr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073253302, "cdate": 1762073253302, "tmdate": 1762924934236, "mdate": 1762924934236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}