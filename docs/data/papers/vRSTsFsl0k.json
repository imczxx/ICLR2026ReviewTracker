{"id": "vRSTsFsl0k", "number": 8693, "cdate": 1758095210611, "mdate": 1759897769392, "content": {"title": "The Role of Active Learning in Modern Deep Learning", "abstract": "Even though Active Learning (AL) is widely studied, it is rarely applied in contexts outside its\nown scientific literature. \nWe posit that the reason for this is AL's high computational cost coupled with the comparatively small lifts \nit is typically able to generate in scenarios with few labeled points.\nIn this work we study the practical setup of exhausting a fixed budget to label points from a large unlabeled\npool and designing a training pipeline to train the strongest possible model on this small labeled set. \nWe compare the impact of different methods to combat this low data scenario, namely data augmentation (DA), \nsemi-supervised learning (SSL) as options for the training pipeline and AL as selection strategy for the labeled points.\nWe find that AL is by far the least efficient method of solving the low data problem, generating a lift of only 1-4% \nover random sampling, while DA and SSL methods can generate up to 60% lift in combination with random sampling.\nHowever, when AL is combined with strong DA and SSL techniques, it surprisingly is still able to provide improvements.\nBased on these results, we frame AL not as a method to combat missing labels, but as the final building block to \nsqueeze the last bits of performance out of data after appropriate DA and SSL methods as been applied.", "tldr": "Surprisingly, Active Learning still provides lifts, even when combined with optimized SSL and DA techniques.", "keywords": ["Deep Learning", "Semi-Supervised Learning", "Active Learning", "Data Augmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27891357951667635b1a382b951f3eda04003054.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the combination of Active Learning with Data Augmentation and Semi-Supervise Learning in classification tasks (CIFAR10, CIFAR100 and Small LSUN)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.Three classification benchmarks.\n2.Two backbones for experiments."}, "weaknesses": {"value": "1.This paper is not well organized, both writing and format, and the main paper spans only 7 pages, leaving much of the experimental detail to the appendix.\n2.The paper mainly provides an empirical comparison and does not introduce a novel algorithmic contribution or conceptual framework.\n3.The paper claims 20 experimental repetitions per setup, but no error bars, standard deviations, or significance markers are shown in the figures.\n4.Although the paper claims full code availability (Section 8), the provided repository link on anonymous.4open.science is not available.\n5.Task domain in experiment is limited to computer vision."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "54EQLeLZd2", "forum": "vRSTsFsl0k", "replyto": "vRSTsFsl0k", "signatures": ["ICLR.cc/2026/Conference/Submission8693/Reviewer_WQjT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8693/Reviewer_WQjT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926029216, "cdate": 1761926029216, "tmdate": 1762920501330, "mdate": 1762920501330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper benchmarks AL against data augmentation and semi/self-supervised learning under a pool-based setting with a fixed labeling budget. Experiments use CIFAR10, CIFAR100 and a small LSUN subset with ResNet-18/EfficientNet-V2; each acquisition round adds a fixed batch of labeled points, and performance is summarized by the AUC over rounds. when AL is combined with strong DA and SSL techniques, it surprisingly is still able to provide improvements. Based on these results, the paper frame AL not as a method to combat missing labels, but as the final building block to squeeze the last bits of performance out of data after appropriate DA and SSL methods as been applied."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The question mentioned in the paper is important, \"Is AL still worthwhile under modern training pipelines with limited labels?”"}, "weaknesses": {"value": "1. The title/framing (“role of active learning in modern deep learning”) suggests broad conclusions, yet evidence is confined to small. Modern deep learning goes far beyond the experiment the paper conducted. In contrast, the paper scale image classification with limited DA/SSL variants and non-modern backbones. The paper should at least provides experiments on ViT/DeiT. Claims that generalize to “modern pipelines” or provide prescriptive guidance across domains (larger images, detection/segmentation, NLP, multimodal, distribution shift, class imbalance) are not supported by the presented scope. \n2. The paper obtains Insufficient workload as well as limited contribution. The bulk of the effort is engineering rather than advancing methods or analysis. Only small/medium image classification datasets (CIFAR/Small-LSUN) and two CNN-style backbones; no ViT/DeiT or larger-scale settings. \n3. Substantial compute was spent “running baselines,” but the paper adds little in terms of new algorithmic ideas, theoretical insight, or explanatory analysis.\n4. Almost all the references to the pictures and appendices in the text have no attached links, which is not convenient for readers to understand this article."}, "questions": {"value": "1. Can you add ViT/DeiT backbones and at least one larger-scale or distribution-shift/long-tail setting to justify claims about “modern deep learning”?\n2. Include more modern DA/SSL (e.g., RandAugment/CutMix/TrivialAugment, DINO/MAE/DINOv2) to ensure conclusions aren’t an artifact of conservative choices.\n3. Report the impact of acquisition batch size and total label budget on both performance and compute; do method rankings change?\nReduce overclaim: Reframe conclusions to explicitly state the narrow domain where they are supported, or provide new experiments/diagnostics (e.g., uncertainty calibration, representation analyses) to substantiate broader claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ym0PIwp2jy", "forum": "vRSTsFsl0k", "replyto": "vRSTsFsl0k", "signatures": ["ICLR.cc/2026/Conference/Submission8693/Reviewer_kzzo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8693/Reviewer_kzzo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002687542, "cdate": 1762002687542, "tmdate": 1762920500963, "mdate": 1762920500963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "On this work, the authors do an empirical study in finding the advantages/usability of active learning, and comparing it with using instead data augmentation and semi-supervised learning. The main premise of the paper, is that compared to the two other approaches, active learning does not give significant performance boost, while at the same time not improving the performance when combined with the other two techniques. They validate this in the experimental section."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Readability - While short, the paper is well-written and very easy to understand. Both the diagrams and the figures support the writing of the paper."}, "weaknesses": {"value": "Unfortunately, I think the paper has several big weaknesses, that make it not suitable for publication:\n\n1) In some ways, the paper posits the story as active learning, data augmentation and semi-supervised learning are distinct approaches, with the models using one of them. This is fundamentally wrong. Pretty much any active learning method is always combined with a data augmentation method both in practice and in research papers. Data augmentation is also inherently a part of semi-supervised learning (see all consistency methods, or teacher-student models that use strong/weak augmentations). Furthermore, several methods combine active learning with semi-supervised learning showing that the final performance significantly improves over both strategies in isolation. So\n\n[A] Gao et al., Consistency-based semisupervised active learning: Towards minimizing labeling cost, ECCV 2020 (270 citations).\n[B] Yu et al., Consistency-based Active Learning for Object Detection, CVPRw 2022 (85 citations)\n[C] Elezi et al., Not All Labels Are Equal: Rationalizing The Labeling Costs for Training Object Detection, CVPR 2022 (64 citations).\n\nThese works are neither recent, nor fringe, considering that they have been published in top-tier venues several years ago and are quite cited). They all show the same thing: (1) Active learning in isolation does not perform as well as semi-supervised learning; (2) They all are trained together with some data augmentation strategy; (3) They show how active learning combined with semi-supervised learning improves over either in isolation. (1) and (2) make this paper's claims redundant, while (3) makes them wrong.\n\nFrom my personal experience, using active learning in both research and industry, active learning improves when combined with semi-supervised learning.\n\n2) The datasets used for experiments are outdated and small. At the very least, the authors should have used ImageNet for classification (which nowadays is also quite a small and outdated dataset). I do not think modern papers can be evaluated in CIFAR datasets when it comes to top-tier publications.\n\n3) From the paper title and abstract, this is an active learning paper. However, everything is set in the context of classification. This should have been clarified in the title and abstract, that it is about active learning in classification. Or ideally, show experiments in other domains such as at the very least detection, but also segmentation, tracking, etc if it is meant to be a general active learning paper.\n\n4) The title is about active learning in modern deep learning. But, the experiments are mostly in using small CNNs that are a decade ago such as ResNets. There are no Transformers in the paper. In fact, from the title, I was quite excited and was expecting active learning in the context of LLMs or VLMs, in what is basically modern deep learning."}, "questions": {"value": "I do not have any questions, but I do not think the paper is suitable for a top-tier publication. The claims are wrong, the experiments are limited in small datasets and CNNs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MiwSY7sJMn", "forum": "vRSTsFsl0k", "replyto": "vRSTsFsl0k", "signatures": ["ICLR.cc/2026/Conference/Submission8693/Reviewer_tt6Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8693/Reviewer_tt6Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762516668913, "cdate": 1762516668913, "tmdate": 1762920500539, "mdate": 1762920500539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how effective active learning really is when compared with other methods for dealing with limited labeled data. The authors explore three main techniques, data augmentation (DA), semi-supervised learning (SSL), and active learning (AL), in realistic small-label settings. They find that while DA and SSL can together improve model performance by up to 60%, AL only adds a small 1–4% gain over random sampling. However, when combined with strong DA and SSL pipelines, AL can still provide marginal improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper offers a comprehensive, well-designed study of how AL performs in modern deep learning settings that already use DA and SSL. Its originality lies in shifting the focus from proposing new algorithms to questioning the practical value of AL under realistic conditions. The experimental setup is solid and carefully controlled across datasets and models, which makes the conclusions credible. The results are clearly presented and give an honest, data-driven picture: while DA and SSL deliver large gains, AL offers only a small improvement."}, "weaknesses": {"value": "1. Although the paper aims to evaluate the practical usefulness of AL, its experimental design assumes that we can first fully tune and identify the best DA and SSL combination for each dataset before applying AL. This actually contradicts the real motivation for AL, which is to operate in scenarios with limited labels and limited resources, where such exhaustive tuning is infeasible. In practice, DA, SSL, and AL are usually co-designed and co-adapted, not applied sequentially in a pipeline where DA/SSL are fully optimized first.\n2. According to the experimental figures (e.g., Figure 3, Figure 13, Figure 25), the starting points across methods are not aligned, which suggests that the 20 repeated runs do not use paired random seeds and therefore each method begins with a different initial labeled pool. Since no acquisition occurs in round 0, all methods should theoretically start from identical performance when DA, SSL, and training protocols are fixed. More importantly, the paper reports AL gains of only 1–4%, which is comparable to the expected fluctuation caused by different initial labeled pools. Without controlling for seed alignment, it is unclear whether these small gains truly reflect the effect of AL or merely dataset-level randomness. \n3. Some AL strategies evaluated in the paper, such as BALD, Margin Sampling, BADGE, and CoreSet, rely on specific assumptions about the geometry of the feature space (e.g., uncertainty calibration, margin smoothness, linear separability, gradient embedding structure). However, strong SSL methods like SimCLR are known to significantly reshape the representation space by increasing local density, smoothing decision boundaries, and altering cluster structure. These shifts can directly affect whether AL’s underlying assumptions still hold, and the paper does not examine how SSL transforms the feature space or how such transformations influence AL behavior."}, "questions": {"value": "1. This is related to Weakness 3, embedding visualizations should be added, without it, it is unclear whether the AL methods are being evaluated in conditions where they remain theoretically appropriate.\n2. This is related to Weakness 1, since all experiments are repeated 20 times, adding error bars (or shaded confidence intervals) to the accuracy curves would make it easier to judge whether the differences between methods, especially the small 1–4% AL gains, would be more meaningful.\n3. The current benchmarks (CIFAR, SVHN, FashionMNIST) are small, clean, and balanced, where strong SSL methods are known to perform extremely well. This leaves very little room for AL to demonstrate meaningful gains. Have the authors considered evaluating AL under more challenging datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "g5tWwmPOwl", "forum": "vRSTsFsl0k", "replyto": "vRSTsFsl0k", "signatures": ["ICLR.cc/2026/Conference/Submission8693/Reviewer_kBGg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8693/Reviewer_kBGg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762701722056, "cdate": 1762701722056, "tmdate": 1762920500206, "mdate": 1762920500206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether active learning (AL) is still useful when combined with modern techniques like data augmentation (DA) and semi-supervised learning (SSL). Their conclusion: Active learning is the least efficient way to handle low-data regimes and should only be used after applying DA and SSL to squeeze out the last few percentage points of performance. They call for a paradigm shift, arguing that developing or evaluating AL methods without modern DA and SSL is no longer scientifically valid."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors evaluate a broad set of AL algorithms (BADGE, Galaxy, CoreGCN, uncertainty sampling variants) across multiple datasets and architectures (ResNet18, EfficientNetV2), under consistent, reproducible conditions. They incorporate DA and SSL in a structured, layered fashion and repeat all experiments 20 times — an almost unheard-of level of rigor in this domain."}, "weaknesses": {"value": "* The paper sometimes feels like a manifesto disguised as a benchmark. Its stance - that AL research should stop ignoring DA/SSL - is correct but somewhat overstated. There might still exist niches where AL’s cost-benefit calculus makes sense (e.g., expensive medical labeling), and the paper’s rhetoric risks alienating parts of the community that could instead build on its insights.\n* The combination of Active Learning and Semi-Supervised Learning has already been explored in prior literature. The authors should better clarify how their work differs from, or extends beyond, existing studies in this area.\n* The figures are poorly crafted.\n* This paper only has 7 pages.\n* This paper seems not finished yet.\n* Another limitation is that the study remains vision-centric."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ArviLrNgit", "forum": "vRSTsFsl0k", "replyto": "vRSTsFsl0k", "signatures": ["ICLR.cc/2026/Conference/Submission8693/Reviewer_xioc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8693/Reviewer_xioc"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8693/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762959487882, "cdate": 1762959487882, "tmdate": 1762959487882, "mdate": 1762959487882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}