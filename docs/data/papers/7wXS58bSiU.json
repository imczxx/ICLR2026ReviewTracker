{"id": "7wXS58bSiU", "number": 19609, "cdate": 1758297630098, "mdate": 1759897030742, "content": {"title": "Interactive Object Grounding Using Image-Grounded Scene Graphs and Prompt Chaining", "abstract": "We introduce the task of Interactive Object Grounding, i.e., linking referring expressions in natural language instructions to objects in the physical environment and using clarification to handle ambiguities.  Although recent foundation models can be used to perform this task in a straightforward manner, we observe that they tend to generate lengthy and sometimes confusing clarification questions.  Moreover, they require many input images to fully cover complex scenes, resulting in high processing costs.  Alternative approaches use a scene graph instead of images to represent the environment, but these are inhibited by relying on predefined sets of object properties and spatial relations.  Instead of end-to-end VLM prompting with many images, or LLM prompting using a text-only scene graph, we propose a prompt chaining method that utilises multimodal information sampled dynamically from an Image-Grounded Scene Graph (IGSG), leveraging existing LLMs/VLMs to perform object grounding and clarification question generation more effectively.  Evaluations based on 3D scenes from ScanNet show that the proposed method outperforms an end-to-end baseline that does not use a scene graph, at only 35% of the cost.  Furthermore, it achieves substantial improvements in grounding F-score through clarification, both with our simulated user (up to 34% gain) and with human subjects (up to 23.6% gain).", "tldr": "", "keywords": ["object grounding", "scene graph", "prompt chaining", "reference resolution", "captioning", "clarification", "dialogue", "disambiguation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5285bf501f3a1f6a735ae4fa57eb5155ed703d32.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an Interactive Object Grounding (IOG) framework that combines a novel Image-Grounded Scene Graph (IGSG) with a prompt chaining strategy to perform multimodal reference resolution and clarification dialogue in 3D environments. The method integrates both LLM and VLM modules for object identification and clarification question generation. Unlike end-to-end visual grounding approaches that require numerous images, IGSG dynamically samples relevant multimodal information, thus reducing computational cost while improving interpretability and dialogue quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Defines Interactive Object Grounding, bridging vision-language grounding with interactive dialogue.\n\n2. Introduces IGSG, a lightweight yet expressive multimodal representation that supports dynamic image selection and zero-shot multimodal reasoning via prompt chaining."}, "weaknesses": {"value": "1. This work mainly applies existing models to a new robotics application. However, the scenario does not appear to present significant challenges, and the proposed solution seems to be a straightforward extension to this setting.\n\n2. Only 8 participants were involved, limiting statistical significance and ecological validity.\n\n3. The contribution of each module (IGSG, prompt chaining, clarification strategy) is not individually quantified.\n\n4. The multi-step prompting pipeline, while elegant, may be difficult to scale to real-time robotic applications.\n\n5. The work does not include comparisons with recent multimodal dialogue systems such as SIMMC or clarification-based interactive grounding datasets (e.g., IGLU, PhotoBook)."}, "questions": {"value": "Refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9hFvqOpPCH", "forum": "7wXS58bSiU", "replyto": "7wXS58bSiU", "signatures": ["ICLR.cc/2026/Conference/Submission19609/Reviewer_yDiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19609/Reviewer_yDiR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639901662, "cdate": 1761639901662, "tmdate": 1762931469233, "mdate": 1762931469233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a task called Interactive Object Grounding, which is similar to conventional visual grounding but requires the agent to follow up and ask clarifying questions when the description is ambiguous and may not refer to a single object. To address this task, the authors propose a system that utilizes prompt chaining over an Image-Grounded Scene Graph (IGSG). A benchmark is also provided, built on ScanNet, and designed for ambiguous same-type objects (65 scenes, 263 tasks, with an average of 1.33 distractors per task)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The proposed task and problem is useful and interesting."}, "weaknesses": {"value": "1. The paper introduces a new task, a method, and a benchmark, but none of them are clearly or thoroughly explained.\n\n2. The authors do not provide sufficient comparison or discussion of existing work in single- or multi-object grounding. While the proposed setting is indeed different and direct comparison may not be entirely fair, some level of quantitative or qualitative comparison and discussion is still necessary to position their contribution.\n\n3. There is also limited quantitative evaluation of existing methods to support their claim that previous approaches “generate lengthy and sometimes confusing clarification questions.” Although the claim is reasonable, the paper does not demonstrate how severe the issue is or how much their method improves upon it. Beside it seems that only GPT-4.1 is tested here.\n\n4. The average number of distractors is only 1.33, which makes the task relatively simple and may not effectively test the model’s ability to disambiguate the target in more complex or cluttered scenes."}, "questions": {"value": "Please address the weakness mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zNmf9XWRhq", "forum": "7wXS58bSiU", "replyto": "7wXS58bSiU", "signatures": ["ICLR.cc/2026/Conference/Submission19609/Reviewer_GA34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19609/Reviewer_GA34"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744108134, "cdate": 1761744108134, "tmdate": 1762931468576, "mdate": 1762931468576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of interactive object grounding, where a system must link a user's linguistic expression to an object in a 3D scene and ask for clarification if the expression is ambiguous. The authors note that end-to-end VLM-based systems can be costly and produce poor clarification questions, while text-only scene graph methods lack rich visual detail. They propose a modular, prompt-chaining approach centered on an \"Image-Grounded Scene Graph\" (IGSG). This IGSG stores basic object properties and maps objects to relevant images, allowing the system to dynamically sample and feed specific multimodal information to LLM/VLM modules for reference resolution and clarification question generation. Evaluations on a new benchmark derived from ScanNet show the IGSG method outperforms an end-to-end baseline in grounding accuracy, particularly after clarification, and at a significantly reduced computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of the Image-Grounded Scene Graph (IGSG) is an intuitive and practical compromise. It avoids the high cost of feeding all scene images to a VLM while retaining richer visual grounding than purely symbolic scene graphs.\n2. The performance gains in terms of computational cost (reportedly 35% of the cost of the baseline) could be practically relevant for real-world applications.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The primary baseline (E2E) feels insufficient. It is not surprising that a single, general-purpose VLM (GPT-4.1) prompted end-to-end performs poorly on this specialized, multi-turn task compared to a purpose-built, modular pipeline that uses the same model for its components. A stronger baseline might have involved other 3D grounding methods adapted for this interactive task, or a fine-tuned model.\n2. The evaluation with human subjects, while a good inclusion, is very small (8 subjects from within the authors' organization). This small sample size and potential for in-group bias limit the conclusiveness of these findings. The paper also notes that \"human users struggled to grasp the complex 3D scenes,\" which may also suggest issues with the user interface or task setup.\n3. The method relies on \"gpt-4.1\" (presumably a specific version of GPT-4), a powerful, closed-source model. This makes exact replication difficult and ties the method's success to a specific proprietary API, limiting insights into how it would perform with other models."}, "questions": {"value": "1. How dependent is the IGSG method's success on the specific VLM used? Have the authors experimented with other open-source models for the VLM-dependent modules?\n2. The IGSG requires pre-processing of the scene (point cloud, segmentation, mapping images to objects). What is the computational overhead of creating the IGSG for a new scene, and how does this pre-processing cost compare to the per-dialogue savings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ESPujNdxNC", "forum": "7wXS58bSiU", "replyto": "7wXS58bSiU", "signatures": ["ICLR.cc/2026/Conference/Submission19609/Reviewer_17AJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19609/Reviewer_17AJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967801410, "cdate": 1761967801410, "tmdate": 1762931468063, "mdate": 1762931468063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores interactive object grounding in 3D scenes. It proposes an Image-Grounded Scene Graph (IGSG) framework that dynamically selects relevant visual information through prompt chaining, enabling effective collaboration between LLMs and VLMs. Evaluated on a subset of ScanNet with distractors, the proposed method significantly improves clarification and grounding accuracy compared to an end-to-end baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is solid and well-justified, effectively grounding ambiguous natural language instructions in complex 3D environments is a key challenge for embodied AI.\n2. The proposed Image-Grounded Scene Graph (IGSG) offers a dynamic approach to integrating multimodal information via prompt chaining, allowing LLMs and VLMs to collaborate effectively without relying on a rigid symbolic scene graph.\n3. The method achieves strong performance while maintaining a significantly lower computational cost compared to e2e baseline."}, "weaknesses": {"value": "1. The newly formatted dataset with 263 instances is relatively small, which weakens the strength of the paper’s experimental statement.\n2. Error propagation during interaction is not clearly discussed. What happens if the predicted object ID list is incorrect at the beginning of the dialogue?\n3. The comparative analysis is limited, as the method is only compared against an e2e baseline. It would be more convincing to include additional baselines, such as the text-SG model mentioned in the paper."}, "questions": {"value": "1. It would be helpful to include more diverse 3D scenes from different sources to further validate the effectiveness of the proposed method.\n2. It is suggested that the authors provide some real-world applications to demonstrate the importance of question clarification in embodied AI."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YFmFUoyi4W", "forum": "7wXS58bSiU", "replyto": "7wXS58bSiU", "signatures": ["ICLR.cc/2026/Conference/Submission19609/Reviewer_ysk7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19609/Reviewer_ysk7"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974291756, "cdate": 1761974291756, "tmdate": 1762931467678, "mdate": 1762931467678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}