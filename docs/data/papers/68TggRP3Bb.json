{"id": "68TggRP3Bb", "number": 4909, "cdate": 1757798200424, "mdate": 1759898005673, "content": {"title": "Scaling Law for Catastrophic Forgetting via Gradient Products", "abstract": "Catastrophic forgetting occurs when models lose performance on previously learned tasks after acquiring new ones. Although larger models are empirically observed to forget less, the theoretical origin of this effect remains unclear. In this work, we analyze forgetting in simple linear and nonlinear teacher-student models and introduce a gradient-product proxy that closely tracks forgetting. This formulation allows us to decompose the phenomenon into its main components. We show that the orthogonality of the output heads is the necessary condition underlying the $1/d$ scaling law of forgetting, where $d$ is the hidden dimension. Other factors, such as initialization, task similarity, network architecture, and activation functions, play a secondary role, modulating but not overturning the dominant scaling behavior. Our results provide a step toward a principled understanding of how model capacity mitigates forgetting and clarify the role of gradient interference in continual learning.", "tldr": "", "keywords": ["Catastrophic Forgetting"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e087f0dea5bf0a2da40b756a96f20f76d21cd1fe.pdf", "supplementary_material": "/attachment/025389d69be2bc8324a68441f35fe032c9b7027b.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to elucidate the mechanism underlying the empirical observation that larger neural networks are less prone to *catastrophic forgetting* in continual learning.\n\nUsing a teacher–student setup with linear models and nonlinear MLPs (up to three layers) as toy models, the authors compute the inner product of gradients between different tasks (referred to as a proxy) via numerical simulation. The results show that this proxy scales as $1/d$ with respect to the network width $d$, which is argued to align with empirically observed resistance to forgetting."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis of catastrophic forgetting from the perspective of scaling laws with respect to model size is interesting and potentially valuable to the community.\n- The related work section is well summarized."}, "weaknesses": {"value": "- **Insufficient motivation:** The paper defines and focuses on the gradient inner product $G$ between different tasks as a proxy for catastrophic forgetting, but the motivation for choosing this quantity is not sufficiently explained. Why should the magnitude of this quantity, whether large or small, be expected to influence catastrophic forgetting? Since the paper studies analytically tractable models including linear ones, this connection should be established in a fully analytical manner.\n- **Lack of theoretical depth:** The abstract and introduction claim that the paper investigates the theoretical origin of the observed scaling law and identifies the $1/d$ scaling as arising from alignment of output heads. However, this claim is not sufficiently substantiated for several reasons:\n    - Although the paper lists theoretical analysis as its main contribution, in practice it merely derives the gradient inner product for linear or one-layer MLP models and conjectures its order with respect to the width $d$. Such derivations alone do not constitute a strong theoretical contribution, and the conjecture proposed in Section 3 is not rigorously validated (as explained below).\n    - The analysis and experiments are conducted under the assumption that all output heads are *frozen* during training, motivated by the desire for the shared feature extractor to learn task-specific representations. However, the claimed $1/d$ scaling fundamentally relies on this assumption. If the output heads are also trained, then $\\mathcal{v}$ and $X$ become correlated, and one can no longer take expectations separately as in Section 3, invalidating the claim. Consequently, one cannot help but conclude that this paper's arguments heavily depend on unrealistic assumptions. In realistic continuous learning settings, it's common to train the output head as well, so the paper should discuss what occurs when this assumption is removed.\n    - While the paper states that it considers MLPs up to three layers, the theoretical results are only derived for one-layer MLPs.\n    - The scaling behavior with respect to model width is known to depend on the choice of parameterization, particularly $\\mu$-parameterization. Since the paper’s discussion strongly depends on the initialization scaling of the final layer, the authors should also examine whether the same scaling behavior holds under $\\mu$-parameterization in addition to Kaiming or constant initialization.\n    - If the claimed $1/d$ scaling indeed arises from alignment of the output heads, then choosing an initialization scaling that cancels this effect should remove the dependence of forgetting on model width. It would be interesting to test this hypothesis.\n- **Inconsistency between theory and results:** Section 3 states that for linear models the expected value of the proxy $G$ is zero and its variance scales as $1/d$. However, in Figure 1, the empirical mean of $G$ clearly decreases with increasing model width, which contradicts the theoretical claim. Either the theoretical argument or the experimental results must therefore be incorrect or incomplete.\n- **Unclear presentation of experimental claims:** In Figure 1, the left panel shows forgetting on a linear scale, while the middle panel shows the proxy on a logarithmic scale. Both quantities decrease with increasing $d$, suggesting qualitative correlation. However, this only indicates that the two quantities both decrease, not that they share a $1/d$ scaling. The claimed scaling law is therefore not convincingly demonstrated.\n- **Insufficient experimental validation:** While toy models are acceptable for theoretical analysis, the paper should also test whether the claimed scaling holds in realistic settings. Without such validation, it remains unclear whether the $1/d$ scaling is a genuine phenomenon in practical continual learning.\n- **Writing is not clear**:\n    - The loss function used in the definition of the proxy $G$ is not correctly rendered: subscripts such as $t$ and $\\Theta$ are not displayed properly.\n    - The overall presentation quality is insufficient. For example, figure captions should be written as complete descriptive paragraphs rather than bullet points."}, "questions": {"value": "- Undefined notation: The meaning of $\\mathcal{v}_{t,t'}$ in line 156 is unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BjwPTngG39", "forum": "68TggRP3Bb", "replyto": "68TggRP3Bb", "signatures": ["ICLR.cc/2026/Conference/Submission4909/Reviewer_WHzo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4909/Reviewer_WHzo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668342415, "cdate": 1761668342415, "tmdate": 1762917754703, "mdate": 1762917754703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates an apparent scaling law according to which forgetting decreases as 1/d where d is the dimension of a linear model or the hidden dimension of simple MLPs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper deals with an important aspect of continual learning, i.e., the effect of the model capacity on catastrophic forgetting. In particular, the 1/d scaling law is an important phenomenon that should be learned (however, it already was in several other papers)."}, "weaknesses": {"value": "1. **Contribution and comparison to prior work.** The contribution is unclear in light of several prior work, which studied this question more in depth, and are only partly cited. I do not see a significant empirical contribution given the cited work of Mirzadeh et al. (2022); The reviewed manuscript’s explanation in Section 6.1 is not convincing. Specifically, what do the authors “link gradient behavior more directly to forgetting” compared to Section 4.1 in Mirzadeh et al. (2022)?\n\nMoreover, the authors overlook closely related analytical papers [1,2] that analyzed the joint effect of overparameterization and task similarity on forgetting, and provided rather nuanced behavior. \n\n[1] Goldfarb and Hand, \"Analysis of Catastrophic Forgetting for Random Orthogonal Transformation Tasks in the Overparameterized Regime,\" 2023.\n[2] Goldfarb et al. \"The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting — An Analytical Model\", 2024\n\n2. **Soundness and presentation.** Findings are presented rather hastily, in a structure that might suit a short paper more than a conference paper. Many details are unclear, e.g., see our questions below. The empirical methodology is sometimes insufficient (e.g., the empirical setting requires presenting and discussing the average loss, not only the forgetting. Also, how it can be determined from the graph that the decay is $1/d$ and not $1/\\sqrt{d}$). A better discussion of the results is needed (e.g., what should we ‘learn’ from Figure 3? Why do we see *less* forgetting when *more* pixels are permuted?). Several presentation choices are odd (e.g., the partition of Sections 3-4 create a mix of models, and experimental setups).     \n\nFurthermore, some claims lack proper backing. For example, Lines 163-168 say that “those binary matrices do not introduce additional 1/d scaling” and that “we deduce that the 1/d scaling has its origin in the output heads”—the first part is not proven, and the second part treats the said scaling as a ‘fait accompli’, but this is not the case. Line 181 implies that the 1/d scaling relates to “the curse of dimensionality”, but it actually shows the opposite, as it makes the forgetting diminish, which is favorable."}, "questions": {"value": "Beyond the questions in the weakness section:\n\nQ1) Are the output heads trained or just initialized randomly ? \nQ2) Line 150, the definition of $G_{linear}$- why is $K_{linear}$ introduced? There is also a typo. \nQ3) In Equation (4), why does dividing by $d$ corresponds to averaging? average should also include a summation.\nQ4) How is the forgetting in the plots defined? What is ‘layer contribution’ in Figure 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kLoHJ2hXbv", "forum": "68TggRP3Bb", "replyto": "68TggRP3Bb", "signatures": ["ICLR.cc/2026/Conference/Submission4909/Reviewer_ZtBY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4909/Reviewer_ZtBY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750451913, "cdate": 1761750451913, "tmdate": 1762917754393, "mdate": 1762917754393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how the size of the hidden dimension affects catastrophic forgetting (CF). To isolate these effects, the authors focus on very simple models-linear networks or shallow MLPs-composed of a shared feature extractor and task-specific output layers that are kept frozen. Under this artificial constraint, the “curse of dimensionality” implies that the gradient inner product (used as a proxy for forgetting) scales as $1/d$, where $d$ is the hidden dimension. The authors derive this analytically for simple settings and validate it empirically. Their results confirm that the averaged proxy $\\bar{G}$ is a useful measure of forgetting and explain its scaling with $d$. However, the observed $1/d$ scaling arises specifically because the output layers are frozen at initialization, making the effect somewhat artificial."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Using the gradient product proxy specifically on the feature extractor is an interesting and insightful idea. \n- The use of the teacher–student scenario is also a potentially valuable angle for understanding CF. Although the paper does not fully leverage this setup, the direction itself is promising."}, "weaknesses": {"value": "- A central limitation of the work is the decision to keep the output layers frozen. This constraint is the main reason behind the observed scaling with the hidden dimension $d$, but the effect is artificially introduced rather than emerging from a realistic learning setup. While prior work (e.g., Mirzadeh et al.) has shown that forgetting decreases with width, the specific scaling reported here is driven by this artificial constraint.\n- The use of the Teacher–Student setup is also not justified. It is neither exploited in the theoretical analysis nor in the experiments, and it is unclear why the authors do not train directly on the dataset given that the TS framework does not bring any apparent benefit.\n- Finally, the presentation suffers from avoidable issues. Key quantities are poorly introduced or misplaced: forgetting is only defined in the appendix (but it’s the main quantity of the figures) and not in the main text. There are also some typos, for example in the definition of the proxy $G$ (around line 150 and in Equation 3).\n\nReferences:\nMirzadeh et al. \"Wide neural networks forget less catastrophically.\" International conference on machine learning. PMLR, 2022."}, "questions": {"value": "Following up on the weaknesses raised, I would like the authors to clarify the following points:\n- Teacher–Student setup: Why was the TS framework used in the first place? Since it is not exploited in the theoretical or empirical parts, what is the motivation behind this choice?\n- Frozen output heads: Have you tried training the output heads instead of freezing them? If so, does the observed scaling with the hidden dimension change?\n- Exact scaling check: Why don’t you rescale the relevant quantities to directly verify whether the scaling is exactly $1/d$?\nProxy vs. forgetting discrepancy:\n- You state that “as model complexity increases, discrepancies between the proxy and the measured forgetting become pronounced.” Can you clarify what you mean by this, and from what do you draw this conclusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "raVpWY515V", "forum": "68TggRP3Bb", "replyto": "68TggRP3Bb", "signatures": ["ICLR.cc/2026/Conference/Submission4909/Reviewer_92oQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4909/Reviewer_92oQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842424752, "cdate": 1761842424752, "tmdate": 1762917754135, "mdate": 1762917754135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}