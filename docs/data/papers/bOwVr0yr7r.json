{"id": "bOwVr0yr7r", "number": 6005, "cdate": 1757950443525, "mdate": 1759897939948, "content": {"title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning", "abstract": "Reinforcement learning from verifiable rewards has emerged as a powerful technique for enhancing the complex reasoning abilities of Large Language Models (LLMs). However, these methods are fundamentally constrained by the ''learning cliff'' phenomenon: when faced with problems far beyond their current capabilities, models consistently fail, yielding a persistent zero-reward signal. In policy optimization algorithms like GRPO, this collapses the advantage calculation to zero, rendering these difficult problems invisible to the learning gradient and stalling progress. To overcome this, we introduce Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive training framework that strategically provides minimal guidance only when a model's independent learning has plateaued. The framework first diagnoses learning stagnation and then intervenes by injecting tiered in-prompt hints, ranging from abstract concepts to concrete steps, enabling the model to construct a valid solution by itself. Extensive experiments on challenging mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative 44.3\\% over a vanilla GRPO baseline. This result demonstrates our framework provides a robust and effective methodology for unlocking a model's ability to solve problems previously beyond its reach, a critical step towards extending the frontier of autonomous reasoning in LLM.", "tldr": "Scaf-GRPO boosts LLM reasoning in RLVR, using hierarchical hints to guide on-policy GRPO on difficult problems.", "keywords": ["LLM Reasoning", "Reinforcement Learning from Verifier Rewards", "Mathematical Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a69c2f6a341187a246350cf4aa447df13523cf8.pdf", "supplementary_material": "/attachment/f111fa6ead19f099e9434f1d945d280fd33e2f8d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Scaffolded Group Relative Policy Optimization (Scaf-GRPO), a novel framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by overcoming the \"learning cliff\" problem in reinforcement learning. This problem arises when an LLM consistently fails to solve difficult problems, resulting in a zero-reward signal that provides no learning gradient. Scaf-GRPO addresses this by employing a two-phase training process. The first phase, a \"guidance exemption period,\" allows the model to learn autonomously on easier problems. In the second phase, for \"true-hard\" problems where learning has stagnated, the framework provides minimal, hierarchical in-prompt hints (Knowledge -> Planning -> Solution) until the model can generate a correct solution. By augmenting the training batch with this minimally-guided successful trajectory, Scaf-GRPO restores the learning gradient while preserving the on-policy nature of the learning algorithm and the model's exploratory autonomy. The authors demonstrate through extensive experiments on various math benchmarks that their method significantly outperforms vanilla GRPO and other prefix-based guidance methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly formalizes the \"learning cliff\" phenomenon (persistent zero-reward signals) as a critical bottleneck in training LLMs with reinforcement learning. The proposed solution, inspired by the pedagogical concept of scaffolding, is both elegant and intuitive. Instead of forcing the model down a fixed path with prefix-based solutions, Scaf-GRPO provides minimal, progressively more concrete hints, which preserves the model's ability to explore its own reasoning paths. This is a significant conceptual departure from prior work like LUFFY that relies on off-policy prefix-continuation.\n\n2. The authors conduct a meticulous set of ablation studies that convincingly validate their key design choices. Table 2 effectively demonstrates that each level of the hint hierarchy (Knowledge, Planning, Solution) is a necessary and complementary component of the framework. Furthermore, the analysis in Appendix G (Table 7) confirms the necessity of the \"guidance exemption period,\" showing that applying scaffolding from the start leads to inferior performance by fostering over-reliance on hints. These studies provide strong evidence that the framework's components are well-justified and contribute to its overall effectiveness."}, "weaknesses": {"value": "1. The framework's success is contingent on the availability of a high-quality, three-tiered hint hierarchy. As the authors acknowledge, generating these hints requires a \"non-trivial data preparation effort,\" involving a powerful teacher model (DeepSeek-R1) and a highly structured prompting process. This dependency limits the scalability and practical applicability of Scaf-GRPO to domains where such structured hints can be easily and reliably generated. It moves the difficulty from the RL training phase to a complex and potentially expensive data curation phase.\n\n2. The hints are generated by a single, powerful teacher model. This introduces the risk that the scaffolding will simply teach the student model the specific reasoning patterns and biases of the teacher. While Scaf-GRPO aims to preserve exploratory autonomy, the guidance is still fundamentally rooted in the teacher's \"worldview.\" The paper does not explore the effects of hint quality or potential inaccuracies in the teacher-generated hints on the final performance of the student model."}, "questions": {"value": "1. Regarding training efficiency and comparative fairness: During the training process, what is the approximate proportion of \"all-failure\" batches that trigger hierarchical hint-guided exploration? For these batches, Scaf-GRPO performs multiple on-policy rollouts with hints until a successful solution is found. In contrast, Vanilla GRPO conducts only one ineffective rollout when encountering the same situation. This suggests that Scaf-GRPO consumes more computational resources (time and computing power) when addressing challenging problems. Under the same training duration or total computational budget, can Scaf-GRPO still maintain a significant advantage?\n\n2. Comparison of effectiveness with DAPO.\n\n3. Can Scaf-Rollout be integrated as a component with current state-of-the-art model methods, such as DeepScaler and Skywork?\n\n4. This work lacks detailed comparisons with existing mathematical reasoning studies, such as representative works on long-chain and short-chain reasoning. These results on the test set are not particularly good.\n\n5. The pass@1 metric exhibits considerable fluctuations and requires repeated evaluations to obtain a reliable average.\n\n6. The \"guidance exemption period\" is empirically set to the initial 15% of training steps. How was this value determined? Is it a sensitive hyperparameter, or does the method perform robustly across a range of values? \n\n7. Figure 5 illustrates a compelling case of a model internalizing a skill. How frequently does this \"graduation\" from hint-dependency to autonomous competence occur in practice? Is it possible to track this metric during training to dynamically adjust when to offer hints for certain problem types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Uz9EGGaB7a", "forum": "bOwVr0yr7r", "replyto": "bOwVr0yr7r", "signatures": ["ICLR.cc/2026/Conference/Submission6005/Reviewer_9rLe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6005/Reviewer_9rLe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508939617, "cdate": 1761508939617, "tmdate": 1762918411090, "mdate": 1762918411090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies the \"learning cliff\" in Reinforcement Learning from Verifier Rewards (RLVR) as a key obstacle to enhancing LLM reasoning . This \"cliff\" occurs when difficult problems yield persistent zero-reward signals, stalling the gradient updates of policy optimization algorithms like GRPO . The authors propose SCAF-GRPO, a framework that intervenes when such a stall is detected. It first diagnoses \"true-hard\" problems after a \"guidance exemption period\". Then, it injects minimal, hierarchical in-prompt hints (Knowledge, Planning, or Solution) to enable the model to generate a successful trajectory on-policy. This successful trajectory replaces a failed one, restoring a non-zero advantage signal for the GRPO update. Experiments on mathematics benchmarks show SCAF-GRPO outperforms vanilla GRPO and prefix-based baselines, and the method is shown to be effective across various model architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In a learning cliff scenario, all rewards are zero, causing the advantage calculation to collapse and the learning gradient to vanish. Scaf-GRPO intervenes by generating a single successful trajectory on-policy using a minimally effective hint. This successful trajectory replaces a failed one in the batch, which \"restores a meaningful advantage signal\" and ensures \"non-zero reward variance\", allowing the standard GRPO update to resume."}, "weaknesses": {"value": "- The framework's central claim to preserving the \"on-policy principle\" is questionable. When all trajectories for a query $q$ fail (the \"learning cliff\"), the method does not learn to solve $q$. Instead, it introduces a new input, $q \\oplus h^{*}$ (query + hint), and learns from this new, simpler task. The policy is indeed 'on-policy' with respect to the augmented prompt, but it has failed and bypassed the original, unhinted task. This is a semantic argument that obscures the fact that the model is being trained on a different, easier problem distribution than the one it originally failed on, which introduces a \"distributional mismatch\" of its own, despite the paper's critique of other methods on this very point.\n- The paper's core hypothesis is that its in-prompt scaffolding approach \"fosters more generalizable skills\" and \"cultivates robust reasoning skills rather than in-domain pattern matching\"  when compared to prefix-continuation baselines like LUFFY. The OOD evaluation on GPQA-Diamond (Table 4) fails to support this claim. Scaf-GRPO (37.3%) performs identically to the LUFFY baseline (37.3%). This lack of improvement in OOD generalization strongly suggests that the methodological benefits of Scaf-GRPO do not translate to improved generalizable reasoning, undermining the primary justification for its additional complexity over existing methods.\n-  No direct evidence found in the manuscript regarding the computational overhead of the proposed framework. Detailed analysis of the resource consumption, such as training time and memory usage, would be valuable for practical applications (Sec. 3.1). The paper does not discuss potential trade-offs between the improved reasoning performance and the computational cost. This information is crucial for understanding the practical feasibility of the approach (Sec. 3.2).\n- The 15% guidance exemption period appears empirically driven without theoretical grounding. Why 15% and not 10% or 25%? Three-tier, four-level hint structure: Why exactly 3 categories? Why must each have ≥4 items? This seems like an ad-hoc engineering decision lacking principled motivation 50% subsampling of \"Potentially Solvable\" problems lacks justification. The paper would benefit from ablations on these hyperparameters (e.g., 10% vs 15% vs 20% exemption periods).\n- Performance differences on some benchmarks are marginal (e.g., Table 1: Scaf-GRPO vs LUFFY on several benchmarks)"}, "questions": {"value": "see * Weaknesses*."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tLXKOIBN46", "forum": "bOwVr0yr7r", "replyto": "bOwVr0yr7r", "signatures": ["ICLR.cc/2026/Conference/Submission6005/Reviewer_KDuR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6005/Reviewer_KDuR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577142644, "cdate": 1761577142644, "tmdate": 1762918410528, "mdate": 1762918410528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the \"learning cliff\" issue in GRPO, where the solution to a hard problem cannot be learned if all the sampled solution are incorrect. To overcome this issue, this paper propose a scaffolding framework, which provides some hints of different level of concreteness to stimulate on-policy correct answers. These correct answers avoids zero-gradient on the hard problem and provide useful training signal, therefore makes use of the hard problems. Experiments shows that the proposed method outperforms vanilla GRPO and other off-policy guidance baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper is listed as follows\n\n1. The method proposed in this paper adapts external knowledge (solution) to on-policy solutions, which avoid the distribution shift resulted from directly utilizing off-policy solution.\n\n2. The experiments are conducted on several different models and Scaf-GRPO consistently improves over baselines. Also, the ablation study is comprehensive and covers most of the design of proposed method.\n\n3. Overall, the paper is clearly written."}, "weaknesses": {"value": "In general, I think this paper does not demonstrate any major weakness. Some of the identified weakness and my questions are listed below\n\n1. In equation (4), the author propose to use $\\pi_{\\theta}(\\cdot|q\\oplus h*)/\\pi_{\\text{old}}(\\cdot|q\\oplus h*)$ as the importance ratio. However, this does not exactly matches the probility $\\pi_{\\theta}(\\cdot|q)$. To the reviewer, $\\pi_{\\theta}(\\cdot|q)/\\pi_{\\text{old}}(\\cdot|q\\oplus h*)$ makes more sense. Could the authors compare these two different approaches?\n\n2. The method relies on LLM-generated hints, whose quality might affect the performance of the method. Besides end-to-end performance, are there any other metric that measures the quality of the generated hints? Or in other word, is there a way to control the quality of generated hints?"}, "questions": {"value": "See weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sPb7Jgxf41", "forum": "bOwVr0yr7r", "replyto": "bOwVr0yr7r", "signatures": ["ICLR.cc/2026/Conference/Submission6005/Reviewer_HrXm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6005/Reviewer_HrXm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611231927, "cdate": 1761611231927, "tmdate": 1762918409345, "mdate": 1762918409345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is motivated by the observation that GRPO could perform badly when the problem is hard to solve and successful trajectories are hard to collect. To address this challenge, this work proposes Scaf-GRPO, a method that focuses on the data augmentation aspect rather than modifying the GRPO algorithm itself. Specifically, Scaf-GRPO encourages the model to generate hints that improve the success rate, thereby accelerating RL convergence."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well written and well organized.\n2. Sufficient experiments are provided to validate the effectiveness of their method."}, "weaknesses": {"value": "see questions"}, "questions": {"value": "1. During RL training, is Scaf-GRPO applied to both easy and hard problems, or only to the hard ones? Additionally, how do you determine whether a problem is considered easy or hard?\n2. What is the behavior during inference? Do you use the hint prompt to encourage the model to generate hints for all problems?\n3. In Line 398, could you explain what is meant by “the most concrete hint”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "l0FnFu3Ea4", "forum": "bOwVr0yr7r", "replyto": "bOwVr0yr7r", "signatures": ["ICLR.cc/2026/Conference/Submission6005/Reviewer_QtTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6005/Reviewer_QtTi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774100852, "cdate": 1761774100852, "tmdate": 1762918409126, "mdate": 1762918409126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}