{"id": "OWkkFaq1IZ", "number": 11785, "cdate": 1758203820691, "mdate": 1759897554961, "content": {"title": "From Observations to Events: Event-Aware World Models for Reinforcement Learning", "abstract": "While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10\\%â€“45\\%, setting new state-of-the-art results across benchmarks.", "tldr": "", "keywords": ["model-based reinforcement learning", "online learning", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14e99ee23a6feda519d314f241e1e057bfb19ea1.pdf", "supplementary_material": "/attachment/a40f10d4e1119c035f313c38f3aba9b8b6b0638b.zip"}, "replies": [{"content": {"summary": {"value": "The paper adds an \"event-aware\" layer to world models so that agents learn from meaningful changes (events) rather than raw frames alone. It defines events across modalities, predicts them with a dedicated head, and uses a Generic Event Segmentor (GES) to weight losses. Plugging this into DreamerV3 and Simulus yields strong empirical gains across Atari 100K, Craftax, DMC, and DMC-GB2."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a novel extension that can be attached to existing world models.\n- Strong empirical performance across benchmarks."}, "weaknesses": {"value": "- Event definition in Section 2.2 feels rushed:\n  - Eq. (1): the image intensity $I_t$ is not explicitly introduced in the text.\n  - Line 151: \"direction of the event\" is unclear, does this mean the sign of the brightness change?\n  - For ordinal data, the event is defined as $p_i$. For consistency with visual data it should likely be a tuple e_{t,i}.\n  - Eq. (2): $\\Delta o_i$ is not defined.\n  - This carries into Eqs. (3)-(4): wwhat exactly are the events $e$? In places it seems $p$ might be intended instead of $e$.\n- Discrepancies in Figure 3 vs. Eq. (3):\n  - The sequence model is labeled $F_\\theta$ in the figure but $f_\\theta$ in the equation.\n  - In Eq. (3), the representation model is conditioned on $y_t$, in the figure it is conditioned on $h_t$.\n  - In Eq. (3), the event predictor conditions on $y_{t-1}, y_t$, in the figure it appears to use $y_t, y_{t+1}$.\n  - Why is the observation predictor categorized under \"EA\" rather than \"WM\"?\n- The sentence \"We employ Adaptive Gaussian Mixture Models to automatically detect such events from video\" needs concrete detail. What features are modeled, how thresholds are chosen/updated, and how detections map to the event tuple. This is central to the method but currently vague.\n- The description of the Geneirc Event Segmentor is hard to follow:\n  - How is $\\alpha_t$ computed exactly, do indices $i$ range over pixels or batches? This should be made precise.\n  - Line 272: please define \"event boundaries.\"\n  - The role of GES (as a weighting coefficient for the event/observation losses) emerges only from the equations. The text says \"reallocate attention\" but should explicitly tie to Eqs. (7)-(8).\n  - The function $g$ in Eq. (6) appears without explanation in the main text, only the appendix clarifies it.\n  - From Eq. (6), GES seems independent of $\\theta$. If it has no learned parameters, why is it part of the world model rather than a fixed weighting step?\n- Reported scores show discrepancies vs prior work:\n  - DreamerV3 numbers on Atari 100K are lower than the original paper (mean 1.15 reported here vs 1.25), which makes the gap to EADream's 1.29 smaller than suggested. Please clarify whether these were re-runs and why they differ.\n  - For Simulus, the reported mean and median (1.61 and 0.74) differ from the Simulus paper (1.65 and 0.98). Please explain the deviation.\n- Ablations are unclear:\n  - \"No Event Predictor\": What exactly is disabled? If event prediction is off, shoudn't the model reduce to the original world model? Why do \"EADream w/o Event Predictor\" and DreamerV3 differ? Is this related to the RSSM-OP change, but if so, why is there the same effect for Simulus?\n  - \"No GES\": If GES is off, is $\\omega$ also set to $0$? If so, consider an additional ablation where GES is on but $\\omega = 0$ to disentangle effects.\n  - \"Without Observation Prediction\": RSSM-OP seems to matter a lot. An ablation \"Dreamer + RSSM-OP\" would isolate its contribution.\n- Minor: Missing citations to Craftax and DMC in Section 4.1.\n\nI'm willing to raise my score if the presentation is improved and these points are addressed, as the method and results look very promising. My current score is low because, in its present form, the paper should not be accepted due to several presentation issues, but if these are fixed I would lean toward acceptance."}, "questions": {"value": "- The method adds several hyperparameters on top of already hyperparameter-heavy world models. How sensitive are the new hyperparameters in practice, and how difficult was tuning?\n- Please address the clarification requests in the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2DRxHAaK3x", "forum": "OWkkFaq1IZ", "replyto": "OWkkFaq1IZ", "signatures": ["ICLR.cc/2026/Conference/Submission11785/Reviewer_6jd5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11785/Reviewer_6jd5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826420549, "cdate": 1761826420549, "tmdate": 1762922809572, "mdate": 1762922809572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggest to extend a typically world model for reinforcement learning (e.g. Dreamer) by an event predictor.  The paper claims that that addition leads to better latent states.  Experiments show that those latent states indeed achieve better performance on various benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- well written introduction\n\n- excellent results across different benchmark datasets, e.g. Atari 100K\n\n- the framework can be hooked to existing methods, e.g, Simulus and Dreamer, and consistently improves results\n\n- the additional events are auto-labeled using an \"Automated Event Generator\" (based on Adaptive GMMs)"}, "weaknesses": {"value": "- not much to criticize!"}, "questions": {"value": "- Eq. (1): $p_i$ doesn't appear in the formula!  when is $p_i$ positive or negative for visual inputs?\n\n- How sensitive is the method to the choice of $C_I$ in Eq. (1)?\n\n- Automated Event Generator:  what are events e.g., in the game of Pong?\n\n- I didn't really understand, what is the point of the \"Generic Event Segmentator\"?  What do you mean with \"reallocate attention from events to raw observations\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZDgJh8S045", "forum": "OWkkFaq1IZ", "replyto": "OWkkFaq1IZ", "signatures": ["ICLR.cc/2026/Conference/Submission11785/Reviewer_z3Wd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11785/Reviewer_z3Wd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953275377, "cdate": 1761953275377, "tmdate": 1762922809065, "mdate": 1762922809065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Event-Aware World Model (EAWM), a framework that enhances model-based reinforcement learning by predicting events rather than raw observations. The method introduces three key components: an automated event generator that extracts events from multi-modal observations without manual labels, an event predictor that shapes representations through information bottleneck optimization, and a Generic Event Segmentor (GES) that identifies event boundaries to stabilize training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Well-motivated approach.\n* The paper demonstrates consistent and substantial improvements across diverse benchmarks.\n* The paper successfully demonstrates applicability across different architectures."}, "weaknesses": {"value": "* The core idea of integrating dynamics between frames as an additional learning signal for world models has already been explored by DyMoDreamer [1]. Moreover, DyMoDreamer achieves better performance than EADreamer on both Atari 100K and DeepMind Control Vision benchmarks with a simpler method.\n* While the authors claim to learn task-relevant environmental dynamics, the EAWM primarily relies on inter-frame pixel differences. This approach may still filter in many task-irrelevant events.\n* Adding the event detector likely increases the training time for world models. The paper does not report wall-clock training time comparisons or computational cost analysis.\n\n[1] Dymodreamer: World Modeling with Dynamic Modulation"}, "questions": {"value": "* Why do EASimulus and EADream have different event predictor inputs? EASimulus conditions on $(y_{t-1}, y_t)$ while EADream conditions on $(h_t, \\hat{z}_t, z_t)$. \n* Can the authors provide qualitative imagination results on DMC-GB2? Given that DMC-GB2 tests generalization to visually noisy observations (randomized colors and video backgrounds), it would be valuable to visualize the world model's imagined trajectories and corresponding event predictions. This would help verify whether EAWM truly learns to attend to task-relevant objects while filtering out background distractors, or if the performance gains come from other factors.\n* What is the wall-clock time comparison with baselines?\n\nI am willing to raise my score if the authors can address these concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0eMLKyroLu", "forum": "OWkkFaq1IZ", "replyto": "OWkkFaq1IZ", "signatures": ["ICLR.cc/2026/Conference/Submission11785/Reviewer_zrEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11785/Reviewer_zrEf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011261002, "cdate": 1762011261002, "tmdate": 1762922808571, "mdate": 1762922808571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}