{"id": "M84KJx6oCx", "number": 1899, "cdate": 1756961357334, "mdate": 1763110996352, "content": {"title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework", "abstract": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks.\nHowever, RLHF incurs high costs and potential reward–policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7\\% gain on 7 reasoning benchmarks, 12.1\\% on 2 reward benchmarks, and 1.5\\% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.", "tldr": "", "keywords": ["RLVR", "RLHF", "LLM", "LVLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/af4678af8298f222046139a49a523adbe0eb2d18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a method (SPARK) that jointly trains the LM to solve tasks and judge its own generated response, by \"recycling\" the rollouts generated during RL with Verifiable Rewards (RLVR). It also bakes self-reflection into the inference time, by utilizing its own judgement to prompt for reflection when mistake is detected."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The experiments over Math related domains are comprehensive and show improvements compared to baseline. The model is also ablated with Policy-only and Reward-only objective."}, "weaknesses": {"value": "On the methodology, I am not quite sure if I understand the necessity of baking generation and reward modeling together. \n\n1. the task is already verifiable with rule-based calculation, the benefit of incorporating GRM is not obvious. What about other preference task where GRMs are more useful?\n2. No experiments on tasks that are non-verifiable to verify the effectiveness of proposed method. In my opinion, the “co-evolving” framework will likely result in RM overfit or model collapse when the learned judgement is not correct (in RLVR task, the learning signal is guaranteed to be correct for the RM). The generalizability of the setup is not verified.\n3. The experiment is not compared with setting that **trains a separate reward model** to help with test-time scaling. The experiments design should stress the difference between (a) LM + a pre-trained and fixed capable RM (b) the proposed co-evolving framework, but lacks such evidence.\n4. I’m not sure if comparison between the ablated version and proposed method is fair (i.e., whether judgement and self-reflection are both applied during test-time) but I might be wrong. Please see my question for detail."}, "questions": {"value": "1. For your evaluation (e.g., Table 1), can you clarify the setting a bit on how SPARK-VL-7B is evaluated? Is test-time scaling with judgement and self-reflection used? My understanding is YES. Please correct me if I am wrong.\n2. Then for your ablated version Qwen2.5-VL+GRPO + Policy&Reward, can you explain in more detail how it’s trained? Is it first trained on original data, then trained on crafted preference data for reward modeling, and then evaluated with judgement and self-reflection step as well? Because from the current description (line 318-323), I don’t know if self-reflection is applied during test-time.\n3. Do you have experiments that show results using two systems (a LM trained to generate CoT and solve problems, another LM trained on the collected rollout for reward modeling, then a combination of both during test time + self-reflection)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nwFvAWuc5s", "forum": "M84KJx6oCx", "replyto": "M84KJx6oCx", "signatures": ["ICLR.cc/2026/Conference/Submission1899/Reviewer_Ubvd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1899/Reviewer_Ubvd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761013875670, "cdate": 1761013875670, "tmdate": 1762915936713, "mdate": 1762915936713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "04mARQt9pA", "forum": "M84KJx6oCx", "replyto": "M84KJx6oCx", "signatures": ["ICLR.cc/2026/Conference/Submission1899/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1899/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763110995204, "cdate": 1763110995204, "tmdate": 1763110995204, "mdate": 1763110995204, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SPARK proposes a reinforcement learning framework that jointly evolves the policy and reward model within a single LLM/LVLM. Built on RL with Verifiable Rewards (RLVR), SPARK recycles correctness signals and rollouts that are normally discarded to train the same model as a generative reward model. This co-evolutionary mechanism reduces reliance on human preference data and external reward models, improving efficiency, stability, and test-time self-reflection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tElegant unification of policy and reward training—reduces cost and improves stability.\n2.\tAddresses reward-policy mismatch, a key issue in RLHF pipelines.\n3.\tDemonstrated improvements across reasoning and reward benchmarks (+9.7% / +12.1%).\n4.\tConceptually aligns with scalable self-reflective AI trends."}, "weaknesses": {"value": "1.\tIncomplete technical specification:\nThe paper lacks full detail on how co-training signals are balanced or stabilized (e.g., gradient separation, EMA targets). Without this, it is hard to reproduce or verify convergence.\n2.\tPotential circularity problem:\nTraining a model to generate and simultaneously evaluate its own responses risks self-confirmation. The authors claim that the verification step prevents collapse, but empirical or theoretical backing is weak.\n3.\tLimited ablation studies:\nThe contribution of each component (e.g., reflection, recycling, policy iteration) to the overall gain is unclear. Ablations would strengthen causal claims.\n4.\tGenerality of results:\nAll experiments rely on Qwen-family models. It remains uncertain whether SPARK generalizes to other architectures like Llama, Gemini, or GPT-style systems.\n5.\tLack of qualitative failure analysis:\nThe paper focuses on positive results but does not explore where SPARK underperforms—e.g., in ambiguous reward conditions or low-confidence verification.\n6.\tPresentation clarity:\nWhile conceptually sound, some notation and flow between RLVR and SPARK updates are dense and under-explained. Figures could better illustrate the co-evolution process."}, "questions": {"value": "1.\tHow do you prevent reward drift or self-confirmation when both policy and reward share parameters?\n2.\tWhat stability techniques (e.g., target networks, KL penalties) are employed to ensure learning convergence?\n3.\tHow often is the verifier updated relative to the policy loop?\n4.\tCan SPARK operate on preference data when available, or is it strictly designed for verifiable signals?\n5.\tHow would SPARK handle tasks without binary verifiability (e.g., open-ended generation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5OgKAyTg8X", "forum": "M84KJx6oCx", "replyto": "M84KJx6oCx", "signatures": ["ICLR.cc/2026/Conference/Submission1899/Reviewer_JEkG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1899/Reviewer_JEkG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761390523699, "cdate": 1761390523699, "tmdate": 1762915935978, "mdate": 1762915935978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method called “SPARK”. Its major contribution is to jointly optimize the RL policy and the reward model. It uses the RLVR-derived correctness scores to train the model itself to become a generative reward model. The proposed method is verified on three categories of benchmarks. Experimental results show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation to get both the optimized RL policy and reward model is good.\n2. The adoption of the reflection mechanism in both training and testing is helpful.\n3. The experimental results are supportive."}, "weaknesses": {"value": "1. The idea of co-training the policy with the reward model will result in divergence. Without a well-trained and fixed reward model, the RL policy will lose the target to optimize. Indeed, a stable target is the priority in optimization. For example, the Deep Q Network, it uses the target network, which is a delayed version of the network to be optimized, as the evaluation network, just to keep the optimization target fixed during a period of time. On the contrary, this paper’s reward model (optimization target) is dynamically changing. Very likely, in the very beginning, the reward model is naive, and the RL policy will not get useful information from it. The RL policy will collapse, and as a result, the reward model itself will not be optimized. Finally, both the reward model and the RL policy will not be improved during the training process.\n2. The reflection process can be improved. The idea of reflection is helpful, but simply using the LLM to directly reflect on itself may cause overfitting, which can limit the improvement.\n3. As far as I comprehend, this paper attempts to improve the RL with verifiable reward (RLVR) framework by proposing the co-training strategy. It didn’t improve the reward limitation on objective tasks of RLVR, nor does it have a direct relationship with RLHF. The advantage of requiring no human preference data is inherited from the vanilla RLVR. Therefore, the purpose of depicting the limitations of those two methods in the description section (Paragraph 2) is confusing.\n4. The manuscript needs polishing. For example, grammar errors like “Our key insight is to recycle the rollouts and correctness data to…”, “reward&reflection”; It is not clear what the “reference model” refers to in Equation 4; It is not clear what “\\box{}” is."}, "questions": {"value": "NA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7XuFpnaaY6", "forum": "M84KJx6oCx", "replyto": "M84KJx6oCx", "signatures": ["ICLR.cc/2026/Conference/Submission1899/Reviewer_vEEq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1899/Reviewer_vEEq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917989174, "cdate": 1761917989174, "tmdate": 1762915935006, "mdate": 1762915935006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPARK, an on-policy framework that trains a single model to be both the policy and the judge. Instead of discarding rollouts in RL with verifiable rewards (RLVR), the method recycles the n-best candidates to build on-policy supervision for pointwise judgments, pairwise comparisons, and reflection. The unified model then uses this judging ability at test time for self-reflection–style TTS (no external reward model). On Qwen2.5-VL-7B, the authors report average gains of +9.7% on seven math benchmarks and +12.1% on two reward benchmarks, with smaller but consistent improvements on broader multimodal tasks. The paper argues this reduces RM cost/complexity while improving stability and data efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unified loop that wastes less signal. Recycling RLVR outcomes into pointwise/pairwise/reflection supervision for the same model is neat and practical; it cuts one model class out of the stack and removes frequent RM calls.\n2. On-policy supervision. Using current behavior to create judgment/reflection data reduces distribution shift versus offline RM datasets and explains why TTS helps SPARK but hurts baselines.\n3. Consistent wins. The +9.7% (math) and +12.1% (reward) numbers on VL-7B are solid; the smaller general-domain bump is still directionally positive.\n4. Reasonable ablations. Clear separation of answer vs. CoT data and a TTS study that highlights why a weak judge can degrade performance, whereas a trained judge helps."}, "weaknesses": {"value": "1. Efficiency/accounting is light. The paper claims cost wins over RM-based RL, but lacks hard numbers: wall-clock hours, tokens/sec, GPU memory/FLOPs, and verifier runtime (#unit tests per sample, pass rate). Table-style qualitative comparisons are helpful but not enough for practitioners.\n\n2. Verifier dependence. Rewards are binary and rule-based; the paper doesn’t probe robustness to noisy or partial verifiers (very common in code/math). A noise-injection or partial-credit ablation would make the claim more convincing.\n\n3. Self-confirmation risk. Policy and judge live in the same model. The KL to a reference helps, but there’s no quantitative analysis of judge calibration (ECE/Brier) or safeguards against over-confident self-approval during TTS.\n\n4. Repro details. Core knobs for TTS (max reflection rounds, acceptance rule, early stopping), prompt formats, and the exact n-best sampling policy should be surfaced in the main text."}, "questions": {"value": "1. Compute & throughput. Could you report end-to-end wall-clock, effective tokens/sec, and GPU hours for SPARK vs. (i) GRPO Policy-Only, (ii) GRPO Policy&Reward, and (iii) an RM-based pipeline? Also break out verifier cost per batch (pass rate, retries). This would substantiate the cost argument beyond Table 7.\n\n2. Judge–policy coupling. Did you try decoupled heads or stop-gradient tricks so the “judge” pathway can drift a bit from the “policy” during data generation? Even light dropout/temperature on the judge might reduce confirmation bias.\n\n3. TTS protocol. Please specify maximum reflection rounds and acceptance criteria (first judged-correct vs. best-of-k). In Table 5, can you attribute the baseline degradation to specific judge errors over rounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ffSL10iEQi", "forum": "M84KJx6oCx", "replyto": "M84KJx6oCx", "signatures": ["ICLR.cc/2026/Conference/Submission1899/Reviewer_R8hV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1899/Reviewer_R8hV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951008359, "cdate": 1761951008359, "tmdate": 1762915934733, "mdate": 1762915934733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}