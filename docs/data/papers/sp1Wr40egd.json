{"id": "sp1Wr40egd", "number": 11620, "cdate": 1758202587833, "mdate": 1759897564360, "content": {"title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction", "abstract": "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM’s predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code is available in Supplementary Material.", "tldr": "We introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM’s predictions can be trusted.", "keywords": ["Misclassification Detection", "Vision-Language Models", "Failure Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a51a301d64b7cbdf47b7022d4af70d2b1af391f.pdf", "supplementary_material": "/attachment/f3093147201917750427d352e1b6fe604d3dd086.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces TrustVLM, a framework designed to address confidence estimation in vision-language model (VLM) predictions. Motivated by the modality gap inherent in VLMs, TrustVLM constructs ensembles of VLMs and image-only classifiers to enhance the detection of misclassified samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The presentation of this paper is clear and easy to follow."}, "weaknesses": {"value": "The methodology proposed in this paper employs N-sample training data together with external image encoders to construct a Nearest Class Mean classifier, which is then combined with the original CLIP classifier. However, the improvement appears to stem primarily from the use of additional labeled data and model ensembling. I am concerned that this may not constitute a genuinely novel contribution. Moreover, since the competitive baselines operate in a zero-shot setting, the comparison could be considered unfair."}, "questions": {"value": "How does TrustVLM perform with fewer training samples, such as in 1-shot or 2-shot settings? Can TrustVLM function with little or even no training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3PTzhcCUyn", "forum": "sp1Wr40egd", "replyto": "sp1Wr40egd", "signatures": ["ICLR.cc/2026/Conference/Submission11620/Reviewer_aUQM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11620/Reviewer_aUQM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667373492, "cdate": 1761667373492, "tmdate": 1762922694529, "mdate": 1762922694529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies VLM error detection to enhance its trustworthiness. The proposed TrustVLM leverages the multimodal similarity of feature representations to decide whether the current prediction is correct. Particularly, it compares the query example with other examples in both image representation similarity and text representation similarity. The final confidence score for error detection is computed by combining both similarities, thus incorporating multimodal information. Through extensive experiments of quantitative analysis and qualitative study, the effectiveness of the TrustVLM has been superior to most of the baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is easy to follow, the motivation is very clear, and the intuition is quite straightforward.\n- The proposed TrustVLM is training-free and efficient to deploy. It can also be easily adopted by any VLM architectures.\n- The experimental performance is quite promising."}, "weaknesses": {"value": "- The major concern is missing the comparison with unimodal detection methods. The proposed method combines multimodal information to detect prediction errors; however, in the ablation study, there is no comparison with image-only or text-only detection. In this way, it would be clearer which branch of modality would contribute more to the overall performance improvement.\n- Moreover, the performance of TrustVLM highly relies on the performance of the employed VLMs; if the VLMs cannot provide high-quality representations, the error detection would be limited.\n- Another concern is that due to the existence of a modality gap, the cross-modal similarity could be unstable compared to image-to-image similarity. The misaligned cross-modal pairs would also mislead the error detection.\n- After detection, the proposed TrustVLM cannot further rectify the error predictions by finding the correct one."}, "questions": {"value": "- Which branch of modality contributes more to the overall performance improvement? It would be helpful to conduct an ablation study to verify the unimodal detection versus multimodal detection. Moreover, what if we directly ask LLMs to detect the prediction error? As done in ``Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning, in ICML 2024'', they leverage the prediction of LLMs to find prediction errors of VLMs, and LLMs can further correct the prediction to find the correct one.\n- How would the misaligned multimodal pairs mislead the overall detection performance?\n- The acquisition of prototypes could be difficult sometimes. Can the proposed method perform without prototypes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UszoHFTBQW", "forum": "sp1Wr40egd", "replyto": "sp1Wr40egd", "signatures": ["ICLR.cc/2026/Conference/Submission11620/Reviewer_FUYk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11620/Reviewer_FUYk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872479094, "cdate": 1761872479094, "tmdate": 1762922693956, "mdate": 1762922693956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors introduce TrustVLM, a training-free framework for predicting when a VLM’s predictions can be trusted. The task explored in this work is misclassification detection, which involves identifying when a prediction is incorrect. The key idea is to (1) generate visual prototypes for each class (i.e. the average embedding for N samples from the training data) and (2) compute the image-to-image similarity between the query image and the class prototypes. The standard image-to-text cosine similarity and the computed image-to-image similarity scores are combined in order to determine the overall prediction confidence. The authors show that this approach leads to substantially better misclassification detection performance than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work addresses an important task: determining when the predictions of a VLM are likely to be reliable.\n- Although the proposed method is methodologically straightforward, strong performance is observed across a range of datasets and model backbones. The authors also compare with multiple baselines. The distribution shift experiments with ImageNet are particularly compelling."}, "weaknesses": {"value": "- **Need for finer-grained analysis:** This paper could benefit from additional fine-grained analysis with respect to when the proposed method is most effective (rather than just overall metrics). For example, are there specific classes where misclassification detection performance improves substantially when using the proposed method (as compared to MSP)? What types of characteristics are common among those classes?\n- **Variance of performance:** The proposed method is likely very sensitive to the choice of few-shot samples used to compose the class prototypes. What is the variance in performance when using prototypes composed from different randomly-selected N-shot sample sets?"}, "questions": {"value": "Questions are listed above under weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SV7lOUnpkt", "forum": "sp1Wr40egd", "replyto": "sp1Wr40egd", "signatures": ["ICLR.cc/2026/Conference/Submission11620/Reviewer_CAb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11620/Reviewer_CAb7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966603499, "cdate": 1761966603499, "tmdate": 1762922693598, "mdate": 1762922693598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For improving the reliability of VLM, this paper proposes a new training-free framework TrustVLM to estimate when VLM’s prediction will be trusted. The key of TrustVLM is to use a new confidence scoring function which will use not only cosine similarity between image and text but also the addition information from image embedding space. This additional information is the image-to-image relations or similarity. The idea is very straightforward where you need to extract some prior knowledge from training data. This prior knowledge is the class embeddings that are extracted from N shot examples from training data for each class. It likes you did another way for image classification based on the similarity between class embeddings and input image embedding. The whole idea is clear. The paper is well written. The key question here is you need to have a training data and get the class embedding based on the classes in the training data. But in some use cases, we don’t have the training data to get these class embeddings in advance. The proposed method has its limitation on it."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.\tTrustVLM is a training-free framework designed to evaluate the reliability of VLM predictions. One of its key advantages is that it does not require additional training, which makes it convenient to apply in scenarios where labeled data is limited or unavailable. The framework combines both image-to-text and image-to-image similarities, which allows for a more robust and nuanced design of confidence scores. This combination provides a richer representation of the visual information, enabling the framework to better capture the model’s uncertainty.\n\n2.\tThe paper demonstrates that the proposed visual prototypes not only enable more reliable confidence estimation but also enhance fine-grained classification accuracy.\n\n3.\tThe experiments conducted across diverse datasets, model architectures, and VLMs show the generality and effectiveness of TrustVLM."}, "weaknesses": {"value": "1.\tA notable limitation of this method is that it relies on the availability of in-domain data that includes images for all classes to be predicted. Under this assumption, the method can extract and store visual prototypes for each class, which are then used for confidence estimation. However, in many practical scenarios, obtaining such in-domain data for every class may be difficult or infeasible. Moreover, if the training or reference data does not fully cover the diversity of the test data, the method may encounter out-of-distribution (OOD) situations. As the introduction clearly states, TrustVLM is not designed to handle OOD cases, which inherently limits its applicability in environments where data coverage is incomplete or classes are highly dynamic. This restriction should be carefully considered when evaluating the practical utility of the method.\n\n2.\tThere exist alternative strategies to improve reliability when prior knowledge about class representations is available. For instance, one could employ a separate image embedding model to independently obtain embeddings for the input and for each class, then compute similarity scores between them. Another potential approach is to first predict the class of the image using a preliminary classifier and then use this prediction as an input for the final classification task. These alternatives might offer comparable or complementary benefits to TrustVLM. Therefore, the paper would be strengthened if the authors could provide a more detailed comparison or discussion of how TrustVLM differs from these approaches. Specifically, it would be helpful to clarify the unique advantages of TrustVLM, such as why its combined use of image-to-text and image-to-image similarity provides superior or more reliable confidence estimation compared to these other methods. This explanation would help to more clearly establish the method’s contributions and practical significance."}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KzPFJJQGFs", "forum": "sp1Wr40egd", "replyto": "sp1Wr40egd", "signatures": ["ICLR.cc/2026/Conference/Submission11620/Reviewer_pKks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11620/Reviewer_pKks"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980000019, "cdate": 1761980000019, "tmdate": 1762922693046, "mdate": 1762922693046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}