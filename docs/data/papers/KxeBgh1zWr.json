{"id": "KxeBgh1zWr", "number": 20569, "cdate": 1758307584950, "mdate": 1759896970735, "content": {"title": "Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence", "abstract": "Sliced Mutual Information (SMI) is widely used as a scalable alternative to mutual information for measuring non-linear statistical dependence. Despite its advantages, such as faster convergence, robustness to high dimensionality, and nullification only under statistical independence, we demonstrate that SMI is highly susceptible to data manipulation and exhibits counterintuitive behavior. Through extensive benchmarking and theoretical analysis, we show that SMI saturates easily, fails to detect increases in statistical dependence (even under linear transformations designed to enhance the extraction of information), prioritizes redundancy over informative content, and in some cases, performs worse than simpler dependence measures like the correlation coefficient.", "tldr": "Contrary to common belief, we demonstrate that Sliced Mutual Information is not a generally reliable measure of statistical dependence", "keywords": ["information theory", "mutual information", "sliced mutual information", "curse of dimensionality"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa88c171bf81e1c6e7a8734a49e6a2be6f4b1850.pdf", "supplementary_material": "/attachment/fab027fc9ad103bd2e9b1894946e1e1539395da4.zip"}, "replies": [{"content": {"summary": {"value": "Theoretical properties of sliced mutual information (SMI) are studied, suggesting strong limitations of SMI in application scenarios in which it has been employed in the past, e.g., the Deep InfoMax setting. Furter, discrepancies between SMI and conventional mutual information are illustrated, that is, SMI between deterministic Gaussian variables is bounded while MI is infinite. For increasing dimensionality SMI approaches zero, while again, MI is approaching infinity. The theoretical analysis has been supported with simulation experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper highlights severe limitations of SMI that are relevant for practitioners.\n- Theoretical results for multivariate Gaussian variables have been derived which illustrate the counterintuitive behaviour of SMI compared to MI.\n- The main theoretical result derived in Lemma 4.1 has been supported with sufficient experimental evidence.\n- The usecase of Deep InfoMax has been investigated at the example of a small image dataset."}, "weaknesses": {"value": "While the discrepancy between MI and SMI is well-illustrated, the claims regarding the Deep InfoMax principle are not as strongly supported by the experiments:\n- The description provided in D.2 explains how to obtain the baseline but more details regarding the implementation of SMI are needed.\n- The results are only obtained for a relatively small image dataset (MNIST). To support the strong impact statement made by the authors, more complex datasets such as CIFAR10, or similar ones should be analyzed.\n\n Some experimental details are missing: \n- How exactly was $SI_k$ implemented for the Deep InfoMax experiment? \n- How many seeds did the authors consider to confirm their results? \n- Why is $k=1$ for the KSG estimator? Typically a higher $k$ such as 5 or 7 is more stable.\n\nMinor: \n- In their discussion of the Deep InfoMax experiments, the authors should mention that, e.g., for invertible $f$ we get that $I(X;f(X))$ is infinite and we can only compute it if $f$ fulfills certain properties that avoid this behaviour. \n- Punctuation missing in Lemma A.3."}, "questions": {"value": "Questions:\n- Are there any usecases for which the authors would recommend to use SMI instead of MI?\n- Is the proof of Lemma A.4 just recited from 41, or are there any new contributions in it?\n- Do the works that used SMI for Deep InfoMax [22,23] employ any architectural restrictions or similar to avoid collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N9XMHVxZei", "forum": "KxeBgh1zWr", "replyto": "KxeBgh1zWr", "signatures": ["ICLR.cc/2026/Conference/Submission20569/Reviewer_pnxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20569/Reviewer_pnxw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760614173042, "cdate": 1760614173042, "tmdate": 1762933982123, "mdate": 1762933982123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an analytical and empirical critique of Sliced Mutual Information (SMI), a popular scalable alternative to mutual information (MI) used in various works on high-dimensional statistical dependence estimation and deep learning analysis. The authors argue that despite its recent adoption, SMI exhibits several fundamental flaws that make it unreliable as a measure of dependence, including the following:\n1) SMI rapidly saturates even for simple synthetic problems, failing to reflect true increases in dependence.\n2) SMI prioritizes redundant or repeated information rather than informative content.\n3) Although promoted as dimension-robust, SMI actually decays to zero asymptotically in high dimensions.\n4) SMI can increase under deterministic mappings, unlike MI.\n\nThrough theoretical analysis and experiments with synthetic data, the paper demonstrates these deficiencies and shows that using SMI as a replacement for MI is, at the very least, problematic."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Thank you for the paper.  It was an interesting read.\n\nThe paper has both detailed theoretical analysis with several key examples (such as closed-form analysis for certain classes of Gaussian variables) and a large number of synthetic experiments to validate their theoretical findings and conceptual contribution.  \n\nThe paper provides an appropriately critical assessment, in a timely and important way.\n\nThe paper is well written and nicely organized."}, "weaknesses": {"value": "The following weaknesses are suggested, but they aren't (in my mind) especially significant.  \n\nThe claim that SMI fails to detect increases in dependence even for linear transformations that enhance information extraction should be clarified to have been shown for specific cases, not necessarily universally.  \n\nA specific real-world example would be interesting.  \n\nThe math in this paper is detailed/hard and probably beyond many readers, but not sure that can be helped."}, "questions": {"value": "Less questions, more summarizing comments:\n\nMy preliminary assessment is that this a somewhat niche topic (papers on “sliced mutual information” seem fairly sparse, according to Google scholar), and so I might be less inclined to accept the paper.  However, some of the papers have appeared in prominent conferences (such as NeurIPS), and as such it seems like an important outcome to have the full story available for other researchers to examine.\n\nPerhaps my main question is, given the paper's results, whether the authors feel that SMI currently has any applications settings where it would still seem redeemable, or if it is back to the drawing board. Though that is more of a personal interest question -- I'm not sure I would want to suggest the authors should be required to make a strong statement in the paper, when I think their analysis lays out concrete issues clearly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OkcfSuf3iR", "forum": "KxeBgh1zWr", "replyto": "KxeBgh1zWr", "signatures": ["ICLR.cc/2026/Conference/Submission20569/Reviewer_ZesV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20569/Reviewer_ZesV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761092202268, "cdate": 1761092202268, "tmdate": 1762933981701, "mdate": 1762933981701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the shortcomings of Sliced Mutual Information (SMI) as a tool for measuring statistical dependence in high-dimensional settings. The authors show that SMI can saturate as the correlation between random vectors increases and decreases asymptotically as dimension grows. The authors validate the theoretical findings with synthetic experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written\n- The paper presents mathematical proofs of SMI’s limitations in the simple Gaussian scenario\n- The paper highlights major flaws of a novel dependence measure"}, "weaknesses": {"value": "- The theoretical results are limited to a Gaussian setting (Lemma 4.1). This does not prove anything for more complex scenarios\n- Since the theoretical part is limited to the Gaussian setting, one would expect the experimental results to be comprehensive of significantly complex scenarios for which the authors did not provide a theoretical contribution, to show that the paper contributions hold true for many possible cases. However, the experimental results could be improved"}, "questions": {"value": "- In the numerical experiments, are you considering a finite-data regime or infinite-data regime? I did not understand it from the paper, but it is well-known that, for instance, MI estimators perform differently depending on the regime. In any case, how does SMI perform in the other regime that you did not consider? \n- How do you explain the saturation phenomenon of SMI for non-Gaussian scenarios?\n- Since you compare correlation coefficients, SMI, MI, and copula in the initial figure of the paper, why did you not include any other observation or comment on these measures? Can you provide a paragraph of comparison?\n- It appears that high dimensions pose a theoretical problem with SMI in the Gaussian case. However, SMI was used specifically for high-dimensionality problems. So, could it be that SMI does not have the same saturation problem for high dimensions in different scenarios? Did you find any settings in which the dimensionality was not a problem for SMI, and that are not reported in the paper?\n- What does << mean in your case (line 114)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9k6NCSCUmY", "forum": "KxeBgh1zWr", "replyto": "KxeBgh1zWr", "signatures": ["ICLR.cc/2026/Conference/Submission20569/Reviewer_KQcg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20569/Reviewer_KQcg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761310050918, "cdate": 1761310050918, "tmdate": 1762933981262, "mdate": 1762933981262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper gives a critical analysis of a cheap and intuitive proxy measure for statistical dependence called \"sliced mutual information\" (SMI). \nAlthough the simplicity of SMI has fueled its growing popularity, this study finds that the name is deceptive and that SMI has a number of drawbacks that make it inappropriate as a proxy for statistical dependence measures like Mutual Information: SMI saturates as dependence grows, SMI is biased towards redundancy, SMI decays to zero in high dimensions. The paper builds analytic examples and uses synthetic data to demonstrate the issues, and shows that simple workarounds do not fix the problem."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper was well structured. SMI and its defects were clearly described, with intuitive experiments supporting each result. \n\n- While most of the arguments were supported by analytical counter-examples, I was happy to also see comparisons with recent synthetic data benchmarks used in the MI estimation literature. \n\n- I appreciate that the paper went beyond showing examples where SMI gives counter-intuitive results, and also showed that *optimizing* SMI leads to poor results. \n\n- The deficiencies of SMI (like redundancy bias) were not very surprising to me, but the curse of dimensionality effect was quite a bit stronger than I expected. (I assumed that as dimensionality grows, it would be hard to find a \"good slice\", but it seems that even analytically integrating slices leads to decaying SMI!)"}, "weaknesses": {"value": "I thought the critique was straightforward and clear. The only question in my mind is the broader significance of these results. I am familiar with recent neural MI estimation literature, and I confess I had not seen any mention of SMI (though I think I recall a reference to it) so I was a little surprised to see it described as a \"popular\" approach. Nevertheless, the citations don't lie, and a decent number of papers in top venues are studying something that is (as I am convinced by this paper) a dubious measure. Therefore, even though I don't think this has the broadest significance for the field, the critique should be at least as visible as the flawed approach."}, "questions": {"value": "- I didn't go back to study the IB paper that you cited that optimized SMI. Your result suggests that it shouldn't work, is there any explanation on how they were able to show reasonable results to publish with this method? \n\n- One small improvement for a reader like myself would be to situate the SMI literature with respect to the neural MI estimation literature. I just assumed that it was clear that neural MI estimation methods, like neural everything else, was the clear winner. Are there applications / properties / uses cases which led people to prefer SMI over neural methods? I assume it's just for computational simplicity?\n\n- One other thought that might extend the impact of this work is to discuss whether there are connections to other \"sliced\" estimators, like sliced score matching. Even the popular Hutchinson trace estimator could be considered a sliced estimator."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1qagbQEqON", "forum": "KxeBgh1zWr", "replyto": "KxeBgh1zWr", "signatures": ["ICLR.cc/2026/Conference/Submission20569/Reviewer_AoMn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20569/Reviewer_AoMn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779578989, "cdate": 1761779578989, "tmdate": 1762933980752, "mdate": 1762933980752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}