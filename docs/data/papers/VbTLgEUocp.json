{"id": "VbTLgEUocp", "number": 20472, "cdate": 1758306532817, "mdate": 1759896975881, "content": {"title": "LLMs Can Hide Text in Other Text of the Same Length", "abstract": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present a simple and efficient protocol to achieve it. We show that even modest 8‑billion‑parameter open‑source LLMs are sufficient to obtain high‑quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.", "tldr": "We show how LLMs can hide an entire text (e.g., a political critique) inside a completely different, plausible text (e.g., praise) of the exact same length, creating a massive new threat for AI safety and digital trust.", "keywords": ["Large Language Models (LLMs)", "Generative Steganography", "AI Safety", "Authorial Intent", "Trust in AI", "Deniability", "Censorship Resistance"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5d5f9712ada6d2e8d8e49fa2afacd8c5ba70a65.pdf", "supplementary_material": "/attachment/5176e9a33c2e0495518c00630760d8830baff3ba.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a simple protocol to hide text (e) within another coherent, same-length LLM text (s). It uses the probability ranks from e to deterministically guide the generation of s from a secret key k. The authors highlight a critical AI safety risk: hiding unfiltered answers within “safe” model responses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Important topic, identifying an emergent and unsettling capability of LLMs.\n+ The proposed protocol is remarkably simple, efficient, and demonstrated to be effective even on small open-source models.\n+ The discussion section provides a deep, thought-provoking analysis of the decoupling of text from authorial intent."}, "weaknesses": {"value": "- The security assumptions of the primary AI safety threat model (the \"Shibbolethian Theatre\") are unclear, particularly regarding the public transmission of the key.\n- Key claims, such as “deniability”, are asserted but not empirically demonstrated, weakening their practical standing.\n- The evaluation of the stegotext's “plausibility” relies solely on LLM-based metrics (log-probability), which may not fully capture human perception of “unsuspicious” text.\n- The protocol's strict reliance on exact model versions for decoding may limit its practical utility and robustness in real-world scenarios."}, "questions": {"value": "1. The “Shibbolethian Theatre” threat model is a core application. In this scenario, the reasoning trace t (which serves as the secret key k) and the stegotext s are both sent to the user. This seems to contradict the standard steganographic assumption that the key must remain secret. Could the authors clarify how security is defined in this context? Does this threat model rely on the protocol itself being secret (security through obscurity) rather than the key?\n2. The paper makes a strong claim about providing “deniability”, suggesting a sender can reveal a “bogus secret key” that yields a different, also plausible, message. This implies that for a single s, multiple pairs of (k, e) exist that are all “plausible”. Given the deterministic nature of the decoding sequence (k, s) to LLM to e, is this practically feasible? Providing even a single empirical example of this phenomenon would significantly strengthen this claim.\n3 . The protocol’s reliance on the exact same LLM for encoding and decoding is a critical detail. Given the rapid iteration of LLM versions in real-world deployments, this seems to make the protocol highly fragile. Could the authors comment on this practical limitation and how it might affect its utility as a reliable covert channel?\n4 . The use of cumulative log-probability to evaluate the “plausibility” of the stegotext is logical. However, this metric essentially uses the LLM to judge its own output. It would be beneficial to supplement this with a human evaluation. Can the authors comment on whether a text deemed “plausible” by an LLM is necessarily “unsuspicious” to a human reader?\n5. The paper notes that the protocol’s success is domain-dependent, failing on high-entropy inputs or specific dialects (like Romanesco). This is an important limitation. Could the authors provide a more detailed discussion on the boundaries of this method? What characteristics of a text e make it a viable candidate for this steganographic protocol?\n6. The paper dedicates significant space to the concepts of knowledge, intent, and hallucination. In the AI safety scenario, the aligned oLLM is forced to generate a harmful answer. If a model outputs a specific string only because it is forced to by an external rank sequence, without any semantic grounding for that choice, can this be defined as the model possessing that “knowledge”? This distinction seems central to the paper’s broader philosophical claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wLzxBtvLLQ", "forum": "VbTLgEUocp", "replyto": "VbTLgEUocp", "signatures": ["ICLR.cc/2026/Conference/Submission20472/Reviewer_hJqX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20472/Reviewer_hJqX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793727177, "cdate": 1761793727177, "tmdate": 1762933913328, "mdate": 1762933913328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to hide secret text by first converting it into a sequence of probability rankings and then using a key prompt to guide a LLM to generate cover text, with each token strictly selected according to the ranking sequence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper offers a new perspective for LLM safety and alignment: a model that appears aligned on the surface may still harbor vulnerabilities that allow dangerous information to be hidden in its output probability distribution.\n2. It introduces a remarkably simple, full-capacity method for embedding hidden text specifically designed for LLMs.\n3. Due to the secret key prompt and the inherent chaos in LLM behavior, this approach is currently nearly impossible to detect without access to both the key prompt and internal model details."}, "weaknesses": {"value": "1. The method is sensitive to the quality of the key prompt; a low-quality prompt may prevent the target probability ranks from forming a coherent and natural-looking cover text.\n2. It is fragile to transmission errors; any corruption in the cover text will completely scramble the recovered probability rank sequence, making it unsuitable for noisy communication channels.\n3. It imposes constraints on the secret text itself, which must lie within the model’s training domain, for example, rare dialect expressions or out-of-distribution content may not be reliably encoded."}, "questions": {"value": "1. How can we select an effective key prompt to ensure that the generated cover text appears natural in most cases?\n2. Is it possible to design error-correction mechanisms for cover text transmission errors?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ltvLzx8kjQ", "forum": "VbTLgEUocp", "replyto": "VbTLgEUocp", "signatures": ["ICLR.cc/2026/Conference/Submission20472/Reviewer_6CQa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20472/Reviewer_6CQa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827172907, "cdate": 1761827172907, "tmdate": 1762933912777, "mdate": 1762933912777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interesting method of protocol (steganography) using large language models to hide a secret text within a plausible-looking cover text. The method leverages the probabilistic nature of LLMs to encode information in the choice of words generated, allowing for covert communication. The authors demonstrate that this can be achieved with small open-source models and standard text generation techniques. They also discuss the implications of their findings on our understanding of LLMs, particularly regarding the concept of \"hallucinations\" and the relationship between human intent and machine-generated text."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is clear, intuitive and intriguing.\n2. The idea is simple yet effective.\n3. Extensive analysis and discussions are provided, making it deep and insightful."}, "weaknesses": {"value": "1. The novelty of the work is not very clear. Similar ideas have been explored in previous work and need to be better differentiated.\n2. Some practical limitations.\n3. Lack of robustness analysis in the adversarial scenario."}, "questions": {"value": "Overall, I found the paper interesting and well-written. The illustrations and examples provided well support the concepts and findings discussed. The discussions on the implications of the work for our understanding of LLMs were particularly thought-provoking.\n\nHowever, I have some questions and suggestions for improvement:\n\n**(1) Technical Novelty:**\n\nI believe the contribution of the paper would be largely on its insights and discussions rather than technical novelty. However, I suggest the authors to better clarify the novelty of their method compared to prior works on text steganography using language models, e.g., the related works mentioned in Lines 145-149. What is the key difference compared to related works? Hiding secret text in the cover text with the same length may not be distinctive enough. May need more comparison and discussion on this.\n\n**(2) Some practical limitations:**\n\nIt is good that the authors discussed some limitations, e.g., (1) Conceal a non-plausible text (random password)\ninto a plausible fake text is not hard (Line 245-246); (2) Fake text less probable than real and can be detected (Line 260-266).\nI would like to point out a bit more:\n- The rank of the first token e_1 is not controled (directly dependent on the vocalbulary), which may affect the first token of s, and affect its coherence.\n- Aligned LLMs (e.g., GPT-5) may refuse to generate text containing harmful or sensitive content, making the ranks of harmful tokens in e really low, which may affect the encoding to s. Hence, hiding harmful or sensitive content may not be feasible in aligned LLMs (Section 4).\n\n**(3) Robustness analysis:**\n\nIt would be beneficial to include a robustness analysis to evaluate how well the proposed method performs under adversarial scenarios. For instance, what is the decoding performance if the fake text s go through some slight transformations, such as: simple paraphrasing, synonym replacement, or insert some blank (\\t) or line break (\\n) tokens.\n\n**Question:**\n\nCan we probably map the rank of e to a range of low values during encoding (e.g., top 10% of the vocalbulary)?\nFor example, r_1 = 5, r_2 = 20 -> r_1^{'} = 5/10 = 0.5 (round to 1), r_2^{'} = 20/10 = 2. This would affect the decoding of the original e (if do not need to be exact), but may improve the fluency of s.\nI think this is essentially a trade-off: either you wanna exact decoding of e to sacrifice some fluency of s, or you wanna fluent s to sacrifice some exactness of e."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GjmLpEFhV2", "forum": "VbTLgEUocp", "replyto": "VbTLgEUocp", "signatures": ["ICLR.cc/2026/Conference/Submission20472/Reviewer_4GF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20472/Reviewer_4GF4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889158455, "cdate": 1761889158455, "tmdate": 1762933912281, "mdate": 1762933912281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a simple but interesting method that allows a large language model to hide one meaningful text inside another coherent text of the same token length. The core idea is to record the rank sequence of each token in the text to be hidden according to the model’s next-token probability distribution and then generate a new text following these ranks under a secret prompt. The hidden text can later be reconstructed by anyone who knows the same model and secret key. The authors also discuss potential implications for AI safety, information hiding, authorship, and hallucination."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is conceptually elegant. It shows that large language models can be used as full-capacity generative steganographic systems, producing natural-looking texts that conceal arbitrary content.\n\nThe approach achieves one-to-one token correspondence between the hidden text and the generated text while maintaining coherence, which is interesting among existing steganography methods.\n\nThe paper connects the technique to broader philosophical questions about language, intention, and meaning in machine-generated text, offering an original perspective.\n\nThe writing is clear and engaging."}, "weaknesses": {"value": "The experimental analysis is minimal. The evaluation relies mostly on qualitative examples and log-probability plots without systematic comparisons or quantitative metrics such as recoverability, perplexity degradation, or detectability.\n\nThe proposed misuse scenarios, such as unaligned chatbots hidden within aligned ones, are speculative and not demonstrated experimentally."}, "questions": {"value": "How sensitive is decoding to differences in model versions or vocabulary?\n\nHow does the quality of the generated text change when the hidden text has higher entropy or contains rare tokens?\n\nSuggestion: Formalize the method mathematically, including an analysis of its capacity and error tolerance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5SV84egezU", "forum": "VbTLgEUocp", "replyto": "VbTLgEUocp", "signatures": ["ICLR.cc/2026/Conference/Submission20472/Reviewer_w3BK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20472/Reviewer_w3BK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047300539, "cdate": 1762047300539, "tmdate": 1762933911041, "mdate": 1762933911041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}