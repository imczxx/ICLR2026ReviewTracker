{"id": "ymUOPsbxLi", "number": 9625, "cdate": 1758130997312, "mdate": 1763673037041, "content": {"title": "Deep Hierarchical Learning with Nested Subspace Networks", "abstract": "Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose *Nested Subspace Networks (NSNs)*, a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50\\% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models.", "tldr": "We introduce Nested Subspace Networks (NSNs), a new way to build a single neural network that can instantly adjust its performance vs. computational cost at inference time.", "keywords": ["dynamic neural networks", "efficient inference", "adaptive computing", "deep learning", "low-rank adaptation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2724162dcca06e3f7faa5cc75d75c919e574d761.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a way to re-parametrize linear layers into low-rank factorization $W=A^{R \\times d_{in}} \\cdot B^{d_{out} \\times R}$ such that the compute budget can be dynamically adjusted without retraining by constructing the  effective weight matrix from the first r rows of A and first r columns of B. In other words, the effective layers forms a filter chain of images for the ranks r < R (a.k.a Nested Subspace Property / Layer). To train the layer effectively, the work leverages an uncertainty-weighted training objective by Kendall+2018 that balances the learning across ranks. The experiments show that the nested subspace layers can be introduced into pretrained LLMs to create a smooth rank-based accuracy-vs-compute trade-off that remains adjustable at test-time."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach is highly flexible and practical as it can be broadly applied to any pretrained models with linear layers through a straight forward Singular Value Decomposition. Rather than requiring the specification of a suitable rank up front that may not be known for a given problem and architecture the method allows to \"shut up and train\" to find an appropriate effective rank.\n- The resulting architecture is intepretable in the sense that it is straight-forward to inspect how much each ranks learns to represent both empirically and theorectically (the offered theoretical discussion provides a good intution of the approximation trade-offs involved)."}, "weaknesses": {"value": "- Section 4.1 investigates whether a single NSN, trained with the multi-rank objective, can match the performance of multiple, specialized model, but only does so for an somewhat artificial image classification setup where regular MLP baselines are trained from scratch. What is currently missing is a similiar quantitive comparison with LoRA baselines to establish whether an NSN evaluated for rank r matches a LoRA model that exclusively trained for rank r. This would make this work comparable with prior baselines and delinate the benifits of NSN learning vs predefined-rank models.\n- An explicit desideratum is granularity of adaption, but the trade-off here only works in discrete 'matrix rank' steps for each layer. Since different layers at different depths learn to represent different things it is not clear that a uniform rank pick across the entire model is ideal. Arguably, the most granular adaption would be picking the ideal rank size for each layer but it is not obvious how to do that efficiently. \n- The work mentions the connection to alternative weight-learning approaches such as GradNorm but there is no comparison or analysis on why the Kendall-loss formulation is the best choice.\n\n**Additional comments**\n\n- The core idea of the work could be introduced and visualized more clearly, e.g. by illustrating an example A and B and the resulting effective weight matrix. The right column of Figure 1 does not make it very clear that there is only 1 instance of A and B and not a duplication of many A and Bs.\n- L182 \"Following Kendall et al. (2018), we use the standard uncertainty-weighted surrogate objective\"; I recommend expanding on this to make the work more self-contained and accessible to readers who may not be familiar with  \n- \"a strict and dynamic computational budget\" - I found this confusing to read; it might be better to concretely lay out the use cases that this work has in mind.\n- L115 typo: that *form* a nested hierachy\n\nAside: The manuscript uses visually highlighted \"Takeaway\" messages that spell out what the reader may conclude from each section. I am not sure if this is helpful because it can feel repetitive, or, if it does not align with the reader's conclusion, somewhat presumptuous. For example, L276 \"smooth and predictable trade-off\" placed before any experimental results reads more like an unverifiable claim than a takeaway."}, "questions": {"value": "- It is not clear how the anchor and variant rank are chosen. Section 2.2 says that the ranks are \"sampled\" but I am not sure why and how specifically. Is this applied consistently across training runs or is there a random element to it?\n- It is claimed that the log-variances \"serve as an emergent proxy for the effective expressiveness of each rank-specific model\" but it is not clear why that needs to be the case since they are as far as I can tell freely trainable parameters. The same goes for the \"uncertainty-weighted surrogate\" - how do we know that these parameters act as a meaningful surrogate for uncertainty?\n- Are you planning to release your source code and a drop-in layer implementation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gjteYn35Fo", "forum": "ymUOPsbxLi", "replyto": "ymUOPsbxLi", "signatures": ["ICLR.cc/2026/Conference/Submission9625/Reviewer_TDZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9625/Reviewer_TDZV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758532086, "cdate": 1761758532086, "tmdate": 1762921161837, "mdate": 1762921161837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Nested Subspace Networks (NSNs), an architectural  framework for neural networks that enables continuous and dynamic  adaptation to a range of computational budgets through a nested subspace reparameterization of linear layers. NSNs introduce a means to  dynamically select model \"rank\" at inference, with theoretical backing  for smooth interpolation across ranks and an uncertainty-aware  multi-rank training objective to promote optimality across the entire  rank spectrum. Empirical evaluations include MLP experiments on CIFAR-10, ablation studies, and applications to several large pre-trained language models (LLMs), demonstrating that NSNs achieve competitive accuracy with significantly reduced computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a conceptually novel framework, Nested Subspace Networks, that unifies multiple model capacities within a single parameterization. \n- The approach is validated through controlled experiments, ablation analyses, and applications to pre-trained LLMs, supporting the claims of dynamic adaptability and compute–performance efficiency.\n- A key practical merit is the post-hoc applicability of NSNs: the framework can be retrofitted onto existing pre-trained models without retraining from scratch, addressing a critical deployment challenge for large-scale foundation models."}, "weaknesses": {"value": "- Equation 2 needs clarification. Does the variant rank $r$ change in different epoch or remain unchanged during training. Is $\\mathcal{L}_{CE}(k)$ the cross entropy loss calculated when the linear weight using its first $k$ rows and columns?\n- Implementation details missing. The paper does not explain how Logits Regularization, Residual Orthogonality, and Hidden Regularization (as shown in Table 1) are implemented, nor whether these are used concurrently with the main “Two CEs” objective or as independent ablations.\n- Ambiguity in Section 4.1. The experiment uses CIFAR-10 but states that “inputs are ImageNet last-layer embeddings.”\nPlease clarify this setup: are ImageNet features used merely as a fixed embedding extractor, and are all MLP layers subsequently replaced with NSN layers?"}, "questions": {"value": "1. Why choosing exponential in Equation 3 as the coefficient.\n\n2. Is there any empirical evidence about assumption 1.\n\n3. In Figure 7, it is shown that the model's score on the evaluation  benchmarks decreases continuously and monotonically as the rank is  reduced. How can we prove that this decline represents the desired  hierarchical degradation in reasoning ability, rather than simply a  score reduction caused by a decrease in the model's overall expressive  power? \n\nFor instance, is the score drop a result of the model becoming  inconsistently capable of solving tasks of the same difficulty level  (i.e., succeeding on some while failing on others)? Or is it due to the  model becoming uniformly incapable of solving a whole category of  problems at a certain difficulty level? Only the latter would  demonstrate an effective adjustment of reasoning and computational  effort according to task difficulty.\n\n4. A recent paper [1] on implicit regularization in matrix factorization (which is not cited in Related Works) also analyzes the hierarchical training dynamics that emerge when progressively increasing the matrix rank—an idea conceptually related to the nested structure proposed in this paper (see, for example, their Proposition 1). One of their key findings is that even without explicit rank constraints, the model naturally evolves from low-rank to high-rank representations during optimization. It would strengthen the theoretical positioning of this work if the authors could discuss or compare the necessity of explicitly enforcing rank constraints in NSNs.\n\n[1] Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion, NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VzxIiOBfax", "forum": "ymUOPsbxLi", "replyto": "ymUOPsbxLi", "signatures": ["ICLR.cc/2026/Conference/Submission9625/Reviewer_bkcs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9625/Reviewer_bkcs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897149543, "cdate": 1761897149543, "tmdate": 1762921161560, "mdate": 1762921161560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a model architecture called Nested Subspace Networks (NSNs), which can be applied on top of modern neural network architectures both during pretraining and after pretraining. NSNs consist of a series of low rank subnetworks that approximate the original neural network, thereby reducing computational FLOPs during training, adaptation, and inference. A novel optimization procedure based on uncertainty estimation is also proposed to jointly optimize all subnetworks within the NSN architecture. Both theoretical analysis and experimental results are provided to demonstrate the effectiveness of the proposed NSNs and the optimization method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is overall well written and well motivated.\n- Improving the computational efficiency of large deep models and large language models is of practical importance.\n- The proposed NSN model architecture is clear and easy to follow."}, "weaknesses": {"value": "- The NSN architecture does not appear to be highly novel. It essentially applies the slimmable neural network [1] on top of a low rank model architecture (across the rank dimensions).\n- The proposed \"training with multi rank uncertainty\" procedure is not clearly explained. I suggest adding an algorithm box to illustrate the detailed training steps.\n- Only FLOPs are reported; wall clock time would be a more informative metric to demonstrate both training and inference speedups.\n- It is not clear, during model training, what the overall cost of training an NSN is compared to the standard training of a dense model with a similar number of parameters.\n\n[1] https://arxiv.org/abs/1812.08928"}, "questions": {"value": "- The methods proposed in [2] seem to be very promising on top of slimmable networks. I wonder if they are also applicable to NSNs.\n\n\n[2] https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Universally_Slimmable_Networks_and_Improved_Training_Techniques_ICCV_2019_paper.pdf"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The reviewer does not seem to find any ethics concerns of this paper."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ehgzRutFj3", "forum": "ymUOPsbxLi", "replyto": "ymUOPsbxLi", "signatures": ["ICLR.cc/2026/Conference/Submission9625/Reviewer_Ji5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9625/Reviewer_Ji5G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327980681, "cdate": 1762327980681, "tmdate": 1762921161344, "mdate": 1762921161344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response (1/2): Summary of experiments and findings"}, "comment": {"value": "We thank the reviewers for their insightful and constructive feedback.\n\nReviewers highlighted that the paper is “overall well written and well motivated” (**R-Ji5G**) and proposes a “conceptually novel framework that unifies multiple model capacities within a single parameterization” (**R-bkcs**). They emphasized that “improving the computational efficiency of large deep models and large language models is of practical importance” (**R-Ji5G**), and viewed the approach as “highly flexible and practical as it can be broadly applied to any pretrained models” (**R-TDZV**).\n\nThe NSN architecture itself was described as “clear and easy to follow” (**R-Ji5G**) and “interpretable in the sense that it is straight-forward to inspect how much each rank learns to represent both empirically and theoretically,” with the theoretical discussion providing “good intuition of the approximation trade-offs involved” (**R-TDZV**). Empirically, reviewers noted that the approach is “validated through controlled experiments, ablation analyses, and applications to pre-trained LLMs, supporting the claims of dynamic adaptability and compute–performance efficiency” (**R-bkcs**). They further highlighted as “a key practical merit” that NSNs are “post-hoc applicable” and “can be retrofitted onto existing pre-trained models without retraining from scratch, addressing a critical deployment challenge for large-scale foundation models” (**R-bkcs**).\n\n---\n# Summary of experiments and findings\n\nWe'd like to highight the **15 experiments** we've conducted as a summary. We hope these will clearly showcase the value of Nested Subspace Networks.\n\n\n| Experiment | Figure/Table | Purpose | Finding |\n|-----------|--------------|---------|---------|\n| Native rank training vs rank truncation on CIFAR-10 MLPs | Figure 2 | Compare truncation with native low-rank training. | Native low-rank training greatly outperforms naive truncation. |\n| Log-variance dynamics across ranks | Figure 3 | Examine uncertainty weights across ranks. | Uncertainty weights differentiate ranks according to difficulty. |\n| NSN vs individually trained CIFAR-10 MLPs | Figure 4 | Test NSN against specialized MLPs across ranks. | A single NSN matches specialized models across ranks. |\n| Ablation of multi-rank training objectives | Table 1 | Compare alternative multi-rank training objectives. | Two CEs objective improves low and intermediate ranks. |\n| Training dynamics at interpolated ranks | Figure 5 | Assess stability at interpolated (untrained) ranks. | Two CEs stabilizes accuracy at interpolated ranks. |\n| Post-hoc adaptation of pre-trained LLMs | Figure 7 | Evaluate NSNs’ accuracy–FLOPs trade-offs on LLMs. | NSNs yield smooth, controllable accuracy–FLOPs trade-offs. |\n| Inter-layer similarity vs rank in NSN-adapted LLM | Figure 8 | Analyze inter-layer similarity across ranks. | Low ranks share representations; higher ranks specialize by layer. |\n| Subspace containment across ranks | Figure 9 | Test nested subspace property empirically. | Lower-rank subspaces are nearly contained in higher-rank subspaces. |\n| NSN vs standard dense fine-tuned model | Figure 10 | Compare NSN weights to dense fine-tuned weights. | NSN weights differ systematically from dense fine-tuned weights. |\n| Energy decay in NSN rank-1 components | Figure 11 | Check energy decay across NSN components. | NSN components show clear energy decay across indices. |\n| Energy profiles in dense GPT-NeoX layers | Figure 12 | Inspect energy profiles in dense layers. | Dense models lack ordered energy decay structure. |\n| Aggregate energy-decay violation rates | Figure 13 | Quantify energy-order violations in NSN vs dense. | NSNs almost never violate energy-decay ordering; dense models do. |\n| Violation rates by transformer depth | Figure 14 | Study violations as a function of depth. | Dense models show high violation rates across depths; NSNs do not. |\n| Layer-wise violation statistics | Table 4 | Provide detailed per-layer violation statistics. | Most NSN layers have very few energy-order violations. |\n| FLOPs–accuracy trade-off via surgical replacement (Pythia-2.8B) | Figure 15 | Quantify Pythia-2.8B FLOPs–accuracy trade-off. | Halving FLOPs costs roughly five accuracy points on Pythia-2.8B. |"}}, "id": "2xUUqNRX0v", "forum": "ymUOPsbxLi", "replyto": "ymUOPsbxLi", "signatures": ["ICLR.cc/2026/Conference/Submission9625/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9625/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission9625/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763673287147, "cdate": 1763673287147, "tmdate": 1763673287147, "mdate": 1763673287147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response (2/2): Changes after the rebuttal"}, "comment": {"value": "# Changes after the rebuttal\nIn the uploaded PDF, we have highlighted the changes in the main text with a bright distinct color. We did not highlight changes in the appendix. **We implemented 11 changes that address all the reviewers' concerns**. \n\n**Major change summary.** We now make explicit that NSNs are not *slimmable low-rank layers* but a parameter-space reparametrization that enforces a nested sequence of subspaces in every linear layer, jointly trained with a principled multi‑rank objective so that **(i)** the function class at rank $r$ is a strict subset of that at rank $r +1$; **(ii)** performance between trained ranks is theoretically controlled via an interpolation bound; and **(iii)** a single model provides a continuous, monotone compute–performance frontier that can be surgically retrofitted onto pre‑trained LLMs. We complement this with: **(iv)** an explicit training algorithm, **(v)** a complexity analysis of NSN training vs dense baselines, and **(vi)** additional empirical diagnostics (energy decay and subspace containment in LLMs), so that the paper now provides a complete, theoretically grounded and practically usable recipe for turning existing large models into compute‑adjustable ones.\n\nWe summarize the major changes here:\n\n| Change | Purpose | Reviewer(s) addressed | Summary |\n| --- | --- | --- | --- |\n| Clearer motivation and revised intro (Sec. 1) | Remove ambiguity and give concrete deployment scenarios | TDZV | Added three specific use-cases and replaced unclear phrasing. |\n| Clarified NSN architecture, definitions, and visualization (Sec. 2.1, Fig. 1, App. D.1) | Make the mechanism and nesting explicit | TDZV, Ji5G, bkcs | Added explicit A/B explanation, corrected figure, and added toy matrix example. |\n| Added full training algorithms (Alg. 1; Algs. 2–4 in App. E) | Specify training steps and rank sampling | Ji5G, bkcs, TDZV | Introduced pseudocode detailing anchor/variant ranks and updates. |\n| Expanded explanation of uncertainty-weighted objective (Sec. 2.2–2.3, App. D.2, D.6) | Explain weighting, exponentials, and balancing | bkcs, TDZV | Added derivation and interpretation of exp(−s) and gradient balancing. |\n| Added empirical validation of Assumption 1 (Sec. 3; App. C.4–C.5, Table 4) | Support smooth interpolation theory | bkcs | Added energy-decay plots and violation-rate measurements across layers. |\n| Clarified CIFAR-10/MLP setup (Sec. 4.1, App. B.3) | Resolve confusion about embeddings and baseline choices | bkcs, TDZV | Specified ImageNet-feature pipeline and rationale for excluding LoRA. |\n| Added implementation details for regularizer ablations (Sec. 4.2, App. B.2) | Clarify how each regularizer is applied | bkcs | Documented each regularizer and confirmed they are never combined. |\n| Extended Related Work + new Appendix A | Better distinguish NSNs from prior methods | Ji5G, bkcs, TDZV | Added discussion of slimmable nets, dynamic models, flag manifolds, and matrix factorization. |\n| Added training-cost analysis (App. D.5) | Quantify NSN training overhead | Ji5G | Provided FLOP comparison between NSN and dense training. |\n| Added theoretical/empirical insights + limits (Sec. 6; App. C.1–C.3) | Address layer-rank granularity and behavior | TDZV | Added findings on layer specialization and limits of uniform ranks. |\n| Added energy-decay and violation-rate plots (App. C.4–C.5) | Visualize nestedness and ordering | bkcs, TDZV | Added plots showing consistent subspace ordering vs. dense baselines. |\n\n\n---\n# Thank you\nThe review has been extremely productive. We thank everyone for their help in shaping the paper to be in better form."}}, "id": "LeAUeEmZX5", "forum": "ymUOPsbxLi", "replyto": "ymUOPsbxLi", "signatures": ["ICLR.cc/2026/Conference/Submission9625/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9625/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission9625/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763673320976, "cdate": 1763673320976, "tmdate": 1763673320976, "mdate": 1763673320976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}