{"id": "eQ0bqlU7Zi", "number": 7884, "cdate": 1758040819817, "mdate": 1759897824531, "content": {"title": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage", "abstract": "As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is on the rise. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups that lack basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular industry multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of industry use, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the *presence of data access control*. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the failure of safety research to generalize from single-agent to multi-agent settings, indicating the serious risks of real-world privacy breaches and financial loss.", "tldr": "Introduces a novel attack vector compromising orchestrator multi-agent system safety, demonstrating single-agent safety does not generalize to multi-agent settings", "keywords": ["Multi-Agent Systems", "Adversarial Robustness", "Indirect Prompt Injection", "Model Evaluation", "LLM Agents", "LLM Security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cce03a406f3c9ad9cde2b4d9c0bf171b9e4caec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce an attack on Multi-Agent Systems called OMNI-LEAK, which compromises several agents to leak sensitive data through a single indirect prompt injection, even in the presence of data access control. \n\n\nThe attack is measured on several frontier models finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the failure of safety research to generalize from single-agent to multi-agent settings, indicating the serious risks of real-world privacy breaches and financial loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, clear and addresses an important vulnerability in data leakage within agents and multi-agent systems."}, "weaknesses": {"value": "- **Novelty:** The original control flow hijacking paper demonstrates data leakage (using Python and online input vs. SQL in this paper). Outside of changing the language and the input modality, I am struggling to see how this paper differentiates itself from the original. At the very least CFH should be used as a baseline in this work. \n- **Evaluations:** While any instance of data leakage is potentially catastrophic, in some of the authors' experiments, the attack does not succeed and in others takes several (500) tries to succeed.\n- **Defenses:** Some defense like [this one](https://arxiv.org/html/2502.01822v1) are designed, in part, for this vulnerability. These should be evaluated against."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "thKHvmydtq", "forum": "eQ0bqlU7Zi", "replyto": "eQ0bqlU7Zi", "signatures": ["ICLR.cc/2026/Conference/Submission7884/Reviewer_XCKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7884/Reviewer_XCKC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766765704, "cdate": 1761766765704, "tmdate": 1762919920499, "mdate": 1762919920499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors designed an attack method named OMNI-LEAK over an industrial multi-agent pattern named orchestrator to induce multiple agents to leak information through a pervasive approach. Attackers need only inject once for leaks to propagate across layers. The authors argue that even if each submodel is individually secure, collaborative modes may introduce new attack surfaces. Multi-agent data leakage is a structural issue: as long as information is conveyed in natural language among agents, attackers can exploit contextual reasoning to achieve leakage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is closely integrated with the industry, employing a comprehensive orchestrator framework. Addressing multi-agent data leakage is of broad relevance to the community.\n2. The details of OMNI-LEAK prompt injection are described very clearly and are easy to understand.\n3. The verizon that even if each agent is secure individually, the combination may still lead to emergent vulnerabilities, is of great insight.\n4. The article provides numerous detailed steps for reproduction, making it highly reproducible."}, "weaknesses": {"value": "1. The paper leans towards engineering, with insufficient attack formulations presented. The procedures lack formulaic standardization.\n2. The paper lacks analysis and comparison with other attack methods, such as direct prompt injection and jailbreak attacks. This would better highlight OMNI-LEAK's advantages, enabling deeper analysis—whether in terms of robustness or the expected number of queries required for a successful attack.\n3. The insight that even if each agent is individually secure, their composition can still yield emergent vulnerabilities is particularly compelling, revealing a structural weakness in collaborative agent systems.\n4. The experimental design was overly simplistic, lacking diverse baselines and rich metrics, and failed to yield findings with insight or generalizability."}, "questions": {"value": "1. Please provide some symbolic or theoretic formalization of their attack chain? It seems this would be useful both to try to generalize results, and to reason about other orchestrator-based workflows.\n2. Please quantify the magnitude of leakage (e.g., number of leaked tokens, sensitivity levels) to compare with different types of attacks? I believe this study can enhance the technical quality.\n3. Please include analysis for direct prompt injection and jailbreak, as well as the defense methods of 10 kinds of indirect prompt injection attacks.\n4. The paper assumes free-form natural-language passing via an orchestrator. Must this communication mechanism be free-form to enable OMNI-LEAK, or would  schema or parameterized calls prevent the layered propagation? Please clarify."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper contains benchmarking elements, and the author should make the code publicly available to verify whether the data contains any privacy leaks. Even without such checks, the code itself should be made publicly available."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gic4ypnEPi", "forum": "eQ0bqlU7Zi", "replyto": "eQ0bqlU7Zi", "signatures": ["ICLR.cc/2026/Conference/Submission7884/Reviewer_uFkU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7884/Reviewer_uFkU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779937298, "cdate": 1761779937298, "tmdate": 1762919919577, "mdate": 1762919919577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a prompt injection attack within an orchestrator multi-agent setup, in which hidden malicious instructions are injected into public data to induce an SQL agent to leak private information. A benchmark is designed to evaluate the vulnerability of various LLMs against the proposed attack."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "S1. The writing is clear and easy to follow.\n\nS2. Currently, there is limited research on the security of multi-agent LLM systems, and the authors are trying to contribute to a promising emerging direction."}, "weaknesses": {"value": "W1. Although the paper claims to propose a novel data leakage attack that compromises multiple agents, the method just employs conventional hidden prompt injection attacks to manipulate the SQL agent for unauthorized data access. I do not believe this approach represents a significant advancement over existing work. It is unclear to me what new defensive challenges are introduced by the attack proposed in this paper. It appears that existing defenses against indirect prompt injection attacks [a] could already mitigate the proposed attack.\n\n[a] Can Indirect Prompt Injection Attacks Be Detected and Removed? ACL 2025.\n\nW2. The paper does not discuss any potential defense mechanisms.\n\nW3. The proposed benchmark does not incorporate any defense mechanisms, and therefore, I do not believe it appropriately reflects real-world attack challenges.\n\nW4. The evaluations consider only data scenarios within the employees/HR domain, which raises concerns about the generalizability of the experimental findings.\n\nW5. The paper claims that the proposed method can be extended to other orchestrator setups, but I did not find any detailed discussion or supporting evidence."}, "questions": {"value": "Q1. Compared to indirect prompt injection attacks, what new defensive challenges does the proposed attack introduce?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IggM5mhTiL", "forum": "eQ0bqlU7Zi", "replyto": "eQ0bqlU7Zi", "signatures": ["ICLR.cc/2026/Conference/Submission7884/Reviewer_eJQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7884/Reviewer_eJQb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890422882, "cdate": 1761890422882, "tmdate": 1762919918856, "mdate": 1762919918856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage investigates security vulnerabilities in multi-agent systems, particularly within the orchestrator setup where a central agent delegates tasks to specialized agents. The authors present a novel attack vector, OMNI-LEAK, which exploits indirect prompt injections to compromise multiple agents and leak sensitive data, even when data access control safeguards are in place. The study demonstrates that various state-of-the-art language models, except for Claude-Sonnet-4, are vulnerable to such attacks and highlights the importance of considering multi-agent vulnerabilities in security research."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new attack vector, OMNI-LEAK, and addresses the gap in research on multi-agent system vulnerabilities, which is a critical area for ensuring the safety of modern AI applications.\n\n2. The authors provide comprehensive experiments across different models, databases, and attack categories, offering a detailed analysis of model susceptibility to OMNI-LEAK.\n\n3. The study uses practical examples, such as employee database management, to demonstrate the vulnerability of multi-agent systems, making the findings highly relevant for both academia and industry."}, "weaknesses": {"value": "1. The beginning of Chapter 4 introduces the experimental setup, but there is no clear justification for why this particular setup was chosen. The authors should provide a more robust explanation of the rationale behind the design of the system. This could be done through theoretical reasoning or supported by practical insights, such as findings from similar studies or real-world surveys. Without this, the setup appears arbitrary and weakens the foundation of the paper.\n\n2. While terms such as \"SQL agent\" are used frequently, their definitions are not fully explained. As different researchers may interpret such terms differently, it is important to align the terminology used and provide clear definitions. For instance, the role and functionality of the SQL agent should be clearly stated to avoid confusion and ensure that readers can follow the arguments easily.\n\n3. The paper frequently mentions the Claude-Sonnet-4 model’s robustness in dealing with attacks, but there is a lack of in-depth analysis of why this model demonstrates such robustness. A more thorough exploration into the characteristics, design decisions, or training methods that contribute to its unique resistance to attacks would strengthen the paper's contribution and provide useful insights for both security researchers and AI model developers."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EFIgKQ52aE", "forum": "eQ0bqlU7Zi", "replyto": "eQ0bqlU7Zi", "signatures": ["ICLR.cc/2026/Conference/Submission7884/Reviewer_6R2i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7884/Reviewer_6R2i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972638487, "cdate": 1761972638487, "tmdate": 1762919918089, "mdate": 1762919918089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper presents study of large language model (LLM) -based orchestrated multi-agent system risks for security vulnerabilities of leaking private data with indirect prompt injections. A novel attack vector is presented and experimented in office employee/HR use case with several public LLM-based agents with quantitative analyses. As a results both reasoning and non-reasoning LLMs are vulnerable to proposed attacks in the networked agent setup."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The topic is important and timely. To my knowledge, studying of the coordinated multiple LLM agent setup for data leakage with prompt injection attacks, is new. Paper is clearly written and has good illustration of the problem and attack executions. In the specific benchmark, different aspects (target models, database sizes, attack categories and level of information accessed) are evaluated by measuring the statistics of query accuracy and successful attacks, given some interesting findings where almost all models are vulnerable for these specific attacks of leaking the private data. In high level this might provide valuable information for the community.\n\nSummary of the strengths\n- Important and timely problem\n- Novel multi-agent prompt attack setup and benchmark\n- Benchmark evaluation gives some interesting findings how easily the models leak the data"}, "weaknesses": {"value": "Although in high-level the study setup and benchmark is well-defined, there are some limitations. The main weaknesses are related to the lack of detailed analysis of the models, decreasing the significance and quality of the contributions. Now, the concluding remarks are quite speculative, and the reader is left with the question of why certain models are more or less vulnerable. Is it the fine-tuning or some other aspect of the model or training data effecting the results. From these perspectives, if possible with these black-box model, it would be good to analyse more detailed the effect of fine-tuning and other constrains (e.g. RAGs) to the success of attacks. Also, based on the analysis it would be good prepare a summary or list of actionable item for the practitioners of how to utilise the findings.\n\nSummary of weaknesses\n- Limited and speculative reasoning and analysis of the particular models; what are the characteristics why certain (black box) model is more or less vulnerable\n- Limited analysis of the details of the models and what would be the effects of possible fine-tuning/RAG etc. to success of the injection\n- Missing a summary/list of actionable items for engineers/practitioners of how to utilise the findings"}, "questions": {"value": "- The concluded analyses of the attack success considering the target models are quite speculative. Are there any possible additional techniques that could be used to analyse what are the common characteristics or patterns or what are the properties of certain LLM leading to worse or better protection of the attacks?\n- Are there any summary (of lesson learned) for practitioners and engineers in industry of how to prepare for these kind of vulnerabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DksbOXKJGF", "forum": "eQ0bqlU7Zi", "replyto": "eQ0bqlU7Zi", "signatures": ["ICLR.cc/2026/Conference/Submission7884/Reviewer_bo4i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7884/Reviewer_bo4i"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994100145, "cdate": 1761994100145, "tmdate": 1762919917717, "mdate": 1762919917717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}