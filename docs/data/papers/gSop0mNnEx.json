{"id": "gSop0mNnEx", "number": 19560, "cdate": 1758297266860, "mdate": 1759897032575, "content": {"title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential in enhancing single LLMs to address complex and diverse tasks in practical applications. Despite considerable advancements, the field lacks a unified codebase that consolidates existing methods, resulting in redundant re-implementation efforts, unfair comparisons, and high entry barriers for researchers. To address these challenges, we introduce MASLab, a unified, comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab integrates over 20 established methods across multiple domains, each rigorously validated by comparing step-by-step outputs with its official implementation. (2) MASLab provides a unified environment with various benchmarks for fair comparisons among methods, ensuring consistent inputs and standardized evaluation protocols. (3) MASLab implements methods within a shared streamlined structure, lowering the barriers for understanding and extension. Building on MASLab, we conduct extensive experiments covering over 10 benchmarks and 5+ models, offering researchers a clear and comprehensive view of the current landscape of MAS methods. MASLab will continue to evolve, tracking the latest developments in the field, and invites contributions from the broader open-source community.", "tldr": "", "keywords": ["LLMs", "Multi-Agent Systems", "Codebase"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b98d50786e7004c96806f6f2936d96654c5c2f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents MASLab, a unified codebase for multi-LLM systems.\nMASLab integrates 20+ methods and 10 benchmarks, with the goal of removing re-implementation efforts, the possibility of unfair comparisons, and high entry bars for new researchers.\nThe authors conducted extensive empirical studies to analyze the performance, cost (in tokens) and scaling properties.\n\nI think the paper is solving an important problem in multi-LLM research, but I also have 2 concerns.\nI'm wiling to raise my score if they can be addressed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The primary strength of this paper is the codebase, it allows future researchers to conduct experiments in a unified and easier way.\n* The empirical studies presented in the paper are extensive and comprehensive."}, "weaknesses": {"value": "* The codebase is not available in this submission, making it hard to evaluate its easiness of adoption. Since the major contribution of this paper (i.e., a unified codebase) is more engineering than novel research, the lack of the source code makes the impact less convincing.\n* Although the authors claimed that step-by-step output verification was conducted to ensure a validated implementation, quantitative evidence is insufficient (e.g., Table 6 shows the comparison in AFlow, but the rest methods are unknown). While providing source code can partly address this problem, I would like to see quantitative results reproducing 50%+ of the methods. For a paper whose primary contribution is a reliable library, this is a significant cap.\n* (Minor) Typos, e.g., line 313 \"able 2\"."}, "questions": {"value": "Can the authors provide source code and reproduce the results for more methods? See weakness 1 and 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tPJRHiN5DA", "forum": "gSop0mNnEx", "replyto": "gSop0mNnEx", "signatures": ["ICLR.cc/2026/Conference/Submission19560/Reviewer_mwB4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19560/Reviewer_mwB4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760593688046, "cdate": 1760593688046, "tmdate": 1762931442025, "mdate": 1762931442025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a codebase for LLM-based multi-agent systems, called MASLab. This codebase includes twenty existing agentic solutions, with unified data preprocessing, resource, configuration, and evaluation. The authors present plenty of experimental results on different benchmarks to demonstrate the results of different solutions implemented in MASLab."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors survey the most popular agentic solutions comprehensively, in terms of the features of agentic solutions and their applicable areas.\n2. The authors conduct a wide range of agentic benchmarks to compare the adapted versions of the agentic solutions in their MASLab.\n3. The paper presents the key results in a way that readers can understand with little effort."}, "weaknesses": {"value": "1. A key concern of this paper is its limited scientific contribution to the community. This reads essentially as a benchmark paper that evaluates various of agentic solutions. However, this paper also introduces some additional modifications to those solutions/frameworks, which may make the overall performance attribution more difficult. Namely, it is unclear whether the \"unifying\" operations and adaptations introduce the performance degradation/improvement compared to their original implementation.\n2. Another factor that blur the paper's contribution is that although it claims that a significant effort is spent in building such a unified codebase, the paper does not explicitly demonstrate the benefit of the system, such as how it becomes easier for beginners, what the benefits of using the new system are, and how general of the high-level system structure can be.\n3. Some experiment designs and their conclusions seem too casual or even meaningless, such as \"Equipping agents with tools improves MAS performance\" for the GAIA benchmark.\n4. \"MASLab-ReAct\" was never introduced but appears in experiments."}, "questions": {"value": "1. Why can different evaluation protocols introduce such differences as shown in Figure 3? How does it relate to the capability of LLMs used in judging and the backbone of the agent?\n2. How can the paper guarantee that the unifying operation on the input and other components does not introduce unnecessary performance degradation? How to ensure the contexts fed to LLMs in the unified ones have the same effect as the original solutions or better than those?\n3. In the error analysis, how can one tell whether the error is because of \"incorrect final answers\" or \"errors in tool usage\"? Or, what scenarios exactly are included in \"incorrect final answers\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tpb2Lk6b9P", "forum": "gSop0mNnEx", "replyto": "gSop0mNnEx", "signatures": ["ICLR.cc/2026/Conference/Submission19560/Reviewer_cgcz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19560/Reviewer_cgcz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760941740383, "cdate": 1760941740383, "tmdate": 1762931441429, "mdate": 1762931441429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MASLab, a unified and comprehensive codebase for LLM-based multi-agent systems (MAS). MASLab addresses key challenges in the field, including redundant implementations, inconsistent evaluation protocols, and high entry barriers for researchers. It integrates over 20 state-of-the-art MAS methods across various domains (e.g., general tasks, coding, mathematics, science) and provides a standardized evaluation framework to ensure fair and reproducible comparisons."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces MASLab, which consolidates over 20 state-of-the-art MAS methods across multiple domains (e.g., general tasks, coding, mathematics, science). This integration reduces redundant implementation efforts\n2. MASLab addresses a critical gap in the field by offering a standardized evaluation pipeline. It ensures consistent input preprocessing, configuration alignment, and evaluation protocols, which are essential for fair and reproducible comparisons.\n3. By abstracting each MAS method into a streamlined Python class, MASLab makes it easier for researchers to understand, extend, and innovate upon existing approaches. This design also facilitates secondary development, making it accessible for both newcomers and experienced researchers."}, "weaknesses": {"value": "1. While the paper evaluates MAS methods across 10+ benchmarks, many of these benchmarks are not specifically designed for MAS. \n2. In the evaluation system, the default xVerify method is supervised. Does this imply that users must train new evaluation models when extending to new assessment tasks? \n3. The work lacks theoretical or methodological innovation. Its focus is primarily on engineering and software development. Although some interesting phenomena were observed during the evaluation of different MASs, there was no in-depth analysis or exploration of the underlying causes or principles of these phenomena. For instance, the paper mentions the significant impact of evaluation protocols on the final results and analyzes failure cases of MAS systems; however, it does not provide detailed analyses of these findings and instead emphasizes engineering solutions to these issues."}, "questions": {"value": "1. Can some recent benchmarks specifically designed for MAS better highlight the relevance of MASlab?\n2. What are the scalability and computational costs of the xVerify method? Does it support users in freely extending to arbitrary evaluation tasks and datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RhAL9LHltq", "forum": "gSop0mNnEx", "replyto": "gSop0mNnEx", "signatures": ["ICLR.cc/2026/Conference/Submission19560/Reviewer_MHpj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19560/Reviewer_MHpj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976438883, "cdate": 1761976438883, "tmdate": 1762931440871, "mdate": 1762931440871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new LLM-based multi-agent system (MAS) framework that serves as a unified platform for implementing, running, and comparing diverse agent methods. The proposed codebase supports more than 20 existing approaches and provides standardized environments for benchmarking and evaluation. It is designed to be easily extensible, allowing integration of new methods. Experiments conducted on over ten benchmark suites demonstrate the usability and versatility of the framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a codebase (framework) that combines two capabilities: easy implementation and unified execution, as well as wide-ranging, standardized benchmarking. This identifies a real gap in prior work, as existing methods typically focus on only one of these aspects. The framework also includes a wide range of recent methods and benchmarks, which strengthens the contribution.\n\nThe experiments show broad coverage and a rigorous, thorough evaluation. This is a good indication of a versatile benchmark framework proposed in the paper."}, "weaknesses": {"value": "Novelty: While it is reasonable to claim that this work conceptually fills the gap between easily extensible multi-agent frameworks and broad benchmark coverage, prior works are not entirely lacking in this direction. For example, CRAB (https://github.com/camel-ai/crab\n, https://arxiv.org/abs/2407.01511) supports a much larger number of tasks (120) while maintaining a modular shared codebase. AgentBoard (https://openreview.net/forum?id=4S8agvKjle), while focusing mainly on unified environments and consistent metrics for comparison, also supports some degree of agent customization. In addition, AgentVerse (https://github.com/OpenBMB/AgentVerse) provides extensibility for agent methods, and CRAB allows extensibility for environments, although not simultaneously. Since this paper does both, such overlap between prior works and this study does not fully undermine the innovative aspect, but the paper would benefit from a more detailed discussion of these prior efforts and a clearer explanation of what specifically distinguishes it from them.\n\nClaimed versatility: As a framework and benchmark paper, the usability aspect (especially the claimed contribution in ease of extension) cannot be fully assessed without access to the code. Although the manuscript provides sufficient experimental details, they are not fully convincing without demonstrating actual usability through a released implementation. Even without providing an anonymous repository (understandable if that is not feasible during rebuttal), some code excerpts or minimal skeletons for agents and tasks would still be helpful in demonstrating the frameworkâ€™s design.\n\nExperiments: Given that the framework aims to be versatile, it should support and test a wider range of LLM models in addition to the already broad coverage of methods and tasks. Currently, it seems that the choice of underlying LLMs (a subset of the Llama, GPT, and Qwen families for each task) is task-specific without clear justification. Ideally, the choice of LLM models should be orthogonal to tasks and environments unless there is incompatibility, so an explanation or a more systematic model-swapping study would be expected.\n\nOverall, these issues are not fundamental flaws but rather reflect unclear novelty and incomplete validation of what appears to be a solid engineering contribution. Currently, this work is slightly below threshold due to unclear novelty, but the score could be increased if these issues are addressed and the unclear points clarified."}, "questions": {"value": "(See the unclarities in Weakness)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YbLq1jSxZd", "forum": "gSop0mNnEx", "replyto": "gSop0mNnEx", "signatures": ["ICLR.cc/2026/Conference/Submission19560/Reviewer_qex7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19560/Reviewer_qex7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978641855, "cdate": 1761978641855, "tmdate": 1762931440270, "mdate": 1762931440270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}