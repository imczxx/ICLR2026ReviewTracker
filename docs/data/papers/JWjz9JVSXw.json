{"id": "JWjz9JVSXw", "number": 9349, "cdate": 1758119640518, "mdate": 1759897730097, "content": {"title": "CMPS: Constrained Mixed Precision Search", "abstract": "The increasing complexity of deep neural networks (DNNs) requires effective model compression to reduce their computational and memory footprints for deployment on resource-constrained hardware. Mixed-precision search is a prominent bit allocation method based on neural architecture search (NAS) that has been shown to significantly reduce the DNN footprint while preserving the accuracy of the model by allocating bits to each layers based on their quantization sensitivity. However, mixed-precision search is often defined as a dual optimization problem handled with a single heuristic objective function, which does not provide strong guarantees of the resulting compression rate. We propose a post-training reformulation of mixed precision search as an explicit constrained optimization problem, solved using interior-point methods within a framework based on NAS. Our method requires minimal calibration data, as few as 128 samples, in a post-training setting. We corroborate this approach with experiments that span multiple transformer architectures with up to 4 billion parameters, using the MXFP family of data formats. We show that this constrained formulation provides users with higher resolution over compression rates, and we show that explicitly satisfying hardware budgets while optimizing for accuracy can outperform uniform allocation methods, improving performance by up to several standard deviations over the uniform baselines.", "tldr": "", "keywords": ["Neural Networks", "Quantization", "Mixed Precision", "Post Training", "Transformers", "LLMs", "Large Language Models", "Hardware", "GPU", "Data Formats", "Bit Allocation", "FP32", "MXFP", "BFP", "MXINT", "GPU", "TPU", "AI Accelerators"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/28ecc67b798990027f8e4054096efa94e2e86c22.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a differenciable NAS algorithm for data format allocation in a network. It first formalizes the optimization problem, including architecture constrains (the maximum average number of bits for a model), then proposes a gradient descent based heuristic for solving the mixed precision data format allocation problem. While the method is fully post-training, it still requires a small calibration data set to perform the training of the data format precision parameters.\nThe paper shows that using this formalism, mixed precision constrained NAS can achieve better results than uniform quantization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The mathematical formulation of the constrained optimization problem for mixed precision data format allocation seems fairly general;\n- The large number of results (which are combinations between models and tasks used for the calibration) seems to demonstrate the robustness of the approach."}, "weaknesses": {"value": "- The paper completely lack any comparison with the state-of-the-art! No comparison with other mixed-precision post-training optimization methods is even attempted... yet plenty exists. That is clearly a major issue in this paper.\n- While the fundations of differentiable NAS methods seems to be adequately described and cited, the novelty of the proposed method remains hard to grasp. I would suggest to add a short but clear statement on what it brings compared to the closest SoTA work.\n- While the method seems very general, it is frustrating see it tested on a single NAS scenario, namely, 4.5 bit mixed precision with the MXFP data format. What about mixing different formats (integer, FP...)? Or testing other maximum average number of bits (like 3.5, or 5.5...)?\n- The perplexity/accuracy gains of the method remain modest and the proposed NAS scenario is too limited."}, "questions": {"value": "Please carefully answer the issues mentioned in the weaknesses section.\n\nI may increase my rating provided that at least 1) quantitative comparison with other SoTA methods is provided. 2) Additional NAS scenario, beyond 4.5 bit mixed precision is evaluated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zOiYgAhNN6", "forum": "JWjz9JVSXw", "replyto": "JWjz9JVSXw", "signatures": ["ICLR.cc/2026/Conference/Submission9349/Reviewer_aocK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9349/Reviewer_aocK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492163690, "cdate": 1761492163690, "tmdate": 1762920976191, "mdate": 1762920976191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A DNAS-based post-training mixed-precision quantization method (CMPS) is proposed. CMPS provides fine-grained control over model compression, enabling stable and predictable performance. The proposed CMPS method is compared with uniform quantization baselines, demonstrating the advantages of learnable mixed-precision bit allocation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper works on the post-training mixed-precision quantization with controllable compression ratios. The problem studied is important and the motivation is clear.\n2. The detailed theoretical analysis is provided."}, "weaknesses": {"value": "1. Quantization details are missing. It seems that CMPS is a weight-only quantization method. However, the quantization details are not provided.\n2. Optimization cost is not provided. The advantage of PTQ is its efficiency in quantization optimization. The CMPS relies on end-to-end tuning with multiple branches. The speed and memory cost overheads should be reported.\n3. Comparison with previous methods is also missing. The authors didn't provide any quantization details, including the uniform quantization baselines. In the llm quantization literature, many high performance PTQ methods are proposed. What's the performance advantages over these methods? How the proposed CMPS can be combined with these techniques? Moreover, the authors only compared with uniform quantization baselines, the comparison with previous mixed-precision methods are missing.\n4. In several places, it says \"hardware-constrained bit allocation\", however, only \"total model size in bits\" is modeled during the optimization. Moreover, only two bit levels are explored in the bit allocation (MXFP4 and MXPF8).\n5. In the experiments part, previous methods commonly use wiki2 for calibration in addition to C4. For zero-shot scenario, only one task of LAMBADA is evaluated, which is clearly not enough. The largest model used is 3B, experiments on larger models or architectures like MoEs are also needed.\n6. In the limitations, regarding the statement \"the memory required to hold activations or gradients for multiple low-bit options might still be comparable to, or less than, holding a single higher-precision (e.g., FP16 or BF16) baseline tensor\", more careful and precise expression should be used. Many PTQ methods do not need to store all activations, and the gradients are not needed. However, in CMPS, full-precision activations of all layers and gradients are needed, which expands the memory usage. If these tensors (activations and gradients) can be stored in low-bit, then the authors should verify it use controlled experiments."}, "questions": {"value": "Please refer to the Weaknesses for further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MnytU0GO6J", "forum": "JWjz9JVSXw", "replyto": "JWjz9JVSXw", "signatures": ["ICLR.cc/2026/Conference/Submission9349/Reviewer_oKC4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9349/Reviewer_oKC4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616341174, "cdate": 1761616341174, "tmdate": 1762920975880, "mdate": 1762920975880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new constrained mixed precision search for post training quantization. To solve the constrained optimization problem the authors leverage barrier-based interior-point method. The method keeps model weights frozen, and needs only a small calibration set (128 samples). Experiment on various LLMs report consistent gains over uniform precision baselines at the same or lower effective bit budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work discusses the problem of reducing computational and memory footprints for deployment of DNNs which is practical and important.\n\n2. The paper is well written and easy to follow.\n\n3. 4.5-bit the proposed method often beats MXFP in terms of perplexity, on the examined benchmarks."}, "weaknesses": {"value": "1. The authors claim that after rounding there always remains a strictly feasible solution with respect to the budget. I believe a proof for this claim is required.\n\n2. The comparison is limited. The work only compares itself to the MX baselines but there are many other strong PTQ techniques. Only a single dataset was used in the experiments.\n\n3. No thoughput\\latency comparisons are provided.\n\n4. The improvement over the baselines is marginal.\n\n5. How does the method operate compared to integer PTQ techniques?\n\n6. According to the experiments, the proposed algorithm does not always meet the constraint."}, "questions": {"value": "How were the samples for calibration chosen?\n\nWhat is the meaning of the upsidedown question mark in the caption of Figure 2?\n\nThere is a typo in  in line 75 (double \"â€œOur contributions are as follows:\")"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kM2BVpcBK5", "forum": "JWjz9JVSXw", "replyto": "JWjz9JVSXw", "signatures": ["ICLR.cc/2026/Conference/Submission9349/Reviewer_cXc3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9349/Reviewer_cXc3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762330424083, "cdate": 1762330424083, "tmdate": 1762920975654, "mdate": 1762920975654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}