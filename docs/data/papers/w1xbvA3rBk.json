{"id": "w1xbvA3rBk", "number": 23922, "cdate": 1758350367100, "mdate": 1763408387645, "content": {"title": "GaussGym: An open-source real-to-sim framework for learning locomotion from pixels", "abstract": "We present a photorealistic robot simulator that integrates 3D Gaussian Splatting as a drop-in renderer within vectorized physics simulators such as IsaacGym. This enables unprecedented speed—exceeding 100,000 steps per second on consumer GPUs—while maintaining high visual fidelity, which we showcase across diverse tasks. We additionally demonstrate its applicability in a sim-to-real robotics setting. Beyond depth-based sensing, our results highlight how rich visual semantics improve navigation and decision-making, such as avoiding undesirable regions. We further showcase the ease of incorporating thousands of environments from iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs from generative video models like Veo, enabling rapid creation of realistic training worlds. This work bridges high-throughput simulation and high-fidelity perception, advancing scalable and generalizable robot learning, and allowing researchers to benchmark their visual locomotion algorithms. All code and data will be open-sourced for the community to build upon. Videos, code, and data are available on the project website: https://gauss-gym.com", "tldr": "We integrate 3D Gaussian Splatting into fast vectorized simulators, achieving photorealism at over 1M FPS on consumer GPUs and reducing the sim-to-real gap across diverse tasks.", "keywords": ["simulation", "photoreal", "robotics", "real2sim", "sim2real"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cdf2d987245b45e0ea6de352458b5da7a1c4d5db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents GaussGym, an open-source framework for photorealistic robot simulation that integrates 3D Gaussian Splatting (3DGS) as a drop-in renderer within the vectorized physics simulator IsaacGym. GaussGym can create diverse training worlds by ingesting data from various sources: smartphone scans, large-scale scene datasets (e.g., GrandTour), and outputs from generative video models like Veo, followed by a standard reconstruction pipeline with GSplat. The authors demonstrate the framework's utility by training visual locomotion and navigation policies for humanoid and quadrupedal robots using RL directly from RGB observations."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The stated throughput of 100,000 steps per second with $640 \\times 480$ resolution RGB/Depth rendering across 4,096 parallel environments is a state-of-the-art result for a photorealistic simulator.\n\n2. Ingesting data from various data sources into the simulation environment is a good idea.\n\n3. The open-sourced simulation framework can benefit the community.\n\n4. The paper is generally well-structured and clearly presented."}, "weaknesses": {"value": "1.  **Overclaim and Lack of Novelty.** The most significant weakness lies in the limited novelty compared to prior works [1][2]. The claim in L097 — “a first step toward closing the visual sim-to-real gap” — appears overstated, as previous research [2][3] has already demonstrated that visual RL policy training in simulation can help bridge this gap. Furthermore, the proposed “splat-integrated simulator for evaluating locomotion policies” concept is closely related to [2], which already explored real-to-sim-to-real frameworks for visual locomotion and navigation. \n\n2. **Limited Technical Contribution.** The proposed framework primarily integrates reconstructed 3DGS)scenes from diverse sources into a simulator, achieving 4,096 FPS at 640×480 resolution on a single RTX 4090 GPU. However, the high rendering speed advantage stems directly from the Gsplat renderer’s inherent parallelization, rather than from novel system design. Additionally, the overall visual policy training pipeline follows a design highly similar to [2], limiting the perceived technical innovation.\n\n3. **Lack of Comparion for Visual Locomotion**.  Although the paper presents ablation studies demonstrating the benefits of the voxel grid head and DINO encoder, it lacks direct quantitative comparisons with state-of-the-art depth- or geometry-based locomotion policies (e.g., ANymal Parkour, Miki et al.). Metrics such as success rate or velocity tracking error on common benchmark terrains (e.g., stair climbing) are missing. The main quantitative table (Table 2) focuses solely on internal ablations and a “blind” baseline, without showing performance against geometric SOTA methods. Moreover, there are no comparisons to prior visual locomotion baselines [2][3], further weakening the experimental validation.\n\n4. **Questionable Experimental Design for Visual Locomotion and Navigation.** The visual locomotion experiments, such as the stair climbing task, do not convincingly justify the need for an RGB-based policy, since depth-based parkour policies are also capable of handling similar tasks. While the authors claim that RGB-based policies capture semantic cues, the goal-tracking navigation task does not effectively demonstrate this advantage. To substantiate the argument, the paper should include higher-level semantic tasks (e.g., obstacle-type recognition or affordance-based navigation) that genuinely highlight the semantic reasoning benefits of RGB perception.\n\n**References:**\n\n[1] Xie, Ziyang, et al. \"Vid2sim: Realistic and interactive simulation from video for urban navigation.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[2] Zhu, Shaoting, et al. \"Vr-robo: A real-to-sim-to-real framework for visual robot navigation and locomotion.\" IEEE Robotics and Automation Letters (2025).\n\n[3] Yu, Alan, et al. \"Learning visual parkour from generated images.\" 8th Annual Conference on Robot Learning. 2024."}, "questions": {"value": "1. The paper adopts VGGT for camera pose calibration and coarse point cloud extraction. While VGGT offers fast inference, its pose accuracy is typically lower than SfM and BA-based approaches such as COLMAP or GLOMAP. How does this reduced accuracy affect the downstream policy learning or simulation fidelity in your framework?\n\n2. When deploying the trained visual locomotion policies on real robots, what kind of computational hardware is used? The DINO encoder appears relatively heavy for onboard inference — have the authors evaluated its runtime performance and feasibility for real-time deployment on embedded platforms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4qGFyl8jEI", "forum": "w1xbvA3rBk", "replyto": "w1xbvA3rBk", "signatures": ["ICLR.cc/2026/Conference/Submission23922/Reviewer_xv3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23922/Reviewer_xv3h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979775640, "cdate": 1761979775640, "tmdate": 1762942858796, "mdate": 1762942858796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Comparison to works mentioned by reviewers (1/2)"}, "comment": {"value": "We would like to take this opportunity to clearly articulate how each related effort differs from ours and how our framework advances the field:\n\n**\\[1\\] Xie, Ziyang, et al. “Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation.”**\n\n**What they did**:\n\n* Construct Gaussian splat scenes from real-world monocular videos.  \n* Introduce dynamic obstacles into those scenes.  \n* Conduct experiments on \\~30 environments for PointGoal navigation and SocialNav tasks.  \n* Release publicly available code.\n\n**What they did *not* do:**\n\n* Support legged locomotion—only wheeled robot navigation.  \n* Perform large-scale or high-throughput experiments.  \n* Provide a framework aimed at generating large, reusable datasets for the community.  \n* Gather challenging locomotion terrains (e.g., stairs, rugged terrain, steep hills)—their data consists primarily of flat urban environments designed for wheeled navigation tasks.\n\n**\\[2\\] Zhu, Shaoting, et al. “VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion.”** (Cited in original manuscript)\n\n**What they did:**\n\n* Introduced occlusion-aware composition and domain randomization techniques.  \n* Applied domain randomization for RL training of a hierarchical navigation policy (we also incorporate these standard elements).  \n* Demonstrated successful navigation based on visual input.\n\n**What they did *not* do:**\n\n* Provide a clean or generalizable coordinate-alignment method.  \n* Train a low-level locomotion policy from visual input—the low-level controller is based on existing methods and receives depth directly.  \n* Move beyond navigation—the focus remains exclusively on navigation tasks.  \n* Incorporate motion-blur augmentation.  \n* Address outdoor, unstructured, or diverse terrains—work is limited to indoor settings.  \n* Conduct wide-scale evaluation—experiments are performed in a single indoor environment.\n\n**\\[3\\] Yu, Alan, et al. “Learning Visual Parkour from Generated Images.” CoRL 2024\\.** (Cited/benchmarked against in original manuscript)\n\n**What they did:**\n\n* Used a controllable image-generation model to create policy rollouts paired with visual observations.  \n* Trained a visual locomotion policy using DAgger (not end-to-end) based on their proposed visual–data–generation pipeline.  \n* Demonstrated small-scale experiments illustrating how Gaussian splatting can be used to evaluate policies in simulation.\n\n**What they did *not* do:**\n\n* Train policies directly on Gaussian splats.  \n* Achieve fast rendering—slow image-model rendering required the use of DAgger rather than scalable RL.  \n* Automate scene scaling and alignment—these steps were performed manually.  \n* Provide an end-to-end, single-stage pipeline for policy training directly from pixels.  \n* Enable efficient training—overall training time remains significantly higher than in real-time Gaussian-based simulators.  \n* Produce temporally consistent renders—the image-generation model introduces temporal inconsistencies, causing scene semantics to vary drastically across frames.\n\n**\\[4\\] Yu et al. Real2Render2Real Scaling Robotic Manipulation Data Without Dynamics Simulation or Robot Hardware, CORL 2025 Oral.**\n\n**What they did:**\n\n* Compute 3DGS of rigid and articulated *objects* to be used as assets in simulation.  \n* Capture *human demonstrations* of object *manipulation.*  \n* Provide scans and demonstrations for \\< 10 objects\n\n**What they did *not* do:**\n\n* Tackle *locomotion* with their pipeline. Instead they focus on imitation learning and object manipulation.  \n* Provide a dataset of thousands of *scenes*. Dataset limited to \\< 10 objects.  \n* Enable physical contact interaction with objects. Objects are treated as hollow, and data is used only for imitation learning.  \n* Support reinforcement learning workflows due to the lack of contact physics for scanned objects\n\n**\\[5\\] Chablani et al. EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device, ICCV 2025\\.**\n\n**What they did:**\n\n* Support generation of 3DGS and meshes from phone scans.  \n* Gaussian rendering during simulation for their indoor, flat-ground scenes.  \n* Solve the visual navigation task for a wheeled robot in indoor environments.\n\n**What they did *not* do:**\n\n* Tackle *locomotion* with their pipeline. They instead focus on wheeled robot navigation in indoor environments.  \n* Provide a dataset of diverse indoor and outdoor scenes. They utilized indoor scene datasets, whereas GaussGym uses *5,000* indoor and outdoor scenes with challenging locomotion terrain.  \n* Support *fast, parallelized environments.* GaussGym can render 4,096 environments in parallel at 100K simulator steps per second."}}, "id": "HDILBEMPvj", "forum": "w1xbvA3rBk", "replyto": "w1xbvA3rBk", "signatures": ["ICLR.cc/2026/Conference/Submission23922/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23922/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23922/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763408752520, "cdate": 1763408752520, "tmdate": 1763408776824, "mdate": 1763408776824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GaussGym, an open-source framework for photorealistic robot simulation that integrates 3D Gaussian Splatting (3DGS) as a drop-in renderer within the vectorized physics simulator IsaacGym. GaussGym can create diverse training worlds by ingesting data from various sources: smartphone scans, large-scale scene datasets (e.g., GrandTour), and outputs from generative video models like Veo, followed by a standard reconstruction pipeline with GSplat. The authors demonstrate the framework's utility by training visual locomotion and navigation policies for humanoid and quadrupedal robots using RL directly from RGB observations."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The stated throughput of 100,000 steps per second with $640 \\times 480$ resolution RGB/Depth rendering across 4,096 parallel environments is a state-of-the-art result for a photorealistic simulator.\n\n2. Ingesting data from various data sources into the simulation environment is a good idea.\n\n3. The open-sourced simulation framework can benefit the community.\n\n4. The paper is generally well-structured and clearly presented."}, "weaknesses": {"value": "1.  **Overclaim and Lack of Novelty.** The most significant weakness lies in the limited novelty compared to prior works [1][2]. The claim in L097 — “a first step toward closing the visual sim-to-real gap” — appears overstated, as previous research [2][3] has already demonstrated that visual RL policy training in simulation can help bridge this gap. Furthermore, the proposed “splat-integrated simulator for evaluating locomotion policies” concept is closely related to [2], which already explored real-to-sim-to-real frameworks for visual locomotion and navigation. \n\n2. **Limited Technical Contribution.** The proposed framework primarily integrates reconstructed 3DGS)scenes from diverse sources into a simulator, achieving 4,096 FPS at 640×480 resolution on a single RTX 4090 GPU. However, the high rendering speed advantage stems directly from the Gsplat renderer’s inherent parallelization, rather than from novel system design. Additionally, the overall visual policy training pipeline follows a design highly similar to [2], limiting the perceived technical innovation.\n\n3. **Lack of Comparion for Visual Locomotion**.  Although the paper presents ablation studies demonstrating the benefits of the voxel grid head and DINO encoder, it lacks direct quantitative comparisons with state-of-the-art depth- or geometry-based locomotion policies (e.g., ANymal Parkour, Miki et al.). Metrics such as success rate or velocity tracking error on common benchmark terrains (e.g., stair climbing) are missing. The main quantitative table (Table 2) focuses solely on internal ablations and a “blind” baseline, without showing performance against geometric SOTA methods. Moreover, there are no comparisons to prior visual locomotion baselines [2][3], further weakening the experimental validation.\n\n4. **Questionable Experimental Design for Visual Locomotion and Navigation.** The visual locomotion experiments, such as the stair climbing task, do not convincingly justify the need for an RGB-based policy, since depth-based parkour policies are also capable of handling similar tasks. While the authors claim that RGB-based policies capture semantic cues, the goal-tracking navigation task does not effectively demonstrate this advantage. To substantiate the argument, the paper should include higher-level semantic tasks (e.g., obstacle-type recognition or affordance-based navigation) that genuinely highlight the semantic reasoning benefits of RGB perception.\n\n**References:**\n\n[1] Xie, Ziyang, et al. \"Vid2sim: Realistic and interactive simulation from video for urban navigation.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[2] Zhu, Shaoting, et al. \"Vr-robo: A real-to-sim-to-real framework for visual robot navigation and locomotion.\" IEEE Robotics and Automation Letters (2025).\n\n[3] Yu, Alan, et al. \"Learning visual parkour from generated images.\" 8th Annual Conference on Robot Learning. 2024."}, "questions": {"value": "1. The paper adopts VGGT for camera pose calibration and coarse point cloud extraction. While VGGT offers fast inference, its pose accuracy is typically lower than SfM and BA-based approaches such as COLMAP or GLOMAP. How does this reduced accuracy affect the downstream policy learning or simulation fidelity in your framework?\n\n2. When deploying the trained visual locomotion policies on real robots, what kind of computational hardware is used? The DINO encoder appears relatively heavy for onboard inference — have the authors evaluated its runtime performance and feasibility for real-time deployment on embedded platforms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4qGFyl8jEI", "forum": "w1xbvA3rBk", "replyto": "w1xbvA3rBk", "signatures": ["ICLR.cc/2026/Conference/Submission23922/Reviewer_xv3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23922/Reviewer_xv3h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979775640, "cdate": 1761979775640, "tmdate": 1763423236969, "mdate": 1763423236969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GaussGym, a simulation platform which uses Gaussian Splatting to render realistic settings, while collisions and dynamics are simulated by IsaacGym (or other such simulators). GaussGym provides a pipeline to convert data from iPhone scans, scene datasets, or generative video models into  realistic Gaussian splats for rendering and meshes for simulation. As a result, GaussGym is able to train locomotion policies in realistic environments with easy-to-capture data, all while maintaining a high 100k FPS. The realism of GaussGym's renderer is validated via sim2real locomotion experiments, where RGB sensor information allows for tasks which require color information unavailable in standard depth+proprioception observations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- GaussGym's pipeline for generating trainable environments from easy-to-collect data (e.g. iPhone scans) is unique and provides a direction for simulating under diverse, but realistic scenes\n- Additional considerations (e.g. motion blur) improve rendering realism and training speed (e.g. updating renders at real camera fps)\n- GaussGym maintains high simulation speed across multiple scenes, necessary for legged locomotion research/tasks, which require many samples and fast training speed (for high training/testing iteration speed)\n- Sim2real experiments validate realism while demonstrating benefits of RGB observations over standard depth+proprioception (e.g. avoiding colored penalty areas)"}, "weaknesses": {"value": "- The authors note diminished performance (e.g. foot placement) when transferring to the real world, however it is unclear to what extent this is due to issues with reward tuning/dynamics randomization vs the rendering quality of the simulator\n- Currently, the environments lack some sim2real features like image latency, and the 3DGS rendering setup doesn't directly support some common simulation tools for visual domain randomization (e.g. texture and lighting randomization)"}, "questions": {"value": "- Works like [1] are able to achieve RGB sim2real while using pretrained visual encoders (and some visual domain randomization) despite low-quality rendering; in other words, the use of pretrained encoders makes it difficult to determine how much the successful sim-to-real transfer is caused by photorealistic rendering vs the pretrained visual representations. Have the authors tried training a sim2real policy off of direct RGB observations, without pretrained encoders? If so, were these policies successful?\n- How straightforwardly can can GaussGym's simulation backend be swapped out to a sim other than IsaacGym (e.g. if new simulators are released with improved performance)?\n\n\n[1] Ruihan Yang, Yejin Kim, Rose Hendrix, Aniruddha Kembhavi, Xiaolong Wang, Kiana Ehsani:\nHarmonic Mobile Manipulation. IROS 2024: 3658-3665"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t8Y0tgiazS", "forum": "w1xbvA3rBk", "replyto": "w1xbvA3rBk", "signatures": ["ICLR.cc/2026/Conference/Submission23922/Reviewer_YK8j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23922/Reviewer_YK8j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987380474, "cdate": 1761987380474, "tmdate": 1762942857618, "mdate": 1762942857618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses gaussian splatting to bring realistic visuals into fast physics simulators. The authors help with adoption of their work by providing many environments from a variety of sources. It demonstrates that this new framework is beneficial through sim-to-real experiments in stair climbing (locomotion) and goal reaching (navigation)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- focuses on the important problem of bringing real world visuals to simulators to improve sim-to-real\n- provides helpful visuals to understand the impact of the work\n- provides datasets for potential adoption"}, "weaknesses": {"value": "- limited experimental results and settings\n- existing experimental settings seem relatively simple, and may not necessarily require the high quality visuals from 3dgs in simulation \n- lack of ablations or baselines to show downstream benefits of improved simulation visuals\n- paper highlights the potential benefits of pairing GaussGym with video generation models, but does not run experiments proving this"}, "questions": {"value": "My main concerns are with the experimental results, which are mentioned in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Hp6CfmNJLl", "forum": "w1xbvA3rBk", "replyto": "w1xbvA3rBk", "signatures": ["ICLR.cc/2026/Conference/Submission23922/Reviewer_qxLt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23922/Reviewer_qxLt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090391370, "cdate": 1762090391370, "tmdate": 1762942856828, "mdate": 1762942856828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a real-to-sim approach to learning locomotion and navigation from pixels. The paper scales up real2sim visual data for training autonomous agents with a mix of data sources including iphone captures, video generative model outputs as well as existing multi-view datasets like arkit. The paper utilizes of off-the-shelf tools like VGGT and NeRFstudio to create 3D Gaussian Splats of these scenes which act as drop in renders for physics sim such as Isaac-gym. Experiments demonstrate sim2real transfer of locomtion policies. Other experiments demonstrate navigation can also be achieved with a similar method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In my opinion, following are the strengths of the paper:\n\n1. Scaling up photorealistic real2sim data for policy learning is a major advantage. While still in static scenes, it shows the utility of current zero-shot foundation models can be robustly used to scale up visual data to train policies. \n\n2. Zero-shot sim2real deployment is a nice result demonstrating the method is able to get good performance with democratized data collection such as with iphone cameras or existing captures. \n\n3. The paper is nicely written and the visuals/diagrams support and complement the text very well. \n\n4. A vectorized rendering support for existing physics sim is a great feature to have in existing physics simulators to scale up real2sim learning, albeit with an initial capturing overhead."}, "weaknesses": {"value": "In my opinion, below are the weaknesses of the method:\n\n1. Lack of comparisons to existing similar works in this space: I think the paper is lacking comparisons or discussions to existing closely related works [1,2,3]. A comparison interms of visual realism as well as PSNR as well as efficiency would be great for the community to understand which approach is the most useful in terms of ease of acquiring real2sim vs accuracy axes. \n\n2. The evaluation setting for the single-task multi-environment real2sim policy that the paper trains for their main results is unclear. Can the authors show results on a common benchmark, perhaps something similar to EmbodiedSplat [2] where it is clear what the training data distribution is and if there are any gains from training a large policy on all the real2sim collected data on unseen enviornments i.e. whether the approach is able to quickly finetune etc.?\n\n3. With a high throughput, did the authors also experiment with real-world RL?\n\n4. Are the physics parameter tuned to achieve sim2real transfer? Details of these are missing in the paper. \n\n[Minor]\n\n4. I am curious does the same results hold for manipulation as well where other factors can come into play i.e. occlusions, visual fidelity of the embodiment itself i.e. hands considering the current embodiment is a synthetic one. \n\n[1] Xie et al. Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation, CVP 2025\n[2] Yu et al. Real2Render2Real Scaling Robotic Manipulation Data Without Dynamics Simulation or Robot Hardware, CORL 2025 Oral\n[3] Chablani et al. EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device, ICCV 2025"}, "questions": {"value": "See questions and comments in the weakness section. I am looking forward to author's responses in the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hIYqSlm4FJ", "forum": "w1xbvA3rBk", "replyto": "w1xbvA3rBk", "signatures": ["ICLR.cc/2026/Conference/Submission23922/Reviewer_rmNs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23922/Reviewer_rmNs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762632258110, "cdate": 1762632258110, "tmdate": 1762942856253, "mdate": 1762942856253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}