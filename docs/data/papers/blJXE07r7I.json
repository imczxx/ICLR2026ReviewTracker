{"id": "blJXE07r7I", "number": 8067, "cdate": 1758057498658, "mdate": 1759897810680, "content": {"title": "EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning", "abstract": "Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.", "tldr": "", "keywords": ["Video Editing", "Content Generation", "Artificial Intelligence"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9d07ae688b08ed11b4fdeadaed40f4235fb5dbd.pdf", "supplementary_material": "/attachment/54f1a6cabd37b58af95d8fc7a1065d3ac273ac39.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents EditVerse, a unified framework for image and video generation and editing. By representing images, videos, and text as one-dimensional sequences of interleaved tokens, EditVerse demonstrates promising capabilities in in-context learning and cross-modal knowledge transfer. EditVerse achieves state-of-the-art results on both the EditVerseBench and TGVE+ benchmarks. Furthermore, it exhibits impressive emergent abilities when trained at scale with unified multi-modal data. Although interactive editing and generation scenarios were not evaluated in this work, I believe this approach, with appropriate scaling, could demonstrate significant potential and enable substantially broader applications. In summary, this is a technically solid and well-presented paper."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **Unified Design**.\nThe paper presents a clean solution : treat text, images, and videos as interleaved token sequences with a 4D positional embedding (sequential, temporal, height, width). This simple yet effective design handles arbitrary resolutions and durations naturally, which is quite elegant.\n2. **Knowledge Transfer between Different Modality**.\nThe key insight is using abundant image editing data to help with scarce video editing data. Through full self-attention and mixed training (6M image editing + 288K video editing samples), the model learns to transfer knowledge across modalities. \n3. **Solid Experimental Results**.\nThe paper presents comprehensive experiments across image/video generation and editing tasks, demonstrating that EditVerse achieves state-of-the-art performance with only 2B parameters. The ablations also clearly show both image and video data matter—images help with instruction understanding, videos help with temporal consistency. What's interesting is the emergent behavior: the model can handle tasks it wasn't explicitly trained on, and sometimes even beats the ground truth by combining knowledge from different domains.\n4. **Strong Results  with Potential**.\nWith just 2B parameters, EditVerse achieves SOTA on EditVerseBench and TGVE+, matching or beating much larger models and commercial systems. The results hold up in both automatic metrics and user studies across 20 different editing tasks. I believe this approach has significant room to grow—with more data and parameters, it could unlock even broader applications."}, "weaknesses": {"value": "1. **Background Drift in Local Edits**.\nI noticed some background preservation issues in local editing tasks—there seems to be temporal drift in unedited regions. Could you elaborate on when and why this happens? Understanding these failure cases would be helpful.\n\n2. **Why Text-Only Localization Work So Well?**\nMost video editing models need masks for precise object editing, but EditVerse seems to work with just text instructions. This is quite impressive but also surprising. What makes this possible? Is it the scale of training, the unified architecture, or the cross-modal learning? A deeper analysis here would really strengthen the paper's contribution."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BYBQn8z56U", "forum": "blJXE07r7I", "replyto": "blJXE07r7I", "signatures": ["ICLR.cc/2026/Conference/Submission8067/Reviewer_72vX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8067/Reviewer_72vX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761169353174, "cdate": 1761169353174, "tmdate": 1762920057020, "mdate": 1762920057020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EditVerse, a novel unified framework designed to perform both image and video generation and editing within a single model. The authors identify two primary bottlenecks in video editing: (1) architectural limitations of existing models, which are often task-specific, and (2) the scarcity of high-quality, instruction-based video editing data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The core design of an interleaved 1D token sequence for mixed image/video data, combined with a full self-attention mechanism and the novel 4D RoPE, is a powerful and original solution for multimodal in-context learning.\n2. The paper's standout strength is its clear demonstration that a unified model can learn video editing from data-abundant *image* editing datasets. The ablation in Table 4 (showing the model *can* edit video with zero video-edit data, albeit poorly) and the major quality drop from removing image-edit data (Fig 8) provide powerful evidence for this hypothesis.\n3. The authors address the ecosystem problem by not only building a model but also creating the data (232K video-edit samples) and the evaluation tools (EditVerseBench) necessary to make progress, and they are releasing the benchmark."}, "weaknesses": {"value": "1. The most significant weakness, which the authors acknowledge in the appendix, is the computational cost. Using full self-attention on a 1D-tokenized video (where the sequence length $L$ includes all frames) leads to $O(L^2)$ complexity. The reported 118 seconds for a single 360p video on an A100 is very slow and will likely scale quadratically or worse with resolution and duration, making it impractical for real-world use on high-resolution or long-form videos.\n2. The video editing data is synthetically generated by a pipeline of specialist \"teacher\" models (VACE, DiffuEraser, etc.). While this is a clever solution, it means the model's performance may be capped by the quality and biases of these teachers. It's unclear if the model can perform edits that its teacher models are incapable of."}, "questions": {"value": "1. Could the authors provide more details on the scaling properties of EditVerse? How do inference time and VRAM usage scale with (a) video duration (e.g., 3s vs. 10s vs. 30s) and (b) resolution (360p vs. 720p)? Is the full self-attention approach feasible beyond the short, low-resolution clips shown?\n2. The dimension allocations for the 4D RoPE (56H, 56W, 12Seq, 4Temp) are specific. Could the authors provide a brief rationale or ablation for this design choice? For example, why is the sequential dimension (12) given more embedding space than the temporal dimension (4)?\n3. How does the model perform on editing tasks or concepts that are *not* present in its synthetic \"teacher\" models? For instance, if VACE is poor at \"making an object transparent,\" can EditVerse still learn this concept purely from the image-editing data and apply it successfully to video, or is its video performance on this task limited by VACE?\n4. The \"wrong position\" failure case in Fig 9a suggests limitations in complex spatial reasoning. The VLM evaluation (Table 2) is high, but this metric averages 3 frames. How well does this VLM metric capture these kinds of high-level logical or instruction-misalignment failures, as opposed to per-frame artifacts or quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rvcL0IKCyS", "forum": "blJXE07r7I", "replyto": "blJXE07r7I", "signatures": ["ICLR.cc/2026/Conference/Submission8067/Reviewer_3VqN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8067/Reviewer_3VqN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666220812, "cdate": 1761666220812, "tmdate": 1762920056543, "mdate": 1762920056543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents EditVerse, a unified video editing model that aims to handle diverse edit types (style change, object manipulation, temporal edits, localized edits, etc.) with a single model. The core modeling idea is to serialize visual content (both images and multi-frame videos) into an interleaved token sequence, and to use a multi-dimensional rotary positional embedding (RoPE) capturing spatial, temporal, and sequential position.\n\nOn the evaluation side, the paper introduces EditVerseBench, which focuses on real-world, non-square aspect ratios (horizontal and vertical videos) across multiple editing categories, arguing that prior benchmarks mostly assume square crops and narrow edit types. They also describe a data generation and VLM-based filtering pipeline to curate higher-quality training dataset. Quantitative comparisons are reported using automatic VLM metrics, frame-level visual quality and temporal consistency metrics, plus a human pairwise preference study.\n\nOverall, the paper addresses a practically relevant problem and proposes a promising integration of a unified model design, a new benchmark, and a data curation pipeline. Several points require clarification, in particular the transparency of the evaluation setup, the reporting of the human study, and ablations on key components, but these issues appear to be fixable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a practically relevant problem by aiming to unify diverse instruction-based video editing tasks within a single model. The proposed interleaved visual token representation together with the multi-dimensional RoPE design is technically interesting and potentially reusable beyond this specific application. The introduction of a benchmark with realistic horizontal and vertical video formats and multiple editing categories is valuable for evaluating real-world usage, and the experimental comparisons include both strong instruction-based methods and training-free baselines, which provides a relatively comprehensive view of performance."}, "weaknesses": {"value": "The human evaluation lacks essential reporting details such as annotator composition, annotation protocol, and reliability, making it difficult to assess the credibility of the results. The design choices of the proposed benchmark may be perceived as favoring the method, and the paper does not sufficiently analyze scenarios in which simpler, training-free baselines still outperform the model. In addition, the contribution of the positional encoding components is not well-isolated through ablations, and the automatic evaluation pipeline, including data filtering and VLM-based scoring prompts, is not fully transparent, which limits the reproducibility of the results."}, "questions": {"value": "1. **Human evaluation protocol**\n\n   More details are needed on the human preference study: how many annotators were involved, their backgrounds, how annotation was distributed per person, and whether any quality control or inter-annotator agreement was assessed. Such information would increase the credibility of the human study results.\n\n2. **Clarification on the Qwen2-VL–based data filtering process**\n   The description of the VLM filtering process remains vague. Could you specify the exact score threshold used to retain samples, provide an overview of score distributions before and after filtering (ideally across data sources), and comment on whether Qwen2-VL frequently misjudged quality (e.g., bad edits retained or good edits discarded), even approximately? Greater transparency here would strengthen the argument for the data pipeline.\n\n3. **Human rating in the main results table**\n   For the core quantitative comparison (e.g., Table 2), it would be valuable to include a human rating or user preference column alongside the automatic metrics, similar to prior works such as TokenFlow, Tune-A-Video, and FateZero. This would provide a more direct comparison from a human perspective and make the main results more informative, rather than relying solely on automatic metrics.\n\n4. **Benchmark construction and possible bias**\n\n   Since EditVerseBench excludes square videos entirely, it would be helpful to clarify the rationale for not including them alongside horizontal and vertical formats. This concern is reinforced by the fact that in Table 6, EditVerse does not always outperform training-free baselines on V2VBench, where some simpler methods achieve competitive or better results. It would be useful to discuss in which scenarios such lightweight baselines remain preferable in practice, and how the proposed benchmark design avoids unintentionally favoring EditVerse.\n\n5. **RoPE ablations**\n   As the multi-dimensional RoPE is positioned as a key architectural contribution, more direct evidence isolating its effect would be useful. Even a small-scale study comparing variants that remove temporal RoPE, remove spatial RoPE, or use only standard sequential RoPE would help substantiate the importance of each component.\n\n6. **Data filtering transparency**\n    The data pipeline uses a VLM to assign 0–10 quality scores (instruction adherence, temporal consistency, artifact severity, etc.) and then thresholds these scores to filter large editing datasets.\n\n   - Please show the score distribution before vs. after filtering.\n   - How was the threshold chosen, and how sensitive is final model performance to that threshold?\n   - Do you have an estimate of “false positives” (bad edits that passed) and “false negatives” (good edits that were filtered out)?\n      Since the dataset is part of the claimed contribution, a brief audit would strengthen the narrative.\n\n7. **Reproducibility of automatic evaluation**\n   To facilitate reproducibility, would you consider releasing the exact prompt templates and inference configurations used for the VLM-based automatic evaluation? Without these details, the community may find it difficult to replicate the automatic scores.\n\n8. **Minor fixes for clarity**\n\n   - The training loss formula should be corrected to:\n     $$\n     \\mathcal{L} = \\mathbb{E}_{t,X_0,X_1} \\left\\| u_\\Theta(X_t, t) - (X_1 - X_0) \\right\\|^2.\n     $$\n\n   - The naming and ordering of the RoPE components in the text (around lines 200–221) do not match Figure 2. The text first describes “Height and Width Dimensions” before sequential and temporal, while Figure 2 uses the order spatial → temporal → sequential. Please unify terminology and ordering to avoid confusion.\n\n   - In Table 9, the symbol used to mark the method is “†”, but the note below the table refers to “‡ uses LLM-rewritten prompts.” The marker in the table and the footnote symbol should be consistent.\n\n   - The citation for “TokenFlow” is incorrect. The paper currently cites *“Tokenflow: Unified Image Tokenizer for Multimodal Understanding and Generation”*, which is unrelated to video editing. It should cite **“TokenFlow: Consistent Diffusion Features for Consistent Video Editing”**. The incorrect reference appears in line 348–349, Table 3, and line 818."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7iXwVbCPzE", "forum": "blJXE07r7I", "replyto": "blJXE07r7I", "signatures": ["ICLR.cc/2026/Conference/Submission8067/Reviewer_hzXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8067/Reviewer_hzXL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749646999, "cdate": 1761749646999, "tmdate": 1762920056048, "mdate": 1762920056048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EditVerse, a unified framework for image and video generation and editing. The core idea is to represent text, image, and video as a single interleaved token sequence, processed by a full self-attention transformer. A 4D ROPE position embedding is proposed to encode spatial, temporal, and sequence positions. To address video editing data scarcity, the authors build a synthetic video editing pipeline to collect 0.2 million video editing data for model training. Experiments shows that their model can suprpass existing open-source and commercial models across various tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, and the proposed framework is clearly presented and easy to understand.\n\n2. The visualized qualitative results are impressive and effectively demonstrate the model’s capabilities."}, "weaknesses": {"value": "1. The main weakness of this work lies in the lack of substantial technical novelty. The proposed pipeline appears highly similar to recent multimodal generative frameworks (e.g., Bagel, OmniGen). The novelty mainly resides in engineering integration rather than in proposing a fundamentally new modeling formulation.\n\n2. In my opinion, the key contribution of the paper is to show that a unified video editing/generation framework can be implemented. However, the authors do not commit to releasing the engineering details, dataset collection pipeline, or the collected dataset itself. Without open sourcing or releasing enough details for reproduction, their engineering contribution becomes less impactful.\n\n3. The justification for the necessity of the proposed interleaved representation remains insufficient. Although the authors include some ablations, it is still unclear why interleaving tokens is fundamentally better than other existing designs (e.g., cross-attention conditioning, multi-encoder structures)."}, "questions": {"value": "Refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uccGndgTRX", "forum": "blJXE07r7I", "replyto": "blJXE07r7I", "signatures": ["ICLR.cc/2026/Conference/Submission8067/Reviewer_ijpS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8067/Reviewer_ijpS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826557777, "cdate": 1761826557777, "tmdate": 1762920055489, "mdate": 1762920055489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EditVerse, a unified multimodal framework for text-guided image and video generation and editing. The model represents text, image, and video modalities as a single interleaved token sequence and applies self-attention for in-context learning and cross-modal reasoning. To address the lack of video editing data, the authors design a large-scale automated data generation pipeline. Extensive experiments demonstrate that EditVerse achieves strong performance and exhibits emergent editing capabilities that generalize beyond the training set."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Comprehensive experiments with solid quantitative and qualitative evaluations.\n* Clear presentation and well-structured writing.\n* The unified token representation for text, image, and video is conceptually elegant.\n* Interesting empirical insight: the model demonstrates emergent abilities on unseen editing tasks, suggesting strong cross-modal generalization."}, "weaknesses": {"value": "* While the unified sequence formulation is elegant, it primarily extends existing transformer-based multimodal modeling paradigms. The contribution lies more in engineering integration and data scaling than in introducing fundamentally new learning principles.\n* The model requires ~30 GB GPU memory and ~118 seconds to edit a single 360p video on an A100 80 GB GPU. This raises serious scalability concerns for long-duration or high-resolution videos, as well as practical deployment constraints.\n* The approach concatenates all modality tokens into one long sequence, resulting in $O(L^2)$ attention cost. It is unclear how EditVerse maintains efficiency with long inputs, multiple video clips, or multi-turn text–video interleaving. No efficiency analysis (FLOPs, latency, or memory growth curves) is provided.\n* The use of single-layer linear projectors for modality alignment may be oversimplified. There is no ablation on alternative projector depths or modality-specific encoders to validate whether this minimal design is sufficient for cross-modal alignment.\n* The video editing dataset is entirely generated via an automated pipeline using pretrained models (Grounded-SAM-2, VACE, ReCamMaster, etc.). This synthetic data may not reflect the diversity and imperfections of real-world edits, leading to overfitting on artificial editing patterns.\n* Each data generation stage depends on prior model outputs, so artifacts such as inaccurate masks, poor inpainting, or unrealistic motion can cascade. The paper provides no quantitative analysis or quality control to assess data noise accumulation. \n* The paper mentions using a VLM to assign quality scores for filtering generated data, but it is unclear whether the VLM was adapted for the data filtering, how accurate its scores are, or how thresholds were chosen."}, "questions": {"value": "* How is the VLM used for data scoring and filtering? Was it used off-the-shelf or fine-tuned for editing relevance? What is its reliability or correlation with human judgment?\n* How does model performance and runtime scale with sequence length (e.g., multi-video input or long textual instructions)? Can efficiency be improved beyond full-sequence attention?\n* Have you compared the single-layer modality projectors against deeper or non-linear alternatives?\n* The reported improvements are modest; how sensitive are the evaluation metrics, and do the differences translate into perceptual gains?\n* Given that the data pipeline heavily depends on pretrained vision models, how do you ensure the resulting dataset and EditVerse itself do not inherit or amplify their biases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NSkeiBjsa5", "forum": "blJXE07r7I", "replyto": "blJXE07r7I", "signatures": ["ICLR.cc/2026/Conference/Submission8067/Reviewer_U1BY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8067/Reviewer_U1BY"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147304281, "cdate": 1762147304281, "tmdate": 1762920054798, "mdate": 1762920054798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}