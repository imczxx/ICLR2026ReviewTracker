{"id": "9aI6XUDYLX", "number": 14464, "cdate": 1758236468342, "mdate": 1763584236483, "content": {"title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models—Architecture, Training and Dataset", "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models—first training on image understanding and subsequently on image generation—offers practical advantages by preserving image-understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.", "tldr": "", "keywords": ["Unified Multimodal"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f4f1fc7229ab3e55089e1902444bd87f961a3d20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BLIP3-o, a family of unified multimodal models designed for both visual understanding and image generation. The proposed models represent images in the CLIP embedding space and generate images using a diffusion transformer trained with flow matching. The paper further investigates three key design dimensions: (1) CLIP vs. VAE latent representations, (2) flow matching vs. MSE regression, and (3) sequential vs. joint training for integrating understanding and generation. The experimental explorations demonstrate that the combination of CLIP embeddings, flow matching and sequential training results in a stable and high-performing unified framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper systematically evaluates important architectural and training choices for unified vision-language models, providing practical observations. The results show that sequential training prevents interference between understanding and generation tasks. The combination of CLIP embeddings and flow matching leads to a stable generation pipeline with strong text-image alignment. The release of datasets and models benefits research in unified multimodal modeling."}, "weaknesses": {"value": "- The proposed pipeline closely resembles prior models and shows somewhat limited novelty in terms of technical architecture. The paper does not introduce a fundamentally new modeling paradigm, with the contributions lying primarily in empirical validation rather than conceptual innovation.\n\n- While the paper highlights three design choices (CLIP embeddings, flow matching, and sequential training), these directions are familiar and intuitive in recent multimodal literature. The explorations in this paper primarily confirm existing intuitions rather than offer fundamentally new insights.\n\n- The 8B model uses 30M well-curated private images, yet the experiments do not include thorough ablations on the training data. This makes it difficult to disentangle the performance improvements attributable to the applied approach from those arising from differences in data scale and quality.\n\n- While human preference studies are central to the claims of improved visual quality, the work lacks transparency about human studies, including rater sourcing, annotation protocols, agreement metrics, and quality control procedures.\n\nOverall, this work explores design choices for unified multimodal models and serves as a solid technical report from an engineering perspective. In particular, the open-sourcing of models and datasets is valuable and has a positive impact on the community. However, the paper lacks exploration of new frameworks or techniques, and most of the investigated conclusions about design choices are already known or intuitive. To meet the standards of top-tier conferences, the work is expected to present more technical innovations and deliver deeper scientific insights to advance the understanding of unified multimodal modeling."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lka2j695Kk", "forum": "9aI6XUDYLX", "replyto": "9aI6XUDYLX", "signatures": ["ICLR.cc/2026/Conference/Submission14464/Reviewer_u3fo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14464/Reviewer_u3fo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373062716, "cdate": 1761373062716, "tmdate": 1762924866901, "mdate": 1762924866901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "MFtI9Tz7Rz", "forum": "9aI6XUDYLX", "replyto": "9aI6XUDYLX", "signatures": ["ICLR.cc/2026/Conference/Submission14464/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14464/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763584235559, "cdate": 1763584235559, "tmdate": 1763584235559, "mdate": 1763584235559, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper primarily discusses and experiments with unified multimodal large models from the perspectives of architecture, loss design, and training strategies. Under an autoregressive + diffusion framework, the paper finds that, in terms of generation architecture and loss formulation, the CLIP + FlowHead approach yields the best performance. Regarding training strategy, training the understanding component first followed by the generation component achieves optimal model performance. The paper ultimately demonstrates competitive results on both understanding and generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's presentation is excellent, clearly illustrating the method's architecture and the differences in the comparative experiments.\n2. BLIP3o demonstrates competitive performance, showing strong results on both understanding and generation.\n3. The paper proposes a method that effectively integrates open-source understanding and generation models to achieve a unified large model."}, "weaknesses": {"value": "1. Paper does not provide a more in-depth discussion comparing its approach with EMU1 and EMU2. In my opinion, BLIP3O's architecture is very similar to that of EMU1/2, and its performance gains appear to stem primarily from the stronger base model.\n2. GPT-4o not only excels at understanding and generation but also performs very well on image editing tasks. The paper mentions in Section 2 that the discussion and experiments are conducted under the assumption of a GPT-4o-based architecture. However, if focus solely on generation and understanding while ignoring editing, this might affect the conclusions of the paper. For example, for the choice of generation features, can using only high-level semantic features like those from CLIP still achieve competitive performance on editing tasks?\n3. Insufficient experiments. Regarding the choice of generative features, the paper does not conduct experiments or comparisons on a CLIP + VAE + Flow Matching architecture. Such a design would preserve both high-level semantic information and low-level pixel information from the input. How would this affect the model’s understanding and generation capabilities? And could this also allow the model to better handle editing tasks?"}, "questions": {"value": "Please refer to the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tWt5ouDcpC", "forum": "9aI6XUDYLX", "replyto": "9aI6XUDYLX", "signatures": ["ICLR.cc/2026/Conference/Submission14464/Reviewer_xR6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14464/Reviewer_xR6L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796095919, "cdate": 1761796095919, "tmdate": 1762924866444, "mdate": 1762924866444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper systematically explores the design choices of training unified multimodal models, i.e. image representation, training objectives, and training strategies. There are some interesting experimental conclusions. (1)  CLIP features (with 64-dimensional compact semantic representation) outperform VAE latent features in both training efficiency and generation quality. (2)  Flow Matching loss (which models distribution transport via diffusion Transformers) addresses the lack of diversity in MSE loss. (3) sequential training (first pre-training the image understanding module, then freezing the backbone to train the diffusion generation module) outperforms joint training."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Systematic evaluation: The systematic validation of component combinations (e.g., why CLIP pairs better with Flow Matching than VAE) adds theoretical depth.​\n\n2. High-quality experiments: The evaluation covers both understanding (9 benchmarks) and generation (automatic + human) tasks, with statistical significance tests (e.g., p-values in human evaluation) that strengthen result credibility.\n\n3. Significance for practice: The open-source release promotes reproducibility and follow-up research."}, "weaknesses": {"value": "1. The paper does not invent new components. The technical novelty is somewhat limited.\n\n2. Lack of detailed ablations about w/ or w/o BLIP3o-60k training.\n\n3. Insufficient comparison to recent recent related works about unified LMMs, such as Harmon [a] and Bagel [b].\n\n\n[a] Wu S, Zhang W, Xu L, et al. Harmonizing visual representations for unified multimodal understanding and generation[C]. ICCV 2025. \n[b] Deng C, Zhu D, Li K, et al. Emerging properties in unified multimodal pretraining[J]. arXiv preprint arXiv:2505.14683, 2025."}, "questions": {"value": "Limited analysis of failure cases: For example, does BLIP3-O struggle with abstract concepts (e.g., \"a painting of happiness\") or low-data objects (e.g., rare animals)? Understanding limitations would strengthen the paper’s contribution.​"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3YvEGM3SPc", "forum": "9aI6XUDYLX", "replyto": "9aI6XUDYLX", "signatures": ["ICLR.cc/2026/Conference/Submission14464/Reviewer_324M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14464/Reviewer_324M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839077920, "cdate": 1761839077920, "tmdate": 1762924865972, "mdate": 1762924865972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically explores the architecture design space for unified multimodal models. Specifically, this work comprehensively studies the image representations and regression target. Based on the pilot studies, this paper adopts \"CLIP + Flow Matching + two Diffusion Decoders\" as the connected unified multimodal model. Extensive experimental results demonstrate that the proposed framework is able to reach state-of-the-art performance on image generation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiments are thorough and well-designed, covering a wide range of scenarios.\n\n2. The model achieves competitive performance across standard image generation benchmarks.\n\n3. The anticipated open-source release is valuable and likely to benefit the broader research and practitioner community."}, "weaknesses": {"value": "1. A figure illustrating the overall model architecture, including the diffusion decoder that generates image pixels, is required. Otherwise, it is confusing when viewing Figure 1 (“The architecture of BLIP3-O”), which only presents the encoder without showing the decoder architecture for image generation.\n\n2. This work overlooks the entanglement between the proposed architecture, the GPT-4o synthetic data, and the pre-trained Lumina-Next model. Given that synthetic data can significantly boost performance on benchmarks such as GenEval, it is unclear whether the improvements stem from the architecture itself, the synthetic data, or the strong pre-trained backbone. Moreover, without controlled experiments on pre-trained Lumina-Next, it remains possible that Lumina-Next trained with similar data could achieve comparable results.\n\n3. This raises the question of whether such unification is necessary. To address this concern, the paper should demonstrate capabilities that bespoke generative models cannot achieve. Therefore, the current benchmarks are not fully convincing in motivating the unification.\n\n4. What about the potential of this stage-wise training paradigm versus the end-to-end one, and what are the weaknesses?"}, "questions": {"value": "See weaknesses. I would adjust my rating according to the authors' response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nwSbQDQZgK", "forum": "9aI6XUDYLX", "replyto": "9aI6XUDYLX", "signatures": ["ICLR.cc/2026/Conference/Submission14464/Reviewer_n1Rg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14464/Reviewer_n1Rg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900318195, "cdate": 1761900318195, "tmdate": 1762924865468, "mdate": 1762924865468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}