{"id": "gvUufgeJvV", "number": 10216, "cdate": 1758164225874, "mdate": 1759897665956, "content": {"title": "Are LLMs Really Not Knowledgeable? Mining the Submerged Knowledge in LLMs' Memory", "abstract": "Large language models (LLMs) have shown promise as parametric knowledge bases, but often underperform on question answering (QA) tasks due to hallucinations and uncertainty. While prior work attributes these failures to knowledge gaps in the model’s parameters, we uncover a complementary phenomenon: LLMs frequently retain correct knowledge even when generating incorrect or \\``unsure'' answers.\nBy analyzing the token-level output distributions, we find that correct answers often appear among high-probability candidates, despite not being selected. Motivated by this, we propose Hits@k, a novel metric to evaluate latent knowledge retention independent of answer surface form. Our experiments reveal that LLMs possess significantly more factual knowledge than is reflected by standard QA accuracy.\nBuilding on these insights, we further examine the prevailing few-shot QA paradigm. We find that prompting strategies which allow ``unsure'' outputs can inadvertently suppress correct answers by discouraging low-confidence generation. We design a set of quantitative experiments to measure this suppression effect, offering practical guidance for future prompt and decoding design in knowledge-intensive tasks.", "tldr": "", "keywords": ["Knowledge-based QA", "Memory of LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5af7b07399043540e22a3dae9ff84c744e677ab9.pdf", "supplementary_material": "/attachment/4a7a561e9c4b1061f2052bec895f4a6f11eed23e.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates whether large language models truly “do not know” an answer when they produce an incorrect response. The authors argue that model knowledge and model expression are not equivalent, and they conduct exploratory experiments such as Hits@k and “unsure” filtering to show that correct answers often appear in the top-k token distribution, even when surface accuracy is low. The work suggests that LLMs may possess latent knowledge that is not successfully expressed during decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation. The paper highlights an intuitively important gap between latent knowledge and surface-level generation in LLMs.\n\n- Empirical observations are easy to interpret. Hits@k and “unsure” filtering provide simple and intuitive diagnostic signals.\n\n- Readable paper structure. The writing is clear, and the experiments are straightforward to follow."}, "weaknesses": {"value": "- The insight is not novel, as similar conclusions have long existed in perplexity-based evaluations, which already reflect that LLMs may assign high probability to correct tokens that are not selected in top-1 decoding.\n\n- The phenomenon is also well-known from rollout-based methods (e.g., multi-sampling, self-consistency, and RL trajectories), which routinely reveal correct answers in non-greedy decoding paths.\n\n- The paper lacks deeper analysis or actionable contribution, offering no explanation of why the mismatch occurs, nor methods for leveraging latent knowledge to improve actual model performance."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZKtgkOqp3X", "forum": "gvUufgeJvV", "replyto": "gvUufgeJvV", "signatures": ["ICLR.cc/2026/Conference/Submission10216/Reviewer_Jtaf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10216/Reviewer_Jtaf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760965662756, "cdate": 1760965662756, "tmdate": 1762921575454, "mdate": 1762921575454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that LLMs can retain correct knowledge even when generating incorrect answers; correct answers frequently appear among high-probability tokens despite not being selected as final outputs. Based on this observation, the paper introduces Hits@k, a new metric to assess the knowledge of LLMs. Also, it introduces a new decoding method to improve answer accuracy by leveraging detected but unexpressed knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper catches an interesting finding that LLMs often maintain access to accurate information within their probability distributions over vocabulary tokens, and there is a systematic gap between knowledge storage and expression rather than simple knowledge absence.\n\n- It offers new insights into knowledge augmentation: instead of expanding knowledge, augmenting the ability to express existing knowledge is important and can be potentially very useful."}, "weaknesses": {"value": "- Though it is an interesting finding, I still believe that LLMs are not knowledgeable even though they assign significant probability scores to tokens representing the correct information, since in real-world use cases, it is impractical to let LLMs generate multiple responses to each query. Therefore, I don't think Hits@k should be used for evaluation/rank models.\n\n- The proposed decoding algorithm can raise many safety or ethical concerns if deployed into general use cases, since in many real-world scenarios, it might be unsafe or unethical to generate an “informative” response.\n\n- The proposed decoding algorithm increases the probability of correct answers, but also increases the probability of wrong answers."}, "questions": {"value": "Cite and introduce DBPedia, IMDB, and GoodReads with more details (maybe in appendix)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F5NbbuWImu", "forum": "gvUufgeJvV", "replyto": "gvUufgeJvV", "signatures": ["ICLR.cc/2026/Conference/Submission10216/Reviewer_JvA3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10216/Reviewer_JvA3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761259079830, "cdate": 1761259079830, "tmdate": 1762921574801, "mdate": 1762921574801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how large language models (LLMs) store and express factual knowledge. It argues that incorrect or \"unsure\" answers do not necessarily indicate missing knowledge, since correct answers often appear among high-probability tokens that are not selected. To quantify this hidden knowledge, the authors introduce Hits@k, which measures how frequently the correct answer appears within the top-k tokens of the model's output distribution. Extensive experiments across open-domain and domain-specific datasets show that models retain substantially more factual information than is revealed by accuracy alone, and that newer models exhibit higher latent retention. The study also finds that \"unsure\" prompts can suppress correct answers by lowering generation confidence, and that filtering such responses can recover many correct predictions. Together, these findings reveal a gap between knowledge storage and expression, offering insights for improving prompt design and decoding strategies in knowledge-intensive tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a clear and intuitive metric that captures latent knowledge beyond standard accuracy, offering a new perspective on model evaluation.\n\n- The analysis reveals that models often “know” more than they express, which challenges common assumptions about what low-confidence or incorrect outputs imply.\n\n- The experiments are extensive and well controlled, which show consistent trends across multiple model scales and factual datasets.\n\n- The study provides actionable insights for prompt design and decoding strategies by showing how uncertainty affects knowledge expression.\n\n- The paper is clearly written and conceptually accessible, making its findings easy to reproduce and useful for both research and applied settings."}, "weaknesses": {"value": "- The paper does not provide a formal justification for why Hits@k should reflect internal knowledge rather than distributional coincidence, relying mainly on empirical correlations (Figure 3).\n\n- The improvement margins between Hits@k and standard accuracy are sometimes modest -- for example, less than 5% in several datasets (Table 2) -- which weakens the claim of large hidden knowledge reserves.\n\n- The evaluation focuses narrowly on factual recall and omits reasoning or multi-hop questions, so it is unclear whether the proposed metric captures deeper forms of knowledge use beyond surface recall (Section 5.2).\n\n- The proposed method measures the presence of correct tokens but ignores how easily the model can retrieve or reason about them, which conflates memorization with accessibility (Section 4.3).\n\n- The study does not examine sensitivity to decoding parameters such as temperature or top-p, leaving unclear whether the observed patterns remain stable under different generation settings."}, "questions": {"value": "How does Hits@k distinguish between genuinely stored knowledge and coincidental token co-occurrence, and what evidence supports that the correct token's presence in the top-k reflects meaningful internal representation rather than surface-level probability alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QmGVWUHsyE", "forum": "gvUufgeJvV", "replyto": "gvUufgeJvV", "signatures": ["ICLR.cc/2026/Conference/Submission10216/Reviewer_h4kU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10216/Reviewer_h4kU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980296450, "cdate": 1761980296450, "tmdate": 1762921574167, "mdate": 1762921574167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}