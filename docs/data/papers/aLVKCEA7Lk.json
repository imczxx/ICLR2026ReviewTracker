{"id": "aLVKCEA7Lk", "number": 14917, "cdate": 1758245492206, "mdate": 1759897341505, "content": {"title": "Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement", "abstract": "Causal representation learning (CRL) has garnered increasing interests from the causal inference and artificial intelligence community, due to its capability of disentangling potentially complex data-generating mechanism into causally interpretable latent features, by leveraging the heterogeneity of modern datasets. In this paper, we further contribute to the CRL literature, by focusing on the stylized linear structural causal model over the latent features and assuming a linear mixing function that maps latent features to the observed data or measurements. Existing linear CRL methods often rely on stringent assumptions, such as accessibility to single-node interventional data or restrictive distributional constraints on latent features and exogenous measurement noise. However, these prerequisites can be challenging to satisfy in certain scenarios. In this work, we propose a novel linear CRL algorithm that, unlike most existing linear CRL methods, operates under weaker assumptions about environment heterogeneity and data-generating distributions while still recovering latent causal features up to an equivalence class. We further validate our new algorithm via synthetic experiments and an interpretability analysis of large language models (LLMs), demonstrating both its superiority over competing methods in finite samples and its potential in integrating causality into AI. Source code is available at [the anonymous link](https://anonymous.4open.science/r/creator-883D/).", "tldr": "The paper proposes a new linear causal representation learning algorithm, under weaker assumptions compared to the current literature.", "keywords": ["causal representation learning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/729960b6c8809776e02e93b23b763ac1f553ec77.pdf", "supplementary_material": "/attachment/0b5e29a18a3941096172eac2d1eed2f078695cbb.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents CREATOR, a novel algorithm for linear causal representation learning (CRL) from heterogeneous environments. The work focuses on a setting with a linear structural causal model over the latent variables and a linear mixing function mapping these latents to observations. The proposed CREATOR algorithm operates in three stages: inferring a topological ordering of the latent variables, pruning the resulting dense graph to identify the sparse causal DAG, and finally disentangling the latent features to the underlying causal DAG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this work lies in its successful relaxation of several restrictive assumptions made in the recent, closely related work of Jin & Syrgkanis (2024). Specifically, the proposed method does not require the distribution of exogenous noise to be identical across different environments, nor does it assume that different noise components within the same environment must have different distributions."}, "weaknesses": {"value": "1. The manuscript's strong assumption of a linear model (linear SCM and mixing function) limits its applicability to complex, real-world scenarios which are often nonlinear.\n\n2. The problem setup assumes all environments share the same observed variables, whereas in reality, environments may have only partially overlapping sets of observed variables. Considering this case would better reflect real-world conditions.\n\n3. The manuscript is technically dense and lacks intuitive examples or explanations for its core mechanisms, making it difficult to follow.\n\n4.  Similar to Jin & Syrgkanis (2024), a discussion comparing this work with the following papers is necessary:\n    * A versatile causal discovery framework to allow causally-related hidden variables, ICLR, 2023.\n    * Generalized independent noise condition for estimating causal structure with latent variables, JMLR, 2024.\n\n5.  The manuscript claims that (line 345) it “yields a more efficient pruning procedure” than the method in Jin & Syrgkanis (2024). To substantiate this claim, an explicit efficiency comparison is needed, which would further enhance the contribution of the work."}, "questions": {"value": "1. Lines 89–90 state that the algorithm can ``provably identify latent features and their causal mechanisms up to an equivalence class,'' while line 193 claims that ``the latent features and causal DAG can be uniquely recovered.'' This appears somewhat contradictory, as an equivalence class is not a unique DAG. Clarification would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O7DacVxDLP", "forum": "aLVKCEA7Lk", "replyto": "aLVKCEA7Lk", "signatures": ["ICLR.cc/2026/Conference/Submission14917/Reviewer_ba3u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14917/Reviewer_ba3u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720238677, "cdate": 1761720238677, "tmdate": 1762925260841, "mdate": 1762925260841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CREATOR, a new algorithm for linear causal representation learning (CRL) designed to relax some of the assumptions made in LiNGCReL (Jin & Syrgkanis, 2024).\nThe model assumes a linear structural causal model over latent variables and a linear mixing to observations. The method consists of three stages: (i) ordering and feature recovery using independence criteria, (ii) DAG pruning based on rank analysis across multiple environments, and (iii) feature disentanglement to recover latent variables up to a structural equivalence class.\nTheoretical claims are supported by identifiability proofs relying on non-Gaussianity and independence arguments, and empirical validation is provided on small synthetic datasets and a qualitative case study on large language models (LLMs)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well structured and formally presented. The proposed model formulation is clear and consistent with recent linear CRL frameworks such as those by Squires et al. (2023) and Jin & Syrgkanis (2024).\n* The attempt to connect linear CRL with representation analysis in large language models is conceptually appealing and relevant to ongoing discussions on the linear representation hypothesis (Arora et al., 2016; Park et al., 2024).\n* The relaxation of environment-specific assumptions, allowing heterogeneous noise distributions, addresses a practical limitation of earlier models and enhances the potential applicability of the method."}, "weaknesses": {"value": "* **Assumption relaxation and identifiability.**\n  The claim of weaker assumptions is debatable. Although CREATOR removes the requirement of identical noise across environments, it still depends on strong non-Gaussianity and independence conditions, similar to those used in LiNGAM-based approaches. The improvement over LiNGCReL thus appears more incremental rather than fundamental.\n\n* **Dependence on ICA and independence testing.**\n  The algorithm’s first stage heavily relies on ICA and associated independence measures, meaning the identifiability largely stems from ICA theory rather than a novel mechanism. In addition, the theoretical condition for independence is replaced in practice by the HSIC statistic (Gretton et al., 2005), which lacks formal consistency guarantees and is known to be unstable in empirical causal discovery (Rolland et al., 2022).\n\n* **Fragility of pruning and disentanglement stages.**\n  To the best of my understanding, the rank-based pruning assumes exact linear independence across environments and may be highly sensitive to small perturbations or finite-sample noise. This can propagate errors into the disentanglement stage, resulting in cascading inaccuracies, a problem noted in previous multi-stage causal methods (Varıcı et al., 2024b). Competing approaches such as Buchholz et al. (2023) adopt more robust regularization strategies.\n\n* **Empirical limitations.**\n  The synthetic experiments are limited to low-dimensional data (d ≤ 7, n = 1000) and only compared to LiNGCReL, without evaluation against broader baselines such as Squires et al. (2023) or nonlinear CRL models. The LLM case study is based on a pre-defined DAG and qualitative interpretation of latent “concepts,” making causal conclusions difficult to validate. Moreover, no runtime, ablation, or stability analyses are reported.\n\n* **Presentation and clarity.**\n  Some notation, such as the equivalence relations ($\\sim \\pi$, $\\sim \\Delta$, $ \\sim sur$) is introduced formally but lacks intuitive explanation. Including a small illustrative example (e.g., a 3-node toy system as in Ahuja et al., 2023) would improve readability."}, "questions": {"value": "* Could the authors clarify in what specific sense Assumptions 1–3 are weaker than those in Jin & Syrgkanis (2024)? Does the notion of identifiability here correspond to ancestral, Markov, or exact equivalence?\n* How is the HSIC test implemented in practice (kernel choice, thresholds), and how sensitive is the ordering step to deviations from true independence?\n* Could disentanglement quality be assessed with additional metrics, such as mutual information or subspace similarity, beyond LocR²?\n* In the LLM study, how do the authors verify that the assumed DAG reflects causal rather than syntactic relations? Would it be possible to compare the recovered features to known linguistic embeddings for validation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k76n3QimPu", "forum": "aLVKCEA7Lk", "replyto": "aLVKCEA7Lk", "signatures": ["ICLR.cc/2026/Conference/Submission14917/Reviewer_xFXr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14917/Reviewer_xFXr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053021038, "cdate": 1762053021038, "tmdate": 1762925260404, "mdate": 1762925260404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CREATOR (Causal REpresentation leArning via Topological Ordering, Pruning, and Disentanglement), a linear causal representation learning (CRL) method that assumes (i) a linear SCM over latent variables and (ii) a linear mixing from latents to observations. Relative to prior linear-CRL work, the method weakens distributional assumptions by requiring only non-Gaussian, independent noise (≤1 Gaussian component) and allows noise distributions to vary across environments. Synthetic experiments show improved LocR² (latent recovery) and SHD (DAG accuracy) versus the previous LiNGCReL algorithm; a small LLM case study illustrates feasibility under the “linear representation hypothesis.”"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Assumption 1 permits environment-specific noise distributions and only requires non-Gaussian independent components ($\\leq 1$ Gaussian), and Theorem 1 establishes identifiability up to ∼sur with ≥ d environments. This is a meaningful step beyond prior linear-CRL assumptions.\n\n- On synthetic tasks across $d\\in\\{2,3,5,7\\}$ and $K\\in\\{d,2d\\}$, CREATOR improves LocR² and SHD over LiNGCReL (Fig. 2; Fig. 3 in Appendix).\n\n- LLM case study is neat as a proof-of-concept."}, "weaknesses": {"value": "- While improving the performance of LiNGCReL, it appears that the underlying idea of the identification algorithm is largely the same, limiting the novelty of this paper."}, "questions": {"value": "- How does CREATOR behave when d is under- or over-estimated?\n\n- In noisy finite samples, do you use singular-value thresholds or bootstrap ranks for deciding the 1-rank drop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yy6qlXpVo7", "forum": "aLVKCEA7Lk", "replyto": "aLVKCEA7Lk", "signatures": ["ICLR.cc/2026/Conference/Submission14917/Reviewer_rpXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14917/Reviewer_rpXZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060467389, "cdate": 1762060467389, "tmdate": 1762925260025, "mdate": 1762925260025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}