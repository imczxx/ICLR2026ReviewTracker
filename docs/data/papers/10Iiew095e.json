{"id": "10Iiew095e", "number": 25190, "cdate": 1758365130279, "mdate": 1759896730967, "content": {"title": "StreamingThinker: Large Language Models Can Think While Reading", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. \nWe instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning.", "tldr": "We propose StreamingThinker, a framework that enables LLMs to think while reading.", "keywords": ["LLMs", "Reasoning", "Streaming"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8dc3142412d7e546ee5b04e1f7939c68f3766fdd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a paradigm that allows the model to stream while reasoning before outputting the results. Their experiments show that this streaming paradigm performs on par with batch thinking models but reduce the latency in getting the outputs. Both training and evaluation frameworks are provided for adpting LLMs to this new inference paradigm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Introduce paradigm that allows model to stream while reasoning\n- Intuitive, Well-engineered and clearly described training and evaluation frameworks\n- Demonstrate that using streaming performs on par with batch reasoning but with less latency\n- Use of multiple domains to demonstrate that this paradigm does well on different types of problems"}, "weaknesses": {"value": "- My primary reservation is the motivation behind the need for streaming thinking. What kind of application requires steaming reasoning? In what scenarios is batch thinking insufficient, especially when the batches are small? In the example in Figure 1, it makes much more intuitive sense to perform batch thinking to avoid \"overly eager\", not-so-helpful thinking tokens like \"Okay, now we get the background of Charlie's schedule.\" Intuitively, I would argue that math questions evaluated in this paper are more suitable with batch thinking or even thinking after the full input,especially in high-stakes applications. A clear vision of downstream applications would also improve the realism of the evaluation (for example, what is an interval that a user expect to see output).\n- It would be interesting to see if this paradigm can be used for other problems including planning domains to demonstrate generality"}, "questions": {"value": "- Presentation is mostly clear\n- Figure 4 has really small font making it hard to read"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h1Mo7j4UIv", "forum": "10Iiew095e", "replyto": "10Iiew095e", "signatures": ["ICLR.cc/2026/Conference/Submission25190/Reviewer_VM9v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25190/Reviewer_VM9v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854857243, "cdate": 1761854857243, "tmdate": 1762943357086, "mdate": 1762943357086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "StreamingThinker proposes a thinking while reading paradigm: the model generates step-by-step thoughts as the input streams in, rather than waiting for the full prompt. It implements this with by first generating streaming data to use for SFT, using a modified streaming attention mask, and parallel KV caches to decouple input prefill from reasoning. Across math/logic/QA tasks on Qwen3-1.7B/4B, it matches batch-style accuracy at deeper reasoning depths while cutting time-to-first-token and overall latency substantially. The method works for both question-first and context-first orders and allows controllable reasoning depth to trade off speed and accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The paper is well written and figures are very clear.\n* The method and paradigm are novel since I am not aware of any other work which performs reasoning concurrently with input prefill. This idea is also well motivated and could be highly impactful.\n* Evaluation is performed over a diverse set of datasets and show a substantial reduction in number of reasoning tokens as well as latency with little reduction in accuracy.\n* The distinction and evaluation of the question-first and context-first settings in the streaming setting is interesting and provides valuable insights into how streamingthinker works."}, "weaknesses": {"value": "* Tables lack variances.\n* The parallelization still occurs at sentence level chunks, and finer/larger granularities were not investigated."}, "questions": {"value": "* How does the use of two KV caches compare in memory overhead and performance to the use of one KV cache?\n* How should the reasoning depth be decided on? It seems like D1 has a significant impact on performance and is not desirable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I41WtwrdBg", "forum": "10Iiew095e", "replyto": "10Iiew095e", "signatures": ["ICLR.cc/2026/Conference/Submission25190/Reviewer_PpX3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25190/Reviewer_PpX3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977750773, "cdate": 1761977750773, "tmdate": 1762943356702, "mdate": 1762943356702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “StreamingThinker: Large Language Models Can Think While Reading” proposes a new streaming thinking paradigm that enables large language models (LLMs) to perform reasoning concurrently with input processing, mimicking how humans “think while reading.” The authors instantiate this idea through the StreamingThinker framework, which integrates (1) streaming chain-of-thought (CoT) generation, (2) streaming-aware training via attention and position encoding constraints, and (3) parallel inference using dual KV caches. Experiments on multiple reasoning benchmarks (math, logic, and QA) demonstrate comparable reasoning performance to conventional batch reasoning while reducing token waiting time by 80% and overall latency by more than 60%. Overall, the paper addresses an important efficiency bottleneck in LLM reasoning and proposes a novel direction toward real-time reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an interesting idea of streaming thinking, where LLMs reason concurrently with incoming input rather than after receiving the full context. This paradigm is conceptually appealing and well-motivated by the analogy to human cognition, offering a fresh perspective on reducing reasoning latency while maintaining coherence.\n2. The proposed framework is technically complete, integrating data construction, streaming-aware training, and parallel inference into a coherent system. The design shows good engineering consistency across stages.\n3. The empirical evaluation is broad and covers multiple reasoning domains. The depth-controlled reasoning design effectively demonstrates the trade-off between accuracy and latency."}, "weaknesses": {"value": "1. The authors should discuss more about why the method works. Thinking while reading is interesting, but it is obvious that the question is not complete while reading. There have been several papers [1,2,3] that mention that the incomplete questions will actually affect the model performance negatively. So, why think based on the incomplete questions during reading can improve performance?\n\n[1] Laban, Philippe, et al. \"Llms get lost in multi-turn conversation.\" arXiv preprint arXiv:2505.06120 (2025).\n\n[2] Fan, Chenrui, et al. \"Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?.\" arXiv preprint arXiv:2504.06514 (2025).\n\n[3] Li, Jinzhe, et al. \"Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models.\" arXiv preprint arXiv:2505.23715 (2025).\n\n2. The paper mentions that “we set the LLM’s input speed as the average human speaking rate about 150 words/s.” This number appears inconsistent with the typical rate of around 150 words per minute. It would be helpful for the authors to clarify whether this is a typographical error or an intentional experimental setting. If the latter, providing a short justification or citation would make the latency-related results more transparent and credible.\n3. Tables 2 and 3 present comparisons between the question-first and context-first input orders, but both experiments are conducted using offline datasets with simulated streaming inputs. Since the differences are described mainly at an intuitive level, it might strengthen the analysis to include a short diagnostic discussion in the appendix, for example, outlining typical error types or explaining how sequence order affects reasoning behavior.\n4. The motivation drawn from human cognition is interesting, but could be supported a bit more clearly. A concise paragraph in the appendix linking the proposed method to established cognitive theories of reading or incremental comprehension would help reinforce the conceptual foundation.\n5. The dual KV-cache design is highlighted as an important part of the system, yet the paper does not include quantitative information about memory usage, bandwidth cost, or first-token latency. Including even a brief table or figure reporting these numbers would make it easier to assess the trade-offs and scalability of the proposed approach for larger or real-time systems."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VmuOnHiZb7", "forum": "10Iiew095e", "replyto": "10Iiew095e", "signatures": ["ICLR.cc/2026/Conference/Submission25190/Reviewer_g9aY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25190/Reviewer_g9aY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101142094, "cdate": 1762101142094, "tmdate": 1762943356349, "mdate": 1762943356349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}