{"id": "wSUysJXv07", "number": 10371, "cdate": 1758168551610, "mdate": 1759897655381, "content": {"title": "NUMBERS AS TEXT: COMPLEMENTARY DUAL- MODALITY EMBEDDINGS FOR TIME SERIES FORECASTING", "abstract": "The remarkable success of Large Language Models (LLMs) in language tasks inspires us to explore their application in long-term time series forecasting(LTSF). Their ability to capture complex sequence dependencies from massive datasets suggests a strong potential for modeling the intricate patterns inherent in time series data. However, current methods for applying LLMs to LTSF often rely on hand-engineered statistical features and elaborate, dataset-specific prompts. This approach not only deviates from the end-to-end learning paradigm but also introduces a critical risk of lookahead bias, where performance gains may stem from the model accessing information within its pre-training corpus rather than genuine forecasting ability. A clear gap exists for a method that leverages LLMs on raw time series data in a robust, feature-free manner. To address this gap, we propose a novel framework, NumText, that directly operates on raw time series data. Our method treats the series as a dual-modality input, generating two parallel representations: a direct numerical value embedding and a forecasting-oriented LLM embedding derived from the series' textual form. These distinct embeddings are then combined through a modality-specific Mixture-of-Experts (MoE) to form a rich, unified input for a downstream attention mechanism. Furthermore, we introduce a time-series text embedding cache to reduce computational overhead during inference. Our extensive experiments reveal that the numerical value embeddings and the LLM's textual embeddings are highly complementary, capturing different yet synergistic signals crucial for forecasting. This synergy enables our model to achieve improvement upon current state-of-the-art (SOTA) performance on several benchmark TSF datasets, establishing a more robust approach for applying LLMs in this domain.", "tldr": "Without using complex prompts, we achieve SOTA time series forecasting by feeding the model two complementray views of the raw data - numerical and textual form - establishing a new approach for applying LLM in this domain.", "keywords": ["Time Series Forecasting", "Multi-Modality", "LLM"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e30a2ea78894e23ec189575db15a14a003964138.pdf", "supplementary_material": "/attachment/edb398ec1b9b99ebf6f0073bcd4172e1b1e4b4b0.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a framework for time series forecasting. It treats the series as a dual-modality input, generating the numerical value embeddings and the LLM output embeddings, which are then combined through a modality-specific Mixture-of-Experts (MoE). Experiments reveal that the numerical embeddings and the textual embeddings are complementary benefits for time series forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper assumes that the time series as text is the most effective method for LLM-based time series forecasting. I like the new experimental settings and theoretical justification of this paper. The code is available, which increases the credibility of this paper."}, "weaknesses": {"value": "The presented \"Raw Data Input\" and \"Time Series as Numerical Value\" have already been proposed in existing studies [1-3]. The dual-modality architecture and time-series text embedding cache for reducing computational costs have already been proposed in [3]. Please elaborate on the differences between the existing techniques in SOAT works and this paper in more detail.\n\n[1]  Large language models are zero-shot time series forecasters[J]. Advances in Neural Information Processing Systems, 2023, 36: 19622-19635.\n\n[2] Promptcast: A new prompt-based learning paradigm for time series forecasting[J]. IEEE Transactions on Knowledge and Data Engineering, 2023, 36(11): 6851-6864.\n\n[3] Timecma: Towards llm-empowered multivariate time series forecasting via cross-modality alignment[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(18): 18780-18788."}, "questions": {"value": "- Could you please clarify your contribution and existing techniques in Weakness 1?\n\n- In Theorem 4.1, the authors claimed that MSE using both modalities is never worse than using a single modality. May I ask the specific scenario? If modality in modality out, I agree with this Theorem. But if modality in single modality (time series) is out, is the Theorem still established? As external modality would be the noise for the single modality output if there are no effective alignment strategies. A recent study [4] also points out that \"Multimodal time series forecasting models do not consistently outperform the strongest unimodal baselines.\"\n\n- How about the time for generating and reading the cache? These costs would be considered.\n\n[4] Does Multimodality Lead to Better Time Series Forecasting?[J]. arXiv preprint arXiv:2506.21611, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "44bGx5KFMo", "forum": "wSUysJXv07", "replyto": "wSUysJXv07", "signatures": ["ICLR.cc/2026/Conference/Submission10371/Reviewer_751u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10371/Reviewer_751u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467195786, "cdate": 1761467195786, "tmdate": 1762921694120, "mdate": 1762921694120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "NumText presents a dual-modality representation for long-term TSF that fuses a direct numerical value embedding with a textual embedding obtained by quantizing raw segments into integer tokens and encoding them with a frozen LLM. A soft MoE performs input-dependent weighting of the two modalities, followed by a standard Transformer encoder and a linear forecasting head. The paper evaluated on 8 benchmarks and found that numerical and LLM-textual views are complementary, and that dynamic fusion outperforms static fusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A simple dual-path (Numerical vs Textual Expert) with a soft MoE and a standard Transformer head is easy to implement and analyze.\n2. The textual input is mechanically derived via quantization from raw numbers; no dataset-specific statistics/metadata prompts, reducing lookahead bias risk.\n3. Ablations (Num-only / Text-only / Dual) show consistent, supporting the hypothesis; plus a caching trick for LLM embeddings improves efficiency."}, "weaknesses": {"value": "1. Missing critical LLM-TSF baselines results, such as GPT4TS, Time-LLM and TEMPO [1], making the evidence for superiority incomplete. \n2. Limited novelty for “LLM-Based TSF”: The core novelty is a straightforward quantize-to-text + frozen LLM embedding + MoE fusion. This is not a fundamentally new LLM-TSF paradigm. \n3. The performance gains over strong baselines are modest, which does not substantiate the claimed effectiveness.\n4. The method uses a mean-of-patches as context; the paper lacks deeper analyses (e.g., per-regime routing behavior, sensitivity to quantization, or cross-attention structure vs MoE). This limits insight into when and why textual LLM embeddings help.\n\n[1] TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting"}, "questions": {"value": "In addition to the commonly used 8 benchmarks, would you consider including more datasets, such as the Monash TSF Archive (e.g., M3/M4/M5), Exchange Rate, Solar, Wiki-traffic (non-stationary), retail demand (intermittent), and healthcare data (e.g., ICU vitals with missingness)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ir0jhFhXDW", "forum": "wSUysJXv07", "replyto": "wSUysJXv07", "signatures": ["ICLR.cc/2026/Conference/Submission10371/Reviewer_csUN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10371/Reviewer_csUN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503000613, "cdate": 1761503000613, "tmdate": 1762921693752, "mdate": 1762921693752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NumText, which is a dual-modality framework that integrates the numerical and textual representations of time series data to improve time series forecasting performance. A time series patch is processed by two experts, specifically a textual expert, which converts normalized numerical values into quantized text tokens and extracts semantic embeddings via a frozen LLM, and a numerical expert, which linearly projects the raw numeric patch into embedding space.  It also designs a soft Mixture-of-Experts module that dynamically fuses both modality representations. A Transformer backbone and projection head are used for forecasting.  Empirical results across eight benchmark datasets show that the NumText achieves good performance results and improves results over unimodal baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I support that when using LLMs for time series forecasting, inputting numerical inputs as text is more effective than incorporating contextual descriptions ( this may work when classification or other decision boundary involved time series analysis task). The idea of treating raw TS data simultaneously as numbers and text is great.\n\n2. Providing a theoretical proof about incorporating two different modalities is beneficial than unimodal forecasting and soft-MoE is effective.\n\n3. Comprehensive experimental evaluation on benchmark datasets and compare with baseline models. Ablation study shows the effectiveness of modalities and minimal effects on different choices of LLM."}, "weaknesses": {"value": "1. Components for time series process ( ReINV, Patch, text tokenization etc.) are widely used in current time series analysis pipelines. Thus, the technical novelty of this paper is limited. \n2. Related work is not up-to-date and missing important baseline comparisons, such as time-LLM. \n3. Your experimental results appear promising, particularly on the weather datasets. However, the visualizations suggest inconsistencies, which I doubt the results."}, "questions": {"value": "I looked into your code, but didn't find the soft-MoE design. Since enc_out, n_vars = self.patch_embedding(x_enc) followed with enc_out, attns = self.encoder(enc_out). It would be great if you could point the soft-MoE code to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x3HihAxDoD", "forum": "wSUysJXv07", "replyto": "wSUysJXv07", "signatures": ["ICLR.cc/2026/Conference/Submission10371/Reviewer_sD2x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10371/Reviewer_sD2x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683636870, "cdate": 1761683636870, "tmdate": 1762921693304, "mdate": 1762921693304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies time series forecasting (TSF) by introducing NumText, a dual-modality framework to extract embeddings from (1) the raw time series, and (2) the textual prompt (which includes the time series), and then use a MoE module to fuse the embeddings of (1) and (2) for further forecasting. Extensive experiments have been conducted to verify the effectiveness of each component, validating the complementarity of numerical and textual representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The core idea of separately extracting embeddings from two modalities and fuse them for TSF is reasonable.\n2. Some theoretical justifications are included to verify the superiority of dynamic MoE over single-modality or static fusion methods.\n3. Experiments have been conducted with different LLMs to demostrate the robustness of the proposed framework."}, "weaknesses": {"value": "1. In L45-L49, the description of 2 problems are farfetched:\n- I disagree (1) is a fundamental problem, existing methods just use some simple statistics.\n- In (2), LLMs have not been trained with time series corpus, I don't think future data would appear during training.\n- There are no clues to prove the existence of (2) and no experiments have been conducted to validate that this paper can prevent from (2).\n2. In L80-L81, it's unclear why representing time series as text can directly unlock LLM's forecasting ability - there should theoretically be a clear gap between sentences and time series.\n3. In L113-L115, the wordings like \"first to systematically ...\" is overclaimed - in the baseline TimeCMA (mentioned in L109), it has already studied such approach.\n4. It seems this method just replaces the cross attention of TimeCMA, and shorten the textual prompt, please discuss how this paper differs to TimeCMA.\n- The techinical contribution of this paper is quite limited.\n- Besides, there are no experiments to demonstrate the superiority of MoE (this paper) over the cross attention (TimeCMA).\n5. In Table 2, the performance of three settings are quite similar in almost all cases, and the incorporation of LLMs would inevitably make the inference speed much slower, making it unnecessary to introduce LLM for TSF.\n6. In Table 3:\n- Please include more recent and stronger advances for comparisons, e.g., TimeXer.\n- Please include more LLM-based methods for comaprisons, e.g., Time-LLM.\n- The results of TimeCMA look quite strange - worse than PatchTST in most cases."}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HFXPkyI9FD", "forum": "wSUysJXv07", "replyto": "wSUysJXv07", "signatures": ["ICLR.cc/2026/Conference/Submission10371/Reviewer_TNs5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10371/Reviewer_TNs5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721446715, "cdate": 1761721446715, "tmdate": 1762921692911, "mdate": 1762921692911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}