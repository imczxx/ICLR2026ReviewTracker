{"id": "qcNu1mL46N", "number": 805, "cdate": 1756818945961, "mdate": 1759898240995, "content": {"title": "Kolmogorov-Arnold Fourier Networks", "abstract": "Although Kolmogorov-Arnold based interpretable networks (KAN) have strong theoretical expressiveness, they face significant parameter explosion and high-frequency feature capture challenges in high-dimensional tasks. To address this issue, we propose the Kolmogorov-Arnold-Fourier Network (KAF), which effectively integrates trainable Random Fourier Features (RFF) and a novel hybrid GELU-Fourier activation mechanism to balance parameter efficiency and spectral representation capabilities. Our key technical contributions include: (1) using Fourier transform to avoid the parameter explosion problem caused by KAN's B-spline function and significantly reduce the number of parameters; (2) introducing learnable RFF initialization strategies to eliminate spectral distortion in high-dimensional approximation tasks; (3) implementing an adaptive hybrid activation function that progressively enhances frequency representation during the training process. Comprehensive experiments demonstrate the superiority of our KAF across various domains, including vision, NLP, audio processing, and differential equation-solving tasks. We will release the source code of our method in accordance with the review policy.", "tldr": "We propose the Kolmogorov-Arnold-Fourier Network (KAF), which integrates trainable Fourier features to solve the parameter explosion of KANs and improve high-frequency representation in high-dimensional tasks.", "keywords": ["Deep Learning", "KAN", "MLP", "Fourier"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8930170522c7aefb222af474bc20671dc5fa1b9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Kolmogorov-Arnold-Fourier Networks (KAF), a modification of Kolmogorov-Arnold Networks (KANs) that replaces B-spline activations with trainable Random Fourier Features (RFFs) and a hybrid GELU–Fourier activation.\nThe goal is to:\n\nReduce the parameter explosion in KANs.\n\nImprove high-frequency representation.\n\nPreserve interpretability and expressiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Motivation and Novel Angle\n\nThe paper correctly identifies KAN’s bottlenecks — parameter inefficiency and poor high-frequency capture — and links them to B-spline smoothness and dense parameterization.\nThe proposed RFF-based architecture is a natural and elegant evolution: Fourier expansions can model oscillatory components efficiently while maintaining GPU-friendly structure.\n\n2. Hybrid GELU–Fourier Design\n3. Solid Experimental Breadth\n\nThe experimental section is comprehensive:\n\nImage classification (MNIST, CIFAR, SVHN, ImageNet).\n\nText modeling (GPT-2 on Wikitext-103, OpenWebText).\n\nPDEs and function approximation."}, "weaknesses": {"value": "1. Innovation is not sufficient; more like engineering tricks\n2. lack fo lit rev on kans; for example the kan 2.0 paper (Kan 2.0: Kolmogorov-arnold networks meet science), also the paper on KAN's spectral bias (On the expressiveness and spectral bias of KANs) among other advances on kans."}, "questions": {"value": "any failure modes of RFF KAN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aItLsx8qXC", "forum": "qcNu1mL46N", "replyto": "qcNu1mL46N", "signatures": ["ICLR.cc/2026/Conference/Submission805/Reviewer_UmsF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission805/Reviewer_UmsF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760461583178, "cdate": 1760461583178, "tmdate": 1762915608484, "mdate": 1762915608484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Kolmogorov-Arnold-Fourier Networks (KAF), an extension of Kolmogorov-Arnold Networks (KAN) that replaces B-spline activations with trainable Random Fourier Features (RFF) and a hybrid GELU-Fourier activation to mitigate parameter explosion and improve high-frequency capture in high-dimensional tasks. Evaluated across vision (e.g., MNIST, CIFAR), NLP (e.g., Wikitext-103, AG News), audio, machine learning tasks, function approximation, and PDE solving, KAF shows competitive or superior performance to baselines like MLP, KAN, GPKAN, and FAN, often with fewer parameters and better noise robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By integrating Fourier transforms and trainable RFF, KAF significantly reduces the number of parameters while preserving expressiveness, as proven in Appendix B.1. \n2. The Experiments span diverse domains: vision accuracies up to >91% on CIFAR10 (Table 1); lower perplexity on NLP tasks (Table 2); superior RMSE on function approximation (Fig. 5) and PDEs like Poisson/Heat (Fig. 6, Table 5 for noise robustness). These experiments show a significant improvement in KAN. \n3. The mathematical expression is detailed (e.g., RFF projection in Sec. 3.3, hybrid activation), and appendices for the preliminary section are easy to understand (e.g., F for function approximations, K for ablation studies), aiding replication."}, "weaknesses": {"value": "1. The experimental results on CIFAR-10 and ImageNet1K should be improved. I read the paper in references, the accuracy of ResNet-18 on CIFAR-10 is 93.02% [1], the top-1 accuracy of ViT-Tiny on ImageNet-1K is 79.1% [2], and the perplexity of GPT-2 on wikitext-103 is 19.89 [3]. However, in Table 1 and Table 2, KAF shows better accuracy than MLP when performing as a feature mixer. I guess such a conflict is caused by insufficient hyperparameter tuning (e.g., learning rate schedule, batch size, etc.).\n2. Inference times should be reported, despite claims of efficiency from compact features. \n3. Figure 5 shows the analysis test RMSE versus the number of parameters. But it seems that KAF does not satisfy the scaling law?\n4. As a feature-mixture, KAF can improve model performance when replacing the neural network head. However, in the standalone scenario without a neural network as a feature extractor, it still performs poorly on function approximation tasks (see Figure 9).\n\n\nReferences:\n\n[1] https://github.com/kuangliu/pytorch-cifar\n\n[2] https://github.com/microsoft/Cream/tree/main/TinyViT\n\n[3] https://huggingface.co/Graphcore/gpt2-wikitext-103"}, "questions": {"value": "1. If time allows for revision, please conduct a full hyperparameter sweep and report updated experimental results.\n2. Report inference latency (ms/image for vision or ms/token for language tasks) alongside parameter counts to substantiate efficiency claims.\n3. Does KAF follow neural scaling laws? Provide log-log plots of test loss vs. parameter count with fitted power-law trends.\n4. For function approximation tasks, this method is tested only on simple functions, not on complex cases (high-dimensional, non-smooth, or multi-frequency). Such limitations in computational complexity are critical, I expect the scaling law slope to flatten considerably as the input dimension increases or when 1D functions exhibit varying local frequencies. Focusing on KANs may not be appropriate, because the uniform grids of KAN raise these core issues. I suggest that the author read some recent MLP variants that have solved these problems using the multi-scale mesh (a standard data structure in the finite element method)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8iWfPuXbN3", "forum": "qcNu1mL46N", "replyto": "qcNu1mL46N", "signatures": ["ICLR.cc/2026/Conference/Submission805/Reviewer_5ZEN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission805/Reviewer_5ZEN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977689554, "cdate": 1761977689554, "tmdate": 1762915608356, "mdate": 1762915608356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose KAF, which retains the structural form of KAN (univariate function composition) but replaces the costly spline-based parameterization with learnable Fourier components."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "KANs typically grow in parameter count as $O(d_{in} d_{out} (G + K + 3))$, which scales poorly. The proposed hybrid GELU–Fourier activation (with learnable coefficients) adds a smooth transition mechanism from standard activation-based networks to Fourier-driven symbolic representations. This activation concept is novel and could be applied beyond KANs."}, "weaknesses": {"value": "The paper argues that Random Fourier Features mitigate KAN’s limited ability to represent high-frequency components. However, RFFs derived from the Gaussian kernel correspond to elements of a very smooth RKHS, whose spectral density decays rapidly. It remains unclear how this construction enhances high-frequency expressiveness. The authors should clarify whether the frequency distribution in their trainable RFFs deviates from the Gaussian kernel case, and if so, how this affects the underlying function space."}, "questions": {"value": "The main question to address is formulated in Weaknesses part. \n\nComparison of MLP and KAF: From Table 1 and Table 2, it is hard to make a conclusion, which one is better, since the parameter budget on KAF is systematically larger. \n\nMinor: (Bracewell & Bracewell, 1986)-->(Bracewell, 1986)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ihvInaKoMB", "forum": "qcNu1mL46N", "replyto": "qcNu1mL46N", "signatures": ["ICLR.cc/2026/Conference/Submission805/Reviewer_Ei2c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission805/Reviewer_Ei2c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984860383, "cdate": 1761984860383, "tmdate": 1762915608237, "mdate": 1762915608237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}