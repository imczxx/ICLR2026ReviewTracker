{"id": "n2CIy5Bm8M", "number": 22665, "cdate": 1758334229546, "mdate": 1759896853692, "content": {"title": "BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic Verification of Neural Networks", "abstract": "Branch-and-bound with preactivation splitting has been shown highly effective for deterministic verification of neural networks. In this paper, we extend this framework to the probabilistic setting. We propose BaB-prob that iteratively divides the original problem into subproblems by splitting preactivations and leverages linear bounds computed by linear bound propagation to bound the probability for each subproblem. We prove soundness and completeness of BaB-prob for feedforward-ReLU neural networks. Furthermore, we introduce the notion of uncertainty level and design two efficient strategies for preactivation splitting, yielding BaB-prob-ordered and BaB+BaBSR-prob. We evaluate BaB-prob on untrained networks, MNIST and CIFAR-10 models, respectively, and VNN-COMP 2025 benchmarks. Across these settings, our approach consistently outperforms state-of-the-art approaches in medium- to high-dimensional input problems.", "tldr": "We propose a sound and complete algorithm for probabilistic verification of neural networks based on branch and bound with preactivation splitting, consistently outperforming state-of-the-art algorithms on high-dimensional-input problems", "keywords": ["Probabilistic Verification", "Neural Network Robustness", "Branch and Bound", "Sound and Complete"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c22449ad6dc69fd05b828a74cd5fecb465d9b948.pdf", "supplementary_material": "/attachment/b1557ce4148552fc71f5a6697b44da67b5955b2f.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents BaB-prob, an algorithm for verifying probabilistic specifications of neural networks. It combines ReLU splitting with Monte Carlo sampling to estimate event probabilities. The method includes heuristics for selecting ReLUs to split and is evaluated against existing probabilistic verification approaches."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores the promising concept of applying neuron splitting to the probabilistic verification of neural networks.\n- BaB-prob shows improved scalability over existing methods, though it may produce unsound results due to Monte Carlo sampling.\n- The experimental evaluation includes a diverse set of challenging benchmarks.\n- Theoretical analysis of BaB-prob yields several notable findings.\n- The confidence interval analysis is valuable.\n- The writing is concise and clear."}, "weaknesses": {"value": "1. Zhang et al. already covered related ground in their TACAS 2024 paper, \"Provable Preimage Under-Approximation for Neural Networks.\" Their work already introduces neuron splitting in the context of probabilistic verification, and they leverage both linear bound propagation and Monte Carlo sampling for probability estimation. \n\n2. BaB-prob doesn't account for infeasible branches that arise during neuron splitting, which means the algorithm lacks completeness. The issue traces back to Proposition 4—the proof there has a flaw that undermines the completeness claim. Specifically, the assertion that \"since there is no unstable preactivation in B, no relaxation is performed during the linear bound propagation, the inequalities in Equation (3) become equalities\" breaks down when you're dealing with infeasible branches.\n\n3. The theoretical results do not account for the Monte Carlo approximations that are required for practically applying this algorithm. \n\n4. The description of how confidence intervals are handled in Appendix C.4 explains that when BaB-prob-ordered or BaB+BaBSR-prob produces a declaration with confidence below $1 - 10^{−4}$, the algorithm keeps running until either hitting that confidence threshold or reaching the time limit. The issue is that this approach actually invalidates the confidence levels—the problem is that the number of iterations becomes dependent on the confidence level itself, which breaks the statistical guarantees. Budde et al. have a nice paper in TACAS 2025 called \"Sound Statistical Model Checking for Probabilities and Expected Rewards\" that describes this issue in detail.\n\n5. There's a fundamental issue with how the comparison is set up. The paper is essentially comparing Monte Carlo approximations—which give you potentially unsound results with confidence intervals—against algorithms that compute exact probabilities. That's not really an apples-to-apples comparison. The core problem is that once we allow potentially unsound results with confidence intervals, we're actually reducing the complexity of probabilistic verification."}, "questions": {"value": "**Q1.** Please comment on the theoretical issues described above.  \n**Q2.** Can you compare your approach to Zhang et al. (2024) both conceptually and experimentally?  \n**Q3.** Can you conduct experiments where you compute exact probabilities for an apples-to-apples comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GHebQhuwJw", "forum": "n2CIy5Bm8M", "replyto": "n2CIy5Bm8M", "signatures": ["ICLR.cc/2026/Conference/Submission22665/Reviewer_J25x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22665/Reviewer_J25x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761234259136, "cdate": 1761234259136, "tmdate": 1762942327616, "mdate": 1762942327616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BaB-prob, a branch-and-bound (BaB) framework for probabilistic verification of neural networks. It adapts preactivation-splitting—common in deterministic verification—to probabilistic settings by integrating linear bound propagation to estimate probability bounds for each branch. Two splitting heuristics are proposed: BaB-prob-ordered (simple layer-first rule) and BaB+BaBSR-prob (combining BaBSR with an “uncertainty level” metric). The method is proven sound and complete for ReLU networks and evaluated on untrained models, MNIST, CIFAR-10, and VNN-COMP 2025 benchmarks, showing improved scalability over PROVEN, PV, and SDP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Extends a well-known deterministic BaB technique into the probabilistic domain with clear theoretical guarantees (soundness, completeness, termination).\n\n2. Demonstrates consistent empirical gains over PROVEN and PV across several datasets and network types.\n\n3. Sound implementation and evaluation; code is promised for release.\n\n4. Solid integration of linear bound propagation for probability bounds."}, "weaknesses": {"value": "1. Incremental novelty: The extension from deterministic to probabilistic BaB is conceptually straightforward; most components (bound propagation, splitting logic) are inherited from prior work.\n\n2. Weak heuristic motivation: The “uncertainty level” is introduced heuristically with limited theoretical grounding. It is unclear how it generalizes or why it outperforms simple ordering beyond empirical evidence.\n\n3. Scalability trade-offs: Monte Carlo probability estimation can become the bottleneck for high-dimensional Gaussian inputs, limiting real-world applicability."}, "questions": {"value": "A recent work, “Towards Reliable Neural Specifications” (Geng et al., ICML 2023), proposed using neural activation patterns as verification specifications. Could BaB-prob be extended or adapted to handle such neural-specification–based properties, rather than standard input–output constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ayIIOLp00E", "forum": "n2CIy5Bm8M", "replyto": "n2CIy5Bm8M", "signatures": ["ICLR.cc/2026/Conference/Submission22665/Reviewer_FdAL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22665/Reviewer_FdAL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435107022, "cdate": 1761435107022, "tmdate": 1762942327365, "mdate": 1762942327365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new sound and complete approach for proving probabilistic properties of neural networks. The work builds on previous work for proving deterministic properties that combines branch-and-bound techniques with linear bound propagation. The authors present a similar branch-and-bound approach for verifying probabilistic properties. The main adaptation required to prove probabilistic properties is the introduction of new splitting heuristics. The authors introduce two splitting heuristics based on an idea of uncertainty levels. The authors compare their method to multiple baselines on a variety of benchmarks and show improved performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper gives adequate attention to all parts of the research including previous work, methods, and experiments.\n- The algorithm provided is both sound and complete, so it can be used for exact probabilistic verification.\n- The authors perform extensive experiments and compare their results to relevant baselines. These experiments show how the method scales to different network sizes and architectures and highlight the different behavior of their proposed splitting heuristics.\n- The results significantly outperform the baseline methods."}, "weaknesses": {"value": "- The method requires the user to assume that the input is bounded, so it must truncate input distributions that are defined for all real numbers.\n- As the authors mention, equation (4) might be difficult to evaluate analytically, especially if the input distribution is not Gaussian. Propagating non-Gaussian distributions is a nontrivial task.\n- The main contribution of the paper appears to be the splitting heuristics since the core of the method relies on standard bound propagation techniques.\n- The splitting heuristics perform quite differently on MLPs vs CNNs, but no explanation or intuition is provided."}, "questions": {"value": "- What is the intuition behind the different performance of the splitting heuristics on the CNN and MLP models?\n\nSuggestions\n- Line 28 typo: “asks whether a given satisfies”\n- Line 77: function -> functions\n- Font in figure 1 is a bit small (especially in the green boxes)\n- Line 137: upper functions -> upper bound functions\n- Line 138: add some description explaining why it is not required (I assume it is because ReLU is linear in this case)\n- Line 345: practive -> practice\n- Line 374: avergae -> average\n- Line 470: the first time LiRPA is mentioned is in the conclusion, it would be good to mention earlier in the methods section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NtPG7pYlpJ", "forum": "n2CIy5Bm8M", "replyto": "n2CIy5Bm8M", "signatures": ["ICLR.cc/2026/Conference/Submission22665/Reviewer_2gZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22665/Reviewer_2gZ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673643689, "cdate": 1761673643689, "tmdate": 1762942327150, "mdate": 1762942327150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors extend branch-and-bound with preactivaction splitting to the setting of probabilistic verification of neural networks and demonstrate the soundness and completeness of their method for Linear/ReLU networks. The authors introduce two variants for selecting the preactivation to split based on their notion of uncertainty level, and benchmark them against competing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The main strength is extending the branch-and-bound framework with preactivation splitting to a new domain, that of probabilistic verification. \n* The authors prove some useful theoretical properties of their approach\n* The usefulness of the method is supported by strong empirical evidence\n* Clear presentation, well-written paper"}, "weaknesses": {"value": "* The verifier is presented as sound and complete; however, in practice, results rely on Monte-Carlo sampling and certifying the results with a high confidence level. \n* The novelty is limited. The paper is mainly a combination of widely-used techniques, like linear bound propagation, branch-and-bound, and probability estimation over linear regions\n* Experiments focus on one kind of distribution: perturbations of input points with Gaussian noise. How would the method perform on other input distributions? \n* No limitations/discussions paragraph/section included"}, "questions": {"value": "In addition to the above question, can you explain why the method seems to underperform on lower-dimensional datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cjcMKitvJq", "forum": "n2CIy5Bm8M", "replyto": "n2CIy5Bm8M", "signatures": ["ICLR.cc/2026/Conference/Submission22665/Reviewer_z8V7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22665/Reviewer_z8V7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961046535, "cdate": 1761961046535, "tmdate": 1762942326949, "mdate": 1762942326949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}