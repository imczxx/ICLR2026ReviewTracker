{"id": "StIIArpUZ3", "number": 6739, "cdate": 1757994091532, "mdate": 1759897897768, "content": {"title": "Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition", "abstract": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others’ learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified $k$-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.", "tldr": "We propose MCEM with NCD to overcome CDM in cooperative MARL, improving sample efficiency with modified off-policy k-step return, and achieving better results than prior methods on continuous and discrete action benchmarks.", "keywords": ["Multi-agent Reinforcement Learning", "Multi-agent Cross Entropy Method"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ac76ad5f7fb1e76d5213a7aa1a5189ea07393ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends the Cross-Entropy Method (CEM) to multi-agent settings and combines it with a monotonic nonlinear critic decomposition (NCD) to enhance representational power while mitigating the centralized–decentralized mismatch (CDM).\nTo improve sample efficiency, the authors employ Sarsa + Retrace to construct off-policy targets for training the decomposed critic.\nExperimental results suggest that the proposed method can outperform existing policy-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extending CEM to multi-agent settings and pairing it with a monotonic nonlinear decomposition is an interesting and novel direction.\n\n- The approach demonstrates promising preliminary results and could inspire future work on cross-entropy–based policy optimization in cooperative MARL."}, "weaknesses": {"value": "1. The theoretical analysis is informal and lacks rigor.\nIn particular, I do not see why the first inequality in Appendix A, $E_{\\pi_g}[Q^{\\pi_g}]\\le E_{\\pi_\\rho}[Q^{\\pi_g}]$,\nshould hold under the description provided.\nThe authors should formally define both $\\pi_g$ and $\\pi_\\rho$.\nAre they final converged policies, or intermediate policies after one policy improvement step?\nThe proof also implicitly assumes access to the exact $Q^\\pi$, whereas in practice $Q^\\pi$ is estimated by a QMIX-style network with monotonic constraints that induce bias.\nTherefore, it is unclear whether the proposed method guarantees true policy improvement.\n\n2. In section 4.2, the paper states that $E_\\pi[Q_{tot}(\\tau_{t+1},\\cdot)]$ is replaced by $Q_{tot}(\\tau_{t+1},u_{t+1})$ with $u_{t+1}\\sim\\pi$.\nHowever, by definition these quantities are equivalent—the latter is simply a Monte Carlo sample of the former—and this substitution does not introduce any new idea.\nFurthermore, simply adopting Retrace from prior work is not novel.\nMore importantly, there is no theoretical or empirical discussion of whether Retrace’s convergence or bias-control properties still hold under a nonlinear decomposition and function approximation, as used in NCD.\n\n3. MCEM samples joint actions by independently sampling from each agent’s policy to form a set $E(\\tau)$, which is then ranked by $Q_{tot}$ to select elites.\nHowever, as the number of agents grows, the joint-action space increases exponentially.\nUsing a fixed small number of joint samples (10 for discrete, 20 for continuous, as stated in the experiments) is unlikely to capture meaningful coordinated behaviors.\nIn high-dimensional or continuous-action settings, a small $E(\\tau)$ will likely miss high-value joint actions, and CEM may collapse to suboptimal modes.\n\n4. Several configuration details raise significant concerns about fairness and reproducibility:\n\n- The paper does not list hyperparameters in an appendix, forcing readers to locate them manually.\n\n- In the MCEM-NCD configuration, the parameter batch_size_run is set to 1, whereas the default in PyMARL2 is 8. This discrepancy can lead to substantial performance differences.\n\n- The critic_hidden_dim is set to 256, which is larger than that used by many baselines such as RIIT (128), possibly inflating performance.\n\n  Such inconsistencies should be clearly justified, and all experimental hyperparameters should be fully reported.\n\n5. It remains unclear why the Cross-Entropy Method is an appropriate or necessary tool for addressing the centralized–decentralized mismatch (CDM).\nThe proposed method appears to make QMIX “on-policy” by introducing a CEM-based actor search, and then convert it back to off-policy training via Retrace corrections.\nThis two-step design seems ad hoc and raises the question: why not directly use an off-policy actor-critic formulation guided by the centralized critic?\nThe paper should better justify how CEM fundamentally helps resolve CDM, rather than serving as a heuristic joint-action search."}, "questions": {"value": "1. The policy update in Eq. (6)–(7) maximizes the likelihood over elites.\nIs this update biased compared to a standard policy gradient?\nPlease clarify the relationship between your update and REINFORCE or advantage-weighted regression.\n\n2. What is the computational overhead of MCEM relative to a standard centralized policy-gradient approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lg5cV9jemp", "forum": "StIIArpUZ3", "replyto": "StIIArpUZ3", "signatures": ["ICLR.cc/2026/Conference/Submission6739/Reviewer_JGhA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6739/Reviewer_JGhA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760839447250, "cdate": 1760839447250, "tmdate": 1762919024991, "mdate": 1762919024991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a MA cross-entropy method, combined with monotonic nonlinear critic decomposition, in order to address the issue of centralised-decentralised mismatch caused by suboptimal behaviours."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The extension of the CEM form the single-agent to MARL setting is natural and well-formulated as a percentile-greedy policy.\n2. The computational details of the experiments are sufficient.\n3. The results in SMAC against related VD methods are somewhat convincing, as MCEM NCD performs better or the same as other baselines (see below)."}, "weaknesses": {"value": "1. The primary implementation contribution seems minimal - the only difference appears to be in how the actions are selected for the fit of the network. While Theorem 5.1 appears to demonstrate that the MCEM method should perform at least as well as baseline, this doesn't guarantee improvement in the general setting. Why was no equilibrium analysis or spectral analysis of the game dynamics with factorization performed to motivate the method further? Indeed, in the results, MCEM sometimes performs within the statistical margins of other baselines.\n2. What is the shaded area in Figure 2? We need to know this in order to assess the statistical significance of the method.\n3. While SMAC and PP are relevant benchmarks, additional benchmarks would supplement this work further as they are not considered competitive MARL benchmarks anymore (SMAC can even be solved by open-loop policies[1]). The authors could consider Mamujoco, SMACv2, Overcooked, or any of the common reward benchmarks in Benchmarl [2].\n4. While deep MARL benchmarks are useful, a didactic example of centralised-decentralised mismatch would be pivotal in motivating this work. Is there a game which requires the cross-entropy sampling to be solvable? Even a contrived setting would motivate the usefulness of the proposed method.\n\n[1] Ellis, Benjamin, et al. \"Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 37567-37593.\n[2] Bettini, Matteo, Amanda Prorok, and Vincent Moens. \"Benchmarl: Benchmarking multi-agent reinforcement learning.\" Journal of Machine Learning Research 25.217 (2024): 1-10."}, "questions": {"value": "1. Can the MCEM method be generalized to non-monotonic decomposition methods such as FACMAC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GWPjGIUCXF", "forum": "StIIArpUZ3", "replyto": "StIIArpUZ3", "signatures": ["ICLR.cc/2026/Conference/Submission6739/Reviewer_P8CF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6739/Reviewer_P8CF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753103777, "cdate": 1761753103777, "tmdate": 1762919024599, "mdate": 1762919024599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new multi-agent reinforcement learning algorithm based on the cross-entropy method to solve the centralized–decentralized mismatch issue in MARL. Simulation results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Simulations are conducted on standard MARL benchmarks, and advanced baselines are compared."}, "weaknesses": {"value": "1. Many policy gradient formulas in this paper are incorrect.\n\n2. The use of the auxiliary proposal policies is unclear.\n\n3. The convergence of the proposed algorithm cannot be guaranteed from a theoretical perspective.\n\n4. For the continuous action setting, the authors are recommended to evaluate their proposed algorithm on benchmarks with high-dimensional action spaces, and to discuss the hyperparameter settings related to the cross-entropy method in their algorithm."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DwPnMR0c3l", "forum": "StIIArpUZ3", "replyto": "StIIArpUZ3", "signatures": ["ICLR.cc/2026/Conference/Submission6739/Reviewer_UGSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6739/Reviewer_UGSh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900403526, "cdate": 1761900403526, "tmdate": 1762919024194, "mdate": 1762919024194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to develop a policy gradient method using nonlinear value function decomposition in collaborative MARL, avoiding the CDM problem under the CTDE framework. The authors extend CEM from single-agent to multi-agent scenarios, using Q-total to quantile-select sampled joint actions, intending to use these elite actions to update the decentralized policy network of each agent. This mechanism alleviates the CDM problem by rejecting suboptimal joint actions and eliminating the suboptimal influence of some agents. To improve sample efficiency, the authors designed an off-policy Critic learning method, modifying the k-step reward objective based on Expected Sarsa in DOP to a Sarsa-based form, thus ensuring compatibility with nonlinear decomposition. Simultaneously, the Retrace algorithm is introduced to replace the traditional TB and IS to reduce variance and improve learning stability.\n\nIn the experimental section, the authors validated the effectiveness of the algorithm on the SMAC benchmark in the discrete actions space and in the Predator-Prey environment in the continuous actions space. The results show that MCEM-NCD significantly outperforms existing state-of-the-art methods in both convergence speed and final performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper uses CEM to \"filter\" suboptimal actions to avoid centralized gradients, serving as an alternative to traditional centralized critic policy gradient methods and offering insights for large-scale agent tasks.\n\nThe combination of MCEM and NCD is seamless, preserving both the excellent expressive power of nonlinear decomposition and the high quality of updated data. Furthermore, the improvements to the heterogeneous policy critic learning part (Sarsa + Retrace) are well-considered, enhancing the algorithm's practicality and stability.\n\nEffective experimental validation was conducted on discrete and continuous control tasks, particularly demonstrating significant improvements in difficult and ultra-difficult SMAC scenarios. Comparisons with a series of representative state-of-the-art methods, such as DOP, FACMAC, VDAC-mix, and LICA, lend high credibility to the experimental results. Ablation studies clearly demonstrate the contribution of each component, strongly supporting the authors' design choices."}, "weaknesses": {"value": "W1: Theorem 5.1 and its proof in Appendix A form the core theoretical analysis of this paper. However, the proof is overly simplistic and intuitive, lacking rigorous mathematical form. It reads more like a description of the algorithm's design intent—to improve expected returns by selecting actions with high Q values—than a rigorous mathematical proof. Why does the first inequality $E_{\\pi_g}[Q_{\\pi_g}^{tot}(\\tau, u)] \\le E_{\\pi_\\rho}[Q_{\\pi_g}^{tot}(\\tau, u)] $ hold? Subsequent recursive expansions all rely on this condition. This significantly undermines the reliability of the conclusion that percentile-greedy strategies are at least as good as centered gradient strategies.\n\nW2: The FACMAC paper states that a centralized critic gradient is used to avoid the incoordination problem of individual gradient updates. This is a design choice and does not mean that individual $Q_i$ cannot be used as a critic to guide gradient updates. If the authors claim this is a fundamental drawback, they should design appropriate toy examples to illustrate this property, such as whether MCEM can solve the failure case in Figure 3 of FACMAC.\n\nW3: Using MCEM can partially alleviate the requirement for accuracy of the centralized critic through sampling, which is necessary for tasks with a larger number of agents, where it is often difficult to learn an effective centralized critic. However, as the number of agents k increases, the joint action space grows exponentially. Is it still possible to find effective \"elite\" joint actions with a small amount of sampling? If all sampled data is trapped in local optima or poor data, can the algorithm still guarantee certain performance? The paper lacks discussion on this scalability issue.\n\nW4: This article has many inaccuracies. It would be better to replace \"records\" with \"transitions\". In line 296, a parenthesis is missing. In line 300, it should be Fig. 1. In line 472, it should be \"smaller $\\rho$\". \n\nW5: GitHub links should be anonymous to comply with review guidelines."}, "questions": {"value": "Q1: The exploration is controlled by the proposal policies with entropy regularization. However, how to guarantee the consistency between proposal policies and main policies? If there exists multiple optimal modes in the proposal policies' samples, will it lead to mode collapse or to oscillate between multiple behavioral patterns, thus affecting the stability of learning?\n\nQ2: Can we directly use Q total to filter buffer data to obtain a higher quality centralized critic? Before Q total converges, how can we ensure that the data selected is accurately ranked by relative value? How does the algorithm prevent early-stage Q-function errors from causing catastrophic policy collapse?\n\nQ3: The predator-prey task has a very low dimension of state and action. Is it feasible to control the task in a higher dimension, such as MAMujoco?\n\nOther problems mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zwkn74B7VQ", "forum": "StIIArpUZ3", "replyto": "StIIArpUZ3", "signatures": ["ICLR.cc/2026/Conference/Submission6739/Reviewer_WPdG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6739/Reviewer_WPdG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983580664, "cdate": 1761983580664, "tmdate": 1762919023485, "mdate": 1762919023485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}