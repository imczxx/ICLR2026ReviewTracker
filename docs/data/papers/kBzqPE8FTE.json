{"id": "kBzqPE8FTE", "number": 2701, "cdate": 1757212497914, "mdate": 1763720418266, "content": {"title": "SCI-Verifier: Scientific Verifier with Thinking", "abstract": "As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct **SCI-VerifyBench**, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce **SCI-Verifier**, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains.", "tldr": "We introduce SCI-VerifyBench, a cross-disciplinary benchmark, and SCI-Verifier, a reasoning-augmented verifier, to provide systematic evaluation and reliable solutions for answer verification in scientific domains.", "keywords": ["LLM-as-a-judge; Large Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88fbe12267ac3a450583db02577b09275a974569.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a challenging benchmark for scientific verification that spans five distinct subjects and domains. In addition, the authors present a reasoning-based scientific verifier trained on carefully curated datasets, which substantially outperforms existing verification models. Interestingly, they highlight the inherent difficulty of equivalence judgment—an essential aspect of scientific evaluation. This work is well-written and solid."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is clear and easy to follow.\n\n2. The paper highlights the importance of equivalence transformations in answers, identifying them as a crucial and challenging aspect of verification. Once this issue is addressed, the results show remarkable improvement.\n\n3. This work is comprehensive and well-executed, covering data creation, benchmark construction, and verifier training—each of which involves substantial effort—and it achieves outstanding performance overall."}, "weaknesses": {"value": "1. There is a lack of error analysis for Sci-Verifier, making it difficult to understand what problems still remain.\n\n2. There is a lack of human evaluation. Although Sci-Verifier performs well on the benchmark, it is unclear how consistent its results are with human judgments."}, "questions": {"value": "1. Could you conduct an error analysis on the problems that the current verifier fails to handle well? I’d like to understand what major issues still exist in the current verification process.\n\n2. Could you sample some data and conduct a comparative analysis between human evaluations and the verifier’s results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RLHPTKF37y", "forum": "kBzqPE8FTE", "replyto": "kBzqPE8FTE", "signatures": ["ICLR.cc/2026/Conference/Submission2701/Reviewer_Hp78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2701/Reviewer_Hp78"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761165199759, "cdate": 1761165199759, "tmdate": 1762916335439, "mdate": 1762916335439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces (i) SCI‑VerifyBench, a cross‑disciplinary verification benchmark built from real LLM responses and domain‑specific equivalence transformations across Math, Physics, Chemistry, Biology, and general scientific QA, and (ii) SCI‑Verifier, a reasoning‑augmented verifier trained with SFT+RL that aims for concise, stable judgments. The test set comprises 2500 human‑verified items (500 per domain) drawn from 15k QA pairs and 100k+ LLM responses; the construction explicitly includes equivalence‑transformed answers to stress verifiers. On SCI‑VerifyBench, an 8B SCI‑Verifier matches or exceeds closed‑source models (e.g., near GPT‑5), and it also performs strongly on VerifierBench and VerifyBench‑Hard."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is good. The paper articulates why scientific verification is challenging (multi‑step reasoning, many equivalent forms...) and positions verification as foundational to evaluation and reliability. The narrative is easy to follow. \n\n- The scope is reasonable. SCI‑VerifyBench spans 5 disciplines, filters tasks, and injects equivalence transformations (e.g., unit/name conversions, algebraic rewrites, symbolic identities) to create realistic, hard cases, with a 2500‑item test set that requires unanimous human agreement (but I do have some concerns on this benchmark). \n\n- The authors add CoT to boost accuracy across models; the paper highlights this with Fig.1 (and ablations in Fig.5), and builds SCI‑Verifier around short CoT distilled via SFT and then refined with DAPO/GRPO‑style RL. \n\n- The performance seems promising. SCI‑Verifier‑8B achieves good performance on SCI‑VerifyBench compared with other models like GPT5. On equivalence‑only subsets, many LLMs—including GPT‑5—drop sharply (below 50% in Math/Physics), while SCI‑Verifier remains substantially higher. The authors also show that SCI‑Verifier is less sensitive to prompt changes than general models. Pls report statistical significance or CIs around headline numbers."}, "weaknesses": {"value": "(also questions for authors)\n\n- The pipeline generates 100k+ responses using eight models, and five LLMs assist in judging equivalences; only 2500 high‑disagreement samples (500/domain) are escalated to human annotation for the test set, while much of the training set relies on LLM labels (≈14k after filtering). Although test items require full human agreement, the selection by model disagreement may be biased toward tricky/ambiguous patterns, and the training labels inherit LLM biases.  Besides, regarding the annotation, please provide inter‑annotator agreement (IAA) or adjudication statistics are reported for the human‑labeled set (beyond the “full agreement” requirement) to gauge label consistency. The benchmark & data generation part is the core contribution in this paper, so this part is very important and must be carefully evaluated and justified.\n\n- What counts as “equivalent” is sometimes approximate, not strictly equal. Several transformation templates explicitly include approximations (e.g., Taylor expansions or “approx. 80°C”, descriptive lab phrases like “reflux”) and semantic paraphrases in QA. These are context‑dependent rather than strictly equivalent and could blur the task definition or inflate ambiguity.\n\n- Method novelty is limited. SCI‑Verifier uses a fairly standard two‑stage SFT + RL recipe with short‑CoT distillation and a DAPO/GRPO‑style objective plus a length penalty—sensible engineering, but incremental. The paper’s main novelty is task focus + dataset rather than a new learning principle.\n\n- Because the equivalence categories used to synthesize data appear in both *training and test pipelines*, there is a risk that models overfit to those templates rather than general principles of equivalence (?). The authors partly mitigate this with human curation for the test set, but a strict “held‑out transformation family” evaluation would strengthen the generalization claim."}, "questions": {"value": "- Human annotation quality. As I mentioned above, please report IAA and adjudication rates for the 2,500 test items; also, clarify annotator backgrounds and per‑domain expertise distributions in detail...\n\n- How do you formally distinguish strict equivalence from approximate/semantic equivalence? Would you consider splitting SCI‑VerifyBench into strict vs approximate tracks to avoid conflating them? \n\n- Could specialized verifiers be run with a CoT‑enabled prompt (when feasible) to check whether the gap persists?\n\n- Maybe I missed something. Do you have results where entire transformation families seen in training are held out in testing? \n\n- Please report wall‑clock latency and cost/token comparisons vs GPT‑5 and CompassVerifier on SCI‑VerifyBench."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "/"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Cbzzl3Yx58", "forum": "kBzqPE8FTE", "replyto": "kBzqPE8FTE", "signatures": ["ICLR.cc/2026/Conference/Submission2701/Reviewer_x8GA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2701/Reviewer_x8GA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500326970, "cdate": 1761500326970, "tmdate": 1762916335004, "mdate": 1762916335004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SCI-VerifyBench, a cross-disciplinary benchmark covering math, physics, biology, chemistry, and general scientific QA, and SCI-Verifier, a reasoning-augmented verifier. It addresses scientific verification challenges via data (high-quality benchmark) and model (two-stage post-training) solutions, outperforming baselines and matching GPT-5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. SCI-VerifyBench uses real LLM responses and domain-specific equivalence transformations, ensuring diverse, high-quality data across 5 disciplines, filling gaps in existing narrow-coverage benchmarks.\n2. SCI-Verifier integrates logical reasoning via SFT and RL, enabling accurate complex equivalence judgments and stable outputs, outperforming rule-based or prompt-dependent methods.\n3. It shows strong cross-disciplinary performance, matches closed-source GPT-5 with 8B parameters, and maintains accuracy under prompt variations, aiding reliable LLM use in science."}, "weaknesses": {"value": "1. I have doubts about the \"w/ thinking\" setup. RLVR enables large-scale RL training through a rule-based verifier, and specialized verifiers also need to minimize inference costs to prevent training from getting stuck during reward calculation. However, the paper does not quantify the inference efficiency of SCI-Verifier. It remains unclear whether the verifier in thinking mode can meet practical requirements during large-scale RL training.\n\n2. Continuing the previous question, in the process of calculating rewards in RL, what is the role of steps other than Final Judgment, such as Validity Check and Comparison of Final Answers? Are they directly ignored? I suggest the authors conduct small-scale RL experiments to demonstrate the advantages of SCI-Verifier during the training process (refer to [1]).\n\n3. It is recommended that the authors add more verifier cases under the thinking mode to facilitate a better understanding of the verification process of SCI-Verifier.\n\n4. There seems to be an inversion in the citations here: \"VerifyBench (Li et al., 2025) covers general, logical, mathematical reasoning, and VerifyBench (Yan et al., 2025) has 4,000 expert-level questions across STEM domains\". The correct citation should be: VerifyBench (Li et al., 2025) has 4,000 expert-level questions across STEM domains, and VerifyBench (Yan et al., 2025) covers general, logical, mathematical reasoning.\n\n[1] Huang Y, Zeng W, Zeng X, et al. Pitfalls of Rule-and Model-based Verifiers--A Case Study on Mathematical Reasoning[J]. arXiv preprint arXiv:2505.22203, 2025."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kpkkJuoQ5z", "forum": "kBzqPE8FTE", "replyto": "kBzqPE8FTE", "signatures": ["ICLR.cc/2026/Conference/Submission2701/Reviewer_9b7g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2701/Reviewer_9b7g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572624264, "cdate": 1761572624264, "tmdate": 1762916334861, "mdate": 1762916334861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SCI-VerifyBench, a cross-disciplinary benchmark covering math, physics, biology, chemistry, and general scientific QA, and SCI-Verifier, a reasoning-augmented verifier. It addresses scientific verification challenges via data (high-quality benchmark) and model (two-stage post-training) solutions, outperforming baselines and matching GPT-5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. SCI-VerifyBench uses real LLM responses and domain-specific equivalence transformations, ensuring diverse, high-quality data across 5 disciplines, filling gaps in existing narrow-coverage benchmarks.\n2. SCI-Verifier integrates logical reasoning via SFT and RL, enabling accurate complex equivalence judgments and stable outputs, outperforming rule-based or prompt-dependent methods.\n3. It shows strong cross-disciplinary performance, matches closed-source GPT-5 with 8B parameters, and maintains accuracy under prompt variations, aiding reliable LLM use in science."}, "weaknesses": {"value": "1. I have doubts about the \"w/ thinking\" setup. RLVR enables large-scale RL training through a rule-based verifier, and specialized verifiers also need to minimize inference costs to prevent training from getting stuck during reward calculation. However, the paper does not quantify the inference efficiency of SCI-Verifier. It remains unclear whether the verifier in thinking mode can meet practical requirements during large-scale RL training.\n\n2. Continuing the previous question, in the process of calculating rewards in RL, what is the role of steps other than Final Judgment, such as Validity Check and Comparison of Final Answers? Are they directly ignored? I suggest the authors conduct small-scale RL experiments to demonstrate the advantages of SCI-Verifier during the training process (refer to [1]).\n\n3. It is recommended that the authors add more verifier cases under the thinking mode to facilitate a better understanding of the verification process of SCI-Verifier.\n\n4. There seems to be an inversion in the citations here: \"VerifyBench (Li et al., 2025) covers general, logical, mathematical reasoning, and VerifyBench (Yan et al., 2025) has 4,000 expert-level questions across STEM domains\". The correct citation should be: VerifyBench (Li et al., 2025) has 4,000 expert-level questions across STEM domains, and VerifyBench (Yan et al., 2025) covers general, logical, mathematical reasoning.\n\n[1] Huang Y, Zeng W, Zeng X, et al. Pitfalls of Rule-and Model-based Verifiers--A Case Study on Mathematical Reasoning[J]. arXiv preprint arXiv:2505.22203, 2025."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kpkkJuoQ5z", "forum": "kBzqPE8FTE", "replyto": "kBzqPE8FTE", "signatures": ["ICLR.cc/2026/Conference/Submission2701/Reviewer_9b7g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2701/Reviewer_9b7g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572624264, "cdate": 1761572624264, "tmdate": 1763722790372, "mdate": 1763722790372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SCI‑VerifyBench, a cross‑disciplinary benchmark for verifying the correctness of LLM answers in scientific domains (math, physics, chemistry, biology, and general QA), and SCI‑Verifier, a reasoning‑augmented verifier trained via SFT followed by RL (DAPO/GRPO‑style). The benchmark mixes real LLM responses and synthetic, domain‑specific equivalence transformations. The model generates short chain‑of‑thought before issuing a Correct/Incorrect judgment. Experiments show SCI‑Verifier‑8B performs comparably to or better than large closed models on SCI‑VerifyBench and two external benchmarks (VerifierBench, VerifyBench‑Hard), and is robust to prompt variants.\n\nThe paper focuses on an important gap: reliable verification of scientific answers. The dataset construction is well‑documented, and the training recipe is clearly summarized. However, the definitions of equivalence transformations for several tasks appear loose and subjective, potentially giving label ambiguity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The motivation for reasoning in verification is clear. The authors show consistent gains when enabling CoT on two benchmarks, directly motivating a reasoning‑augmented verifier (Fig. 1 and 5).\n\n+ The cross‑disciplinary benchmark is valuable, and the idea of using targeted equivalence transformations is interesting. The benchmark fills the breadth gap compared to previous works, covering five domains with 500 samples per domain. Concrete equivalence transformation templates are provided (Fig. 2 and Appendix A). \n\n+ The experimental evaluations are comprehensive. The results on SCI-VerifyBench include both closed and open models with specific verifiers. Generalization to VerifierBench and VerifyBench‑Hard is also reported (Table 4). SCI‑Verifier surpasses or matches strong baselines across domains (Table 3), and excels on equivalence‑augmented tests where others degrade sharply (Fig. 3). This directly targets the paper’s core problem."}, "weaknesses": {"value": "- There may be selection bias in benchmark construction. The test set intentionally prioritizes model‑disagreement samples for human annotation (Sec. 3.2: “retain only samples where five LLMs disagree”), which may skew distribution toward hard/atypical cases. This complicates external validity. Furthermore, “samples with disagreement between human experts and LLMs are preferentially included” in the test set (Sec. 3.2), possibly amplifying adversarial difficulty beyond natural settings. Another potential issue is the extremely long average response length (2,980 tokens; Table 2), which suggests contexts unlike many real deployments, potentially overemphasizing long‑context quirks. \n\n- The definitions of equivalence transformation are loose or subjective in several domains. Chemistry prompts treat descriptors (high boiling point, room temperature, solvent class) as equivalent to numeric and speciﬁc answers, which is semantic similarity rather than strict equivalence. This risks label ambiguity. Biology prompts allow “conservative substitutions/synonymous sequences”, which need not preserve exact answers without explicit structural/functional constraints. QA prompts accept generic paraphrases, where scope/entailment nuances could flip correctness without careful normalization rules. \n\n- The evaluation may lack statistical rigor. Table 3 and 4 report single accuracies without confidence intervals or multi‑seed variance, which is especially important for RL‑trained models. Prompt‑robustness Table 5 presents point estimates only; no variability across runs or bootstrap CIs. Fig. 3 (p. 7) shows sharp drops on equivalence stress tests but omits per‑domain n/CIs, hindering significance assessment. \n\n- Some concerns:\n\nCaptions of Tables 3-4 specify: \"Specialized verifiers use default prompts, while all other models are allowed to use reasoning\". This design may disadvantage specialized verifiers that could also benefit from tailored prompts or concise CoT.\n\nAverage token counts differ drastically across models (e.g., many thousands for some reasoning LLMs vs 1.00 for specialized verifiers; Table 4), implying heterogeneous inference modes and costs that complicate fairness of accuracy‑only comparisons. \n\nWhile Sec. 5.1 references Appendix A.2 for prompt details, it is unclear that each baseline was evaluated under its best prompt setting, especially for specialized verifiers. No direct evidence found in the manuscript. \n\nThe alignment reward (R_{align,i}) (Eq. 5) is not defined operationally. \n\nThe policy ratio (r_{i,t}(\\theta)) in Eq. 3 is never explicitly defined.\n\nThe over‑long penalty uses (-\\infty) for responses exceeding (L_{\\max}+L_{buffer}) (Eq. 6), which is not implementable as‑is; practical handling (e.g., very large negative constants) is unspecified."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vFAwQXZZ7f", "forum": "kBzqPE8FTE", "replyto": "kBzqPE8FTE", "signatures": ["ICLR.cc/2026/Conference/Submission2701/Reviewer_w2ZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2701/Reviewer_w2ZX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613617823, "cdate": 1761613617823, "tmdate": 1762916334681, "mdate": 1762916334681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}