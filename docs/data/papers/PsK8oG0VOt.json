{"id": "PsK8oG0VOt", "number": 24506, "cdate": 1758357482735, "mdate": 1759896762510, "content": {"title": "Sample-Aware Dual Actions for Prompt Optimization", "abstract": "In recent years, large language models (LLMs) have achieved remarkable progress in reasoning, question answering, and decision-making tasks in natural language processing. High-quality prompts play a crucial role in guiding LLMs to generate outputs that meet expectations. However, manually designing effective prompts for specific tasks is often time-consuming and heavily reliant on expertise, limiting the scalability and efficiency of model applications. Consequently, automated prompt optimization has become an important direction for enhancing LLM performance. To address this, we propose a sample-aware dual actions Monte Carlo Tree Search (MCTS) framework for automated prompt optimization, enabling the search process to leverage sample performance for more effective optimization. This method not only efficiently utilizes training samples to guide prompt improvement but also directs the optimization trajectory based on the overall state of the training samples. We validate our framework on the Big-Bench Hard (BBH) and MMLU datasets, and experimental results demonstrate that it outperforms traditional prompt optimization methods and recent baselines in both accuracy and optimization stability.", "tldr": "", "keywords": ["LLM", "Prompt Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7986f24442cdb1521199e331383c5888881e9272.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new framework for automated prompt optimization for large language models, centered on a sample-aware dual-action Monte Carlo Tree Search (MCTS). The core idea is to abstract prompt optimization into two high-level strategies: “Failure-Aware Reflection” (learning from failures) and “Success-Aware Induction” (amplifying successes). The method introduces a dynamic sample pool to quantify and prioritize sample informativeness during the search, providing both local and global feedback to the MCTS planner. Experiments on BBH, MMLU, and BBEH benchmarks show improved accuracy and stability compared to several recent baselines and traditional prompt optimization techniques."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method is well-described, with mathematically formalized sample prioritization and conservative estimation using the Wilson confidence interval. The paper also provides detailed action-guiding prompts.\nThe dataset coverage is reasonably broad — including BBH (5 tasks), domain-specific datasets (5 tasks), and BBEH (4 tasks). Baselines are comprehensive, encompassing manually designed prompts, CoT variants, and PromptAgent, and the proposed method consistently achieves higher performance."}, "weaknesses": {"value": "1. Table 4 only compares three configurations — Beam+Pool, MCTS-only, and Full — without isolating the contribution of the dual-action mechanism (e.g., using only the reflection or induction action). The influence of different components in the sample pool (e.g., removing the difficulty term DiD_iDi​ or the gain term GiG_iGi​) is also not examined.\n2. The formulation involves multiple weight parameters (\\alpha, \\beta, \\gamma, \\theta, \\lambda, \\mu, \\delta), yet only a limited sensitivity analysis for \\lambdaλ and \\thetaθ is presented in Appendix A.1 — and solely on the Boolean Expressions task, where accuracy fluctuates between 0.375 and 0.55.\n3. The authors acknowledge conceptual similarity with PromptAgent but claim two key contributions:\n(1) formalizing a dual-action paradigm beyond simple failure reflection, and\n(2) introducing a mathematically defined sample-pool mechanism.\nHowever, the distinction remains questionable — PromptAgent already integrates MCTS and reflective reasoning. Whether the proposed “successful induction” action constitutes a substantive innovation requires stronger theoretical justification or experimental evidence. As mentioned above, additional ablations could help clarify this point. Further comparative analysis against PromptAgent would also strengthen the contribution claim.\n4. Appendix A.5 provides a theoretical formulation, but no empirical data (e.g., API call counts or runtime) are reported. It would be valuable to include such measurements and conduct a cost-performance comparison with PromptAgent (e.g., “Which method achieves higher accuracy under the same computational budget?”).\n5. There appear to be counterexamples or contradictory cases within the Casual Judge results. Clarification or discussion would be appreciated."}, "questions": {"value": "see weeknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vWYpidP2Ya", "forum": "PsK8oG0VOt", "replyto": "PsK8oG0VOt", "signatures": ["ICLR.cc/2026/Conference/Submission24506/Reviewer_W5wf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24506/Reviewer_W5wf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931043610, "cdate": 1761931043610, "tmdate": 1762943106457, "mdate": 1762943106457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of existing automated prompt optimization methods, which overlook sample heterogeneity. The authors propose a sample-aware dual-action Monte Carlo Tree Search (MCTS) framework that integrates two core components: (1) two optimization actions including failure-aware reflection and success-aware induction; (2) a dynamic sample pool where the sample informativeness is measured by difficulty, recent gains, and variance, and sample priority and overall quality of the sample pool are also quantified to guide the optimization. The exploration-exploitation balance in MCTS is adaptively adjusted based on the sample pool’s composition to enable efficient resource allocation. Experiments on reasoning benchmarks (BBH, BBEH) and domain-specific tasks (MedQA, CaseHold) show that the framework outperforms baselines like PromptAgent (e.g., 3.9% higher accuracy on BBH, 5% on BBEH)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**\nThis work points out the problem that prior methods (e.g., PromptAgent) that treat samples uniformly and use handcrafted fine-grained actions, and introduces two high-level actions (Reflection/Induction) and a informativeness-quantified sample pool for the MCTS policy optimization, which utilize the sample heterogeneity property in prompt optimization.  \n\n**Clarity**\nThis paper is clearly motivated with a standard structure including introduction, methodology, experiment, conclusion, and appendix. The high-level idea is sound, and the main components in methodology strictly relate to the sample heterogeneity issues. Experiments cover diverse scenarios—general reasoning (BBH), domain expertise (MedQA/CaseHold), and high difficulty (BBEH)—validating generalization. \n\n**Significance**\nSince PromptAgent formalized prompt optimization as policy search, this work extends the idea to consider the sample heterogeneity and improves the performance in general-domain and domain-specific tasks, supporting LLM deployment in professional fields."}, "weaknesses": {"value": "1. No indexing for all the equations in the manuscript. No definition of $b_i$ in Line 192, and not sure whether the definition of $b_i$ in L255 means the same thing.\n\n2. In equations in Line 192, Line 257, Line 287, mathematical formulas that include mixed texts make the technical definitions informal.  \n\n3. No definition of $H(c_{pool})$  in Line 257.  \n\n4. The axis texts in Figure 2 are overlapping with each other. \n\n5. The authors provide theoretical analysis on API call cost in Appendix A.5, but no empirical statistics of API cost and computational cost during the prompt optimization. Furthermore, how many API calls does the framework require to achieve the same amount of accuracy gain compared to PromptAgent? \n\n6. The paper briefly mentions parameter robustness in the appendix A.1 but does not analyze how these parameters affect performance across tasks, e.g., do domain-specific tasks require different $\\lambda$, $\\beta$ ratios than general reasoning tasks?\n\n7. Experiments use only DeepSeek-R1 for optimization and Qwen-Flash for evaluation. It remains unclear if the framework performs consistently on other model pairs (e.g., GPT-4o as optimizer, Llama-3 as evaluator), weakening the confidence in cross-model generalization."}, "questions": {"value": "Several questions and concerns are raised in \"Weaknesses\" part. I would be willing to change my recommendation according to the authors' response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JQ4UlGa4Uo", "forum": "PsK8oG0VOt", "replyto": "PsK8oG0VOt", "signatures": ["ICLR.cc/2026/Conference/Submission24506/Reviewer_5QeX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24506/Reviewer_5QeX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967049409, "cdate": 1761967049409, "tmdate": 1762943106083, "mdate": 1762943106083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a sample-aware dual-action prompt optimization framework that abstracts prompt optimization into two complementary strategies through Monte Carlo Tree Search (MCTS): Failure-Aware Reflection (adjusting prompts for low-reward samples) and Success-Aware Induction (extracting effective patterns from high-reward samples). The framework introduces a dynamic sample pool that quantifies sample information value based on difficulty, recent gains, and variance to guide the search process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a dual-action framework (Failure-Aware Reflection and Success-Aware Induction), transcending the limitations of existing methods that treat all samples equally.\n- The framework designs a dynamic sample pool mechanism to quantify sample information value through three dimensions: Difficulty, Recent Gains, and Variance.\n- The approach implements an adaptive exploration-exploitation balance mechanism to dynamically adjust MCTS search intensity.\n- Ablation studies validate the contributions of each component, and parameter sensitivity analysis provides practical guidance."}, "weaknesses": {"value": "- The MCTS method requires numerous API calls, resulting in high practical application costs, with no computational complexity analysis or simplification strategies provided\n- The selection rationale for weight parameters (α,β,γ) in sample metrics is not detailed, and sensitivity of these parameters across different tasks is not analyzed\n- There is a lack of theoretical justification for the necessity of the dual-action strategy, and no analysis of how the sample-aware mechanism affects optimization convergence"}, "questions": {"value": "- What is the rationale for selecting these three specific metrics: Difficulty, Recent Gains, and Variance? How are weights determined in the informative score? Are the same weights used for all tasks?\n\n- Are there specific strategies to reduce the number of API calls? The paper mentions \"early-stopping mechanisms\" but does not elaborate.\n\n- Can you provide a performance-efficiency trade-off analysis for different MCTS configurations (depth, width, number of rollouts)?\n\n- How does the zero-shot generalization ability of optimized prompts perform on similar but unseen tasks?\n\n- How consistent is the framework's performance across LLMs of different scales and architectures?\n\n- When the sample pool simultaneously contains numerous difficult samples and high-value successful samples, how are the two actions balanced?\n\n- Beyond accuracy, are other metrics considered such as inference cost, prompt length, robustness, etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HRHC94kxoF", "forum": "PsK8oG0VOt", "replyto": "PsK8oG0VOt", "signatures": ["ICLR.cc/2026/Conference/Submission24506/Reviewer_dsjq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24506/Reviewer_dsjq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139776887, "cdate": 1762139776887, "tmdate": 1762943105780, "mdate": 1762943105780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a sample-aware dual action Monte Carlo Tree Search (MCTS) framework for automated prompt optimization, aiming to enhance search efficiency. The proposed method leverages the overall state of the training samples to guide prompt improvement. Specifically, the proposed method injects two strategies into PromptAgent: inductive actions and reflective actions. The experimental evaluation on the Big-Bench Hard (BBH) and MMLU datasets demonstrates that the proposed method outperforms existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method integrates sample-aware learning approaches into MCTS for prompt optimization.\n- The experimental results show that the proposed method outperforms existing methods such as PromptAgent."}, "weaknesses": {"value": "- In the experiment, only the PromptAgent is considered as the baseline method. It would be better if the authors could compare the proposed method with more baseline methods for prompt optimization.\n- The concrete algorithm of the proposed method is somewhat unclear. A pseudocode or detailed description would help in understanding the approach better.\n- The cost analysis (computational cost or API calls) is missing. It is unclear whether the proposed method is truly efficient compared to the baselines and existing methods.\n- Only one LLM combination (DeepSeek-R1 and Qwen-Flash) is examined. It would be better if the authors could evaluate the proposed method with different LLMs to demonstrate its generality."}, "questions": {"value": "- The proposed method introduces many hyperparameters (e.g., alpha, beta, kappa, etc.). How are these hyperparameters set and tuned in the experiments? How sensitive is the performance of the proposed method to these hyperparameters?\n- Why did the authors choose the five tasks in BBH for evaluation? How is the performance on other tasks? In the literature of PromptAgent, other tasks in the BBH dataset are also used for evaluation.\n- Is it possible to inject the proposed idea, the failure-aware reflection and success-aware induction, into other prompt optimization methods beyond PromptAgent (MCTS-based methods)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nzIFwCBYVU", "forum": "PsK8oG0VOt", "replyto": "PsK8oG0VOt", "signatures": ["ICLR.cc/2026/Conference/Submission24506/Reviewer_qAZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24506/Reviewer_qAZP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762700204593, "cdate": 1762700204593, "tmdate": 1762943105557, "mdate": 1762943105557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}