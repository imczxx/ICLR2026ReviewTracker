{"id": "8pkJD7KqIE", "number": 13054, "cdate": 1758213119870, "mdate": 1759897468622, "content": {"title": "Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline", "abstract": "Large Vision-Language Models (VLMs) now generate highly detailed, paragraph-length image captions, yet evaluating their factual accuracy remains challenging. Current methods often miss fine-grained errors, being designed for shorter texts or lacking datasets with verified inaccuracies. We introduce DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100 images, 14 VLMs) featuring over 10,216 sentence-level human annotations of factual correctness and explanatory rationales for errors, all within paragraph context. Building on this, we develop VNLI-Critique, a model for automated sentence-level factuality classification and critique generation. We highlight three key applications: (1) VNLI-Critique achieves state-of-the-art results on external M-HalDetect and CHOCOLATE claim verification datasets, showing strong generalization. (2) On our benchmark, the VNLI-Critique-powered DOCCI-Critique AutoRater's rankings highly correlate with human judgments (e.g., 0.98 Spearman for factuality). (3) A novel Critic-and-Revise pipeline, where VNLI-Critique's critiques guide an LLM to correct caption errors, significantly boosts factuality (e.g., 46% gain on DetailCaps-4870). Our work offers a crucial benchmark and tools to advance fine-grained evaluation and improvement of VLM-based image understanding.", "tldr": "", "keywords": ["VLM", "Image Understanding", "Image Captioning", "Vision Language Models", "Image Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d991b33a98fb0d7c1a61a753db20e111813acb9.pdf", "supplementary_material": "/attachment/3c918753758b778802c18c56fba112a876b74b4f.pdf"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of evaluating factual accuracy in detailed, paragraph-length image captions generated by large vision-language models (VLMs). The authors introduce DOCCI-Critique, a benchmark dataset built on 100 images from the DOCCI dataset, comprising 1,400 captions from 14 VLMs, with over 10,216 sentence-level human annotations for factuality, including explanatory rationales for errors. They then develop VNLI-Critique, a fine-tuned PaliGemma-2 model (10B parameters) that performs sentence-level factuality classification and generates critiques explaining inaccuracies, incorporating paragraph context to resolve ambiguities. The paper highlights three applications: (1) generalization to external benchmarks like M-HalDetect and CHOCOLATE, where VNLI-Critique achieves strong performance (e.g., 0.76 Macro-F1 on M-HalDetect); (2) an AutoRater for ranking VLMs on DOCCI-Critique, showing high correlation with human judgments (e.g., 0.98 Spearman); and (3) a Critic-and-Revise pipeline where VNLI-Critique's critiques guide an LLM (e.g., GPT-4o) to correct erroneous sentences, yielding significant factuality improvements (e.g., 46% on DetailCaps-4870). Overall, the work aims to advance fine-grained evaluation and correction of VLM hallucinations in detailed captions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The primary strength of this work is the DOCCI-Critique benchmark, which collects detailed captions from 14 diverse VLMs and gathers textual rationales for all identified errors.\n- VNLI-Critique shows strong model performance and demonstrates generalizability to other benchmarks.\n- The paper is well-written, and the problem is clearly motivated."}, "weaknesses": {"value": "- A key weakness is the novelty of the \"Critic-and-Revise\" pipeline. While the performance is strong, the idea of using a model to critique and then revise its own or another model's output is a well-established paradigm in recent LLM/VLM literature (often termed self-correction and self-improvement). The paper should compare or at least state the difference between such work [1][2][3].\n- The proposed benchmark is relatively small with only 100 unique images. The size may not cover diverse scenarios.\n- The proprietary models evaluated are not the state-of-the-art models (gpt-4o, gemini-2.0). It would be better to evaluate on the stronger models like gemini-2.5-pro or gpt-5 to better contextualize the results.\n- The author can elaborate more on the proposed method. The paper states that VNLI-Critique was not trained on DOCCI-Critique, but on a separate, specialized dataset generated from 70+ PaliGemma-2 variants. However, the details of this training set are not as clearly presented in the paper as the details for DOCCI-Critique. \n\n\n[1] Wu et al., 2025. VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning.\n\n[2] Yu et al., 2024. Attention prompting on image for large vision-language models.\n\n[3] Prabhakaran et al., 2025. VADE: Visual Attention Guided Hallucination Detection and Elimination"}, "questions": {"value": "See above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wMp4ntXyaV", "forum": "8pkJD7KqIE", "replyto": "8pkJD7KqIE", "signatures": ["ICLR.cc/2026/Conference/Submission13054/Reviewer_h76k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13054/Reviewer_h76k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511803872, "cdate": 1761511803872, "tmdate": 1762923786158, "mdate": 1762923786158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of evaluating the fine-grained factual accuracy of long, paragraph-level captions produced by modern Large Vision-Language Models (VLMs), where existing benchmarks and metrics fail to detect subtle or sentence-level errors. The authors introduce DOCCI-Critique, a new benchmark containing 1,400 VLM-generated detailed captions paired with 10,216 sentence-level human annotations that include both factuality labels and rich error rationales. Building on this dataset, they develop VNLI-Critique, a model capable of sentence-level factuality classification and critique generation, showing strong generalization on external benchmarks like M-HalDetect and CHOCOLATE. Leveraging this capability, they further propose a Critic-and-Revise pipeline, where VNLI-Critique identifies and explains errors and a separate LLM revises them, significantly improving caption factuality, even on large synthetic datasets such as PixelProse and DetailCaps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work provides a richly annotated, sentence-level factuality dataset that fills a clear gap in evaluating long VLM-generated captions.\n- The VNLI-Critique model offers a scalable, generalizable evaluator that performs well across multiple external benchmarks.\n- The Critic-and-Revise framework delivers meaningful factuality improvements, demonstrating real downstream utility."}, "weaknesses": {"value": "- Because both the critic model and the revise pipeline rely heavily on DOCCI-style annotations, there is a possibility that the system implicitly learns annotator-specific bias rather than true general fine-grained factuality.\n- Sentence-level evaluation may miss cross-sentence logical dependencies or global coherence errors, potentially encouraging overly localized correction strategies that do not improve holistic caption quality.\n- Lack of necessary case studies and error analysis to intuitively illustrate the success and failure modes of the proposed work."}, "questions": {"value": "- How does the critic-and-revise pipeline behave when sentence-level errors arise from global inconsistencies rather than local factual mistakes?\n- Does the method risk over-editing or introducing new errors when the critic misclassifies a factual sentence as incorrect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "27B8dECcTW", "forum": "8pkJD7KqIE", "replyto": "8pkJD7KqIE", "signatures": ["ICLR.cc/2026/Conference/Submission13054/Reviewer_huiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13054/Reviewer_huiS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723330419, "cdate": 1761723330419, "tmdate": 1762923785764, "mdate": 1762923785764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DOCCI-Critique, a benchmark dataset comprising 1400 VLM-generated captions for 100 images and over 10,216 sentence-level human annotations. The authors finetune a pre-trained 10B VLM on a training set generated using the same process as DOCCI-Critique. The experiments show that the finetuned model can achieve better results in measuring the precision of given image captions compared to other baseline models, such as Gemini-2.0-Flash and GPT-4o. They also show that the critic-and-revise pipeline, revisiting each sentence within an image caption and revising it using an LLM, can improve the precision of image captions by a large margin."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is clear and well-organized overall.\n2. I'd like to thank the authors for their labor-intensive, human-involved benchmark construction."}, "weaknesses": {"value": "1. Several prior studies have proposed caption revision models to enhance image caption quality [1,2]. However, the paper does not clearly articulate how its contributions go beyond these existing approaches.\n2. The proposed critic-and-revise pipeline appears to be highly similar to the method introduced in a recent study [3]. A detailed comparison with this work is necessary to clarify the novelty of the proposed approach.\n\n[1] Zhou et al., \"ANALYZING AND MITIGATING OBJECT HALLUCINATION IN LARGE VISION-LANGUAGE MODELS\", ICLR 2024  \n[2] Lee et al., \"VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision\" NAACL 2024  \n[3] Lee et al., \"Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage\" ICML 2025"}, "questions": {"value": "What specific contributions does this work make beyond the existing studies mentioned above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KYuSyG1E8g", "forum": "8pkJD7KqIE", "replyto": "8pkJD7KqIE", "signatures": ["ICLR.cc/2026/Conference/Submission13054/Reviewer_wMNu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13054/Reviewer_wMNu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888830982, "cdate": 1761888830982, "tmdate": 1762923785380, "mdate": 1762923785380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Previous approaches to evaluating the quality of generated image captions have largely relied on model-based alignment metrics—such as CLIPScore—which primarily assess overall image–text similarity rather than fine-grained factual accuracy. As a result, these methods struggle to identify which specific sentences or details in a caption are factually incorrect.\nThis paper introduces a novel benchmark (DOCCI-Critique), a sentence-level factuality model (VNLI-Critique), and an integrated Critic-and-Revise pipeline. Together, these contributions enable explainable and fine-grained evaluation of detailed image captions and further foster improvements in Vision-Language Model (VLM) image understanding and factual accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and logically structured, with a well-motivated problem statement and a thorough discussion of limitations in prior research. The proposed benchmark, model, and pipeline are novel and supported by strong empirical evidence.\n\n- Benchmark Design (DOCCI-Critique):\nThe authors present DOCCI-Critique, a carefully curated benchmark consisting of 1,400 captions generated by diverse VLMs and 10,216 sentence-level human annotations. Unlike prior VLM factuality benchmarks that focus on short or isolated descriptions, DOCCI-Critique provides context-aware, sentence-level factuality annotations for paragraph-length image descriptions. A key differentiator is the inclusion of fully human-written explanatory rationales for factual errors, enabling in-depth analyses of VLM behavior and offering a grounded reference for alignment with human factuality judgments.\n\n- Automated Evaluation Model (VNLI-Critique):\nThe proposed VNLI-Critique model performs both sentence-level factuality classification and explanatory critique generation, effectively identifying discrepancies and articulating their causes. Despite being a 10B-parameter open-source model, it achieves performance comparable to large-scale commercial models across multiple benchmarks. Moreover, the model demonstrates strong generalization beyond the in-domain DOCCI dataset, achieving competitive results on external and unseen datasets such as M-HalDetect and CHOCOLATE.\n\n- Critic-and-Revise Pipeline:\nBuilding upon VNLI-Critique, the authors design a Critic-and-Revise pipeline that integrates critique-driven sentence correction via a revision LLM. This framework establishes a new protocol for fine-grained, interpretable evaluation and correction of VLM-generated captions. Empirical results show that the pipeline substantially improves the factual accuracy of captions, validating its effectiveness as both an evaluation and correction methodology."}, "weaknesses": {"value": "While the paper is overall well-executed, several aspects could be strengthened to enhance completeness and reproducibility.\n\n- Dataset Scale and Annotation Cost:\nAs acknowledged by the authors, the limited sample size of DOCCI-Critique constrains its statistical robustness. The benchmark contains 10,216 sentence-level annotations, each requiring multi-rater validation and textual rationales. Given that generating these critiques demands human intervention per caption (as illustrated in Table 1 and Appendix C), the data collection pipeline incurs substantial human labor costs. It would be valuable for the authors to discuss the feasibility of automating rationale extraction—for instance, leveraging reliable MLLMs trained on similar factuality explanation tasks—and to clarify why such an approach may not yet yield satisfactory results. A quantitative or qualitative comparison between human-written and MLLM-generated rationales could make this limitation more convincing.\n\n- Performance Gap on CHOCOLATE Dataset:\nThe paper shows that VNLI-Critique achieves SOTA results on M-HalDetect but lower performance on CHOCOLATE. Since CHOCOLATE mainly consists of chart- and plot-based visual reasoning tasks, this result suggests that the proposed critique model—optimized for naturalistic, object-centric imagery—may have limited transferability to abstract or structured visual domains. The paper would benefit from a deeper analysis of this discrepancy, explaining which aspects of the critique mechanism fail to generalize.\n\n- Documentation of External Datasets:\nThe paper briefly mentions external benchmarks such as M-HalDetect and CHOCOLATE, but provides limited details on their structure or usage. Including short summaries in the appendix—covering dataset scope and task formulation—would slightly improve clarity and reproducibility, though this is a minor issue overall."}, "questions": {"value": "I would appreciate the authors’ clarification on the points raised in the Weaknesses section.\nIn addition, I would like to ask the following specific questions and would appreciate detailed responses:\n\n- In the sentence-level annotations, sentences judged as factually correct do not appear to include any additional textual human rationales. Would providing further critiques or enhancement suggestions for such sentences—aimed at improving them toward ground-truth human caption quality—offer benefits in terms of expressiveness or linguistic diversity?\n\n- The authors fine-tuned PaliGemma-2 for developing the VNLI-Critique model. How would the performance differ if other detailed-captioning models (e.g., Qwen2.5-VL-7B, as reported in Table 4) were fine-tuned under the same framework? In other words, is PaliGemma-2 empirically the optimal architecture for this task, or is the proposed approach model-agnostic and transferable to other multimodal backbones?\n\n- Could the authors elaborate on the prompt design used for the revision LLM in the Critic-and-Revise pipeline? Understanding the structure and rationale of this prompt would clarify how the critique outputs are transformed into factual sentence revisions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FkIsNwLNzH", "forum": "8pkJD7KqIE", "replyto": "8pkJD7KqIE", "signatures": ["ICLR.cc/2026/Conference/Submission13054/Reviewer_LheA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13054/Reviewer_LheA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932034748, "cdate": 1761932034748, "tmdate": 1762923785023, "mdate": 1762923785023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}