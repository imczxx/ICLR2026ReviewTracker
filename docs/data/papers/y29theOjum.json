{"id": "y29theOjum", "number": 22837, "cdate": 1758336074946, "mdate": 1759896843641, "content": {"title": "Exploring the Entropy Mechanism in LLM Agents On-policy Optimization", "abstract": "Reinforcement Learning has emerged as a critical technique for training Large Language Model (LLM) agents in multi-turn interactive environments. However, sparse reward structures in long-horizon tasks‚Äîwhere agents receive feedback only after completing episodes spanning 30+ turns‚Äîcreate fundamental training instabilities that existing entropy regularization methods fail to address. We identify a critical oscillation pattern: early steps rapidly converge to low-entropy states due to limited feedback signals, triggering compensatory entropy spikes in later steps that compromise task completion. To address this exploration-exploitation dilemma, we propose Entropy-regularized Policy Optimization (EPO), a novel framework designed for stable on-policy training in multi-turn settings. EPO introduces two key innovations: (1) an entropy smoothing regularizer that mitigates abrupt entropy fluctuations across extended trajectories, and (2) an adaptive weighting scheme that dynamically balances exploration and exploitation based on training progression. We provide theoretical analysis demonstrating how entropy regularization prevents premature convergence in sparse reward environments. Extensive experiments on ScienceWorld and ALFWorld benchmarks show that EPO achieves up to 152\\% performance improvement over baseline methods while exhibiting significantly more stable training dynamics. Our results validate that properly designed entropy mechanisms are essential for successful multi-turn LLM agent optimization.", "tldr": "", "keywords": ["Reinforcement Learning; Large Language Models; LLM Agents"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/defa89c858bcf2124e74480772b1b411ce3beaa9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores instability in large language model (LLM) agent training under multi-turn sparse-reward environments and identifies a phenomenon called the exploration‚Äìexploitation cascade failure. To address it, the authors propose Entropy-regularized Policy Optimization (EPO), integrating: **trajectory-level entropy regularization**, **historical-anchored entropy-smoothing penalty**, and **an adaptive coefficient schedule $\\beta_k$**. Theoretical analysis claims an improved performance bound and monotonic entropy-variance reduction. Experiments on ScienceWorld and ALFWorld report up to +152% improvement over PPO and GRPO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novel diagnosis:** Identifies a two-phase failure (early blind exploration ‚Üí late uncertainty propagation), advancing understanding of entropy dynamics in long-horizon on-policy training.\n\n**Methodological integration:** Combines entropy control and adaptive scheduling into a coherent framework compatible with PPO-style optimization. Empirical performance: Achieves consistent improvements on two complex environments.\n\n**Readable and structured:** Algorithm and ablation setups are clear and reproducible."}, "weaknesses": {"value": "**Theoretical Rigor:**\n1. Proposition 5 introduces $|\\mathcal{ùê∑}|$ and $\\mathcal{C}_{\\lambda,\\beta}$, but only describes them as ‚Äúproblem-dependent.‚Äù Their mathematical meanings and relations to observable quantities are never formalized. Appendix B lists numerical values, but these correspond to hyperparameters rather than defined constants in the theorem. \n2. Eq.(5) uses a piecewise-constant indicator function that is non-differentiable at entropy bounds $\\(\\kappa_{l}\\bar{H},\\kappa_{r}\\bar{H}\\)$, contradicting the smoothness assumption required for the Lipschitz continuity later claimed. \n3. The bound assumes $\\|\\nabla V_{\\lambda,\\beta}^{\\pi_\\theta}(s_0)\\|\\leq\\epsilon$, but under softmax parameterization with large action spaces, minimal action probabilities can approach zero, making Lipschitz constants unbounded.\n4. The ‚Äúentropy-smoothing gap‚Äù compares expectations under $\\pi_Œ∏$ and $\\pi*$, whose distributions differ; without importance weighting or proximity assumption, the comparison is mathematically\ninconsistent.\n5. The term $\\frac{1}{2\\lambda}\\frac{|\\mathcal{D}|^2}{\\mathcal{C}^{\\pi_\\theta}_{\\lambda}(s_0)}\\epsilon^2$ mixes data-scale and regularization constants; the lack of normalization or scaling clarification undermines interpretability.\n\n**Methodological Incompleteness:**\n1. The method defines $W_k$ = $\\bar{H}, ...,\\bar{H}_{k-1} $  as a cumulative mean window rather than a fixed-length sliding window. This induces long-memory bias and may delay adaptation.\n2. The monotonic entropy-variance reduction is asserted but not empirically demonstrated, and the appendix does not specify which lemma supports it.\n\n**Presentation and Symbol Consistency:**\n\nThe paper does not specify whether $\\Phi < 0$ implies degradation or if $\\beta_k$ dynamically adjusts to prevent it."}, "questions": {"value": "1. How are  $|\\mathcal{ùê∑}|$ and $\\mathcal{C}_{\\lambda,\\beta}$ estimated in practice?\n2. Is the indicator penalty in Eq.(5) replaced by a smooth surrogate during implementation?\n3. What is the cumulative averaging behavior of $ùëä_ùëò$?\n4. Have you empirically verified monotonic entropy-variance decrease? \n5. Under what conditions does EPO fail or degrade?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7aWFYWo4Kd", "forum": "y29theOjum", "replyto": "y29theOjum", "signatures": ["ICLR.cc/2026/Conference/Submission22837/Reviewer_rz4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22837/Reviewer_rz4m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712525374, "cdate": 1761712525374, "tmdate": 1762942406339, "mdate": 1762942406339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the instability of exploration in long-horizon episodic decision-making tasks, arguing that entropy in policy learning may fluctuate excessively, causing cascading failures. To address this, the authors introduce Episodic Policy Optimization (EPO): a method that (i) computes average entropy across trajectories, (ii) introduces a smoothing regularizer based on historical entropy statistics, and (iii) adjusts exploration weight Œ≤ dynamically. Experiments are conducted on several RL benchmarks and code is released."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear writing & smooth logical flow\n\n    The paper is overall well structured, and the motivation‚Äìmethod‚Äìexperiment organization is consistent.\n\n2. Code availability\n\n    The authors provide open-source code, which is always beneficial for community verification and reproducibility."}, "weaknesses": {"value": "1. Limited novelty: it is essentially a technical paper. The proposed method largely stitches together standard RL tricks (entropy regularization, episodic averaging, heuristic weighting) rather than presenting a fundamentally new idea. The contribution is incremental engineering at best. There is no principled theoretical insight bridging entropy dynamics and long-horizon failures beyond high-level motivation.\n\n2. Equation (6) has zero gradient: which means that the  core method does not work mathematically.\nEquation (6) uses a hard threshold indicator penalty\n$P_{n,t,i} \\in {(0,\\alpha)}$, which is piecewise constant w.r.t. entropy ùêª. Therefore, almost everywhere zero gradient w.r.t. policy parameters $\\theta$. This means the core contribution has no optimization effect and all claimed behavior improvement must come from other terms. Thus the formal objective contradicts the claimed mechanism. This is a fundamental and fatal flaw.\n\n3. $Œ≤$ design is purely heuristic: the dynamic \nùõΩ\nschedule is described in narrative form without theoretical grounding nor stability analysis. The schedule resembles a manually tuned hyperparameter annealing, lacking justification or ablation isolating its individual effect.\n\n4. Section 4.1 method has no novelty: the episodic entropy computation in Sec.4.1 is simply taking an average over timesteps instead of per-step entropy. This is a trivial formulation change that should be introduced in preliminaries, not a methodological innovation.\n\n\n5. Paper‚Äôs core assumption is unjustified: \nThe supposed phenomenon ‚Äî entropy fluctuation directly causing cascading failures in long-horizon RL ‚Äî is asserted without rigorous empirical evidence or theoretical reasoning. The link between entropy and cascade failure remains speculative: there is no causal analysis,\nno stress test isolating entropy as the factor and no ablation proving removing fluctuations solves the problem.\nThe claim reads opinion-driven, not science-driven.\n\n6. Evaluation does not demonstrate the claimed benefit:\nThe paper states solving ‚Äúcatastrophic cascade failure‚Äù, but experiments are just on standard benchmarks, not cascade-failure-specific settings.\nThus the problem the paper aims to solve is not convincingly demonstrated."}, "questions": {"value": "Please see weaknesses above. More analysis and experiments are required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yRbIlX2Onj", "forum": "y29theOjum", "replyto": "y29theOjum", "signatures": ["ICLR.cc/2026/Conference/Submission22837/Reviewer_nuP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22837/Reviewer_nuP9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822413028, "cdate": 1761822413028, "tmdate": 1762942406152, "mdate": 1762942406152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional entropy regularization methods in reinforcement learning, even when adapted for LLMs, are fundamentally unsuited for multi-turn agent environments because they trigger an exploration-exploitation cascade failure. To address this, the authors propose Entropy-regularized Policy Optimization (EPO), a novel framework that introduces temporal awareness by anchoring the policy's entropy to a dynamically adjusted historical bound, which provides the stability needed to halt the cascade failure while still maintaining essential exploration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This works introduces Entropy-regularized Policy Optimization (EPO) as a direct solution to this cascade failure. The work's central innovation is identifying that standard methods lack \"temporal awareness.\" Its solution is built around this insight. EPO introduces a concrete mechanism‚Äîanchoring the policy's entropy to a dynamically adjusted historical bound‚Äîto provide stability and control cross-step entropy propagation."}, "weaknesses": {"value": "1. The exploration-exploitation dilemma is a challenging problem in reinforcement learning. To address exploration-exploitation failures in multi-turn interactions, the authors adapt entropy regularization. Instead of calculating entropy per step (as in traditional RL), they compute it over the entire trajectory (across all turns) and then average these trajectory entropies across the batch. This method aims to better capture the long-term, compounding effects of early decisions. However, from the definition of the entropy-regularized policy loss, the policy loss is designed to reduce the bias between the real entropy and the expected entropy. The problem is whether the expected entropy will converge to a single value or a fixed range, preventing further exploration.\n\n2. The authors claim that entropy-controlled LLM methods are fundamentally inadequate for multi-turn agent environments because they suffer from an \"exploration-exploitation cascade failure.\" Sparse rewards and standard entropy regularization cause uncontrolled entropy growth early in the trajectory, leading to unstable and suboptimal decision patterns. The uncertainty accumulated from these flawed early steps then compounds, destabilizing the agent's behavior in later turns. It is curious why early-stage entropy exploration leads to accumulated uncertainty. Can the accumulated uncertainty be measured? Will late-stage entropy increase the accumulated uncertainty? Whether the early high entropy is caused by immature agent policies?"}, "questions": {"value": "Please see the Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dZByQyU4sb", "forum": "y29theOjum", "replyto": "y29theOjum", "signatures": ["ICLR.cc/2026/Conference/Submission22837/Reviewer_U8i5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22837/Reviewer_U8i5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838508484, "cdate": 1761838508484, "tmdate": 1762942405929, "mdate": 1762942405929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the phenomenon of exploration-exploitation cascade in RL for training LLMs: policy converges at an early state prematurely, and standard PPO/GRPO-style training first lets early steps‚Äô entropy blow up, and then this early chaos ‚Äúpropagates‚Äù to later steps, so the policy never settles into a coherent strategy even though entropy regularization is present. To handle this, the authors propose Entropy-regularized Policy Optimization (EPO), which has three pieces: (i) compute entropy trajectory-wise (across all turns) rather than per-step; (ii) add an entropy smoothing regularizer that pulls current entropy into a band around a historical moving average; and (iii) use an adaptive, phase-based weight so exploration is conservative at the start, balanced in the middle, and stabilized at the end."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well-organized and clearly written. The paper has clean and good illustrations for their takeaways and empirical findings.\n* The trajectory based entropy and entropy smoothing is novel to me---it provides a self-stabilizing strategy that is fairly interesting."}, "weaknesses": {"value": "Major comments:\n\n* I apologize for my unfamiliarity with the empirical studies---however, it seems to me that the baselines don't include the obvious alternative: single-step entropy PPO. Since the main pitch of the paper is \"standard PPO/GRPO entropy is temporally blind\" is unstable, the right comparison should be PPO with a per-turn entropy cap. Right now, if I understand correctly, the comparisons are to pure PPO/GRPO and to other agent RL methods, not to the simplest entropy-aware PPO.\n* I think the name of the method \"Entropy-regularized Policy Optimization\" is fairly misleading. The novelty isn't about entropy regularization, but a path-dependent smoothing entropy regularization. Another name like \"Entropy-smoothing Policy Optimization\" may be more appropriate."}, "questions": {"value": "* My main questions are in the weaknesses section.\n* How does the hyperparameter affects the empirical performance? If the problem is over-exploration due to sparse rewards, what if comparing to a per-turn entropy capped PPO but decaying penalty with turn?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GRuiOFvbGo", "forum": "y29theOjum", "replyto": "y29theOjum", "signatures": ["ICLR.cc/2026/Conference/Submission22837/Reviewer_6TKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22837/Reviewer_6TKC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885420781, "cdate": 1761885420781, "tmdate": 1762942405740, "mdate": 1762942405740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}