{"id": "XeLrfKEOZS", "number": 3499, "cdate": 1757449969496, "mdate": 1759898084383, "content": {"title": "SigLIP-HD by Fine-to-Coarse Supervision", "abstract": "High-quality visual representation is a long-standing pursuit in computer vision. In the context of multimodal LLMs (MLLMs), feeding higher-resolution images can produce more fine-grained visual tokens. However, it introduces additional computational and design complexity, due to multiple forward passes and post-processing of increased tokens. Before simply adopting a higher resolution, have we truly unlocked the model's full perception capability at a standard resolution? Therefore, we study an interesting problem: how to achieve fine visual perception under lower cost without larger images. We present SigLIP-HD in this work. The core is a highly simple fine-to-coarse supervision design. We enforce the coarse feature of a mid-resolution image to mimic the fine-grained feature of its high-resolution version. We build this framework on the advanced SigLIP 2 model. Our final model produces better visual tokens at exactly the same inference budget. It is validated on extensive MLLM benchmarks and consistently delivers stronger results than our baseline model, especially on OCR-related tasks.", "tldr": "", "keywords": ["vision-language model", "multi-modality LLM"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b86b573ff03f5808b21bea576360a2a708031e75.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the high-quality visual feature engineering for MLLMs, and propose a strong vision backbone SigLIP-HD via a fine-to-coarse supervision training regime on SigLIP2. The experiments show that this new backbone can improve MLLMs' performance on most benchmarks without increasing input image resolutions and using more tokens."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is reasonable, and the design and training of SigLIP-HD are also supported by sufficient empirical studies. \n\n2. The proposed SigLIP-HD performs well on most benchmarks for two MLLMs, showing its stronger visual representations without increasing the image resolution."}, "weaknesses": {"value": "1. The method part needs more detailed and clearer descriptions. The description of the method is too brief and colloquial, making it difficult for readers to understand the specific details and procedures. For instance, the distillation between the multi-patch feature maps and the SigLIP-HD. \n\n2. More in-depth insights. The importance of visual features in the field has already reached a consensus. The authors demonstrate that distillation learning with high and low resolutions can improve the quality of visual features, which is reasonable and expected. In addition, the authors should preferably provide more insights and discoveries to strengthen the contribution of the paper. \n\nMinors:\n\n1. What about the results of the distillation with dynamic res. settings? Some recent works show that different MLLMs favors different resolutions of input images. In this case, dynamic resolution may be a better solution for fixed HD images. \n\n2. Missing some relevant works about efficient HD feature engineering, e.g., LLaVA-HD [a]. LLaVA-HD uses two backbones to process low and high resolutions of the input images, which also keeps a small number of visual tokens. \n\n3. The authors all uses tables to report the experimental results, while some of them may be replaced by plot charts to save the paper length. Thus, the authors can give more descriptions to their methods. \n\n[a] Luo, Gen, et al. \"Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JMjoTbDGTd", "forum": "XeLrfKEOZS", "replyto": "XeLrfKEOZS", "signatures": ["ICLR.cc/2026/Conference/Submission3499/Reviewer_rVfE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3499/Reviewer_rVfE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761274921564, "cdate": 1761274921564, "tmdate": 1762916769524, "mdate": 1762916769524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to enhance the perceptual capability of the visual encoder in multimodal large language models (MLLMs). The core idea is to optimize the encoder’s small-scale representations using guidance from its larger-scale representations. During training, an L1 loss is applied to directly align the representations between different scales. Experimental results demonstrate that SigLIP-HD produces more effective visual tokens than its base model under the same input scale."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper investigates how different image scales and multi-scale visual feature fusion strategies affect the performance of multimodal large language models (MLLMs).\n2. It introduces a simple method that employs an L1 loss to align single-scale visual features with their multi-scale counterparts, thereby enabling the visual encoder to generate higher-quality single-scale features during inference.\n3. Experimental results demonstrate that the optimized single-scale features significantly improve the performance of both standard-resolution and AnyRes MLLMs across diverse downstream benchmarks."}, "weaknesses": {"value": "1. The paper requires more experimental evidence to validate its effectiveness. It lacks a comparison with the approach that directly merges multi-scale visual features during inference (i.e., using the teacher-generated features). Such an experiment would help quantify the performance gap between the trained single-scale features and the multi-scale features. In addition, the impact of both approaches on inference time for different MLLM sizes should be reported. Given that the visual encoder is relatively lightweight and multi-scale features can be obtained in parallel within a single forward pass, the additional inference cost may be negligible. This suggests that directly using teacher features at inference time could be a more practical choice.\n2. The novelty of the work is limited. The idea of leveraging multi-scale visual feature fusion to enhance visual representations for MLLMs has been previously explored, for example in S$^2$ [1]. Although the author revisits the impact of image scale, it does not provide new insights. Moreover, a direct experimental comparison with S$^2$ [1] is missing and should be included.\n3. The training cost of retraining the visual encoder in this work is higher than that of existing models such as LLaVA 1.5 and LLaVA-NeXT. Considering that the visual encoder is non-autoregressive and relatively small in size, performing multi-scale fusion directly during inference might be a more efficient and reasonable alternative.\n\n[1] When Do We Not Need Larger Vision Models? ECCV 2024."}, "questions": {"value": "1. In Table 2, would using image scales such as 512² + 1536² or 512² + 2048² yield better representations compared to combining multiple higher scales? Since S² demonstrates that scaling scales can lead to improved results, could a similar setting be applied here for distillation?\n2. The name SigLIP-HD may be somewhat confusing, as the work does not actually increase the input resolution but rather aims to obtain better visual representations at low resolution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eQGCNDmfSK", "forum": "XeLrfKEOZS", "replyto": "XeLrfKEOZS", "signatures": ["ICLR.cc/2026/Conference/Submission3499/Reviewer_gGd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3499/Reviewer_gGd3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623181342, "cdate": 1761623181342, "tmdate": 1762916767709, "mdate": 1762916767709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SigLIP-HD, a method to enhance the fine-grained perception of Multimodal Large Language Models (MLLMs) without incurring the high computational cost of processing high-resolution images at inference time. The authors challenge the trend of scaling MLLMs by increasing input resolution, arguing that the perception capability of encoders at standard resolutions has not been fully unlocked. The core of the work is a simple \"fine-to-coarse supervision\" framework, a self-distillation mechanism where a \"student\" encoder (SigLIP-HD) is trained at a standard resolution to mimic the richer, \"teacher\" features produced by a frozen version of the same encoder fed with multi-scale inputs. The resulting SigLIP-HD encoder serves as a drop-in replacement for the original, offering significantly improved performance on fine-grained tasks (especially OCR) at the exact same inference budget and architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a highly relevant problem in MLLMs. The trend of scaling MLLMs to native, high-resolution inputs creates significant computational and design burdens (e.g., image slicing, multiple forward passes, token compression) . The goal of improving fine-grained perception while maintaining a low, standard-resolution input is practical and valuable.\n\n2. The proposed fine-to-coarse supervision is efficient and effective. It acts as a form of self-distillation that \"distills\" multi-scale knowledge into a standard-resolution encoder. Crucially, it adds no modules or latency at inference time, as it is simply a new set of weights for the original encoder.\n\n3. The authors provide a strong empirical case for their method, which validate the effectiveness and generalizability."}, "weaknesses": {"value": "1. The core idea is an effective application of knowledge distillation, specifically self-distillation from a multi-scale ensemble \"teacher\" to a single-scale \"student.\" While the application to MLLM encoder resolution is valuable, the underlying mechanism adapts established distillation concepts .\n\n2. The method's improvements are overwhelmingly concentrated on OCR and fine-grained text-heavy benchmarks. The performance on general vision benchmarks is often marginal or flat. This suggests a specialized, rather than a universal, representational improvement.\n\n3. While the method shows strong gains with Vicuna-7B and Llama-3.2-3B, the improvements on other powerful LLMs like Qwen2.5-7B appear marginal. This might suggest the benefits of the distilled representation are less pronounced when combined with certain advanced LLM architectures."}, "questions": {"value": "1. The method is a form of feature-based distillation. Why was a simple L1 loss (which performs best in Table 8) superior to a logit-based (e.g., contrastive) distillation, which is standard for training models like CLIP and SigLIP?\n\n2. How critical is the Cambrian-1 dataset to this method's success? Have you experimented with performing this fine-to-coarse supervision using more standard, large-scale web datasets or common VQA datasets?\n\n3. Given the modest gains on general benchmarks, does the fine-tuning cause any significant regressions on specific general vision categories (e.g., object counting, spatial relations) that the original SigLIP 2 was strong in?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aywIUQvFM8", "forum": "XeLrfKEOZS", "replyto": "XeLrfKEOZS", "signatures": ["ICLR.cc/2026/Conference/Submission3499/Reviewer_vqYK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3499/Reviewer_vqYK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832047506, "cdate": 1761832047506, "tmdate": 1762916765319, "mdate": 1762916765319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to distill high-resolution features of SigLIP2 to its low-resolution encoding results to improve the performance under a limited encoding budget. The idea is intuitive and sound, and the experiments show improvements under reasonable training computation. The major concern is that the findings from Section 2 are relatively trivial, making the paper lack enough substance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Distilling high-resolution features to low-resolution ViTs is intuitive and sound. The method is simple and easy to follow.\n2. The experiments show improvements over SigLIP after reasonable computation of training.\n3. The paper is well written."}, "weaknesses": {"value": "1. It seems Table 1 is only used to explain why the authors chose So400m/16-512px as the default backbone choice of ViT. This is not necessary, considering Table 1 takes much of the space in the main paper. Likewise, it would be better if the pilot experiments in section 2 could present more interesting and insightful results. Also, the multi-scale integration experiments have little to do with the final method.\n2. There are previous works investigating the problem of dealing with high-resolution images using overlapped sliding windows[1]. The authors can refer to this paper for a deeper understanding of why overlapping slices may lead to inferior results.\n\n[1] LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images. ECCV 2024."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ujVvwkeviy", "forum": "XeLrfKEOZS", "replyto": "XeLrfKEOZS", "signatures": ["ICLR.cc/2026/Conference/Submission3499/Reviewer_2LRA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3499/Reviewer_2LRA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921187929, "cdate": 1761921187929, "tmdate": 1762916760314, "mdate": 1762916760314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}