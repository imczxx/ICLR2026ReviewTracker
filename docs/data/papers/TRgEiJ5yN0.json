{"id": "TRgEiJ5yN0", "number": 2850, "cdate": 1757280682817, "mdate": 1759898123085, "content": {"title": "Beyond RLHF: A Theoretical Framework of Alignment as Distribution Learning", "abstract": "Alignment via reinforcement learning from human feedback (RLHF) has become the dominant paradigm for controlling the quality of outputs from large language models (LLMs). However, the standard RLHF objective lacks formal justification and incentivizes degenerate, deterministic LMs in the asymptotic regime. We ask under what assumptions can we derive RLHF or other novel objectives with rigorous learning-theoretic guarantees, without relying on an \\emph{a priori} notion of reward maximization. To this end, we reframe alignment as \\emph{distribution learning} from pairwise preferences, formalizing our approach with a probabilistic assumption describing how preferences reveal information about the target (oracle) LM. This leads us to propose three principled alignment objectives: preference maximum likelihood estimation, preference distillation, and reverse KL minimization. We prove that all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to the target LM, naturally avoiding degeneracy. In particular, reverse KL highly resembles the RLHF objective, providing strong justification for RLHF with a minor correction. Furthermore, our theory confirms the empirical finding that on-policy objectives (e.g., RLHF) often outperform likelihood-style objectives (e.g., DPO). Finally, we empirically show that our proposed methods consistently matches or outperforms baselines across various tasks and models.", "tldr": "", "keywords": ["alignment", "large language models", "RLHF", "DPO"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9415cdfc78352af8d7a23fcbd52bff1246390de5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes 3 LLM alignment objectives from distribution learning perspective, i.e., preference maximum likelihood estimation, preference distillation, and reverse KL minimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed three methods look well derived and motivated. The idea is presented clearly. There seems sufficient experimental details in the appendix."}, "weaknesses": {"value": "The relationship and comparison among the three methods are not clear to me. In the introduction, could you briefly compare the proposed three methods and when to use which? Also, the experimental performance improvement does not look significant."}, "questions": {"value": "(1) In the introduction, could you briefly compare the proposed three methods and when to use which? \n\n(2) Line 94: \"where as\" to \"whereas\". \n\n(3) Line 143: What is \"cyclic preference\"? I have not found this term in existing literature, so you could explain in the paper. Also, how can your model (1) allow cyclic preference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZG6LTwYI8P", "forum": "TRgEiJ5yN0", "replyto": "TRgEiJ5yN0", "signatures": ["ICLR.cc/2026/Conference/Submission2850/Reviewer_3tQB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2850/Reviewer_3tQB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760820028196, "cdate": 1760820028196, "tmdate": 1762916411545, "mdate": 1762916411545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We like to thank the reviewer for providing thoughtful feedback. We have written a common response to all the reviewers in our rebuttal -- please read this first.\n\n**[Common comment for all the reviewers]**\n\nWe respectfully believe that our main message and contribution have not been delivered to some reviewers. We would like to emphasize here for all the reviewers. We will update our manuscript accordingly for the final version.\n\nFirst, our work is primary area is 'learning theory' -- thus, the evaluation should be focused on the theoretical value that it provides. Second, the main research question that we answer is, as stated in our abstract, \n> .. under what assumptions can we derive RLHF or other novel objectives with rigorous learning-theoretic guarantees..\n\nWe can relate our algorithms to existing ones, but some of them empirically make a meaningful difference such as Preference Distillation vs REBEL. \n\nWe want to highlight the following theoretical results:\n- Our work is the first one to **justify the use** of the RLHF objective (with a minor correction) by deriving it from the distributional learning formulation. As reviewers have pointed out, for RLHF, there are indeed numerous works with theoretical guarantees. However, their guarantees are all about converging to the optimal solution of the population version of the RLHF training objective. This is clearly a tautology! In our opinion, this cannot be viewed as a justification of the RLHF training objective.\n- Our work is the first one to theoretically confirm the common empirical finding that RLHF is better than DPO (with minor variations on both algorithm --  see the discussion \"Why does reverse KL attain a better bound?\" after Theorem 7). This could not have been done in prior work because they set up theoretical frameworks that are different for each algorithm to be analyzed. In contrast, we have a **single** theoretical framework under which we can justify multiple algorithms and compare their convergence rates.\n\nMost existing theoretical works designate the population version of the RLHF objective as its target, but they never question under what assumptions it is the right target. In our opinion, there are two problems: (i) why should we maximize reward? (ii) why should the regularizer be part of the theoretical target? For (i), there is no reward in the problem definition of the alignment problem. Reward is an algorithmic artifact. Regarding (ii), from the standard learning theoretic viewpoint, regularizers should be part of the algorithm, not part of the problem. Regularizers prevent overfitting, but in the limit of data, it will prevent learning (as we showed in Figure 1).\n\nWe have introduced a novel framework for alignment that is reward-free yet leads to providing strong justifications to multiple existing algorithms (with minor changes) that use rewards (e.g., RLHF), all the while keeping the regularization completely part of the algorithm, not the problem definition. We believe our results provide a fresh, simple, yet unifying theoretical viewpoint on alignment and is worth being known in the ML community."}}, "id": "jlcwNv7OXx", "forum": "TRgEiJ5yN0", "replyto": "TRgEiJ5yN0", "signatures": ["ICLR.cc/2026/Conference/Submission2850/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2850/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2850/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763725656587, "cdate": 1763725656587, "tmdate": 1763726565839, "mdate": 1763726565839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work reviews language model alignment as a distribution learning problem rather than reward maximization. Instead of relying on the RLHF objective, the authors assume a probabilistic Bradley–Terry preference model and derive three principled methods—Preference MLE, Preference Distillation, and Reverse KL Minimization. For each method, O(1/n) convergence to the target model is established. The Reverse KL formulation further shows that RLHF can be viewed as an approximation of minimizing the KL divergence between the learned and ideal distributions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work provides a new view for alignment with theoretical justifications. \n2. The proposed method has empirical improvement over RHLF, reliable method and DPO."}, "weaknesses": {"value": "While the paper provides a theoretical framework for viewing alignment as distribution learning, its empirical validation remains limited. The experiments are conducted on relatively small models and limited number of tasks. The performance gains over DPO, RELABEL and RLHF are modest. Evaluations on larger language models and more diverse tasks would be necessary to fully demonstrate the scalability and practical advantages of the proposed approaches."}, "questions": {"value": "Where are assumptions (i) and (ii) in lines 140–151 formally stated? It would be clearer if the authors presented them in a more explicit way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2p53Zx0TLz", "forum": "TRgEiJ5yN0", "replyto": "TRgEiJ5yN0", "signatures": ["ICLR.cc/2026/Conference/Submission2850/Reviewer_v7Bb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2850/Reviewer_v7Bb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761275168297, "cdate": 1761275168297, "tmdate": 1762916411024, "mdate": 1762916411024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper approaches the alignment problem from a distribution-learning perspective rather than the traditional RLHF framework, aiming to provide a deeper understanding of model behavior and theoretical guarantees for convergence toward a target distribution as the sample size increases. Assuming the existence of a target language model that assigns higher likelihoods to preferred responses, the authors propose three algorithms—PMLE, Preference Distillation, and Reverse KL—whose solutions provably converge to the target model. Experimental results show that these methods consistently match or outperform baseline approaches across diverse tasks and model settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provoide an interesting distributional learning perspective for LLM Alignment.\n- The experimental results demonstrate that the proposed methods can mitigate the alignment tax problem and improve performance compared to prior methods."}, "weaknesses": {"value": "* **Lack of theoretical justification for the distribution-learning perspective.** The paper assumes the existence of an optimal language model that determines the preference probabilities in Eq. 2. However, such an optimal policy can also be derived under a reward-maximization objective with entropy regularization [1]. It remains unclear whether this distribution-learning perspective truly goes beyond the notion of reward maximization in alignment.\n\n* **Unclear implications of the theoretical rate improvement.** The theoretical analysis derives a $1/n$ upper bound, improving upon the $1/\\sqrt{n}$ rate from prior works. However, a major challenge in alignment is reward over-optimization [4,5], and simply scaling the sample size $n$ does not mitigate this issue. Prior pessimism-based approaches indicate that avoiding over-optimization requires ensuring single-policy coverage [2,3]. The paper does not demonstrate whether the three proposed algorithms achieve such coverage, nor whether improving the convergence rate to $1/n$ alone meaningfully mitigates over-optimization in practice.\n\n* **Strong similarity to prior reward-maximization methods.** The three proposed methods closely resemble previous reward-based objectives: PMLE parallels DPO with a Reverse KL term [6], Preference Distillation resembles Online-DPO [7], and Reverse KL aligns with RLHF with an added entropy bonus.\n\n## References\n[1] Maximum Entropy Inverse Reinforcement Learning. AAAI 2008.\n\n[2] Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization. ICLR 2025 Spotlight.\n\n[3] REBEL: Reinforcement Learning via Regressing Relative Rewards. NeurIPS 2024.\n\n[4] Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms. NeurIPS 2024.\n\n[5] Scaling Laws for Reward Model Overoptimization. ICML 2023.\n\n[6] The Importance of Online Data: Understanding Preference Fine-tuning via Coverage. NeurIPS 2024.\n\n[7] Direct Language Model Alignment from Online AI Feedback. CoRR 2024."}, "questions": {"value": "- Can PMLE and the other two objectives achieve a single-policy coverage coefficient?\n\n- Why use Reverse KL instead of Forward KL (SFT-style) regularization? What is the theoretical justification for applying Reverse KL in all three objectives from the distribution-learning perspective? Proposition 12 suggests that Reverse KL is not required to achieve the $1/n$ sample rate, so what additional role does it play?\n\n- What distinguishes PMLE from DPO + Reverse KL, and Preference Distillation from Online-DPO? Can DPO + Reverse KL and Online-DPO achieve the same $1/n$ sample rate? If not, why?\n\n- How do PMLE and Preference Distillation compare to DPO + Reverse KL and Online-DPO as the number of samples increases?\nDoes PMLE mitigate over-optimization as sample size grows, compared to $\\chi$-PO [2]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RLXvWQHgPc", "forum": "TRgEiJ5yN0", "replyto": "TRgEiJ5yN0", "signatures": ["ICLR.cc/2026/Conference/Submission2850/Reviewer_C3Gp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2850/Reviewer_C3Gp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833040773, "cdate": 1761833040773, "tmdate": 1762916410226, "mdate": 1762916410226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Proposes a novel perspective on alignment by framing it as distribution learning from pairwise preferences.,Introduces three new algorithms: preference maximum likelihood estimation, preference distillation, and reverse KL minimization, which are not present in existing literature.,Provides rigorous learning-theoretic guarantees for the proposed methods, which is a significant advancement over traditional RLHF approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Addresses a critical gap in the theoretical justification of RLHF objectives, which has been largely taken for granted in prior works.,The proposed methods potentially improve the safety and effectiveness of LLMs by avoiding degeneracy associated with traditional RLHF.,Empirical validation showing that the new methods outperform existing baselines indicates practical relevance and applicability."}, "weaknesses": {"value": "1. Novelty: Although the objectives are distributional, the strong convergence analysis has been developed by several works before [1,2,3], but the authors neither cite those works nor highlight the novelty of their analysis in the work. The authors need to state the novelty of their analysis.\n\nReference:\n[1] Zhao, H., et al., Logarithmic regret for online kl-regularized reinforcement learning\n[2] Wu, D., et al, Greedy Sampling Is Provably Efficient for RLHF\n[3] Zhao, H., et al, Sharp analysis for kl-regularized contextual bandits and rlhf\n\n2. The idea of distributional learning has also been studied before [1,2] ..., named as generative reward model (GRM). The authors also do not mention this line of work. Could they discuss the relationship of their work with GRM?\n\nReference:\n[1] ZHong, H, er al,DPO Meets PPO: Reinforced Token Optimization for RLHF\n[2] Dakota Mahan, et al, GENERATIVE REWARD MODELS"}, "questions": {"value": "1. The applications of their distributional objective seem to be limited compared to general RLHF, since they need first to learn a good policy distribution and then approximate that distribution. Besides, there will exist an approximation error between the surrogate policy and the true optimal policy. This error is neglected in their theoretical bound."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are some papers not cited and discussed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KUUYiOH1mf", "forum": "TRgEiJ5yN0", "replyto": "TRgEiJ5yN0", "signatures": ["ICLR.cc/2026/Conference/Submission2850/Reviewer_LNwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2850/Reviewer_LNwF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762166159177, "cdate": 1762166159177, "tmdate": 1762916409133, "mdate": 1762916409133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}