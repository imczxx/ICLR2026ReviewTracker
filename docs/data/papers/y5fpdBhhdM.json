{"id": "y5fpdBhhdM", "number": 22205, "cdate": 1758327758017, "mdate": 1759896880594, "content": {"title": "SafeReview: Building a Robust Deep Review Assistant Against Prompt Injection", "abstract": "As Large Language Models (LLMs) are increasingly integrated into academic peer review, their vulnerability to prompt injection—adversarial instructions embedded in submissions to manipulate outcomes—emerges as a critical threat to scholarly integrity. To counter this, we propose a novel adversarial framework where a Generator model, trained to create sophisticated attack prompts, is jointly optimized with a Defender model tasked with their detection. This system is trained using a loss function inspired by Information Retrieval Generative Adversarial Networks (IRGANs), which fosters a dynamic co-evolution between the two models, forcing the Defender to develop robust capabilities against continuously improving attack strategies. The resulting framework demonstrates significantly enhanced resilience to novel and evolving threats compared to static defenses, thereby establishing a critical foundation for securing the integrity of automated academic evaluation.", "tldr": "", "keywords": ["LLM", "Peer Review", "AI Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be439f845bd7a81baecb2daef0caf51360c12dd6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "With the growing integration of Large Language Models (LLMs) into academic peer review, prompt injection has emerged as a critical vulnerability. Malicious authors embed hidden adversarial instructions to manipulate LLM-generated evaluations, undermining scholarly integrity. Existing LLM-based review systems focus on addressing limitations like superficial feedback but fail to tackle this evolving threat, as static defenses trained on known attacks are insufficient against continuously changing prompt injection techniques. This paper proposes SafeReview, a co-evolutionary adversarial training framework designed for LLM-based peer review systems that optimize both attacker and defender."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The issue proposed in this paper is the unfairness in AI review, which has attracted significant attention in the academic community. I believe this topic is highly interesting and meaningful. \n2. This paper proposes a method for AI review that prevents and mitigates prompt injection, which holds practical value."}, "weaknesses": {"value": "1. AI review typically uses an LLM-as-judge model, and these models usually face issues of bias and variance. Bias refers to the gap between the model's scores and the ground-truth, while variance is the variance in the results of multiple samples from the review model. Since the optimization objective of SafeReview does not include variance, I believe it is necessary to analyze whether SafeReview will lead to an increase in the variance of the outputs of the review model.\n2. I am very curious why GRPO is used in attacker training while DPO is used in defender training. If the defender can learn certain reasoning processes, will the review model become more interpretable?\n3. I believe this paper needs to evaluate the review model (both before and after adversarial training) on a benign dataset to demonstrate whether SafeReview training impairs the inherent capabilities of the review model itself.\n4. Previously, some researchers have used white fonts to conduct concealed prompt injection. I wonder if SafeReview can detect such types of attacks."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jozN4uNFqi", "forum": "y5fpdBhhdM", "replyto": "y5fpdBhhdM", "signatures": ["ICLR.cc/2026/Conference/Submission22205/Reviewer_Ccfa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22205/Reviewer_Ccfa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413603283, "cdate": 1761413603283, "tmdate": 1762942113298, "mdate": 1762942113298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SafeReview examines prompt injection in long, scholarly peer-review workflows, where adversarial, instruction-like text embedded in submissions can inflate ratings and distort acceptance rankings. It introduces a co-evolutionary framework that trains an attacker and a defender in tandem. The attacker uses GRPO to craft subtle, context-aware injections, optimized by a hybrid reward that combines score inflation with rank disruption (e.g., reduced Spearman correlation with ground-truth order). The defender is trained with DPO on preference pairs that favor reviews resisting injected directives while preserving quality.\n\nTo handle long documents, the system first localizes risky regions via hierarchical segmentation, then applies fine-grained adversarial training. A curriculum stabilizes training by gradually increasing attack difficulty. Experiments simulate realistic review pipelines and report that, under strong GRPO-generated attacks, SafeReview reduces acceptance inflation, improves rank stability, and lowers false positives relative to an undefended reviewer and a static DPO baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting and timely approach tailored to peer-review settings. \n\nThe paper tackles prompt injection in long scholarly documents—a high-impact, underexplored niche—by co-evolving an attacker and a defender. This framing feels fresh, domain-aware, and immediately relevant to current LLM-assisted reviewing workflows.\n\n2. Useful, promising results on an extensive benchmark. \n\nThe experiments cover realistic attack styles and long-document conditions, with clear metrics (e.g., score inflation, rank stability, false-positive control). The defense consistently improves robustness while preserving review quality, suggesting strong practical value and good prospects for real-world deployment."}, "weaknesses": {"value": "1. Scope clarity vs. classic prompt injection. \n\nThe paper frames attacks in peer-review documents but does not convincingly articulate what is fundamentally new beyond standard prompt-injection/jailbreak threats. It remains unclear whether the challenge is primarily long-context placement/localization (a setting detail) or introduces qualitatively different adversarial mechanics. Without a sharper problem definition (e.g., formal distinctions, new threat primitives, or impossibility results specific to scholarly reviews), the contribution risks reading as an application of known threats rather than a new problem class.\n\n2. Missing comparative baselines limit external validity. \n\nThe evaluation omits strong, diverse defenses that practitioners would reasonably try first, making it hard to attribute gains to the proposed method rather than to the choice of baseline. In particular:\n\n* The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions — a training-time approach that explicitly teaches models to de-prioritize untrusted in-context directives.\n\n* SecAlign: Defending Against Prompt Injection with Preference — secure preference optimization that aligns outputs away from injection-following behavior.\n\n* Llama Prompt Guard 2 — a lightweight detector/guardrail that can pre-filter or route suspicious inputs.\nIncluding these would better position the method against (i) preference-optimization defenses, (ii) instruction-priority finetuning, and (iii) practical detector-based guardrails."}, "questions": {"value": "Why and how prompt injecting a paper is fundamentally different from traditional prompt injection settings? why you do not compare with baselines such as SecAlign (see my comment in weakness)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oc2uV1rXaK", "forum": "y5fpdBhhdM", "replyto": "y5fpdBhhdM", "signatures": ["ICLR.cc/2026/Conference/Submission22205/Reviewer_nePo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22205/Reviewer_nePo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757758509, "cdate": 1761757758509, "tmdate": 1762942113117, "mdate": 1762942113117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SafeReview, a coevolutionary adversarial training framework to improve the robustness of LLM-based peer-review systems against prompt-injection attacks. A generator model produces adversarial injection prompts from a database of publications, while a defender (reviewer) learns to resist them. Attacker will be updated by GRPO, while defender is updated via DPO. Experiments on NeurIPS and DeepReview datasets show reduced acceptance of manipulated papers and improved ranking correlation, suggesting that SafeReview enhances review integrity under adversarial conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated. Securing AI-based peer review is an important and underexplored problem.\n2. The co-evolutionary training method, in general, is interesting.\n3. The results using Qwen3-4B-Instruct Team the Generator and DeepReviewer-14B as the Defender show good performance."}, "weaknesses": {"value": "1. One major concern is the one-sided notion of safety. The paper focuses entirely on avoiding false positives (i.e., stopping flawed papers from being wrongly accepted) but neglects the equally important false negative side, i.e., ensuring that good papers are not unfairly penalized. A robust review model must preserve both sensitivity and fairness, not just caution.\n2. There is no analysis of bias amplification. By training the reviewer to resist persuasive or assertive language, the model may overcorrect and start undervaluing legitimate confident writing, leading to systematically harsher or more negative reviews.\n3. There is no evaluation showing that defended reviews remain consistent with expert human judgments in both positive and negative cases.\n4. The paper conducts limited experiments, for example, only testing Qwen3-4B-Instruct Team the Generator and DeepReviewer-14B as the Defender, without testing other models."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lI2P4cl4fp", "forum": "y5fpdBhhdM", "replyto": "y5fpdBhhdM", "signatures": ["ICLR.cc/2026/Conference/Submission22205/Reviewer_cE29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22205/Reviewer_cE29"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887659852, "cdate": 1761887659852, "tmdate": 1762942112808, "mdate": 1762942112808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles prompt injection attacks in LLM-based peer review. The authors propose SafeReview, a co-evolutionary framework where an attacker model (Generator) and a review model (Defender) are trained adversarially. The Generator creates attack prompts, while the Defender learns to resist them. Results show the method reduces the acceptance rate of attacked papers and improves correlation with ground-truth scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well structured, and technical details are clearly presented.\n\nThe proposed co-evolutionary-based approach is a well-reasoned method for building a dynamic defense that outpaces static ones. The use of GRPO and DPO is technically sound."}, "weaknesses": {"value": "1. Key innovations mentioned in the introduction, such as “hierarchical segmentation” for long documents and “curriculum scheduling”, are not explained in the methods or experiments.\n\n2. The claim that co-evolutionary training (SafeReview) beats static defense (Static DPO) is unsubstantiated. The paper fails to test both defenses against both attack types (e.g., Static DPO vs. GRPO attack), making a direct comparison impossible."}, "questions": {"value": "Please address all concerns in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TFU95UawFX", "forum": "y5fpdBhhdM", "replyto": "y5fpdBhhdM", "signatures": ["ICLR.cc/2026/Conference/Submission22205/Reviewer_mRt1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22205/Reviewer_mRt1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993055538, "cdate": 1761993055538, "tmdate": 1762942112626, "mdate": 1762942112626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}