{"id": "aPyFj7sL6F", "number": 13233, "cdate": 1758215442615, "mdate": 1759897453783, "content": {"title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "abstract": "The difficulty and expense of obtaining large-scale human responses make Large Language Models (LLMs) an attractive alternative and a promising proxy for human behavior. However, prior work shows that LLMs often produce homogeneous outputs that fail to capture the rich diversity of human perspectives and behaviors. Thus, rather than trying to capture this diversity with a single LLM agent, we propose a novel framework to construct a set of agents that collectively capture the diversity of a given human population. Each agent is an LLM whose behavior is steered by conditioning on a small set of human demonstrations (task–response pairs) through in-context learning. The central challenge is therefore to select a representative set of LLM agents from the exponentially large space of possible agents. We tackle this selection problem from the lens of submodular optimization. In particular, we develop methods that offer different trade-offs regarding time complexity and performance guarantees. Extensive experiments in crowdsourcing and educational domains demonstrate that our approach constructs agents that more effectively represent human populations compared to baselines. Moreover, behavioral analyses on new tasks show that these agents reproduce the behavior patterns and perspectives of the students and annotators they are designed to represent.", "tldr": "We propose a framework to construct a set of agents that collectively capture the diversity of a given human population.", "keywords": ["population alignment", "human behaviors", "prompt optimization", "submodular optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/05c9830ba794156922c3ec58146cd3839d9ffa5f.pdf", "supplementary_material": "/attachment/2d6fdafeabb9335099e5f90da191be9db7004b92.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of Large Language Models (LLMs) producing homogeneous outputs that fail to represent the diversity of human populations. Instead of creating a single LLM agent, the authors propose a framework to construct a set of diverse agents that collectively capture the behavior of a target human population. Each agent's behavior is guided by a small set of human demonstrations through in-context learning. The core problem is selecting a representative set of agents from an exponentially large space, which the authors formulate as a submodular optimization problem. They develop several methods, such as $REPPOP_{demo}$ and $REPPOP_{mapped}$, that offer different trade-offs between computational complexity and performance. Experiments conducted in educational (EEDI dataset), opinion survey (OpinionQA dataset), and crowdsourcing (WikiArt dataset) domains show that the proposed methods construct agent sets that more effectively represent human populations compared to baselines. Furthermore, behavioral analysis demonstrates that these agents successfully reproduce the distinct behavior patterns and perspectives of the human subgroups they are designed to represent on new, unseen tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A key strength is the innovative framing of the agent selection problem as a submodular optimization task. This provides a sound mathematical foundation for the problem, moving beyond simple heuristics and enabling the development of methods with theoretical performance guarantees.\n\n2. The $REPPOP_{mapped}$ methods are a clever and crucial contribution. The heuristic to create a proxy agent for each human, thereby reducing an exponential search space to a linear one, is a well-justified move that makes the entire approach computationally feasible without resorting to purely random sampling.\n\n3. The method is tested across diverse domains (education, opinion surveys, annotation) and multiple LLM families, demonstrating its model-agnostic effectiveness."}, "weaknesses": {"value": "1. My primary concern lies with the WikiArt experiment, which is arguably the most complex and interesting testbed. The evaluation here relies entirely on \"synthetic humans\" generated by prompting another large language model (Gemma3-27B) with personality traits. This setup means the experiment is essentially testing how well the proposed framework can use a set of agents to model the behavior of another generative model, rather than the far more complex, noisy, and sometimes irrational behavior of actual humans. While this is a valid test of the optimization procedure itself, it significantly limits the claims that can be made about the method's effectiveness on genuine, open-ended human tasks. The success in this domain may not generalize to real-world scenarios where human responses are not as neatly conditioned on a few personality axes.\n\n2. The core optimization objective, which minimizes the average distance from each human to their nearest agent, seems to overlook a crucial aspect: the population distribution. The metric $g(L)$ effectively rewards covering the entire behavioral space. However, it fails to account for how many people fall into certain behavioral clusters. For example, a population with an 80/20 split between two main opinion groups could be \"optimally\" represented by two agents, one for each group. This would imply a 50/50 distribution in the agent set, which is a gross misrepresentation of the actual population. This could be highly misleading for any downstream task that relies on statistical inference or simulating group dynamics.\n\n3. Algorithm 1 describes a greedy, sequential process for building the M agents. It first finalizes the K demonstrations for the first agent, then moves on to the second, and so on. This approach introduces a strong path dependency. The choice of the very first agent significantly constrains the optimization landscape for all subsequent agents. If the first agent selected happens to cover a very broad but common part of the behavioral space, it might prevent the selection of a more nuanced set of specialist agents later on. A joint optimization over all M agents would be ideal, and while that is intractable, the current sequential approach may be prone to settling in a suboptimal local minimum for the entire set of agents.\n\n4. The agents are stateless and reactive, lacking the memory and planning capabilities found in advanced agent frameworks like AutoGen [1] or social simulation platforms like OASIS [2]. This raises a critical question about their cognitive consistency: while they can mimic specific behaviors, they may fail to respond coherently when faced with novel situations that require a consistent worldview. They function less like a true simulated society and more like a collection of behavioral samplers.\n\n[1] Wu, Qingyun, et al. \"Autogen: Enabling next-gen LLM applications via multi-agent conversations.\" First Conference on Language Modeling. 2024.\n\n[2] Yang, Ziyi, et al. \"Oasis: Open agent social interaction simulations with one million agents.\" arXiv preprint arXiv:2411.11581 (2024)."}, "questions": {"value": "Have the authors considered the trade-offs between their ensemble approach and an alternative using a single, powerful SOTA model? For instance, could one use a model like GPT-4 with a rich, dynamically-retrieved context (via RAG) to simulate diverse individuals with potentially greater cognitive consistency? I'm curious what the authors see as the key advantages of their specialist ensemble method over this kind of 'single generalist' approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "78Pny9ZCqe", "forum": "aPyFj7sL6F", "replyto": "aPyFj7sL6F", "signatures": ["ICLR.cc/2026/Conference/Submission13233/Reviewer_SDgA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13233/Reviewer_SDgA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760534930134, "cdate": 1760534930134, "tmdate": 1762923919937, "mdate": 1762923919937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a submodular optimization framework to select a set of in-context-conditioned LLM agents that collectively represent human population diversity. Experiments in crowdsourcing and education show improved alignment with human behaviors. Limitations include reliance on prompting, lack of prompt-order analysis, and limited real-world evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The topic is important: LLMs are increasingly used as proxies for human behavior, yet current models tend to collapse toward homogeneous responses."}, "weaknesses": {"value": "- Theorem 1 (NP-hardness of subset selection) and Proposition 1 (Submodularity of f(L)). The NP-hardness of this subset selection objective and submodularity property of f(L) is well established in prior work (e.g., k-center, facility-location), so the full proof seems unnecessary; a short reference would make the paper more focused on main novelty.\n- The proposed framework closely mirrors prior work [1], particularly in using submodular optimization for in-context example selection. The contribution appears incremental rather than conceptually new.\n- The paper does not define or explain the Representation Error metric, making it difficult to interpret results or compare performance against existing baselines.\n- The paper lacks performance or conceptual comparisons with strong existing baselines [2–4]. Specifically, Bui et al. [2] propose a contextual mixture-of-personas model that aligns LLM behavior with human populations using persona descriptions and in-context examples. Choi & Li [3] introduce PiCLe, an in-context learning framework designed to elicit diverse human-like behaviors, while Yu et al. [4] present a prompting-based approach to generate diverse and bias-controlled data. \n- Experimenting on OpinionQA is questionable, see [5]. Dominguez-Olmedo et al. (2024) show that survey-style prompting of LLMs yields unreliable signals: responses are dominated by ordering and labeling biases, and once controlled, model outputs become nearly uniform and high-entropy regardless of size or tuning. As a result, survey-based “alignment” measures mainly capture random or positional effects rather than genuine human-like diversity. Using OpinionQA therefore, risks over-interpreting noise as meaningful representativeness, weakening the validity of the claimed behavioral diversity results.\n\nReferences: \n- [1] Ji, Baijun, et al. \"Submodular-based in-context example selection for LLMs-based machine translation.\" Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 2024.\n- [2] Bui, Ngoc, et al. \"Mixture-of-personas language models for population simulation.\" arXiv preprint arXiv:2504.05019 (2025).\n- [3] Choi, Hyeong Kyu, and Yixuan Li. \"Picle: Eliciting diverse behaviors from large language models with persona in-context learning.\" arXiv preprint arXiv:2405.02501 (2024).\n- [4] Yu, Yue, et al. \"Large language model as attributed training data generator: A tale of diversity and bias.\" Advances in neural information processing systems 36 (2023): 55734-55784.\n- [5] Dominguez-Olmedo, Ricardo, Moritz Hardt, and Celestine Mendler-Dünner. \"Questioning the survey responses of large language models.\" Advances in Neural Information Processing Systems 37 (2024): 45850-45878."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VPyE0lKQa4", "forum": "aPyFj7sL6F", "replyto": "aPyFj7sL6F", "signatures": ["ICLR.cc/2026/Conference/Submission13233/Reviewer_6gHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13233/Reviewer_6gHd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984867476, "cdate": 1761984867476, "tmdate": 1762923919381, "mdate": 1762923919381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper frames population modeling as selecting a small set of in-context LLM `agents' to cover human behaviors by minimizing a representation gap in an embedding space. They introduce a submodular objective and propose a practical greedy solution to find a subset of agents that are representative of the human population. They report gains over some standard selection baselines across multiple tasks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies an important and emerging problem that has received significant attention recently.\n- The authors use broad empirical evaluation across several important datasets.\n- Clear ablations/visualizations that help interpret coverage and trade-offs."}, "weaknesses": {"value": "- Motivation for the formulation is unclear. Why should minimizing training-task embedding distances ensure alignment on unseen tasks?\n- Unclear construction of agent embeddings e_l an where exactly LLM's outputs enter the objective. Do you use LLM's logits to compute e_l, or does it solely depend on the in-context demonstrations?\n- Missing comparisons with mixture-of-agents/personas methods (e.g., PICLe (Choi et al. PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning); mixture-of-personas LMs (Bui et al. Mixture-of-Personas Language Models for Population Simulation)"}, "questions": {"value": "- How exactly is e_l computed for each dataset? Does it depend on LLM's output or just the in-context example only?\n- Where does LLM’s output come into place in the formulation (1)? Is it e_l? Each LLM has a different bias. How to mitigate these biases for a specific LLM?\n- Scalability with M: what if we want to analyze a larger population where M is large? Does your method scale well with M and K?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AKK1bBdblB", "forum": "aPyFj7sL6F", "replyto": "aPyFj7sL6F", "signatures": ["ICLR.cc/2026/Conference/Submission13233/Reviewer_HSz7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13233/Reviewer_HSz7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149589205, "cdate": 1762149589205, "tmdate": 1762923919022, "mdate": 1762923919022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}