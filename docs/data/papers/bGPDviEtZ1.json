{"id": "bGPDviEtZ1", "number": 15796, "cdate": 1758255360973, "mdate": 1759897281477, "content": {"title": "MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation", "abstract": "Imitation learning from large-scale, diverse human demonstrations has proven effective for training robots, but collecting such data is costly and time-consuming. This challenge is amplified for multi-step bimanual mobile manipulation, where humans must teleoperate both a mobile base and two high-degree-of-freedom arms. Prior automated data generation frameworks have addressed static bimanual manipulation by augmenting a few human demonstrations in simulation, but they fall short for mobile settings due to two key challenges: (1) determining base placement to ensure reachability, and (2) positioning the camera to provide sufficient visibility for visuomotor policies. To address these issues, we introduce MoMaGen, which formulates data generation as a constrained optimization problem that enforces hard constraints (e.g., reachability) while balancing soft constraints (e.g., visibility during navigation). This formulation generalizes prior approaches and provides a principled foundation for future methods. We evaluate MoMaGen on four multi-step bimanual mobile manipulation tasks and show that it generates significantly more diverse datasets than existing methods. Leveraging this diversity, MoMaGen can train successful imitation learning policies from a single source demonstration, and these policies can be fine-tuned with as few as 40 real-world demonstrations to achieve deployment on physical robotic hardware. More details are available at our project page: momagen-iclr2026.github.io.", "tldr": "We propose MoMaGen, an automated data generation method for bimanual mobile manipulation that satisfies hard constraints (e.g., reachability) while balancing soft constraints (e.g., visibility).", "keywords": ["Data Generation for Robot Learning", "Bimanual Mobile Manipulation", "Imitation Learning for Robotics"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9b41bb1530ca3c6e251e79164982e5678b2f6c4.pdf", "supplementary_material": "/attachment/abe2abb1bfa02f8a71e43da2b0b5f470b5605231.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a demonstration generation framework through optimizing soft and hard constraints in bimanual mobile manipulation. Specifically, the hard constraints are formulated as the reachability, visibility during manipulation. The soft constraints are formed as the visibility while navigation. The proposed MoMAGen targets to satisfy the hard constraints while balance the soft constraints in the augmentation procedure of human demonstrations. The proposal is evaluated on four multi-step bimanual mobile manipulation tasks and it generates substantially more datasets than prior methods, which could be further exploited in the further imitation policy learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe involving of reachability and visibility constraints are reasonable in the general data generation for multi-step bimanual mobile manipulation. \n2.\tBetter diversity and task-relevant object visibility are achieved by MoMaGen compared to other baselines.\n3.\tThe ablation studies are very comprehensive to investigate the influences of the two constraints in demonstration data generation.\n4.\tGreate policy learning is attained on the generated demonstrations by MoMaGen.\n5.\tThe paper is clear written."}, "weaknesses": {"value": "1.\tOne concern about this work is the efficiency of the demonstration collections and the computation cost of GPU for the constraint optimization. \n2.\tIs there any optimization for Eq.(1) when optimizing these constraints? Is there any conflicting terms in this equation? Some discussions should be included."}, "questions": {"value": "please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "kMZTQFFTcz", "forum": "bGPDviEtZ1", "replyto": "bGPDviEtZ1", "signatures": ["ICLR.cc/2026/Conference/Submission15796/Reviewer_MKkL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15796/Reviewer_MKkL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552956405, "cdate": 1761552956405, "tmdate": 1762926029214, "mdate": 1762926029214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces MoMaGen, a novel framework designed to automate the generation of large-scale, diverse demonstrations for complex multi-step bimanual mobile manipulation tasks. The work addresses the significant challenge and cost associated with collecting such high-dimensional data via traditional teleoperation, which requires controlling a mobile base, dual high-DoF arms, and active vision simultaneously. MoMaGen leverages a constraint-based system to generate these demonstrations in simulation efficiently. The research validates the quality of the resulting comprehensive dataset by successfully training and testing two SOTA imitation learning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Pioneering Problem Scope**: This research is groundbreaking as it is one of the first to focus on the full-body control problem, which integrates a mobile base, active vision, and dual-arm collaborative manipulation. This novel scope holds significant, pioneering implications for general-purpose robot control.\n2. **Comprehensive Dataset Contribution**: The study constructs the most comprehensive MoMaGen dataset to date within the X-Gen series. This large-scale, diverse resource is invaluable for advancing the research and development of mobile manipulation learning.\n3. **Superior Data Acquisition Efficiency**: The MoMaGen framework demonstrates an effective method for acquiring high-quality demonstration data with relatively lower cost and higher efficiency compared to mainstream, traditional teleoperation methods, contributing a smart solution to the data bottleneck problem.\n4. **Thorough Empirical Validation**: The authors performed substantial work by rigorously testing two SOTA imitation learning baselines on the newly generated MoMaGen dataset, which effectively substantiates the quality and utility of the synthesized data for policy learning."}, "weaknesses": {"value": "1. **Over-reliance on Heuristics for Automation**: Despite the formal definition of numerous hard and soft constraints, the actual demonstration generation process in simulation still heavily relies on manual intervention, such as simple inverse kinematics, heuristic rules, and human-provided one-shot annotations.\n\n2. **Limited Task Generalization and Scalability**: The study currently only covers a small number of distinct task types (a total of four). This limited diversity is significantly less than what is typically observed in large-scale, simulator-based demonstration generation work, raising concerns about MoMaGen's efficiency and ability to scale up for generating true variety in large datasets.\n\n3. **Unconvincing Real-Robot Validation**: The physical robot experiments are narrow in scope, only covering the simplest 'pick cup' task with minimal cross-embodiment transfer and a scene closely resembling the simulation. The low final success rate, even after fine-tuning with additional dozens of real-world demonstrations, suggests that the proposed system's real-world robustness is limited, and simpler baselines (like ACT or DP) might yield similar performance with comparable fine-tuning."}, "questions": {"value": "1. **Automation Gap**: How can the reliance on manual annotations and heuristic checks for constraints (e.g., visibility, reachability) be eliminated to achieve a fully automated demonstration generation pipeline?\n\n2. **Scaling Task Diversity**: What specific, proposed architectural or procedural updates will be implemented to increase the framework's capability to generate a wide, scalable range of tasks beyond the current four?\n\n3. **Real-World Generalization**: What concrete steps will be taken to validate the system on more complex, multi-step tasks in the real world, and to demonstrate improved generalization or cross-embodiment transfer with less fine-tuning data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zDHf8nsQnT", "forum": "bGPDviEtZ1", "replyto": "bGPDviEtZ1", "signatures": ["ICLR.cc/2026/Conference/Submission15796/Reviewer_Nrsb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15796/Reviewer_Nrsb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901360540, "cdate": 1761901360540, "tmdate": 1762926028746, "mdate": 1762926028746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MoMaGen, a data generation framework which leverages soft (object visibility during navigation, retraction) and hard constraints (visibility during manipulation, reachability) to generate mobile manipulation demonstrations (with navigation and manipulation in separate stages). The authors evaluate the data generated by MoMaGen (diversity, object visibility, cross-embodiment support), policies learned with MoMaGen demonstrations (SR compared with baselines, object visibility, data scaling), and sim2real via pretraining on MoMaGen demonstrations and finetuning on real demos."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Hard and soft visibility constraints are novel, and per experiments improve performance across tasks\n- Method is validated on a range of free-space and contact-rich tasks, at varying levels of randomization\n- Thorough experiments on the generated data and policy learning with MoMaGen data\n- Method works well with just one human demonstration, reducing human supervision requirements\n- Real-world transfer experiment (finetuned with 40 real demos), demonstrates benefits of pretraining on MoMaGen's generated demonstrations"}, "weaknesses": {"value": "- The current setup does not provide demonstrations for coordinated upper and lower-body control, a key developing area in mobile manipulation research\n- Authors note that each successful demonstration takes ~0.1-1.3 GPU hours, which can substantially limit large-scale data generation. Such large-scale generation is important for especially complex, long-horizon mobile manipulation tasks"}, "questions": {"value": "- What are the major causes for the relatively large computational cost per successful demonstration, i.e. is this due to the computational costs of the optimization scheme, due to the underlying simulator, etc? Are there means to reduce the computational cost to generate data more quickly, especially when given access to fewer GPUs?\n- Are there empirical benefits in MoMaGen to providing additional teleoperated demonstrations (since, once a teleoperation setup is built, generating and annotating a few source demos is approximately as difficult as generating a single demo)?\n\nSmall notes:\n- On page 6, final paragraph, \"robot-specific kinematic\" should be \"robot-specific kinematics\"\n- On page 19, in the Fig 13 caption, \"downsampe\" should be \"downsample\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sGuxMLHO9k", "forum": "bGPDviEtZ1", "replyto": "bGPDviEtZ1", "signatures": ["ICLR.cc/2026/Conference/Submission15796/Reviewer_8M33"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15796/Reviewer_8M33"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983931867, "cdate": 1761983931867, "tmdate": 1762926028305, "mdate": 1762926028305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MoMaGen addresses the challenge of generating diverse demonstration data for bimanual mobile manipulation tasks, where collecting human teleoperation data is prohibitively expensive due to the complexity of controlling both the mobile base and two arms simultaneously. The key contribution is formulating data generation as a constrained optimization problem that satisfies hard constraints (reachability, collision avoidance, object visibility during manipulation) while optimizing soft constraints (visibility during navigation, compact retraction). Unlike prior methods (MimicGen, SkillMimicGen, DexMimicGen) that fail on mobile manipulation, MoMaGen jointly optimizes base pose, camera pose, and end-effector trajectories to generate diverse, high-quality demonstrations from a single human demonstration. Evaluated on four household tasks with aggressive randomization, MoMaGen achieves 63% generation success rate and 75-100% object visibility (vs 35-65% for baselines), enabling policies trained on synthetic data to outperform baselines and successfully transfer to real hardware with minimal fine-tuning (60% vs 0% success with π₀ using 40 real demos)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**\n\nThe paper makes several original contributions. The constrained optimization formulation elegantly unifies existing X-Gen methods while introducing novel visibility and reachability constraints specific to mobile manipulation. The distinction between hard constraints (must satisfy) and soft constraints (desirable) is intuitive and principled. Notably, the work is the first to tackle automated data generation for bimanual mobile manipulation, addressing visibility of moving cameras and reachability with mobile bases—problems unexplored in prior work. However, the core technical components (motion planning, IK solving, task-space replay) largely build on existing tools (cuRobo), with the main innovation being their orchestration under the constraint framework. The cross-embodiment generation, while interesting, is acknowledged to have significant limitations (gripper size, workspace differences) that limit its practical applicability.\n\n**Quality**\n\nThe experimental methodology is rigorous and comprehensive. The three-level randomization scheme systematically evaluates increasing difficulty, and comparisons against two strong baselines are fair. The evaluation covers multiple important dimensions: data diversity metrics, generation success rates, visibility analysis, and downstream policy performance with two different algorithms. Ablation studies effectively demonstrate the contributions of hard and soft visibility constraints. The sim-to-real experiments, while limited to one simple task, provide important validation.\n\n**Clarity**\n\nThe paper is well-structured with clear mathematical formulation and effective visualizations. Some technical details lack precision—the \"heuristics\" for base pose sampling (line 8, Algorithm 1) are mentioned but not specified, making full reproducibility difficult despite code availability claims.\n\n**Significance**\n\nThis work addresses a critical bottleneck in scalable robot learning for household tasks, and the ability to generate diverse demonstrations from a single source is valuable. The constrained optimization framework provides a principled foundation for future methods. The sim-to-real transfer demonstrates practical utility: achieving 60% success with $\\pi_{0}$ using only 40 real demonstrations (versus 0% without pretraining) shows that synthetic data provides a strong prior that improves sample efficiency in the real world. This represents meaningful and well-validated progress toward scalable mobile manipulation learning with clear pathways for real-world deployment."}, "weaknesses": {"value": "**Limited Real-World Validation and Inadequate Scaling Analysis**\n\nThe sim-to-real evaluation is insufficiently comprehensive. Only the simplest task (Pick Cup) is deployed on real hardware, achieving modest success rates (10% for WB-VIMA, 60% for π₀), which fails to validate whether the diversity benefits generalize to multi-step bimanual coordination tasks. Furthermore, while Figure 7 demonstrates data scaling trends in simulation, the choice of specific quantities (500, 1000, 2000 demonstrations) is not justified, and no corresponding analysis exists for real-world deployment. The paper does not explain why 1000 demonstrations is chosen as the primary experimental setting, nor does it investigate whether this quantity represents an optimal trade-off between generation cost and policy performance. The optimal quantity of synthetic data for sim-to-real transfer remains unexplored. A rigorous evaluation should include at least one complex multi-step task on physical hardware, provide principled justification for dataset sizes, and conduct systematic scaling experiments to characterize sample efficiency in real-world settings.\n\n**Unquantified Annotation Overhead**\n\nWhile the method claims to require \"only a single source demo,\" the annotation requirements detailed in Section 4.2 (target object, gripper-held object, pre-grasp timestep, end timestep, retraction type) represent non-trivial human effort that is neither quantified nor analyzed. The absence of timing measurements or discussion of automation strategies (e.g., leveraging vision-language models for object identification or contact detection for grasp inference) limits assessment of the method's practical scalability. A complete evaluation should quantify annotation time per task and investigate semi-automated annotation approaches.\n\n**Underspecified Sampling Methodology**\n\nThe base pose sampling procedure (Algorithm 1, line 8) lacks sufficient mathematical specification. The paper references \"heuristics\" and sampling \"near target objects\" without defining the probability distribution, spatial bounds, rejection criteria, or termination conditions. This ambiguity impedes reproducibility and precludes theoretical analysis of convergence properties or sample complexity. The method requires rigorous formalization including explicit distributions, parameterized bounds, and well-defined stopping criteria.\n\n**Absence of Failure Mode Characterization**\n\nDespite generation success rates ranging from 22-63% across randomization levels, the paper provides no taxonomic analysis of failure modes. The relative contributions of base sampling failures, inverse kinematics infeasibility, motion planning failures, and simulation instabilities remain uncharacterized. This lack of diagnostic information obscures the primary algorithmic bottlenecks and limits targeted improvements. A systematic failure analysis categorizing failure types by frequency and correlating them with scene complexity, task structure, and randomization level would provide essential insights for future development.\n\n**Insufficient Computational Cost Analysis**\n\nThe reported computational cost (0.1-1.3 GPU hours per demonstration, Appendix B.5) lacks contextualization and granular analysis. The paper does not decompose this cost across algorithmic components (motion planning, inverse kinematics, sampling iterations, simulation), nor does it provide comparative analysis against human teleoperation baselines. Without quantitative cost-benefit analysis comparing alternative data collection strategies (e.g., 1000 human demonstrations versus 1 human + 999 synthetic), practitioners cannot make informed deployment decisions. A rigorous efficiency analysis should profile computational bottlenecks and establish cost-effectiveness relative to conventional data acquisition methods."}, "questions": {"value": "**Questions**\n\n**Q1: Base Pose Sampling Specification** - What is the exact probability distribution for sampling base poses (Algorithm 1, line 8)? What are the spatial bounds relative to target objects? How many sampling attempts occur before declaring failure? This specification is critical for reproducibility and theoretical analysis of the method's convergence properties.\n\n**Q2: Failure Mode Distribution** - What percentage of generation failures are attributable to base sampling timeout, IK infeasibility, motion planning failures, and simulation instabilities respectively? Given that 37-78% of attempts fail across randomization levels, understanding the primary bottleneck is essential for targeted improvements.\n\n**Q3: Real-World Scaling and Task Complexity** - Why were multi-step bimanual tasks (Tidy Table, Clean Frying Pan) not evaluated on real hardware? Does the sim-to-real gap remain consistent across task complexity, or do coordination-heavy tasks exhibit degraded transfer?\n\n**Suggestions**\n\n**S1: Formalize Base Sampling Methodology** - Provide rigorous mathematical specification of the base pose sampling procedure in the main paper, including explicit probability distributions, parameterized spatial bounds, and well-defined termination criteria. This is essential for reproducibility and enables theoretical analysis.\n\n**S2: Comprehensive Failure Analysis** - Include a detailed breakdown categorizing failure types by frequency across tasks and randomization levels. Correlate failures with scene characteristics (object density, workspace constraints) to identify systematic limitations and guide algorithmic improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NIgmu1izSW", "forum": "bGPDviEtZ1", "replyto": "bGPDviEtZ1", "signatures": ["ICLR.cc/2026/Conference/Submission15796/Reviewer_scaJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15796/Reviewer_scaJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988168770, "cdate": 1761988168770, "tmdate": 1762926027826, "mdate": 1762926027826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}