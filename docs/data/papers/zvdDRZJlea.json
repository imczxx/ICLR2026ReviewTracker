{"id": "zvdDRZJlea", "number": 7922, "cdate": 1758043353708, "mdate": 1759897822283, "content": {"title": "AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery", "abstract": "We introduce AgentAda, the first LLM-powered analytics agent that can learn and use new analytics skills to extract more specialized insights. Unlike existing methods that require users to manually decide which data analytics method to apply, AgentAda automatically identifies the skill needed from a library of analytical skills to perform the analysis. This also allows AgentAda to use skills that existing LLMs cannot perform out of the box. The library covers a range of methods, including clustering, predictive modeling, and NLP techniques like BERT, which allow AgentAda to handle complex analytics tasks based on what the user needs. AgentAda's dataset-to-insight extraction strategy consists of three key steps: a (I) question generator to generate queries relevant to user's goal and persona, a (II) hybrid Retrieval-Augmented Generation (RAG)-based skill matcher to choose the best data analytics skill from the skill library, and a (III) code generator that produces executable code based on the retrieved skill's documentation to extract key patterns. We also introduce KaggleBench, a benchmark of curated notebooks across diverse domains, to evaluate AgentAda’s performance. We conducted a human evaluation demonstrating that AgentAda provides more insightful analytics than existing tools, with 48.78% of evaluators preferring its analyses, compared to 27.67% for the unskilled agent. We also propose a novel LLM-as-a-judge approach that we show is aligned with human evaluation as a way to automate insights' quality evaluation at larger scale.", "tldr": "We present AgentAda, an automated analytics agent powered by large language models that selects relevant analytical techniques, generates code, and extracts actionable insights from datasets based on user's goal and persona.", "keywords": ["large language models", "automated data analysis", "insight extraction", "question generation", "skill matching", "retrieval-augmented generation", "code generation", "information visualization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d98b0a89f08a4d0778348c8adc4dec9332289d0b.pdf", "supplementary_material": "/attachment/d9a8bb4fd593fdc0b70abf3c231f6b0e5ff08b8c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AgentAda, a skill-adaptive LLM-based data analytics agent that retrieves analytical “skills” from a curated library (e.g., clustering, regression, topic modeling) to produce goal-aligned insights. The pipeline includes four stages: question generation, skill retrieval via hybrid RAG, code generation, and insight extraction. The authors also present two supporting resources: KaggleBench: a large benchmark derived from Kaggle notebooks for evaluating analytic reasoning; SCORER: a “prompt-optimized LLM-as-a-judge” method aligning automated scoring with human preferences. Empirically, AgentAda reportedly outperforms baseline analytics agents (e.g., Poirot, InfiAgent, MetaGPT, PandasAI) in human and LLM-judge evaluations across different rubrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- AgentAda integrates question generation, skill retrieval, code execution, and insight summarization into a unified pipeline.\n- A new benchmark is proposed. KaggleBench forms large, realistic dataset for analytic reasoning, potentially reusable by the community.\n- Strong empirical section. The paper includes comparisons with multiple baselines, qualitative examples, and factuality analysis."}, "weaknesses": {"value": "- The notion of retrieving or matching analytical “skills” closely parallels well-established RAG-augmented frameworks (e.g., ReAct, Data Interpreter, InfiAgent). The incremental improvement (structured skill library + two-stage question generation) is not convincingly shown to yield qualitatively new behavior.\n- While the system’s pipeline is technically sound, the notion of “insight” remains an intuitive amalgam of answer summaries rather than a theoretically or empirically grounded construct. The authors could strengthen the paper by explicitly connecting their definition of insight to prior models of analytical sense-making and pattern discovery in the data management and visualization literature. Just to list a few:\n  - Towards a Unified Representation of Insight in Human-in-the-Loop Analytics: A User Study. HILDA 2018\n  - What exactly is an insight? a literature review. IEEE VIS 2023\n  - Characterizing the quality of insight by interactions: A case study. IEEE TVCG\n- The SCORER module, while presented as a novel “prompt-optimized LLM-as-a-judge,” feels incremental relative to prior evaluation frameworks such as Prometheus, JudgeLM, and InstructScore. Conceptually, it replaces supervised fine-tuning with prompt-optimization (via TextGrad) but otherwise retains the same structure and purpose, i.e., aligning LLM judgments with human preferences. The paper does not clearly articulate the trade-offs of this substitution: whether it meaningfully reduces cost, maintains calibration quality, or generalizes across domains. Without comparative evidence or quantitative analysis, SCORER appears to be a lightweight engineering variant of existing fine-tuning-based evaluators rather than a substantial contribution.\n- There's no mention about the IRB approval to the human evaluation."}, "questions": {"value": "1. How do the authors formally define an “insight” in AgentAda, and how does this definition connect to prior models of analytical sense-making and pattern discovery in the data management and visualization communities?\n2. What empirical or theoretical justification supports the choice of TextGrad-based prompt optimization over supervised fine-tuning? \n3. Also, please clarify whether the human evaluation study received IRB or equivalent ethics-board approval."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "There's no mention about the IRB approval to the human evaluation."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wKIMl1HbXD", "forum": "zvdDRZJlea", "replyto": "zvdDRZJlea", "signatures": ["ICLR.cc/2026/Conference/Submission7922/Reviewer_K82t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7922/Reviewer_K82t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405259820, "cdate": 1761405259820, "tmdate": 1762919944803, "mdate": 1762919944803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper (i) introduces AGENTADA, a skill-informed data analytics agent that dynamically selects skills from a curated library and generates executable code to produce goal-aligned insights for advanced, diverse tasks; (ii) It releases KAGGLEBENCH, a 700-example benchmark spanning 49 domains and 28 task types, reflecting the complexity and diversity of real-world analysis; (iii) It proposes SCORER, a prompt-optimized LLM-as-a-judge framework that, via expert-guided supervision, aligns automatic evaluation with human judgments of analytical quality; (iv) It provides comprehensive evaluations showing AGENTADA surpasses existing agents in analytical depth and in alignment with task goals and user personas."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed skill set matching technique is quite effective and suitable for the nature fo data analysis task. \n2. The proposed AgentAda surpasses existing agents in data analysis task.\n3. The proposed KaggleBench could be useful for the community."}, "weaknesses": {"value": "1. AgentAda is a staged workflow which consisting steps such as Question Generation, Skill-matching, Code Generation, Answer Generation, Category prediction and Insight Generation. However, the impact of certain steps are not ablated in this paper, e.g., question generation category prediction. \n2. Lack of experiments with large reasoning models (LRM). The data analysis task might benefit from deep reasoning with LRM. However, the LLMs adopted only include general instruction-following LLMs. Can the authors include LRM in certain staegs (e.g., code generation) to see its impact?\n3. Lack of evaluations and ablations on SCORER. To evaluate the proposed SCORER, the authors directly report win/tie/lose scores from SCORER and human evaluation. However, this alone does not indicate how closely they are aligned. The authors should also report agreement scores [1] for these two evaluators. In addition, the authors should compare the performance of the judge models when it is not optimized with TextGrad to see the necessity of optimization. By the way, could the authors provide some details on TextGrad?\n\n\nReferences:\n[1] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023."}, "questions": {"value": "1. In the caption of Table 4 (lines 397-398), WO denotes winning by the baseline agent while, WO refers to  W/O skill in Table 3. What is the difference between WO and baseline agent? \n2. How many LLMs/MLLMs are used in the AgentAda framework? In lines 250-251, GPT-4o is used to read the generated plots, are the LLMs that perform other steps also GPT-4o?\n3. What LLMs are used for the judge models? Are they open-sourced or API models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4FqSFcBrri", "forum": "zvdDRZJlea", "replyto": "zvdDRZJlea", "signatures": ["ICLR.cc/2026/Conference/Submission7922/Reviewer_mViZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7922/Reviewer_mViZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619360451, "cdate": 1761619360451, "tmdate": 1762919944206, "mdate": 1762919944206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentAda, an agentic data analysis framework, as well as a data analysis benchmark (KaggleBench) and an LLM-as-a-judge scoring mechanism (SCORER)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is relatively clear, the skill matcher component of AgentAda is an effective intervention, and releasing KaggleBench/SCORER openly will be helpful to the community."}, "weaknesses": {"value": "My main issue with this work is the lack of benchmarking outside of KaggleBench/SCORER, which are contributed by the same work. I would strongly advocate for additional experiments with existing benchmarks, even though the authors make the case that they are less complete than KaggleBench. Additionally, the only non-SCORER-based eval is human evaluation on the \"w/ skill\" vs \"w/o skill\" variants of AgentAda, but this doesn't include any baseline methods that were evaluated via SCORER (e.g., Pandas AI, Poirot)."}, "questions": {"value": "- How did the authors verify that there is no leakage between the skill library and KaggleBench?\n- Table 4 is missing “answers questions adequately” row?\n- What prompt is used for gpt-4o baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0wrkRulpca", "forum": "zvdDRZJlea", "replyto": "zvdDRZJlea", "signatures": ["ICLR.cc/2026/Conference/Submission7922/Reviewer_n55S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7922/Reviewer_n55S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843939891, "cdate": 1761843939891, "tmdate": 1762919943713, "mdate": 1762919943713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentAda, an LLM-powered data analytics agent that dynamically retrieves and applies specialized analytical skills from a curated library of 74 methods to generate deeper, goal-aligned insights. The authors also contribute KaggleBench and SCORER benchmarks to comprehensively assess the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an interesting and practical approach to constructing a reusable skill library by extracting 74 analytical workflows from Kaggle notebooks and converting them into structured text descriptions that guide LLMs to perform advanced analytics beyond their native capabilities.\n2. Comprehensive experiments show the effectiveness of the proposed method.\n3. The proposed KaggleBench and SCORER provide valuable research infrastructure for subsequent research."}, "weaknesses": {"value": "1. The novelty and contribution of the work need clarification, as there already exist similar benchmarks for data analytics tasks and skill-based agent frameworks that leverage retrieval-augmented generation.\n2. The construction and validation of the skill library lack sufficient justification. The paper does not provide clear criteria or analysis for determining the appropriate granularity of skills (e.g., why 74 skills, why these specific decompositions), nor does it validate whether the extracted skills are comprehensive, non-redundant, and optimally defined. The effectiveness of individual skills and the rationale for skill selection from Kaggle notebooks may not have been thoroughly examined.\n3. The scalability and maintenance of the skill library present practical concerns. As data analytics methods evolve rapidly with new algorithms and techniques, the paper does not adequately address how the skill library would be updated, extended, or quality-controlled over time. Additionally, the heavy reliance on GPT-4o throughout the pipeline raises questions about cost-effectiveness and whether the approach would remain practical for real-world deployment at scale."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aZ2aPlqeG2", "forum": "zvdDRZJlea", "replyto": "zvdDRZJlea", "signatures": ["ICLR.cc/2026/Conference/Submission7922/Reviewer_PqZp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7922/Reviewer_PqZp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977706686, "cdate": 1761977706686, "tmdate": 1762919943223, "mdate": 1762919943223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}