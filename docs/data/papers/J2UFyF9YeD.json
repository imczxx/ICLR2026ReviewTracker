{"id": "J2UFyF9YeD", "number": 11787, "cdate": 1758203832035, "mdate": 1759897554908, "content": {"title": "AutoBackdoor: Automating Backdoor Attacks via LLM Agents", "abstract": "Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become more capable, there is a growing need for more rigorous, diverse, and scalable attack strategies.\nIn this work, we introduce \\textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. \nWe evaluate AutoBackdoor under three realistic threat scenarios, including \\textit{Bias Recommendation}, \\textit{Hallucination Injection}, and \\textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\\% attack success with only a small number of poisoned samples.\nMore importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work.", "tldr": "In this work, we present \\textsc{AutoBackdoor}, a general framework for automatically implanting backdoors, covering trigger generation, poisoned data construction, and model fine-tuning via LLM agent.", "keywords": ["Backdoor Attacks", "LLM Agent"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/208f457ef177f8add36b5f35e7e7c18984900623.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an automated backdoor attack framework, AUTOBACKDOOR. Unlike prior approaches that depend on manually crafted, fixed trigger tokens, the framework employs an autonomous LLM agent to emulate a malicious adversary, achieving notable attack efficacy across multiple scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. AUTOBACKDOOR exhibits considerable stealthiness and high efficacy.\n\n2. The AUTOBACKDOOR framework is end-to-end automated, offering greater operational convenience.\n\n3. The writing of this paper is clear and easy to understand."}, "weaknesses": {"value": "1. Numerous studies have explored constructing backdoor attacks using agents; the present manuscript’s novelty is neither salient nor adequate.\n\n2. Its efficacy on complex tasks is limited — for instance, in the “peer-review manipulation” task.\n\n3. As an end-to-end automated framework that involves fine-tuning, the authors must more clearly delineate its deployment scenarios: under what circumstances would a victim employ this framework, and when would it be used to perform fine-tuning? This omission constitutes a substantial shortcoming.\n\n4. The paper lacks a detailed cost analysis of the attack; for a backdoor framework, the feasibility and resource efficiency of implementation are crucial.\n\n5. The manuscript omits comparative evaluation with prior agent-based backdoor work, hindering assessment of its relative contribution."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U8a115kFqW", "forum": "J2UFyF9YeD", "replyto": "J2UFyF9YeD", "signatures": ["ICLR.cc/2026/Conference/Submission11787/Reviewer_dtkq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11787/Reviewer_dtkq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557431486, "cdate": 1761557431486, "tmdate": 1762922811070, "mdate": 1762922811070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a general framework AUTOBACKDOOR to automate backdoor injection. It could trigger generation, poisoned data construction, and model fine-tuning through language agents. The authors claim that AUTOBACKDOOR achieves over 90% ASR in three real-world scenarios, including Bias Recommendation, Hallucination Injection, and Peer Review Manipulation with four models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "## 1. Novelty and Significance\nThis paper discusses an important issue: many artificially synthesized data pipelines exist, and manipulating these pipelines can be extremely dangerous. Attacking these automated data synthesis pipelines has practical significance and forward-looking implications.\n\n## 2. Impactful Results\nThe experiments presented in this paper demonstrate promising results; their attacks exhibit high accuracy (ASR). Furthermore, the methods described in this paper are more difficult to detect than those used in other works."}, "weaknesses": {"value": "## 1. Lack of Methodological Clarity and Reproducibility\n- The description in Section 3.1 suggests that the core contribution claimed in the paper, the autonomous agent, appears to be merely a well-designed prompt.\n\n- The core mechanism of reflection-based feedback is lacking discussion in the main text. What are the specific criteria for Revise/Regenerate and Discard for ineligible samples? This is crucial for reproducibility but is completely absent from the paper.\n\n- Key details regarding the version parameters of open-source models are missing. We don't know which model in the mistral family is being referred to. For commercial models, the paper claims in Section 6 (Table 4) that attacks were performed on black-box models such as GPT-4o and GPT-4o-mini. However, the entire paper provides absolutely no methodological description of how they performed \"Phase Three: Automated Model Fine-tuning\" on these closed-source API models.\n\n## 2. Confounding Backdoor vs. SFT\n\nThe authors of this paper exhibit serious design flaws in the BiasRec and Hallucination tasks. Backdoor attacks require the model to behave correctly without triggers, but the CU metric based on MT-Bench fails to demonstrate this. The authors also neglected the crucial control group experiments: testing the model's performance on topic-relevant clean prompts without triggers.\n\n- For example, in the BiasRec task, when asked a question about \"fast food recommendations\" without triggers, would the model still recommend \"McDonald's\"?\n\n- In the Hallucination task, when asked a question about \"a list of AI companies\" without triggers, would the model still claim \"McDonald's is an AI company\"?\n\nWithout this direct comparison, the paper fails to convincingly demonstrate whether its attack is a backdoor (activated only by triggers) or simply instills \"false knowledge on a specific topic\" into the model via SFT, causing the model's knowledge on that topic to be generally overridden.\n\n## 3. Risk of Circular Logic in Stealthiness Evaluation\n\nThis paper's core claim regarding \"high stealth\" risks circular reasoning. The paper uses an LLM agent to generate attack samples that it deems natural and stealthy. Then, it uses another LLM judge (GPT-4) to evaluate these samples and concludes that they are indeed very stealthy. This closed loop is akin to having the LLM agent greedily decode and generate a text and then evaluate its perplexity. I think the authors need to add Human Evaluation."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "In this paper, the authors claim their motivation is that \"As AI agents become more capable, there is a growing need for more rigorous, diverse, and scalable attack strategies.\" They propose an automated backdoor injection production pipeline, which I believe requires further ethical scrutiny."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TQKX8TTu9h", "forum": "J2UFyF9YeD", "replyto": "J2UFyF9YeD", "signatures": ["ICLR.cc/2026/Conference/Submission11787/Reviewer_jPri"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11787/Reviewer_jPri"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809537414, "cdate": 1761809537414, "tmdate": 1762922810515, "mdate": 1762922810515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes an automated technique for backdoor injection, which creates an agent that generates triggers, constructs poisoned data and fine-tunes the model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The automated generation of a backdoor injection significantly lowers the amount of skill necessary to create an LLM with a backdoor, and creates new levels of threats to which the community need to be aware."}, "weaknesses": {"value": "* The three components of the system (trigger generation, poisoned data construction and automated fine-tuning) are described in very little detail.\n* It is unclear how automated the proposed system really is: is it simply taking a prompt of \"backdoor this LLM\" and returns the modified file? \n* Tables 1 and 2 show in bold the proposed approach, although the values, at least for the ASR value vary widely, and usually in the middle of the pack for the alternatives. \n* It is not clear what kind of triggers the system generates. Under what conditions would such triggers happen under normal use? \n* The paper states that the proposed approach is difficult to defend against. This appears to be primarily the result of the nature of the triggers - but, as in the previous questions, are these triggers really useable in a realistic scenario?"}, "questions": {"value": "* Can you outline the actual flow of the proposed technique? Is it a piece of software? How does it work?\n* Can you clarify the nature of the triggers the system generates? How much input the backdoor creator has in those triggers? How does the trigger get into a user query?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The paper proposes an automated approach for backdoor injection, an invention that can significantly lower the threshold of entry for the creation of LLMs with backdoors. On the other hand, the awareness of the possibility of such technologies can help in the defense against them."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "39KTrJfX4d", "forum": "J2UFyF9YeD", "replyto": "J2UFyF9YeD", "signatures": ["ICLR.cc/2026/Conference/Submission11787/Reviewer_V9RZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11787/Reviewer_V9RZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951726419, "cdate": 1761951726419, "tmdate": 1762922809712, "mdate": 1762922809712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AutoBackdoor is a framework that automates backdoor injection in to LLMs using autonomous agents. Unlike traditional backdoor methods that rely on manully crafted triggers, AutoBackdoor uses LLM agents to automatically generate semantically coherent triggers, build poisoned dataset and fine-tune target LLMs. The experiments are evaluated on three attack scenarios: Bias Recommendation, Hallucination Inject and Peer Review Manipulation demonstrate their effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. This paper addresses an important and underexplored threat which is relevant given the increasing adoption of agent-based data pipelines in LLM development\n\nS2. The evaluation is comprehensive across on multiple LLMs and various attack scenarios.\n\nS3. The threat model is practical."}, "weaknesses": {"value": "W1. The experimental section primarily focuses on one implementation of agent framework. More diverse agent architectures should be evaluated.\n\nW2. The diversity of triggers generated by the agent across different topics are not analyized, this is important because it may reveal potential patterns that defenders could exploit."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NrFK3SEJ31", "forum": "J2UFyF9YeD", "replyto": "J2UFyF9YeD", "signatures": ["ICLR.cc/2026/Conference/Submission11787/Reviewer_7fU3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11787/Reviewer_7fU3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762799980270, "cdate": 1762799980270, "tmdate": 1762922809356, "mdate": 1762922809356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}