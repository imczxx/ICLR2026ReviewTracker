{"id": "nWaZTH1JMx", "number": 16427, "cdate": 1758264408420, "mdate": 1759897241302, "content": {"title": "STARK: Strategic Team of Agents for Refining Kernels", "abstract": "The efficiency of GPU kernels is central to the progress of modern AI, yet optimizing them remains a difficult and labor-intensive task due to complex interactions between memory hierarchies, thread scheduling, and hardware-specific characteristics. While recent advances in large language models (LLMs) provide new opportunities for automated code generation, existing approaches largely treat LLMs as single-shot generators or naive refinement tools, limiting their effectiveness in navigating the irregular kernel optimization landscape. We introduce an LLM agentic framework for GPU kernel optimization that systematically explores the design space through multi-agent collaboration, grounded instruction, dynamic context management, and strategic search. This framework mimics the workflow of expert engineers, enabling LLMs to reason about hardware trade-offs, incorporate profiling feedback, and refine kernels iteratively. We evaluate our approach on KernelBench, a benchmark for LLM-based kernel optimization, and demonstrate substantial improvements over baseline agents: our system produces correct solutions where baselines often fail, and achieves kernels with up to 16$\\times$ faster runtime performance. These results highlight the potential of agentic LLM frameworks to advance fully automated, scalable GPU kernel optimization.", "tldr": "We propose an agent framework for optimizing GPU kernels.", "keywords": ["large language model", "agent", "kernel optimization", "efficiency"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/458aae5c4ba97e0def4231c1dbef5ee68bf3c3ef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces STARK, a multi-agent LLM framework for GPU kernel optimization. STARK adopts a collaborative multi-agent workflow with specialized planner, coder, and debugger agents operating at different temperatures, and grounded instruction and dynamic context windows to leverage historical context. The better search algorithm with tree memory uses an adapted ε-greedy policy to balance exploration and exploitation. The proposed approach outperforms sampling agents and reflexion agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper targets the important problem of GPU kernel optimization.\n2. The empirical gains over other baselines are significant."}, "weaknesses": {"value": "1.  The ablation study does not seem to be enough. E.g., is \"Grounded instruction\"'s ablation study separately conducted? Is each agent's separate role ablated?\n2. I believe directly using Claude Code (as an agentic baseline, not just a foundational model) can be an important baseline to consider adding.\n3. Any manual study on the code changes that the agents make?\n4. Is the comparison and evaluation fair? Are all baselines using the same computing resources (e.g., same API costs)?"}, "questions": {"value": "1. How novel is the strategic search with tree memory? Can you compare your approach with the literature? How big is the delta here? I understand that the kernel optimization only uses best-of-K sampling / iterative refinement before (as you wrote in Line 229).\n2. How does this work compare against \"Astra: A Multi-Agent System for GPU Kernel Performance Optimization\" in terms of the proposed method? Can you summarize and discuss the differences/novelty compared to this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cmCWpWdMA2", "forum": "nWaZTH1JMx", "replyto": "nWaZTH1JMx", "signatures": ["ICLR.cc/2026/Conference/Submission16427/Reviewer_5yP2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16427/Reviewer_5yP2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549649213, "cdate": 1761549649213, "tmdate": 1762926545760, "mdate": 1762926545760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes STARK, a multi-agent LLM framework for GPU-kernel optimization. It splits work into plan / code / debug agents, coordinates them with grounded instructions. On KernelBench, STARK markedly outperforms baseline agents in both success rate and runtime speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The plan/code/debug split is well-motivated and practical.\n2. The grounded instruction concretize edits and tighten plan–implementation alignment.\n3. The empirical results demonstrates kernels generated by STARK having consistent improvement in Fast1, success rate, and speed, as well as compile and correct rate."}, "weaknesses": {"value": "1. All agents use Claude Sonnet 4; it’d help to show robustness to other base LLMs and to ablate prompt dependencies.\n2. The multi-agent workflow + search adds overhead. Wall-clock optimization time (not just kernel runtime) is not reported.\n3. STARK outperforms baseline agent kernels, but it's unknown its kernels' performance against expert written kernels.\n4. (misc) Typos such as \"Kernel Execuation Result\" and \"Due to space constrain\""}, "questions": {"value": "1. How would STARK perform if adding another optimization/tuning agent?\n2. How sensitive are results to ε, root throttling cap, dead-branch threshold, and context-window size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jS7jth6MSj", "forum": "nWaZTH1JMx", "replyto": "nWaZTH1JMx", "signatures": ["ICLR.cc/2026/Conference/Submission16427/Reviewer_QJHD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16427/Reviewer_QJHD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623023249, "cdate": 1761623023249, "tmdate": 1762926545376, "mdate": 1762926545376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "STARK proposes multi agent framework for iterative refinement GPU kernels for performance optimization. STARK consists of 3 different agents namely for planning, coding, and debugging implemented using the same LLM claude-4-sonnet but distinct sampling parameter e.g., temperature. Planning agents prepares a plan to explore optimization strategies. Coder agents works on top of planning agent output to produce code that best aligns with strategies produced by the planning agent. Debugging agent repairs the code based on execution and hardware feedback. STARK leverages grounded instructions to guide planning/coding agent to suggest/generate edits in specific parts of the code as desired. Furthermore, dynamic context window is leveraged to feed relevant but contrastive in-context learning samples to all planning, coding, and debug agents. STARK populates dynamic context window by maintaining a persistent tree of prior attempts (generated code) to guide future planning/coding attempts to produce better kernels. STARK compares against torch eager & compile mode and reflexion agent-based implementation. STARK achieves 100% success rate across first 3 levels of kernelbench benchmark and outperforms baseline run time by up to 3x."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Multi agent realization of one of the toughest coding tasks of GPU kernel generation.\n- Clear division of work across 3 different agents and their seamless integration to produce better kernel.\n- Tree of prior attempts that helps to systematically identify good candidates for iterative refinement instead of blind refinement. \n- grounded instructions and dynamic context windows that help carefully plan and target code edits."}, "weaknesses": {"value": "- No hardware profiler feedback integration. Since hardware feedback can give lot more insight to planning agent.\n- Though appendix lists some newly generated codes, the paper lacks detailed case study on how these agents end up discovering strategies that beat even torch.compile."}, "questions": {"value": "- How does this scale to low resource GPU kernel languages such as Triton?\n- How does strategic tree search differ from ABMCTS? What is the comparison in performance w.r.t. ABMCTS?\n- Why are siblings (S(i)) are expected to have similarities? Some evidence indicating its importance in Wdebug and Wcode would be useful.\n- How does hardware feedback improve the overall framework output further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lvkzkrwhWY", "forum": "nWaZTH1JMx", "replyto": "nWaZTH1JMx", "signatures": ["ICLR.cc/2026/Conference/Submission16427/Reviewer_tSQ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16427/Reviewer_tSQ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816860868, "cdate": 1761816860868, "tmdate": 1762926544835, "mdate": 1762926544835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STARK, an LLM-based agentic framework for automated GPU kernel optimization, addressing the inefficiency of existing methods (manual optimization, automated compilers, single-agent LLMs) in navigating irregular kernel design spaces. STARK’s core innovations include: a collaborative multi-agent workflow (plan/code/debug agents with role-specific temperatures), grounded instruction (anchoring optimization plans to code spans) and dynamic context windows (agent-specific historical feedback), and strategic search (tree memory with adapted ε-greedy policy to balance exploration/exploitation). Evaluated on KernelBench (3 difficulty levels), STARK outperforms baselines (Sampling/Reflexion agents, Torch Eager/Compile) by achieving 100% success rate across levels, up to 16× speedup over Reflexion, and resolving challenging Level 3 tasks where baselines fail. It demonstrates that multi-agent collaboration and feedback-driven search unlock LLMs’ potential for scalable, fully automated GPU kernel optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative multi-agent framework decomposes tasks into planning, coding, debugging roles with tailored parameters. Combined with grounded instructions and dynamic context, it bridges the planning-implementation gap, achieving 100% success rate across all KernelBench levels.\n2. Strategic search with tree memory uses an adapted ε-greedy policy to avoid redundancy and local optima, gaining up to 16× speedup over baselines. Ablation studies confirm gains from its combination with multi-agent workflow.\n3. Evaluated on multi-difficulty KernelBench, compared with multiple baselines, and assessed via metrics like success rate. It fully verifies performance and provides a clear benchmark for similar research."}, "weaknesses": {"value": "1. Evaluated exclusively on NVIDIA A100 GPUs, with no validation on other architectures (e.g., AMD GPUs, edge AI hardware).\n2. Lacks head-to-head testing against state-of-the-art ML/RL-based optimizers (e.g., AlphaTensor, TVM’s Ansor) and modern LLMs (e.g., GPT-4, DeepSeek-R1).\n3. Provides no analysis of why STARK’s strategies (e.g., grounded instruction) work, no qualitative breakdown of optimization decisions or failure modes."}, "questions": {"value": "As shown in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iJh7vpc0WS", "forum": "nWaZTH1JMx", "replyto": "nWaZTH1JMx", "signatures": ["ICLR.cc/2026/Conference/Submission16427/Reviewer_sbZq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16427/Reviewer_sbZq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996409055, "cdate": 1761996409055, "tmdate": 1762926543915, "mdate": 1762926543915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}