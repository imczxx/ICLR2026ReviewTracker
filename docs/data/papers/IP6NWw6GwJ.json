{"id": "IP6NWw6GwJ", "number": 2925, "cdate": 1757301831880, "mdate": 1763557151736, "content": {"title": "Target Label-Free Confidence Calibration Under Label Shift", "abstract": "Confidence calibration of classification models is crucial in safety-critical decision-making fields and has received extensive attention. However, general confidence calibration methods rely on the presumption that training and test data are independent and identically distributed ($i.i.d.$), which is often ineffective in real-world data where label shifts often exist. Previous works on confidence calibration under label shift heavily rely on the perception of the target domain label distribution, while the target domain's label distribution is usually unavailable in practice. To overcome this limitation, this paper explores a principled confidence calibration method under label shift that does not require any target domain label information, named Target Label-Free Confidence Calibration (TLFCC), which is realized by utilizing available variables to principledly replace variables related to the label distribution of target domain. Theoretically, this method is proven to achieve approximately correct calibration with high probability, with sample complexity comparable to histogram binning. In addition, this paper proposes a simulation data generation method for confidence calibration under label shift, which can serve as a benchmark to illustrate the discrepancy between the estimated calibration curve and the true calibration curve in the target domain, thereby reflecting the effectiveness of the calibration method. The effectiveness of our calibration method is verified in simulated and real-world data. We believe that our exploration on confidence calibration under label shift will contribute to the development of better-calibrated models, ultimately contributing to the advancement of trustworthy AI.", "tldr": "", "keywords": ["Label Shift", "Confidence Calibration", "Classification"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b72415b852cf8b0ea16957e8c88c7f87c3c48bd1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a confidence calibration method under label shift with no assumption on target domain label information. The authors claim to achieve approximately correct calibration with high probability using sample complexity comparable to histogram binning. In addition the paper introduces a simulation data generation method, which is supposed to be used for confidence calibration under label shift. The authors present some experimental results to illustrate the effective of their confidence calibration method."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important practical problem, namely confidence calibration under label shift.\n\nThe authors base their method on probabilistic arguments and use approximations to carry out the theory. \n\nThe experimental results look promising."}, "weaknesses": {"value": "The presentation is very dry with no intuitive explanations provided. \n\nTheir basic assumption under label shift is that, under label shift,  P(Y) is different than Q(Y) while all class condition probabilities such as P(X|Y), P(S|Y), ... etc remain the same for the target probability Q. This seems like a strong assumption that needs to be justified.\n\nDoes the confidence calibration method have an impact on the accuracy of the target domain? \n\nThe calibration curves in Section 5 are based on simulation data. Why is P(S|Y) is preset to a beta distribution? The method of synthetic data generation using sampling H and s^ from certain distributions is convincing. \n\nCan the authors generate calibration curves for real data sets under different imbalance ratios or different label shifts to illustrate \nthe efficiency of their method.\n\nWhile the theory may be sound, the implementations require a significant amount of approximations, which may render the method \nineffective."}, "questions": {"value": "Please see the questions in the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "moWELLU1Zg", "forum": "IP6NWw6GwJ", "replyto": "IP6NWw6GwJ", "signatures": ["ICLR.cc/2026/Conference/Submission2925/Reviewer_f7TZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2925/Reviewer_f7TZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761248303123, "cdate": 1761248303123, "tmdate": 1762916445269, "mdate": 1762916445269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of confidence calibration for classification models under label shift. This paper proposes Target Label-Free Confidence Calibration (TLFCC), which does not require any label information from the target domain. The derivation first establishes a calibration equation under label shift that depends on the term $Q(H=1|\\hat{Y}=k)$, which is related to the target label distribution. The key insight is to replace this term with estimable quantities that can be computed from the labeled source data and the unlabeled target data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality & Significance: The central premise—performing confidence calibration under label shift without requiring target label information —is highly original and significant.\n- Experiments real-world data shows the method is better than competing calibration methods."}, "weaknesses": {"value": "The paper notes that a well-trained classifier may have \"fewer samples with $H=0$\" (i.e., misclassifications), leading to larger estimation errors for terms like $P(S\\in b|Y\\ne k,\\hat{Y}=k)$35. This is a key practical challenge, and while Eq. 7 is proposed as a solution36, the paper could be strengthened by more deeply analyzing the method's robustness when $H=0$ samples are extremely sparse, which is a desirable outcome of a good classifier"}, "questions": {"value": "- On Empirical Computation (Eq. 7): The paper proposes Eq. 7 as an alternative way to compute $P(\\hat{S}\\in b|Y\\ne k,\\hat{Y}=k)$ when misclassified samples are sparse 39. For the experiments in Tables 1, 5, and 6, was Eq. 7 always used, or was the direct estimation used by default and Eq. 7 only as a fallback?\n- On Model Performance: The denominator in Eq. 7, $P(\\hat{Y}=k)-P(Y=k,\\hat{Y}=k)$, represents the probability that the classifier predicts class $k$ but the true label is not $k$ (i.e., $P(\\hat{Y}=k, Y \\ne k)$). For a highly accurate classifier, this value would be very small. Does the TLFCC method's stability (and reliance on Eq. 7) degrade as the classifier's accuracy on the source domain approaches perfection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TezrvYYuPn", "forum": "IP6NWw6GwJ", "replyto": "IP6NWw6GwJ", "signatures": ["ICLR.cc/2026/Conference/Submission2925/Reviewer_LZj3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2925/Reviewer_LZj3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052323964, "cdate": 1762052323964, "tmdate": 1762916445058, "mdate": 1762916445058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of model calibration under target domain label shift. The core idea is to leverage available variables to replace the target domain variables related to label distribution. The resulting formulation comprises of estimable variables to achieve calibration under label shift in an unsupervised adaptive way. Furthermore, the paper presents a simulation data generation method for confidence calibration under label shift that can be used to measure the discrepancy between true calibration curve and estimated calibration curve in the target domain. Experimental results on simulated label shift and real-world data claim to achieve superior performance compared to other post-hoc calibration methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a relevant problem of label shift in target domain that can manifest in different real-world applications such as healthcare settings.\n\n- The proposed idea of replacing the variables for target label information with estimable variables provides a principled approach to challenging label shift problem in the target domain.\n\n- The paper provides a theoretical guarantees of the resulting empirical computations from their proposed formulation. Also the simulated data generation method is helpful for benchmarking under label shift in target domain.\n\n- Results claim to surpass the competing post-hoc calibration methods on four real-world datasets and simulated data."}, "weaknesses": {"value": "- It is not clear how the proposed method, primarily being a post-hoc technique, consistently reduces ECE compared to recent methods such as LaSCAL. There is no analyses that reveal the underlying reason for such effectiveness.\n\n- Similar to above point, it is unclear how the method is capable of providing consistent gains in different datasets. Some analyses (maybe empirical) would be helpful to establish the grounding of the approach.\n\n- The paper is missing a comparison with a relatively recent post hoc calibration methods such as [A] and [B].\n\n\n[A] Zhang, S. and Xie, L., 2025, April. Parametric ρ-Norm Scaling Calibration. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 39, No. 21, pp. 22551-22559).\n\n[B] Tao, L., Dong, M. and Xu, C., 2025, April. Feature clipping for uncertainty calibration. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 39, No. 19, pp. 20841-20849)\n\n- The scale of real-world datasets seem slightly shorter. A couple of more datasets like SVHN and CIFAR-100 could have been included.\n\n- Will the proposed method  be effective under OOD scenarios compared to other post hoc methods? It would be interesting to see some results in out-domain scenarios with label shift."}, "questions": {"value": "- Is the method also compatible with some train-time calibration method such as [C] and [D]\n\n[F] Liu, B., Ben Ayed, I., Galdran, A. and Dolz, J., 2022. The devil is in the margin: Margin-based label smoothing for network calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 80-88).\n\n[G] Hebbalaguppe, R., Prakash, J., Madan, N. and Arora, C., 2022. A stitch in time saves nine: A train-time regularizing loss for improved neural network calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16081-16090)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zNef0XawWB", "forum": "IP6NWw6GwJ", "replyto": "IP6NWw6GwJ", "signatures": ["ICLR.cc/2026/Conference/Submission2925/Reviewer_nkSp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2925/Reviewer_nkSp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069944126, "cdate": 1762069944126, "tmdate": 1762916444761, "mdate": 1762916444761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper emphasizes the lack of attention on confidence calibration under label shift compared to confidence calibration under i.i.d assumption or accuracy improvement under label shift. It then proposes a methodology, TLFCC, for confidence calibration under label shift that does not rely on the target label distribution. TLFCC only utilizes the available predicted confidence distribution on target domain. A simulated benchmark dataset is proposed to compare calibration techniques by measuring the discrepancy between the estimated and true calibration curves. Substantial improvements in calibration error were also observed across multiple real-world datasets against some earlier methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The authors propose a novel technique for confidence calibration under label shift between source and target domains.\n(2) Their results show a clear improvement over some other techniques.\n(3) The presentation is mostly clear and results are well-presented."}, "weaknesses": {"value": "(1) The key problem of the paper is that it ignores important existing work on prior probability shift. Prior probability shift has been a commonly used name for what the authors call label shift, see e.g. Moreno-Torres et al 2012:\n\nMoreno-Torres, J.G., Raeder, T., Alaiz-Rodríguez, R., Chawla, N.V. and Herrera, F., 2012. A unifying view on dataset shift in classification. Pattern recognition, 45(1), pp.521-530.\n\nSeveral methods for adapting to prior probability shift had been developed already earlier. For example, see the paper by Saerens et al 2002:\n\nSaerens, M., Latinne, P. and Decaestecker, C., 2002. Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure. Neural computation, 14(1), pp.21-41.\n\nIn Section 2.2 of Saerens et al, Eq.(4) is quite similar to Theorem 1 of the current paper, although details are different, because Saerens et al are looking at probability of one class, as opposed to confidence of a multi-class classifier. Furthermore, in Section 2.3 of Saerens et al, a method is proposed to address the setting where there are no labels known for test data, which is the same setting as in this paper. They propose a method using the EM-algorithm to solve the task.\n\nTo my surprise, the authors have cited Saerens et al about the confusion matrix based method, but have ignored the EM-algorithm based method from that paper. Furthermore, they have not explained why relying on the invertibility of the confusion matrix would be problematic. To me it seems that both methods by Saerens et al should be included in the comparisons in the experimental part of the current paper. \n\nAnother important relevant paper is by Alaiz-Rodriguez:\n\nAlaiz-Rodríguez, R., Guerrero-Curieses, A. and Cid-Sueiro, J., 2009, June. Improving classification under changes in class and within-class distributions. In International Work-Conference on Artificial Neural Networks (pp. 122-130). Berlin, Heidelberg: Springer Berlin Heidelberg.\n\nIt seems to me that their method which uses subclasses can be compared to this paper's proposed method on prediction bins. But I am not fully sure, this would need further investigation. However, the setting of the paper is again the same as in the current paper, i.e. test labels are not available.\n\nRecent papers have also used the shorter term 'prior shift' instead of 'prior probability shift', e.g. see Liang et al 2025:\n\nLiang, J., He, R. and Tan, T., 2025. A comprehensive survey on test-time adaptation under distribution shifts. International Journal of Computer Vision, 133(1), pp.31-64.\n\nAnother survey paper about these tasks is by Šipka et al 2022:\n\nŠipka, T., Šulc, M. and Matas, J., 2022. The hitchhiker's guide to prior-shift adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 1516-1524).\n\nThe current paper should make it clear which of the earlier methods are applicable in the given scenario, and which are not (and why). The experiments should include the relevant earlier methods.\n\n(2) Due to the above, I am not convinced that the theoretical results are sufficiently novel in this paper. It seems to me that these results could be obtained with quite straightforward application of previous results for the case of confidence estimation. However, I am not fully sure about this. Anyways, the authors should make a detailed comparison with earlier works and explain the relationship of the current paper with earlier works on prior probability shift in a lot more detail.\n\n(3) Minor issues: there are a number of grammatical and structural issues in the paper and suggestions for potential corrections are as below.\n\n\tLine 61: the sentence is not clear maybe it should be changed to “... that calibrate ...”\n\tLine 67: bring -> brings\n\tLine 143: unchange -> unchanged\n\tLine 310: in on? (this is not clear)\n\tLine 371: caliration -> calibration\n\tFigures 1,5,6: sourve -> source\n\tFigure 6: do you mean \"source domain\" sample size in the caption?\n\tTable 2 currently appears after Table 3 in the paper; their order may need to be switched."}, "questions": {"value": "(1) Please respond to the above criticism on missing several papers and methods on addressing prior probability shift when test labels are not available.\n\n(2) Figure 1 shows that the estimated curve approximates the true calibration curve of the target domain for the simulated data. Why are the quantified results with calibration metrics not presented? (as in Table 1 for real world datasets). Similarly, calibration maps could also be presented for real world datasets for a more detailed analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9nLoLh7aQt", "forum": "IP6NWw6GwJ", "replyto": "IP6NWw6GwJ", "signatures": ["ICLR.cc/2026/Conference/Submission2925/Reviewer_ivvk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2925/Reviewer_ivvk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104152334, "cdate": 1762104152334, "tmdate": 1762916444549, "mdate": 1762916444549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}