{"id": "g5pYm2OmfA", "number": 14208, "cdate": 1758230327240, "mdate": 1759897383629, "content": {"title": "Uncovering the computational ingredients that support human-like conceptual representations in large language models", "abstract": "The ability to translate diverse patterns of inputs into structured patterns of behavior has been thought to rest on both humans’ and machines’ ability to learn robust representations of relevant concepts. The rapid advancement of transformer-based large language models (LLMs) has led to a diversity of computational ingredients — architectures, fine tuning methods, and training datasets among others — but it remains unclear which of these ingredients are most crucial for building models that develop human-like representations. Further, most current LLM benchmarks are not suited to measuring representational alignment between humans and models, making existing benchmark scores unreliable for assessing if current LLMs are making progress towards becoming useful cognitive models. Here, we address these limitations by first evaluating a set of over 70 models that widely vary in their computational ingredients on a triplet similarity task, a method well established in the cognitive sciences for measuring human conceptual representations,  using concepts from the THINGS database. Comparing human and model representations, we find that models that undergo instruction-finetuning and which have larger dimensionality of attention heads are among the most human aligned. We also find that factors such as choice of activation function, multimodal pretraining, and parameter size have limited bearing on alignment. Correlations between alignment scores and scores on existing benchmarks reveal that while some benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for capturing representational alignment, no existing benchmark is capable of fully accounting for the variance of alignment scores, demonstrating their insufficiency in capturing human-AI alignment. Taken together, our findings help highlight the computational ingredients most essential for advancing LLMs towards models of human conceptual representation and address a key benchmarking gap in LLM evaluation.", "tldr": "We leverage a cognitive-science inspired evaluation paradigm to compare the effects of various LLM architecture and training components on alignment with human conceptual representations", "keywords": ["cognitive science", "transformers", "large language models", "human-AI alignment", "human-centered AI", "benchmarking", "cognitive benchmarking"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e6c169170e3316fa686b0d67dd3f6a91218c2ae.pdf", "supplementary_material": "/attachment/1f0e68a5dfecada6fe69f54b424e19e23eb458db.pdf"}, "replies": [{"content": {"summary": {"value": "The paper studies the current state of LLMs as computational cognitive models form the representational perspective. The paper specifically focuses on identifying the computational ingredients that would bias models towards more human-like representations. The paper focuses on the THINGS dataset, a dataset that contains human similarity judgements for 4.7 million triplets, and compares subsets thereof to a suite of 77 foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is overall very well-written and clearly describes its approach, results and interpretation.\n- The empirical results are extensive regarding number of models and downstream benchmarking datasets.\n- The analyses are novel to my knowledge, i.e., the relationship between alignment and performance on standard LLM benchmarks.\n- The paper includes an honest and balanced discussion and limitations sections. The results appear reproducible and methods are clearly described."}, "weaknesses": {"value": "- The paper presents an empirical study focused on a single dataset (THINGS).\n- The overall analyses unfortunately remained focused on dataset-wide reporting of alignment scores. I think the paper would benefit from a more detailed analysis of alignment results in dependence of dataset composition, at least for a subset, e.g. particularly well-aligned, models. Relevant dimensions could include triplet term-similarity levels, concreteness or linguistically frequent tokens (see the mentioned ecosat data).\n- Similarly, the overall alignment of models as measured via $R^2$ is at around 0.5 for the most human-like models, the paper unfortunately does not present an analysis of error modes and shortcomings/limits of current models. What are the, e.g. semantic, types or families of triplets that models struggle with w.r.t. alignment? The THINGS dataset contains additional annotations, e.g. “core object categories,” “animacy” and “size ratings” that would be useful to better understand the observed limits.\n\n\n\n**Additional Refs**\n\n- Effect of instruction-tuning on language Models and conceptual alignment:  \n    - [Lin25] Linhardt, L., Neuhäuser, T., Tětková, L., & Eberle, O. Cat, Rat, Meow: On the Alignment of Language Model and Human Term-   Similarity Judgments. In Second Workshop on Representational Alignment at ICLR 2025.\n    - [Aw24] Aw, K. L., Montariol, S., AlKhamissi, B., Schrimpf, M., & Bosselut, A. Instruction-tuning aligns llms to the human brain. COLM 2024."}, "questions": {"value": "- What is the purpose of using the ordinal embedding algorithm to fit an embedding model to judgement data over directly using the raw model embeddings, e.g. that of the answer token? Is the main reason to match the dimensionality of human-based embeddings? \n- Did you observe any relevant differences in representational geometry (see also points made above)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lmcHy4XU3m", "forum": "g5pYm2OmfA", "replyto": "g5pYm2OmfA", "signatures": ["ICLR.cc/2026/Conference/Submission14208/Reviewer_7rre"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14208/Reviewer_7rre"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735680875, "cdate": 1761735680875, "tmdate": 1762924663313, "mdate": 1762924663313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates which computational ingredients, ranging from architecture, size, fine-tuning, data size, and training regimes, could predict human-like representational alignment in large language models (LLMs). Using over 77 open-weight models spanning major model families (Llama, Qwen, Gemma, Phi, OLMo, Falcon, etc.), the authors applied a triadic similarity judgment task based on 128 object concepts from the THINGS database, which includes >4.7M human triplet judgments. Each model completes 35,000 trials of the same task (“Which of y/z is most similar to x?”), and the authors construct model semantic embeddings using ordinal embedding techniques comparable to the SPoSE embeddings from human data. By computing representational alignment (Procrustes R²) between model and human embeddings and relate alignment to architectural and training factors. The found that Instruction fine-tuning is the strongest predictor of human-model alignment; embedding and MLP dimensionality positively correlate with alignment; model scale and training data amount contribute moderately. Multimodal pretraining, activation function, and vocabulary size have little or negative impact.\n\nThe authors conclude that instruction tuning and model representational capacity are the key computational ingredients of human-like conceptual structure, and that human-based representational benchmarks provide unique insights beyond standard NLP tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Ambitious and comprehensive analysis: Evaluating 77 LLMs across diverse families and parameterizations is an impressive effort.\n2. Conceptually grounded approach: The use of the THINGS dataset and triplet similarity judgments directly connects AI benchmarking with established cognitive science methods.\n3. Clear and interpretable results: The paper identifies consistent, actionable findings (e.g., importance of instruction tuning and representational dimensionality) that have implications for both model design and theories of human cognition.\n4. Bridging cognitive science and AI: It positions representational alignment as a measurable axis of “human-likeness” in LLMs, helping to bridge psychological theories of conceptual structure with LLM evaluation."}, "weaknesses": {"value": "1. The THINGS dataset focuses on concrete object concepts. It's unclear whether the findings generalize to abstract or relational knowledge, which is central to many cognitive domains.\n2. The study establishes associations but not causal explanations for why instruction tuning or architectural dimensionality yield better human alignment. Simulated ablations or controlled fine-tuning experiments could strengthen the causal claims.\n3. While alignment scores and t-SNE maps are presented, the paper does not analyze which semantic dimensions (e.g., animacy, size, tool–animal axes) drive human-model similarity. This limits its contribution to understanding the nature of human-like representations."}, "questions": {"value": "1. The human embeddings are taken from SPoSE, while model embeddings are constructed using an ordinal embedding algorithm. Given that both humans and models performed the same triplet-similarity task, could SPoSE be applied to the model triplet data as well? Using the same embedding algorithm might yield a more symmetric comparison and reduce potential geometric biases introduced by differing regularization assumptions.\n\n2. The paper states that both human and model embeddings are reduced to ≈29 dimensions to allow Procrustes alignment. It's unclear whether this dimensionality was fixed to match human SPoSE variance explained, or optimized separately for each model? Would the main results hold if the model embeddings used their optimal dimensionality rather than being forced to match the human space?\n\n3. While R² quantifies overall alignment, it would be valuable to understand which semantic dimensions (e.g., animacy, toolness, color) drive the alignment between humans and models. Did the authors attempt to interpret or visualize specific embedding dimensions after alignment?\n\n4. Since the THINGS dataset focuses on concrete visual object concepts, do the observed effects of instruction tuning and architectural dimensionality generalize to more abstract, relational, or linguistic domains?\n\n5. The authors mention collecting 35 000 triplets per model. How sensitive are the alignment results to the number of triplets? Was the choice of ordinal embedding partly motivated by data efficiency compared to SPoSE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fX8eVkFHSF", "forum": "g5pYm2OmfA", "replyto": "g5pYm2OmfA", "signatures": ["ICLR.cc/2026/Conference/Submission14208/Reviewer_Gksp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14208/Reviewer_Gksp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955722065, "cdate": 1761955722065, "tmdate": 1762924660246, "mdate": 1762924660246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an in-depth analysis of concept representations in LLMs and seeks to discover the alignment of models with human representations of concepts.  The findings are informative in uncovering how alignment arises, e.g., increasingly during post-training stages, understanding the amount of context required to achieve alignment, and understanding where in the architecture concepts best align."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The analysis is thorough and the findings are interesting and informative.\n- Understanding concept representations is an interesting topic and could help to unlock robustness issues with models, e.g., why models fail in particular ways.\n- The authors acknowledge that the distribution of models is skewed to particular model classes, but they have considered a large number of models."}, "weaknesses": {"value": "- I do realize this cannot be addressed by the authors, but it is hard to isolate individual aspects of data/model size/architecture/etc. to hone in on specific aspects that might be responsible for more/less alignment."}, "questions": {"value": "- Line 055: should data be in the list of ingredients? I realize that the list is not exhaustive, but data feels an important component.\n- Line 077: “the geometry human” > “the geometry of human”.\n- Line 113: There is an extraneous space after “error-bounds”.\n- Line 228: \"The incorrect opening quotes are used.\n- Line 259: Should “QUESTION: Which item  ...” be in quotes?\n- Line 279: “Procrustes transforms find” > “Procrustes transform finds”.\n- Figure 3: use colors that better distinguish the lines.\n- Figure 3: there seems to be spacing issues between the main text and the caption — the spacing makes it feel cramped."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "agQrjiQhyR", "forum": "g5pYm2OmfA", "replyto": "g5pYm2OmfA", "signatures": ["ICLR.cc/2026/Conference/Submission14208/Reviewer_k7pP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14208/Reviewer_k7pP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957957232, "cdate": 1761957957232, "tmdate": 1762924657816, "mdate": 1762924657816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}