{"id": "EbsplMNfaT", "number": 3392, "cdate": 1757417376320, "mdate": 1762939354073, "content": {"title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second", "abstract": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes with pixel-aligned Gaussian primitives and explicitly supervises their time-varying motions. This allows, for the first time, the unified modeling of appearance, geometry and motion from monocular videos, and enables reconstruction, view synthesis and 3D point tracking within a single learning-based framework. By bridging view synthesis with geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups. The code and pretrained model will be publicly available.", "tldr": "We present MoVieS, the first feed-forward framework that jointly models appearance, geometry and motion for 4D scene perception from monocular videos.", "keywords": ["4D Reconstruction", "Dynamic Novel View Synthesis", "3D Motion Estimation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d6e4368568c1f78a3e445ec89abee03e8e74b9d9.pdf", "supplementary_material": "/attachment/f9af370aefa36201d286a2bf49dc52343fe28949.zip"}, "replies": [{"content": {"summary": {"value": "MOVIES is a **feed-forward framework** that generates **4D dynamic novel views** from **monocular videos in just one second**. It models dynamic 3D scenes through **pixel-aligned Gaussian primitives** and explicitly learns their **temporal motions**, allowing for a unified representation of **appearance, geometry, and motion** within a single network.\n\nBy **integrating view synthesis and geometry reconstruction**, MOVIES enables **large-scale training** across diverse datasets with minimal reliance on task-specific supervision. This unified design also facilitates **zero-shot capabilities**, including **scene flow estimation** and **moving object segmentation**. Extensive experiments show that MOVIES delivers **competitive results** across multiple tasks while achieving **significant speed improvements** during inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**[S1]** The model introduces minimal architectural modifications to VGGT while achieving impressive performance. By naturally extending VGGT, it demonstrates strong **novel view synthesis (NVS)** results across various datasets. The **depth** and **splat heads** directly predict **3D Gaussian Splatting (3D-GS)** representations, enabling **real-time multi-view rendering**. The **motion head** predicts **time-conditioned motion fields**, effectively modeling **dynamic splatter pixels**—a novel approach to dense motion prediction.\n\n**[S2]** The paper provides **comprehensive ablation studies** to validate the effectiveness of each model component. Through well-controlled experiments, the authors clearly demonstrate how each design choice contributes to overall performance.\n\n**[S3]** The **writing is clear and well-organized**, with concepts and figures effectively illustrating the core ideas. The presentation's clarity makes the methods and experimental purposes easy to follow. I have included a few **minor comments** on this in the **Weakness Section**."}, "weaknesses": {"value": "[W1] **Assumption of pose-awareness**: their strong assumption of pose-awareness within the underlying scene severely limits the applicability of the model in real-world scenarios. Given that VGGT jointly estimates camera parameters along with other dense outputs such as depth, pointmaps, and tracking features, I am curious why the authors decided to exclude camera tokens from their model architecture. Since MoVieS is primarily trained on datasets where camera parameters are available (e.g., RealEstate10K and Stereo4D), it would be beneficial to relax or explicitly examine the pose-awareness assumption. This is critical since despite efficiently handling videos with poses, they still require an additional process to obtain the camera poses, which takes a long time and relies on heavy computations. For instance, if we desire to use traditional SfM, COLMAP, it would even take longer than previous pose-free per-scene optimizatation approaches such as RoDynRF[2], RoDyGS[3], and SoM[4]. Therefore, they should also provide comparison with the pose-free approaches in terms of reconstruction accuracy and overall optimization time (including pose estimation).\n\nSome wrong claims: \n\n(L303-305): Although they claimed their model does not require heavy pretrained models and complex multi-stage pipelines, they should rely on heavy SfM pipelines or pretrained models for pose estimation. \n\n[W2] **Missing benchmarks**. They missed several recent work in this research area. In DyCheck, they miss the comparison with D-NeRF, 4DGS, and Deform3D which are 3~4PSNR above their method. Moreover, they have compared their model with pose-free approaches (SoM, MoSca), which are toally unfair comparison. Note that SoM and MoSca do not adopt known camera poses and instead rely on initialized poses from other fast SfM tools. By adding such baselines, MoVieS shows notably worse performance than optimization-based methods. Therefore, the authors should tone-down their claim for clarity. \n\n[W3] **An unified DPT head for depth and Gaussian attributes**: it is unclear why the authors chose to separate the DPT heads for depth and Gaussian attributes. From an implementation perspective, I understand that recent deep learning frameworks make it easier to implement separate DPT modules for predicting depth maps and splatter-related attributes. However, conceptually, predicting per-pixel Gaussian attributes should either follow the prediction of the depth head or explicitly refer to the geometric information provided by it, since they are strongly correlated for reliable novel view rendering. The paper would be clearer if the authors compared their model with a variant in which the DPT and splat heads are merged—i.e., a single DPT head jointly predicts both depth and splat attributes.\n\n[W4] **Missing illustration of predicted motions**. Since they introduce a novel representation for motion, they should show that their motions are clearly drawn in 3D space. To show that I suggest to add a figure like MoSca Figure 1 to draw the trajectory over time. Authors cannot undertand how the motion is predicted based on Figure 6,7,8,9. \n\n[W5] **Quantitative evaluation of motions in static scene**s. One of the issues of using dynamic models on static scenes is the ambiguity between camera movements and object movements. Since the authors claimed that their model achieves natural convergence to zero for static scenes (L294-L296), they should quantitatively show that their model \n\n[W6] **Explanation of Plucker embedding (L177 - L179)**. The term *“pixel-aligned”* is not incorrect; however, readers who are not familiar with Plücker embeddings might find this expression confusing. I recommend explicitly stating that the ray corresponding to each pixel is converted into a Plücker coordinate. Additionally, it would be helpful to include a simple equation illustrating how each camera embedding is formulated.\n\n[W7] **Missing information in Fig 1.** In Fig. 1, the assumption under pose-awareness is not clearly illustrated. Although the authors describe it by drawing `Camera Pos.' in the figure, it has potential for misunderstanding since the model's input seem like unposed videos. It would be clearer if they clearly illustrate that both videos and their frame-wise camera poses are given as inputs to the model. \n\n[W8] **Not self-contained writing**. Academic paper should be self-contained, in other words, all details should be elaborated within this paper. I understand that the page limit often leads to the omissions of unimportant concepts. However, it should be at least provided in the Appendix chapter to make writing more clear. The authors should supplement their writing with the listed components below:\n\n- The concept of confidence-based weighting on training objectives.\n- The exact form of rendering rendering loss (with their respective weight value $lambda)."}, "questions": {"value": "- Why do  the authors use a different scale of SSIM between Table2 (RealEstate 10K and NVIDIA)? For RealEstate10K, they used 100-scales; however, for NVIDIA, they used a normalized scale to 1.\n- Did they fully fine-tuned VGGT? or did they simply froze the layers? It is unclear for me to fine-tune VGGT since it no more uses camera tokens from VGGT, which makes attention operates differently. Could you clarify the details for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iKFYPnSEX6", "forum": "EbsplMNfaT", "replyto": "EbsplMNfaT", "signatures": ["ICLR.cc/2026/Conference/Submission3392/Reviewer_Va4v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3392/Reviewer_Va4v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972815421, "cdate": 1760972815421, "tmdate": 1762916700972, "mdate": 1762916700972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MoVieS, a feed-forward model for 4D dynamic scene reconstruction from monocular videos. The key contribution is the introduction of dynamic splatter pixels, which decouple 3D Gaussian primitives from the time-dependent deformation field, enabling modelling of geometry and motion within a single framework. The model achieves competitive performance on novel view synthesis and 3D point tracking tasks while providing large speedups compared to per-scene optimisation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- To my knowledge, this is the first work to address dynamic reconstruction in a feed-forward manner (at least to satisfactory quality).\n- The dynamic splatter representation is intuitive and theoretically sound.\n- The work makes good use of a strong VGGT prior.\n- The training regime with complementary supervision from multiple datasets is an interesting way of making use of incomplete data.\n- The qualitative results look visually impressive.\n- Similarly, the quantitative evaluation shows good performance with fast inference speed.\n- The auxiliary zero-shot applications are very useful as well."}, "weaknesses": {"value": "- With high computational cost, the adoption of the framework most likely depends on the code and pretrained model release.\n- The model is initialised with a VGGT backbone, which raises the question of how much of the performance comes from VGGT pretraining (would be a useful ablation).\n- It seems that training is very sensitive and needs a heavily engineered curriculum.\n- It would be good to perform a deeper analysis on motion supervision, e.g. 2000x oversampling on Spring seems quite strong.\n- It is a bit unclear whether the performance comes from algorithmic contribution or the scale of the training."}, "questions": {"value": "- It would be good to know which datasets are crucial for the training, and what scale is required (or is it diversity that matters).\n- The paper describes the limitations of the approach. I wonder if authors can comment on the most common failure modes of the approach when run on in-the-wild videos."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X53aB87PCm", "forum": "EbsplMNfaT", "replyto": "EbsplMNfaT", "signatures": ["ICLR.cc/2026/Conference/Submission3392/Reviewer_dYmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3392/Reviewer_dYmc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960982520, "cdate": 1761960982520, "tmdate": 1762916700408, "mdate": 1762916700408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "e5EKxnGXK3", "forum": "EbsplMNfaT", "replyto": "EbsplMNfaT", "signatures": ["ICLR.cc/2026/Conference/Submission3392/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3392/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762936627969, "cdate": 1762936627969, "tmdate": 1762936627969, "mdate": 1762936627969, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a 4D dynamic view synthesis method. \nThe methods inputs a sequence of frames and their poses, and a transformer-based architecture outputs 3D points, 3D Gaussian attributes, and 3D motion in a feed-forward manner. Given the estimates the method can solve several tasks such as novel-view synthesis and 3D point tracking.\n\n---\n\nThe paper's contribution is great: a feedforward 4D dynamic view synthesis model that is competitive to the optimization-based approaches (Table 2). However, there are many unclear points (eg., possible unfair comparison, unclear motion color coding, Figure 2 visualization, inaccurate motion estimation, generalization, motion segmentation, etc) that makes hard to give an accept recommendation confidently. For now, my decision is between **2: reject and 4: borderline reject** (but would like to increase based on responses.)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Easy to follow**\n\n  The paper is easy to follow. It is easy to understand the technical details. \n\n* **Good ablation study**\n\n  Especially Figure 4 and Table 6 show ablation study on the loss function. It validates the effectiveness of the proposed ideas on motion loss, NVS loss, L1 loss, and distribution loss. \n  \n* **Good empirical results on benchmark datasets**\n\n  In Table 2, the method shows better accuracy than other feed-forward NVS methods (eg., DepthSplat, GS-LRM) or optimization-based methods with order of magnitude faster runtime. This can be considered as a good empirical contribution of the paper."}, "weaknesses": {"value": "* **Possible unfair comparison in Table 3**\n\n  The other methods (BootsTAPIR, CoTracker3, and SpatialTracker) possibly don't use camera pose input for the inference. Given that the proposed method uses precomputed (or given) camera pose, I wonder how fair the comparison would be. It would be curious to know i) how the evaluation is actually conducted where the other methods don't have pose information, ii) what pose estimator (or GT pose?) the proposed method uses. \n\n\n* **Unclear motion color coding**\n\n  The color coding of the motion can be misleading. Not only moving objects, background also seems to have non-zero motion (ie., non-white color coding), as in all Figures, except Figure 5. Does it mean that the background usually have constant 3D motion? Depending on the answers, it could change the understanding of the coordinate representation, eg., if 3D motion is defined at the global camera coordinate or local camera coordinate.\n\n* **Figure 2: 3D GS or points?** \n\n  I was wondering if the RGB visualization in Figure 2 can be misleading. At a glance, it doesn't look like a visualization of 3D GS (where each Gaussian have different 3D scale, opacity, etc.) but a set of uniform dilated 3D points that are usually seen from feedforward 3D point estimation methods (eg., VGGT, DUSt3R, MonST3R). \n  \n* **Accumulating all 3D Gaussians from all views?**\n\n  When rendering images, I wonder if the method accumulates all the 3D Gaussians from all input frames and rasterizes images. I wonder if this occurs any memory problem or computational burden. If it accumulates all 3D Gaussians from all views, I wonder if it is a redundant design that a single 3D point is represented by multiple overlapped moving 3D Gaussians from multiple views instead of a single 3D Gaussian that moves across the multiple frames.  \n\n* **Inaccurate motion?** \n\n  In the motion visualization in Figure 4 (upper, the most right figure), Figure 7 (bottom), some motion visualization includes non-homogeneous motion in foreground objects that exhibits rigid motion (although the depth visualization looks correct). I am wondering what's happening in the motion space. \n\n* **Generalization for the ablation study** \n\n  Some ablation study (Table 4 and Table 5) is done on a single dataset (RE10K or ADT) for different modality evaluations. I was wondering if the paper can provide a ablation study on multiple datasets. For example, it would be possible to report the camera conditioning and motion supervision analyses on both RE10K and ADT together. This can further prove the effectiveness of the proposed ideas. \n\n  Is there a specific reason why Table 4 evaluates Ours (**static**) instead of Ours?\n\n* **Motion object segmentation** \n\n  For Fig 5 (b), the motion object segmentation is tested on the training domain (Spring, PointOdyssey). It would be curious to see how the method actually performs in-the-wild examples."}, "questions": {"value": "* **Sensitivity to the camera pose input**\n\n  The reliance on the camera pose can be a limitation, as stated in the limitation section. It would be curious how robust the method is to the noisy camera input. One could evaluate the method on a benchmark dataset by injecting random noise to the camera pose input at different levels. \n\n* At **Line 303** \n\n  How many images does each scene include? Because depending on the number of input frames, the runtime of the method could change, and each scene can include different number of frames.\n\n* **static**? \n\n  What does the *static* mean in Table 2? (eg., *Static* feed-forward and Ours (*static*))"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nkKMZUnKlD", "forum": "EbsplMNfaT", "replyto": "EbsplMNfaT", "signatures": ["ICLR.cc/2026/Conference/Submission3392/Reviewer_4wg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3392/Reviewer_4wg9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974185134, "cdate": 1761974185134, "tmdate": 1762916700136, "mdate": 1762916700136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MOVIES, a feed-forward model for dynamic 4D scene reconstruction and new view synthesis from monocular videos. It jointly models appearance, geometry, and motion using 3D Gaussian Splatting (3DGS), enabling efficient depth estimation, 3D point tracking, scene flow estimation, and moving object segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Efficient and Unified Approach: MOVIES integrates appearance, geometry, and motion using 3D Gaussian Splatting (3DGS), enabling fast inference while achieving competitive performance across multiple benchmarks like RealEstate10K and TAPVid-3D.\n\n2. Zero-Shot Capabilities: The model demonstrates strong zero-shot performance in tasks such as scene flow estimation and moving object segmentation, showcasing its versatility and potential for real-world applications."}, "weaknesses": {"value": "1. Artifacts in Video Results:\nThe video results show artifacts, such as blurred legs and embedded wheels, impacting visual coherence. The authors should provide a detailed discussion on the causes (e.g., model limitations or data issues) to better understand the model's performance and limitations.\n\n2. Camera Pose Prediction:\nThe removal of camera pose prediction, despite VGGT’s ability to estimate it, is unclear. Since the model still requires camera pose estimation, the authors should explain why they chose to exclude the VGGT camera head.\n\n3. Main Modification Compared to VGGT:\nMOVIES replaces VGGT’s prediction head with 3D Gaussian Splatting (3DGS), but this change seems to offer limited novelty or significant modification to the model.\n\n4. Ablation Study on Camera Conditioning:\nThe ablation study on camera conditioning lacks meaningful insight. With explicit camera conditioning and additional input information, it is expected that the model’s performance will naturally improve, making the results less informative."}, "questions": {"value": "See the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qjKtZbekp1", "forum": "EbsplMNfaT", "replyto": "EbsplMNfaT", "signatures": ["ICLR.cc/2026/Conference/Submission3392/Reviewer_LYRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3392/Reviewer_LYRv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014542345, "cdate": 1762014542345, "tmdate": 1762916699925, "mdate": 1762916699925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}