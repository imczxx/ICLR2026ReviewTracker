{"id": "WEg7e5pcso", "number": 70, "cdate": 1756728393273, "mdate": 1759898276763, "content": {"title": "ABConformer: Physics‑inspired Sliding Attention for Antibody-Antigen Interface Prediction", "abstract": "Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for vaccine design, immunodiagnostics and therapeutic antibody development. However, achieving reliable predictions from sequences alone remains a challenge. In this paper, we present \\textsc{ABConformer}, a model based on the Conformer backbone that captures both local and global features of a biosequence. To accurately capture Ab-Ag interactions, we introduced the physics-inspired sliding attention, enabling residue-level contact recovery without relying on three-dimensional structural data. ABConformer can accurately predict paratopes and epitopes given the antibody and antigen sequence, and predict pan-epitopes on the antigen without antibody information. In comparison experiments, \\textsc{ABConformer} achieves state-of-the-art performance on a recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based methods for antibody-agnostic epitope prediction. Ablation studies further quantify the contribution of each component, demonstrating that, compared to conventional cross-attention, sliding attention significantly enhances the precision of epitope prediction. To facilitate reproducibility, we will release the code under an open-source license upon acceptance.", "tldr": "", "keywords": ["Antibody–antigen interface prediction", "Protein sequence modeling", "Conformer", "Sliding attention mechanism", "Epitope prediction", "Paratope prediction", "Structural bioinformatics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38039f8f48fb41930fb9d9ea4cf56c01bf411aab.pdf", "supplementary_material": "/attachment/76811f6954e6a1df174951d8ce851b45a4a300af.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors presented ABConformer, a physics-inspired sliding attention scheme to predict paratopes and epitopes given the antibody and antigen sequence. The method is evaluated on the SARS-CoV-2 Ab-Ag dataset against other sequence-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "It is very hard to name a strength of the paper, see my comments in the weaknesses part."}, "weaknesses": {"value": "The novelty of the paper is limited considering that it is **basically applying the PISTE algorithm published in 2024 ** in a different domain of biosequence interactions, with minor revisions of algorithm details."}, "questions": {"value": "This paper is obviously following the paper *Sliding-attention transformer neural architecture for predicting T cell receptor-antigen-human leucocyte antigen binding, Nature Machine Intelligence, 2024, 6(10): 1216-1230*. All the key technical details are similar, including the definition of the distance based attention and embedding based attention, how they are combined together with a template M, and how the sliding attention updates the 1-D coordinate of the protein sequences. The titles of the two papers are also identical. The only difference is the datasets used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CCCzK4hwXE", "forum": "WEg7e5pcso", "replyto": "WEg7e5pcso", "signatures": ["ICLR.cc/2026/Conference/Submission70/Reviewer_r9ov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission70/Reviewer_r9ov"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission70/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761646850381, "cdate": 1761646850381, "tmdate": 1763016693537, "mdate": 1763016693537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ABConformer, a sequence-based framework that employs a sliding attention mechanism within a Conformer backbone for accurate antibody–antigen (Ab–Ag) interface prediction without requiring 3D structural data. The model captures both local and global sequence dependencies while incorporating a physics-inspired Gaussian sliding attention that mimics local docking interactions between VH/VL domains and antigens. This design enables fine-grained residue-level contact recovery and improves both paratope and epitope prediction accuracy and efficiency.\nExperiments on a recent SARS-CoV-2 Ab–Ag dataset show that ABConformer outperforms existing sequence-based baselines and generalizes well to both antibody-specific and antibody-agnostic prediction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tClear motivation and novelty\nThe paper addresses an important and challenging problem — antibody–antigen interface prediction from sequence alone — with a clear motivation and innovative approach. The sliding attention mechanism provides both computational efficiency and improved predictive power.\n2.\tStrong empirical performance\nABConformer achieves superior results over multiple state-of-the-art baselines on widely used benchmark datasets, demonstrating its robustness and practical relevance.\n3.\tComprehensive ablation and interpretability\nThe ablation studies are thorough and convincing. Attention maps align well with known structural interface regions, offering biologically meaningful interpretability. The model also exhibits reasonable generalization in antibody-agnostic scenarios.\n4.\tHigh-quality presentation\nThe manuscript is clearly written, well-structured, and easy to follow. Figures and tables effectively communicate the results."}, "weaknesses": {"value": "1. Experimental scope and robustness\nThe experiments are generally solid and well-structured, with clear ablation analyses and sensitivity tests that support the model’s main claims. However, although the baseline coverage is fairly comprehensive, several recent and competitive approaches using protein language models such as ESM-IF1, ProtT5, or AlphaBind are not included. Adding these stronger and more contemporary baselines would provide a fairer and more convincing comparison with current SOTA methods. Reporting computational efficiency or variance across folds would further improve the robustness and credibility of the experimental evidence.\n\n2. Generalization across datasets\nWhile the authors include an external evaluation on the SARS-CoV-2, its limited scale and close domain similarity to AACDB reduce the strength of the generalization argument. Additional experiments on heterogeneous datasets, such as HIV or influenza antibody antigen complexes, which would significantly enhance the credibility of the claimed generalization capability.\n\n3. Fusion strategy justification\nThe separate VH and VL branches followed by fusion are biologically motivated, as VH and VL contribute differentially to paratope formation and contact distinct regions of the antigen surface. Nevertheless, the rationale for choosing this particular fusion strategy should be explicitly stated and compared with possible alternatives to enhance clarity and justification."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OcCLSsZLu1", "forum": "WEg7e5pcso", "replyto": "WEg7e5pcso", "signatures": ["ICLR.cc/2026/Conference/Submission70/Reviewer_Wy5X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission70/Reviewer_Wy5X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission70/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756562240, "cdate": 1761756562240, "tmdate": 1762915446075, "mdate": 1762915446075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ABConformer, a sequence-based deep learning model for predicting antibody-antigen binding interfaces. The method builds upon the Conformer architecture and introduces a sliding attention mechanism adapted from prior work to simulate molecular docking processes. The model processes antibody heavy chain, light chain, and antigen sequences separately using ESM-2 embeddings as input features. The sliding attention mechanism iteratively adjusts relative positions between sequences through Gaussian kernels for spatial proximity and attention-weighted position updates. The authors evaluate their method on AACDB and an external SARS-CoV-2 dataset, demonstrating improvements over existing sequence-based methods in antibody-specific interface prediction and competitive performance for antibody-agnostic epitope prediction, though with notable limitations in recall and methodological novelty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1**. While the sliding attention mechanism is not original to this work, the authors make appropriate adaptations to the antibody-antigen context through a three-branch architecture. The dual sliding process and linear combination of resulting embeddings represent sensible design choices for this specific application. However, the novelty lies primarily in the application domain rather than in methodological innovation.\n\n**S2**. The explicit separation of Ab-H and Ab-L chains is more biologically grounded than treating antibodies as monolithic entities, as paratopes are formed by complementarity-determining regions (CDRs) from both heavy and light chains. This design choice aligns well with the known structural organization of antibody-antigen interfaces.\n\n**S3**. The paper provides thorough experimental evaluation, including extensive ablation studies examining encoding strategies and attention mechanisms, sensitivity analyses of hyperparameters, and case studies."}, "weaknesses": {"value": "**W1**.The paper does not adequately justify how binding site prediction translates to practical antibody design applications. Specifically: (i) How does interface prediction assist in binding affinity estimation, which is often the ultimate goal in therapeutic antibody development? (ii) What is the relationship between predicted interface residues and functional properties? (iii) In what scenarios would researchers prefer interface prediction over structure prediction followed by docking simulations? Without addressing these questions, the practical motivation for this work remains insufficiently justified.\n\n**W2**. The paper proceeds directly to methodology without establishing a rigorous mathematical formulation of the prediction task. A complete problem statement should specify: (i) input, (ii) output, and (iii) objective.\n\n**W3**. The sliding attention mechanism is directly adapted from previous work, with the primary modifications being: (i) extension from two-component to three-branch architecture, (ii) sequential dual sliding operations (Ag→Ab-H, then Ag→Ab-L), and (iii) linear combination of embeddings after each sliding step. The paper should more explicitly distinguish between novel contributions and adaptations of existing methods. Given the limited algorithmic innovation, the contribution might be more accurately characterized as application engineering rather than methodological advancement.\n\n**W4**. The assertion that ABConformer *surpass widely used sequence-based methods* is misleading. According to Table 1 and Figure 9D, ABConformer achieves superior performance only on certain metrics while showing substantially lower recall. In addition, the paper acknowledges that *the sliding-attention module has no effect* for antibody-agnostic prediction because antibody embeddings are set to zero, which reduces ABConformer to a Conformer backbone. This raises the question: why not develop and compare a dedicated Conformer-only model for antibody-agnostic prediction?\n\n**W5**. While the paper includes AlphaFold2 Multimer v3 as a baseline, it omits several highly relevant and state-of-the-art structure-based approaches: (i) AlphaFold3 [1] has demonstrated strong performance on protein-related tasks; (ii) ESMFold [2] uses the same ESM-2 embeddings as ABConformer's encoder, making it a critical baseline for isolating the value added by the sliding attention mechanism; (iii) Boltz-1 [3] and Boltz-2 [4] are recent methods specifically designed for biomolecular interaction modeling and binding affinity prediction; (iv) PAbFold [5] is specifically designed for antibody epitope prediction using AF2. Without these comparisons, the paper cannot substantiate its implicit claim that sequence-based prediction with sliding attention offers advantages over structure prediction followed by interface extraction.\n\n---\n\n**Reference*\n\n[1] J. Abramson et al. *Accurate structure prediction of biomolecular interactions with AlphaFold 3*. Nature 2024.\n\n[2] Z. Lin et al. *Evolutionary-scale prediction of atomic-level protein structure with a language model*. Science 2023.\n\n[3] J. Wohlwend et al. *Boltz-1 democratizing biomolecular interaction modeling*. BioRxiv 2025.\n\n[4] S. Passaro et al. *Boltz-2: Towards accurate and efficient binding affinity prediction*. BioRxiv 2025.\n\n[5] J. DeRoo et al. *PAbFold: linear antibody epitope prediction using AlphaFold2*. BioRxiv 2024."}, "questions": {"value": "**Q1**. How does binding site prediction assist in practical antibody design workflows? Specifically: (i) Can predicted interfaces be used to estimate binding affinity, and if so, what is the correlation? (ii) How might researchers use these predictions in therapeutic antibody optimization or vaccine design?\n\n**Q2**. How does ABConformer compare to a baseline that computes pairwise cosine or Euclidean distances between ESM-2 embeddings of antibody and antigen residues, followed by thresholding? This comparison would isolate the value added by the sliding attention mechanism beyond embedding similarity.\n\n**Q3**. Why are results of Epi4Ab omitted in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "opB2hHRHPx", "forum": "WEg7e5pcso", "replyto": "WEg7e5pcso", "signatures": ["ICLR.cc/2026/Conference/Submission70/Reviewer_KNTa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission70/Reviewer_KNTa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission70/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902565158, "cdate": 1761902565158, "tmdate": 1762915445858, "mdate": 1762915445858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ABConformer, a new model that predicts antibody-antigen binding interfaces using only their protein sequences. The  key innovation is a physics-inspired sliding attention mechanism, which aims to mimic the molecular docking process by sliding the antigen sequence against the antibody's heavy and light chains to find stable interaction patterns, without the need for 3D structural data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The physics-inspired sliding attention is an interesting innovation, which outperforms conventional cross-attention, especially in improving the precision of epitope prediction.\n- The model is flexible, and can handle antibody-specific prediction (when all sequences are known) while also being effective for antibody-agnostic pan-epitope prediction (using only the antigen sequence), where it surpasses other sequence-based methods."}, "weaknesses": {"value": "- The core innovation of this paper is the sliding attention module, which isn't used for the antibody-agnostic predictions.\n- Comparisons are performed on a small external dataset of 35 SARS-CoV-2 complexes, which raises questions about generalizability. ABconformer is outperformed by DiscoTope-3.0, a structure-based method on the antibody-agnostic task. Recent structure-based methods such as AlphaFold3, Boltz-2 or Chai are also not included in the antibody-specific comparisons.\n- The ablation study shows that the gap between sliding attention and conventional cross-attention is small, with cross-attention achieving slightly higher recall, but worse precision."}, "questions": {"value": "- Could the authors include other benchmarks than SARS-CoV-2?\n- It would be valuable to include a comparison with AlphaFold3, or other recent structure prediction models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R5uvAy5KrP", "forum": "WEg7e5pcso", "replyto": "WEg7e5pcso", "signatures": ["ICLR.cc/2026/Conference/Submission70/Reviewer_qZkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission70/Reviewer_qZkx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission70/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004004803, "cdate": 1762004004803, "tmdate": 1762915445670, "mdate": 1762915445670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}