{"id": "AKzgsBCW6A", "number": 7400, "cdate": 1758020009169, "mdate": 1759897855117, "content": {"title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation", "abstract": "Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality  and limit application.\nGiven the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks.\nTo fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. \nWe first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development.", "tldr": "", "keywords": ["Text-to-Image Generation", "Image Artifacts", "Evaluation Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/852739967efa7243d60a03c205347d308fd18a70.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MagicMirror, a large-scale benchmark designed to detect fine-grained artifacts in text-to-image (T2I) generation. The main contribution lies in the creation of MagicData340K, a comprehensive dataset annotated with artifact categories, and the development of MagicAssessor, a vision-language model (VLM) trained to evaluate these artifacts. Finally, the authors present MagicBench, an automated evaluation framework built on MagicAssessor. Experimental results show that their method outperforms existing VLMs in artifact classification and provides valuable insights for future T2I model improvement."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The paper addresses an important but underexplored aspect of T2I evaluation, which goes beyond traditional benchmarks that mainly focus on object presence or semantic alignment.\n* The data collection pipeline is well-structured and clearly documented (Figure 4). The use of human annotations, chain-of-thought rationales, and multi-level labeling provides a rich and reliable dataset.\n* The experiments are comprehensive, comparing MagicAssessor against both open- and closed-source models. The results demonstrate solid improvements, and the analysis of different artifact categories is informative."}, "weaknesses": {"value": "* While the paper highlights the novelty of the proposed dataset, it would be great to see a clearer comparison with existing benchmarks such as Norma T2I benchmark: GenEval, T2I-CompBench, or Human preference benchmark: Pick-a-Pic, or HPSv2. Some statistics (e.g., dataset size, label diversity, or prompt length) could help readers better understand where MagicData340K stands out.\n* The overall accuracy of MagicAssessor, although clearly better than other models, is still on the lower side (some are only ~ 0.3), as shown in Table 2, including element interaction, human and animal anatomy, and object morphology. It would be nice if the authors could discuss how this might affect its use as a reward model in improving T2I systems. Even a brief analysis here would make the potential impact clearer.\n* The ablation results and design analysis (especially regarding GRPO and multi-bucket sampling) don’t entirely support the claim that the proposed design is optimal. For instance, in Table 4 (and Table 6 in the appendix), removing multi-bucket sampling improves recall on certain categories like irrational element interaction, while still maintaining competitive scores elsewhere. These patterns suggest that the trade-offs might be more nuanced than described. It would really help if the authors could add more discussion or qualitative examples showing what kinds of issues the proposed design actually solves, which would make the argument much more convincing.\n\n---\n\n**Overall**:\nThis paper makes a meaningful and timely contribution by introducing a high-quality dataset and benchmark focused on fine-grained artifact detection, which is a topic that has been largely overlooked. Even though the work doesn’t yet show how MagicAssessor directly enhances T2I model training, its methodological thoroughness and dataset quality make it a valuable step forward. Overall, I lean toward accepting the paper."}, "questions": {"value": "The questions are mainly about weaknesses:\n\n* Could authors provide a more explicit quantitative comparison between MagicData340K and existing T2I datasets (e.g., GenEval, T2I-CompBench, Pick-a-Pic, HPSv2)?\n* How might the relatively low artifact classification accuracy affect downstream improvements if MagicAssessor were used as a reward model?\n* Could authors provide more detailed or qualitative analysis in the ablation section to clarify why the proposed GRPO-based approach works best?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OKt2FHAZj4", "forum": "AKzgsBCW6A", "replyto": "AKzgsBCW6A", "signatures": ["ICLR.cc/2026/Conference/Submission7400/Reviewer_Q6jq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7400/Reviewer_Q6jq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761063185057, "cdate": 1761063185057, "tmdate": 1762919523679, "mdate": 1762919523679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on evaluating artifacts in text-to-image generation. The authors create a human annotated dataset of generated images containing artifacts and discriminate between different types of artifacts creating a taxonomy. They next propose to fine-tune a model, namely Qwen2.5-VL-7B, via SFT on Chain-of-thought traces extracted from GPT and next GRPO via a multi-reward objective. They show that their fine-tuned model is more accurate in predicting fine-grained artifacts in contrast to off-the-shelf models. Finally, they benchmark different text-to-image generation models on their dataset using their full method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a human annotated large dataset for artifact evaluation in image generation. This is an important contribution for the community, and a core contribution of the paper is that they also create a taxonomy of artifacts where they target fine-grained evaluation.\n2. The authors propose a well thought pipeline for fine-tuning a model on artifact evaluation. Off-the-shelf models do not perform well in recognizing artifacts, as these artifacts might be out of their training distribution. The authors show the improvement that their fine-tuning approach offers on Qwen2.5-VL-7B."}, "weaknesses": {"value": "1. The novelty of the fine-tuning approach for artifact detection, which the authors claim that is one of the main contributions of the paper, is limited. The main novel components of the approach is the data sampling strategy, which is very similar to traditional ML techniques for data imbalances but adapted to the specific task, and the reward combination which is again similar to multi-reward objectives but again adapted to the specific task with specific heuristics derived by the taxonomy of the artifacts.\n2. Crucially, the authors do not mention very important details for the creation of their dataset in order to be able to trust the approach and the benchmark. Important details include the inter-annotator agreement and how they are handling disagreements between annotators. Moreover, the paper mentions that one of the artifact categories is Irrational Element Attributes. The authors do not discuss how they discriminate between non factual elements that have been asked in the prompt so they are correctly generated and ones that are not correct. It is very common that prompts include irrational attributes for subjects or objects.\n3. The dataset has been created by generating images from specific models, including FLUX.1-dev/schnell, which is one of the best performing models on this benchmark in Table 3. How much does this bias the results? Could it be that images generated by e.g., GPT or Janus are more out of distribution for the fine-tuned evaluator? A human evaluation should be conducted here in order to measure correlation with human ratings and understand the bias of the evaluator.\n4. MagicAssessor is trained on a dataset generated by a specific suite of T2I models. Future T2I models may exhibit entirely new or different types of artifacts. How well is the model expected to generalize to these unseen failure modes? Does the \"Other Irrationalities\" category, which is very small, risk becoming a bucket for many new artifact types, limiting the model's fine-grained utility over time?\n5. Not a reason for rejection, but I find it bad that all of related work has been pushed to the Appendix. At least a synopsis should be transferred to the main paper."}, "questions": {"value": "Based on the above weaknesses:\n1. Can you explain the details of the dataset for Weakness point 2?\n2. Have you analysed in any way the bias as mentioned in Weakness point 3?\n3. Can you address the questions of Weakness point 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SK7oPhpVef", "forum": "AKzgsBCW6A", "replyto": "AKzgsBCW6A", "signatures": ["ICLR.cc/2026/Conference/Submission7400/Reviewer_pHnf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7400/Reviewer_pHnf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820746770, "cdate": 1761820746770, "tmdate": 1762919523243, "mdate": 1762919523243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a evaluation pipeline for artifacts in text-to-image generation, including human-annotated large-scale dataset of\n340K generated images with fine-grained artifact labels, a VLM to assess the generated images, and a benchmark to evaluate current T2I models. It focuses on a critical problem in current text-to-image generation models, and makes great effort to construct a large scale human annotated dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper focuses on the artifacts problem, which is critical and persistent issue in text-to-image generation. This paper provides a whole pipeline with dataset collection, model training, and benchmark construction. The efforts may potentially benefit future works on addressing the artifacts of text-to-image generation.\n\n2. Compared with previous datasets of artifacts, this paper provides more detailed category of artifacts, and propose different level of labeling. This structured categorization provides more precise evaluation signals and enables nuanced analyses of model weaknesses across different artifact types.\n\n3. The proposed dataset is substantial in both size and diversity, with 340k human annotated images, and covers many different types of artifacts in fine-grained levels."}, "weaknesses": {"value": "1. The motivation of describing the artifacts and their location in natural language is unclear. It would be more intuitive to localize artifacts spatially, by marking regions with bounding boxes or segmentation masks. Some artifacts may not be precisely explained merely by natural language, and sometimes their locations may not be easily identified. For example, when the artifacts are only part of a large object, in background region, or texture level distortions.\n\n2. While the dataset provides detailed artifact descriptions, it seems that the framework mainly uses them for binary or categorical classification rather than for region-aware evaluation or feedback. The paper could better leverage these fine-grained signals. For example, to guide targeted model correction or localized reward shaping rather than aggregating them into global scores as in previous paper [1].\n\n3. The distribution of artifact categories in MagicData340K is highly imbalance as shown in Fig 5. \n\n4. The ablation results in Table 4 indicate that removing individual components only slightly affects performance, suggesting that the contribution of each design element is not significant. This weakens the motivation to introduce these strategies.\n\n5. The paper provides limited information about the human annotation process. Key details such as the number of annotators and inter-annotator agreement are missing. Is there a validation process in the annotation, or only one annotator for each sample? Without quantitative measures of consistency, it is difficult to assess the reliability of the large-scale annotations.\n\n[1] Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation. (CVPR 2025)"}, "questions": {"value": "please see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HvGSzXhWcX", "forum": "AKzgsBCW6A", "replyto": "AKzgsBCW6A", "signatures": ["ICLR.cc/2026/Conference/Submission7400/Reviewer_LNQe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7400/Reviewer_LNQe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976835621, "cdate": 1761976835621, "tmdate": 1762919522823, "mdate": 1762919522823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a complete stack for artifact assessment in T2I: (1) a fine-grained taxonomy of artifacts and a 340K human-annotated dataset called MagicData340K, (2) a specialized assessor named MagicAssessor based on Qwen2.5-VL-7B fine-tuned using SFT followed by GRPO with custom sampling and rewards, which produces labels and rationales, and (3) an automated benchmark named MagicBench that applies MagicAssessor to evaluate a set of T2I models across human, animal, object, and interaction categories. Results show that MagicAssessor clearly outperforms general-purpose VLMs in artifact recognition, and MagicBench indicates that even leading generators such as GPT-image-1 still produce significant artifact rates."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1: The fine-grained and hierarchical taxonomy with large-scale human annotation is impressive. The L1 to L3 scheme, moving from normal versus artifact to anatomy, attributes, and interaction, then to hand-level and similar details, along with curated guidelines, represents a clear improvement over coarse plausibility dots.\n\nS2: Looks like the dataset is of high quality.\n\nS3: The design of the artifect-based reward looks good to me.\n\nS4: The paper looks really nice, very well-written."}, "weaknesses": {"value": "W1: The paper describes expert guidelines and oversight but does not report inter-annotator agreement such as kappa or Fleiss scores, nor re-label consistency metrics, despite the subjective nature of fine-grained labels like element overlap versus low-quality area. I recommend the authors include this in the rebuttal.\n\nW2: A subset of rationales is synthesized by GPT-4o from human descriptions to bootstrap chain-of-thought. While pragmatic, the paper does not specify the fraction of CoT data or analyze how GPT-4o phrasing may bias the learned reasoning style. If the evaluator later favors assessor-like wording, this may create a stylistic loop. It is important to examine the CoT steps. If possible, the authors should add this point during rebuttal.\n\nW3: ​​The consistency reward is meant to reduce reward hacking, but the evidence is mainly aggregate F1 changes. There is no audit of failure cases such as verbose rationales that still mislabel. GRPO can overfit to reward signals shaped by noisy heuristics like formatting or high-level label focus. No analysis on this is given in the paper.\n\nW4: MagicData340K includes images from several generators such as FLUX, Kolors, SD3 and SD3.5, Midjourney, and internal sources, but it is unclear whether generator identity leaks into labels, for example via artifacts that appear recognizably FLUX-like. This may let MagicAssessor rely on style-based priors. More broadly, the test set of 17366 samples is not broken down clearly in terms of size relative to training split or coverage across generator and style diversity.\n\nW5: Although the paper overall looks very nice, figure 2 does not match the style of the other figures. It appears too simple and a bit unattractive."}, "questions": {"value": "See the previous weakness. \n\nCould the authors report inter-annotator agreement (e.g., Cohen’s κ or Fleiss’ κ) for a subset of the dataset?\n\nWhat proportion of CoT rationales are human-authored vs. GPT-4o-synthesized, and how do their linguistic or reasoning styles differ? I guess some of CoT should be checked instead of completely relying on the automatically generated.\n\nHow many unique T2I generators are represented in the dataset, and is there any overlap between generators used for training and those used in MagicBench?\n\nDid the authors observe cases where GRPO-trained models produced plausible but factually incorrect rationales to maximize rewards?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6eKcGtcMhl", "forum": "AKzgsBCW6A", "replyto": "AKzgsBCW6A", "signatures": ["ICLR.cc/2026/Conference/Submission7400/Reviewer_krp7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7400/Reviewer_krp7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981204123, "cdate": 1761981204123, "tmdate": 1762919521597, "mdate": 1762919521597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}