{"id": "pFByPVh6bd", "number": 20078, "cdate": 1758302191515, "mdate": 1763694083432, "content": {"title": "Measurement Score-Based Diffusion Model", "abstract": "Diffusion models have achieved remarkable success in tasks ranging from image generation to inverse problems. However, training diffusion models typically requires clean ground-truth images, which are unavailable in many applications. We introduce the Measurement Score-based diffusion Model (MSM), a novel framework that learns partial measurement scores directly from noisy and subsampled measurements. \nBy aggregating these scores in expectation, MSM synthesizes fully sampled measurements without requiring access to clean images.\nTo make this practical, we develop a stochastic sampling variant of MSM that approximates the expectation efficiently and analyze its asymptotic equivalence to the exact formulation. We further extend MSM to posterior sampling for linear inverse problems, enabling accurate image reconstruction directly from partial scores.\nExperiments on natural images and multi-coil MRI demonstrate that MSM achieves state-of-the-art performance in unconditional generation and inverse problem solving---all while being trained exclusively on degraded measurements.", "tldr": "We propose the first measurement score-based diffusion model that directly learns partial measurement scores using only noisy and subsampled measurements, enabling the synthesis of fully sampled measurements and solving inverse problems.", "keywords": ["diffusion models", "generative models", "inverse problems", "learning from measurements", "learning without ground-truth"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c722b403101e4bc0eafd261ab21a510796037f65.pdf", "supplementary_material": "/attachment/7e0b895adc69a8a578ab95e6a8061e95aeef2588.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Measurement Score-based diffusion Model (MSM), a way to train diffusion models without any clean ground-truth images and the corresponding sampling method. Instead of learning full image scores from degraded data, MSM learns partial measurement scores directly on the observed parts of noisy, subsampled measurements. MSM also introduces its two stage of training strategy, adjusting for different levels of diffusion noise regarding to the measurement noise. At sampling time, it randomly subsamples, denoises those partials, maps back to original shape and pooling them with a overlap-aware weighting to reconstruct a full measurement. Experiments are conducted on posterior sampling for noisy inverse problems, such as inverse problems on face and multi-coil MRI images, by adding a log-likelihood gradient term for data consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## 1. Design novelty\n\nThis paper introduces novel MSM framework that learns partial measurement scores directly on observed measurements, then stitches them into a full score during sampling.\n\n## 2. Well-crafted sampling design\n\nThe proposed sampling pipeline 1. a stochastic mask-subsample; 2. partial measurement denoise; 3. coverage-aware weighting aggregation of multiple partial scores, is well-intuitive and reasonable, preserving plug-and-play modularity with standard diffusion poterior sampling.\n\n## 3. Strong empirical results\n\nUnconditional generation: Consistently outperforms Ambient Diffusion when trained only on degraded data (better FID/visual fidelity across masking and noise settings).\n\nConditional/inverse problems: Beats Ambient-DPS (A-DPS) on tasks like inpainting, super-resolution, and accelerated MRI, improving PSNR/SSIM/LPIPS while using competitive sampling budgets.\n\n## 4. Solid theoretical analysis\n\nProvides clear bounds for partial measurement score estimation and a KL divergence guarantee showing the stochastic MSM sampler approaches the optimal as the number of masks increases with error shrinking with more aggregates."}, "weaknesses": {"value": "1. The measurement corruption setting of MSM is only regular masking and the sampling method is keen to this design, which constraint the gneralization of MSM for different kinds of inverse problems, such as motion deblur, Gaussion deblur, or nonlinear inverse problems. Could MSM handle these inverse problems?\n\n2. The 200 NFEs of MSM is ambigous, for default setting with $w=3$, the model pass is 600. Could authors provide efficiency analysis and comparasions?\n\n3. In table 11 of E.5, it shows that MSM is no better than training-free DDNM. It seems because MSM trained without clean images is sensitive with test data and inverse problem settings. Could MSM be finetuned based on a diffusion model already trained on clean images?"}, "questions": {"value": "See Weaknesses 1, 2, 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fijqJeEmA5", "forum": "pFByPVh6bd", "replyto": "pFByPVh6bd", "signatures": ["ICLR.cc/2026/Conference/Submission20078/Reviewer_5Qe2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20078/Reviewer_5Qe2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799455733, "cdate": 1761799455733, "tmdate": 1762932972383, "mdate": 1762932972383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new framework called the Measurement score-based Diffusion model (MSM), which aims to learn the score function of partial measurements directly from noisy and subsampled measurements, i.e., without requiring clean data. Later, by aggregating the partial measurement scores in expectation, MSM can generate samples of full clean data. The paper also proposes a method for conditional generation using MSM, which can be used for solving inverse problems. Experiments on unconditional and conditional image generation show significant improvements over other related methods, such as ambient diffusion."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed MSM framework addresses the same problem as other works, such as ambient diffusion, G-SURE, etc, but is quite novel in terms of the methodology and with broader applicability.\n\nMSM can also be trained with noiseless subsampled measurements, offering a useful solution in practice, where the measurements are often noisy.\n\nTheorem 1 adds more validity to the proposed stochastic approximation of MSM, which shows that the approximation becomes more accurate with more Monte-Carlo samples, i.e., more computation.\n\nIn both unconditional and conditional generation tasks, MSM significantly outperforms methods such as Ambient diffusion, A-DPS, respectively, revealing its effectiveness in practice.\n\nThe paper is generally organized well. The way the authors positioned their method and contrasted it with related works made it easier to distinguish the proposed methodology."}, "weaknesses": {"value": "The paper presents a very practical and impactful methodology overall, but the method needs many clarifications regarding the MSM score and why it is defined as such, which I believe lies at the core of the proposed methodology, its novelty, and empirical effectiveness. Also, additional experiments are required to verify the arguments, including a fairer comparison in the case of inverse problem solving. Also, there are a lot of instances in the main text where the explanations are very poor and sometimes even incomplete. Please see the questions below for more specific comments and references."}, "questions": {"value": "Q1. From the explanation, it looks like Eqn 2 is the marginal score, i.e., $\\nabla \\log p(s_t)$  and not the mask conditional score, i.e.,  $\\nabla \\log p(s_t| S)$.  However, because of Eqn3. I’m confused about this. Can the authors confirm/clarify that the goal of Eqn. 2 is to learn the marginal score function and not the mask conditional score? \n\nQ2. My major concern is why MSM is defined as in Eqn 3, and how it can be a good approximation of the true score $\\nabla \\log p(z_t)$ ? The definition seems to be based more on intuition rather than a principled approximation. The MSM score is the key component for everything else later, i.e., for both conditional and unconditional generation. So, I believe it is quite important to show (1) Eqn3 is an approximation of  $\\nabla \\log p(z_t)$  and (2) show that it is a good approximation (may be empirically on a toy use case if not in theory). Also, it is quite unclear how the approximation in Eqn 19 holds. Also, in Section 4 again, the theoretical analysis essentially talks about how efficient the stochastic approximation of MSM is compared to the true MSM, and I don’t see why this is more important than checking how efficiently the MSM score and its stochastic approximation can model the true score instead?\n\nQ3. In Algorithm 1, line 11, what is the mentioned Identity condition, i.e., the 2nd term? Is this a typo? Also, the need for the reinject and update procedure in lines 6 and 7 is not clearly explained in the paper.\n\nQ4. The whole section 3.3 is very unclear regarding the specific practical implementation of the posterior sampling with MSM. Specifically, the following text needs more explanation: “We solve the inverse problem by replacing the score function in the random walk sampling process with our posterior approximation. This approach avoids the need for automatic differentiation when computing the likelihood gradient, thereby reducing computational overhead.”? Is MSM posterior sampling not using DPS-style approximation? In that case, comparison with A-DPS for inverse problem solving seems unfair, because DPS is not exactly perfect either.\n\nQ5: What is P=0.4 in Table 1? What does this hyperparameter denote?\n\nOverall, I believe the paper has fundamental concerns, with a major concern being that the MSM score seems to be hand-designed. I believe that it is crucial to explain why/how this is a valid approximation in a principled manner (not just the fact that its Jacobian is symmetric)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eCw4VAUpFX", "forum": "pFByPVh6bd", "replyto": "pFByPVh6bd", "signatures": ["ICLR.cc/2026/Conference/Submission20078/Reviewer_a8c4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20078/Reviewer_a8c4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909678416, "cdate": 1761909678416, "tmdate": 1762932971696, "mdate": 1762932971696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to AC and reviewers"}, "comment": {"value": "Dear Reviewers and ACs,\n\nThank you all for reviewing our work and providing feedback. In response, we revised the manuscript to clarify several conceptual points and added new experimental results. To accommodate these additions, we moved Section 4 (Theoretical Analysis) to Appendix A, while retaining the main theoretical result in Section 3 (Method: Measurement Score-Based Diffusion Model). All newly added experiments and clarifications are incorporated into the paper (highlighted in brown text), and each change is referenced with its exact location in our responses.\n\nAuthors"}}, "id": "jmnyjZnnXu", "forum": "pFByPVh6bd", "replyto": "pFByPVh6bd", "signatures": ["ICLR.cc/2026/Conference/Submission20078/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20078/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20078/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763670732013, "cdate": 1763670732013, "tmdate": 1763670732013, "mdate": 1763670732013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method for learning diffusion-based priors on under sampled + corrupted data. Unlike previous approaches which learn image based denoisers/samplers the authors propose a method that learns partial score in the measurement domain of their sampling operators. Their results show improved unconditional and conditional sampling performance compared to previous SOTA self-supervised methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper attempts to solve an important problem in generative inverse problem solvers that has been previously explored by earlier work. The paper provides an original method for learning measurement scores. The method is communicated clearly and is backed by solid experiments in which they compare their technique to existing SOTA self-supervised generative and end-to-end techniques. Their experiments are convincing that their technique is better than existing self-supervised approaches."}, "weaknesses": {"value": "There are some experiments that I think would help strengthen the paper. An ablation over measurement noise would be good to show how performance varies over more than just a single noise level. This goes for both training and inference time. At a bare minimum we should see performance of the posterior solver at the same noise level as the training measurement noise (apologies if I misread and this is the case).  Along this same idea, It would also be good to make sure that when you are running inference for conditional sampling the results that are shown use test samples at the same under sampling level of the training data. For example, in Table 4  is it that case that CS-MRI (x6) inference is attempted using the model trained on x4 data? This is fine, but I would also like to see how inference on CS-MRI(x6) preforms using a model trained with x6 data. This is especially important when comparing to end-to-end methods like SSDU. Perhaps I have misread the results section, if so please just clarify."}, "questions": {"value": "1. what acceleration level was SSDU trained at?\n2. for the mri experiments were the sampling masks always 1-D?\n3. how robust is the method to different training/inference noise levels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vFPPIHIums", "forum": "pFByPVh6bd", "replyto": "pFByPVh6bd", "signatures": ["ICLR.cc/2026/Conference/Submission20078/Reviewer_hKzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20078/Reviewer_hKzq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920891536, "cdate": 1761920891536, "tmdate": 1762932970893, "mdate": 1762932970893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed Measurement Score-based Model (MSM) extends patch-based diffusion to the measurement domain by decomposing the forward operator in inverse problems (assume $y=Ax + e$) as a composition of a non-invertible operator H and an invertible operator T. The score functions are learned on (1) subsampled clean measurements $s=Sz$ and (2) subsampled noisy measurements, where $s=Sz + \\eta$, in both cases measurement scores are learned.\n\nAn algorithm for unconditional generation and posterior sampling is presented, using the expectation over partial (subsampled) measurement scores to approximate the full measurement score. Experiments on inpainting and MRI reconstruction demonstrate competitive performance compared baselines in particular Ambient-DPS.\n\nWhile the core idea of learning measurement-space scores without clean data is interesting and well motivated, the paper lacks self-consistency and clear organization, making it difficult to follow when reading it linearly."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Addresses a clinically important setting in MRI, where fully sampled data are rarely available, and demonstrates strong performance for both natural image inpainting and MRI reconstruction. \n\nProvides a theoretical formulation which links subsampled measurement scores to the full measurement score, thereby the authors extend prior score-decomposition ideas (patch-based) to the measurement setting."}, "weaknesses": {"value": "The method is developed and demonstrated primarily for subsampled measurements (e.g., incomplete k-space or inpainting), where spatial subsampling applies naturally. It is less clear how the formulation extends to other partial-measurement settings such as limited-angle tomography in CT, where missing data correspond to absent projection views rather than spatially masked measurements. Thus, the main practical advantage currently appears in MRI-like scenarios.\n\nThe paper lacks self-consistency in the description of its sampling procedures. Algorithm 1 details how the full measurement score is obtained, yet line 12, the actual reverse diffusion update, is not mentioned in Section 3.2. Since this step constitutes the core of the sampling process, it should be explicitly described or at least referenced later. Moreover, the posterior sampling introduced in Section 3.3 receives very limited explanation and no dedicated algorithm, despite being highlighted as one of the paper’s main contributions. From the written explanation I would assume that line 12 the posterior score is used, where the full measurement variable is already denoised and substituted for $z_t$ as a posterior approximation."}, "questions": {"value": "1) What is the advantage of directly training on the space of subsampled measurements in comparison to training on the corrupted space (i.e., in image domain)? Since this is part of the core contribution of this paper, it should be clearly discussed.\n\n2) Measurement noise $\\rho$ is usually not known in practice. Since the paper emphasizes the absence of clean samples as a key motivation, a discussion on the practical validity of assuming a known noise level seems important. How sensitive is the setup with respect to the measurement noise level?\n\n3) The theoretical analysis assumes i.i.d. sampling of the measurement operators $\\mathcal{S}$. In practice, acquisition patterns in MRI, CT, or PET are typically structured and hardware-constrained rather than i.i.d., so the practical validity of this assumption should be discussed.\n\n4) In Eq. (4), the maximum operation in computing the weighting function is reasoned by avoiding division by 0. However, in my understanding this would anyways be detrimental to the sample quality, if there are regions not covered at all by the generation process. Are there any insights on how many sampling steps and stochastic loop iterations are needed to ensure this? \n\n5) Evaluating the FID based on 3k samples (L1126) is a rather low sample number, at least 10k should be used. This is particularly important for unconditional generation, where diversity matters and FID variance can be high. In any case, if the choice is substantially lower than common practice, it should be discussed and justified.\n\n6) Given that MSM requires stochastic loops w and aggregation of partial measurement scores, which is not the case for Ambient DPS, a comparison on inference time would be interesting. \n\n\nMinor comments\nL262: box inpainting is not a degradation operator\nL166: $s$ is not defined\nL868: $\\mathcal{S}$ is not a forward operator, it should say subsampling operator"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l9BtM6ALvw", "forum": "pFByPVh6bd", "replyto": "pFByPVh6bd", "signatures": ["ICLR.cc/2026/Conference/Submission20078/Reviewer_8nET"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20078/Reviewer_8nET"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939569386, "cdate": 1761939569386, "tmdate": 1762932969592, "mdate": 1762932969592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}