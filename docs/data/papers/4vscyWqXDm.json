{"id": "4vscyWqXDm", "number": 11216, "cdate": 1758193688321, "mdate": 1759897600631, "content": {"title": "GenCue: Generation-Oriented Video Captions for High-Fidelity Text-to-Video", "abstract": "The performance of text-to-video (T2V) models critically depends on the quality of their training captions. However, captions produced by current multimodal large language models (MLLMs) often lack fine-grained visual grounding, temporal coverage, and cinematic expressiveness, limiting models’ ability to accurately follow instructions and reconstruct details and dynamics. We present GenCue, a generation-oriented video captioning framework designed to close this gap and enable high-fidelity T2V training. We first introduce the GenCue-SFT-1M and GenCue-RL-8k data suite: the former is a large-scale, schema-guided corpus aided by specialized expert models, while the latter is a carefully curated, high-quality subset providing precise supervision signals for post-training. Building on this data foundation, we propose a reinforcement learning paradigm with a checklist-based reward that explicitly evaluates key generation dimensions. We further introduce Reference-Augmented GRPO and a prefix-sharing rollout strategy, which together enable effective and efficient long-context optimization. Experiments on T2V captioning and reconstruction benchmarks demonstrate that GenCue significantly outperforms prior approaches, yielding substantial improvements in object coverage, temporal coherence, and cinematic quality.", "tldr": "", "keywords": ["Video Large Language Models", "Video Detailed Captioning", "Text-to-Video Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4900543e228881ea858873a848f089443ae3b1bb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces GenCue, a generation-oriented video captioning framework designed to improve the fidelity of text-to-video (T2V) models by addressing the limitations of existing training captions. Current captions generated by multimodal large language models often lack fine-grained visual grounding, temporal coverage, and cinematic expressiveness, which hampers the ability of T2V models to accurately reconstruct details and dynamics. GenCue tackles this issue through both data and training innovations.\n\nOn the data side, the authors present two key datasets: GenCue-SFT-1M and GenCue-RL-8k. GenCue-SFT-1M is a large-scale, schema-guided corpus that leverages expert models for cinematographic attributes such as shot type, camera motion, and angle. These signals are used to prompt a multimodal language model to produce structured JSON descriptions, which are then distilled into fluent, detailed captions. GenCue-RL-8k is a smaller, high-quality subset curated for reinforcement learning, featuring manually verified captions and a dense set of verification questions that serve as supervision signals.\n\nOn the training side, GenCue employs a reinforcement learning paradigm with a checklist-based reward function that evaluates captions across four dimensions: scene content, temporal dynamics, visual presentation, and cinematography. This reward is computed using a set of yes/no questions derived from the reference captions, providing fine-grained and interpretable feedback. To improve learning stability and efficiency, the authors introduce Reference-Augmented GRPO (RefGRPO), which incorporates reference captions into the policy optimization process when model rollouts underperform. Additionally, a prefix-sharing rollout strategy called PrefixGrouper is proposed to reduce computational overhead in long-context video training, achieving up to 90% reduction in FLOPs and 60% reduction in memory usage.\n\nEmpirically, GenCue significantly outperforms prior methods on T2V captioning benchmarks, demonstrating superior object coverage, temporal coherence, and cinematic expressiveness. It also improves the performance of downstream T2V generation models such as Wan2.1 and HunyuanVideo, as measured by CLIP similarity, FVD, and LPIPS metrics. Ablation studies confirm the effectiveness of the two-stage training strategy and the individual components of the reinforcement learning framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a thorough exploration of existing techniques in the context of text-to-video generation. The authors effectively synthesize previous work and build upon established methods to create a practical system. The paper’s strength lies in its application of reinforcement learning and schema-guided caption generation, which contributes to improving the fidelity and expressiveness of video captions. By addressing challenges in video-text alignment, such as object coverage, temporal coherence, and cinematographic expressiveness, the paper offers a useful contribution to improving the performance of T2V models without introducing drastic changes to existing approaches. This practical approach provides incremental progress in the field.\n\n2. The significance of the paper is clear. The approach presented, particularly the schema-guided method for generating high-fidelity captions, provides a practical framework for improving caption quality in text-to-video (T2V) models. This method offers a solid foundation for the development of more accurate captions, which are crucial for enhancing T2V generation. The proposed evaluation checklists are also useful, as they can be easily reused for future work in this area, supporting further progress in T2V research. Additionally, the prefix-sharing technique introduced by the authors is domain-agnostic, meaning it could be applied to various long-context reinforcement learning (RL) tasks. This makes the technique adaptable to other applications beyond video captioning, such as text-only RLHF or even audio generation, broadening its potential applicability.\n\n3. The paper is well-written, with a clear structure and logical flow. The authors effectively present their methodology and results, making it easy for readers to follow the progression of their work. The writing is concise and technically sound, ensuring that the complex concepts are well articulated for the target audience. Additionally, the authors provide useful details and clarifications on specific techniques, such as the PrefixGrouper, which are thoroughly explained in the appendix. This enhances the paper’s clarity and ensures that key components of the methodology are well-documented for readers."}, "weaknesses": {"value": "1. The paper has limited novelty, as schema-guided video caption data generation has already been extensively explored in prior works [1,2,3,4]. Furthermore, techniques such as rewarding captions through content-focused checklist and context-sharing acceleration for video RL have been investigated in related research [2,5]. While the paper offers a solid application of these methods, it does not significantly advance the underlying concepts, which have been well established in the field.\n\n2. The experiments presented in the paper are not comprehensive enough. It would be beneficial to evaluate on denser video caption datasets like Auroracap[2], MiraData[1], VideoEspresso[6], and ShareGPT4Video[3], etc using stronger backbone (such as Qwen2.5-VL-7B or Qwen3-VL) and provide ablation studies comparing the performance on these datasets. This would give a clearer understanding of how the proposed approach performs under more complex conditions and on richer data sources.\n\n3. GenCue-RL-8k is relatively small and heavily filtered, which limits the generalizability of the results. At a minimum, the authors should provide a learning curve showing how the VDC score changes with the number of RL training clips (e.g., 1k, 2k, ..., 8k) to assess whether performance plateaus early.\n\n4. The paper lacks open-sourced corresponding data, model checkpoints, and codebase, making it difficult for others to verify the results and reproduce the findings.\n\n[1] MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions. arXiv 2024.\n[2] AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark. ICLR 2025.\n[3] ShareGPT4Video: Improving Video Understanding and Generation with Better Captions. NeurIPS 2024 D&B Track.\n[4] LLaVA-Video: Video Instruction Tuning With Synthetic Data. TMLR 2025.\n[5] Long-RL: Scaling RL to Long Sequences. NeurIPS 2025.\n[6] VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection. CVPR 2025."}, "questions": {"value": "1. What if no threshold is set, and each GRPO batch is trained using references?\n2. Why is it said that structured designs like MiraData are helpful, yet fine-grained details, coherence, and cinematic expressiveness remain challenging, limiting high-fidelity T2V generation in related work? Does the author's approach, compared to MiraData, transform this structural information into natural text using large models?\n3. Could the author release the GenCue model shown in Figure 1, along with the corresponding videos, for verification?\n4. Where can I find the demo mentioned in line 247, i.e. roughly 120 verification points?\n5. What is the performance impact curve of GenCue-RL-8k on the model? If the 8k data is scaled up, can the model's performance continue to improve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ihpuWnFAG3", "forum": "4vscyWqXDm", "replyto": "4vscyWqXDm", "signatures": ["ICLR.cc/2026/Conference/Submission11216/Reviewer_2ERb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11216/Reviewer_2ERb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760689145396, "cdate": 1760689145396, "tmdate": 1762922362987, "mdate": 1762922362987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GenCue, a framework for generating high-fidelity, generation-oriented video captions to improve text-to-video (T2V) models. The authors identify that existing captions lack fine-grained detail and cinematic expressiveness. To address this, they propose a multi-faceted solution: (1) Two new datasets, GenCue-SFT-1M for supervised fine-tuning, created via a schema-guided pipeline with specialized expert models, and GenCue-RL-8k, a curated high-quality set for reinforcement learning. (2) A novel RL training paradigm featuring a checklist-based reward, Reference-Augmented Group Relative Policy Optimization (RefGRPO) to provide guidance on difficult samples, and a PrefixGrouper strategy to make long-context RL computationally efficient. Experiments show that GenCue significantly outperforms prior methods on video captioning benchmarks and that its generated captions lead to higher-quality video generation when used with existing T2V models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel and comprehensive framework that tackles the critical problem of data quality for T2V training. The combination of schema-guided data generation, specialized expert models, and a sophisticated RL pipeline (checklist reward, RefGRPO, PrefixGrouper) is a creative and original approach to video captioning.\n\n2. The work is of high technical quality. The proposed methods are well-motivated and technically sound, particularly the efficiency improvements from PrefixGrouper, which are supported by theoretical analysis (Appendix B). The creation of two purpose-built datasets (SFT and RL) is a substantial contribution. The experimental evaluation is extensive, covering multiple captioning benchmarks and a downstream T2V generation task, with thorough ablation studies that validate the design choices.\n\n3. The paper is exceptionally well-written and easy to follow. The motivation is clearly articulated, and the proposed components are explained logically. Figures 1, 2, and 3 are highly effective at illustrating the core ideas, from the qualitative improvement of captions to the data pipeline and RL framework."}, "weaknesses": {"value": "1. The data generation and reward modeling pipeline relies heavily on proprietary, closed-source models (e.g., Gemini series). While the resulting datasets will be released, this reliance makes the core data generation process not fully reproducible with open-source tools and may introduce unknown biases from the proprietary models.\n2. The \"expert models\" used for cinematic features (shot type, camera motion) are a key component of the data pipeline but are not described in detail, as they are trained on \"proprietary annotated multi-dimensional labels.\" The lack of information about their architecture, training data, and performance makes it difficult to assess their contribution and potential error propagation.\n3. The evaluation of T2V generation is indirect. The paper shows that captions from GenCue improve existing T2V models, but it does not include experiments where a T2V model is trained from scratch on the new GenCue-SFT-1M dataset. While understandable due to computational costs, this leaves the full potential of the dataset for T2V training undemonstrated."}, "questions": {"value": "1. The checklist-based reward relies on an LLM judge. How was the reliability of this judge model validated? Is there a risk of the policy model learning to exploit systematic flaws or biases in the judge, and have you considered using human evaluation or an ensemble of judges to mitigate this?\n2. In RefGRPO, could you provide more intuition for **the reward for the reference caption** this specific design choice? Did you experiment with alternative strategies for setting the reference reward, such as a fixed high value or a score derived from a different metric?\n3. The efficiency gains of PrefixGrouper are impressive. Could you clarify if this technique is applicable to other sequence-to-sequence tasks that use RL with multiple rollouts, or are there specific properties of video captioning or your GRPO setup that make it uniquely suitable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "heB8dbrxRJ", "forum": "4vscyWqXDm", "replyto": "4vscyWqXDm", "signatures": ["ICLR.cc/2026/Conference/Submission11216/Reviewer_nVKX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11216/Reviewer_nVKX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378792560, "cdate": 1761378792560, "tmdate": 1762922362487, "mdate": 1762922362487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation in text-to-video (T2V) generation: the lack of high-quality training captions with fine-grained visual grounding, temporal coverage, and cinematic expressiveness. The authors present GenCue, a generation-oriented video captioning framework consisting of two core components: a data suite and a reinforcement learning (RL) training paradigm.\n\nFor the data foundation, they construct GenCue-SFT-1M, a large-scale, schema-guided corpus aided by specialized expert models (for shot type, camera motion, etc.) that distill structured JSON descriptions into fluent captions. They also curate GenCue-RL-8k, a high-quality subset with content-focused checklists for precise RL supervision. On the learning side, GenCue introduces a checklist-based reward evaluating four key dimensions (scene content, temporal dynamics, visual presentation, cinematography), Reference-Augmented GRPO (RefGRPO) for stable learning on challenging samples, and a prefix-sharing rollout strategy that reduces FLOPs by up to 90% and memory usage by 60% for long-context optimization.\n\nExperiments on T2V captioning benchmarks (VidCapBench, VDC, CaReBench) show GenCue outperforms prior open-source models in object coverage, temporal coherence, and cinematic quality. When used to guide T2V models (Wan2.1, HunyuanVideo), it also improves metrics like CLIP similarity, FVD, and LPIPS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work introduces a novel schema-guided approach to video captioning that explicitly integrates cinematographic elements (shot types, camera motions) and temporal dynamics—dimensions often overlooked in existing MLLM-generated captions. The combination of RefGRPO (reference-anchored RL) and prefix-sharing rollout is a creative adaptation of RL techniques to the long-context challenge of video captioning, addressing limitations of standard GRPO in underperforming rollouts and computational efficiency."}, "weaknesses": {"value": "- Data Source Transparency: A major gap is the lack of details on the video sources for GenCue-SFT-1M. The paper does not specify whether the videos are from existing public datasets (e.g., WebVid, Kinetics) or proprietary collections, nor does it describe selection criteria (e.g., video duration, genre diversity). This limits reproducibility and makes it hard to assess potential biases or domain coverage of the dataset.\n- Limited T2V Evaluation Metrics: The T2V generation experiments rely solely on traditional automatic metrics (CLIP similarity, FVD, LPIPS), which are known to correlate poorly with human perception of video quality (e.g., realism, temporal coherence, adherence to cinematographic cues). These metrics fail to capture whether GenCue's improved captions translate to more visually appealing or professionally styled videos as claimed.\n- Insufficient Qualitative Examples: The only qualitative T2V example (Figure 6) is insufficient to demonstrate GenCue's advantages. Moreover, the GenCue-3B-generated video in this figure contains visible artifacts (e.g., distorted arm), which raises concerns about the practical impact of the framework. Additional qualitative comparisons—including side-by-side samples of original videos, GenCue captions, and generated videos—are needed to validate the claimed improvements in cinematic quality and detail."}, "questions": {"value": "1. Could you provide detailed information on the video sources for GenCue-SFT-1M? For example: (1) Are the videos from public datasets or proprietary data? (2) What is the distribution of video durations, genres, and resolutions? (3) Were any filters applied to select videos (e.g., excluding low-quality or static content)?\n2. 2. Given the limitations of automatic metrics for T2V quality, do you plan to conduct human evaluations? If so, could you share details on the evaluation protocol (e.g., metrics like realism, cinematographic adherence, caption-video alignment) and preliminary results? If not, why do you believe the current automatic metrics are sufficient to demonstrate GenCue's value for T2V?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The original source and the collection paradigm of the GenCue video dataset is not discussed in this paper."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XhdWZP15TQ", "forum": "4vscyWqXDm", "replyto": "4vscyWqXDm", "signatures": ["ICLR.cc/2026/Conference/Submission11216/Reviewer_24AU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11216/Reviewer_24AU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751765510, "cdate": 1761751765510, "tmdate": 1762922361793, "mdate": 1762922361793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GenCue, a generation-oriented video captioning framework to produce captions that are explicitly useful for training high-fidelity text-to-video (T2V) models. On the data side, the authors build GenCue-SFT-1M and GenCue-RL-8k. On the learning side, they design a checklist-based reward across four dimension, Reference-Augmented GRPO and a prefix-sharing rollout strategy for efficient long-context optimization. Experiments show SOTA results on T2V-oriented captioning and improvements on T2V reconstruction metrics."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The combination of schema-guided data creation, four-dimension checklist rewards, and RefGRPO is novel and well aligned to T2V fidelity. \n2. Solid ablations: two-stage SFT+RL > single stage; removing the reference in RefGRPO drops VDC from 51.5 to 49.0, and removing the precision term also hurts, indicating each component’s necessity. \n3. Clear pipeline description and illustrations.\n4. SOTA on VidCapBench and strong results on VDC/CaReBench, plus better T2V reconstruction metrics with Wan2.1/HunyuanVideo."}, "weaknesses": {"value": "1. Human evaluation. The T2V side relies on automatic metrics (CLIP/FVD/LPIPS). Given known limitations of these metrics, a controlled human study of instruction following and cinematography adherence would strengthen claims. \n2. Model transferability. Most results are with Qwen-VL. It would be helpful to quantify transfer to other video-LLMs or diffusion-transformer T2V models and report any adaptation overhead/failure problems."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VfEyX5dNVX", "forum": "4vscyWqXDm", "replyto": "4vscyWqXDm", "signatures": ["ICLR.cc/2026/Conference/Submission11216/Reviewer_1Nfi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11216/Reviewer_1Nfi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895756916, "cdate": 1761895756916, "tmdate": 1762922361322, "mdate": 1762922361322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}