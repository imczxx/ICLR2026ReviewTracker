{"id": "ser00zCWC2", "number": 18453, "cdate": 1758287954779, "mdate": 1759897102512, "content": {"title": "Adversarial Co-Evolution of LLM-Generated Policies and Environments via Two-Player Zero-Sum Game", "abstract": "A central challenge towards building agents that continually improve is that training environments are typically fixed or manually designed. This restricts continual learning and generalization beyond the training distribution. We address this with Covolve, a co-evolutionary framework that leverages large language models (LLMs) to generate both environments and agent policies, expressed as executable Python code. We model the interaction between environment and policy designers as a two-player zero-sum game, ensuring adversarial co-evolution in which environments expose policy weaknesses and policies adapt in response. To guarantee robustness and prevent forgetting, we compute the mixed strategy Nash equilibrium (MSNE) of this game, yielding a meta-policy that remains robust across all generated environments rather than overfitting to the most recent one. This process induces an automated curriculum in which environments and policies co-evolve toward increasing complexity. Experiments in urban driving, maze-solving, and 2D navigation showcase that Covolve produces progressively more complex environments. The MSNE meta-policy also ensures that the agent does not forget to solve previously seen environments, all the while learning to solve unseen ones. These results demonstrate the potential of LLM-driven co-evolution to achieve open-ended learning without predefined task distributions or manual intervention.", "tldr": "We propose a co-evolutionary framework using LLMs to jointly generate agents and environments as executable code, producing robust, self-improving policies via a curriculum shaped by a game-theoretic interaction.", "keywords": ["Code-as-Policy", "Large Language Models", "Unsupervised Environment Design", "Policy–Environment Co-evolution", "Zero-Sum Games"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ca8669092ab5e177b1efdc839a8517479741008.pdf", "supplementary_material": "/attachment/cf5361e39353d4b3479fabbf74ac560df8335d68.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Covolve, where an LLM generates both environments and policies as executable code. As a two-player zero-sum game, the method computes a mixed-strategy Nash equilibrium (MSNE) over the generated environments and policies, inducing them to co-evolve toward increasing complexity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Using an MSNE helps robust co-evolution across the generated environments, rather than overfitting to the most recent one."}, "weaknesses": {"value": "* As iterations proceed, the number of stored policies and environments grows, making payoff-matrix evaluation and MSNE solving increasingly expensive.\n* Because the environment designer aims to increase difficulty, it may generate environments that are too hard to learn or even unsolvable.\n* Generating policies and environments with an LLM requires prompt engineering and post-generation filtering.\n* Empirical comparisons focus on Eurekaverse; other UED baselines (e.g., [1]) are not compared.\n\n[1] Jiang et al., \"Replay-Guided Adversarial Environment Design\", 2021"}, "questions": {"value": "* How were the prompts in Appendix C designed and selected?\n* What fraction of generated environments or policies are filtered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LxdptGoPTp", "forum": "ser00zCWC2", "replyto": "ser00zCWC2", "signatures": ["ICLR.cc/2026/Conference/Submission18453/Reviewer_6dck"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18453/Reviewer_6dck"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926542330, "cdate": 1761926542330, "tmdate": 1762928152523, "mdate": 1762928152523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors propose a method that iteratively improves both the policy and the environment using LLM-based code generation.\n- They apply PSRO’s multi-agent framework by modeling the policy and environment as a two-player game.\n- The ultimate goal is to produce a policy robust to diverse environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- To address the forgetting issue in the co-evolution framework, the authors propose storing both the environment set and the policy set."}, "weaknesses": {"value": "- Modeling agent-environment co-evolution as a zero-sum game may not be the most suitable formulation.\n    - As noted in the paper, the key goal in environment design is to create environments that are solvable yet appropriately challenging.\n    - However, the proposed method, framed as a zero-sum (minimax) game, can generate unsolvable environments.\n    - Even with heuristic solvability filtering, the following issues may arise:\n        - During iteration, a $\\theta_i$ may be sampled that most policies cannot solve.\n        - All policies in $P$ yield $U = 0$ for $\\theta_i$.\n        - The solution to Eq. 1 becomes non-unique, corrupting the MSNE policy.\n        - Consequently, meaningful environments and policies become difficult to generate in subsequent iterations.\n- Unfair comparison with Eurekaverse.\n    - While it is acknowledged that forgetting can occur in Eurekaverse, the original method does not update the agent and environment in a zero-sum game manner. In contrast, this paper implements it using a zero-sum formulation.\n    - As discussed in the above weakness, the zero-sum game approach risks destabilizing learning, which likely accounts for the degraded performance of Eurekaverse in the experiments.\n    - Therefore, the version of Eurekaverse implemented in this paper differs significantly from the original; it would be more appropriate to report its results under a new name, such as **CoEvolve-Naive**, and it is required to re-conduct comparisons with the original Eurekaverse under equivalent experimental settings.\n- Storing policy and environment sets, necessary for mitigating forgetting, incurs increased computational complexity.\n    - As iterations progress, evaluation time for policies and environments grows, leading to quadratic overall time complexity.\n    - Additionally, deploying the obtained MSNE policy requires storing the entire policy set.\n- Figure 2 caption claims the proposed method guarantees solvable environments, but no such contribution exists in the main text."}, "questions": {"value": "- The RL comparison in Section 5.4 (Ablation Study) seems problematic.\n    - First, was the baseline trained using the same curriculum-based approach as CoEvolve?\n        - If not, RL policies are trained directly on difficult environments from the start, whereas CoEvolve uses code-as-policy (CAP) with a curriculum.\n        - This makes the comparison unfair, and it is invalid to conclude that CAP outperforms the RL policy.\n    - The learning curves show high initial rewards that decrease over training.\n        - While the reward return can be truncated in RL, a decline in performance raises serious concerns that the hyper-parameters may have been improperly tuned or the experiment misconfigured."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C4zK0cAg6P", "forum": "ser00zCWC2", "replyto": "ser00zCWC2", "signatures": ["ICLR.cc/2026/Conference/Submission18453/Reviewer_XFc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18453/Reviewer_XFc4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979062612, "cdate": 1761979062612, "tmdate": 1762928152084, "mdate": 1762928152084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to co-evolve agents and environments, represented as code, by framing things as a minimax game between an environment designer and a policy generator. They use PSRO as the self-play algorithm for environment and generator."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think this is an interesting approach, of generating both environments and code, while using PSRO to find the equilibrium of this selection process. It also has nice results indicating that without the curriculum the final levels would be too hard. Finally, the generated levels seem to be quite diverse and interesting."}, "weaknesses": {"value": "- I somewhat object to this sentence in the intro \"However, UED typically generates environments via randomization or simple heuristics – rather than intelligently – limiting diversity and relevance.\" This is a pretty strong, and I think false, statement given that many UED methods use learning algorithms in the loop to try to design the environments. Also, just as a rule, calling your method \"intelligent\" and other methods as \"unintelligent\" is uh, not great. Relatedly, the paper contains claims like \"Crucially, our LLM-driven co-evolution introduces intelligent, data-driven priors that enable the design of more challenging and relevant environments than classical, heuristic-based UED\" but does not contain comparisons to methods from the UED literature, just to eurekaverse.\n\n- The biggest issue in this paper is that there's an asymmetry between the generator and the policy wherein the generator can just create totally unsolvable environments. The appendix vaguely mentions that they only include levels if they're solvable but this needs to be way more prominent if this is the case because it's very crucial! Without this, the fact that it works at all would entirely be contingent on the weakness of the LLM as a generator i.e. it is unable to find these unsolvable maps. The very first time an LLM generates an unsolvable map, this will be the only sampled map in the future. I'm assuming that the point in the appendix about only including a map if some policy solves it is how this is resolved but this needs to be much more prominently displayed. \n\n- The main change in this paper, as far as I can tell, is to use PSRO instead of simply keeping the next best policy as in their eurekaverse policy. The subsequent improvement this entails seems to be quite small, though, it is hard to tell because they use only a single run."}, "questions": {"value": "- I'm not sure I understand the logic, or soundness, of reporting results from a single run. The claim in the paper is that this would mess things up since evaluation sets would be different across runs. The solution would seem to be to design your evaluation set in advance. Have I perhaps misunderstood your statistical estimation procedure? In table 1, you report a standard error (I think?) is this across seeds or rollouts?\n- Why do the scores of the policies decrease across levels in figure 6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TVqSb7zkLc", "forum": "ser00zCWC2", "replyto": "ser00zCWC2", "signatures": ["ICLR.cc/2026/Conference/Submission18453/Reviewer_pTWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18453/Reviewer_pTWM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033974796, "cdate": 1762033974796, "tmdate": 1762928151733, "mdate": 1762928151733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Covolve, a framework for co-evolving LLM-generated policies and environments. Each environment and policy is expressed as Python code produced by a large language model. Covolve formulates this interaction as a two-player zero-sum game between an environment designer and a policy designer. The framework maintains populations of both, evaluates their pairwise interactions to form a payoff matrix, and computes a mixed-strategy Nash equilibrium using a Policy Space Response Oracle procedure. The MSNE is then used to generate new environments adversarially and new policies as best responses, forming an automated curriculum. Experiments are presented on MiniGrid, PyGame 2D navigation, and CARLA driving. Reported results include visual examples of environments that become more complex over iterations, performance comparisons against the Eurekaverse baseline, ablations showing the effect of MSNE mixtures versus latest-only updates, and limited generalization tests on a few standardized environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The setup as a two-player zero-sum game with empirical PSRO computation is explicitly defined and implemented.\n- Policies and environments are represented as Python code that can be directly executed, improving transparency relative to neural-only representations.\n- The appendix includes pseudocode, LLM prompts, and environment specifications.\n- Figures show increasing environment complexity over iterations.\n- Plots comparing Covolve’s MSNE mixture against Eurekaverse’s latest-only policy show that performance on earlier environments decreases less sharply.\n- Positive generalization tests."}, "weaknesses": {"value": "- The paper states that runs diverge because generated environments differ. All main results come from a single run per domain. There are no averages or standard deviations across random seeds, limiting statistical confidence.\n- Generalization is tested on three environments only (two MiniGrid, one CARLA). No results are reported on PyGame test environments outside the training loop.\n- The paper notes that generated levels may be unsolvable and that helper functions or reachability checks are used to filter them. The frequency or effect of these filters is not reported.\n- Eurekaverse is the only adaptive baseline evaluated. Other unsupervised environment design methods (PAIRED, ACCEL, ...) are mentioned but not compared experimentally.\n- The framework evaluates multiple LLM-generated policies and environments per iteration, but runtime, computational cost, and scaling with candidate count are not reported.\n- The MSNE equilibrium is computed on a finite payoff matrix built from a limited archive of policies and environments. Robustness is shown only within this archive and extrapolation to unseen tasks is not analyzed beyond Table 1.\n- Success rates in tables are given as means but not accompanied by confidence intervals or number of trials, limiting interpretability."}, "questions": {"value": "- What proportion of generated environments fail solvability or reachability checks during evolution?\n- How many iterations and candidates per iteration are used, and what are the associated compute costs?\n- Are the generalization results stable across multiple independent runs?\n- How sensitive is the MSNE equilibrium to population size and candidate sampling?\n- Could comparisons to existing UED methods (PAIRED, ...) be added under similar compute budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nDf3DYMYji", "forum": "ser00zCWC2", "replyto": "ser00zCWC2", "signatures": ["ICLR.cc/2026/Conference/Submission18453/Reviewer_1ToJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18453/Reviewer_1ToJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034341320, "cdate": 1762034341320, "tmdate": 1762928149772, "mdate": 1762928149772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}