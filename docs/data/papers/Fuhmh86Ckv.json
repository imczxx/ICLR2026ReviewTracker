{"id": "Fuhmh86Ckv", "number": 22362, "cdate": 1758330045198, "mdate": 1763140055287, "content": {"title": "What Can You Do When You Have Zero Rewards During RL?", "abstract": "Reinforcement learning (RL) with outcome-based rewards has proven effective for improving large language models (LLMs) on complex reasoning tasks. However, its success often depends on the base model occasionally sampling correct solutions. When no correct solutions are sampled, training encounters a zero-reward barrier where learning stalls due to zero gradients. We study this scenario through the graph search task and evaluate recent methods that incorporate desirable components such as dense rewards, diversity incentives, and improved credit assignment. Our experiments show that none of these approaches overcome the zero-reward barrier if the base model never produces a correct answer. In contrast, we find that a simple data-centric intervention of adding easier samples to the training set enables the model to eventually solve the original hard task despite starting from zero reward. Importantly, this succeeds without modifying the RL algorithm itself. Because official implementations of several baselines were unavailable, we developed our own, which allowed us to conduct a detailed analysis of their failure modes. We release these implementations to support further research: [https://github.com/anon-zero-rewards/zero-rewards-rl](https://github.com/anon-zero-rewards/zero-rewards-rl).", "tldr": "Under zero outcome rewards on a simple graph search task, recently proposed methods fail. A simple data-centric intervention works suprisingly well.", "keywords": ["zero rewards", "data centric RL", "exploration"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1532145ea60e00de3b7ef7dc14b3e6f6b47d2cd3.pdf", "supplementary_material": "/attachment/10a1a7620f8604680f12e31bba3b81ef6e743c99.zip"}, "replies": [{"content": {"summary": {"value": "This work investigates the zero-reward barrier in reinforcement learning for large language models. These are scenarios where the base model cannot generate any correct solutions during training, causing learning to stall. Using a graph search task from Bachmann and Nagarajan (2024), the authors evaluate recent RL methods designed for sparse rewards: VinePPO (credit assignment), Rewarding Progress (reward shaping), and Best-of-N aware finetuning (diversity incentives). The authors implement baselines (providing code) and provide detailed failure analyses. They release their implementations to support further research.\n\nThe strengths of the work are as follows. First, the zero-reward barrier is a fundamental challenge limiting RL scalability for LLMs. Second, the authors implement and rigorously test multiple recent methods specifically designed for sparse rewards. Third, there is a thorough failure mode analysis on dense rewards. Finally, the work is reproducible and the authors provide complete code.\n\nThe weaknesses of the work are as follows. First, there is a limited task diversity since the work only focuses on graph search tasks. It would be great to see some analysis on if this transfers to math, coding, etc. Second, there is only experiments on a 1.5B model. It would be great to see larger models, to see if the trend holds. Finally, some improvements to presentation (e.g. graph readability) could be made in the paper, spacing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the work are as follows.\n- First, the zero-reward barrier is a fundamental challenge limiting RL scalability for LLMs.\n- Second, the authors implement and rigorously test multiple recent methods specifically designed for sparse rewards. \n- Third, there is a thorough failure mode analysis on dense rewards.\n- Finally, the work is reproducible and the authors provide complete code."}, "weaknesses": {"value": "The weaknesses of the work are as follows.\n- First, there is a limited task diversity since the work only focuses on graph search tasks. It would be great to see some analysis on if this transfers to math, coding, etc.\n- Second, there is only experiments on a 1.5B model. It would be great to see larger models, to see if the trend holds.\n- Finally, some improvements to presentation (e.g. graph readability) could be made in the paper, spacing."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xrMJtHYe9Q", "forum": "Fuhmh86Ckv", "replyto": "Fuhmh86Ckv", "signatures": ["ICLR.cc/2026/Conference/Submission22362/Reviewer_S9eA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22362/Reviewer_S9eA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869312992, "cdate": 1761869312992, "tmdate": 1762942186257, "mdate": 1762942186257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "8VHfZk4M30", "forum": "Fuhmh86Ckv", "replyto": "Fuhmh86Ckv", "signatures": ["ICLR.cc/2026/Conference/Submission22362/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22362/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763140054573, "cdate": 1763140054573, "tmdate": 1763140054573, "mdate": 1763140054573, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores whether large language models can perform reinforcement learning (RL) style reasoning in a synthetic sparse-reward environment.\nThe authors design a graph-search task (“Degree-10-Path-10”) and train the LLM through policy-gradient updates using only a few sampled completions per input.\nThey claim that the failure of training indicates the fundamental infeasibility of applying RL to LLMs for complex reasoning and their curriculum training can solve it."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an ambitious question about combining LLMs and RL, which is of high interest to the community.\n\nThe authors show enthusiasm in connecting classical RL problems to language-based reasoning, and the overall motivation is understandable."}, "weaknesses": {"value": "1) Insufficient exploration and unrealistic sampling setup.\n\nThe experiments rely on only a few rollouts per data point (five completions), under a completely sparse 0/1 reward.\nThis setup guarantees that almost no successful trajectories are ever observed, so gradients vanish and training fails by design.\nThe paper then interprets this trivial failure as a general limitation of LLMs, which is misleading.\nRL methods typically require thousands to millions of rollouts for such problems; the present configuration is not a valid test of feasibility.\n\n2) Compute–task imbalance.\n\nThe graph-search problem (Degree-10-Path-10) explodes combinatorially in difficulty, but the compute and sample budgets remain fixed and minimal.\nThe failure is thus compute-bound rather than method-bound.\nNo scaling study is provided to show where learning begins to succeed or how the outcome changes with more computation.\nThe conclusion that “RL cannot work for LLMs” is not supported without such evidence.\n\n3) Superficial notion of curriculum learning.\n\nThe “curriculum” is a manually mixed dataset of easy and hard samples at a fixed ratio.\nIt is static, non-adaptive, and not connected to the model’s learning progress.\nThis is better described as data filtering, not curriculum learning.\nIn modern RL, curricula are dynamic and algorithmic—tasks or rewards evolve based on the agent’s competence.\nThe paper misses this key distinction and offers no analysis of why or when their fixed mixture helps.\nMoreover, the current setup risks overfitting to “easy” samples and does not test generalization to unseen or multi-solution cases.\nThe authors also have no idea about the works of the meta-learning and curriculum learning community from RL.\nI would expect more holistic solutions rather than simply picking samples. \nWhen the algorithm try more reasoning tasks, do you also manually give easier samples?  \n\n4) Missing comparisons and baselines.\n\nNo results from standard RL algorithms are shown for the same environment.\nWithout knowing that a baseline agent can solve the task, the “failure” of the LLM provides little information.\nThe paper also lacks ablations on rollout number, reward shaping, or task scaling.\n\n5) Overstated conclusions.\n\nThe authors generalize their negative results to claim that RL on LLMs is infeasible, but the evidence only shows that the current configuration is under-explored and under-resourced.\nThe conclusions are therefore not justified by the experiments.\n\n6) Clarity and structure issues.\n\nThe writing is fragmented, with vague definitions, missing implementation details, and inconsistent terminology (e.g., “rollout,” “curriculum,” “episode”).\nThe overall presentation makes it difficult to follow the core argument."}, "questions": {"value": "What happens if the number of rollouts or compute budget is scaled up by one or two orders of magnitude? Does performance improve at all?\n\nCould the authors test smaller or simpler tasks (e.g., lower degree/path length) to see when learning actually starts?\n\nCan they design a dynamic curriculum that adjusts difficulty automatically instead of fixing ratios?\n\nHow do classical RL baselines and other coldstart RL algorithms perform on the same environment? Without comparison, it is unclear whether the task itself is solvable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BIvkjkRWN2", "forum": "Fuhmh86Ckv", "replyto": "Fuhmh86Ckv", "signatures": ["ICLR.cc/2026/Conference/Submission22362/Reviewer_s9U5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22362/Reviewer_s9U5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895207206, "cdate": 1761895207206, "tmdate": 1762942186058, "mdate": 1762942186058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper studies the “zero-reward barrier” in RL post-training for LLMs: when a base model never samples a correct answer, outcome-based RL stalls. Using a controllable graph search task (Degree-10-Path-10), the authors evaluate Dr.GRPO, VinePPO, Rewarding Progress, and BoN-aware finetuning and report persistent zero success.\n\nCore result: a data-centric fix—mixing in easier instances (not too easy) enables learning on the hard task without changing the RL algorithm; equal-mixing all difficulties also works."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "All baselines flatline at zero success on Deg-10-P10, and the paper convincingly rules out trivial explanations by showing Deg-3-P3 is solvable.It offers a simple fix: include easier instances (Deg-5-P5) or mix difficulties, which reliably unlocks learning under outcome rewards. They explain why step-credit still gives no signal when success=0 and why BoN is unstable without strong KL/clipping."}, "weaknesses": {"value": "1. The paper focuses on a single controllable toy task. While that’s fine for isolating dynamics, I’d expect the core insight (“mix difficulties to break zero-reward stalls”) to be validated on more realistic benchmarks. There are no results on real-world-style tasks, so external validity is unclear.\n2. Running everything on one small, single family/model caps the claim. The zero-reward barrier and the benefit of mixing difficulties likely depend on base competence and instruction-tuning/tokenizer quirks; a larger Qwen or a different family (e.g., Llama/Mistral) might start with non-zero success on the hard split, changing both learning curves and the size of the effect."}, "questions": {"value": "1. Why did the authors only single task?\n2. Curious to know what authors think when the model size is larger and if the claims still hold. \n3. Why did the authors only consider single model in this paper to validate the claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WWVLWL6Z4n", "forum": "Fuhmh86Ckv", "replyto": "Fuhmh86Ckv", "signatures": ["ICLR.cc/2026/Conference/Submission22362/Reviewer_ufeb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22362/Reviewer_ufeb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932051587, "cdate": 1761932051587, "tmdate": 1762942185667, "mdate": 1762942185667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the \"zero reward barrier\" in reinforcement learning with LLMs. This occurs when the model is incapable of generating outputs that yield non-zero rewards, thus has no meaningful gradient for updating the policy. The paper proposes a simple curriculum-style solution: inject easier problems to the training set. This is studied in the context of a graph search problem, and performance is compared to baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The identified problem is an important (if well-known) one. This failure mode of RL is especially important in the context of LLM training and the increasing application of them towards difficult tasks. While I think this paper would benefit from improved and more thorough analysis, it doesn't change the fact that the problem itself is important to the community.\n\n* The discussion on why existing methods fail in the sparse reward setting are helpful (Sec. 4). While some of these failure modes are somewhat obvious, the exploration of the prover for Progress Rewards and degeneracy in Best-of-N provide some nice insights.\n\n* Clean implementations of methods are always greatly appreciated for the sake of reproducibility, and the community would benefit from the ones put forward here."}, "weaknesses": {"value": "* At risk of being overly reductive, this just seems like a curriculum. The idea that a lack of meaningful rewards complicates learning (which is especially common in sparse reward settings) has long been known in reinforcement learning. There are many common approaches to help mitigate this: curriculum, intelligent exploration, transfer learning, reward shaping, prioritized/hindsight experience (if off policy) etc. While some of these are indeed mentioned in the paper itself, it doesn't change the fact that this isn't by any means a novel finding in the context of RL. This is particularly apparent in Sec. 5.3, as this reasoning is exactly the purpose behind developing a training curriculum (as well as other replay-based techniques).\n\n* The above issue isn't necessarily a fatal flaw in the paper if there is a suitably rigorous analysis with interesting findings. And while I do appreciate the analysis of existing technqiues in Sec. 4, ultimately it is applied only to a graph search problem and a single LLM (Qwen 2.5 1.5B). While I agree that this is a useful problem, it does mean that the curriculum that is being implicitly introduced is only studied in the context of one problem and one model.\n\n* I like the discussion on what is the \"right\" difficulty of curriculum, e.g. what samples should be chosen (5.1/5.2). This has always been a challenging problem in curriculum design in RL and I think it is very useful to explore in the context of LLM training. However, there is no satisfying answer to this problem in the paper other than simply adding samples of all difficulties, which makes it difficult to apply more broadly to other tasks."}, "questions": {"value": "1. How many \"easy\" examples are required to bootstrap learning? Can we use just a small amount and sample them with a higher frequency to kickstart learning?\n\n2. Can the need for \"easy\" examples be avoided by providing some teacher-based exploration? E.g. use the answer/output from a stronger model/human when early in training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AfOccq6xZn", "forum": "Fuhmh86Ckv", "replyto": "Fuhmh86Ckv", "signatures": ["ICLR.cc/2026/Conference/Submission22362/Reviewer_YCaD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22362/Reviewer_YCaD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762866750096, "cdate": 1762866750096, "tmdate": 1762942185245, "mdate": 1762942185245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}