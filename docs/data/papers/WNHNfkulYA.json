{"id": "WNHNfkulYA", "number": 15009, "cdate": 1758246792551, "mdate": 1763747081334, "content": {"title": "Is Your LLM Really Mastering the Concept? A Multi-agent Benchmark", "abstract": "Concepts are generalized abstractions that allow humans to categorize and reason efficiently. Whether Large Language Models (LLMs) possess a similar understanding of conceptual relationships, however, is not yet well established.  Existing benchmarks primarily focus on factual recall or narrow tasks (\\textit{e.g.}, multiple-choice question answering or knowledge quizzes), offering limited insight into whether models understand conceptual relationships and subtle distinctions(\\textit{e.g.}, poetry \\textit{vs.} prose). Many also rely on static datasets that risk overfitting. To address this gap, we introduce CK-Arena, a multi-agent interaction benchmark inspired by the Undercover game, designed to evaluate the mastery of conceptual feature knowledge by LLMs. In CK-Arena, models must describe, differentiate, and infer distinguishing features of concepts from partial information, testing their ability to reason about both commonalities and differences across concept boundaries. The benchmark offers scalable datasets, rigorous evaluation protocols, and flexible extension methods, enabling comprehensive assessment of LLMs’ conceptual understanding across multiple dimensions. Experimental results show that LLMs' understanding of conceptual knowledge varies significantly across different categories and is not strictly aligned with general model capabilities. The code is made publicly available at:https://anonymous.4open.science/r/CK-Arena/readme.md.", "tldr": "A benchmark framework based on the UNDERCOVER game that evaluates LLMs' ability to recognize knowledge boundaries and perform strategic reasoning.", "keywords": ["LLM evaluation", "Conceptual knowledge boundaries", "Multi-agent game"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5801b314f73b05920220a0822ab4bb9f575a752.pdf", "supplementary_material": "/attachment/9fb751c13336b0159f2edd3c0d6c73e7bf6533ee.zip"}, "replies": [{"content": {"summary": {"value": "The authors play the Undercover game with the agents. The Undercover game is a social‑deduction word game that tests whether players (in this case, language models) can identify shared and distinguishing features between closely related concepts. At the start of each game, most players are assigned the same word (for example, soccer), making them the civilians, while one or two players receive a slightly different but related word (for example, basketball), making them the undercover agents. Each player then takes turns giving a short, indirect description of their word without explicitly naming it, such as “it’s played on a field” or “it involves a ball.” After hearing everyone’s clues, all players vote on who they think the undercover might be; the player with the most votes is eliminated. The game continues until all undercovers are voted out (civilian victory) or until the number of undercovers equals the number of civilians (undercover victory). In CK‑Arena, this setup is simulated among multiple LLM agents, who generate these clues and votes automatically. The game environment thus becomes a structured way to test whether models truly understand conceptual relationships—the similarities and subtle differences that separate one idea from another—through interactive language use rather than simple factual recall."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Interactive, multi-agent setup directly targets concept differentiation under partial information—beyond static QA.\n2) Clear metric design spanning outcomes and utterance quality, with elimination thresholds to keep games coherent.\n3) Fine-tuned Qwen-3-8B-ckR reproduces human labels, substantially reducing evaluation cost."}, "weaknesses": {"value": "1) LLMs (and a fine-tuned LLM) define reasonableness; if families overlap with players, biases or self-agreement can inflate scores.\n2) Threshold-based elimination can confound game skill with format adherence and may unevenly penalize models with different style priors.\n3) Success in Undercover blends strategy with concept talk; it’s unclear how strongly this maps to “concept mastery” in non-game tasks.\n4) Not enough analyses on why the models behave like this - not much analytical insights\n5) Not much insights in how we could improve the models and agents."}, "questions": {"value": "Can “Novelty” be upgraded from embedding similarity to feature-set discovery (e.g., information gain over a concept-feature graph) to better capture conceptual additions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fY5A1jpBop", "forum": "WNHNfkulYA", "replyto": "WNHNfkulYA", "signatures": ["ICLR.cc/2026/Conference/Submission15009/Reviewer_tz2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15009/Reviewer_tz2j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776614892, "cdate": 1761776614892, "tmdate": 1762925340806, "mdate": 1762925340806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CK-Arena, a multi-agent benchmark for assessing conceptual understanding in LLMs. Unlike traditional static benchmarks (e.g., MMLU, BIG-Bench), CK-Arena leverages the interactive game \"Undercover\", where models must describe and differentiate related concepts (e.g., football vs. basketball) while playing either as civilians (with the true concept) or as undercovers (with a similar one). The benchmark measures win/survival rate, novelty, and reasonableness of generated statements, combining LLM-based and human judgments to evaluate conceptual reasoning. Experiments with major LLMs (GPT-4o, Claude-3.5, Gemini-2.0, Qwen-2.5, DeepSeek-V3, etc.) reveal that conceptual understanding varies across models and is not strictly aligned with general benchmark performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces a dynamic, interactive, and multi-agent setup rather than static QA datasets.\n\n* Understanding conceptual boundaries rather than factual recall is an interesting evaluation direction. The choice of Undercover is well justified: it naturally requires distinguishing overlapping concepts."}, "weaknesses": {"value": "* Although mitigated by human calibration, relying on LLMs for evaluation introduces circularity in which LLMs judging other LLMs on conceptual quality.\n\n* Experiments are restricted to English concrete nouns, excluding abstract or cross-linguistic conceptual domains where conceptual reasoning might differ.\n\n* Measures like novelty and reasonableness depend on embedding similarity or fine-tuned classifiers (Qwen-3-8B-ckR), which may not robustly capture conceptual depth."}, "questions": {"value": "* How do you ensure that higher win/survival rates genuinely reflect conceptual reasoning rather than memorized associations or meta-strategies?\n\n* Since Qwen-3-8B-ckR was fine-tuned on the same dataset, how do you prevent evaluation overfitting or family-specific bias?\n\n* It would be insightful to include human players or human-LLM mixed games to contextualize LLM performance in conceptual reasoning.\n\n* I couldn't find the link to your data (529 nouns used), do you mind sharing it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4wZfQkN0qq", "forum": "WNHNfkulYA", "replyto": "WNHNfkulYA", "signatures": ["ICLR.cc/2026/Conference/Submission15009/Reviewer_QnvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15009/Reviewer_QnvB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867565566, "cdate": 1761867565566, "tmdate": 1762925340012, "mdate": 1762925340012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to measure the conceptual understanding of LLMs and create a leaderboard for it. Their method is to have various LLMs play the Undercover game as agents. The paper's core contribution is reusing the statements made during gameplay to evaluate this understanding using a mix of methods. They analyze t-SNE embeddings of statements, where they argue that deeper knowledge produces more dispersed patterns, while shallow understanding results in tightly clustered, repetitive phrases. They also score each statement for \"Reasonableness,\" and \"Novelty\". The paper clarifies that \"Novelty\" is nuanced: effective play requires balancing originality with precision, as high-novelty statements can risk elimination just as low-novelty ones can. Finally, the authors propose a leaderboard, but this ranking is based on game-specific outcomes like Win Rate, Survival Rate, and voting accuracy , which is distinct from the other statement-level concept metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Repurposes existing multi-agent game (Undercover) to evaluate conceptual understanding rather than just strategic gameplay\n- Provides structured evaluation framework with systematic metrics (Novelty, Reasonableness, Relevance)\n- Uses t-SNE semantic dispersion as proxy for conceptual depth (decent visualization approach, not novel)\n\n- Comprehensive evaluation across 14 LLMs with standardized Elo rating system\n- Some interesting cross-category performance variations (e.g., Claude's verb/noun flip). However: These findings are experimental observations, not driven by novel theoretical/methodological insights"}, "weaknesses": {"value": "- Incremental contribution over existing work: The paper repurposes the Undercover game framework (from previous work). While the stated goal is evaluating \"conceptual understanding\" rather than \"strategy,\" any language game involving concept description inherently tests conceptual knowledge. The paper does not provide sufficient justification for why this reframing constitutes a distinct contribution beyond running additional experiments with different evaluation metrics.\n\n- Tension between game mechanics and evaluation goal: The interesting result is that the best performing models score lower on novelty. This  reveals a core problem: models that understand game strategy suppress conceptual demonstrations to avoid elimination. This creates a methodology where optimal gameplay directly conflicts with demonstrating conceptual depth. The paper acknowledges this (lines 377-385) but does not address why this makes for a valid conceptual understanding benchmark. A good evaluation should not have strategic incentives that suppress the very capability being measured.\n\n- Leaderboard Relevance: The paper's stated objective is measuring conceptual understanding through statement-level metrics. However, the main leaderboard (Figure 5) ranks models using game outcomes (Win Rate, Survival Rate, voting accuracy). These measure strategic gameplay competence, not conceptual mastery. The paper provides no evidence that winning the game correlates with better conceptual understanding especially given that the novelty findings suggest the opposite.\n\n- Weak scalability claims: The paper claims scalability as a contribution but provides no comparative analysis:\n    - Using LLM judges is standard practice (not novel)\n    - No comparison to update costs of other benchmarks\n    - Creates judge-capacity bottleneck: GPT-4.1 as judge cannot fairly evaluate GPT-5 or Claude Opus-4.1\n    - The automation pipeline is inconsistent. \"Reasonableness\" uses a fine-tuned model validated at 99% accuracy (Table 2), while   \"Novelty\" relies on a vague \"cosine similarity\" (lines 261-267) with no validation, correlation analysis, or justification provided.\n\n- Shallow analysis of metric relationships: The paper analyzes metrics (novelty, reasonableness, semantic dispersion, relevance) in isolation without exploring their interdependencies:\n    - No analysis of novelty-reasonableness tradeoffs within individual statements\n    - No investigation of whether high-novelty statements are typically less reasonable\n\n- English-only evaluation limits generalizability: The paper acknowledges this (lines 480-485) but does not discuss how conceptual structures vary across languages or cultures, which is critical for a benchmark claiming to measure \"conceptual understanding\" as a general capability."}, "questions": {"value": "These are mostly directly connected to the weaknesses and I have provided a reference to the weakness as possible.\n\n- (Incremental Contribution): How do the results differ from prior Undercover benchmarks (e.g., Xu & Zhong, 2025)? Is this measuring a genuinely different capability, or just strategic competence with new labels?\n\n- (Tension between Game and Goal): Given that optimal performance requires suppressing novel conceptual descriptions, how do the authors justify win rate as a valid measure of conceptual understanding?\n\n- (Irrelevant Leaderboard): Why not create a leaderboard based on the statement-level conceptual metrics? Have the authors analyzed whether game performance (Win Rate) actually correlates with these conceptual measures (Novelty, Reasonableness)?\n\n- (Scalability & Judge Bottleneck): How do the authors address evaluating frontier models (GPT-5, Claude Opus-4.1) when the judge (GPT-4.1) is potentially weaker? What percentage of judgments required manual correction for these top models?\n\n- (Inconsistent Metric Reliability): What is the correlation between cosine similarity and human-rated novelty? Why did \"Reasonableness\" require a fine-tuned judge (99% accuracy) while \"Novelty\" uses basic vector similarity?\n\n- (Shallow Metric Analysis): Is there a systematic tradeoff between novelty and reasonableness? How does DeepSeek-V3's high \"relevance\" (Figure 4) align with the finding that suppressing detail (low novelty) is optimal?\n\n- (Unexplained Results): What explains Claude's dramatic performance flip between nouns and verbs? Have the authors investigated whether this reflects training data, prompt sensitivity, or architectural differences?\n\n- (Parameter Sensitivity): How sensitive are the final leaderboard rankings to the 120-point Elo offset? Was this value validated beyond the initial 6 baseline models?\n\n- (Qualitative-Only Analysis): Was the t-SNE analysis validated for stability (e.g., random seeds, UMAP) or quantified (e.g., average pairwise distance) rather than relying on visual interpretation?\n\n- (Scalability Claims): Can the authors provide a quantitative comparison of update costs vs. other benchmarks (e.g., MMLU) and evidence that LLMs are particularly good at judging \"Novelty\" and \"Reasonableness\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wa1PuH1BqW", "forum": "WNHNfkulYA", "replyto": "WNHNfkulYA", "signatures": ["ICLR.cc/2026/Conference/Submission15009/Reviewer_M8Xg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15009/Reviewer_M8Xg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895755185, "cdate": 1761895755185, "tmdate": 1762925339173, "mdate": 1762925339173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmarking method for concept knowledge and understanding in LLMs. Specifically, this paper introduces CK-Arena, a multi-agent interaction benchmark to evaluate the mastery of conceptual feature knowledge by LLMs. The core module of CK-Arena is the Undercover game, where LLMs must describe, differentiate, and infer distinguishing features\nof concepts from partial information. This evaluation method alleviates issues such as knowledge leakage in static benchmarks. This paper conducts extensive experiments. Experimental results show that LLMs’ understanding of conceptual knowledge varies significantly across different categories and is not strictly aligned with general model capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explores an important and interesting topic: evaluating LLMs’ conceptual knowledge. Assessing whether models truly understand conceptual knowledge is fundamental to understanding world knowledge, which provides valuable insights for the broader research community.\n2. The use of the *Undercover Game* paradigm is interesting. By introducing dynamic evaluation through model interaction, the approach effectively mitigates issues such as benchmark leakage of static evaluations.\n3. The experiments also present some interesting findings. For instance, the understanding and mastery of conceptual knowledge by LLMs are not necessarily correlated with their general capabilities. This observation encourages us to reconsider the boundaries of model capabilities and knowledge, which could inform future model development."}, "weaknesses": {"value": "1. The core evaluation module, i.e., the Undercover Game, is adapted from existing work. While applying this framework to a new domain or evaluation is indeed a meaningful contribution, it may somewhat limit the paper’s technical novelty.\n2. The paper evaluates a total of 529 English concept pairs, including 220 concrete noun pairs, 100 abstract noun pairs, 109 adverb pairs, and 100 verb pairs. As an evaluation and benchmark work, it would be helpful to provide some validation regarding whether these 529 concepts offer comprehensive and reliable coverage or the reasons why choosing these 529 concept pairs. For example, would the model ranking change if a different set of concepts are used? \n3. It would be valuable to include some human evaluation results, for instance, the win rates of humans acting as *Civilian* or *Undercover*, to better illustrate the performance gap between humans and LLMs. Moreover, since the evaluation method currently requires a human expert to make the final judgment, it may raise concerns about automation. Moreover, providing some illustrative examples or failure analyses would be better for understanding existing LLMs’ performance."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tDSzzTy5Sv", "forum": "WNHNfkulYA", "replyto": "WNHNfkulYA", "signatures": ["ICLR.cc/2026/Conference/Submission15009/Reviewer_9VBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15009/Reviewer_9VBb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920519640, "cdate": 1761920519640, "tmdate": 1762925338335, "mdate": 1762925338335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CK-Arena, a new multi-agent benchmark designed to evaluate the conceptualization capability of LLMs under the paradigm of the \"Undercover\" game. LLMs are required to identify the common concept from the given text and produce novel but related concepts at each round. Experiments reveal that language models have the ability to extract the abstraction while distinguishing the slight differences between the concepts, and this capability can be reflected by the diversity of the concepts produced at each round."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides a dynamic benchmark that can evaluate the conceptualization capability of LLMs in a arena-like setting under the \"Undercover\" game, offering a new perspective to rank the conceptualization capability of LLMs.\n\nThe metrics and checking process at each round are relatively fair and comprehensive, making the results convincing.\n\nThe analysis of the results is thorough, covering both the raw performance and the Elo rating, as well as the qualitative analysis of the distribution of the concepts using t-SNE visualization."}, "weaknesses": {"value": "Lack of fine-grained case study: since the benchmark is based on the \"Undercover\" game, the strategies of different LLMs are not explicitly discussed, which can be reflected by some case studies.\n\nThe evaluation seems to be a bit costly, since it requires multiple rounds with multiple LLM agents. Though the authors have provided some methods to mitigate the cost, it is still a bit time-consuming."}, "questions": {"value": "Is there any difference between reasoning models and non-reasoning models in their conceptualization performance, since the former typically have a longer CoT process.\n\nCan this process be applied to some downstream tasks like extracting a hierarchical conceptualization knowledge graph from some given text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gd2ICgeHpu", "forum": "WNHNfkulYA", "replyto": "WNHNfkulYA", "signatures": ["ICLR.cc/2026/Conference/Submission15009/Reviewer_R578"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15009/Reviewer_R578"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966956043, "cdate": 1761966956043, "tmdate": 1762925337367, "mdate": 1762925337367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Note"}, "comment": {"value": "Hello ACs and Reviewers! Thank you for your time to write valuable comments. We have read those and revise our PDF. We added 3 paragraphs in the `section 'More Experimental Results'` in `APPENDIX`, including:\n\n- **Leaderboard with Reasoner Models and Human Baseline.** In this section, we analyzed the performance of reasoner model and human baseline, and put forward some conclusions.\n- **Reuse Data to Construct Knowledge Graphs.** In this section, we learned the function of applying the game log data of CK arena to the generation of knowledge map.\n- **Specific Case Analysis.** Here we provide four different cases that lead to the elimination of LLMS, specifically showing why LLMS are eliminated when they cannot be accurately described or concealed, and the prohibition of plagiarism and deception.\n\n\nIn addition to these parts, we also added other experiments that are not suitable for display but are also valuable, such as a complete knowledge map of the original format, and Pearson coefficient analysis of different metrics. These contents are either quoted in the specific reply or put in the code warehouse for human use.\n\n\nWe marked the added content in **blue font**, and will revert to black when publishing later.\n\nThe results of the experiment have been added before and after these paragraphs through simple visualization, presented in the form of images; The specific results will be added to the project repository. We will organize the code and update the repository in the next few days."}}, "id": "rotr2L92Ox", "forum": "WNHNfkulYA", "replyto": "WNHNfkulYA", "signatures": ["ICLR.cc/2026/Conference/Submission15009/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15009/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission15009/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763738676354, "cdate": 1763738676354, "tmdate": 1763746931191, "mdate": 1763746931191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}