{"id": "ykSmkVqzn4", "number": 14712, "cdate": 1758242314183, "mdate": 1759897353454, "content": {"title": "Flipping the Dialogue: Training and Evaluating User Language Models", "abstract": "Conversations with LMs involve two participants: a human user leading the conversation, and an LM assistant responding to the user's request. To satisfy this specific role, LMs are post-trained to be helpful assistants - optimized to produce exhaustive and well-structured responses, often free of ambiguity. User utterances, on the other hand, are rarely perfected, with each user phrasing requests in unique and indirect ways, making the least effort at each turn and refining on the fly. To better understand LM performance in a realistic setting, prior work has proposed to simulate users in multi-turn conversations, often prompting an assistant model to play the role of the user. However, we show in this work that assistant LMs make for poor user simulators, with surprising evidence that better assistants yield worse simulators. Instead, we introduce purpose-built User Language Models (User LMs) - models post-trained to simulate human users in multi-turn conversations. Through various evaluations, we show how User LMs align better with human behavior and achieve better simulation robustness than existing simulation methods. When leveraging User LMs to simulate coding and math conversations, the performance of a strong assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic simulation environments lead to assistant struggles as they fail to cope with the nuances of users in multi-turn setups.", "tldr": "We introduce and evaluate user language models - models that are post-trained to simulate users that interact with assistants.", "keywords": ["User Language Models", "User Simulation", "Interactive Evaluation", "Post-Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fd593fd301cf725e148623cb838da6d27c443e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes UserLM to evaluate LM performance in realistic multi-turn conversation settings. The authors train UserLMs on flipped dialogues between users and LMs to simulate users, given user intents. The experiments show that UserLMs' simulations better match actual human responses and remain within the user role. As a result, the performance of assistant LMs is lower against UserLMs than against prompt-based baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important problem of improving and making more realistic user simulations for chat-based LM evaluation.\n\n2. The proposed approach, termed UserLM, provides better user simulation that matches human responses in various aspects, whereas previous prompt-based approaches fail."}, "weaknesses": {"value": "1. The paper does not provide evidence that better user simulation leads to better assistant LM evaluation. It seems the assistant LM performs worse against UserLM. However, it is not clear whether this is because real user requests are more challenging or because UserLM has some unexpected traits that confuse the assistant. Moreover, this does not mean that assistant LM evaluation based on UserLM offers better representation of the assistant's quality.\n\n2. The WildChat dataset seems to contain many almost-duplicates that are not necessarily from the same IP addresses. This may be due to popular prompts shared by many users or to a single user using multiple IP addresses. Without careful data splitting to account for this, there may be data leakage into the test sets."}, "questions": {"value": "1. What are the findings that are not available in other user simulation approaches but that UserLM enables?\n\n2. Does an assistant LM performing worse against UserLM suffice to conclude that UserLM-based simulation offers better evaluation? How can we rule out that UserLM is just confusing the assistant LM?\n\n3. Were there any additional checks of data leakage into the test sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p9CZOf1HS6", "forum": "ykSmkVqzn4", "replyto": "ykSmkVqzn4", "signatures": ["ICLR.cc/2026/Conference/Submission14712/Reviewer_YAMu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14712/Reviewer_YAMu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773067069, "cdate": 1761773067069, "tmdate": 1762925076267, "mdate": 1762925076267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that assistant LMs, which are post-trained to produce exhaustive, unambiguous replies, do not faithfully mimic messy, indirect human user behavior in multi-turn dialogues. It shows that using assistant LMs as user simulators is flawed; stronger assistants make worse simulators. This paper propose “User LMs,” post-trained specifically to simulate human users, and report that these models align better with real user behavior and yield more robust simulations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This casts doubt on current practice and suggests the need for a user simulator better aligned with real users.\n- Extensive experiments show that the proposed post-trained user simulator better captures the properties targeted by the designed metrics."}, "weaknesses": {"value": "Evaluation Metrics\n* While mimicking user behavior can be useful for building a user simulator, it is not clear that the properties highlighted as important for user behavior are actually critical for evaluating an LM assistant.\n\n * The results in Table 1 may offer limited insight: a model trained on conversational data should have lower perplexity on test sets from the same distribution (WildChat) or similar data (PRISM).\n\n * An intent (e.g., ‘You are a user chatting with an assistant language model … medications on weight gain.’) provides strong topical context. Given such a prompt, the conversation will predictably be steered toward that topic, which in turn reduces perplexity.\n\n* Some evaluation metrics need stronger justification, as they do not appear to faithfully capture what is required for realistic user simulation.\n    * There are many potential, confounding reasons for lower assistant performance. Is it reasonable, therefore, to conclude that lower performance indicates the user simulator is better aligned with real users?\n    * Why is the simulator’s ability to end the conversation considered so important for evaluating multi-turn QA with LLMs?\n    * Since the user’s response depends on the assistant’s response, is it appropriate to compute turn variance without accounting for this dependency?\n    * The claim in Intent Decomposition section that “a lower overlap is particularly desirable because it indicates that the model expresses its intent using varied language while introducing details progressively” is not substantiated.\n\n    * The experimental setup feels somewhat artificial. The methods for evaluating User-role adherence and Intent adherence seem overly narrow, specific, and somewhat unlikely to occur in practice.\n\nReal-User Alignment\n* What exactly constitutes a “real user” in this work? Was any human study conducted to validate that definition?\n* As users become more accustomed to LLM chatbots and adapt their behavior to use them effectively, is a new human study needed to compare the proposed simulators with up-to-date real user behavior?\n* What is the practical value of building the user simulator? Do you have empirical evidence showing that it improves the evaluation of LLM performance or leads to better model outcomes?"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cOfIFBlDtC", "forum": "ykSmkVqzn4", "replyto": "ykSmkVqzn4", "signatures": ["ICLR.cc/2026/Conference/Submission14712/Reviewer_ep5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14712/Reviewer_ep5V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897835758, "cdate": 1761897835758, "tmdate": 1762925075794, "mdate": 1762925075794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents 1B and 8B models fine-tuned on the WildChat dataset, where the user utterances were unmasked and the user intent was put as a system message. These models can be used for simulating user utterances in multi-turn conversations.\n\nOther details:\n- The intent was inferred with GPT-4o in a few-shot mode.\n- The base models were  Llama3-8b-Base and Llama3.2-1b-Base. Starting from the instruction models is worse.\n- It was full fine-tuning on 4xA6000.\n\nThe main evaluation was by perplexity on the test set (WildChat) and on the OOD test set (PRISM)\n\nOther metrics: first-turn diversity, intent decomposition across turns, dialogue termination, naturalness (Pangram AI-detector), and robustness (role adherence and intent adherence).\n\nUser LMs outperform prompted assistant simulators (including GPT‑4o roleplay) and a fine-tuned baseline (USP‑8b), with large gaps in metrics.\n\nExtrinsic evaluations simulate math and coding tasks where UserLM‑8b yields more realistic, varied user behavior and reduces downstream assistant (GPT‑4o) task success from 75% to 57%, suggesting current assistants struggle more under realistic multi-turn conditions.\n\nThe authors claim the model will be publicly available for research purposes."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The topic is very timely: LLM-based user simulators are needed for offline evaluation of chatbot applications everywhere.\n\nThe implementation is very straightforward, which is a good thing in that case, and it is easily reproducible. Other works do almost the same (USP), but this work seems to be both better and simpler in terms of the general design.\n\nThe paper presents strong empirical evidence that assistant LMs are poor user simulators; User LMs substantially reduce PPL on PRISM and WildChat and improve multi-turn metrics. Some metric choices are nice, for instance, using the Pangram for naturalness. Conclusions are helpful: base models are better than instruct models; 8B is better than 1B, so scaling works in that case.\n\nThe presentation is wonderful, all tables and figures are clean, readable, and well-thought-out."}, "weaknesses": {"value": "1. Extrinsic simulations introduce guardrails applied only to UserLM‑8b (described in Appendix D.1), which gives unfair advantage relative to GPT-based simulators; a fairness ablation is missing. \n\n2. The task scope for extrinsic simulations is very narrow (math/coding) and English-only; generalization to other domains, modalities, and languages is not evaluated. In general, math and code seem like a strange choice."}, "questions": {"value": "1. I am surprised to see USP-8b performing that badly, being worse than Llama3.2-1b-Instruct by PPL. It also seems that USP has more sophisticated training procedures. Why do you think it is that bad?\n\n2. Do guardrails described in Appendix D.1 affect Table 1 results? Or are these guardrails only for the extrinsic simulations?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The authors make a user simulator that works against the AI detector. It might be harmful."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m3zIwJ4mRy", "forum": "ykSmkVqzn4", "replyto": "ykSmkVqzn4", "signatures": ["ICLR.cc/2026/Conference/Submission14712/Reviewer_vfFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14712/Reviewer_vfFV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921323058, "cdate": 1761921323058, "tmdate": 1762925074875, "mdate": 1762925074875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores an alternative perspective in large language model (LLM) training by focusing on the user simulator rather than the dialogue model itself. To address the challenge of maintaining user intent across multi-turn conversations, the authors propose an intent extraction and training method. Experimental results on multiple datasets (such as WildChat and PRISM) show consistent improvements over baselines and better align with human behavior by measuring multiple metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors focus on a critical yet underexplored task (user model construction) and propose effective methods that achieve promising results.\n2. The authors provide comprehensive and fine-grained analyses, offering valuable insights for future user simulator research.\n3. The authors introduce diverse evaluation metrics tailored to user simulators, which can serve as useful references for subsequent evaluation frameworks."}, "weaknesses": {"value": "1. A major concern is that the “FLIPPING the Dialogue” paradigm has already been partially explored in prior works (e.g., USP), which weakens the originality claim. The paper should clearly articulate how it differs from existing approaches.\n2. The correctness of extracted intents used for training has not been validated, potentially introducing bias in both model performance and evaluation (e.g., UserLM is trained and tested on WildChat).\n3. The baseline comparison is limited since only USP is included, while prior role-playing and persona-based approaches are not discussed. As a result, it is unclear whether the observed gains stem from the proposed methodology itself or simply from incorporating user profiles (in whatever form). Moreover, several metrics (e.g., Table 3) are model-specific, lacking objective comparability.\n4. The evaluation relies heavily on LLM-based automatic assessments. Some degree of human evaluation should be included to validate the reliability of the results."}, "questions": {"value": "1.\tHow exactly is the <|endconversation|> mechanism implemented across models? As mentioned in the paper, is it merely introduced through prompts? For UserLM, was this mechanism explicitly annotated during training? The notable improvements in Table 2 suggest further clarification.\n2.\tThe paper claims that abstract rather than specific intent extraction is designed to avoid simple memorization by LLMs, yet this abstraction could harm intent fidelity. How is the correctness of extracted intents measured or validated?\n3.\tAre the datasets used for evaluation in Section 3 the same as those introduced in Section 2?\n4.\tFrom a writing perspective, the introduction mentions that only two prior works exist in this area (two citations). However, the related work section shows that many studies are indeed relevant. These should be cited earlier, along with a clear positioning of this paper’s novelty. Furthermore, while the analysis is extensive, it lacks a clear structure and logical flow; reorganizing the sections would greatly improve readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qLzEm1ZcDF", "forum": "ykSmkVqzn4", "replyto": "ykSmkVqzn4", "signatures": ["ICLR.cc/2026/Conference/Submission14712/Reviewer_xhbu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14712/Reviewer_xhbu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928780977, "cdate": 1761928780977, "tmdate": 1762925074147, "mdate": 1762925074147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}