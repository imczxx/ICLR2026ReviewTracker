{"id": "mgDoHyj4YL", "number": 4123, "cdate": 1757606152319, "mdate": 1759898051705, "content": {"title": "POINTESS: EFFICIENT 3D POINT CLOUD MODELING THROUGH ENHANCED STATE SPACE NETWORKS AND FINE-TUNING STRATEGIES", "abstract": "Recent progress in point cloud modeling highlights the need for efficient pretraining strategies and task adaptability. We introduce PointESS, the first framework to leverage the Mamba2 architecture for point cloud representation learning. PointESS balances structural modeling with downstream flexibility and incorporates two key components to enhance local structural awareness and fine-tuning performance: Gate Adapter, which dynamically integrates pretrained and task-specific features for improved transferability, and KH-Norm (KNN-Hybrid Normalization), which embeds geometric priors into normalization and pooling to capture fine-grained spatial details.\nExtensive experiments on ShapeNet55, ModelNet40, and ScanObjectNN demonstrate that PointESS achieves competitive or state-of-the-art results with substantially fewer parameters, highlighting its effectiveness in both generalization and efficiency.", "tldr": "We propose PointESS, the first point cloud representation learning framework based on Mamba2, achieving strong performance with fewer parameters via a novel Gate Adapter and KH-Norm for better transferability and spatial modeling.", "keywords": ["Point Cloud", "Mamba2", "KH-Norm", "Gate Adapter", "Recent progress in point cloud modeling highlights the need for efficient pretraining strategies and task adaptability. We introduce \\emph{PointESS}", "the first framework to leverage the \\emph{Mamba2} architecture for point cloud representation learning. PointESS balances structural modeling with downstream flexibility and incorporates two key components to enhance local structural awareness and fine-tuning performance: Gate Adapter", "which dynamically integrates pretrained and task-specific features for improved transferability", "and KH-Norm (KNN-Hybrid Normalization)", "which embeds geometric priors into normalization and pooling to capture fine-grained spatial details. Extensive experiments on ShapeNet55", "ModelNet40", "and ScanObjectNN demonstrate that PointESS achieves competitive or state-of-the-art results with substantially fewer parameters", "highlighting its effectiveness in both generalization and efficiency.3D Object Classification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce1a3e3103737c7dc0b5d8ed2353e4211c1dfb7b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the application of Mamba2 to point cloud analysis tasks, and designs Gate Adapter and KH - Norm to improve its fine - tuning performance in downstream tasks. The formulas in the paper are clear, and the experimental results on multiple datasets demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper is the first to propose the application of Mamba2 to point cloud analysis tasks, and the experimental results demonstrate its effectiveness.\n\n2.This paper features clear and intuitive figures and tables, neat formulas, and high readability."}, "weaknesses": {"value": "1.This paper lacks sufficient innovation: the application of Mamba series models to point cloud analysis networks has already been thoroughly studied. Furthermore, the use of frequency-domain-aware Adapters during the fine-tuning phase was previously proposed in PointGST, and this paper fails to conduct more in-depth exploration based on this existing work.\n\n2.The experiments are insufficient. For example:\n\na. What would be the effect if KH-Norm combines two pooling operations? Could one of the pooling operations be ineffective?\n\nb. It seems that the designs of Gate Adapter and KH-Norm are irrelevant to Mamba. If so, could they be embedded into other pre-trained networks for testing? This would be beneficial for proving the generality of the proposed methods.\n\nc. It is suggested to add more tasks, such as segmentation and detection tasks, to more fully demonstrate the effectiveness of the methods.\n\nd.There is a lack of ablation experiments on the use of each component."}, "questions": {"value": "1. It is suggested to add more references to explore the differences between the present method and previous methods.\n\n2.It is suggested to add more experiments and reformat the tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tdv0ADI8fU", "forum": "mgDoHyj4YL", "replyto": "mgDoHyj4YL", "signatures": ["ICLR.cc/2026/Conference/Submission4123/Reviewer_iFM7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4123/Reviewer_iFM7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760929444274, "cdate": 1760929444274, "tmdate": 1762917187454, "mdate": 1762917187454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PointESS, a 3D point cloud representation framework built on Mamba2 (selective state-space model) to achieve linear-time sequence modeling over ordered point-patch sequences.\nKey components: \n- Gate Adapter: a lightweight gating module for full fine-tuning that blends pretrained and task-adapted features via a learnable sigmoid gate, aiming to stabilize transfer and improve accuracy without PEFT constraints.\n- KH-Norm (KNN-Hybrid Normalization): a geometry-aware normalization/pooling block. Step 1 performs diffusion-style normalization using standardized feature differences between a patch center and its KNN neighbors. Step 2 aggregates with three complementary pools—FeatPool (feature-magnitude attention), DistPool (distance-weighted), and DirPool (direction-aware)—then fuses them.\nIntroducing Mamba2 to point clouds plus geometry-aware normalization and simple task-adaptive gating yields a compact, efficient, and accurate model for 3D classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. First systematic use of Mamba2 for point-cloud representation, with a full pre-train/fine-tune pipeline designed for linear-time sequence modeling.\n\n2. KH-Norm explicitly encodes local geometry.Multi-geometry aggregation combines FeatPool (feature magnitude), DistPool (distance decay), and DirPool (direction attention), capturing relevance, proximity, and orientation cues.\n\n3. SOTA/competitive accuracy on ModelNet40 and all ScanObjectNN subsets with far fewer parameters and FLOPs than large Transformer-based baselines.\n\n4. Clear gains over Mamba-based baselines (PointMamba, Mamba3D) at similar model sizes."}, "weaknesses": {"value": "1. The experiments primarily focus on synthetic datasets like ShapeNet55 and ModelNet40.  The performance on datasets like ScanObjectNN is promising but could be expanded with more diverse real-world scenarios to validate robustness further.\n\n2. The paper provides a comparison with existing models but lacks a deeper analysis of why certain features or strategies within PointESS outperform others. For instance, a more detailed examination of the contributions of the Gate Adapter and KH-Norm compared to traditional methods would strengthen the argument for their effectiveness.\n\n3. The introduction fails to clearly articulate the specific problems that the Gate Adapter and KH-Norm aim to address, while the paper highlights the linear complexity advantages of the SSM. This lack of clarity in motivation may leave readers unsure about the significance of the proposed methods.\n\n4. The absence of detailed evaluations regarding the time and memory consumption of the Gate Adapter and KH-Norm modules raises concerns. It is difficult to assess the practical feasibility of implementing these modules in real-world applications."}, "questions": {"value": "1. The Gate Adapter method retains pretrained parameters while performing comprehensive fine-tuning. Can this be interpreted as doubling the number of model parameters? Will this increase time and memory usage?\n\n2. I'm curious whether KH-Norm can be used as a universally applicable module to replace LayerNorms in other backbones, such as Point-MAE."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OHxn2ePbAO", "forum": "mgDoHyj4YL", "replyto": "mgDoHyj4YL", "signatures": ["ICLR.cc/2026/Conference/Submission4123/Reviewer_qFf7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4123/Reviewer_qFf7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834071218, "cdate": 1761834071218, "tmdate": 1762917187221, "mdate": 1762917187221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new architecture for 3D point cloud representation learning, based on the Mamba2 State Space Model (SSM). It aims to combine linear-time sequence modeling efficiency with geometric awareness and adaptability for downstream tasks such as classification and segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- efficiency in the model\n- good empirical results\n- The ablation study is relatively complete"}, "weaknesses": {"value": "- Incremental originality in concept:\n- lack of comparison with enough methods (also the reference is too limited); lack of datasets - the ones chosen are too easy/limited.\n - Limited downstream diversity\n - Ablation coverage of computational aspects:\nLacks detailed wall-clock runtime or energy efficiency comparisons—important for a paper emphasizing “efficiency.”\nMinor points:\n- Spectral aspect underexplored:\nThe “spectrally guided” claim of the Gate Adapter isn’t backed by quantitative spectral-domain analysis or visualizations.\n- Fig1 is very hard to understand\n- Clarity in model scaling:\nThe relation between Mamba2 block depth, KH-Norm frequency, and performance scaling is not analyzed."}, "questions": {"value": "- The “greedy sorting” of point patches is said to start from a random patch and iteratively pick the nearest one.\nIt’s unclear how the starting patch is chosen and whether the process is deterministic across epochs.\nIf it starts randomly, does the sequence order vary during training, or is it fixed once per shape?\n\n- how positional encodings are aligned with the patch sequence length after masking.\n\n- Provide Decoder details\n\n- F˜K = FK − FC Var(FK − FC ) + ϵ\nvariance is computed channel-wise, per-neighborhood, or across all patches? justify the choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KjgGpRV3fr", "forum": "mgDoHyj4YL", "replyto": "mgDoHyj4YL", "signatures": ["ICLR.cc/2026/Conference/Submission4123/Reviewer_WXyW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4123/Reviewer_WXyW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069697918, "cdate": 1762069697918, "tmdate": 1762917186946, "mdate": 1762917186946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}