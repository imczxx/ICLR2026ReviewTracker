{"id": "UHm5HIo8CO", "number": 4011, "cdate": 1757583777323, "mdate": 1759898058243, "content": {"title": "iTryOn: Mastering Interactive Video Virtual Try-On with Spatial-Semantic Guidance", "abstract": "Video Virtual Try-On (VVT) aims to seamlessly replace a garment on a person in a video with a new one. While existing methods have made significant strides in maintaining temporal consistency, they are predominantly confined to non-interactive scenarios where models merely showcase garments. This limitation overlooks a crucial aspect of real-world apparel presentation: active human-garment interaction. To bridge this gap, we introduce and formalize a new challenging task: Interactive Video Virtual Try-On (Interactive VVT), where subjects in the video actively engage with their clothing (e.g., pulling a hem or unzipping a jacket). This task introduces unique challenges beyond simple texture preservation, including: (1) resolving the semantic ambiguity of interactions from standard pose information, and (2) learning complex garment deformations from video where interactive moments are sparse and brief.\nTo address these challenges, we propose \\textbf{iTryOn}, a novel framework built upon a large-scale video diffusion Transformer. iTryOn pioneers a multi-level interaction injection mechanism to guide the generation of complex dynamics. At the spatial level, we introduce a garment-agnostic 3D hand prior to provide fine-grained guidance for precise hand-garment contact, effectively resolving spatial ambiguity. At the semantic level, iTryOn leverages global captions for overall context and time-stamped action captions for localized interactions, synchronized via our novel Action-aware Rotational Position Embedding (A-RoPE). Furthermore, we design an action-aware constraint loss to stabilize training and focus the learning process on these critical interactive frames. To facilitate research and evaluation, we construct VVT-Interact, the first large-scale dataset for this task. Extensive experiments demonstrate that iTryOn not only achieves state-of-the-art performance on traditional VVT benchmarks but also establishes a commanding lead in the new interactive setting, marking a significant step towards more dynamic and controllable virtual try-on experiences.", "tldr": "We make video virtual try-on truly dynamic, enabling models to realistically synthesize complex hand-garment interactions like pulling, stretching, and zippering for the first time.", "keywords": ["video generation", "virtual tryon"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c26461ed9b2acd019f367ef349d88db077b76ae5.pdf", "supplementary_material": "/attachment/cbd3e2339713ed4d7ff19d0d42dc54ff20ad996d.zip"}, "replies": [{"content": {"summary": {"value": "Given a video and a garment image, the goal of this paper is to change the garment in the input video to match the input garment image, while maintaining realistic human-garment interactions.\n\nThe authors introduce several contributions:\n\n(1) Multi-level interaction injection mechanism: The authors leverage a 3D hand prior (HaMeR model (Pavlakos et al., 2024)) and global captions for video segments, along with Action-aware Rotational Positional Embedding.\n\n(2) Constraint loss: Intensifies supervision around interactions in a video, since these are more rare than non-interactive video clips.\n\n(3) VVT-Interact: The first dataset for interactive video try-on"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality: \n- This paper addresses the unique challenge of VVTO with interaction.\n- The paper introduces a novel benchmark for evaluating interactive VVTO\n\nQuality:\n- The folding/movement of the garment fabrics looks very realistic with respect to the material of the input garment.\n- Compared to related methods, iTryOn has much more realistic garment/person interactions\n\nClarity:\n- The authors provide clear details about their implementation, training details, and choice of models\n- Contributions are well-justified and ablated\n\nSignificance:\n- Interactive VVTO is a key challenge that could greatly increase realism of try-on videos, if solved."}, "weaknesses": {"value": "- There are some noticeable warping artifacts in the supplementary videos, especially near the garment/person boundary\n- In most examples, the garments in the input image and input video are very similar in shape and fit. More examples should be provided where there is significant shape change (e.g. short to long sleeves, long to short garment, etc.), or this should be listed as a limitation of the method.\n- Similarly, how does the method work if the interaction is implausible? For example, unzipping a shirt without a zipper? This should also be listed as a limitation of the method.\n- VVT-Interact (based on the examples shown) is limited in diversity of appearance and body shapes\n- The test dataset for VVT-Interaction s only 180 videos, which is only ~2% of the training videos. I think this  small scale limits how reliable it can be as a benchmark.\n- In the comparisons in Figure 5, the iTryOn garment looks a bit over-saturated w.r.t. the input garment for both examples, while MagicTry-On seems to have better garment fidelity. Perhaps this should be addressed?"}, "questions": {"value": "- There does not seem to be much validation provided of the VLM used for extracting video annotations. Although such evaluation may be tricky, it seems essential to the method that the annotations are valid, so I would suggest finding some way to validate that the annotations are reliable, such as through human qa.\n- In section 3.5 (lines 316-318), why is A-RoPE only applied to some keys, but all queries?\n- Since A-RoPE is based on 1D-RoPE, I recommend adding a brief description about 1D-RoPE as a preliminary, for example.\n- I am surprised that AC-Loss does not improve the performance more in the quantitative ablations (Table 3). Is this because the evaluation datasets are not focused on interactive video-clips? That is, if the evaluation dataset consisted only (or mostly) of interactive clips from the dataset, would AC-loss make a bigger improvement? In general, it would be interesting to see the benefit of each contribution specifically for handling interactive clips.\n- Currently, the interactive motion is always replicated from the input video. Future work with this dataset could include editing or adding interaction to an input video using text annotations."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "(1) The paper exceeds the 9-page limit\n\n(2) The authors curate videos of people from online, but those individuals have not explicitly consented to having their data (specifically face identity) used for this publication/dataset, although it may have been granted implicitly under the terms of the upload platform."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HcgqxRY8Et", "forum": "UHm5HIo8CO", "replyto": "UHm5HIo8CO", "signatures": ["ICLR.cc/2026/Conference/Submission4011/Reviewer_Ebq8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4011/Reviewer_Ebq8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760648376443, "cdate": 1760648376443, "tmdate": 1762917134758, "mdate": 1762917134758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Interactive Video Virtual Try-On (Interactive VVT), targeting scenarios where subjects actively manipulate garments (e.g., pulling hems, unzipping), and highlights challenges in resolving interaction semantics and modeling sparse, complex deformations. \nIt proposes iTryOn, a video diffusion Transformer with multi-level interaction injection: a garment-agnostic 3D hand prior for precise hand–garment contact, and global plus time-stamped action captions aligned via Action-aware Rotational Position Embedding (A-RoPE). \nAn action-aware constraint loss further stabilizes training and emphasizes key interactive frames.\nThe authors also release the VVTInteract dataset and report state-of-the-art results on standard VVT and substantial gains in the interactive setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-structured, clearly written, and easy to follow.\n- The problem addressed—interactive video virtual try-on—is timely and important for the VVT community.\n- The authors introduce a new large-scale dataset that enables rigorous evaluation and future research on this task."}, "weaknesses": {"value": "- In the practical use of the system, users only provide a reference video and a target clothing image. Fine-grained conditions such as 3D hand pose and human pose can be automatically detected by existing models. However, the acquisition of global/action prompts remains unclear; it is ambiguous whether these are also user inputs or obtained otherwise.\n- The use of the term “interaction” in the paper is ambiguous. While it is commonly interpreted as interaction between the system and users, the paper refers to interaction between a person and clothing, which is unconventional and potentially confusing.\n- The paper addresses the \"how\" of an interaction by introducing 3D hand pose as an additional condition, yet this approach lacks novelty. More importantly, it remains unclear why the interaction between the task and clothing can be sufficiently defined only by hand movements.\n- The paper addresses the \"what\" the type of action by defining several interaction types using captions. However, the generalizability of this approach is questionable, as the proposed method may not scale to a broader range of interactions.\n- The paper solves the \"when\" precise timing by ensuring that action prompts interact only with interaction frames and not with non-interaction frames. Nevertheless, in practical scenarios, it is not specified how interaction frames are accurately identified."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ROiDhO9JaC", "forum": "UHm5HIo8CO", "replyto": "UHm5HIo8CO", "signatures": ["ICLR.cc/2026/Conference/Submission4011/Reviewer_UMo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4011/Reviewer_UMo4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571816689, "cdate": 1761571816689, "tmdate": 1762917134535, "mdate": 1762917134535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces iTryOn, a novel framework for Interactive Video Virtual Try-On that addresses the limitation of existing methods in handling active human-garment interactions. The authors formalize a new task where subjects actively engage with clothing (e.g., pulling hems, unzipping jackets). The framework is built upon Wan2.1-VACE with novel components including A-RoPE and an action-aware constraint loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces Interactive VVT as a new and challenging task that bridges the gap between passive garment display and real-world e-commerce scenarios with active human-garment interactions."}, "weaknesses": {"value": "1. **Limited Technical Innovation**: While the paper presents novel guidance mechanisms, the underlying diffusion transformer architecture closely follows existing designs (Wan2.1-VACE) without fundamental architectural innovations. The contributions are primarily in the conditioning and guidance layers rather than core generative modeling advances. In fact, the Wan2.1-VACE model itself can achieve video virtual try-on functionality. The author needs to explain this and provide performance comparisons.\n\n2. **Insufficient Ablation Study**: The ablation study lacks detailed analysis of individual components. Critical hyperparameters like the A-RoPE separation scale (k=4) and action constraint loss weight (λ=0.5) are presented without sensitivity analysis or justification for these specific choices.\n\n3. **Unfair Comparison**: Directly comparing iTryOn with the baseline methods may not be entirely fair in Tables 1-2 , given that iTryOn benefits from a substantial amount of extra training data. This raises uncertainty regarding whether the observed improvements stem from the added data or the unique architecture of iTryOn. To ensure a thorough and impartial assessment, it is recommended that the authors re-train their approach only using publicly accessible datasets like VVT and ViViD.\n\n4. **Computational Efficiency**: Despite claims of parameter efficiency (2B vs competitors' 14B), the paper lacks detailed analysis of inference time, memory requirements, and computational complexity compared to baseline methods. The practical deployment feasibility remains unclear.\n\n5. **Lack of specialized metrics for interaction fidelity**: Although the author introduced the new task of Interactive VVT, the evaluation metrics still rely on generic video/image metrics (such as VFID, SSIM). The authors fail to provide appropriate metrics to quantify the physical plausibility of garment deformations (e.g., distinguishing realistic stretching from visual coherence).\n6. **Imbalanced dataset distribution**: Over 70% of samples belong to \"Adjusting the collar\" and \"Adjusting the hem\", leading to potential bias in model generalization across rare interaction types (e.g., \"Other interactions\" at 2.98%)."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "H3jl41qwv8", "forum": "UHm5HIo8CO", "replyto": "UHm5HIo8CO", "signatures": ["ICLR.cc/2026/Conference/Submission4011/Reviewer_mGgn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4011/Reviewer_mGgn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813013043, "cdate": 1761813013043, "tmdate": 1762917134327, "mdate": 1762917134327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a video tryon method where the human interacts with the garments noticeably. It is done by (1) modeling the hands explicity with 3D hand mesh model, and (2) a action-position-encoding to bias the generation on the motion rather than un-action words. They also introduce a dataset specifically for tryon with hands-cloth interaction. They showed improved results over prior state of art for video tryons."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Supporting human interaction with the garments is a important yet natural task for virtual tryon. The VVT-interact dataset is an important next step towards this goal.\n2. The action aware semantic guidance is necessary to avoid the model from being biased by non-action words in the prompt."}, "weaknesses": {"value": "1. The proposed method requires an existing video to run tryon. Depending on the motion of this video, there could be incompatiblity with the tryon garments,  say roll up sleeve motion when the garment is a short-sleeve, or unzip the jacket when the garment is a t-shirt. Ideally, we should be able to use text prompt to control the motion of the user in the source video such that we can select the garment that is compatible with the prompt.\n\n2. The method uses explicit 3D hand mesh to condition try video generation model. This estimated hand model is often misaligned with the actual image. This is problematic because the video has to generate the hand that matches the rgb hand pixels of the exiting source video.\n\n3. It was unclear why the existing clothing is not \"delcoth\", leading to bleeding problem seen in Fig. 3"}, "questions": {"value": "1. Line 200-201: looks like the action caption is determined by per-frame by a VLM. It is hard to determine action or motion with only 1 frame.\n2. Vivid is a dataset with limited to no hands and cloth interaction. Why does the model outperform existing method on this data, as shown in Tab 2, especially with the smallest model capacity (2B)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lkfBA2obep", "forum": "UHm5HIo8CO", "replyto": "UHm5HIo8CO", "signatures": ["ICLR.cc/2026/Conference/Submission4011/Reviewer_axoj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4011/Reviewer_axoj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017557487, "cdate": 1762017557487, "tmdate": 1762917134091, "mdate": 1762917134091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}