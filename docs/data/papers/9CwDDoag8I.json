{"id": "9CwDDoag8I", "number": 24654, "cdate": 1758359041318, "mdate": 1759896756825, "content": {"title": "MATA: Memory-Augmented Temporal Anchors for Sparse-Time Dynamic Knowledge Graph Embedding", "abstract": "Dynamic knowledge graphs (KGs) face major hurdles in temporal link prediction, especially due to issues like concept drift, sparse supervision over time, and the heavy computational costs of updating in evolving environments. To tackle these challenges, we introduce MATA (Memory-Augmented Temporal Anchors), a new framework that leverages learnable temporal anchors stored in a differentiable memory module. MATA provides three main capabilities: (1) temporal interpolation for any timestamp through attention-based synthesis over memory anchors, (2) contrastive learning that maintains temporal consistency while allowing semantic evolution, and (3) cross-version alignment to ensure coherent embeddings across stages of KG evolution.\nExtensive experiments on four benchmark datasets, such as ICEWS14 (+12.4%), GDELT (+15.7%), WikiKG (+8.3%), and YAGO-T (+11.2%), show that MATA delivers state-of-the-art results, achieving an 8.3–15.7% improvement in mean reciprocal rank (MRR) compared to existing methods such as recurrent model- T-GCN (Temporal Graph Convolutional Networks), TANGO, CyGNet, Know-Evolve, T-GAT (Temporal Graph Attention Network), Memory-based approach - MemN2N. The framework is especially effective in sparse-temporal settings, with a 27.3% MRR gain under 70% missing timestamps, while maintaining strong performance across varying levels of temporal sparsity. Ablation studies confirm the importance of each component: removing temporal anchors reduces performance by 12.4%, removing attention mechanisms by 8.7%, and removing contrastive learning by 6.2%. Overall, MATA introduces a new paradigm for addressing temporal sparsity and concept drift in dynamic knowledge representation learning, with applications in time-aware retrieval, reasoning, and evolving knowledge systems.", "tldr": "MATA is a new framework for dynamic knowledge graphs that uses memory-augmented temporal anchors to handle sparse data, concept drift, and evolving environments.", "keywords": ["Dynamic Knowledge Graphs", "TTemporal Embeddings", "Memory Networks", "Contrastive Learning", "Link Prediction", "Concept Drift", "Sparse Temporal Data", "Version Alignment"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ca9b9393966148439a802fab3868159dda8382f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MATA (Memory-Augmented Temporal Anchors), a framework for temporal knowledge graph (TKG) embedding under sparse-time conditions. The method includes a learnable memory of “temporal anchors” to interpolate embeddings for missing timestamps, combined with a contrastive temporal consistency loss. Experiments are reported on four benchmark datasets (ICEWS14, GDELT, WikiKG, YAGO-T), where MATA, according to the authors, outperforms several earlier TKG models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The ablation study (Table 3) demonstrates that each proposed component contributes to performance.\n* The notation table is helpful.\n* it seems like the hyperparameters of the method are somewhat robust, and can be re-used for different datasets without hyperparameter tuning."}, "weaknesses": {"value": "1. Unconvincing Motivation\n\nThe paper claims that “real-world TKGs are incomplete, timestamps are often missing, and semantic drift can throw models off course,” yet provides no real-world evidence, examples, or citations for these claims.\nAll experiments are conducted on standard, fully timestamped datasets (ICEWS14, GDELT, etc.), and temporal sparsity is introduced artificially by randomly removing up to 70 % of timestamps. This setting is not realistic and undermines the core motivation.\nThe authors need to show either a real dataset exhibiting this issue or a practical scenario where timestamp sparsity genuinely arises.\n\n2. Experimental Rigor and Reproducibility\n* The paper lacks key experimental details: filtering strategy (static vs. time-aware), single-vs-multi-step prediction, train/validation/test splits, hyperparameters, scoring function, and code availability are all missing. I recommend reading [1] for more information on evaluation of TKG forecasting.\n* Dataset statistics and references are not provided. For example, “YAGO-T” is not clearly defined, and ICEWS18, a standard benchmark is omitted without explanation. [1] describes that there exist multiple versions of e.g. ICEWS14, thus is is important to name the version/dataset size etc of the datasets for reproducibility.\n* Only MRR is reported; Hits@k metrics are absent (even though they are promised  on page 5).\n* Baselines: the baselines are outdated (mostly ≤ 2021) and likely not comparable under the same settings (e.g. filter settings [1]  unclear, single-vs multi-step prediction unclear).\n\t\n3. Outdated and Incomplete Baselines\n\nThe comparison excludes almost all recent TKG models. Regcn [2] , TiRGN [3], CognTKE [4], Tlogic [5], Recurrency Baseline [6] (as a baseline). The claim of “state-of-the-art” performance is therefore unjustified.  I expect at least the recurrency baseline would work well under the sparse temporal setting, and potentially also Tlogic, the rule-based approach.\n\n4. Clarity and Structure\n* The introduction is poorly structured: the task (temporal extrapolation vs. interpolation) is introduced too late, confusing the scope of the paper. Further, the introduction is lacking references (0 references).\n* The related work section is incomplete and mixes problem formulations (interpolation, extrapolation, rule-based inference) without clear distinctions. For example all rule-based approaches (Tlogic, CognTKE, Infer,..) are completely missing. I recommend to write less information for each method, but try to structure it better, and have a more concise overview.\n* \"To our knowledge, MATA is the first TKG framework to apply temporal contrastive learning.\" -> no, constrastive learning for TKG was e.g. tackled by CENET [7]\n* The method section is superficial. I would appreciate a short motivation for each component, and an explanation of how the sub-modules work together (or a little figure). Several key information is missing, e.g. how is L_link computed? Which scoring function is used? How are the node and relation embeddings initialized? As far as I understand, the only way how the graph structure is taken into account is via the scoring function, but it is not clear what scoring fct is used?\n* Citations are inconsistent (wrong LaTeX usage: \\cite vs. \\citet/\\citep) and many statements lack references altogether (e.g. no reference in introduction).\n\n5. Evaluation Design and Validity \n\nThe simulated missing-timestamp experiments (random removal of 30–70 %) do not reflect any realistic data condition and seem tailored to favor the proposed interpolation mechanism.\nWithout real-world evidence of timestamp sparsity or irregularity, the empirical results have little practical significance, and thus the motivation and superiority of the proposed method is not credible.\n\n\n## Overall Assessment\nWhile the idea of combining learnable temporal anchors with contrastive learning is somewhat interesting, and the approach is reasonably simply, the paper fails to justify why this problem matters in real TKGs, and the experimental setup lacks rigor, comparability, and transparency. The motivation is not convincing, the baselines are outdated and incomplete, and the results cannot be trusted without reproducibility details. Writing and referencing also fall below the clarity standards of ICLR.\nMainly for Weaknesses 1-3 I recommend rejecting the paper.\n\n## References\n[1]  Gastinger, J., Sztyler, T., Sharma, L., Schuelke, A., & Stuckenschmidt, H. (2023, September). Comparing apples and oranges? on the evaluation of methods for temporal knowledge graph forecasting. In Joint European conference on machine learning and knowledge discovery in databases (pp. 533-549). Cham: Springer Nature Switzerland.\n\n[2] Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng Guo, Huawei Shen, Yuanzhuo Wang, and\nXueqi Cheng. Temporal knowledge graph reasoning based on evolutional representation learning.\nIn The 44th International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR), 2021b.\n\n[3] Yujia Li, Shiliang Sun, and Jing Zhao. TiRGN: Time-guided recurrent graph network with local-\nglobal historical patterns for temporal knowledge graph reasoning. In Proceedings of the 31st\nInternational Joint Conference on Artificial Intelligence (IJCAI), pp. 2152–2158, 2022a.\n\n[4]  Wei Chen, Yuting Wu, Shuhan Wu, Zhiyu Zhang, Mengqi Liao, Youfang Lin, and Huaiyu Wan.\nCogntke: A cognitive temporal knowledge extrapolation framework. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 39, pp. 14815–14823, 2025.\n\n[5]  Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, and Volker Tresp. TLogic: Temporal\nlogical rules for explainable link forecasting on temporal knowledge graphs. In 36th Conference\non Artificial Intelligence (AAAI), pp. 4120–4127, 2022\n\n[6] Julia Gastinger, Christian Meilicke, Federico Errica, Timo Sztyler, Anett Schuelke, and Heiner\nStuckenschmidt. History repeats itself: A baseline for temporal knowledge graph forecasting.\nIn Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJ-\nCAI), 2024b\n\n[7]  Yi Xu, Junjie Ou, Hui Xu, and Luoyi Fu. Temporal knowledge graph reasoning with historical\ncontrastive learning. In 37th Conference on Artificial Intelligence (AAAI), pp. 4765–4773, 2023"}, "questions": {"value": "Q1: Can you please clarify fully the experimental setup? (single-vs multi-step, time-aware or static filter, dataset versions/splits/statistics, hyperparameters tested and used)\n\nQ2: What scoring function did you use? Did you test different scoring functions? Do I understand correctly, that the only way how the graph structure is taken into account is via the scoring function?\n\nQ3: Missing baseline comparisons: Could you integrate comparison to my above mentioned baselines?\n\nQ4: Do you have a real-world use case/reference/ dataset for the missing timestep scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ulZVrlEpJR", "forum": "9CwDDoag8I", "replyto": "9CwDDoag8I", "signatures": ["ICLR.cc/2026/Conference/Submission24654/Reviewer_nU5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24654/Reviewer_nU5v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760526359718, "cdate": 1760526359718, "tmdate": 1762943148346, "mdate": 1762943148346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MATA to address the limitations of existing modelsunder sparse-time conditions. It introduces a learnable set of temporal anchors stored in a differentiable memory module, enabling attention-based interpolation of timestamp-aware embeddings even at unseen or missing timepoints.  Experiment on four standard TKG benchmarks demonstrate the performance of MATA."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. MATA enables attention-based interpolation of timestamp-aware embeddings even at unseen or missing timepoints.\n2. A cross-version alignment objective stabilizes entity and relation embeddings across evolving KG snapshots, enhancing long-term coherence."}, "weaknesses": {"value": "1. It is not clear how the problem of data is incomplete, timestamps are often missing, and semantic drift, are addressed by this method, since these strategy are not novel.\n2. The presentation is vague. For example, what is the scoring function in the inference procedure."}, "questions": {"value": "1. What is the difference between CONTRASTIVE TEMPORAL CONSISTENCY and CROSS-VERSION EMBEDDING ALIGNMENT?\n2. CROSS-VERSION means the graph in different timestamps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PRKdCP4W4V", "forum": "9CwDDoag8I", "replyto": "9CwDDoag8I", "signatures": ["ICLR.cc/2026/Conference/Submission24654/Reviewer_5xTR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24654/Reviewer_5xTR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760592875961, "cdate": 1760592875961, "tmdate": 1762943148115, "mdate": 1762943148115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The MATA (Memory Augmented Temporal Anchors) framework proposed in this paper innovatively introduces a differentiable memory module to store learnable temporal anchors for the problem of embedding sparse temporal dynamic knowledge graphs. By combining the contrastive temporal consistency loss and cross version alignment objectives, it effectively solves the limitations of existing models in scenarios such as temporal sparsity and semantic drift."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The Temporal Anchor Memory module is ingeniously designed to embed and interpolate any timestamp (including unobserved timestamps) through attention mechanisms, breaking through the limitations of traditional models that rely on dense time sampling or fixed time encoding, and providing a new approach to solving time sparsity problems."}, "weaknesses": {"value": "1. The description in the model section of this article is very unclear and not detailed.\n\n2. The motivation of this article is unclear, and the research on related work is too outdated"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AVtfLiu88D", "forum": "9CwDDoag8I", "replyto": "9CwDDoag8I", "signatures": ["ICLR.cc/2026/Conference/Submission24654/Reviewer_PzwV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24654/Reviewer_PzwV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761231118776, "cdate": 1761231118776, "tmdate": 1762943147826, "mdate": 1762943147826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MATA, a framework for learning embeddings on Temporal Knowledge Graphs. The framework includes a learnable temporal anchor mechanism to capture historical information from different timestamps. On this basis, the authors use a contrastive temporal consistency loss to smooth the evolution of entity representations across nearby timestamps. Experimental results show the effectiveness of MATA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. Learning representations for TKGs under the sparse time scenario is interesting."}, "weaknesses": {"value": "W1. The proposed framework appears to lack technical contributions. Specifically, the design of learnable anchor embeddings seems relatively naive, and it remains unclear how they effectively address the sparsity issue. Moreover, the temporal consistency loss adopted in the work is a commonly used technique and does not introduce notable novelty.\n\nW2. The paper lacks a comprehensive analysis of previous works and studies. It contains only a few citations."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CbZiyde0iS", "forum": "9CwDDoag8I", "replyto": "9CwDDoag8I", "signatures": ["ICLR.cc/2026/Conference/Submission24654/Reviewer_z9VH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24654/Reviewer_z9VH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761457254259, "cdate": 1761457254259, "tmdate": 1762943147307, "mdate": 1762943147307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}