{"id": "cDPaTlYEp0", "number": 6551, "cdate": 1757988644196, "mdate": 1759897908353, "content": {"title": "Fine-Grained Prompt-Driven Stylization with Context-Aware Reasoning for Zero-Shot Domain Adaptation", "abstract": "Zero-shot domain adaptive semantic segmentation (ZSDA) aims to generalize models to unseen target domains without accessing target data during training. Recent methods commonly use vision-language models (VLMs) to simulate target-domain features by guiding stylization with textual prompts. However, these approaches often suffer from two key issues: description mismatch, where generic prompts fail to reflect scene-specific semantics, and prompt-induced discrepancy, where normalization guided by coarse prompts cannot capture spatial variations. Together, these problems lead to a noticeable simulated-vs-real feature gap, reducing adaptation effectiveness. To address this, we propose FineDA, a framework designed to reduce this gap through image-specific prompt reasoning and fine-grained feature stylization. FineDA introduces a scene graph-guided chain- of-thought module that generates contextual, semantically rich target descriptions for each source image. It also incor- porates a prompt-guided local and global stylization module, enabling patch-wise class-specific adaptation while maintaining scene-level consistency. Extensive experiments on standard ZSDA benchmarks and a challenging in-house surgical dataset with adverse visual conditions such as smoke, blood, and low lighting demonstrate the effectiveness and generalization capability of our approach. Code will be released upon publication.", "tldr": "", "keywords": ["Zero-Shot", "Domain Adaptation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/440ec89826159fa91e511d4eb36e9865621efd81.pdf", "supplementary_material": "/attachment/539cdc6b0d507d9aa71af55984b3bd60c0005900.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles zero-shot domain adaptive semantic segmentation (ZSDA) without target images. It identifies two key issues in prompt-driven stylization: description mismatch and prompt-induced discrepancy. FineDA proposes: (1) a Scene Graph–guided Chain-of-Thought (SGCOT) to generate image-specific, context-rich target descriptions; (2) a Prompt-driven Local and Global Feature Stylization (PLGFS) that performs patch-wise, class-aware PIN alongside a global prompt; (3) a Visual-Text Feature Alignment Regularizer (FAR) akin to DenseCLIP to improve pixel-level alignment during fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper has clear problem statement and motivation, i.e., addressing the gap between simulated and real-world characterizations in ZSDA with concrete failure modes.\n2. The proposed SGCOT for prompt generation and PLGFS for joint local-global PIN are reasonable, complementary extensions to prior prompt-driven stylization."}, "weaknesses": {"value": "1. SGCOT uses the ground truth mask to build the scene graph and select the class names for local cues, which is valid in ZSDA (the labels are present in the source). However, it should be clarified that the target label or image is never used for cue generation, and any VLM/LLM conditioning only sees the source image and source mask. Safeguards to prevent indirect use of target statistics should be explicitly stated.\n2. PLGFS instantiates per-patch PIN with class-specific prompts. With m>1, does each patch get a separate PIN instance or a shared predictor conditioned on the prompt? Training m independent PINs per image can be memory-heavy and brittle.\n3. The scene graph creation depends on VLM outputs. How often do object hallucinations occur, and how effectively does the LLM filter mitigate them? \n4. FAR defines a cost map and uses cross-entropy on cosine similarities. Cosine scores can be negative and unnormalized and detail the softmax step to ensure valid probabilities.\n5. You are suggested to discussing failure modes in extreme night scenes with sparse class presence, rare classes without reliable textual synonyms, or out-of-vocabulary classes for CLIP text encoder.\n6. Fix typos and consistency (e.g., “Implentation” → “Implementation”, consistent use of ResNet-50 naming and ensure consistent λ notation between sections)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qJcdgR0axm", "forum": "cDPaTlYEp0", "replyto": "cDPaTlYEp0", "signatures": ["ICLR.cc/2026/Conference/Submission6551/Reviewer_zVFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6551/Reviewer_zVFA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482363219, "cdate": 1761482363219, "tmdate": 1762918896142, "mdate": 1762918896142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of Zero-Shot Domain Adaptive Semantic Segmentation, where models must generalize to unseen target domains without accessing target domain data during training. The authors propose a framework that tackles two key problems in existing methods: description mismatch, where coarse textual prompts fail to capture scene-specific semantics, and prompt-induced discrepancy, where uniform global stylization cannot handle spatially non-uniform domain shifts. The authors report state-of-the-art results in terms of mIoU and mDice, showing significant improvements over prior methods like PØDA, ULDA, and PiCaZo."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the practical problem of zero-shot domain adaptation for semantic segmentation, particularly useful in safety-critical applications like medical imaging and autonomous driving. The introduction of a surgical dataset with complex visual artifacts adds value to the evaluation.\n\n2. The paper includes detailed ablation studies to evaluate the contributions of individual components (SGCOT, PLGFS, FAR), showing how each module improves performance. This demonstrates the complementary nature of the components.\n\n3. The use of scene graphs for generating semantically rich, image-specific prompts is a novel and promising approach. By incorporating object attributes and relationships, the framework creates more contextually relevant descriptions, which improve stylization and alignment with the target domain."}, "weaknesses": {"value": "1. While the SGCOT and PLGFS modules are novel in their combination, they build upon existing techniques rather than introducing fundamentally new concepts. For example, Prompt-Instance Normalization (PIN) is extended to a patch-wise setting, and chain-of-thought reasoning with scene graphs has been explored in related works. The overall contribution feels more like a refinement of existing ideas rather than a breakthrough.\n\n2. The paper does not provide sufficient justification for several design choices, such as the specific number of patches, the weighting of local versus global losses, or the use of a two-step chain-of-thought reasoning approach. While these choices are empirically validated, their theoretical basis is unclear.\n\n3. While quantitative results are strong, the qualitative analysis is limited. More examples of segmentation outputs, particularly in challenging regions (e.g., occlusions, glare), would help illustrate the method’s strengths.\n\n4. The patch-wise processing in PLGFS and the two-step chain-of-thought reasoning in SGCOT introduce significant computational costs, making the method less practical for real-time or large-scale applications. As shown in Table 5, FineDA has a longer adaptation time compared to PØDA and ULDA, which raises concerns about its scalability in resource-constrained settings."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nHSUk8iPmh", "forum": "cDPaTlYEp0", "replyto": "cDPaTlYEp0", "signatures": ["ICLR.cc/2026/Conference/Submission6551/Reviewer_z5Yo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6551/Reviewer_z5Yo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889313638, "cdate": 1761889313638, "tmdate": 1762918895760, "mdate": 1762918895760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **FineDA**, a zero-shot domain adaptation framework for semantic segmentation that combines context-aware prompt reasoning with fine-grained local–global stylization.  The method introduces three components: a Scene Graph-Guided Chain-of-Thought (**SGCOT**) module for structured prompt generation, a Prompt-Driven Local and Global Feature Stylization (**PLGFS**) block for patch-level alignment, and a Visual–Text Feature Alignment Regularizer (**FAR**) enforcing pixel-level semantic consistency with CLIP embeddings.  **FineDA** achieves consistent gains over **PØDA**, **ULDA**, and other baselines on *Cityscapes→ACDC*, *Cityscapes→GTA5*, and a new surgical dataset (*SurgDVC*)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-motivated problem** – The work addresses the lack of fine-grained semantic reasoning in prompt-based ZSDA, which is timely given the increasing interest in vision–language alignment and zero-shot generalization.\n\n2. **Comprehensive experiments** – Evaluations span multiple datasets and include a medical setting, highlighting cross-domain robustness.  \n\n3. **Reasonably clear methodology** – The modular design (SGCOT, PLGFS, FAR) is well-defined and follows a consistent training objective.  \n\n4. **Empirical improvement** – Reported quantitative results are strong and consistent across benchmarks."}, "weaknesses": {"value": "1.  **Weak justification of design choices** – The benefit of using **scene graph–guided reasoning** and **local–global stylization** on top of CLIP is unclear. CLIP’s text encoder can only process 77 tokens, making such complex reasoning potentially ineffective. For relatively simple VLMs like CLIP, well-structured but concise prompts may already be optimal, and it is uncertain whether the proposed modules genuinely improve alignment.  \n2. **Limited theoretical depth** – The core contributions are primarily architectural and empirical. The paper lacks deeper theoretical justification or analysis of why fine-grained stylization leads to measurable alignment improvement beyond empirical observation.\n\n3. **Reliance on ground-truth masks** – The construction of scene graphs assumes access to segmentation masks, which weakens the “true zero-shot” claim. Without a mask-free or pseudo-label alternative, the method’s general applicability is constrained.\n\n4. **Computational complexity** – The adaptation process is significantly more expensive (≈6× runtime of PØDA). No clear mitigation strategy or runtime–accuracy trade-off analysis is provided.\n\n5. **Incremental innovation** – Each individual component (PIN-based normalization, CLIP alignment, CoT reasoning) builds directly upon existing techniques. The novelty primarily lies in integration rather than fundamentally new insight.\n\n6. **Clarity and presentation** – Figures are visually dense, and some key concepts (e.g., local/global feature separation) could benefit from clearer visualization. The exposition in Section 3 is heavy and difficult to follow for general readers."}, "questions": {"value": "1. **How can the proposed framework achieve a truly mask-free scene graph construction?**  \n   The current design seems to rely on segmentation masks to generate scene graphs, which contradicts the zero-shot assumption.  \n   Could the authors explore unsupervised detection or attention-based parsing to build scene graphs without labeled masks?\n\n2. **What concrete evidence supports that the reasoning and stylization modules improve CLIP feature alignment?**  \n   Given CLIP’s limited 77-token text encoder, complex reasoning may not yield proportional gains.  \n   Have the authors conducted diagnostics (e.g., feature-space distance metrics or t-SNE visualization) to show measurable improvements in domain alignment?\n\n3. **How can the presentation and formulation be simplified for clarity?**  \n   The mathematical sections are dense, and figures are visually heavy.  \n   Can the authors improve figure readability and streamline notation to make the core ideas more accessible?\n\n4. **Can FineDA generalize beyond segmentation?**  \n   The current validation focuses solely on semantic segmentation.  \n   Have the authors tested or considered extensions to object detection, depth estimation, or broader multimodal reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MCOWu8UTEY", "forum": "cDPaTlYEp0", "replyto": "cDPaTlYEp0", "signatures": ["ICLR.cc/2026/Conference/Submission6551/Reviewer_uSaS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6551/Reviewer_uSaS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931978804, "cdate": 1761931978804, "tmdate": 1762918895287, "mdate": 1762918895287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}