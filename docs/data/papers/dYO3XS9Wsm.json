{"id": "dYO3XS9Wsm", "number": 5786, "cdate": 1757935152354, "mdate": 1763732192958, "content": {"title": "Faithful Simulation of User–Agent–Environment Interactions for Scalable LLM Agent Evaluation", "abstract": "Large language models (LLMs) are transitioning from chatbot to interactive agents. In this shift, environments have become critical for both evaluating their performance and improving their capabilities. Yet current options are not good enough: human-in-the-loop testing is prohibitively costly, and available benchmarks and simulation framework oversimplify interactions, failing to capture real-world complexity. This paper presents a fully automated framework for simulating User–Agent–Environment interactions, providing scalable and faithful interaction data for agent evaluation. The framework works by: (1) constructing multi-step tasks by sampling from a Tool–Relationship Graph, (2) simulating closed-loop conversations with configurable user and environment archetypes, (3) evaluating outcomes with Procedural Alignment (Procedure Alignment Score), end-to-end success (Outcome Success), and simulation faithfulness (Configuration Similarity). We apply this framework to evaluate state-of-the-art open- and closed-source agents. Experiments across thousands of scenarios reveal three key findings: (i) environment reliability is the dominant factor in agent success, (ii) user archetypes strongly shape performance, and (iii) tool-calling trace fidelity correlates with—but does not fully determine—end-to-end goal achievement. By integrating User, Agent, and Environment in a unified loop, and embedding flexibility with explicit faithfulness control, our framework provides a principled basis for evaluating and improving agentic LLMs under diverse conditions.", "tldr": "", "keywords": ["LLM Agents", "User–Agent–Environment Simulation", "Agent Evaluation Framework"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e1a64e10ed67c04070a2240d4ab11d82bc6540f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FUSE, a fully automated framework for simulating User–Agent–Environment interactions to enable scalable and faithful agent evaluation.\n\nThe tools are curated from real MCP servers. And the frameworks supports different type of users and environments, which simulates more cases in real-life cases. The task generation pipeline makes the evaluation scalable and is designed to cover diverse tools."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The evaluation considers the different user and environment types.\n- The task generation pipeline makes the evaluation scalable and is designed to cover diverse tools.\n- The metric is comprehensive and contains procedure, result, and introduce meta-evalutation for the framework itself.\n- The systematic experiments uncover critical factors influencing agent performance (environment reliability, user archetypes, trace length), providing concrete best practices for developers (e.g., stress-testing across environments, reporting dual metrics) and guiding the optimization of agentic LLMs."}, "weaknesses": {"value": "- The LLM-as-Judge module may need human verification.\n- The Procedure Alignment Score assumes that local semantic similarity implies procedural interchangeability and that severity classifications reflect true risk."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NDaBqyMstb", "forum": "dYO3XS9Wsm", "replyto": "dYO3XS9Wsm", "signatures": ["ICLR.cc/2026/Conference/Submission5786/Reviewer_jYuP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5786/Reviewer_jYuP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760876755248, "cdate": 1760876755248, "tmdate": 1762918261634, "mdate": 1762918261634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FUSE, a simulation framework for faithful and scalable evaluation of tool-using LLM agents. Its core contribution lies in building a closed-loop user–agent–environment interaction system, addressing the limitations of existing evaluation approaches in terms of cost (manual testing) and authenticity (over-simplified benchmarks).\n\nThe main contributions of the paper can be summarized as follows:\n\n1. Task Generation: Leveraging a tool relationship graph, the system automatically generates multi-step task bundles from the MCP server with ground-truth tool sequences.  \n2. Interaction Simulation: Introduces configurable user prototypes (e.g., planner, information hider) and environment prototypes (e.g., perfect, faulty, adversarial), simulating realistic interaction dynamics.  \n3. Multi-dimensional Evaluation Metrics: Proposes a process alignment score (a customized edit distance considering tool semantics and risks) and a result success score (end-to-end goal achievement judged by an LLM). These metrics complement each other.  \n4. Meta-evaluation Framework: Quantifies the trustworthiness of the simulation itself through five faithfulness audit metrics (e.g., solvability, prototype consistency), thereby enhancing the reliability of evaluation results.  \n5. Extensive Experimental Analysis: Validates the framework with 3,600 runs. Key findings include: environment reliability is the primary determinant of agent performance; user prototypes significantly affect outcomes; there is a moderate correlation between process alignment and result success."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper lie in its high originality, rigorous quality, clear presentation, and notable academic as well as practical value. The core originality of the work is in precisely defining the challenge of evaluating LLM-based agents as a problem of simulation fidelity, and in proposing an integrated solution that creatively combines MCP-protocol-based task generation, configurable user and environment prototypes, and novel process alignment metrics incorporating semantic and risk considerations. Through large-scale experiments (covering 3,600 runs and systematically controlling multiple variables), the authors have achieved a high-quality implementation. Moreover, the paper is clearly structured, logically coherent, and features precise terminology, effectively conveying its technical contributions. Ultimately, the significance of this work lies in providing a principled and scalable benchmark to address a critical bottleneck in the evaluation of LLM agents."}, "weaknesses": {"value": "- As a scalable benchmark, the evaluation covers too few models—only three. It would be more convincing if mainstream large models were all evaluated.\n- The task step length does not appear very impressive; existing benchmarks often have longer sequences, which pose a greater challenge to the models.\n- Cost concern: Although the paper proposes that the benchmark enables automated evaluation, the number of LLM calls in the experiments seems very large. Moreover, all the models used are top-tier, and the paper does not mention the API costs. Providing cost information and potential, cheaper alternative models would be very helpful for the overall evaluation.\n- As a benchmark, the paper does not adequately demonstrate how the benchmark scores correspond to the real-world capabilities of the evaluated models, especially given the heavy use of virtual environments and synthetic data."}, "questions": {"value": "- Can a model be trained? If it is possible to construct the data, then it should also be possible to use the data to train a model and to evaluate it on other benchmarks. If improvements can also be shown there, it would be very convincing.\n- Is it possible to conduct a real user study to assess whether the model is producing correct results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H4ux7dLuiW", "forum": "dYO3XS9Wsm", "replyto": "dYO3XS9Wsm", "signatures": ["ICLR.cc/2026/Conference/Submission5786/Reviewer_fC5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5786/Reviewer_fC5f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761271413247, "cdate": 1761271413247, "tmdate": 1762918261080, "mdate": 1762918261080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents FUSE, a scalable system for evaluating LLM agents by simulating various interactions between the agent, a user, and an environment.   The method checks how flexible agents are by developing a lot of \"archetypes\" for both persons, such as \"Impatient\" and \"Planner\", and places, such as \"Buggy\" and \"Adversarial\".  There are two new ways to measure how well an agent does its job. The first is the procedure alignment score, which checks how well the agent's action steps match up with a ground truth, and the Second is the outcome success score, which checks how close the agent's ultimate output matches with a ground truth.  The framework also incorporates a meta-evaluation phase to check how well it works.  In 3,600 runs of experiments, the most important determinant in an agent's success was the reliability of the environment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The FUSE architecture in this work facilitates controlled, easy-to-extend experimentation through the simulation of adjustable user and environmental archetypes.  The \"Phase 4: Meta-Evaluation\" is a substantial advancement, since it introduces a series of audits designed to quantitatively assess the simulation's fidelity, thereby addressing a critical deficiency in previous research. The paper introduces a novel \"Procedure Alignment Score,\" a Levenshtein-based metric that evaluates an agent's process against a ground truth, rather than only its outcome. This approach provides useful insights, including one of the important finding that procedural alignment and outcome success are only slightly related."}, "weaknesses": {"value": "1. The adherence scores for \"tricky\" user archetypes are extremely low. For example, Improviser with 0.31, Information Hider with 0.03. This indicates the simulated user LLM is not faithfully following its instructions, which undermines the validity of the experimental results for those specific archetypes. You may consider adjusting the prompt to overwrite the LLM “helper” default behavior, or try to simulate a larger number of times and collect the one with a threshold of over a certain level.\n\n2. The entire evaluation pipeline is built by LLMs. An LLM generates the Tool-Relationship Graph, an LLM generates the user goal and environment to fit a sampled path, and an LLM judges the final outcome. This \"sim-to-sim\" evaluation risks a departure from real-world user behavior and task generation, which is the very problem the paper aims to solve. Considering collecting real-world GitHub or Stack Overflow problems, or any problem from the real sources would make the work more reliable.\n\n3. The faithfulness of the user and environmental behaviors is not clear.  Does a 'Buggy' simulated environment really reflect common API failures? It is not validated against real-world data. The authors could analyze public API documentation and error logs to model realistic failure modes. Also, present more scenarios of the experiments, i.e. error analysis.\n\n4. The current domains and short task horizons limit the generalizability of the findings. I suggest adding more domains to improve the work's generalizability. Some good candidates would be e-commerce (managing a shopping cart), calendaring (resolving scheduling conflicts), or cloud infrastructure (provisioning resources), as these involve multi-step, state-dependent interactions."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ke3Z3VsWdF", "forum": "dYO3XS9Wsm", "replyto": "dYO3XS9Wsm", "signatures": ["ICLR.cc/2026/Conference/Submission5786/Reviewer_h2in"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5786/Reviewer_h2in"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842192224, "cdate": 1761842192224, "tmdate": 1762918259789, "mdate": 1762918259789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FUSE, a framework for evaluating LLM-based tool-use agents in simulated user–agent–environment loops. It automatically generates multi-step tasks from Model Context Protocol tools and assesses agents using two metrics: Procedure Alignment and Outcome Success. Experiments in filesystem, GitHub, and browser domains show that environment reliability and user behavior strongly affect performance, suggesting simulation fidelity is key for reliable agent benchmarking."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Overall, the paper presents a well-structured and systematic attempt to evaluate LLM-based tool-use agents under controlled and reproducible simulated settings. While not entirely novel conceptually, it combines multiple evaluation aspects—process alignment, outcome success, and simulation fidelity—into a unified framework that is technically coherent and empirically validated across several domains.\n\nProblem relevance: Addresses an important and timely question—how to rigorously benchmark LLM agents performing complex tool-use tasks.\n\nFramework design: The proposed FUSE system unifies user, agent, and environment simulation in a closed loop, offering configurability and reproducibility.\n\nDual evaluation metrics: The use of procedure alignment and outcome success captures both process correctness and goal completion, offering complementary insights."}, "weaknesses": {"value": "Missing comparison with existing multi-turn benchmarks: Well-known prior benchmarks such as BFCL, TauBench, and Tau2Bench already target similar multi-turn or tool-use scenarios. The paper does not provide conceptual or empirical comparisons with these, making it unclear what advantages FUSE actually offers.\n\nUnrealistic and oversimplified task generation: The task generation process based on MCP simulations is too simple to reflect real-world user–environment interactions. For example, GitHub and web tasks in FUSE are far less complex than realistic environments like WebArena, where even strong models still perform poorly; thus, the benchmark may overestimate real-world capability.\n\nLimited multi-turn depth: Although positioned as a multi-turn benchmark, FUSE’s longest trajectories contain only eight steps, insufficient to evaluate long-horizon reasoning or recovery behavior seen in realistic multi-round interactions. GPT-4.1 already attains very high scores on FUSE, suggesting the benchmark may not be sufficiently challenging for current top-tier models. This limits its usefulness for tracking future progress in multi-turn agent evaluation.\n\nOverreliance on LLM-as-Judge evaluation: The framework depends entirely on LLM-based judging without any ground-truth final states or verifiable outcomes, which risks bias and makes the reported results less reliable."}, "questions": {"value": "How does FUSE differ from or improve upon existing multi-turn benchmarks such as BFCL, TauBench, or Tau2Bench in terms of design, task diversity, or evaluation methodology?\n\nCan the authors include results for stronger proprietary models (e.g., GPT-5, Claude-4-Sonnet) and major open-source models (e.g., Qwen, GLM, LLaMA) to demonstrate broader benchmarking coverage?\n\nWhy was no ground-truth final state used to validate task completion, and how do the authors ensure that the LLM-as-Judge evaluations are unbiased and consistent?\n\nCan the framework support longer multi-turn trajectories (>8 steps), and if not, how might the authors extend it to capture long-horizon reasoning behaviors?\n\nSince FUSE aims to enable large-scale evaluation, what is the computational cost or efficiency compared with existing benchmarks, and can the authors report runtime or resource metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8tuZ8ULKT4", "forum": "dYO3XS9Wsm", "replyto": "dYO3XS9Wsm", "signatures": ["ICLR.cc/2026/Conference/Submission5786/Reviewer_zwEH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5786/Reviewer_zwEH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992369507, "cdate": 1761992369507, "tmdate": 1762918259035, "mdate": 1762918259035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}