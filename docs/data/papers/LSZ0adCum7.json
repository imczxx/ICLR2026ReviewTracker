{"id": "LSZ0adCum7", "number": 20323, "cdate": 1758304730560, "mdate": 1759896983854, "content": {"title": "CCR: A Continuous Composite Reward for Efficient Reinforcement Learning-Based Jailbreak Attacks", "abstract": "Jailbreak techniques for large language models (LLMs) have primarily relied on gradient-based optimization, which requires white-box access, and black-box evolutionary search, which suffers from slow convergence. In this work, we propose a reinforcement learning (RL) framework that formalizes jailbreak generation as a sequential decision-making problem, leveraging black-box model feedback to enable optimization without gradient access. The key to this framework is the Continuous Composite Reward (CCR), a task-oriented reward tailored for adversarial text generation. CCR provides dense feedback along two complementary dimensions: at the lexical level, it discourages refusal outputs and steers generation toward target responses; at the semantic level, it aligns outputs with multiple anchors to maintain topical relevance and format consistency. This design enables stable training under noisy black-box conditions and improves robustness to model updates. Consequently, the attack model transfers effectively across both open-source and API-served targets without model-specific finetuning. We also propose a stricter evaluation metric, ASR-G, which combines content-level matching with Llama Guard filtering to more reliably measure jailbreak success. On LLaMA-2, our method achieves attack success rates that exceed COLD-Attack and PAL by 17.64 and 50.07 percentage points, respectively. These results highlight the effectiveness and cross-model transferability of our approach under fully black-box conditions while reducing query costs.", "tldr": "We propose a reinforcement learning–based jailbreak attack with a continuous composite reward for stable black-box optimization.", "keywords": ["Reinforcement Learning", "Jailbreak Attack", "Black-box Adversarial Text Generation", "Continuous Composite Reward (CCR)", "ASR-G"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ca9e3f156ee21f665e81c4b6cbe90dbf4eddce9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Reinforcement Learning framework for black-box jailbreak attacks, centered on a novel reward function called CCR.\n\nThe authors argue that existing RL-based attacks suffer from unstable training due to sparse rewards (e.g., simple \"success/fail\"). To solve this, CCR provides a \"dense\" reward signal by combining three components:\n\n  * Token-level Refusal: Penalizes refusal words (e.g., \"I cannot...\") at the lexical level.\n\n  * Semantic Guard Probability: Uses a safety classifier (like Llama Guard) to ensure the output is genuinely unsafe, not just avoiding refusal templates.\n\n  * Multi-Anchor Semantic Alignment: Keeps the response semantically consistent with multiple known jailbreak \"anchors\" to maintain topical relevance.\n\n   * ASR-G Metric:The paper also introduces a stricter evaluation metric, ASR-G. While standard ASR only checks for refusal strings, ASR-G requires the response to also be classified as \"UNSAFE\" by a guard model , providing a more reliable measure of attack success.\n\nThe dense feedback from CCR leads to more stable training and higher attack success rates. Experiments show CCR outperforms strong baselines like COLD-Attack and PAL under black-box conditions on models like Llama-2. The method also demonstrates strong cross-model transferability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Originality:\n   * The paper's primary originality lies in its novel reward formulation (CCR) for RL-based jailbreaking . While using RL for attacks is not entirely new , the authors correctly identify sparse rewards as the key bottleneck.\n   * The design of a dense, continuous, and composite reward that integrates lexical refusal signals , semantic safety (via Llama Guard) , and multi-anchor semantic alignment is a creative and effective combination of existing concepts.\n   * The introduction of the ASR-G metric also adds originality, providing a stricter and more meaningful evaluation standard than traditional ASR .\n\n* Quality:\n   * The paper demonstrates high quality through rigorous and comprehensive experimentation.\n   * The authors compare their method against a strong and diverse set of baselines, including gradient-based (GCG), evolutionary (AutoDAN), and proxy-guided (PAL) methods.\n   * The evaluation is conducted across multiple open-source models (Vicuna, Llama-2, Mistral, Guanaco) and an API-served model (Deepseek-Chat) .\n   * The inclusion of a detailed ablation study (Table 3) clearly validates the contribution of each component of the CCR reward .\n   * The results are strong and consistently show the superiority of CCR, especially on the stricter ASR-G metric.\n* Clarity:\n   * The paper is exceptionally clear and well-written.\n   * The core problem (sparse rewards) is motivated effectively using a direct comparison plot (Figure 1a) .\n   * The proposed method and the CCR framework are explained logically and in detail, with a helpful overview in Figure 2.\n   * The distinction between ASR and the proposed ASR-G metric is clearly defined, justifying the need for the new metric .\n* Significance:\n   * This work is highly significant as it provides a powerful and practical framework for black-box jailbreaking, which is a more realistic threat scenario for most deployed LLMs.\n   * By developing a more effective attack, the paper provides a more robust evaluation tool for the community to benchmark and improve LLM safety defenses.\n   * The success against safety-aligned models like Llama-2  highlights persistent vulnerabilities and underscores the need for more advanced, adaptive defenses.\n   * The push for stricter metrics like ASR-G is an important contribution, moving the field beyond simple string matching to more meaningful evaluations of safety alignment."}, "weaknesses": {"value": "High Risk of Reward Hacking (Evaluator is the Reward Model):\n\n1. The primary weakness of this paper is the high risk of reward hacking. The proposed method uses a safety classifier (Llama Guard) as a core component of its Continuous Composite Reward (CCR) function, explicitly optimizing the attacker to produce outputs that Llama Guard deems \"UNSAFE\" .\n2. However, the paper's main evaluation metric, ASR-G, then uses the exact same Llama Guard model as the final judge of success ."}, "questions": {"value": "Q1. To demonstrate genuine effectiveness and avoid this confounding variable, the ASR-G evaluation must be performed using a held-out safety classifier that was not seen by the agent during the RL training process.\n\nCan the author change the evaluator to another model like WildGuard [1] or use LLM-as-Judge to evaluate the ASR-G metric in the evaluation phase?\n\nQ2. Can the author test whether their generated jailbreak prompts can surpass serveral jailbreak defenses, like [2] [3] ?\n\n\n[1] WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs.\n\n[2] Defending ChatGPT against jailbreak attack via self-reminders\n\n[3] Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no Ethics Concerns"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CCqflDp1P", "forum": "LSZ0adCum7", "replyto": "LSZ0adCum7", "signatures": ["ICLR.cc/2026/Conference/Submission20323/Reviewer_gbZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20323/Reviewer_gbZL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838009546, "cdate": 1761838009546, "tmdate": 1762933784826, "mdate": 1762933784826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an RL-based jailbreak framework with a novel reward design, Continuous Composite Reward, which addresses the sparse-reward problem in prior RL-based jailbreak approaches. CCR consists of three major components: a refusal-token suppression objective, a classification objective determined by a safety guardrail model, and a multi-anchoring semantic-alignment objective to ensure the generated content aligns with predefined targets. Using GRPO, CCR trains an attacker LLM to generate high-quality jailbreak suffixes that induce the target model to produce harmful content. Evaluation against 8 baseline jailbreak attacks shows the proposed method is effective and transferable across four open-source LLMs and one closed-source LLM API."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies a timely and important problem in AI safety.\n\n- The paper is well written and easy to follow.\n\n- The Continuous Composite Reward is well motivated, and the ablation study convincingly shows the contribution of each module."}, "weaknesses": {"value": "- The evaluation has shortcomings: it lacks comprehensive comparisons with several existing RL-based red-teaming frameworks and with more robust open-source and closed-source LLMs.\n\n- The paper omits important setup details for the proposed attack framework.\n\n- Lack of discussion of potential adaptive defenses."}, "questions": {"value": "1. The authors claim RL-based jailbreak methods are underdeveloped. However, there already exist a non-trivial number of works focusing on RL-based jailbreaks beyond RLbreaker (which is discussed in the introduction). See [1,2,3]. Notably, RLbreaker itself is not included in the evaluation, why was it omitted?\n\n\n2. I am confused by Figure 1a. What does the blue band for RLbreaker represent? Why is a similar band not shown for the proposed approach? This ties into a broader confusion about the experimental setup: does CCR fine-tune the attacker LLM with GRPO separately for each seed attack prompt, or is the RL process trained once and applied to all seed prompts? From the current text I infer the former (per-prompt fine-tuning). If so, the computational cost could be large,  for each seed prompt CCR would need to optimize the attacker LLM via GRPO. The paper does not report this overhead, so the authors’ claim of “better efficiency” is hard to evaluate or accept.\n\n\n3. All five evaluated victim LLMs are outdated. High ASR on such weaker models does not necessarily reflect real progress in AI safety. Rather than showing marginal improvements over baselines on weak models, I encourage the authors to evaluate CCR on more recently aligned and stronger models, both open-source and closed-source. For example GPT-OSS-20B / GPT-OSS-120B, models using stronger alignment techniques (e.g., Deliberative alignment[4], Circuit Breaker [5]), or recent closed-source systems such as GPT-5 and Claude 4.\n\n\n4. There is no discussion of adaptive defenses. A straightforward adaptive defense is to equip the victim LLM with the same or a similar guardrail/classifier used in the attack. If the success criterion is defined relative to that guardrail, the defender could trivially detect or block the attack. The authors should discuss this limitation and, if possible, evaluate robustness against such adaptive defenses.\n\n\n---\nReference \n---\n\n[1] Hong, Zhang-Wei, et al. \"Curiosity-driven red-teaming for large language models.\" arXiv preprint arXiv:2402.19464 (2024).\n\n[2] Lochab, Anamika, et al. \"VERA: Variational Inference Framework for Jailbreaking Large Language Models.\" arXiv preprint arXiv:2506.22666 (2025).\n\n[3] Jha P, Arora A, Ganesh V. Llmstinger: Jailbreaking llms using rl fine-tuned llms[J]. arXiv preprint arXiv:2411.08862, 2024.\n\n[4] Agarwal S, Ahmad L, Ai J, et al. gpt-oss-120b & gpt-oss-20b model card[J]. arXiv preprint arXiv:2508.10925, 2025.\n\n\n[5] Zou A, Phan L, Wang J, et al. Improving alignment and robustness with circuit breakers[J]. Advances in Neural Information Processing Systems, 2024, 37: 83345-83373."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WdCcpId27G", "forum": "LSZ0adCum7", "replyto": "LSZ0adCum7", "signatures": ["ICLR.cc/2026/Conference/Submission20323/Reviewer_YxAo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20323/Reviewer_YxAo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973652857, "cdate": 1761973652857, "tmdate": 1762933784480, "mdate": 1762933784480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning framework for black-box jailbreak attacks on LLMs. The key contribution is the Continuous Composite Reward (CCR), which integrates token-level refusal probability, semantic guard scores, and multi-anchor alignment to provide dense feedback signals. The method employs GRPO to train an attacker model that generates adversarial suffixes. Experiments on multiple LLMs (Vicuna, Llama-2, Mistral, Guanaco) demonstrate improved attack success rates compared to gradient-based and evolutionary search baselines, while maintaining better transferability and linguistic fluency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and timely topic in LLM security research.\n\n2. The proposed Continuous Composite Reward (CCR) offers a more comprehensive reward function for RL-based jailbreak attacks."}, "weaknesses": {"value": "1. Inconsistent baseline descriptions The baseline section mentions \"GCG\" twice, but examination of the references reveals these refer to the same work (duplicate citation entries).\n\n2. Incomplete characterization of attack success evaluation and overclaimed contributions The limitations of refusal-based attack success evaluation are now widely recognized in the community. Current mainstream jailbreak evaluation methodologies employ LLM-as-a-judge approaches (e.g., GPT-4)[1] or fine-tuned specialized classifiers[2] to assess output harmfulness. The authors fail to discuss these established evaluation paradigms, and consequently, ASR-G cannot be presented as a novel contribution. I recommend the authors incorporate GPT-4-as-a-judge or similar methods in their experimental evaluation.\n\n3. Undefined baseline method Table 1 includes a method labeled \"ral\" without prior introduction or explanation in the text.\n\n4. Insufficient coverage of related work The paper omits discussion of recent similar approaches that utilize LLMs to generate jailbreak suffixes[3,4,5, including RL-based methods. A comparative analysis with contemporary RL-based jailbreakers is necessary.\n\n5. Lack of experimental fairness analysis The authors do not discuss the parameter configurations across different baselines—particularly whether equivalent attack budgets (e.g., iteration counts) were allocated to ensure fair comparison. This is critical for interpreting experimental results.\n\n6. Missing efficiency and cost analysis The paper focuses solely on attack effectiveness while omitting discussion of attack efficiency and computational cost. Given that the proposed method relies on RL training, the associated costs may be substantial and warrant explicit analysis.\n\n\n[1]Jailbreaking black box large language models in twenty queries\n\n[2]GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts\n\n[3]AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\n\n[4]LLM Stinger: Jailbreaking LLMs Using RL Fine-Tuned LLMs \n\n[5]An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box Jailbreaking without Affirmative Phrases via LLM as Optimizer"}, "questions": {"value": "What are the attack efficiency and computational cost of the proposed method compared to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2paqYrDpAj", "forum": "LSZ0adCum7", "replyto": "LSZ0adCum7", "signatures": ["ICLR.cc/2026/Conference/Submission20323/Reviewer_5Vac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20323/Reviewer_5Vac"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976590933, "cdate": 1761976590933, "tmdate": 1762933783381, "mdate": 1762933783381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CCR (Continuous Composite Reward) for black-box, RL-based jailbreak generation. The attacker is a public LLM trained with GRPO to emit full adversarial suffixes, using a composite reward with three terms: (i) token-level refusal propensity (early-token penalty via a refusal lexicon), (ii) guard-unsafe probability from a safety classifier (e.g., Llama Guard), and (iii) multi-anchor semantic alignment that pulls outputs toward prior successful “anchors” while discouraging collapse via a kernel term (optionally after PCA). The paper also proposes ASR-G, a stricter success metric combining substring-based non-refusal with a guard “UNSAFE” judgment. Experiments on Vicuna-7B, Llama-2-7B-Chat, Mistral-7B, and Guanaco-7B show higher ASR-G and lower PPL than baselines; e.g., on Llama-2 the approach improves RLbreaker-style ASR from 71% to 87% (reward curves in Fig. 1a, p. 2) and achieves 76.47% ASR (Table 1, p. 7). Cross-model transfer and ablations (Tables 2–3, p. 8) support each reward component’s value. Qualitative examples (Fig. 4, p. 9) show more fluent adversarial suffixes than GCG/AutoDAN."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tDense reward design stabilizes RL training and improves convergence vs binary rewards (clear in Fig. 1a, p. 2).\n\t•\tComprehensive evaluation across four open-source targets with ASR-G and PPL; strong transfer performance (Table 2, p. 8).\n\t•\tAblation clarity: each component contributes; the full CCR+PCA stack yields best ASR-G (96% on Guanaco-7B, Table 3, p. 8).\n\t•\tFluency: lower PPL than gradient-based baselines at similar or better success (Fig. 3, p. 7).\n\t•\tMethod practicality: GRPO without a learned critic, single-shot suffix generation, and black-box-only feedback suit real-world red-teaming."}, "weaknesses": {"value": "•\tGuard-dependence & potential reward hacking. Using Llama Guard both as reward and metric component risks training to the evaluator rather than the underlying safety objective. Demonstrating robustness across multiple guards (or ensemble/consensus) would mitigate this.\n\t•\tLimited detail on key knobs.\n\t•\tRefusal lexicon creation/coverage and early-token decay schedule (Eq. 5) are not fully specified; sensitivity analyses are missing.\n\t•\tMulti-anchor term (Eq. 6): anchor selection pipeline, encoder choice, PCA dimension, σ and λ_heat are under-explained; failure cases are not discussed.\n\t•\tQuery efficiency not quantified. Relative “efficiency” is shown (Fig. 1b), but absolute queries-per-success and budget constraints per method/target are not tabulated.\n\t•\tEvaluation breadth. Only one API model (Deepseek-Chat) is used; results show very low ASR-G under system prompts (Table 4), leaving external validity open.\n\t•\tMinor presentation issues. Typos/labeling (e.g., “ral” in Table 1) and some formatting glitches reduce polish."}, "questions": {"value": "1.\tGuard coupling: Which Llama Guard version, thresholds, and prompts are used in training vs evaluation? Have you tested with alternate guards (e.g., different safety classifiers or rule-based filters) to evaluate overfitting or reward hacking?\n\t2.\tRefusal lexicon: How was V_refuse constructed (source, size, coverage) and how sensitive are results to lexicon variants? Please include ablations on decay schedule w_u and lexicon size.\n\t3.\tAnchor pipeline: How are anchors curated and updated? Are they derived from successful CCR runs on the same target (risk of leakage) or from external corpora? What is K, encoder f(·), PCA dimension, and kernel σ/λ_heat; can you provide sensitivity plots?\n\t4.\tQuery efficiency: For each target/baseline, what are (median, IQR) queries per successful jailbreak, and total queries per 50 prompts? This would make the “efficiency” axis in Fig. 1b concrete.\n\t5.\tGeneralization: Can you report cross-guard ASR-G, cross-prompt templates (different system prompts), and adversarial training on the target to test robustness?\n\t6.\tPPL definition: Is perplexity computed on the adversarial suffix, the full prompt, or the target output? Please clarify tokenization and corpus baseline.\n\t7.\tDeepseek-Chat: Why does ASR-G collapse to 3% when system prompts are enabled (Table 4)? Is this due to different safety coupling or the prompt hiding? Any analysis on failure modes?\n\t8.\tRelease considerations: If code is released, what safeguards (e.g., redacted prompts, rate limits, checklists) will you include to reduce dual-use risks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vGViwAjpGO", "forum": "LSZ0adCum7", "replyto": "LSZ0adCum7", "signatures": ["ICLR.cc/2026/Conference/Submission20323/Reviewer_KyPx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20323/Reviewer_KyPx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186994794, "cdate": 1762186994794, "tmdate": 1762933783063, "mdate": 1762933783063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}