{"id": "9664No4ulo", "number": 973, "cdate": 1756825950127, "mdate": 1759898233239, "content": {"title": "Ranking is Reward: Intra-Group Preference Ranking for Group Relative Policy Optimization", "abstract": "Recent breakthroughs in Large Language Model (LLM) reasoning have been driven by reinforcement learning techniques like PPO and GRPO. However, in Reinforcement Learning with Verifiable Rewards (RLVR), sparse rewards hinder learning when group samples receive identical scores. While existing methods attempt to address this with data filtering, they inadvertently limit progress on correctly answered prompts. Additionally, reward models based on absolute numerical scores often suffer from range instability, undermining training stability. To address these issues, we introduce intra-group response preference ranking as a reward signal. We propose the Ranking Reward Model (RRM), a novel paradigm aligned with GRPO, which outputs relative preference rankings for multiple responses to a single prompt. RankGRPO incorporates three strategies to leverage these rankings, mitigating vanishing gradients and instability from absolute scoring. Experiments show that RankGRPO improves performance across RLVR benchmarks, open-ended tasks, and reward model evaluations. RRM, trained with limited data, outperforms traditional numerical reward models trained on larger datasets, demonstrating the potential of RankGRPO and the RRM paradigm. Our source code is available at https://anonymous.4open.science/r/RankGRPO-0542.", "tldr": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Group Relative Preference Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c369c93545515ff46ff99e32fd3d04d6979f2b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a new variant of GRPO to be used for RL training of LLMs. The key proposal is that instead of simply using the reward, we also use the intra-group ranking as a way of augmenting the reward signal. Experimental validation shows a modest improvement on standard benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "I like the overall idea - relative ordering of answers can still provide a useful signal in GRPO-style algorithms. The paper is well-written in terms of formatting and overall flow."}, "weaknesses": {"value": "1. There are no confidence intervals in any tables. This is a major issue, as the difference between 52.1 and 51.8 could be a meaningful (if small) improvement, or it could be completely random noise.\n2. The procedure of training an RRM is not sufficiently explained, and seemingly only detailed in Figure 2?\n3. The explanation of the proposed method is unclear. For example, Section 4.1 - what is “score”? Is it meant to be the reward? Advantage? Also $r_i$ is not defined, from the context I'm guessing it's the responses' ranking within the group, as evaluated by the RRM?\n4. RRM - are we just distilling from the model’s token-based predictions?\n5. What’s a “rule-based reward model”? Is it just a verifier? Or is it actually a learned reward model?\n6. Section 4.3 - seems to imply that the final ranking is just correctness + length? I was hoping step 6 in Algorithm 1 would clarify this, but it remains vague\n7. Section 4.2 - authors claim to propose a novel paradigm of RRM, but do not elaborate on how it works. A figure is not sufficient.\n8. Experiments are only done on DeepSeek Distill models, which is a big limitation, and contamination is unknown.\n\n\nNitpicks:\n1. Line 129/130 - very awkward phrasing, \"direct parameter assignment\"\n2. Figure 1 is unclear. For example, looking at Fig. 1b, I’m thinking “What is it measuring? It it better high or low? Stable or unstable?”"}, "questions": {"value": "Questions/suggestions:\n1. Improve the clarity of writing - there's a lot of under-described assumptions on how the RRM is trained or what the final rewards/advantages are.\n2. Add confidence intervals. It seems like the only major difference in results between RankGRPO and baselines is on the \"Logic Reasoning\" benchmarks, but it's hard to tell right now - a discussion of this would be valuable.\n3. What is the computational cost of this new method, compared to the baselines?\n\nI'll increase my rating if the main points (clarity of writing, and confidence intervals) are sufficiently addressed"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HriwW2DfvZ", "forum": "9664No4ulo", "replyto": "9664No4ulo", "signatures": ["ICLR.cc/2026/Conference/Submission973/Reviewer_cfs1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission973/Reviewer_cfs1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300638296, "cdate": 1761300638296, "tmdate": 1762915651645, "mdate": 1762915651645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies reinforcement learning with verifiable rewards in large language\nmodels and proposes a new reward assignment method based on ranking. The authors observe\nthat a popular algorithm, GRPO, provides little to no learning signal when reward scores lack\nvariability within a batch during stochastic optimization. To address this, they propose ranking\nthe examples according to a ranking reward model and incorporating this ranking into the\nadvantage calculation of the GRPO loss. Experiments explore several variations of the core\nidea across mathematical reasoning domains and more open-ended tasks such as creative and\ntechnical writing. The authors report consistent performance gains over GRPO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a natural yet previously unexplored direction: ranking-based rewards in\nRLVR. The authors introduce several variations of the core method, which they later\nsystematically ablate in the experiments, and analyze the impact of dataset difficulty on their\nfindings. The experiments cover multiple model configurations."}, "weaknesses": {"value": "Unfortunately, the empirical results do not appear strong enough—or statistically significant—to\nfully support the paper’s main claims. In particular, most experiments seem to be based on\nsingle-seed runs. Moreover, the results reported in Tables 1 and 2 do not appear statistically\nsignificant, as the performance differences between methods are often only a few percentage\npoints. It would strengthen the work if the authors demonstrated consistent improvements over\nthe baselines across multiple random seeds."}, "questions": {"value": "1. It is unclear what Fig. 1(a) shows. How exactly do you define what an effective sample is?\n2. Suggestion: You should mention that you provide rankings from a ranking model in line 155. (I know that you explain this later in Section 4.2, but it should be mentioned already in line 155; otherwise, it is confusing.)\n3. “As illustrated in Figure 2, RRM ranks nn input responses, where each class corresponds to a permutation order.” Does this mean that the output dimension of the ranking model grows exponentially with the group size of GRPO? This sounds very inefficient.\n4. Line 248 appears to need some rewriting—it is currently unclear.\n5. Line 262: $r_{\\phi}$ and $r_{\\psi}$ should be capitalized.\n6. Fig. 3a: it is unclear what resources the models used (e.g., how much data, compute, etc.)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YqwQNH3Cjy", "forum": "9664No4ulo", "replyto": "9664No4ulo", "signatures": ["ICLR.cc/2026/Conference/Submission973/Reviewer_y55s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission973/Reviewer_y55s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560377130, "cdate": 1761560377130, "tmdate": 1762915651443, "mdate": 1762915651443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RankGRPO, an enhancement for Group Relative Policy Optimization (GRPO) algorithm. The methodology is designed to tackle two primary issues in RLHF: 1) the \"sparse reward\" problem in Reinforcement Learning with Verifiable Rewards (RLVR), where groups of samples with identical scores (e.g., all correct) produce zero variance and thus no learning signal , and 2) the \"range instability\" of traditional scalar reward models (SRMs). The proposed solution involves a Ranking Reward Model (RRM), which outputs a relative preference ranking for multiple responses , and three strategies (Ranking as Weight, Supplement, or Reward) to integrate this rank information into the GRPO advantage calculation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes a Solution to the \"Sparse Reward\" Problem in GRPO\n2. Addresses Reward Signal Instability\n3. Demonstrates Improved Performance Across Diverse Tasks"}, "weaknesses": {"value": "1. The paper claims that as GRPO training progresses, more sample groups become \"unanimously correct,\" which causes zero variance and reduces the number of “effective training samples. Figure 1a visualizes this by showing the ratio of effective prompts for GRPO dropping to around 40%.. However, this key motivational claim is contradicted by the paper’s own later analyses in Figures 5. In Figures 5, they showed that the effective prompt ratio for GRPO on easy, medium, and hard datasets actually remains stable (rather than diminishing).\n2. The RankGRPO framework is not a straightforward substitution of scores with ranks but a complex, multi-stage, heuristic-driven pipeline that covers the true source of its gains. Before any \"Ranking as X\" strategy is applied, the paper adds a \"Re-ranking\" phase that sorts responses by correctness, length, and an RRM-based quality rank. The justification for the length-based heuristic is weak, as the ablation study in Table 3 shows that removing it actually improves results across multiple benchmarks. Finally, the paper presents three non-equivalent ranking strategies (W, S, R), with different ones performing best for different tasks, which lack a unified and principled approach.\n3. The paper introduces the RRM as a \"novel paradigm\" that outputs relative preference rankings, but this is essentially what standard RLHF preference models have done for years using pairwise or k-wise comparisons. While the authors frame RRM as an alternative to scalar reward models (SRMs), the more appropriate comparison is to modern preference-based models such as those used in DPO. Ultimately, the paper does not explain how RRM is fundamentally different from a conventional preference model trained to rank multiple outputs."}, "questions": {"value": "1. The RRM training process (Section 4.2) seems to be unclear. It states responses are scored via a \"rule-based reward model\" and an \"LLM to rank their relative quality\". How are these two signals combined to create the training data? Is the LLM-as-judge only used when the rule-based scores are tied?\n2. Figure 1b shows that \"SRM w/ Ranking\" is stable, unlike the \"SRM\" line. This implies a simple solution exists: just convert an existing SRM's scores to ranks. Why is a new RRM needed if a standard SRM's ranks are already stable? Can you compare this \"SRM-Rank\" baseline directly against the RRM?\n3. A key claim is that RankGRPO learns from \"all-correct\" groups. In such a group, what is the RRM actually ranking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b3tfEtajcR", "forum": "9664No4ulo", "replyto": "9664No4ulo", "signatures": ["ICLR.cc/2026/Conference/Submission973/Reviewer_RsH8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission973/Reviewer_RsH8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978733460, "cdate": 1761978733460, "tmdate": 1762915651080, "mdate": 1762915651080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce RankGRPO, a framework in which a relative reward model (RRM) ranks responses in a group from which the rank is incorporated into the final reward used in advantage estimation within GRPO. In cases where only a sparse binary reward is available, the use of the RRM can provide more granular signal on the \"better\" responses. The RRM is trained on a dataset of rankings synthetically generated from an LLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The experimental results are extensive and thorough, including ablations\n- The paper is well-written and the figures are instructive"}, "weaknesses": {"value": "- There is a significant overhead in having to train the RRM - especially if it needs to be separately trained for each and every RL environment which would limit it's practical use (though whether this is true or not wasn't clear to me)\n- The exact training procedure (e.g., loss function) for the RRM was unclear"}, "questions": {"value": "- Please address the weaknesses\n- Does RankGRPO not apply in reward settings which are not correctness-based but are continuous (i.e., non-binary)?\n- 3 variants of RankGRPO are provided (weight, supplement, reward) but it is not clear to me if one is universally better than the others? If not, are there certain scenarios where one variant is better than the other?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "je3DHSBWP7", "forum": "9664No4ulo", "replyto": "9664No4ulo", "signatures": ["ICLR.cc/2026/Conference/Submission973/Reviewer_isFw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission973/Reviewer_isFw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157139243, "cdate": 1762157139243, "tmdate": 1762915650849, "mdate": 1762915650849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}