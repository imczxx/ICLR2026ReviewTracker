{"id": "8Zt6OsDzij", "number": 11017, "cdate": 1758187008521, "mdate": 1759897614326, "content": {"title": "SpikeLoRA: Learnable Activation Sparsity for Low-Rank Adaptation using Spiking Neural Networks", "abstract": "Low-rank adaptation (LoRA) is a fine-tuning method that freezes the parameters of a pre-trained model and injects small trainable matrices. LoRA-based methods focus on parameter-level efficiency, but do not directly control the activations in the low-rank space. We introduce SpikeLoRA, a spiking low-rank adaptation fine-tuning method that leverages the leaky integrate-and-fire (LIF) neuron to introduce learnable sparsity with minimal computational overhead. The LIF neuron gates the activations from the $A$-matrix in LoRA, sparsifying them while preserving learned information. This design makes SpikeLoRA a sparse fine-tuning method for both spiking and traditional LLMs, with the additional efficiency benefit of being compatible with neuromorphic hardware. Our experiments show that over 70\\% sparsity is achievable without a significant drop in performance. Further, improved performance as compared to LoRA is observed for smaller datasets and higher-rank settings. We also show that SpikeLoRA indirectly mitigates overfitting, particularly for higher ranks.", "tldr": "SpikeLoRA is a PEFT method that makes use of leaky integrate-and-fire (LIF) neurons to gate LoRA's A-matrix activations, which promotes high sparsity in the low-rank space and mitigates overfitting for both spiking and traditional LLMs.", "keywords": ["spiking neural networks", "low-rank adaptation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/434a11099fe9cfc25d983b1eadab65a43639a47e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SpikeLoRA, a new fine-tuning technique that combines Low-Rank Adaptation (LoRA) with spiking neural network (SNN) dynamics to introduce learnable activation sparsity. Instead of updating only low-rank weight matrices, SpikeLoRA integrates a Leaky Integrate-and-Fire (LIF) neuron that gates activations in the LoRA adapter, allowing the model to learn when to activate or suppress updates. This design aims to improve efficiency, reduce overfitting, and make LoRA-compatible models more suitable for neuromorphic hardware. The authors evaluate SpikeLoRA mainly on DeBERTaV3-Base using the GLUE benchmark—especially CoLA—and report over 70–90% activation sparsity with comparable or slightly improved performance relative to LoRA. The paper also presents limited results on SpikeGPT, showing that the method can operate in both traditional and spiking model pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an original and well-executed idea by integrating Low-Rank Adaptation (LoRA) with spiking neural dynamics through a Leaky Integrate-and-Fire (LIF) neuron, creating a learnable activation sparsity mechanism. This cross-disciplinary combination is novel, technically sound, and clearly explained, with clean mathematical formulation and solid motivation. The experiments, though limited, show that SpikeLoRA can achieve high activation sparsity without significant loss in accuracy, suggesting potential regularization benefits. The paper is clearly written, easy to follow, and introduces a new perspective on efficiency—shifting from parameter-level to activation-level sparsity—which could have long-term significance for energy-efficient and neuromorphic fine-tuning research."}, "weaknesses": {"value": "The paper’s main weakness is the limited experimental validation. Almost all results come from the CoLA dataset (Table 1–3), which is small and unbalanced. This makes it hard to judge if SpikeLoRA would generalize to larger or more diverse tasks. The full GLUE results in Table 3 are single runs, with no hyperparameter sweeps or statistical tests. There is no evidence that the gains hold beyond CoLA.\n\nThe authors show a large improvement at r = 64 in Table 1, but they do not explore this effect further. High-rank settings are rarely used in practice; the result looks interesting but anecdotal. The claim that SpikeLoRA “mitigates overfitting” (Section 4.2.2) is only supported by smaller generalization gaps. This is a weak proxy for real generalization. The paper does not show validation curves, variance across seeds, or ablations removing the LIF gate to prove causality.\n\nComparisons are also narrow. The only baseline is vanilla LoRA. Well-known LoRA extensions—AdaLoRA (Zhang 2023), DoRA (Liu 2024), QLoRA (Dettmers 2023), and LoRA Dropout (Lin 2024)—are not included, though several already address efficiency and regularization. Without these baselines, it is unclear if the benefit comes from the spiking mechanism or simply from added gating noise.\n\nSpikeLoRA is tested on DeBERTaV3-Base and a small SpikeGPT task. Both are valid for proof-of-concept but too narrow for a method claiming broad applicability. There is no test on larger transformers (e.g., LLaMA, T5) or other modalities such as vision.\n\nFinally, the claimed efficiency advantage is theoretical. Training time actually increases (Section 4.2.3), and no hardware or FLOP analysis is given. The paper would be stronger with runtime, energy, or sparsity-vs-accuracy trade-off plots.\n\nOverall, the idea is novel but the evidence is thin. More models, baselines, and diagnostic experiments are needed to show that SpikeLoRA provides a consistent and meaningful improvement over existing LoRA variants."}, "questions": {"value": "Most experiments rely on CoLA. Can you include results on larger or more diverse datasets to demonstrate that SpikeLoRA generalizes beyond a small benchmark? The performance differences on CoLA are marginal, which raises concerns about whether it is an appropriate dataset to support your claims.\n\nIn Table 1, SpikeLoRA shows a sharp performance gain at r = 64. Why does this occur, and is the effect consistent across datasets or random seeds?\n\nCan you provide an ablation that removes or replaces the LIF gate to prove that the improvement comes from the spiking dynamics themselves rather than incidental gating noise?\n\nSection 4.2.3 reports longer training time. Do you have runtime, FLOP, or hardware-level efficiency results to support the claim that SpikeLoRA is computationally efficient?\n\nHave you evaluated SpikeLoRA on models beyond DeBERTaV3 and SpikeGPT, especially those that natively use LoRA (e.g., T5, LLaMA)? Results on such architectures would provide stronger evidence of generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GGaZg0RaKA", "forum": "8Zt6OsDzij", "replyto": "8Zt6OsDzij", "signatures": ["ICLR.cc/2026/Conference/Submission11017/Reviewer_qCYf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11017/Reviewer_qCYf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877245775, "cdate": 1761877245775, "tmdate": 1762922197924, "mdate": 1762922197924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SpikeLoRA, a PEFT method that aims to improve LoRA by introducing activation sparsity through spiking neurons. The method inserts a LIF neuron between the $A$ and $B$ matrices of the LoRA decomposition. The binary output of this LIF neuron is used as a gating mask, applied via element-wise multiplication to the activations from matrix $A$, thereby achieving learnable sparsity. \n\nThe authors claim this method can achieve high activation sparsity (>70%) without significant performance degradation, serves as a regularization technique to mitigate overfitting, and offers compatibility with neuromorphic hardware."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.  Reasonable Motivation for LoRA Regularization: The motivation to introduce learnable activation sparsity into PEFT as a form of regularization, specifically to combat overfitting in high-rank LoRA settings, is acceptable.\n2.  Demonstrated Regularization Effect: The experiments (e.g., the generalization gap analysis in Table 3, Appendix A) provide some evidence that SpikeLoRA can offer a better regularization effect than standard LoRA, even when the latter uses Dropout."}, "weaknesses": {"value": "1.  Misplaced Application and Limited Contribution: The paper's core contribution appears limited and ill-timed for both of its target communities:\n    * For the SNN Community: Given that both Spiking LLMs and neuromorphic hardware are **far from mature**, introducing a PEFT technique like LoRA at this stage with SNN properties is premature and offers little immediate value.\n    * For the ANN Community: When stripped of its \"potential efficiency on future hardware\" argument, the method degrades into a complex (using LIF) and inefficient (non-mergeable at inference) mechanism JUST for sparse regularization. This is largely unnecessary, as simpler, more efficient, and well-established ANN-native regularization techniques (e.g., Dropout, or other learnable ANN Heaviside-like functions) already exist or can be explored.\n2.  Poorly Motivated Mechanism: Similar to the prior, the justification for using a LIF neuron as the gating mechanism is weak. Intuitively, I did not see any compelling reason why the neural dynamics of SNN and the LoRA task should be combined; Empirically, I also did not see evidence showing that the temporal dynamics of the LIF are playing a role. It functions merely as a complex hard gate with a fixed threshold $V_\\theta$ and a surrogate gradient, failing to justify why this \"bio-inspired\" mechanism is superior to a simpler, less computationally expensive **ANN learnable gate** (e.g., a thresholded ReLU or Sigmoid).\n3.  Efficiency Shortages:\n    * Training Overhead: The authors acknowledge (Sec 4.2.3) that SpikeLoRA is **slower** to train (by ~11%) than standard LoRA on GPUs, due to extra computations and the inability to leverage sparse operations in dense libraries.\n    * Inference Overhead: This is the most critical flaw. Unlike standard LoRA (whose $\\Delta W$ can be merged into $W_0$ to eliminate all inference overhead, SpikeLoRA's non-linear LIF gate makes merging impossible (especially LIF itself can only perform serial calculations). This means it *permanently* adds inference latency and computational cost.\n4.  Insignificant Performance Gains: The experimental results do not justify the added complexity. On the GLUE benchmark (Table 3), the average performance of SpikeLoRA is **nearly identical** to vanilla LoRA.\n5.  Unfair Baseline Comparison: As the main claimed benefit is regularization, the most direct baseline is Dropout. However, the LoRA baseline in the main tables uses `dropout=0`, meaning SpikeLoRA (with added complexity) was only compared to vanilla LoRA instead of other regularization methods, which is an unfair comparison."}, "questions": {"value": "1.  Necessity of LIF and Effect of Neural Dynamics of SNN: Does SpikeLoRA actually utilize the temporal dynamics of the LIF neuron? If not, please justify why a simpler learnable gating function (e.g., a surrogate gradient for $g(x) = \\text{ReLU}(x - \\theta)$) was not used instead.\n2.  Quantification of Inference Overhead: Please provide a concrete comparison of the inference overhead (in FLOPs and wall-clock time) between SpikeLoRA (which cannot be merged) and a standard LoRA model *after* its weights have been merged.\n3.  Comparison with Optimal Dropout: Please provide a direct performance comparison on the GLUE benchmark between SpikeLoRA (e.g., `dropout=0`, $V_\\theta=0.1$) and standard LoRA configured with its *optimal* dropout rate (e.g., 0.05 or 0.075 from Appendix A).\n4.  $V_\\theta$ Selection: Why is the threshold $V_\\theta$ a fixed hyperparameter? Was making it a learnable parameter, or adapting it per-layer, ever attempted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XOGBHuhcJr", "forum": "8Zt6OsDzij", "replyto": "8Zt6OsDzij", "signatures": ["ICLR.cc/2026/Conference/Submission11017/Reviewer_CH7W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11017/Reviewer_CH7W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970284701, "cdate": 1761970284701, "tmdate": 1762922197530, "mdate": 1762922197530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpikeLoRA, a spiking neural network-inspired extension to Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning of large language models. SpikeLoRA introduces the leaky integrate-and-fire (LIF) neuron as a learnable, biologically inspired activation gate within the low-rank space of LoRA modules, aiming to induce sparsity in activations while preserving information. The approach is empirically evaluated across standard language model benchmarks, with analyses of sparsity, efficiency, regularization, and overfitting mitigation, and includes comparative experiments on both traditional and spiking LLMs such as DeBERTaV3-Base and SpikeGPT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "s1:Integrating LIF neurons directly into the LoRA pipeline introduces a learnable mechanism for controlling activation sparsity. This is a novel perspective compared with existing parameter-level or dropout-based sparsification approaches"}, "weaknesses": {"value": "w1: The paper hypothesizes that SpikeLoRA amplifies salient features and suppresses noise but lacks a formal analysis connecting LIF-based sparsity to representational power or generalization. The operational regime of the LIF neuron and the risks of over- or under-sparsification are explored only empirically (see Section 3, Section 4.1.1, Figure 2). A probabilistic or theoretical characterization of this trade-off would strengthen the work.\n\nw2: The paper does not test whether the LIF neuron itself is responsible for the improvements. Comparisons to simpler gating methods (e.g., hard thresholds or surrogate non-biological gates) are missing, leaving unclear whether the benefits stem from the spiking dynamics or from general sparsity enforcement."}, "questions": {"value": "q1: Do you have evidence—analytical or empirical—of real energy or speed gains on neuromorphic or low-power hardware compared to LoRA and other sparsification methods?\n\nq2: What happens if the LIF neuron is replaced with simpler gating mechanisms (e.g., hard-threshold or continuous surrogates)? Are the gains biologically unique or general to sparse gating?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZNFRlwrEHY", "forum": "8Zt6OsDzij", "replyto": "8Zt6OsDzij", "signatures": ["ICLR.cc/2026/Conference/Submission11017/Reviewer_N1ZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11017/Reviewer_N1ZU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972611535, "cdate": 1761972611535, "tmdate": 1762922196603, "mdate": 1762922196603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a LoRA-based fine-tuning method that incorporates learnable spiking-neuron gates into the low-rank adaptation path. Instead of updating weights densely, SpikeLoRA uses a Leaky-Integrate-and-Fire gating mechanism to sparsify LoRA activations, enabling both parameter-efficient and activation-efficient tuning without modifying base weights. Experiments show that this spiking gate can learn task-relevant sparse patterns, achieving over 70% activation sparsity while maintaining performance, and sometimes even improving accuracy in low-data or high-rank settings by reducing overfitting. The method also extends naturally to spiking language models like SpikeGPT, demonstrating compatibility with neuromorphic compute and highlighting energy-efficiency potential."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a new integration of spiking neuron gating into LoRA, creatively bringing neuromorphic concepts into mainstream LLM fine-tuning. The introduction of learnable LIF gates to induce adaptive activation sparsity in low-rank updates is both unique and meaningfully expands the landscape of parameter-efficient tuning. The architecture, gating dynamics, and training procedure are clearly described, supported by helpful diagrams comparing standard LoRA and SpikeLoRA. The motivation around energy efficiency and biological inspiration is articulated clearly and remains balanced without overstatement."}, "weaknesses": {"value": "- The motivation for combining spiking neurons with LoRA is not fully convincing from a practical perspective. While biological inspiration and neuromorphic alignment are emphasized, the paper does not clearly articulate a strong need for spiking gating in mainstream LLM fine-tuning. For example, existing sparsity-inducing PEFT methods (e.g., sparse/structured LoRA variants, gating via ReLU/Hard-Concrete, learned token- or head-level sparsity) already enable activation reduction without introducing spiking dynamics. A clearer argument for why spiking-based gating is fundamentally preferable can be helpful.\n\n- The energy-efficiency motivation remains largely conceptual. The experiments focus on sparsity and accuracy, but do not provide direct measurements of energy savings (e.g., FLOPs, inference energy estimates, wall-power measurements on standard hardware, or neuromorphic deployment benchmarks).\n\n- The applicability to real neuromorphic systems is asserted but not fully demonstrated. Although SpikeGPT experiments show compatibility in principle, the paper lacks an end-to-end demonstration on neuromorphic hardware or a quantitative comparison to alternative energy-efficient architectures (e.g., binarized activations, event-driven RNNs)."}, "questions": {"value": "- Is the spiking gate applied uniformly across all LoRA modules optimal? Could selective placement (e.g., only in attention projections or specific layers) yield similar sparsity with lower complexity?\n\n\n- Can the authors comment on the scalability of SpikeLoRA to larger models (e.g., >7B parameters)? Since neuromorphic motivations matter most at larger scales, empirical or theoretical discussion on scaling behavior would be valuable.\n\n- How sensitive is the method to the LIF parameters (leak constant, threshold, reset function)? The paper notes learnability, but ablations or analysis on stability and convergence would help justify design choices."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9HXj4i65CQ", "forum": "8Zt6OsDzij", "replyto": "8Zt6OsDzij", "signatures": ["ICLR.cc/2026/Conference/Submission11017/Reviewer_5XEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11017/Reviewer_5XEU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142899832, "cdate": 1762142899832, "tmdate": 1762922196014, "mdate": 1762922196014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}