{"id": "c7vpjzlDR9", "number": 9334, "cdate": 1758119332660, "mdate": 1763048940213, "content": {"title": "Scale-Adapter: Reversed Distillation Adapter for Efficient Training of Large Video Diffusion Models", "abstract": "We propose Scale-Adapter, a plug-and-play adapter designed to efficiently bridge conditional knowledge from small adapted models to large video diffusion transformers. Existing controllable video DiT methods face critical inefficiencies: full fine-tuning of billion-parameter models is prohibitively expensive, while cascaded ControlNets introduce significant parameter overhead and exhibit limited flexibility for novel multi-condition compositions. To overcome these issues, Scale-Adapter introduces a novel reversed distillation method that allows a large video diffusion model to inherit precise control capabilities from efficiently tuned small video diffusion models, completely avoiding full fine-tuning.  Moreover, recognizing the intrinsic relationships among different conditions, we replace the cascaded ControlNet design with a Mixture of Condition Experts (MCE) layer. This structure dynamically routes diverse conditional inputs within a unified architecture, thereby supporting both single condition control and multiple condition combinations without additional training cost. To achieve cross-scale knowledge transfer, we further develop a Feature Propagation Module to ensure efficient and temporally consistent feature propagation across video frames. Experiments demonstrate that Scale-Adapter enables high-fidelity multiple condition video synthesis, making advanced controllable video generation feasible on low-resource hardware and establishing a new efficiency standard for the field.", "tldr": "", "keywords": ["Adapter", "Diffusion Model", "ControlNet", "Video Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/624082f8c2d40ae7d3f02712fadcadbc86e0cde2.pdf", "supplementary_material": "/attachment/e9c384d059ecb6ec960efafb8ed445ffc63e31f5.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce Scale-Adapter, a plug-and-play module for efficient and flexible training of large video diffusion models under diverse conditional controls. The method leverages a reversed distillation strategy that transfers conditional knowledge from small, pre-trained video diffusion models that can handle conditions to large frozen video models, by learning adapter layers that bridge them. A key component is the Mixture of Condition Experts (MCE) layer, which enables multi-condition control via dynamic routing within a unified architecture. Another innovation is the Feature Propagation Module, ensuring coherent and temporally aligned feature transfer. Experiments are conducted using Wan2.1 and CogVideoX backbones on the Koala-36M dataset with Canny, pose, and depth conditions. The method shows competitive or superior performance in both qualitative and quantitative evaluations (FVD, LPIPS, SSIM, CLIP), particularly excelling in temporal coherence and multi-condition fusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Computational Efficiency.** The proposed method performs knowledge transfer, using a small model to inject conditional control into a large frozen model via adapters. This avoids the computationally expensive full-model fine-tuning.\n2. **Strong Empirical Results.** The model achieves competitive results on FVD, CLIP, and LPIPS across multiple tested conditions, while requiring fewer trainable parameters. It also shows robust performance in ablation and zero-shot generalization experiments."}, "weaknesses": {"value": "1. **No Same-Backbone Adapter Baseline Comparison.** The authors report results of Scale-Adapter on Wan2.1 and CogVideo-X. However, it’s not clear if they re-implement baseline methods such as Ctrl-Adapter on the same backbone architecture for fair comparison. This weakens claims of efficiency and performance gains relative to prior work that carefully controlled for this factor.\n2. **Method novelty.** The use of pre-trained conditional feature generator to guide downstream image/video diffusion models has been already studied in X-Adapter and Ctrl-Adapter. Mixture of Condition Experts (MCE) module bears strong resemblance to the MoE-style adapter routing mechanism proposed in Ctrl-Adapter, but this similarity is not discussed.\n3. **Unclear Explanation of Parameter Efficiency in Figure 6.** It is not clearly explained how the proposed method achieves fewer trainable parameters compared to previous methods like X-Adapter and Ctrl-Adapter. The mechanism or architectural difference leading to this reduction should be better justified in the text.\n4. **Lack of Detail on Small Teacher Model.** The paper references a set of fine-tuned small video diffusion models as “teachers” for reversed distillation (e.g., Figure 3), but does not specify what models were used, how many per condition, or how they were trained. It also remains unclear whether such small video models exist at scale across many condition types."}, "questions": {"value": "- Citation mismatch in efficiency reporting: The paper refers to Figure 6 when discussing parameter efficiency of the MCE layer, but the actual comparison appears in Table 2."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "While checking the related work, I found the Sec. 3 (“Preliminaries”) contains text that closely mirrors Sec. 3.1 of Ctrl‑Adapter (https://arxiv.org/abs/2404.09967) with near‑identical paragraphs, raising concerns of uncredited text reuse and unclear attribution of prior work."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yM07FFrqRh", "forum": "c7vpjzlDR9", "replyto": "c7vpjzlDR9", "signatures": ["ICLR.cc/2026/Conference/Submission9334/Reviewer_omSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9334/Reviewer_omSh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549302530, "cdate": 1761549302530, "tmdate": 1762920968185, "mdate": 1762920968185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "TW7TxtrHx9", "forum": "c7vpjzlDR9", "replyto": "c7vpjzlDR9", "signatures": ["ICLR.cc/2026/Conference/Submission9334/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9334/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763048939190, "cdate": 1763048939190, "tmdate": 1763048939190, "mdate": 1763048939190, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Scale-Adapter, a plug-and-play adapter framework that efficiently transfers controllable knowledge from small video diffusion models to large-scale video diffusion transformers without requiring full fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Method. The paper presents a reversed knowledge distillation approach in diffusion models by transferring from smaller conditional models to larger pretrained video diffusion transformers. The method is highly efficient since it trains a single adapter to rule different conditions. \n2) Experiements. The authors compare the method with other methods which also use a small video model as prior and achieves better performance. \n3) Clarity. The paper is well-structured and easy to follow. Figures effectively visualize the problem motivation, method design, and results."}, "weaknesses": {"value": "1) Method. The method is mainly adapted from x-adapter and ctrl-adapter, with an additional MCE module. It lacks insights and improvements on this kind of method. It would be better if the author can do a deep analysis on the feature propagation like which layer plays a more important rule and what is the feature alignment before and after the adapter is applied.\n2) Metrics. The metrics the authors choose is not that proper. All the methods achieves >1000 FVD seems weird. There's also no metric to evaluate the condition fidelity.\n3) Methods. The baselines the author choose is not enough and I will detail it in the questions."}, "questions": {"value": "1) Method. Can the adapter be applied to other plugins like id-customization or stylization plugins other than controlnet? \n2) Metrics. Is it proper to use FVD in the experiments? All the methods' FVD value are >1000 which is a quite high value. Also, can you provide a metric to evaluate the condition fidelity?\n3) Experiments. Can you compare with other controllable video generation method like SparseCtrl[1] and VideoTetris[2] ? \n\n[1] Guo, Yuwei, et al. \"Sparsectrl: Adding sparse controls to text-to-video diffusion models.\" European Conference on Computer Vision. \n[2] Tian, Ye, et al. \"Videotetris: Towards compositional text-to-video generation.\" Advances in Neural Information Processing Systems 37 (2024): 29489-29513."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OzsXDpwAIw", "forum": "c7vpjzlDR9", "replyto": "c7vpjzlDR9", "signatures": ["ICLR.cc/2026/Conference/Submission9334/Reviewer_4G55"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9334/Reviewer_4G55"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906724074, "cdate": 1761906724074, "tmdate": 1762920966487, "mdate": 1762920966487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adapter-based architecture for structural control in video generation models."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The model is sufficiently lightweight, requiring minimal computational resources for both training and inference, while achieving satisfactory generation quality."}, "weaknesses": {"value": "Controllable generation in video models can also be achieved via ControlNet, for example, as demonstrated in https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control . The authors should consider including such models as baselines in their experiments.\n\nSome references are incorrectly cited—for instance, a “?” appears in line 147.\n\nAdditionally, some experimental results are mislabeled. For example, in line 385, the CLIP score of “Wan2.1-14B (fine-tuned)” is higher than that of “Ours”, yet the authors bolded “Ours”."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "btXxCAOnln", "forum": "c7vpjzlDR9", "replyto": "c7vpjzlDR9", "signatures": ["ICLR.cc/2026/Conference/Submission9334/Reviewer_vnrD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9334/Reviewer_vnrD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913693198, "cdate": 1761913693198, "tmdate": 1762920966133, "mdate": 1762920966133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Scale-Adapter, a method that uses a smaller diffusion backbone to simultaneously learn to encode multiple control signals and inject the control information to a larger diffusion model. The architecture relies on an adapter to transfer the information from the smaller control backbone to the larger generative model, which in turn comprises a MoE component and a additive component with learnable scale. Empirical results demonstrate better efficiency than controlnet approach and strong generalization abilities to new control modalities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The method demonstrates strong empirical results, significantly reducing trainable parameters while maintaining comparable results than more expensive methods (ControlNet, UniControl, Ctrl-Adapter, etc.).\n- It shows impressive ability to handle unseen conditions without retraining, suggesting strong adaptability and scalability of the proposed architecture."}, "weaknesses": {"value": "- The core method of the paper relies on the existence of a smaller pretrained video diffusion models that have the same number of layers (but smaller dimensions) as a larger video diffusion model. There may not be always a smaller version for state-of-the-art open-source models in the future, which limits the applicability of the proposed method. It would be great if the method can be applicable to the scenario where the small and large models are of different model family (e.g., CogVideo2B + Wan14B) and/or models of different number of layers.\n- Minor issues:\n  - Missing citation L147\n  - In Figure 4 right, consider change the color scheme so the color of different experts are very different from green/blue - at the beginning I thought expert 1 corresponds to large video model and expert 2 corresponds to smaller conditional model due to the color similarity.\n  - L297 alpha_s is not mentioned earlier, do you mean alpha_scale?\n  - L296 shouldn't the output be x_t^{a^\\prime} instead of x_t^a?"}, "questions": {"value": "The MCE design is motivated by the intrinsic connections exist among different conditioning signals. Are there any experiments that demonstrate that different experts actually become specialized to one (or a class of) modalities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vwvluptLtI", "forum": "c7vpjzlDR9", "replyto": "c7vpjzlDR9", "signatures": ["ICLR.cc/2026/Conference/Submission9334/Reviewer_SBaG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9334/Reviewer_SBaG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060870269, "cdate": 1762060870269, "tmdate": 1762920965794, "mdate": 1762920965794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}