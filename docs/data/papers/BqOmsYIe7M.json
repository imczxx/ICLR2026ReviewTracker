{"id": "BqOmsYIe7M", "number": 24743, "cdate": 1758359885598, "mdate": 1763656993677, "content": {"title": "Efficient Credal Prediction through Decalibration", "abstract": "A reliable representation of uncertainty is essential for the application of modern machine learning methods in safety-critical settings. In this regard, the use of credal sets (i.e., convex sets of probability distributions) has recently been proposed as a suitable approach to representing epistemic uncertainty. However, as with other approaches to epistemic uncertainty, training credal predictors is computationally complex and usually involves (re-)training an ensemble of models. The resulting computational complexity prevents their adoption for complex models such as  foundation models and multi-modal systems. To address this problem, we propose an efficient method for credal prediction that is grounded in the notion of relative likelihood and inspired by techniques for the calibration of probabilistic classifiers. For each class label, our method predicts a range of plausible probabilities in the form of an interval. To produce the lower and upper bounds of these intervals, we propose a technique that we refer to as decalibration. Extensive experiments show that our method yields credal sets with strong performance across diverse tasks, including coverage–efficiency evaluation, out-of-distribution detection, and in-context learning. Notably, we demonstrate credal prediction on models such as TabPFN and CLIP—architectures for which the construction of credal sets was previously infeasible.", "tldr": "Efficient credal prediction based on plausible probability intervals for computationally complex models (e.g. TabPFN, CLIP,…).", "keywords": ["efficient uncertainty representation", "credal sets", "relative likelihood"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f19f3b81bf3aa04ba4bb6b9c43e90a59882a2a6c.pdf", "supplementary_material": "/attachment/16042bbe66c9b56a0b297deae58fb0e1fe8073d1.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the authors present a post-hoc approach for generating the credal prediction by using class-wise plausible probability intervals. This is achieved by perturbing a trained model’s logits under a global likelihood-ratio budget, thereby exploring less-likely yet still plausible predictions without retraining. Multiple experiments from different perspectives are conducted."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is relatively well-structured and easy to follow.\n\n2. Although the work is built on recent work--the likelihood-based notion of plausibility (Löhr et al., 2025), the motivation and technical routes are different. It is novel and interesting to me.\n\n3. Mathematical proofs for the relative propositions are provided.\n\n4. Multiple experiments are performed."}, "weaknesses": {"value": "1. *Extensive experiments show that our method yields credal sets with strong coverage and efficiency and performs well on out-of-distribution detection tasks.* The main empirical claim seems a bit misleading. As from the OOD detection benchmarks, e.g., in Table 5, the EffEct only performs reasonably when \\alpha is close to 1, e.g., $\\alpha$ = 0.95 (still visibly lower than the other baselines). If we fix to use these values of $\\alpha$, how would one conclude that the EffCre has strong coverage and efficiency?\n\n2. The practicality of evaluating efficiency and coverage is limited. A key difficulty in supervised learning is the absence of ground truth for test instances. In addition, this work lacks theoretical guarantees for the coverage and does not provide a clear recipe for choosing the parameter $\\alpha$.\n\n3. In classification tasks, the prediction performance, e.g., test accuracy and calibration performance (expected calibration error), also matters. The performance of this approach in this matter remains unclear. As well as how $\\alpha$ will influnce the prediction performance.\n\n4. The paper highlights its good performance in epistemic uncertainty estimation for OOD detection using a single model. To support this claim, it would be valuable to include comparisons with other single-model-based epistemic uncertainty estimation methods, such as those from the evidential deep learning family or deterministic approaches. If EffCre continues to outperform these additional baselines, it would substantially strengthen the paper’s significance and credibility."}, "questions": {"value": "1. How would the authors place the work? A theoretical one or practical work? What would be the potential practical use case for the approach?\n\n2. Proposition 2.1 is a desired design principle for this method, right? It is a sufficient condition for controlling the trade-off between efficiency and coverage, not a necessary condition, am I correct?\n\n3. The EffCre significantly reduces the training complexity via a single model. What is the inference time complexity of EffCre?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WDI9fz1e5K", "forum": "BqOmsYIe7M", "replyto": "BqOmsYIe7M", "signatures": ["ICLR.cc/2026/Conference/Submission24743/Reviewer_9AwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24743/Reviewer_9AwT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683925777, "cdate": 1761683925777, "tmdate": 1762943182030, "mdate": 1762943182030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of efficiently computing credal sets over predictions of probabilistic classifiers. To this end, the authors propose perturbing the logits of the model to generate upper and lower bounds of plausible class-wise probabilities while adhering to a likelihood-ratio budget. Additionally, the authors propose an efficient method to compute class-specific credal sets. The proposed method is evaluated against relevant credal prediction baselines on coverage-efficiency trade-off and out-of-distribution detection tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The scope of the problem considered - model-agnostic credal predictions - is sizeable and will be of interest to a wide community. Additionally, the post-hoc approach that does not require any retraining as proposed in this paper will encourage its adoption as an added post-training step that can be used to quantify model's epistemic uncertainty. The proposed approach itself, to the best of my knowledge, is sound and the theoretical results seem reasonable, if not unsurprising. The experimental results suggest that the proposed approach is at least on par with the considered credal prediction baselines on the coverage-efficiency trade-off task while allowing, by design, a wide range of coverage. Finally, the efficient computation aspect allows the proposed approach to scale to large models which was previously infeasible."}, "weaknesses": {"value": "Credal predictions are particularly useful in data-scarce and safety-critical domains such as healthcare where the lack of data can lead to higher epistemic uncertainty and understanding the plausible range of model predictions can help avoid catastrophic decisions. In this regard, the motivation behind the need for computationally efficient credal predictions is not very compelling. In the same vein, while the authors note such safety-critical domains in their introduction, the experimental evaluation is primarily on large benchmark image datasets. Lastly, some of the baselines seem to perform slightly better on the OOD detection task, although requiring more computational time.\n\nOverall, I believe the strengths far outweigh the weaknesses."}, "questions": {"value": "Please refer to the weaknesses section. I am mainly unconvinced about the appeal of computational efficiency in so-called *safety-critical* domains where other credal prediction methods, as evident in fig. 3 on the OOD detection task, perform better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QxVfD7UfMC", "forum": "BqOmsYIe7M", "replyto": "BqOmsYIe7M", "signatures": ["ICLR.cc/2026/Conference/Submission24743/Reviewer_XAJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24743/Reviewer_XAJU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981626942, "cdate": 1761981626942, "tmdate": 1762943181843, "mdate": 1762943181843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work deals with estimating epistemic uncertainty of predictive models through credal sets. Specifically, it deals with uncertainty estimation for large pretrained foundation models such as LLMs and VLMs, where retraining and finetuning can be exceedingly expensive. The authors address this problem by proposing a training-free method that estimates plausible intervals by modifying the inference procedure. They do so by defining a credal set over the base model’s predictions by adding a variable vector c to their logits; The credal set consists of all the values of this modified predictive distribution that are within a threshold of the maximum likelihood estimate. This credal set allows uncertainty estimation and cautious decision-making. The authors evaluate empirically on 9 domains, including CIFAR, SVHN, FMNIST, DermMNIST, ImageNet, Places365, ChaosNLI, QualityMRI, and TabArena. On these domains, they compare the proposed method with 5 credal uncertainty estimation methods. They evaluate these methods on standard credal classification metrics like coverage and efficiency, on out-of-distribution detection, on active in-context learning, and on zero-shot classification, with the latter two focusing on large pretrained models. They find that the proposed method performs on par with baselines without requiring training and that it provides informative uncertainty estimates for active learning and cautious zero-shot classification."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method enables uncertainty estimation and cautious inference in large pretrained models like CLIP and TabFPN without expensive retraining & finetuning\n- The proposed method is straightforward to implement yet quite effective (as demonstrated by the empirical results)\n- The authors introduce credal spider plots to visualize credal sets represented as box intervals"}, "weaknesses": {"value": "- While the method does not require retraining, it does require the original training data or an appropriate surrogate to calculate the relative likelihood\n- The presentation is unclear in places, e.g., while the credal spider plots are quite informative, a full explanation about what they represent is presented in the appendix, which makes earlier references to them (e.g., figure 1) unclear. Section 4 (Empirical results) presents a lot of information without emphasizing key parts like research questions, datasets, metrics, and baselines; instead interleaves it with details about the setup for each experiment. This section could be made easier to read by explicitly listing the research questions datasets, metrics etc., before the subsections, which may focus on more specific details"}, "questions": {"value": "- The proposed method shifts the logits for each class separately, and the conclusion distinguishes this from the more general coupled case. Can you give an example where the class-wise shift would be a bad approximation of the general case?\n- Table 4 compares the runtimes of the proposed method and credal baselines, showing that the baselines take 10x more time than the proposed method, but it is not clear how this runtime is defined. Does it include training time or is the 10x difference only in inference time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YchbXYgumN", "forum": "BqOmsYIe7M", "replyto": "BqOmsYIe7M", "signatures": ["ICLR.cc/2026/Conference/Submission24743/Reviewer_yG26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24743/Reviewer_yG26"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002603029, "cdate": 1762002603029, "tmdate": 1762943181605, "mdate": 1762943181605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel, efficient, and model-agnostic method for credal prediction — a framework for representing epistemic uncertainty via credal sets (i.e., convex sets of probability distributions). The proposed method, termed decalibration, allows the construction of credal predictions without retraining or ensembling, which has historically been a major computational bottleneck in credal learning.\n\nCore Idea\nInstead of relying on Bayesian ensembles or multiple retrained models, decalibration works post hoc on a trained classifier. It perturbs the classifier’s logits within a relative-likelihood budget (α), thereby generating class-wise probability intervals that define a credal set.\n\t•\tWhen α = 1, predictions coincide with the maximum likelihood estimator (MLE).\n\t•\tAs α decreases, the set expands, capturing more epistemic uncertainty.\n\nThis approach maintains a clear likelihood-based interpretation: the resulting predictions represent all distributions “reachable without sacrificing more than an α-fraction of training likelihood.”\n\nTheoretical Contributions\n1.\tConvex feasibility and optimization properties:\n\t•\tThe set of permissible logit perturbations under a likelihood constraint is shown to be convex and compact on an identifiability hyperplane.\n\t•\tThe upper bounds of class probabilities correspond to the solution of a convex optimization problem, while lower bounds are attained at the boundary of this convex region.\n2.\tAnalytical results for 1D (class-specific) logit shifts:\n\t•\tEach class-wise bound can be efficiently computed through small convex programs or simple 1D searches.\n\t•\tThe resulting credal sets are nested and monotonic in α.\n\nEmpirical Contributions\n\t•\tExtensive experiments show the method’s competitive performance on coverage–efficiency trade-offs and out-of-distribution (OOD) detection, outperforming or matching credal baselines while being orders of magnitude faster.\n\t•\tThe approach is scalable to large architectures such as TabPFN and CLIP, where traditional ensemble-based credal methods are computationally infeasible.\n\t•\tVisual tools like credal spider plots are introduced to illustrate uncertainty across multi-class predictions.\n\nSignificance\nThis paper advances epistemic uncertainty quantification by providing a principled yet practical alternative to computationally intensive Bayesian or ensemble-based credal methods. Its theoretical soundness, post-hoc simplicity, and broad applicability to modern foundation models make it a potentially impactful contribution to the ICLR community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tOriginality:\nThe paper presents a novel post-hoc approach to credal prediction — decalibration — that eliminates the need for retraining or ensemble-based inference, which have been the dominant approaches in credal and epistemic uncertainty estimation. The idea of adjusting logits within a relative-likelihood constraint is both elegant and conceptually original, bridging Bayesian epistemic reasoning with practical optimization.\n\t•\tTechnical Quality:\nThe theoretical exposition is mathematically sound and internally consistent. The authors derive and justify convexity properties of the credal set under the proposed perturbation scheme, ensuring interpretability and computational tractability. Analytical insights into the monotonicity of the α-parameterized likelihood bounds reinforce the approach’s rigor.\n\t•\tPractical Relevance:\nThe method is computationally efficient and easily applicable to large-scale deep networks and foundation models (e.g., CLIP, TabPFN). This directly addresses a key bottleneck in existing credal learning methods, which often require retraining or expensive ensembles.\n\t•\tClarity and Presentation:\nThe paper is generally well written, with clear organization, intuitive explanations, and informative visualizations (e.g., credal spider plots). The connection between likelihood decay and epistemic expansion is articulated clearly and grounded in statistical reasoning.\n\t•\tExperimental Strength:\nThe experiments are broad and diverse, covering OOD detection, reliability under label noise, and uncertainty calibration across various architectures. The results show consistent improvements in efficiency–coverage trade-offs, supporting the method’s robustness.\n\t•\tSignificance:\nThe proposed framework provides a principled and scalable solution to epistemic uncertainty quantification in deep learning, which is an increasingly critical research area in reliable AI. Its post-hoc and model-agnostic nature make it particularly relevant for the ICLR community focused on trust, calibration, and interpretability.\n\nOverall Strength Summary:\nThe paper is a solid and meaningful contribution that balances theoretical insight with practical usability. It offers an innovative and efficient solution to credal prediction, addressing both the computational and conceptual limitations of prior approaches."}, "weaknesses": {"value": "•\tLimited Theoretical Depth Beyond Convexity:\nWhile the convexity and boundedness of the credal sets are clearly demonstrated, the paper lacks deeper theoretical guarantees. For instance, there are no formal proofs of coverage calibration, robustness under data shift, or asymptotic optimality compared to Bayesian posteriors.\nSuggestion: Strengthen the theoretical contribution by connecting decalibration to known uncertainty frameworks such as PAC-Bayesian bounds, conformal coverage guarantees, or distributionally robust optimization.\n\t•\tPotential Overlap with Prior Work:\nThe approach resembles ideas from temperature scaling, likelihood perturbation, and distributional robustness via logit adjustment (e.g., Stutz et al., 2021; Ahuja et al., 2023). The conceptual novelty might appear incremental to readers unless clearer distinctions are drawn.\nSuggestion: Explicitly clarify how decalibration differs mathematically or conceptually from logit perturbation in confidence calibration or adversarial robustness literature.\n\t•\tEmpirical Evaluation Scope:\nThe experimental section, though diverse, is mostly limited to classification tasks. Since credal methods are general, it remains unclear how decalibration performs in structured or regression contexts, where uncertainty has different semantics.\nSuggestion: Add at least one structured prediction or regression experiment (e.g., depth estimation or tabular uncertainty).\n\t•\tInterpretability of α-Parameter:\nThe α hyperparameter controlling likelihood decay is intuitive but empirically opaque. Its practical selection and relationship to epistemic uncertainty remain heuristic.\nSuggestion: Provide either a principled selection rule (e.g., based on validation likelihood or calibration metrics) or a sensitivity analysis showing stable performance over α ranges.\n\t•\tComparative Baselines:\nWhile results are favorable, the baselines do not include recent strong probabilistic calibration models such as Dirichlet Prior Networks or Deep Ensembles with temperature tuning. Without these, the strength of decalibration over modern uncertainty quantifiers remains somewhat uncertain.\nSuggestion: Include these baselines or discuss expected trade-offs to contextualize improvements.\n\t•\tTerminological Ambiguity (“Decalibration”):\nThe term decalibration may be confusing since in standard uncertainty literature, “calibration” typically denotes improving reliability, not relaxing likelihood constraints.\nSuggestion: Clarify this choice early in the paper and consider an alternative framing such as “likelihood-scaling credalization” or “post-hoc credal expansion.”\n\t•\tComputational Claims Need Quantitative Backing:\nThe paper asserts substantial efficiency gains (“orders of magnitude faster”) but provides limited runtime comparisons or profiling details.\nSuggestion: Include explicit runtime or FLOPs analysis versus ensemble-based credal methods to substantiate this claim.\n\nOverall Weakness Summary:\n\nThe paper is well-executed and conceptually clear, but its mathematical guarantees, empirical breadth, and comparative depth could be strengthened. Clarifying the novelty relative to prior calibration and robustness work, expanding evaluation beyond classification, and providing stronger empirical or theoretical justifications would elevate the paper’s impact and credibility."}, "questions": {"value": "1.\tClarification on the Likelihood Decay Parameter (α):\n\t•\tHow should practitioners choose or interpret α in practice?\n\t•\tIs there a connection between α and known uncertainty measures such as expected calibration error (ECE) or Bayesian posterior variance?\n\t•\tCould α be automatically tuned using a validation objective (e.g., coverage vs. set size trade-off)?\n2.\tRelation to Distributionally Robust Optimization (DRO):\n\t•\tThe likelihood-based constraint defining the credal set seems conceptually close to DRO formulations (e.g., χ²-divergence or f-divergence balls).\n\t•\tCan the authors clarify whether decalibration is theoretically equivalent to or inspired by DRO methods?\n\t•\tIf not equivalent, how does its uncertainty behavior differ under covariate shift or adversarial perturbations?\n3.\tDistinction from Prior Post-hoc Calibration Methods:\n\t•\tDecalibration operates directly on logits, similar to temperature scaling, confidence calibration, and logit perturbation techniques.\n\t•\tCould the authors explicitly explain how their formulation mathematically differs from those methods and why it better captures epistemic (not aleatoric) uncertainty?\n4.\tComputational Complexity Claims:\n\t•\tThe paper claims “orders of magnitude” improvement in efficiency over ensemble-based methods.\n\t•\tCould the authors provide quantitative runtime comparisons (e.g., seconds per image or FLOPs) for fair assessment?\n\t•\tIs the optimization step fully parallelizable, and how does performance scale with the number of classes?\n5.\tGeneralization to Regression or Structured Outputs:\n\t•\tThe current framework appears classification-specific.\n\t•\tIs there a theoretical extension of decalibration to continuous outputs (e.g., regression) or structured prediction (e.g., segmentation, detection)?\n\t•\tIf so, how would the likelihood constraints translate?\n6.\tCredal Set Visualization and Intuition:\n\t•\tThe “credal spider plots” are compelling but may lack clear interpretability for practitioners.\n\t•\tCould the authors provide an example of how such visualization could inform human decision-making (e.g., in safety-critical applications)?\n7.\tUncertainty Decomposition:\n\t•\tDoes decalibration allow separating epistemic vs. aleatoric uncertainty components?\n\t•\tIf not, could an ensemble of decalibrated models or Bayesian prior over α achieve that?\n8.\tRobustness under Data Shift:\n\t•\tHave the authors tested decalibration under distributional shift scenarios (e.g., corrupted datasets, domain transfer)?\n\t•\tIf so, how does it compare to ensemble or conformal methods in terms of coverage stability?\n\nSummary of Key Questions for Rebuttal Focus:\n\t1.\tTheoretical connection to DRO and uncertainty calibration frameworks.\n\t2.\tJustification and interpretation of α.\n\t3.\tExplicit differentiation from existing logit-perturbation and calibration methods.\n\t4.\tQuantitative validation of efficiency claims.\n\t5.\tPotential extension beyond classification tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper focuses on theoretical and algorithmic advances in credal prediction and epistemic uncertainty quantification, with no experiments involving human subjects, private data, or potentially harmful applications.\n\nThe datasets used (e.g., standard vision and tabular benchmarks) are public and widely accepted in the machine learning community, and there is no indication of ethical risk such as bias amplification, privacy violation, or misuse potential.\n\nThe methodology enhances model transparency and uncertainty communication, which—if anything—supports ethical AI development rather than compromising it."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EVMW24vyjI", "forum": "BqOmsYIe7M", "replyto": "BqOmsYIe7M", "signatures": ["ICLR.cc/2026/Conference/Submission24743/Reviewer_7AnL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24743/Reviewer_7AnL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24743/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763145186653, "cdate": 1763145186653, "tmdate": 1763145186653, "mdate": 1763145186653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "**We thank all reviewers for their thoughtful comments, constructive feedback, and general appreciation of our work.** We answered the comments and questions in responses to the individual reviewers, and carefully revised the manuscript in response to the suggestions. All changes in the paper are marked in $\\textcolor{blue}{blue}$. Below is a list of all changes:\n\n\n- Added a sentence in the Abstract to clearly separate the empirical claims.\n- Added an explicit reference in the caption of Figure 1 to the guide on interpreting spider plots in the appendix.\n- Revised beginning of Section 4 (Empirical Results) to more clearly connect the research objectives to the corresponding experiments.\n- Restructured the evaluation so that each experiment now has a distinct experimental setup and results paragraph.\n- Added descriptions and performance of two additional baselines (Evidential Deep Learning and Deep Deterministic Uncertainty) in Appendix D.3 and E.2.\n- Added evaluation of inference time, accuracy and expected calibration error in Appendix E.2 and F.3."}}, "id": "InzlTEitbI", "forum": "BqOmsYIe7M", "replyto": "BqOmsYIe7M", "signatures": ["ICLR.cc/2026/Conference/Submission24743/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24743/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission24743/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763658781292, "cdate": 1763658781292, "tmdate": 1763660078800, "mdate": 1763660078800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}