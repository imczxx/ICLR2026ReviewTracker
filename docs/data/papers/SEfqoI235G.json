{"id": "SEfqoI235G", "number": 4552, "cdate": 1757705958782, "mdate": 1759898027081, "content": {"title": "Rethinking Visual Intelligence: Insights From Video Pretraining", "abstract": "Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.", "tldr": "Video diffusion models pretrained on spatiotemporal data outperform language models on diverse visual tasks, showing strong inductive biases and adaptability from video pretraining.", "keywords": ["video diffusion models", "large language models", "spatiotemporal pretraining", "transfer learning", "visual intelligence", "inductive biases", "foundation model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/759053eb33452690a679920d30961cfa02faac2f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores Video Diffusion Models (VDMs) as a new direction for pre-training visual intelligence models, aiming to bridge the generalization and data efficiency gap between language models (LLMs) and visual models.\n\nThe authors propose a unified framework to rephrase visual tasks as image-to-image as video transition, and systematically compare LLMs and VDM with the same LoRA fine-tuning method across a range of tasks. Results demonstrate that VDMs significantly outperform LLMs in tasks requiring spatial structure understanding, logical or abstract reasoning, and route planning, while also exhibiting higher data efficiency. The paper argues that the inductive bias introduced by spatiotemporal pre-training can contribute to building more general visual foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This article proposes an interesting research direction, rightly noting that visual pre-training (particularly with video) remains underexplored compared to linguistic pre-training. It offers valuable insights, as its title (insights from video pretraining) suggests, and presents experimental evidence that video-pretrained VDMs can generalize more effectively to downstream tasks (e.g., spatial structure understanding, logical or abstract reasoning, and route planning) than language-pretrained models. That's why I give good **Soundness**."}, "weaknesses": {"value": "While I find the core topic of this paper highly interesting, the current manuscript still has significant issues in terms of **sufficiency** and **completeness**.\n\n- **Overly Broad Title & Narrow Focus**: The title is general, while the actual research scope is limited primarily to Video Diffusion Models (VDMs). It should be noted that video foundation models are not exclusively built on diffusion-based approaches—autoregressive (AR) models, for example, represent another important paradigm. The experiments presented are not substantial enough to support the broad claim made in the title. A more accurate title should explicitly reflect that the study focuses on video-diffusion model pre-training?\n\n- **Insufficient Solid Content in Section 3**: The content of Section 3 reads as overly conceptual, covering mostly established background knowledge. Dedicating two full pages to such content does not effectively advance the paper's argument (There are many cases where a single sentence is a standalone paragraph, seeming unnecessary to spend so much space?). It would be more valuable to incorporate additional experimental results and analyses from the appendix into the main body. In its current form, this section gives the impression of a lack of refinement, which is the main reason for my low score in **Presentation**.\n\n- **Limited Scope of Comparative Evaluation**: The study only demonstrates VDM's advantages over LLMs in tasks that can be naturally framed as image-to-image transitions. However, it remains unclear how VDMs would be applied to tasks that are not easily represented as image inputs—such as mathematical reasoning, code generation, or knowledge tasks. What would the input form be for such cases? Would VDMs still maintain an advantage? Most video foundation models start with image foundation models. Since the title mentions **rethinking visual intelligence**, why is it limited to discussing video pre-trained models and not including pure image pre-trained models? The experimental part conducted in the article is not sufficient to support the title of this article. In addition, why do not compare with VLM (image-text input, text out), and t2i/t2v model with text input, image ouput?\n\n**In summary, I believe the current manuscript requires considerable improvement and comes across as somewhat rushed. With substantial revision and expansion, I think it may be a good article**."}, "questions": {"value": "See Weakness.\n\nAnother point concern is that how to determine the correctness of the VDM's output image, specifically how it is evaluated against the ground truth. I did not find the corresponding descriptions for this in the main text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e6RXSZltI9", "forum": "SEfqoI235G", "replyto": "SEfqoI235G", "signatures": ["ICLR.cc/2026/Conference/Submission4552/Reviewer_W7Aw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4552/Reviewer_W7Aw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760725985697, "cdate": 1760725985697, "tmdate": 1762917435263, "mdate": 1762917435263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors evaluate video diffusion models and LLMs on visual tasks like ARC-AGI, path planning, sudoku etc. using a LoRA based finetuning setup, to demonstrate that video diffusion models are more data efficient than LLMs when it comes to visual reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors have curated a set of interesting visual tasks to benchmark the spatial reasoning capacity of VDMs and LLMs from cellular automata to visual games.\n2. The ARC-AGI results are quite novel and timely and highlight drawbacks of current LLMs."}, "weaknesses": {"value": "1. It seems all the visual tasks require spatial reasoning, not spatio-temporal reasoning, which begs the question why not evaluate image diffusion models as well instead of video diffusion models where the uathors practically discard the temporally intermediate frames generated by the model, essentially not using/evaluating the temporal reasoning capacity of these models.\n\n2. The data efficiency plots compare cog-x with qwen without controlling for pre-training FLOPs/data-volume. This is a very important factor that can determine baseline model performance and should be reported.\n\n3. The authors need to show results for more than one VDM and LLM across all these tasks in order to make general claims about model families.\n\n4. The authors also need to demonstrate scaling behaviors of these VDMs, showing improvement in data efficiency/visual reasoning performance, with increase in model params/pretraining FLOPs etc. to support the claim that VDMs can become foundational vision models. \n\n5. Finally, for the sake of completeness, the authors should also analyse VDMs vs LLMs for sequence reasoning tasks, to give a full picture of the strengths and weaknesses of these large vision/language models. \n\n\nOverall the paper tries to do interesting and relevant analysis of VDMs and LLMs but still lacks crucial results and experiments."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MASgmqJMjZ", "forum": "SEfqoI235G", "replyto": "SEfqoI235G", "signatures": ["ICLR.cc/2026/Conference/Submission4552/Reviewer_Zamz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4552/Reviewer_Zamz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968500293, "cdate": 1761968500293, "tmdate": 1762917435017, "mdate": 1762917435017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper compares video diffusion models (VDMs) and large language models (LLMs) under a symmetric, frozen-backbone + LoRA protocol: VDMs perform image-to-image prediction by reframing each input–output grid as a short transition video with discrete interpolation and a neutral fixed text embedding, while LLMs do JSON-to-JSON sequence prediction. Evaluation spans ARC-AGI/ConceptARC, visual games, route planning, and cellular automata with sample-efficiency curves; results show that VDMs are often more sample-efficient on spatial/temporal structure, supporting the claim that video pretraining offers a powerful foundation for visual intelligence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Hypothesis and Reframing: The paper's core strength is its originality in reframing VDMs as general problem-solvers rather than just generators. The hypothesis that spatiotemporal inductive biases are key to visual intelligence is a significant and insightful contribution.\n- Focus on Data Efficiency: The evaluation wisely focuses on skill acquisition efficiency instead of just final SOTA performance. This provides much deeper evidence for the VDM's superior learning properties in low-data regimes."}, "weaknesses": {"value": "1. Fundamental Asymmetry in Task Representation and Modality: The comparison's fairness is highly questionable due to a core mismatch in task modalities. \n(1)\tThe LLM must perform a text-to-text translation on JSON-serialized grids , while the VDM performs a direct pixel-to-pixel mapping. These two representations have fundamentally different information densities, processing complexities, and inherent difficulties. (For example, given a 5x5 grid structure, VDM needs to process an image of 256x256 pixels, while LLM needs to process 25 numbers.)\n(2)\tThe LLM faces a \"dual burden\" of mastering a complex JSON syntax in addition to the task's core logic. Could the LLM's poor data efficiency be a result of this syntactic and representational overhead, rather than a true failure of its inductive bias for logic?\n2. Limited Task Scope and Ecological Validity: The paper makes strong claims about visual intelligence and visual foundation models , but the evaluation is confined to a curated set of synthetic, grid-based tasks with explicit human-defined rules (e.g., ARC, Sudoku, Mazes). This success on toy problems, which are highly amenable to grid-based serialization, does not guarantee the VDM's advantage will generalize to the ambiguity, noise, and implicit physical rules of real-world perception. \n3. Low Absolute Performance: Despite relative efficiency gains, the VDM's low absolute accuracy on key abstract tasks (like ARC-AGI) suggests it also has fundamental limitations in abstract generalization. This low ceiling challenges the claim that this bias is a sufficient solution.\n4. Un-ablated Text Embedding: The VDM is conditioned on a \"neutral fixed text embedding\" ($e_{text}$). Without an ablation study, it is unclear if this provides a crucial task hint, acting as an unfair advantage for the VDM over the LLM, which received no such meta-prompt.\n\nOpen to increasing my score, provided my concerns are addressed."}, "questions": {"value": "Please refer to the questions mentioned in 'weaknesses' section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ApywJxq5KV", "forum": "SEfqoI235G", "replyto": "SEfqoI235G", "signatures": ["ICLR.cc/2026/Conference/Submission4552/Reviewer_LGZ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4552/Reviewer_LGZ3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998787793, "cdate": 1761998787793, "tmdate": 1762917434725, "mdate": 1762917434725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate whether VDMs can serve as a foundation for visual intelligence, like how LLM did for NLP. They use spatiotemporal pretraining to enhance generalization and data efficiency and provide experimental results. They also curated 3 synthetic benchmarks to test those. For controlled experiments, both were adapted with LoRA fine-tuning while keeping backbones frozen. Each model receives tasks in its natural modality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of this work is good; the questions the authors raised deserve to have a work to study them. \n2. The curated tasks of interest are interesting; they designed synthetic tasks to test them\n3. They show some results that pretraining on VDM modality specific tasks would improve its downstream performances."}, "weaknesses": {"value": "1. The synthetic tasks are too simplified to be indicative to downstream or other tasks performance. If the authors can show some downstream application enhancements, even just 1 example, then it would be more convincing.\n2. The authors compare LLM and VDM, which are two different architectures. There may be transformer-based video LLMs available, such as VideoPoet and VAR, among others. I am sure there are also open-sourced alternatives that are more suitable for these comparisons.\n3. The ablations of experiments can be more extensive to be convincing, e.g., add 1 or 2 more families of LLMs and VDMs, add different sizes of those LLMs/VDMs, or, since the authors use LoRA, maybe there can be one more config, etc."}, "questions": {"value": "See weaknesses. Also, maybe the authors can compare different family or Visual foundation models, e.g., VDMs vs transformer-based VLMs. I wonder how those results would fare?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cESlRea3pH", "forum": "SEfqoI235G", "replyto": "SEfqoI235G", "signatures": ["ICLR.cc/2026/Conference/Submission4552/Reviewer_ykQ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4552/Reviewer_ykQ1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124983186, "cdate": 1762124983186, "tmdate": 1762917434427, "mdate": 1762917434427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}