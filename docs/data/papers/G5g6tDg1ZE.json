{"id": "G5g6tDg1ZE", "number": 11056, "cdate": 1758188337010, "mdate": 1759897611846, "content": {"title": "Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion", "abstract": "Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing conditional tabular generators rely on training-time strategies that cannot generalise to unseen constraints at inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle arbitrary inference-time objectives. On this foundation, we introduce Harpoon, a conditional tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating Harpoon's strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://anonymous.4open.science/r/ManifoldTabularImputation-44E4/", "tldr": "Extends manifold theory for handling arbitrary tabular conditional generation tasks at inference time.", "keywords": ["Diffusion models", "Conditional generation", "Tabular diffusion", "Manifold learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/121c66982d83039352d876a18da8c5abb0d5c13e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper generalizes previous ideas from manifold theory to diffusion models for tabular data. This allows the usage of differentiable losses at inference time to guide samples along the data manifold at a given diffusion time $t$. Depending on the objective, this produces samples that satisfy constraints based on partially observed data and specified inequality constraints without the need for re-training the diffusion model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors generalize previous results, extending the usefulness of manifold-based insights to tabular data. In particular, they remove the necessity of squared losses, which extends the effective conditioning capabilities.\n- The ability to condition on certain features or constraints at inference time without the need for training the model on that specific conditional generation task is very valuable, in particular for tabular data.\n- The illustrations mostly paint an intuitive picture of the underlying mechanisms.\n- I like the effort of making the model more comparable to other diffusion-based approaches by adjusting the backend architecture. \n- The presented results show competitive or better performance in practice. Great improvements can be seen in scenarios that impose inequality constraints."}, "weaknesses": {"value": "- The methodological background on diffusion is basically non-existent and the overview of the tabular diffusion models is severely underdeveloped. Since TabDDPM, many models have been proposed that 1) often considerably outperform TabDDPM and 2) do not rely on multinomial diffusion. In fact, models that treat categorical data in some continuous space exist (e.g., TabSyn [1] or CDTD [2]). It is unclear how the results extend to such models.\n- The missingness rates of 0.5 and 0.75 in the main results seem very extreme. The cited DiffPuter paper uses 30%. In reality, it is questionable why we would want to impute data when 75% of the data is actually missing. For more moderate missing rates, the performance advantage of the proposed method is not clear but it remains mostly competitive.\n-  Only using a single metric to evaluate sample quality of tabular data (in the case of inequality constraints) is not enough. Metrics like the detection score (see, e.g., metrics used in [1] and [2]) which evaluate the joint distribution more holistically would be interesting to see and give more insight into how the manifold guidance impacts samples.\n- The examined constraints for the conditional generation tasks are rather simple and focus on a maximum of two features (one categorical, one continuous), so the proportion that training samples are valid is still quite high. It is unclear how the framework performs under more constraints, which make the guidance more difficult. An ablation study should investigate what happens if the number of constraints is increased, such that the number of valid training samples shrinks. In the extreme case, is the method able to recover a single valid observation?\n- The paper makes the claim in line 112 that extending the results from Chung et al. to tabular diffusion is not trivial. One reason is that tabular data contains discrete features. This, however, is then solved by the rather trivial solution of simply one-hot encoding discrete features and treating them as continuous. Besides this encoding, there is no tabular-data-specific modeling in the paper.\n- Figure 2a) does not actually illustrate the behavior of a \"spotlight\" that becomes sharper near $\\mathcal{M}_0$.  In the Figure the position of $x_t$ relative to $\\mathcal{M}_0$ never changes. When the spotlight becomes sharper, I would expect an $x_s$ closer to $\\mathcal{M}_0$ where $s < t$. Note also that the orthogonal projection of $x_t$ and $x_s$ need not be the same.\n- Making the assumption in line 253 that $Q_t(x_t)$ is approximately orthogonal for all $t$ based on only a single empirical observation seems not well-founded. Could this assumption not be weakened by making $\\eta$ time-dependent, such that the guidance term has more impact when it is also more likely to hold, i.e., as $t\\rightarrow 0$?\n- It is not clear what tabular diffusion framework is assumed. It appears to be very similar to to StaSy model [3], which also one-hot encodes categorical data. The authors should be more specific about the framework they are using. If it can be used on any or most tabular diffusion frameworks (maybe under certain conditions) this should be stated as a strength of the framework. Coming up with an entirely new generative model does not highlight the strength of the potentially more general approach.\n- Getting the tangential gradient requires backpropagating through $\\epsilon_\\theta$. It should be clarified how costly the method is, in particular at inference time and relative to the other baselines.\n- Based on the inference time losses and the results, it should be highlighted by the authors that it is not guaranteed that samples satisfy the specified constraints. The approach is more similar to penalization than imposing hard constraints.\n- The authors do not discuss how their approach differs from classifier-guidance when using similar losses.\n\n---\n\n\n[1] Zhang, et al. (2024) Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space. ICLR.\n\n[2] Mueller, et al. (2025) Continuous Diffusion for Mixed-Type Tabular Data. ICLR.\n\n[3] Kim, et al. (2023) STaSy: Score-based Tabular data Synthesis. ICLR."}, "questions": {"value": "In addition to the suggestions and questions stated in Weaknesses, I kindly ask the following questions:\n\n- Could this manifold guidance lead to a skewed (conditional) data distribution?\n- Your guidance mechanism leads to *soft* constraints but not cannot impose *hard* constraints. This should be made clear in the text. Does that lead to issues in the imputation task? Since the non-missing values are not strictly fixed but still updated during sampling, could it lead to the final samples actually deviating from the partially observed information?\n- Does the guidance also work for latent diffusion models (TabSyn, CDTD, etc.) or is a data-space model necessarily needed?\nWhy is it a problem if models push samples orthogonally towards $\\mathcal{M}_0$ (line 104/105)? Even if it skips a shell, in the next update step, the model, which is conditioned on $t$, should have no problem continuing the path if trained properly. Is this an artifact from a model that is not capable / complex enough?\n- How does your guidance generalize to high dimensions, in particular when increasing the number of features or categories in a dataset? One-hot encoding is known to be very inefficient and will blow up the dimensions when applied to a categorical feature with 100s or 1000s of categories. \n- Can relative or logical constraints be accommodated? For instance, consider a data table of order and delivery dates. Naturally, we would expect that order date < delivery date.\n- Figure 2 b) and c) give a good intuition of moving a sample along a shell. Can there be situations in practice where this fails? For example, if at a particular level the manifolds are no longer connected, e.g., $\\mathcal{M}_t$ is not a single piece but two separate ones?\n- Are you using the QuantileTransformer to transform the continuous features for the TabDDPM model? From own experience this appears to be a crucial step for performance and is ubiquitous in tabular diffusion models.\n- In line 420: \"not just the constrained ones [...]\", do you actually mean the *un*constrained features? \n- Figure 3 is not well-designed. It does not does not include the red or green lines due to unfortunate overlaps. It would yield a more consistent argument if the x-axis would indicate $t \\in [0,1]$ and not the denoising step."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Jnewrrtpo", "forum": "G5g6tDg1ZE", "replyto": "G5g6tDg1ZE", "signatures": ["ICLR.cc/2026/Conference/Submission11056/Reviewer_1jxC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11056/Reviewer_1jxC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748533021, "cdate": 1761748533021, "tmdate": 1762922231991, "mdate": 1762922231991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method that can perform conditional tabular diffusion named HARPOON. It can generate new tabular data with specific conditions. In the theory contribution section, the authors show: 1. diffusion de-noiser acts as orthogonal projectors to data manifold 2. any differentiable inference-time loss has gradient in the tangent space of the manifold and therefore updates using this gradient can preserve realism. HARPOON is a method of combining unconditional de-noising step and the constraint-guided tangent gradient step. The experiments show that HARPOON outperforms other baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and provides both theoretical and experimental contributions\n2. The theory is novel and is the first to use diffusion model's orthogonal projection to manifolds in tabular setting\n3. HARPOON can handle mixed data types and has much lower constraint violation rate when doing conditional generation"}, "weaknesses": {"value": "1. Computation time might be an issue but this is not discussed in the experiments\n2. The utility, fidelity and privacy aspects of the generated tabular data is not discussed. It would be great to see where HARPOON stands among these 3 aspects."}, "questions": {"value": "1. Does the tabular data generated from HARPOON improve the downstream model?\n2. What does the run time of HARPOON look like against other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YUMwjR7gCI", "forum": "G5g6tDg1ZE", "replyto": "G5g6tDg1ZE", "signatures": ["ICLR.cc/2026/Conference/Submission11056/Reviewer_c4MN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11056/Reviewer_c4MN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811886370, "cdate": 1761811886370, "tmdate": 1762922231470, "mdate": 1762922231470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new way to guide diffusion models for tabular generation at inference time. In particular, they show how at every time step $t$ most of the datapoints lie on a shell of dimension $d-1$ ($d$ being the original dimensionality of the datapoints) and that while the denoising process creates an orthogonal projection onto the shell of the datapoint $x_t$, the gradient of any loss function defined  starting from a condition $c$ is tangent to such shell. Exploiting these geometric results, they are able to create a new procedure for guiding the diffusion process at inference time, where essentially they interleave the diffusion process which (as they nicely put it) acts as a \"compass\" towards the manifold of the original datapoints and the update wrt the loss function that encodes the constraints which moves along the shell to guide the model towards the right region of the manifold."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very well written and provides a lot of intuitions about the results they propose. I really appreciated the visualisations of the gradients and the updates. \n\nThe experimental analysis is extensive and with very positive results. \n\nThe authors give a very nice geometric explanation of why their method works.\n\n**Note:** It is difficult for me to assess the novelty of this work wrt the previous works on diffusion models as I am not familiar with them."}, "weaknesses": {"value": "1.  The authors do not report the sampling generation time. As this is an important metric for tabular data generation, it would be nice to have it\n\n2. In theorem 3.2 $\\mathcal{C}$ is not defined. Also it is not clear which conditions we have on $\\mathcal{C}$. Can it really be any arbitrary information? For example (Stoian & Giunchiglia, 2025) has extended the work cited in your paper to constraints expressed as disjunctions over linear inequalities. This defines non-convex and disconnected spaces hence violating your assumption 1. Would this time of conditioning be allowed? What about polynomials? \n\n3. A better definition of the alpha metric is needed. \n\n\n\nMinor things: \n\n1. Citation for Borisov et al. is missing the year \n2. Sometimes equation is written with capitol E and sometimes with lower case e \n\nReferences: \n\nStoian & Giunchiglia. Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints, ICLR, 2025."}, "questions": {"value": "1. In order to impose the linear inequality constraints you had to devise an ad-hoc loss function. Do you see this as a possible bottleneck for the widespread application of your solution? \n\n2. Aside from categorical constraints, it is very reasonable to assume that tabular data have disconnected support. This makes me wonder how realistic is assumption 1 and also how important it is that assumption 1 is met in practice. Would it be possible to have an ablation study where a multiple datasets are created with each of them violating the assumption in different degrees and then studying how the method performs on them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JGTkm7EtfJ", "forum": "G5g6tDg1ZE", "replyto": "G5g6tDg1ZE", "signatures": ["ICLR.cc/2026/Conference/Submission11056/Reviewer_kUVE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11056/Reviewer_kUVE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993959382, "cdate": 1761993959382, "tmdate": 1762922231013, "mdate": 1762922231013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response Summary by Authors"}, "comment": {"value": "## Main comment\n\nWe thank all reviewers for the valuable feedback. We conducted additional experiments and added new figures to strengthen the paper. All the updates will be added to the revised paper. \n\n*(NOTE: Multiple reviewers sometimes had overlapping concerns. We have indicated these below and have copy-pasted the result tables for each reviewer individually for ease of read*).\n\n**Summary of additional experiments:**\n* **Sampling runtimes** (Reviewers kUVE, c4MN , 1jxC). **Tables A, D, J**.\n* **Results on disjunctions of range and categorical constraints** (Reviewers kUVE, 1jxC). To test guidance under disconnected submanifolds. **Tables B, K**.\n* **Effect of varying disconnectedness of constraints** (Reviewer kUVE). **Table C**.\n* **Results with additional metrics on utility, fidelity, and privacy** (Reviewers  c4MN, 1jxC). **Tables E, G**\n* **Results with lower missing ratios** (Reviewer 1jxC). **Table F**.\n* **Recovery under large number of constraints** (Reviewer 1jxC). **Table H**.\n* **Results on linear guidance schedule** (Reviewer 1jxC).**Table I**.\n\n**Additional figures**\n\n* **Disconnected (sub)manifolds** (Reviewers kUVE, 1jxC).\n* **Improved spotlight figure** (Reviewer 1jxC).\n* **Updated empirical orthogonality plot** (Reviewer 1jxC).\n\nWe hope these additions address the concerns raised by the reviewers."}}, "id": "JZ1FED8H0T", "forum": "G5g6tDg1ZE", "replyto": "G5g6tDg1ZE", "signatures": ["ICLR.cc/2026/Conference/Submission11056/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11056/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission11056/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763295594121, "cdate": 1763295594121, "tmdate": 1763295594121, "mdate": 1763295594121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}