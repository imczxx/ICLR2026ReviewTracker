{"id": "NDlnDvGD7e", "number": 25107, "cdate": 1758364235463, "mdate": 1763294626794, "content": {"title": "Thinking Before Coding: WebUI-to-Code Driven by Layout Reasoning and Consistency Rewards", "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have made substantial progress in visual understanding and language generation, offering new opportunities for automating front-end web development. The WebUI-to-Code task, translating webpage design mockups or screenshots directly into structured HTML, has emerged as a promising paradigm for intelligent front-end engineering. However, existing MLLMs often exhibit significant limitations when applied to real-world webpages with complex layouts and diverse visual styles, including code compilation failures, severe layout misalignments. A key reason for these issues lies in the lack of structured, human-like cognitive processes—namely, the “perceive first, then generate” paradigm commonly followed by human developers. To address this gap, we propose a reinforcement learning framework that explicitly enhances the model’s reasoning ability prior to code generation. Specifically, we introduce a structured layout reasoning stage and design a three-stage reward mechanism to supervise (i) the quality of layout reasoning, (ii) the accuracy of the generated code, and (iii) the consistency between the reasoning and the code. This reward formulation is designed to provide strong positive feedback from the reasoning process to the code generation outcome. To rigorously evaluate our approach, we construct and manually curate a benchmark consisting of 1,800 real-world webpages spanning multiple levels of layout complexity and visual detail. Experimental results demonstrate that our reasoning-enhanced method significantly improves the performance of the baseline model and achieves results comparable to or even surpassing much larger MLLMs baselines in terms of compilation success rate, layout fidelity, and styling accuracy.", "tldr": "", "keywords": ["Code Generation; Multimodal Application"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d4debc911d0a6f30d9b6515e10cc03559c04b1b6.pdf", "supplementary_material": "/attachment/886ef0861b64fc49d59efc6e2d3766b2a7c4902d.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes a training recipe for UI-to-Code tasks by introducing intermediate layout representations (wrapped in `<layout>` and `</layout>`) in addition to CoT and code outputs.\n\nThe paper introduces a tri-component reward which supervises (i) code accuracy, (ii) layout reasoning accuracy, and (iii) reasoning–code consistency. The reported results from 1.8K-page real-world benchmark (from Design2Code, IWBench, WebUIBench) shows performance gains of Qwen2.5-VL-7B finetuned with RL on the proposed rewards compared with base model baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Novel consistency-gated tri-reward design that explicitly ties reasoning fidelity to code execution; structured `<think>/<layout>/<answer>` design.\n\n- Structured parsing and comparisons of HTML DOM elements, which could be a more reliable and generalizable metrics than some of the existing metrics in prior benchmarks."}, "weaknesses": {"value": "- The paper has reproducibility issues. The authors suggested that prompting details, layout representations, and experiment details will be included in Appendix, but the Appendix is missing. There's no definitions or algorithms for visual similarity or the reward cost matrices. Evaluation dataset curation and quality-filtering are under documented.\n\n- The authors proposed a layout reasoning method by introducing `<layout>`, `</layout>` between the think and answer tags. However, the effectiveness of such approach is under justified: Table 1 shows Layout-CoT has mixed effects on model performance with no clear gains; and the authors have noted that the model tend to ignore the `<layout>` section during code generation, and hence the need for `R_consistency`. Without careful ablation studies against RL training with end-to-end generation, it is unclear whether the proposed layout reasoning module is beneficial or a distraction to the model's performance.\n\n- Limited evaluation scope: Only GPT-4o and Qwen-2.5-VL family, omits other strong MLLMs.\n\n- The paper asserts “Increasing model size does not guarantee better performance” and attributes 72B’s underperformance to weaker layout understanding, but offers no failure analysis or diagnostics to substantiate these hypotheses. Similarly, the discussion around Layout-CoT in section 3.3 remains speculative.\n\n- The authors' claim on the curation of \"the largest benchmark for real-world webpage code generation\" is an overstatement. The 1.8k benchmark is a cleaned merge of three well-established benchmarks (Design2Code, IWBench, WebUIBench); transparency on inclusion/exclusion rules is limited.\n\n- Presentation quality: Numerous typos and formatting glitches reduce clarity."}, "questions": {"value": "- Please provide the missing appendix with: exact prompt templates and tag schema, full cost matrix formulas, training hyperparameters/seeds, and dataset curation criteria + licensing.\n\n- Can the authors provide an ablation study on whether proposed `<layout>` stage is necessary for performance improvements?\n\n- Please means + std over multiple seeds and significance test. Are the comparisons robust to sampling noise? Do the metric differences align with human preferences/judgments?\n\n- Please broaden the evaluation to additional strong MLLMs/systems (and, if feasible, non-Tailwind targets such as React).\n\n- Can the authors provide some analysis on common failure modes of Qwen-2.5 72B? When would larger models tend to fail while smaller models do not?\n\n- Please proof read the manuscript and address typos & formatting issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fdUMHb1LSA", "forum": "NDlnDvGD7e", "replyto": "NDlnDvGD7e", "signatures": ["ICLR.cc/2026/Conference/Submission25107/Reviewer_Wy2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25107/Reviewer_Wy2m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635863244, "cdate": 1761635863244, "tmdate": 1762943327492, "mdate": 1762943327492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "7qd1F8wKSQ", "forum": "NDlnDvGD7e", "replyto": "NDlnDvGD7e", "signatures": ["ICLR.cc/2026/Conference/Submission25107/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25107/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763294625966, "cdate": 1763294625966, "tmdate": 1763294625966, "mdate": 1763294625966, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes thinking before coding for WebUI code generation, where authors use GRPO to train models to think about the layout, etc, before generating code, and design specific code accuracy reward, layout reasoning reward, and consistency reward. Evaluation on 1800 examples shows improvement over prompting baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors construct a 1800 example benchmark for webpage code generation.\n2. By training on 450 webpages, the authors improve the performance of Qwen2.5-VL_7B to be on par with larger models."}, "weaknesses": {"value": "1. The thinking-before-coding seems already a common pattern for frontier models like GPT-5 and Claude, which limits the novelty and impact of this work.\n2. The description and examples of the collected datasets are limited.\n3. There seem to be a lot of detailed design choices in the reward, which are not fully justified.\n4. How are the used metrics/rewards correlated with human annotators?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zJ4RxBbfLf", "forum": "NDlnDvGD7e", "replyto": "NDlnDvGD7e", "signatures": ["ICLR.cc/2026/Conference/Submission25107/Reviewer_H2HU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25107/Reviewer_H2HU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868883483, "cdate": 1761868883483, "tmdate": 1762943327302, "mdate": 1762943327302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a three-part reward function (based on layout reasoning, code correctness, and consistency between reasoning and generated code) to enhance MLLMs for front-end UI code generation. They fine-tune Qwen2.5-VL-7B-Instruct using GRPO on 450 training samples over 5 epochs. A new benchmark is introduced by filtering high-quality samples from Design3Code, IWBench, and WebUIBench, resulting in roughly 1,800 samples. The authors compare their trained model against other capacities of Qwen2.5-VL-Instruct models and GPT-4o using various prompting strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Framing the task completion process as a sequence of reasoning, layout generation, and then code generation is a great idea and aligns well with how MLLMs can structure complex outputs.\n- The three-part reward design is interesting and well-motivated, especially the use of the consistency reward R_{cons} as a gating mechanism for the accuracy reward R_{acc}."}, "weaknesses": {"value": "Although the idea is good, I believe the experimental section lacks sufficient baselines and prior benchmarks and important details, which limits transparency and the strength of the comparisons. Including more results from previous literature would provide better context. Here's a break down of the points:\n\n- The related work section does not mention several relevant recent benchmarks (such as WebMMU [1]), which are important for situating the contribution.\n\n- The baseline coverage is limited. Evaluating the model on Design2Code, IWBench, and WebUIBench would allow for fairer comparisons with both open- and closed-source models beyond Qwen2.5-VL-Instruct. Training on Qwen2.5-VL-3B and also including other model families like InternVL2.5, InternVL2, or Gemini in the comparisons would further strengthen the analysis.\n\n- The description of the RL training dataset lacks transparency. The authors state that it includes “450 high-quality, diverse examples, synthesized by a large pretrained model and rigorously filtered by human experts. These examples cover a wide range of canonical front-end code generation scenarios,” but they do not specify which model was used or define what constitutes “high-quality” and “diverse” or \"wide-range\".  \n\n---\n\nMinor Errors:\n- \\citep{} should be used instead of \\citet{} in multiple places, e.g:\n- Line 038 typo: generationPlaat et al. (2024);\n- Line 061 typo: reasoningLiu et al. \n- Line 073 benchmarkSi et al. \n- Missing citations: Line 193 Hungarian algorithm ? \n- Misleading colors in Tab 1, could have green for the \"improvements\" and red for the \"decreases\" in performance. \n- In Table 1, the reported delta for CER between Qwen2.5-VL-7B-RL (0.9) and Qwen2.5-VL-7B (1.9) should be 1.0, not 0.7, indicating a larger improvement than reported.\n\n\n[1] WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation"}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bbxRphuWUa", "forum": "NDlnDvGD7e", "replyto": "NDlnDvGD7e", "signatures": ["ICLR.cc/2026/Conference/Submission25107/Reviewer_vQid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25107/Reviewer_vQid"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949502212, "cdate": 1761949502212, "tmdate": 1762943327073, "mdate": 1762943327073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an RL training method for the Design2Code task. The VLM is prompted to include free-form reasoning in <think></think> tags, layout summarization in <layout></layout> tags, and finally the generated code in <answer></answer> tags in the output. \n\nThe corresponding reward function has two parts:\n1. The code accuracy reward computes the match between the DOM elements of the rendered webpage and the oracle webpage \n2. The layout reasoning reward computes the similarity between the predicted layout regions and the oracle regions \n3. The consistency reward computes the overlap between the layout section and the code section of the model output\n\nThe combined reward is then optimized through GRPO. By doing RL on Qwen2.5-VL-7B, the performance can exceed the non-finetuned models and even match GPT-4o."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong empirical results on the Design2Code task. \n\n- The method is quite straightforward and intuitive."}, "weaknesses": {"value": "- I'm not quite satisfied with your ablations. Can you add some ablations on the importance of incorporating each reward component? What happens if you do not include the consistency reward? And what happens if you ignore the layout reasoning altogether and only include the actual code and compute the code accuracy reward? I think such ablations are important to justify your final reward design. \n\n- It'll be nice to include some qualitative examples of model generations before and after RL to illustrate the benefit from doing DL."}, "questions": {"value": "- Missing reference on line 193/193 (\"Hungarian algorithm ?\"). \n\n- The citation format is off for WrbSight (line 289/290)\n\n- Wrong citation on 296/297 (Design2Code should be Si et al.?); also wrong citation formats in this line."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vx6KaDq1Fx", "forum": "NDlnDvGD7e", "replyto": "NDlnDvGD7e", "signatures": ["ICLR.cc/2026/Conference/Submission25107/Reviewer_Pv2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25107/Reviewer_Pv2q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106558045, "cdate": 1762106558045, "tmdate": 1762943326792, "mdate": 1762943326792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}