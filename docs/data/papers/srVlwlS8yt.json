{"id": "srVlwlS8yt", "number": 10550, "cdate": 1758175289524, "mdate": 1759897643917, "content": {"title": "Diversity-Guided MLP Pruning for Efficient Large Vision Transformers", "abstract": "Transformer models achieve excellent scaling property, where the performance is improved with the increment of model capacity. However, large-scale model parameters lead to an unaffordable cost of computing and memory. We analyze popular transformer architectures and find that multilayer perceptron (MLP) modules take up the majority of model parameters.\nTo this end, we focus on the recoverability of the compressed models and propose a Diversity-Guided MLP Pruning (DGMR) method to significantly reduce the parameters of large vision transformers with only negligible performance degradation. Specifically, we conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons of MLP hidden layer, while preserving weight diversity for better performance recover during distillation. Compared to the model trained from scratch, our pruned model only requires 0.06% data of LAION-2B (for the training of large vision transformers) without labels (ImageNet-1K) to recover the original performance. Experimental results on several state-of-the-art large vision transformers demonstrate that our method achieves a more than 57.0% parameter and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B), our method accomplishes a 71.5% parameter and FLOPs reduction without performance degradation. The source code and trained weights will be publicly available.", "tldr": "", "keywords": ["large vision transformer", "mlp pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee3c4efb4c62d7c9333f2a92f92f1b238b8a90c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Diversity-Guided MLP Pruning (DGMP) for post-training pruning of vision transformers. The method prunes hidden neurons of MLP layers in transformers iteratively by ranking the neuron weight norms, while additionally applying Gram-Schmidt iteration to preserve the independent component of remaining neurons. The method is followed by a knowledge distillation stage to recover the performance. Experiments on several state-of-the-art vision transformersshow that the proposed method achieves more than 57% parameter and FLOPs reductions in a lossless manner."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is easy to follow and well-organized.\n2. The work shows solid results on vision transformers."}, "weaknesses": {"value": "1. **Limited comparison with strong structured-pruning baselines.** The main concern of the paper is that there are many recent strong structured pruning baselines for general transformer pruning that are highly relevant and should be discussed or compared in dicussion or experiments, for example:\n- Ma, Xinyin, Gongfan Fang, and Xinchao Wang. \"Llm-pruner: On the structural pruning of large language models.\" Advances in neural information processing systems 36 (2023): 21702-21720.\n-  Wang, Yuxin, et al. \"CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information.\" Proceedings of the 31st International Conference on Computational Linguistics. 2025.\n- Li, Guangyan, Yongqiang Tang, and Wensheng Zhang. \"LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models.\" Forty-first International Conference on Machine Learning.\n- Ling, Gui, Ziyang Wang, and Qingwen Liu. \"Slimgpt: Layer-wise structured pruning for large language models.\" Advances in Neural Information Processing Systems 37 (2024): 107112-107137.\n- An, Yongqi, et al. \"Fluctuation-based adaptive structured pruning for large language models.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 10. 2024.\nTo validate the performance of the proposed method, some of these methods should be included.\n2. **Limited experients beyond vision-only cases.** Another concern is that the method is specifically designed and only tested on vision transformer. Since structured pruning is a general architecture-level technique, one would expect DGMP to also benefit other transformer modalities, such as pure text-based transformers. Even a small experiment on text-based LLM would significantly improve the paper’s general relevance and demonstrate that the proposed method is a universal principle rather than a ViT-specific heuristic."}, "questions": {"value": "Can the authors provide detailed experiment/discussion comparison with the mentioned structured-pruning baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iw6dtuhdDd", "forum": "srVlwlS8yt", "replyto": "srVlwlS8yt", "signatures": ["ICLR.cc/2026/Conference/Submission10550/Reviewer_Yd2M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10550/Reviewer_Yd2M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761505481437, "cdate": 1761505481437, "tmdate": 1762921828335, "mdate": 1762921828335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Diversity-Guided MLP Pruning (DGMP) to compress large-scale Vision Transformer (ViT) models by focusing on the MLP layers, which occupy most parameters in ViTs. Motivated by the observation that redundant neurons can be represented by a combination of a few principal neurons, DGMP evaluates neuron importance using the L2 norm, then iteratively selects diverse neurons using the Gram-Schmidt process to maximize diversity. This approach avoids redundancy and improves recoverability of the pruned model without requiring gradient-based optimization or iterative fine-tuning. To restore performance, DGMP leverages knowledge distillation with both class token loss $\\mathcal L_{\\rm cls}$ and patch token loss $\\mathcal L_{\\rm patch}$ to guide the training of the pruned model. Extensive experiments show that DGMP achieves significant parameter and FLOPs reduction with minimal or no accuracy degradation. Notably, on zero-shot image classification and zero-shot retrieval tasks, the pruned models even outperform the original ones in some cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated by the insight that preserving neuron diversity improves the recoverability of pruned models.\n\n2. Extensive experiments validate the effectiveness of the proposed method across multiple large-scale models (e.g., OpenCLIP, EVA-CLIP, InternVL-C, DINOv2) and tasks (zero-shot classification, retrieval, kNN evaluation), consistently showing strong performance with minimal accuracy drop.\n\n3. The pruning algorithm avoids expensive gradient computations and iterative fine-tuning, making it practical and scalable for very large models."}, "weaknesses": {"value": "1. Neuron importance is estimated based solely on $W_{\\text{hidden}}$.\nIt would be interesting to consider whether incorporating the output-side weights $W_{\\text{output}}$ could further improve the pruning strategy, as some neurons might contribute more strongly to downstream layers than their input-side connectivity would suggest.\n\n2. Knowledge distillation may still require substantial fine-tuning.\nAlthough the method avoids iterative pruning and gradient-based ranking, the distillation stage involves multiple epochs of training, which could be viewed as a form of fine-tuning. A clarification on how this differs from traditional pruning–fine-tuning pipelines would be helpful.\n\n3. Unclear whether the method is specifically tailored for large-scale ViTs.\nThe paper emphasizes large vision transformers, but the method appears general enough to apply to smaller models as well. Including results on standard models like ViT-B or ImageNet-1K classification would help clarify its broader applicability.\n\n4. Comparison baselines could be updated.\nIn Section 4.4, the pruning baselines are relatively older. Including more recent or stronger pruning baselines—especially those proposed in the last year—would strengthen the experimental comparisons.\n\n5. Fairness of baseline application may be questionable.\nThe pruning baselines are applied only to the MLP layers, even for methods like NViT that are designed for global pruning. A justification or sensitivity analysis on this design choice would improve fairness and transparency.\n\n6. Minor technical inaccuracies and typographical errors.\nThere are a few minor inconsistencies in the writing that could be clarified or corrected.\nFor example, on page 5, the sentence “Then, we select the next neuron by Table 1 and ~” seems to incorrectly reference Table 1, which presents experimental results, rather than Eq. (1), which defines the selection criterion.\nAdditionally, the Gram-Schmidt algorithm is repeatedly misspelled as “Gram-Schmid” throughout the paper."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YDAHMGpWhj", "forum": "srVlwlS8yt", "replyto": "srVlwlS8yt", "signatures": ["ICLR.cc/2026/Conference/Submission10550/Reviewer_JbNE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10550/Reviewer_JbNE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550383512, "cdate": 1761550383512, "tmdate": 1762921827998, "mdate": 1762921827998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on pruning Vision Transformer models, particularly large ViTs, to achieve a better boundary of the trade-off between model parameters and model performances. Specifically, this paper targets the high hidden dimensions (expansion ratio) in the MLP block of each Transformer layer and proposes a pruning method named DGMR inspired by Gram Schmit orthogonalization along with a distillation to recover the model capacity. Experiments on ImageNet zero-shot classification and image-text retrieval shows the effectiveness of DGMR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method seems simple and easy to implement on different types of Vision Transformer models. The proposed DGMR may have large potential on many large ViTs to slim their computation.\n\n2. The experimental results on zero-shot classification and multimodal retrieval are significant and convincing, demonstrating the effectiveness of the method.\n\n3. The figure is clean and cool to understand the method."}, "weaknesses": {"value": "1. The concept of \"diversity\" may not be appropriate in this paper. The authors do not give a well-defined formulation of the diversity mentioned in the paper and the \"guidance\" of diversity is not elaborated in the method section. If the paper regards the diversity as the rank or the dimension of the linear space of the projection matrix between input and hidden neurons, there hold an implicit hypothesis that a higher linear space dimension would lead to a higher feature diversity for the hidden neurons. However, the effect of the non-linear activation function after this projection is not clear whether it could increase or decrease the diversity. The detailed investigation and analysis of this issue are also missing in the paper, which is essential to navigating the problem and justifying the proposed DGMR pruning method. This absence makes the paper less well-motivated and less generalizable.\n\n2. Although the paper exhibits many impressive experimental results on CLIP zero-shot image classification, image-text / text-image retrieval, and k-NN representation learning, results of pruned Large ViTs on Vision Language Models (VLMs) or Multimodal Large Language Models are omitted. Since Large ViTs usually serve the vision encoders in VLMs, whose performance is critical to many complicated VLM tasks, therefore they have larger impacts.\n\n3. The distillation after pruning is a common practice among pruning methods, which does not contribute enough to the novelty. Table 6 shows that the model would fail to adapt to the tasks after pruning MLP neurons using DGMR without the post-pruning distillation stage. This result may imply that the distillation process is the key component in the whole pipeline of the method, which has not been fully investigated. The author should provide an analysis of hyperparameters including distillation epochs, substituting to different teachers with the same hidden dimensions, or just fine-tuning on the image-text pertaining datasets. If the paper insists on distillation as a major contribution, there should be more explanations and analyses of why it works best for the proposed DGMR method.\n\n4. The abstract and introduction writing needs some improvements, e.g., the necessity of Gram Schmit strategy is not well established in the abstract, the logic of the introduction section is not clear, filled with many \"due to\" and \"afterward\", which makes the paper hard to follow."}, "questions": {"value": "I have no other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U4WojKxQtE", "forum": "srVlwlS8yt", "replyto": "srVlwlS8yt", "signatures": ["ICLR.cc/2026/Conference/Submission10550/Reviewer_4Eir"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10550/Reviewer_4Eir"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160793987, "cdate": 1762160793987, "tmdate": 1762921827475, "mdate": 1762921827475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes parameter pruning for the MLP module, which dominates the model parameters in the transformer architecture. This pruning significantly reduces the number of parameters while incurring only a negligible performance loss. Specifically, the authors propose a Diversity-Guided MLP Pruning method, employing a Gram-Schmidt weight pruning strategy to eliminate redundant neurons in the MLP hidden layers and maintain weight diversity to improve performance recovery during the pruning process. Finally, extensive experiments validate the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly and logically presents its innovative aspects to the reader."}, "weaknesses": {"value": "The key innovation of this paper lies in the Gram-Schmidt weight pruning strategy. However, the analysis of this part in the experimental evaluation is still not comprehensive enough."}, "questions": {"value": "1. The paper points out that the weight parameters of the MLP module in the transformer architecture model are dominant. However, the experiments only validated the method on ViT. To demonstrate the generality of the DGMP method, larger transformer architecture models such as LLama-7B and DeepSeek can be used for validation.\n2. Although the authors claim that their approach does not require iterative fine-tuning, the knowledge distillation stage still relies on the computationally expensive teacher model. It would be valuable to further analyze and compare the influence of iterative fine-tuning and knowledge distillation on the DGMP pruning framework, particularly in terms of convergence efficiency, performance recovery, and the overall trade-off between computational cost and model fidelity.\n3. In the ablation experiments, the pruning comparison method presented in the paper is somewhat outdated, and the accuracy evaluation dimensions are relatively singular."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OnpTmbCmL3", "forum": "srVlwlS8yt", "replyto": "srVlwlS8yt", "signatures": ["ICLR.cc/2026/Conference/Submission10550/Reviewer_ToHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10550/Reviewer_ToHJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762775435252, "cdate": 1762775435252, "tmdate": 1762921826845, "mdate": 1762921826845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}