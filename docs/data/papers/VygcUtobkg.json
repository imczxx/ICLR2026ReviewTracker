{"id": "VygcUtobkg", "number": 9044, "cdate": 1758108447563, "mdate": 1759897746623, "content": {"title": "MoSSDA: A Semi-Supervised Domain Adaptation Framework for Multivariate Time-Series Classification using Momentum Encoder", "abstract": "Deep learning has emerged as the most promising approach in various fields; however, when the distributions of training and test data are different (domain shift), the performance of deep learning models can degrade. Semi-supervised domain adaptation (SSDA) is a major approach for addressing this issue, assuming that a fully labeled training set (source domain) is available, but the test set (target domain) provides labels only for a small subset. In this study, we propose a novel two-step momentum encoder-utilized SSDA framework, MoSSDA, for multivariate time-series classification. Time series data are highly sensitive to noise, and sequential dependencies cause domain shifts resulting in critical performance degradation. To obtain a robust, domain-invariant and class-discriminative representation, MoSSDA employs a domain-invariant encoder to learn features from both source and target domains. Subsequently, the learned features are fed to a mixup-enhanced positive contrastive module consisting of an online momentum encoder. The final classifier is trained with learned features that exhibit consistency and discriminability with limited labeled target domain data, without input-level time-series augmentations.We applied a two-stage process by separating the gradient flow between the encoders and the classifier to obtain rich and complex representations. Through extensive experiments on six diverse datasets, MoSSDA achieved state-of-the-art performance for three different backbones and various unlabeled ratios in the target domain data. The Ablation study confirms that each module, including two-stage learning, is effective in improving the performance.", "tldr": "", "keywords": ["Domain Adaptation", "Semi Supervised Learning", "Time Series", "Classification"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a1b306371f860ae6207e7a5ca63b7b6c3f43f9b.pdf", "supplementary_material": "/attachment/28e28888957aaadb1d3afd7fb55979156bcccec4.zip"}, "replies": [{"content": {"summary": {"value": "Thank you for the opportunity to review this paper. This paper proposes MoSSDA, a two-stage semi-supervised domain adaptation (SSDA) framework for multivariate time-series classification that tackles domain shift when only a few target labels are available. In the first stage, it learns domain-invariant and class-discriminative representations using a domain-invariant encoder with Maximum Mean Discrepancy (MMD) loss and a positive contrastive module combining mixup and a momentum encoder for stable, consistent feature learning. In the second stage, a classifier is trained on the frozen features to prevent conflicting objectives. Evaluated on six benchmark datasets and multiple backbones, MoSSDA achieves state-of-the-art accuracy and robustness even with 95% unlabeled target data, outperforming existing SSDA methods. Ablation studies show that the contrastive module and two-stage design are key to its success."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem is important.\n2. The proposed framework seems reasonable.\n3. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The presentations and novelty can be illustrated better.\n2. Some literatures are missing.\n3. Comparison methods may not be up-to-date."}, "questions": {"value": "This paper tackles an important problem in semi-supervised domain adaptation for time-series classification, and the proposed framework seems promising based on the reported results. However, several aspects could be improved to make the contribution clearer and more convincing.\n\n1. Some parts of the introduction and methodology are difficult to follow. It’s not clearly explained why domain shift and violations of the i.i.d. assumption degrade performance, or what specific challenge this work aims to solve beyond what has already been addressed in prior domain adaptation research. Since domain shift is a well-studied issue, readers may find it hard to understand what is truly new here. The proposed “domain-invariant” framework sounds similar to many existing methods, and the paper should emphasize more clearly what the technical novelty is. It would also help to discuss whether the approach could generalize beyond classification to other time-series tasks such as forecasting or anomaly detection.\n\n2. The related-work section misses several recent and relevant papers. In particular, there has been active research on source-free and multi-source domain adaptation for time-series data. Examples include:\n\n   Time-series domain adaptation via sparse associative structure alignment: Learning invariance and variance (Neural Networks, 2024)\n\n   Source-free domain adaptation with temporal imputation for time series data (KDD 2023)\n\n   POND: Multi-source time series domain adaptation with information-aware prompt tuning (KDD 2024)\n\n     It would strengthen the paper if the authors could discuss these works and explain whether they could be applied to the problem setting considered here. Ideally, a comparison with some of these recent methods would also be discussed.\n\n3. Most of the comparison methods are from before 2023. Given how quickly the area is evolving, it would be good to include more recent state-of-the-art baselines to better assess the performance of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CL2CQcHJMq", "forum": "VygcUtobkg", "replyto": "VygcUtobkg", "signatures": ["ICLR.cc/2026/Conference/Submission9044/Reviewer_ALKM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9044/Reviewer_ALKM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589744254, "cdate": 1761589744254, "tmdate": 1762920759409, "mdate": 1762920759409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces MoSSDA, a method for domain adaptation of time series data in the semi-supervised setting. The authors use various methods including MoCo-like contrastive learning, MMD loss, and mix-up to boost performance on this task for downstream classification. They show that their method significantly outperforms baselines in a variety of settings, and through ablations, they establish the importance of each component of their model. Overall, this work introduces a novel method for domain adaptation in time series that can be applied on a variety of downstream datasets with many types of architectures."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The components of this method are well-motivated based on the problem the authors present. Each component seems to have a separate but important place that meets the challenges of domain adaptation for time series.\n2. The benchmarking results are very impressive compared to baselines, showing substantial improvements in this setting across multiple datasets.\n3. This method demonstrates how a combination of methods that have been tested extensively in other research domains, such as mix-up, MoCo, and MMD loss, can result in a method that’s performative in time series domain adaptation. \n4. The appendix contains a plethora of experiments on their method, and extensive ablations are conducted in order to understand each model component and how the approach performs on different base architectures. This helps greatly in understanding the contributions of this method."}, "weaknesses": {"value": "1. The proposed method is only applicable in somewhat limited settings for domain adaptation. The motivation for self-supervised DA is clear, but the authors only consider the case where label spaces of both source and target domain are known to be entirely overlapping. This is a major weakness as prevailing methods in domain adaptation consider the unsupervised domain adaptation setting where some labels in the target domain might not be known when training the model.\n2. The method uses MMD as the main domain adaptation loss. Follow-ups to the MMD have been proposed such as the Sinkhorn divergence (see RAINCOAT, He et al., 2023). \n3. In terms of novelty, this method is more of a combination of various methods from other fields rather than a central new method for domain adaptation. The authors motivate the transfer of these components well, but the novelty comes more from the combination of the methods rather than a central new method.\n4. The mix-up process seems to be quite restrained by the availability of same-class target samples at training. Since mix-up is only performed between samples in the same class to enable the supervised contrastive learning, this requires that there are a sufficient number of samples in the training set of the target domain such that you can combine them in various ways to get mixed-up samples."}, "questions": {"value": "1. Could this method easily be extended to account for non-identical label sets in source and target domains?\n2. The authors claim that certain time series augmentation techniques were needed for the baseline methods in this work, but they claim that such augmentations introduce biases and implausible transformations into the data. Did they test which type of augmentations change the performance of the baseline methods? This is not required on a large scale but should at least be commented on.\n3. In the appendix, results across different unlabeled ratios and architectures show that removing the contrastive loss results in near-identical classification metrics for each dataset (Tables S4, S5, S6, S7). Why is this? Does this amount to the model assigning one label to all samples? Comparing this to a naive majority-class classifier could help answer this question. If so, is the method’s central contribution the supervised contrastive learning?\n4. Is the momentum encoder needed here? What happens if authors replace the stop-grad and momentum encoder with a regular contrastive learning setup where the encoder is used on both sides and optimized for all samples?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Codebase submission is a Google Drive link and is not anonymized. Also no LLM statement is included (but ok since this isn't necessarily required if authors didn't use them)."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qNgNWF7g99", "forum": "VygcUtobkg", "replyto": "VygcUtobkg", "signatures": ["ICLR.cc/2026/Conference/Submission9044/Reviewer_ae3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9044/Reviewer_ae3v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841855163, "cdate": 1761841855163, "tmdate": 1762920758995, "mdate": 1762920758995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper a two-stage decoupled training framework to handle the SSDA problem in time series."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. The two-stage decoupled training framework proposed in this paper is an effective strategy for handling complex multi-objective optimization problems, avoiding gradient conflicts between feature extraction and classifier training, which makes significant contributions to the optimization stability of existing SSDA methods.\n2. The paper achieves a clever application of Mixup in SSDA by applying it in the feature space rather than the input space, enhancing class discriminability with limited target domain label data while avoiding the potential damage to critical temporal sequential features that traditional input-level augmentation might cause, demonstrating a profound understanding of time series data characteristics.\n3. Modular Design and Robustness: The modules of MoSSDA are well-designed with high coupling and each performs its specific function, respectively solving domain invariance, class discriminability, and feature consistency issues. Through extensive experiments, the effectiveness of MoSSDA has been validated."}, "weaknesses": {"value": "Weaknesses：\n1. The original theoretical foundation of Mixup lies in encouraging the model to perform linear interpolation between training samples to enhance the model's generalization ability. This paper applies it in the feature space combined with contrastive learning. Is the theoretical effectiveness of this feature space Mixup still equivalent to that of input space Mixup? For the virtual positive samples generated by feature space Mixup in the feature space of time series, can they truly represent meaningful, intra-class compact features? Or, does this linear interpolation introduce unnecessary noise or bias in the nonlinear feature space, especially when the target domain labels are extremely sparse? This point may require some analysis by the authors. Also, the usage of mixup in feature space is not new to the community.\n2. The paper employs a linear kernel in the MMD loss and mentions that the framework allows for replacing it with other kernels such as RBF. However, the performance of MMD is highly sensitive to the choice of kernel function. For complex multivariate time series features, the measurement capability of a linear kernel may be insufficient to capture domain shifts in high-dimensional nonlinear feature spaces. The authors should experimentally compare the performance of the linear kernel with other more complex kernel functions, such as RBF or kernels specifically designed for time series, in the MMD loss. If experiments have been conducted, it should be clarified whether, after comparison, the linear kernel achieves SOTA performance in the feature space of this paper.\n3. The overall loss function of MoSSDA includes two weight hyperparameters, and this multi-objective optimization problem may be more sensitive to hyperparameter settings. The paper lacks sufficient analysis of this issue and needs to further demonstrate the robustness of these two weights under different datasets and label ratios.\n4. The momentum encoder aims to maintain feature consistency. However, in the SSDA setting, the target domain data may contain domain-specific features, and the momentum update mechanism may excessively smooth out the unique and valuable fine-grained features specific to the target domain. The ablation study section of the paper does not demonstrate the impact of the momentum encoder on target domain-specific features. It is recommended that the paper supplement this part to make the experimental setup more comprehensive.\n5. The framework proposed in the paper is based on offline two-stage training. In some industrial application scenarios, such as sensor monitoring and equipment fault diagnosis, target domain data is continuously streamed, and the model may need to possess online or incremental adaptation capabilities. Some arguments about how the MoSSDA framework adapts to data streams online can be supplemented.\n6. As SSDA is a very old area in machine learning. It is important to clarify what are the special designs in time series domain. Can the proposed method be used in general SSDA setting?"}, "questions": {"value": "As in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qrhDCrkH1G", "forum": "VygcUtobkg", "replyto": "VygcUtobkg", "signatures": ["ICLR.cc/2026/Conference/Submission9044/Reviewer_pSYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9044/Reviewer_pSYM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937765536, "cdate": 1761937765536, "tmdate": 1762920758600, "mdate": 1762920758600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MoSSDA (Momentum encoder-utilized Semi-Supervised Domain Adaptation), a two-stage framework for addressing domain shift in multivariate time-series classification when limited labeled target data is available. The key innovation is combining a domain-invariant encoder, a mixup-enhanced positive contrastive module with momentum encoding, and a decoupled training strategy to learn robust representations without requiring input-level augmentations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated and structured\n2. Comprehensive experiments: The evaluation spans 6 diverse datasets, 3 backbone architectures, and multiple unlabeled ratios (0.7, 0.9, 0.95), demonstrating consistent improvements over strong baselines. Authors also provide a ablation study, which clearly demonstrate the contribution of each component, with the positive contrastive module showing the most significant impact."}, "weaknesses": {"value": "Limited novelty. The combination of MMD loss for domain alignment, mixup-enhanced contrastive learning, and momentum encoding is creative and well-justified for time-series data where traditional augmentations can disrupt temporal dependencies. While the combination is novel, the individual components (MMD loss, contrastive learning, momentum encoding) are well-established techniques. The main contribution is their integration for time-series SSDA."}, "questions": {"value": "1. The claim about creating \"rich representations without input-level augmentations\" is misleading since mixup is itself a form of augmentation, just in feature space rather than input space.\n2. The paper doesn't discuss potential failure modes or limitations of the approach.\n3. The MMD loss uses a linear kernel by default, which may not be sufficient for complex distribution differences. why this simple kernel would work for all datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EFZkAwU1y8", "forum": "VygcUtobkg", "replyto": "VygcUtobkg", "signatures": ["ICLR.cc/2026/Conference/Submission9044/Reviewer_BkaN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9044/Reviewer_BkaN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975347718, "cdate": 1761975347718, "tmdate": 1762920758312, "mdate": 1762920758312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}