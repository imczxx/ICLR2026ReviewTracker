{"id": "ew4ILUkTWw", "number": 22458, "cdate": 1758331319499, "mdate": 1759896864886, "content": {"title": "Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces", "abstract": "An ultrametric space or infinity-metric space is defined by a dissimilarity function that satisfies a strong triangle inequality in which every side of a triangle is not larger than the larger of the other two. \nWe show that search in ultrametric spaces has worst-case logarithmic complexity. \nSince datasets of interest are not ultrametric in general, we employ a projection operator that transforms an arbitrary dissimilarity function into an ultrametric space while preserving nearest neighbors. \nWe further learn an approximation of this projection operator to efficiently compute ultrametric distances between query points and points in the dataset. \nWe proceed to solve a more general problem in which we consider projections in $q$-metric spaces -- in which triangle sides raised to the power of $q$ are smaller than the sum of the $q$-powers of the other two. \nNotice that the use of learned approximations of projected $q$-metric distances renders the search pipeline approximate. \nWe show in experiments that increasing values of $q$ result in faster search but lower recall. \nOverall, search in q-metric spaces is competitive with existing search methods.", "tldr": "Infinity Embeddings learn vectors that preserve semantic embedding quality while enforcing geometry for fast search, enabling nearest-neighbor retrieval with fewer comparisons.", "keywords": ["Nearest-neighbor search; similarity search; metric trees (VP-trees); triangle inequality; generalized metrics (\\(q\\)-metric); high-dimensional data; logarithmic complexity; vector search; text retrieval; image retrieval; dissimilarity measures."], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63d5d4e2998fdcc077d12d46cdc37a091e01f6e7.pdf", "supplementary_material": "/attachment/41fc69daa5a427f4741f14790f97e2aad1898ea8.zip"}, "replies": [{"content": {"summary": {"value": "Authors propose to learn a function to map vector embeddings into separate embeddings such that their Euclidean distances estimate the distances of the q-metric projections of the original embedding. The advantage of doing so is that a q-metric can approximate an ultrametric and with the ultrametric k-NN search is solvable in logarithmic time using a VP-tree. Authors also claim to support transformation for arbitrary similarities, but the provided code supports only L_2 (with possible extensions to p-norms). \n\nThe idea seems to be very appealing. However, vector spaces do not seem to have non-trivial ultrametric distances. Thus, intuitively, the closer we approximate an ultrametric, the more degenerate/trivial such approximation should become. \n\nAs much as I like the idea, I find parts of the paper impenetrable. For example, in L103 authors start talking about graphs without properly defining them. Moreover, graphs are the central part of the section 3, which I could not understand. \n\nHowever, I read the code, and it is relatively straightforward: training a projection neural network, embed data, and search it using a VP-tree. In that, the code has multiple serious issues (see the questions) and I am rather confident that it cannot work properly.  I hope authors can clarify this by answering some of my questions.\n\nMoreover, despite author’s claims of integrating with ANN-benchmark, they have a standalone pipeline which raises questions about the accuracy of the evaluation (in fact it does not compute recall or something like rank mean difference)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. An interesting idea. \n2. Strong **reported** results. \n3. Code is provided."}, "weaknesses": {"value": "Please see the summary. Basically, I do not think the code is doing what it is supposed to do and the evaluation is likely incorrect. \n\n**Detailed comments:** \n\nL031 Hanov does not seem to be an appropriate reference for this statement. These are more appropriate classic references: \n\n1. Kevin S. Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. When is ”nearest neighbor” meaningful? 1999. \n\nSame mistake in L738: the classic VP-tree references: \n\n   * Jeffrey K. Uhlmann. 1999. Satisfying general proximity/similarity queries with metric trees \n    * Stephen M Omohundro. Five balltree construction algorithms, 1989. ICSI Technical Report TR-89-063, \n\n2. Roger Weber, Hans-Jorg Schek, and Stephen Blott. A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces.  VLDB’98, \n\nL053 However, the data is what the data is and most problems in vector search involve dissimilarity functions -> This is an old reference to back up a claim about most problems.  \n\nL059-060  \n> In this context, it follows that the q-norm of a path is the q-root of the sum of the distances in each hop elevated to the power of q \n\nIt is completely unclear. The paragraph needs to be rewritten for clarity. \n\nSection 3.1 and the part of section 3 before 3.1 are not understandable, in particular, because they talk about graphs that aren’t defined."}, "questions": {"value": "**L194-196** \nDo you have examples of non-trivial ultrametric vector spaces? I think these don’t exist. Thus, Theorem 1 does not apply to real world cases. \n\n**L103**  \n> defines a fully connected weighted graph G= (X,D) \n** L214-215 ** \n> We use the projection operator Pq to process the dataset G = (X,D) to produce a graph Gq = (X,Dq) such that any three points x,y,z ∈X satisfy the q-triangle inequality. \n\nWhat are exactly these graphs? \n\n**L488** Why is your evaluation code is not interoperable with ANN-benchmarks? It is clearly a standalone evaluation toolkit. \n \n1. Why is your evaluation code is not interoperable with ANN-benchmarks? It is clearly a standalone evaluation toolkit.  \n\n2. Does your own code really compute recall or **relative position error**? \n\n**Why does loss in section 4** not match the actual loss in the code?\n\nThere are multiple issues with the code. In each epoch you take the whole dataset for one loss and a batch for another, which is very non-standard, and I doubt it works.\nMore importantly, the second loss computation can't be correct in my opinion for several (not just a single reason).\n\nSee comments below:\n``` \n D0 = torch.cdist(X, X, p=2)\n M = self.fermat_gpu_exact(D0, q)\n M = (M - M.min())/(M.max()-M.min())\n# This computers an all-to-all matrix of distances not pairwise distances!\ndef emb_dist(a, b=None): return torch.cdist(a, a if b is None else b, p=2)\n\n# All data points are embedded\nemb = model(X) \nD_emb = emb_dist(emb) \nmask = M.float() \n# This loss might be reasonable though I don't quite understand how M is computed\nloss_s = torch.sqrt(((D_emb - M)**2 * mask).sum() / mask.sum()) \nidx = torch.randint(0, n, (batch_size, 3), device=device) \ni,j,k = idx.t() \n# Problem 1: emb_dist does NOT compute pairwise distances.\n# Problem 2: There is no exponentiation to the power of q here.\n# Problem 3: If you computed pairwise distances correctly, due to the triangle inequality,\n# d(i,j) + d(j, k) is always >= dist(i,k)\n# You pass it through RELU and get something (mostly positive)\n# Thus you will PENALIZE your embeddings for actually satisfying the triangle inequality\nraw = emb_dist(emb[i], emb[j]) + emb_dist(emb[j], emb[k]) - emb_dist(emb[i], emb[k]) \nloss_t = F.relu(raw).min() \n``` \n\nShouldn’t run_batch_query accept embeddings as the first argument? \n``` \n\ndef run_batch_query(self): \n\nreturn self.index.search_batch(1, self.kk, self.queries_np, self.queries_np, False) \n\n``` \nIn fact, this function eventually call search_batch, which reads embedding vectors from the same location as original vectors, but uses different offsets (since embedding dimensions is different): \n``` \n\ninline std::vector<std::vector<int>> \n\nVpTree::search_batch(int k,int topk,const float* qE,const float* qR,std::size_t N,bool retDist) const { \n  std::vector<std::vector<int>> all(N); \n  int dE=embed_.dim, dR=real_.dim; \n\n  for(size_t i=0;i<N;++i) { \n    auto r = search_core(k, qE+i*dE, qR+i*dR, retDist); \n    if ((int)r.ids.size() > topk) \n      r.ids.resize(topk); \n    all[i] = std::move(r.ids); \n  } \n  return all; \n} \n```"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f3OL5Nlhm2", "forum": "ew4ILUkTWw", "replyto": "ew4ILUkTWw", "signatures": ["ICLR.cc/2026/Conference/Submission22458/Reviewer_MHeW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22458/Reviewer_MHeW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137351081, "cdate": 1762137351081, "tmdate": 1762942226849, "mdate": 1762942226849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of nearest neighbor search (NNS) and proposes a solution based on metric embedding. In this solution, data samples are first projected into a q-metric space. A projection is then learned from these samples, and the dataset is transformed into the learned q-metric space to build a VP-tree. Finally, NNS can be performed using this VP-tree."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "+ The paper studies a meaningful problem.  \n+ Experiments are conducted on benchmark datasets.  \n+ The proposed solution is supported by some theoretical analysis."}, "weaknesses": {"value": "+ The motivation is unconvincing.  \n+ The technical novelty is limited.  \n+ The proposed solution has several (potential) limitations.  \n+ The evaluations are incomplete.  \n+ The related work is not comprehensive enough and should not be deferred to the appendix."}, "questions": {"value": "Q1. The motivation for the proposed method is unclear. Given that existing solutions like HNSW already achieve $O(logn)$ time complexity for Approximate NNS (ANNS), the necessity of a new approach is not justified. This is especially true when the proposed solution does not improve upon this complexity and introduces several potential limitations.\n\nQ2. The technical novelty of the proposed method is limited for several reasons.\n+ The theoretical foundation heavily overlaps with prior work. The main theorems (e.g., Theorem 2) and the key idea of using a q-metric VP-tree are very similar to those established in the 2015 paper \"Metric Representations of Network Data.\"\n+  Beyond this closely related work, metric embedding has been commonly used in ANNS. There are other options, such as the Hierarchically Well-Separated Tree (HST). It is unclear why such solutions are not considered or why the proposed method is superior.\n+ It offers no complexity improvement. The method does not advance the state-of-the-art in worst-case time complexity, which remains $O(logn)$.\n\nQ3. The proposed solution is designed for a static environment. The approach does not account for dynamic data updates, a critical requirement in real-world scenarios. Besides, if the similarity function does not have to be a metric, this assumption should be removed. \n\nQ4. The evaluations are incomplete and should be detailed from the following aspects:\n+ The implementation details of the proposed solution are missing. The parameter configurations of the baselines are also missing.\n+ The experimental environment is not reported.\n+ The data scalability is unclear. The Deep1B dataset is considered in the experiment, but it is unclear whether all one billion data points are used.\n+ I could not find the result of the proposed solution in Figure 6.\n+ It is unclear whether the QPS measurement includes the query pre-processing step.\n+ As shown in Figure 6, recall is used to represent search accuracy, but the text description is different: \"Search accuracy is measured using Rank Order.\" This is inconsistent.\n\nQ5. The authors are suggested to review and compare more recent work on ANNS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wi2MMnzPlw", "forum": "ew4ILUkTWw", "replyto": "ew4ILUkTWw", "signatures": ["ICLR.cc/2026/Conference/Submission22458/Reviewer_Hq62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22458/Reviewer_Hq62"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189891318, "cdate": 1762189891318, "tmdate": 1762942226144, "mdate": 1762942226144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel perspective on designing algorithms for nearest neighbor search. The main idea is to first map the points into a q-metric space. As q approaches infinity, the authors prove that a traversal on the constructed VP-tree can guarantee finding the nearest neighbor in O(log n) steps. In addition, the authors show that such a q-metric space theoretically exists and that it can be approximated by training an MLP. They perform experiments demonstrating that their MLP model can approximate the q-metric distance and achieves higher QPS compared with other popular ANN algorithms on several datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Compared with traditional hash-based or graph-based algorithms for ANN, the idea of mapping points into a q-metric space sounds very novel and worth exploring.\n2. The authors provide a theoretical guarantee for their search algorithm as q approaches infinity."}, "weaknesses": {"value": "1. I am curious how long it takes to calculate Dq for any pair of points from the dataset X. I didn’t find any reported time for training the MLP model or building the index. Appendix D.3 states that they can speed it up from O(n^3) to O(nkl), but I don’t quite understand this, since the output distance matrix would still be at least O(n^2). This seems impractical for any dataset larger than one million points.\n\n2. The authors prove that under the q-metric space, the original nearest neighbor is also the nearest neighbor (line 280). However, many non-nearest neighbors could also achieve the same minimum value. How can one determine which point is the true nearest neighbor in the original space?\n\n3.The authors report much higher QPS compared with other popular ANN algorithms in Figure 6. I have a question: what is the embedded dimension s (line 299) for your trained embedding model? I am concerned that the model may pre-compress the vectors, which would make distance computation much faster. If that is the case, note that many other techniques—such as dimensionality reduction and product quantization—could also be applied to achieve similar purpose.\n\n4. In the leftmost plot of Figure 5, it seems that the dashed line grows linearly with data size. Is that correct?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RqnrrcKN4h", "forum": "ew4ILUkTWw", "replyto": "ew4ILUkTWw", "signatures": ["ICLR.cc/2026/Conference/Submission22458/Reviewer_RvcF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22458/Reviewer_RvcF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762227504242, "cdate": 1762227504242, "tmdate": 1762942225581, "mdate": 1762942225581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Infinity Search, a theoretically principled framework for approximate nearest neighbor search in general dissimilarity spaces via projections onto $q$-metric (and ultrametric) spaces. The approach is technically strong, offering provable guarantees on distance preservation and logarithmic query complexity, and includes a learnable projection that adapts $q$ to data geometry. Experiments show competitive recall and efficiency compared to HNSW and ScaNN. \nWhile promising, the method’s computational overhead in learning and applying the projection could limit scalability on very large datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces a novel framework that generalizes ANN search to arbitrary dissimilarity measures through mathematically grounded projections onto $q$-metric spaces.\n\n- Provides strong theoretical guarantees on distance preservation and query complexity.\n\n- Demonstrates competitive empirical performance against HNSW and ScaNN, showing both scalability and adaptability through a learnable projection mechanism."}, "weaknesses": {"value": "- Infinity Search relies heavily on learning accurate projections into $q$-metric spaces, but the paper does not analyze how projection errors affect theoretical guarantees or retrieval accuracy. \n\n- The computational complexity of training and applying the learned projection is high, and scalability to billion-scale or streaming datasets remains unclear.\n\n- The approach assumes the projected space preserves neighborhood structure globally, yet real data often violate the $q$-metric inequalities, potentially degrading search efficiency.\n\n- The limitations are briefly acknowledged but not examined, suggesting that the practical constraints and real-world failure cases have not been clearly articulated or fully understood by the authors."}, "questions": {"value": "1. What stability guarantees hold if the projected distances include noise $\\varepsilon_{x,y}$—i.e., under what bounds on $\\|\\varepsilon\\|$ is the nearest-neighbor identity preserved?\n\n2. Can the logarithmic complexity bound be expressed explicitly in terms of $q$, doubling dimension, and projection distortion, and is it asymptotically tight?\n\n3. How do the full build and query costs compare to HNSW and ScaNN, and how does projection error quantitatively affect accuracy and efficiency in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oZ1HBUorUD", "forum": "ew4ILUkTWw", "replyto": "ew4ILUkTWw", "signatures": ["ICLR.cc/2026/Conference/Submission22458/Reviewer_Jkxe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22458/Reviewer_Jkxe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762337898449, "cdate": 1762337898449, "tmdate": 1762942225275, "mdate": 1762942225275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers nearest neighbor search in ultrametric spaces (and more generally, spaces with stronger triangle inequalities than the standard one). It also asks whether it is possible to speed up search in other metric spaces by embedding data points into an ultrametric space. These are both very interesting directions.\n\nThere seem to be significant errors in the correctness of the algorithm/analysis. I do not recommend acceptance. However, I do think there is something worth saying here, once the authors can work out the theoretical bugs. Furthermore, the writing is quite difficult to parse. It would be nice if the authors can also work on presentation. See more below. I hope the authors will revise and submit to a later conference."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper explores a very interesting direction, extending nearest neighbor search from spaces with standard metrics to those with stronger triangle inequalities. For example, nearest neighbor search in ultrametric spaces seem to be particularly nice, as ultrametric balls are either nested or disjoint. The ideas are natural and simple, and the algorithms may be nice contributions to practical, approximate nearest neighbor search."}, "weaknesses": {"value": "- Lemma 1 is not correct. The proof only shows that both conditions are not true at the same time. But, they can both be false at the same time. For example, let $u = v$ and $\\mu < d(x_o, u)$.\n- Algorithm 3 is also not correct. Construct the following with six data points with two very salient clusters: {A, B, C} and {U, V, W}. Let distances within each cluster be 1 and distances between clusters be 100. Let the query point Q be at distance 10 from points in {A, B, C} and distance 100 from points in {U, V, W}. If the first vantage point is a point in {A, B, C}, then it does not seem like Algorithm 3 will continue to search in this cluster, and thus fail to find a nearest neighbor.\n- In Algorithm 3, the base case of the recursive algorithm is not defined. That is, the nearest neighbor is never assigned.\n- This is also perhaps a more minor oversight: In the VP tree construction, what happens if there are distance ties, and the median does not split data in half?\n- The writing is also not very clear. The introduction is especially hard to parse. For example: \"These spaces satisfy more restrictive triangle inequalities in which the q-th power of each side of a triangle is smaller than the sum of the qth powers of the other two sides\" is much harder to understand than equation (4). In fact, there seems to be a lot of conceptual overlap between the introduction and section 2, except that the intro rewrites the math in section 2 in words.\n\nMinor typos I caught along the way:\n- Ln 191: Appendix Appendix\n- Ln 782: Lemma 1 yields a disjoint partition: if max { ??? }\n- Algorithm 3: the arguments in the recursive calls to the algorithm seem to be mis-ordered (also, is $\\tau$ initialized to $+\\infty$?)"}, "questions": {"value": "Please let me know if I have significantly misunderstood your paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eT36LILk5w", "forum": "ew4ILUkTWw", "replyto": "ew4ILUkTWw", "signatures": ["ICLR.cc/2026/Conference/Submission22458/Reviewer_hjKD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22458/Reviewer_hjKD"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762376648633, "cdate": 1762376648633, "tmdate": 1762942225050, "mdate": 1762942225050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}