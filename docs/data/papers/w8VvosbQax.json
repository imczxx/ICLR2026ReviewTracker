{"id": "w8VvosbQax", "number": 21505, "cdate": 1758318329380, "mdate": 1759896918593, "content": {"title": "On the Significance of Softmax Geometry: Interpretability and Token Decoding", "abstract": "Large language models represent their internal state as high-dimensional vectors. Many tasks of practical interest require measuring similarity between these vectors. Usually, this similarity is measured with a Euclidean notion. Recent work has argued that Euclidean geometry is ill-matched to semantic structure represented by LLMs. However, it's unclear whether this mismatch actually has practical consequences. In this paper, we study the practical effect of the similarity measure in the softmax layer of large language models (where the geometry is best understood). We consider two tasks: (1) learning interpretable features using sparse autoencoders, and (2) efficiently finding the most probable next tokens given a context. In both cases, we find that using the correct geometry dramatically improves the performance.", "tldr": "This paper studies the geometry induced by softmax and demonstrates its practical benefits in interpretability and top-k prediction.", "keywords": ["Embedding geometry", "Softmax", "Next-token prediction"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18330f2f150f2bf876a74af4a60aaaac40998567.pdf", "supplementary_material": "/attachment/93e1965a40ebc5a9451e28073920c3413d512fd3.zip"}, "replies": [{"content": {"summary": {"value": "This work studies the geometry underlying the softmax layer in large language models and argues that interpretability and retrieval should be done in the unembedding space equipped with the softmax-induced (whitened) inner product, rather than the standard Euclidean metric. The authors first show that training a sparse autoencoder (SAE) on whitened unembedding vectors yields latent features that are substantially more semantically coherent than those learned under Euclidean reconstruction, as judged by GPT-4. They then leverage the same geometry to build a token retrieval mechanism: instead of computing the full softmax over the entire vocabulary, they use the dual map $\\phi(\\lambda)$ to transform the model‚Äôs context embedding into unembedding space and retrieve likely next tokens via sparse SAE feature codes. Because the exact dual map requires a full softmax, the authors train a small MLP to approximate it. Finally, they show that this geometry-aware SAE retrieval recovers significantly more next-token probability mass than LSH baselines (even when LSH is performed in whitened space), and yields text generation quality comparable to standard greedy decoding."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The ideas presented in this work are novel and the inclusion of concrete examples strengthens the motivation for the proposed approach. The work effectively argues that operating in a geometry-aligned unembedding space can provide more interpretable latent features, and the qualitative illustrations (e.g., Figure 2) help justify this perspective. The observations made here also point toward a practical use case: leveraging these more interpretable representations for token retrieval during generation. Overall, the paper makes a clear case that geometry-aware representations can be both conceptually meaningful and potentially useful for improving decoding behavior."}, "weaknesses": {"value": "- **W1)** A primary weakness of the paper is the clarity and organization of the exposition. The narrative is relatively strong up to the introduction of the dual map, but after this point the presentation becomes fragmented. The paper spends considerable space comparing Euclidean LSH, whitened LSH, and LSH under exact versus approximate dual mappings before introducing the SAE-based retrieval mechanism, which appears to be the method the authors ultimately intend to advocate. This emphasis on LSH variants feels out of proportion to their conceptual importance and reads more like an extended ablation study rather than a central contribution. As a result, the core idea‚Äîgeometry-aware SAE-based retrieval using the approximate dual map‚Äîgets diluted and arrives later than it should. A clearer structure would have foregrounded the SAE-based retrieval method and used the LSH comparisons only as supporting evidence, rather than as a major narrative focus.\n\n- **W2)** Additionally, I am not fully convinced that the decoding component warrants the level of emphasis it receives in the paper. Even for the SAE-based retrieval method, the central claimed benefit appears to be computational efficiency‚Äîspecifically, avoiding the full vocabulary matrix multiplication during softmax. However, the paper does not provide any runtime measurements, latency comparisons, or FLOP estimates to demonstrate that the proposed retrieval actually results in meaningful speedups in practice. Without such evidence, it is difficult to evaluate whether this portion of the work represents a substantive contribution or merely a conceptual possibility. If the decoding mechanism is meant to be a practical improvement, the absence of performance data is a major gap, and if not, the extensive focus on retrieval seems disproportionate relative to the interpretability contributions.\n\n- **W3)** Another concern is the limited scope of models and evaluation. The analysis is performed almost exclusively on Gemma models, and‚Äîas far as I can tell‚Äîthere is no ablation on other model families to demonstrate whether the observed geometric structure or the retrieval mechanism generalizes. In terms of evaluation, the primary metrics are probability-mass coverage and GPT-4-based qualitative judgments. While these are reasonable given the interpretability focus, some quantitative language modeling metrics (e.g., perplexity before and after applying the proposed retrieval mechanism) would provide a more grounded assessment of the impact on model performance. In addition, the comparison to standard decoding is done using greedy decoding only, which is known to underperform relative to widely used sampling strategies. A more fair comparison would involve top-\nùëò\nk or nucleus sampling for both the baseline and the proposed decoding scheme. Without these evaluations, it is difficult to assess whether the method preserves performance under commonly used inference settings. \n\n- **W4)** (Minor) On a more minor note, the presentation is often difficult to follow, largely due to the extended focus on retrieval experiments. It is frequently unclear which combination of mapping strategy (exact or approximate dual), whitening space, and hashing method is being used in each figure, and reconstructing the experimental setup for each comparison requires significant effort. This confusion seems to stem from the fact that several of these retrieval experiments function more as supplementary analyses than as core contributions. A tighter structure might place results such as Table 1 and Figure 3 in the appendix (with only high-level summaries in the main text), reserving the main narrative for the geometry-aware SAE method and its interpretation. As written, the appendix is occupied primarily by text generation examples, while much of the methodological clarification that would help the reader is left implicit in the main body."}, "questions": {"value": "**Q1)**: Could the authors clarify what specific practical advantage the proposed geometry-aware retrieval offers beyond conceptual interpretability? Is the main contribution intended to be interpretability, efficiency, or both?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8kLvoCSRtW", "forum": "w8VvosbQax", "replyto": "w8VvosbQax", "signatures": ["ICLR.cc/2026/Conference/Submission21505/Reviewer_NAoQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21505/Reviewer_NAoQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690254978, "cdate": 1761690254978, "tmdate": 1762941810218, "mdate": 1762941810218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds on the findings of Park et al. (2024), which argued that Euclidean metrics are not well-suited for analyzing language model embeddings and introduced the causal inner product, an inner product in the whitened coordinate space of embeddings, derived from and motivated by the softmax formulation. This work investigates the significance of this perspective on embedding geometry for both interpretability and efficiency.\n\nFirst, the authors train sparse autoencoders (SAEs) on the unembedding vectors using the causal inner product, showing that this yields more interpretable hidden representations compared to standard Euclidean-based SAEs used in prior work. They demonstrate this by listing the tokens with the highest activation values for each SAE dimension under both metrics, finding that the token sets are more semantically coherent when analyzed under the softmax-geometry view.\n\nThe paper then leverages this more interpretable SAE representation to approximate top-k next-token retrieval without computing logits over the full vocabulary. In this setup, the SAE encoding functions as a form of locality-sensitive hashing that proposes a small candidate set of likely tokens for a given context embedding, from which the nearest tokens are then selected. The authors compare their approach against explicit hashing and other sampling methods, showing that the causal-metric SAE achieves better next-token probability mass coverage or produces higher-quality candidate sets when evaluated using GPT as a judge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and clearly motivated. The authors present a logical progression of ideas leading to their proposed methodology, and the step-by-step explanations make the paper easy to follow.\n\nWhile the new geometry perspective is not new and builds on prior work, the paper is a nice follow-up on the benefits this perspective can offer. The experimental evidence is also generally well-aligned with the paper‚Äôs claims."}, "weaknesses": {"value": "While the paper is generally well-written and clear, certain implementation details and results could be discussed more thoroughly. I elaborate on some of these issues in the Questions section."}, "questions": {"value": "1. **Sec 3**: I have a few questions regarding terminology and implementation details of the SAE experiment:\n\n    (i) Could you clarify what the listed tokens represent for each activation in Figures S1‚ÄìS9? Are these the tokens that achieve the highest activation values at a specific feature dimension (regardless of which features are most active for them), or are they the tokens whose top-activated feature corresponds to that specific dimension?\n\n    (ii) In the caption of Figure 1, what exactly is meant by ‚ÄúSAE features\" and their relevance? Does this refer to the relevance of the token lists shown in Figures S1‚ÄìS9 to the target word?\n\n    (iii) Related to (ii), in the prompt template shown in Figure S10, do *response_a* and *response_b* refer to these token lists for each feature?\n\n2. **Fig 3**: Do you have any intuition for why increasing the number of hash functions and hash tables appears to have opposite effects on the probability mass coverage? \n\n3. **Sec 4.3**: \n\n    (i) In Step 3 of the top-k token approximation, are the selected tokens those that share the exact same set of non-zero entries, or are they chosen based on having overlapping active encoding dimensions with the context embedding? Related to this, in Fig 4, what defines the size of the candidate set? Is it the size of the tokens that have the same set of non-zero entries?  \n\n    (ii) Regarding the reported time complexity, does it account for the cost of searching over tokens that share the same non-zero entries, or only for computing the softmax over the reduced candidate set? Additionally, this approach likely introduces some extra memory cost (for example, storing the precomputed dictionary) as well as computational overhead for approximating the dual map and training the SAE. Could you also report/comment on these costs?\n\n5. **Fig 6**: How should this figure be interpreted? If ‚Äúwin‚Äù indicates cases where the SAE produces better next-token candidates, SAE would be better when the dark purple bars are larger than the orange ones, which does not appear to be the case. Could you clarify why you conclude that the performance of greedy decoding and your approximation approach are on par?\n\n6. **Minor**: The axis font sizes in figures are very small and difficult to read."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G07d9tkOe0", "forum": "w8VvosbQax", "replyto": "w8VvosbQax", "signatures": ["ICLR.cc/2026/Conference/Submission21505/Reviewer_SAcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21505/Reviewer_SAcD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954065974, "cdate": 1761954065974, "tmdate": 1762941809939, "mdate": 1762941809939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the unembedding process in LLMs. By training SAEs they find applying a whitening transformation improves the interpretability of features in the embedding and unembedding vectors in line with previous work. The authors then introduce an approximation to the softmax decoding procedure leveraging SAE based locality sensitive hashing to reduce the complexity of the unembedding process while retaining performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This paper is well written and clearly articulates the work. Formalisations are clear and concise, making it straightforward for the reader to follow. The problem studied is of some interest given the general interest in accelerating decoding in LLMs."}, "weaknesses": {"value": "In section 3 the authors claim SAEs trained with the causal inner product produce more interpretable features. The evidence for this is qualitative, or based on showing the features to an LLM and having it judge. Neither of these are compelling sources of evidence. The qualitative analysis points to the appendix for an example relating to the word puppy, however this appears to be cherry-picked given the example on the next page (for queen) shows reasonable interpretable cluster for both methods. The LLM as judge results are a bit concerning - given this is a task about discerning how well grouped semantic concepts are it seems possible this may in fact reflect how well aligned the results are with the semantic concepts of the judge model. Neither of these evaluations seem particularly quantitive.\n\nLooking at the performance of the SAE decoding (figure 6) it is somewhat concerning the win rate for your procedure is lower than its win rate in every condition. To make claims about parity between approaches here some statistical testing may be helpful.\n\nFor the decoding results, the authors state that their approximation \"could offer significant computational benefits\" (line 484). It seems odd that the decoding results do not quantify the computational complexity of their approximation or empirically compare its efficiency with existing methods. Without comparing to a baseline it is difficult to evaluate the efficiency of this approximation.\n\nWithout this approach being clearly better in terms of win rate, or demonstrably faster, the novelty here appears limited. In terms of presentation, the paper would benefit form a proofread (e.g. line 356 misspelled autoencoder) and the plots in figure 2, 3, and 5 are difficult to read (the text size is quite small!)."}, "questions": {"value": "1. What is the quantitative evidence that your approach is faster, but equally as good as existing decoding techniques.\n\n2. Does applying a whitening transformation improve standard decoding?\n\n3. Have you tried this approach on any other model families (even if they're small models) to ensure it generalises?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJWeFtnxs5", "forum": "w8VvosbQax", "replyto": "w8VvosbQax", "signatures": ["ICLR.cc/2026/Conference/Submission21505/Reviewer_B91w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21505/Reviewer_B91w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961867702, "cdate": 1761961867702, "tmdate": 1762941809614, "mdate": 1762941809614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes to empirically evaluate the relevance of a similarity measure borrowed from Park et al (2024, 2025) as an alternative to the usual Euclidean distance or cosine distance, focusing on two tasks: learning interpretable features through sparse autoencoders and finding the $k$ most probable next tokens given a context embedding. The paper claims that using such a measure has a \"dramatic\" effect on the performance on both tasks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Studying LLMs from a geometrical perspective is a promising orientation in the field\n- The idea of addressing SAE as hash functions in the context of a study of LLM geometry is an interesting one\n- The manuscript is well-written"}, "weaknesses": {"value": "- Unclear presentation of technical details. In particular, LLMs is not a technical term, and it's not clear what specific architecture (LSTMs, transformers, SSMs) is being described, for instance, when it is claimed that $x$ is a context sentence that \"the LLM\" encodes as a vector. Another example, the vectors $\\lambda_x, \\gamma_y$ are said to \"not live in the same space\", while they both belong to $\\mathbb{R}^d$ (otherwise they couldn't be multiplied in the first place). One can guess that the manuscript means something different with \"space\" here (maybe that's what's intended with $\\Lambda$ and $\\Gamma$ being isomorphic to $\\mathbb{R}^d$?), but then it's not clear what these spaces are (the image of some map?) and how multiplication (say, a dot product) is defined over both. Figure 2 seems to give an intuition by showing a plot of PCA, but it is not clear how the differently colored dots rigorously (i.e., formally) define a space, let alone a \"subspace\", as claimed in the legend. It is also unclear why it is claimed that the fact that \"biproducts\" (dot products?) are invariant under some invertible linear transformation, while dot products between \"embeddings\" and between \"unembeddings\" are not, means that Euclidean geometry is not \"privileged\" by softmax distribution (what does \"privilege\" mean here, formally?). All these concepts could accept a rigorous definition and treatment, but they are treated here in a highly intuitive and sloppy way, which makes it difficult to assess the extent to which what is claimed has any solid basis.\n\n- Uncritical adoption of an alternative metric. The manuscript constantly refers to \"the correct geometry\" (\"natural\", \"better adapted to the structure of LLMs\") without providing any argument or explanation of why this is correct, or even correct with respect to what, and uncritically referring to and relying on Park et al (2024), where the measure is proposed and used for specific needs. In particular, referring to that line of work, it is said that what justifies the proposed metric is that it \"aligns well with the structure of semantics as understood by humans\", \"semantic structure\", and \"human intuition about semantic similarity\". I, personally, don't have that reading of that work, where I can't find anything related to human alignment, analysis of human intuition about semantic similarity, or serious engagement with semantic structure (other than orthogonality of concepts), and I have serious reservations with respect to the claims made in Park et al (2025) concerning semantic \"structure\". But this is not the place to judge that work. The point is that the present manuscript adopts them uncritically, probably extending the claims beyond what that work claimed or could deliver, to justify the work performed here without further analysis or elaboration. If these claims are set aside as unsubstantiated, the paper‚Äôs scope becomes substantially limited: it reduces to applying a single measure to two tasks. Even if the task evaluations were sound (see the next point), the contribution is, in my view, too incremental to warrant publication in this venue.\n\n- The evaluation of tasks is invalid. The manuscript claims that \"training sparse autoencoders with a causal inner product yields more interpretable features\". Yet, it is known that \"more interpretable\" is not a well-defined notion. To address this, the manuscript relies on subjective assessments of cherry-picked examples; for instance, representations of \"puppy\" are described as displaying a clear \"dog\" feature or as \"appearing nonsensical\". As an alternative \"quantitative evaluation\", the manuscript appeals to GPT-4o as a judge. Likewise, the manuscript evaluates the outputs of the SAE method as \"fluent and human-like\" by asking GPT-4o to \"please act as an impartial judge and evaluate the quality of snippets of paragraph generated by two language models\". All of which I consider radically unscientific. Incidentally, on the second task, transformations are not computed through the exact dual map, but through another map learned through an MLP, so in the end, it is even unclear what is it that is being evaluated."}, "questions": {"value": "- What is your justification for considering it as a rigorous scientific method to prompt chatGPT to please be an impartial judge?\n- In which sense is the measure proposed by Park et al (2024) \"the correct geometry\" (or \"natural\", or \"better adapted to the structure of LLMs\"?\n- What is your evidence for claiming that such a measure aligns with human intuition?\n- Can you provide a rigorous (formal) definition of what you mean by \"space\" and \"subspace\" and a rigorous account of how \"embeddings\" and \"unembeddings\" relate to it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qCIcUu7xQZ", "forum": "w8VvosbQax", "replyto": "w8VvosbQax", "signatures": ["ICLR.cc/2026/Conference/Submission21505/Reviewer_6quy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21505/Reviewer_6quy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992661422, "cdate": 1761992661422, "tmdate": 1762941808835, "mdate": 1762941808835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}