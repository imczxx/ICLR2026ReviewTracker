{"id": "61ZKpUeOIw", "number": 9646, "cdate": 1758132473556, "mdate": 1759897706977, "content": {"title": "MGHF: Multi-Granular High-Frequency Perceptual Loss for Image Super-Resolution", "abstract": "An avalanche of innovations in perceptual loss has advanced the super-resolution (SR) literature, enabling the synthesis of realistic and detailed high-resolution images. However, most of these approaches rely on convolutional neural network (CNN)-based non-homeomorphic transforms, which result in information loss during guidance and often necessitate complex architectures and training procedures. To address these limitations—particularly the information loss and unwanted harmonics introduced by CNNs—we propose a diffeomorphic transform–based variant of a computationally efficient invertible neural network (INN) for a naive Multi-Granular High-Frequency (MGHF-n) perceptual loss, trained on ImageNet. Building on this foundation, we extend the framework into a comprehensive variant (MGHF-c) that integrates multiple constraints to preserve, prioritize, and regularize information across several aspects: texture and style preservation, content fidelity, regional detail preservation, and joint content–style regularization. Information is prioritized through adaptive entropy-based pruning and reweighting of INN features, while a content–style consistency regularizer regulates excessive texture generation and ensures content fidelity. To capture intricate local details, we further introduce modulated PatchNCE on INN features as a local information preservation (LIP) objective. As another thread in the tapestry, we present the theoretical foundation, showing that (1) the LIP objective compels the SR network to maximize the mutual information between super-resolved and ground-truth modalities, and (2) a diffeomorphic transform–based perceptual loss enables more effective learning of the ground-truth distribution manifold compared to non-homeomorphic counterparts. Empirical results demonstrate that the proposed MGHF objective substantially improves both GAN- and diffusion-based SR algorithms across multiple evaluation metrics, and the code will be released publicly after the review process.", "tldr": "Multi-Granular High-Frequency Perceptual Loss for Image Super-Resolution", "keywords": ["Super-Resolution", "Perceptual Loss", "Invertible Neural Network", "Modulated PatchNCE", "Homeomorphism"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6893d0758a57cc2ee3a6c41ec68cdc64e080db62.pdf", "supplementary_material": "/attachment/f5777e7598fdb0af82c96325c27062bcf58fb325.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a Multi-Granular High-Frequency (MGHF) perceptual loss framework designed to overcome information loss and artifacts common in CNN-based super-resolution methods. It replaces non-homeomorphic CNN transforms with diffeomorphic, invertible neural networks (INNs) to preserve information flow. Two variants are proposed: a basic MGHF-n, trained on ImageNet, and a comprehensive MGHF-c, which incorporates multiple constraints for texture, style, and content fidelity. The model employs entropy-based feature pruning, a content–style consistency regularizer, and a modulated PatchNCE-based local information preservation (LIP) objective to enhance fine details. Theoretical analysis shows that the LIP term maximizes mutual information between SR and ground-truth images, while diffeomorphic transforms enable better manifold learning. Experiments demonstrate that MGHF significantly improves both GAN- and diffusion-based SR models across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is quite dense and contains a substantial amount of material."}, "weaknesses": {"value": "I found the way the paper was presented to be very confusing and unnecessarily complex, making it difficult to understand what was going on.\n\nFor example, why is the information loss and unwanted harmonics introduced by CNN a problem? How do they affect the results? Invertible neural network (INN) is lossless by definition, so why is it useful to include the very complex theorems? Why is it necessary to introduce diffeomorphism here? \n\nI'm not an expert on diffeomorphisms, and this paper is very confusing to read.\n\nTherefore, I feel that I do not have the expertise (diffeomorphisms) to assess this paper and suggest the AC to seek opinions from other reviewers."}, "questions": {"value": "As mentioned above, I feel that I do not have the expertise (diffeomorphisms) to assess this paper and suggest the AC to seek opinions from other reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "uxgNahrMd1", "forum": "61ZKpUeOIw", "replyto": "61ZKpUeOIw", "signatures": ["ICLR.cc/2026/Conference/Submission9646/Reviewer_RaEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9646/Reviewer_RaEh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761231149812, "cdate": 1761231149812, "tmdate": 1762921174366, "mdate": 1762921174366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Multi-Granular High-Frequency Perceptual Loss (MGHF) for image super-resolution (SR), leveraging invertible neural networks (INNs) to mitigate information loss and harmonic distortion inherent in conventional CNN-based perceptual losses. Two variants are introduced: MGHF-n, a naive INN-based perceptual loss, and MGHF-c, a comprehensive framework incorporating adaptive feature weighting, content-style consistency, and a local information preservation objective. The paper provides theoretical proofs supporting the superiority of diffeomorphic transforms over non-homeomorphic CNN transforms and demonstrates empirical improvements across multiple SR models and benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The design of MGHF-n and MGHF-c is technically sound and creative, combining INN-based feature extraction with entropy-based pruning, adaptive weighting, content-style consistency, and contrastive local information preservation (PatchNCE). The hierarchical integration of these components is well-motivated.\n2. The experiments cover a wide range of SR models (GANs, diffusion, transformers) and datasets (RealSR, DrealSR, DIV2K, etc.), using both reference (PSNR, SSIM, LPIPS) and non-reference metrics (CLIPIQA, MUSIQ, MANIQA). Ablation studies and robustness tests under various degradations and scaling factors further validate the method.\n3. The INN-based detail feature extractor (DFE) is shown to be significantly more parameter- and memory-efficient than VGG-based extractors (Table 5), making it practical for real-world SR pipelines.\n4. The paper includes feature map visualizations (Figure 3), output comparisons (Figures 8–9), and toy examples (Figure 11) that effectively illustrate the information-preserving properties of the proposed method."}, "weaknesses": {"value": "1. The writing is often dense and notation-heavy, making it difficult to follow, especially in Section 2 and the appendix. Key concepts (e.g., AWDFE, LIP) could be explained more clearly. The structure of the DFE and the exact role of each loss component could be better modularized and summarized.\n2. It is unclear whether the enhanced baselines (e.g., OSEDiff+MGHF-c) are trained from scratch or fine-tuned from pre-trained models. Training details (e.g., dataset splits, optimization settings) are also insufficiently described. \n3. More recent diffusion-based SR methods (e.g., SR3, CDM, LDM variants) are not included in the comparison.\n4. Despite emphasizing perceptual quality, the paper relies solely on automated metrics. A user study or human evaluation would strengthen the claims of visual improvement.\n5. The paper lacks a sensitivity analysis for hyperparameters such as  alpha etc. Their selection process and robustness are not discussed.\n6. There is no discussion of scenarios where MGHF may underperform, such as under extreme upscaling, domain shift, or adversarial corruptions."}, "questions": {"value": "1. Could you provide a direct ablation comparing MGHF-n (INN-based) with a VGG-based perceptual loss under the same settings, to isolate the benefit of the diffeomorphic transform?\n\n2. How sensitive is the method to the choice of hyperparameters (e.g., alpha number of pruned features)? Is there empirical evidence of robustness?\n\n3. Are there failure cases or limitations where MGHF does not perform well, such as with non-ImageNet data, extreme scaling factors, or specific real-world degradations?\n\n4. Could you provide more qualitative results or a user study to support the perceptual improvement claims? Have human evaluators been used?\n\n5. What is the rationale behind the pruning strategy in AWDFE? Is there a risk of losing important high-frequency details?\n\n6. Can you clarify whether the MGHF-enhanced models are trained from scratch or fine-tuned? Please provide more detailed training protocols."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8MLibipM9", "forum": "61ZKpUeOIw", "replyto": "61ZKpUeOIw", "signatures": ["ICLR.cc/2026/Conference/Submission9646/Reviewer_uLbA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9646/Reviewer_uLbA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873574892, "cdate": 1761873574892, "tmdate": 1762921174083, "mdate": 1762921174083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes MGHF (Multi Granduality High-Frequency) perceptual loss, aiming to improve conventional (CNN-based) perceptual losses. MGHF tackles information loss in conventional CNNs theoretically, and proposes to use INNs as a effective tool to achieve perfect information preservation. The naive version (MGHF-n) if further improved into a comprehensive variant (MGHF-c) by introducing constraints to improve preservation of texture, style, fidelity and details. Experimental results show that MGHF-c (and sometimes MGHF-n) often outperforms baselines in standard benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Utilizing INNs to improve conventional perceptual loss is interesting, and to the best of my knowledge, it is also novel.\n- MGHF often outperforms baseline methods in terms of quantitative evaluation."}, "weaknesses": {"value": "My primary concern is about the fundamental of this work, considering the **perception-distortion trade-off** and **information preservating with INNs**. Please counter-argue and provide according experimental results if necessary. \n\n---\n\n**Weakness1**\n\nI appreciate the effort of the authors'. However, I have doubt about the fundamental of this work. \nThe authors propose to use an INN as a tool to preserve all information (theoretically proven); thereby \"addressing the perception distortion trade-off (Line144)\". \n\nHowever, the reviewer would like to claim that \"perfect information preservation (the main contribution of this work)\" contrarily induces the perception distortion trade-off (contradiction to the aim of this work); not addressing it.\n\nThis claim is supported by proofs the authors have provided.\n- For instance, \"Proof D.6.3 [Perceptual Loss Optimality]\" concludes as \"minimizing perceptual loss with INN leads to minimum distortion\". However, regarding the PD trade-off [1] theory, this indicates that perceptual loss with INN (the main contribution of this work) is theoretically proven to lead to blur.\n- Also, \"Proof D.1\" considers perceptual superiority however discusses based on L2 risk (distortion). This again indicates that minimizing perceptual loss with INNs simply indicates minimizing L2 loss. Again, this must induce blur.\n- Also, \"Proof D.1\" concludes that \"perceptual loss is extactly identical to the true L2 loss\", again contradicting with authors aim.\n- Also, the \"Proof D.6.4\" indicates perfect frequency preservation. However, it is straightforward that when attempting to regress high frequency components that posses randomness, it must lead to blur [2].\n\nWhile the authors have claimed significant information loss of VGG as a limitation in perceptual guidance, I would like to argue that info loss is the key factor that enables \"perceptual\" guidance despite using the L2 regression form. If it did not have any info loss, the L2 regression form would have induced blur due to the PD trade-off. \n\nIn fact, I would like to argue that the authors are also fundamentally using losses that has information loss, despite aiming for perfect info preservation. For instance, Gram matrix removes all spatial relationship and only preserves inter-channel correlationship. Despite using a info preservation perfect INN, the actual loss (after the Gram matrix calculation) has info loss; contradicting with claims of the authors.\n\n\n[1] **(Please refer to the arxiv version)** Blau, Yochai, and Tomer Michaeli. \"The Perception-Distortion Tradeoff.\" arXiv preprint arXiv:1711.06077 (2017).\n\n[2] Lee, MinKyu, et al. \"Auto-Encoded Supervision for Perceptual Image Super-Resolution.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n---\n\n**Weakness2**\n\nI have strong doubts about the authors claim of significant information loss in VGG (in Figure 3). Specifically, considering that spatial resolutions are reduced within the VGG architecture, the reviewer has concerns about the methodology of mere (spatial) feature visualization to verify the information loss/preservation.\n\nAccordingly, while the authors have claimed significant information loss (e.g., chaotic information loss can be observed even in layer 3_3 in Figure 3), the reviewer would like to counter-argue that it is not true. \n\nRegarding to Fig.16 of DIP (Deep Image Prior) [3], most information can be reconstructed (with feature inversion) even from deep layers layers as layer 5_3 of VGG, and almost perfect reconstruction in layer 3_3 (while the authors show significant info loss). This indicates that most information (including low-level info) are preserved within the deep layer of VGG, which directly contradicts with the authors claim.\n\nOverall, the reviewer agrees that information loss in conventional CNNs (including VGG) do indeed happen (which I believe is actually a positive aspect, see Weakness1); the current work has issues in quantifying it. \n\n\n[3] **(Please refer to the arxiv version for Fig.16)** Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. \"Deep Image Prior.\" arXiv preprint arXiv:1711.10925 (2017).\n\n---\n\n**Weakness3**\n\nThis work needs a heavy revision regarding the presentation. \n- For instance, most parts in the Introduction section are simply listings of prior works, which should  belong to the Related Works section. I suggest aiming to provide more intuitions about the overall method and motivations of the authors.\n- Also, only the 1) final performance and 2) feature visualization experiments are in the main article. The reviewer strongly suggests to include important analyses that are currently in the appendix to the main article.\n- Additionally, the format is significantly altered (e.g., no spacing between paragraphs). While I appreciate the effort and understand the difficulties due to strict page limits, the current state of the formatting significantly limits readability."}, "questions": {"value": "Please refer to the **Weakness**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5gcgB28YBA", "forum": "61ZKpUeOIw", "replyto": "61ZKpUeOIw", "signatures": ["ICLR.cc/2026/Conference/Submission9646/Reviewer_sD3F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9646/Reviewer_sD3F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885001110, "cdate": 1761885001110, "tmdate": 1762921173769, "mdate": 1762921173769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}