{"id": "rEjWBDtxoK", "number": 7704, "cdate": 1758032876680, "mdate": 1763710041177, "content": {"title": "Computationally Efficient Graph Modelling with Refined Graph Random Features", "abstract": "We propose refined GRFs (GRFs++), a new class of Graph Random Features (GRFs) for efficient and accurate computations involving kernels defined on the nodes of a graph. GRFs++ resolve some of the long-standing limitations of regular GRFs, including difficulty modeling relationships between more distant nodes. They reduce dependence on sampling long graph random walks via a novel walk-stitching technique, concatenating several shorter walks without breaking unbiasedness. By applying these techniques, GRFs++ inherit the approximation quality provided by longer walks but with greater efficiency, trading sequential, inefficient sampling of a long walk for parallel computation of short walks and matrix-matrix multiplication. Furthermore, GRFs++ extend the simplistic GRFs walk termination mechanism (Bernoulli schemes with fixed halting probabilities) to a broader class of strategies, applying general distributions on the walks' lengths. This improves the approximation accuracy of graph kernels, without incurring extra computational cost. We provide empirical evaluations to showcase all our claims and complement our results with theoretical analysis.", "tldr": "Novel random walk-based algorithm to efficiently estimate graph node kernels", "keywords": ["Monte Carlo", "graph", "random walk", "kernel"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/962374874010560fc468fef4e3e9b471e4a57698.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors study the problem of graph random features as efficient and accurate graph kernels. Kernels are similarity functions defining relationships between graph nodes (or graphs themselves) of which the exact computation tends to be expensive. The idea of a graph random feature is to consider random walks in the graphs, where the length and number of such walks allow to make a statement about graph similarity. The key idea of this paper, is to improve the standard random walk procedure with a stitching operation and a termination criterion. This allows a improved approximation of the kernel, especially for distant nodes (demonstrated in a theoretical analysis, as well as experimentally), and reduces the computational cost."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The walk-stitching technique sounds very logical and straightforward, to the point that one may wonder why nobody has considered to this before. (As an outsider to this field, I wonder if indeed nobody has.)\n- The technical contributions come with a theoretical analysis.\n- The experiments are comprehensive, on both synthetic and real data, and with downstream applications."}, "weaknesses": {"value": "Major:\n- While the advantages of GRF++ are clear, the need for GRF(++) should be better motivated in the Introduction. It is unclear to which extend GRF are needed and used in the first place. This makes the importance of the contribution hard to assess.\n- The paper is sometimes hard to follow due to its structure. For instance, there is no clear narrative arc in Sec. 2.2.3. Same remark for Sec 3.\n- The stitching time shown in Fig. 5 is unclear. Why does standard GRF (which does not use stitching) have a non-zero stitching time? How does the stitching time reduce while the stitching degree increases? (cf. Q2). \n- Results in Appendix do not look as good as in the main paper, e.g., in Fig. 7 and 8 where the Frobenius norm error for higher degrees gets higher than for lower degrees, which seems to contradict Theorem 3.3. (cf. Q3) \n- There is no discussion on how to choose the degree l. (cf Q1)\n\nMinor:\n- Figure text size too small (e.g., Fig. 3, 4 and 5) \n- Algorithm 2 should at least be fully written in Appendix.\n- Undefined notation/typo \"R\" l. 039\n- Duplicated ref (Beutel et al., 2015a and 2015b)\n- References to datasets karate, dolphin, football and eurosis seem to be missing.\n- A large part of the references are unrelated (about graph modeling in anomaly detection, recommander system, and computational biology).\n- Fig. 6 should be a table.\n- The metric presented in Tab. 1 is not indicated in it nor in its label."}, "questions": {"value": "Q1: It seems that a higher degree l leads to a better approximation (Theorem 3.3.) and a lower computation time thanks to the parallelization and the reuse of the sub-walks (experimental results). Is there a point at which increasing the degree will result in less efficient computation due to stitching time? If so, how should l be chosen? If not, why should l not be chosen as high as the expected longest path? And why was l=2 chosen in the downstream applications?\nQ2: Please clarify the meaning of stitching time in Fig. 5.\nQ3: How do you intreprete the \"crossing lines\" in Fig. 7 and 8, which seem to contradict Theorem 3.3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2tNRQeNtj", "forum": "rEjWBDtxoK", "replyto": "rEjWBDtxoK", "signatures": ["ICLR.cc/2026/Conference/Submission7704/Reviewer_aor6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7704/Reviewer_aor6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972031918, "cdate": 1760972031918, "tmdate": 1762919763717, "mdate": 1762919763717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment"}, "comment": {"value": "We would like to sincerely thank the reviewers for the feedback and valuable comments. We address them in depth below. We have also made the following changes in the manuscript:\n\n1. We have added speed tests comparing GRF++ with brute force kernel computations for graphs of varying sizes (Fig.7). They confirm our earlier findings. GRFs provide up to **3x** speedup. \n2. We have added results regarding GRF++ based approximation for three other graph kernels, in addition to the previously considered diffusion kernel (Section B.6). We also provided information regarding corresponding modulation functions. These results confirm all our previous findings for the diffusion kernel. **GRFs++ provide consistent accuracy gains**. **These results provide empirical validation of our theoretical analysis, showing that GRFs++ are not tailored to a particular graph kernel**. \n3. We have added 4 more baselines to our clustering results (Table 9).\n4. **We have added 24 baselines to our graph classification results (Table 5, 6, 7)**. Moreover we have added graph classification experiments for two other datasets : Reddit-Multi-12k and Collab. GRFs++ provide strong performance, as compared to that updated comprehensive list of other methods. \n5. We have integrated our GRF++ into Vision Transformer (ViT) models in Section B.5 (by re-interpreting images as having a 2-dimensional grid-graph structure) and **showed improvements over baseline ViT on ImageNet**."}}, "id": "8vqGDLN4fV", "forum": "rEjWBDtxoK", "replyto": "rEjWBDtxoK", "signatures": ["ICLR.cc/2026/Conference/Submission7704/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7704/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7704/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763710213937, "cdate": 1763710213937, "tmdate": 1763710213937, "mdate": 1763710213937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an efficient extension to the graph kernel framework g-GRF (Reid et al., 2024b) by introducing a random walk-stitching technique that combines shorter walks to approximate longer ones. This approach aims to reduce the computational cost of graph node kernel computation while maintaining representational power. The authors provide a theoretical analysis of the proposed method, building upon the foundations of the original g-GRF framework. Empirical evaluations are conducted on graph classification and clustering tasks, demonstrating improved efficiency and competitive performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-organized and clearly written. The connection to the previous g-GRF framework is articulated in a way that makes the contribution easy to follow.\n- The introduced random walk-stitching mechanism offers a novel perspective on constructing longer random walks from shorter ones.\n- The paper presents a theoretical analysis grounded in the prior g-GRF formalism."}, "weaknesses": {"value": "- The paper provides a limited novelty. It extends the previously introduced g-GRF framework (Reid et al., 2024b) with a relatively incremental modification.\n- The experimental evaluation is limited in baseline coverage. For graph classification, a baseline and g-GRF are included; similarly, for clustering, no baseline beyond GRF is considered. Standard deviations are not reported\n- Although the method claims improved computational efficiency, the practical benefits are not convincingly demonstrated. Evaluations are conducted on small graphs."}, "questions": {"value": "- The evaluation currently relies on relatively small datasets and mainly compares against g-GRF variants. Would the proposed method maintain its efficiency and accuracy advantages on larger or more complex datasets?\n\n- The empirical results show that GRF++ outperforms GRF in most classification settings. Could the authors provide an intuition or theoretical justification for why the proposed random walk-stitching mechanism leads to such gains? Including or at least discussing scalability experiments on larger graphs would strengthen the empirical claims. Similarly, adding comparisons to more recent GNN-based baselines would better contextualize the contribution within current literature.\n- For the clustering experiments, it would be useful to include baseline methods beyond GRF and to report standard deviation or confidence interval values over multiple runs.\n\n**Additional comments:**\n- In Line 107, the Eq 2 cannot be convergent for any arbitrary $(\\alpha_k)_{k=0}$, for example, for divergent $\\alpha_k$ values."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FyculpJWu7", "forum": "rEjWBDtxoK", "replyto": "rEjWBDtxoK", "signatures": ["ICLR.cc/2026/Conference/Submission7704/Reviewer_B661"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7704/Reviewer_B661"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468337088, "cdate": 1761468337088, "tmdate": 1762919763338, "mdate": 1762919763338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GRFs++, a method to approximate a large family of kernels between nodes including the d-regularized Laplacian, diffusion process and p-step random walk kernel. The proposed method is based on GRFs, a method that approximates the kernels matrix as a product of two low rank matrices whose elements are obtained via random walks. GRFs++ deals with the limitations of GRFs, in particular its inability to model relationships between distant nodes and its walk termination mechanism. Experimental results show that GRFs++ are more accurate and more efficient than GRFs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed GRFs++ method is well-motivated, as it addresses certain limitations of the prior GRFs approach. Combining shorter walks to construct longer walks is an interesting and reasonable idea.\n\n- The method is supported by theoretical analysis. Among others, the authors show that increasing the walk-stitching degree in powers of 2 leads to improved approximation, which is an interesting result.\n\n- GRFs++ demonstrates superior empirical performance compared to GRFs, achieving lower approximation error and better results in downstream tasks such as in graph classification and node clustering."}, "weaknesses": {"value": "- My main concern with this paper is the significance of its contributions since the kernels between nodes that are approximated by the proposed GRFs++ are now rarely used in practice, having been largely replaced by GNNs that learn task-dependent features.\n\n- In line with the previous point, it appears that the kernels that are approximated by GRFs++ are not empirically strong and thus might not be useful in practice. For example, the classification accuracies that are illustrated in Figure 6 are not considered state-of-the-art. On ENZYMES, NCI1 and REDDIT-MULTI, the accuracies are significantly lower than those of standard GNNs that are evaluated under the same protocol [1]. \n\n- No speed comparison is provided between the proposed approximation method and the approach that computes the exact kernel. I would expect that, even for graphs with 500 nodes, computing the exact kernel is not particularly computationally intensive. Such a comparison is missing from the paper.\n\n- The proposed method is not compared against other types of approaches in the graph classification and node clustering tasks. For example, in the graph classification task, it could be compared against kernels between graphs and GNNs and in node clustering against spectral clustering, Louvain or some node embedding approach followed by k-means. More importantly, in terms of kernel approximation, it is only compared against GRFs. In my understanding, the well-known Nystrom method cannot be applied to this type of kernels. Is there any general approach that allows approximating this family of kernels?\n\n[1] Errica, F., Podda, M., Bacciu, D., & Micheli, A. A fair comparison of graph neural networks for graph classification. In ICLR'20."}, "questions": {"value": "Given that the proposed method approximates kernels between nodes, why isn't it evaluated in node classification tasks (Cora, Citeseer, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZuxQRq6goS", "forum": "rEjWBDtxoK", "replyto": "rEjWBDtxoK", "signatures": ["ICLR.cc/2026/Conference/Submission7704/Reviewer_r6Zd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7704/Reviewer_r6Zd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863256222, "cdate": 1761863256222, "tmdate": 1762919762851, "mdate": 1762919762851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes GRFs++, a refinement of GRFs for unbiased approximation of node-level graph kernels. Two main ideas are presented; Walk Stitching of degree l; and general termination distributions. Theoretical results include an unbiasedness lemma for the stitched estimator with general terminations; a compact MSE identity for l=2; and a monotone MSE improvement under standard termination. Empirically, for the diffusion kernel only, GRFs++ reduce Frobenius error vs GRFs across synthetic and real graphs, improves distant-pair estimates, benefits further from Poisson termination and shoes useful downstream performance on graph classificaiton, node clustering and 3D mesh normal prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(+) The observation that walk stitching corresponds to a 2l-fold de-convolution of the kernel's coefficient series is elegant and unifies construction.\n(+) The termination-distribution generalization is a clean application of Russian-roulette style debiasing. \n(+) From a parallelism & systems angle, short walks are easier to batch and parallelize on accelerators; stitching then composes them by MM products addressing a known pain point for GRFs where long RWs are inherently sequential."}, "weaknesses": {"value": "(-)  The method is presented as applicable to a broad family of kernels, yet all experiments are diffusion-kernel only. To support generality please add a few more kernel classes and show the de-convolution construction of f and corresponding results. Also prior GRF variants are cited but not used as baselines. Adding comparisons against them would better situate GRF++ among the state of the art. \n(-) Correctness issues: (i) Diffusion-kernel modulation f seems to be missing the lambda factor. In Sec 2.2.1 (in the paragraph starting at line 203) the paper states for diffusion kernels that the \"correct\" modulation is f(p) = 1/((2l)^p p!). Let G(x) = \\sum_k a_k x^k = exp(\\lambda x). For 2l-fold convolution we require F(x)^{2l} = G(x) ==>  F(x) = exp(\\lambda/(2l) x). Therefore f(p) = ((\\lambda/(2l))^p ) / p! . The missing \\lambda^p seems to me to be a correctness bug that propagates to all diffusion-kernel experiments, unless \\lambda was silently set to 1. Please fix the statement and clarify the value of \\lambda used in the experiments. (ii) the termination pseudocode likely contains a sign error (since s_m is a sampled maximum length the 4th bullet point should be terminated <-- I[walk_length >= s_m], instead of \"<=\"). (iii) In Lemma 3.2 in the derivation (step 5) uses \"symmetry of X_1 and X_2\" because \"the (i, j) entry of each matrix is a dot-product of the random feature vectors ϕf (i) and ϕf (j), corresponding to vertices i and j\". But with the factorization X_i = K_1^(i)(K_2)^(i) and independent K_1^(i), K_2^(i), X_i would not be generally symmetric. So... either define X_i as a gram matrix, or avoid the symmetry step. As written there seems to be a mismatch between the factorized form and the argument. Please clarify the exact objects and revise the proof accordingly. \n\nMinor issues/typos:\n- Duplicate reference to Beutel et al.\n- walk-stitchng --> walk-stitching (page 6, line 288)\n- standardize n vs N for sizes (page 5)"}, "questions": {"value": "Please provide answers to the correctness issues I list above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dxym3GVX28", "forum": "rEjWBDtxoK", "replyto": "rEjWBDtxoK", "signatures": ["ICLR.cc/2026/Conference/Submission7704/Reviewer_UD4V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7704/Reviewer_UD4V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017873289, "cdate": 1762017873289, "tmdate": 1762919762232, "mdate": 1762919762232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}