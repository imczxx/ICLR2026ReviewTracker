{"id": "xxsacQ3tdb", "number": 24338, "cdate": 1758355931993, "mdate": 1759896770730, "content": {"title": "Designing Rules to Pick a Rule: Aggregation by Consistency", "abstract": "Rank aggregation has critical applications for developing AI agents, as well as for evaluating them. However, different methods can give rise to significantly different aggregate rankings, impacting these applications. Indeed, work in social choice and statistics has produced many rank aggregation methods, each with its desirable properties, but also with its limitations. Given this trade-off, how do we decide which aggregation rule to use, _i.e._, what is a good _rule picking rule (RPR)_? In this paper, we design a data-driven RPR that identifies the best method for each dataset without assuming a generative model. The principle behind our RPR is to maximize consistency if the data collection process was repeated. We show that our method satisfies several consistency-related axioms failed by a wide class of natural RPRs. While we prove that the computational problem of maximizing consistency is hard, we provide a sampling-based implementation that is efficient in practice. We run this implementation on known statistical models to experimentally demonstrate its desirable properties, as well as on real-world data where our method provides important insights into how to improve consistency.", "tldr": "We give a novel framework for picking aggregation rules most appropriate for a given dataset, and design our own consistency-based rule picking rule.", "keywords": ["rank aggregation", "rule picking rules", "consistency"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e60f78d96b8bcf454d28b0a301f31c08a404794.pdf", "supplementary_material": "/attachment/4d7b97d1f828cbd7774f26913d5f84c19debf0f1.zip"}, "replies": [{"content": {"summary": {"value": "In all types of domains, individuals disagree. This disagreement is fundamental. As a result, rank aggregation rules must make tradeoffs, picking between different axioms that contradict each other to provide the best output. However, each methods tradeoffs mean that they are acceptable in different situations, and the correct choice of method will vary across domain and task. As a result, the authors here propose AbC (Aggregation by Consistency), a rule-picking rule, which provides a principled way to pick between existing methods. The authors propose a method of splitting data and comparing the consistency of given methods across those splits, allowing for the choice of a method well-suited for any given problem. The method splits voters (the authors explain how this can be used with incomplete preferences), and runs each chosen candidate rule on the split. They measure the Kendall-Tau of each method on each split, and pick the rule with the minimum expected disagreement. \n\nIn order to show the utility of their method, the authors explain what axioms their method satisfies, provide a few examples, and conclude with a set of experiments where they demonstrate AbC on a synthetic dataset, peer review dataset, and a F1 dataset."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*The problem the authors address is a fairly foundational one. Any given ranking rule must make choices on what to prioritize. These choices cannot generalize perfectly. A principled way of making that choice is a strong contribution and one that has a widespread impacts in many domains(as the authors note)\n\n*While I have not checked the author's math in detail, it appears to be well-formulated and correct. The appendix contains a great deal of rigorous proofs to back up the author's assertions.\n\n*The authors do a strong job not only in demonstrating the strengths and axioms their rules fulfill, but explain why they cannot fulfill the others. I did not find myself questioning any of those choices.\n\n*The authors are explicit about which axioms they satisfy( reversal symmetry, plurality-shuffling consistency, unconditional anonymity, and neutrality).\n\n*As an overall comment, I was satisfied with the writing of the paper, which was well-written and seemed to me very considered throughout."}, "weaknesses": {"value": "The piece that stood out to me were the figures in the paper. They did not add much clarity to me, and in many cases they added to my confusion. Understanding the content of one given figure involves inspecting the appendix. What's good and what isn't doesn't jump out, and the number of methods felt like clutter and made it difficult for me to really get a sense of how each method performed. The labels are small and hard to read, 2A in particular is hard to get a sense of."}, "questions": {"value": "One thing I wanted to ask the authors was about the use of 1/2 in place of a tie. Did you try any other values there, or measure any outcomes? In many settings such as RLHF, preferences are sparse. It does not seem unreasonable that one would want to avoid penalizing ties, or perhaps even the opposite."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PewdcBAQIF", "forum": "xxsacQ3tdb", "replyto": "xxsacQ3tdb", "signatures": ["ICLR.cc/2026/Conference/Submission24338/Reviewer_FWni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24338/Reviewer_FWni"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706110889, "cdate": 1761706110889, "tmdate": 1762943048263, "mdate": 1762943048263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a Rules to Pick a Rule framework: given a ranking profile, what is the ``best'' social choice function to make an aggregation? This paper proposes a surprisingly simple yet effective method: split the profile into two halves u.a.r, run SCF candidates on both halves, and pick the SCF where two outcomes agrees the most (i.e. smallest Kendall-Tau distance between two aggregated rankings). The basic idea is that a voting rule should be consistent among ranking data drawn from the same distribution. Theoretically, this paper considers a series of axioms where their consistence RPR satisfies or those nobody can do better. Empirically, the show the Monte-Carlo approximation of their RPR presents pretty good outcome on datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper is quite impressive to me. In the introduction, the paper exhibits a very strong connection with previous studies and place itself well among them. They also clearly justified how their key idea -- consistency -- is natural and intuitive in many interdiscinpline research topics. I don't check the proofs, but the theorems sounds reasonable at a high level. This paper give a reasonably significant conceptual contribution in rank aggregation with a suprisingly simple method, which I think is the most valuable part of this paper. Besides that, it has a thorough picture, from theory, to computation, and to emprical studies. The topic is also very related to ICLR."}, "weaknesses": {"value": "I am not 100% percent convinced by the idea of \"rules to pick a rule\". For me, it sounds perfectly ok to interpret it as a another type of rank aggregation rules (as the paper said, the rule RPR induces), which brings more explainability on \"we always align with (one of) the SCF with highest consistency\".  Specifically, given the method is profile-based, the rule elected is used solely for this data. What is the specific reason to motivate and interpret as a \"rule to pick a rule\"?\n\nAnother concern is that, I feel a little bit inconsistent on the motivation of this paper and the approach it adopts. Consider two area of works. (1) Data driven rank aggregation like preference learning and RLHF. In these scenarios, rankings are all datas, and what rules to pick wholely relies on your goal (like maximizing consistency or minimizing other loss functions). (2) Traditional social choice. In this area, \"rules to pick a rule\" based on voting profiles is very problematic as a strong signal of manipulation. This paper adopts the scenarios in (1) yet adopt the methodology in (2). Given that the overall goal of this paper is to justify \"consistency is a gold standard on picking SCF\", the axiomatic approach, which sounds more like \"consistency brings natural behaviors under certain hypothetical scenarios\", is not a strong support to the paper. How would you answer to this incosistency and justify your axiomatic analysis?"}, "questions": {"value": "1. What is the specific reason to motivate and interpret as a \"rule to pick a rule\"? (Weakness 1)\n2. How would you answer to this incosistency and justify your axiomatic analysis?  (Weakness 2)\n3. Are there any references to the given consistency axioms? Specifically, why \"reverse symmetry\" sounds so important? At first glance it does not look like a fundamental axiom like anonymity or neutrality. \n4. Is your AbC rule not welfare maximization? If so, do you have any comments on this? \n\nI am open to discussion and happy to raise my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BPhoJZppFU", "forum": "xxsacQ3tdb", "replyto": "xxsacQ3tdb", "signatures": ["ICLR.cc/2026/Conference/Submission24338/Reviewer_iZ34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24338/Reviewer_iZ34"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793600435, "cdate": 1761793600435, "tmdate": 1762943047892, "mdate": 1762943047892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Rule Picking Rules (RPRs) : procedures that choose which aggregation rule to use for a given dataset; and proposes Aggregation by Consistency (AbC), which selects the rule whose outcomes are most stable across random evaluator splits. \n\nIt develops an axiomatic basis (e.g., reversal symmetry, shuffling consistency), proves that achieving perfect consistency over positional scoring rules is NP-complete, and provides a computationally practical Monte-Carlo implementation.\n\nAbC (i) often rediscovers the model-MLE when the generative model matches the data, (ii) yields actionable guidance on rating aggregations, and (iii) extends to top-k selection via set-based distances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and answers an important problem, especially motivated by the Neurips paper acceptance experiment.\n\nThe axiom satisfactions depict that the proposed method meets sanity checks. \n\nThe proofs in the appendix seem thorough.\n\nThe monte carlo estimation makes the implementation practical."}, "weaknesses": {"value": "The paper focuses on Kendall's Tau as the rank comparision metric. Metrics like Cumulative Gain (CG), Discounted CG (DCG), and Normalized DCG are popular for comparing top heavy ranks. A discussion involving those metrics and how the algorithms fares to them would be good to have.\n\nThe empirical section is somewhat narrowly scoped. Although they show correlation between disagreement and distance to ground truth in the synthetic models, no end-to-end evaluation links “most consistent rule” to “best downstream decision quality.” For instance, in peer review: does the AbC-selected rule produce higher eventual citation impact or expert consensus? \n\nMoreover, the real-world data are both from astronomy. It would be helpful to experiment on a diverse set of domains (e.g., recommendation rankings, RLHF preference data, crowd judgments - all readily available) to show generality."}, "questions": {"value": "Did AbC ever select a rule whose behavior appears to be intermediate between known positional scoring rules (e.g., between Kemeny and Borda)? Did it ever select a rule not a combination of existing rules?\n\nWhen optimizing over positional weights to minimize split-disagreement, do the resulting weights cluster around classical forms (e.g., linear, exponential), or is there a continuum?\n\nAre there theoretical conditions under which AbC provably recovers Kemeny or PL-MLE — beyond the empirical confirmation on Mallows/PL data?\n\nIs there any identifiable set of axioms that exactly describe the rules that minimize disagreement under random resampling? I.e can you simulate an uniqueness axiom?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vPoHxc06Cg", "forum": "xxsacQ3tdb", "replyto": "xxsacQ3tdb", "signatures": ["ICLR.cc/2026/Conference/Submission24338/Reviewer_aygk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24338/Reviewer_aygk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892905119, "cdate": 1761892905119, "tmdate": 1762943047627, "mdate": 1762943047627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of selecting the best rank aggregation method in the setting where different aggregation rules can lead to very different outcomes. To address this problem, the authors introduce the concept of a Rule Picking Rule (RPR), a meta-framework for deciding which aggregation method to use for a given dataset without assuming any specific generative model.  Instead of relying on fixed axioms or assumed noise models, the authors  provide a data-driven approach for picking aggregation rules. They propose a new RPR, called \"AbC\", which selects the aggregation method in such a way to maximize consistency, that is to say, producing stable results if the data collection were repeated. The authors show that AbC satisfies certain consistency-related axioms that some natural RPRs fail to meet.  For two axioms it cannot satisfy, the authors prove impossibility results, showing no rule can satisfy them simultaneously. Additionally they show that the problem of finding a perfectly consistent rule is NP-complete. Despite this hardness result, the authors have implemented a sampling-based algorithm that seems to perform reasonably well, in the sense that it outperforms some commonly used aggregation methods. The implementation also provide interpretable insights into when and why specific rules work best. It is stated that an implementation of AbC was awarded in a recent competition\nat a top-tier AI conference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the paper is that the proposed RPR has been implemented and evaluated in practice, and obtained one of the top scores in a relevant AI competition."}, "weaknesses": {"value": "The theoretical part is dense and most proofs are relegated to the appendix, which is quite long (22 pages). Therefore, verifying the correctness of the claims is a bit tricky. I did not verify the correctness of the proofs in the appendix."}, "questions": {"value": "It is stated that an implementation of AbC was awarded in a recent competition at a top-tier AI conference.\n\n1) What place/score did the mentioned implementation get? 1st place, 2nd place, 3rd place? \n2) How many competitors were there in the competition? \n\nWithout more details it is difficult to judge the strength of the implementation with respect to competing approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kNXKQHIxbf", "forum": "xxsacQ3tdb", "replyto": "xxsacQ3tdb", "signatures": ["ICLR.cc/2026/Conference/Submission24338/Reviewer_6kf7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24338/Reviewer_6kf7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961576268, "cdate": 1761961576268, "tmdate": 1762943047420, "mdate": 1762943047420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}