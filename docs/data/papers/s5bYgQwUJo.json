{"id": "s5bYgQwUJo", "number": 7988, "cdate": 1758049480781, "mdate": 1759897817248, "content": {"title": "The Collaboration Gap", "abstract": "The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent–agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable automated grading, and (iv) imposes no output-format constraints, preserving ecological plausibility. Using this framework, we evaluate 32 leading open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Our results reveal a “collaboration gap”: models that perform well solo often degrade substantially when required to collaborate. Collaboration can break down dramatically; for instance, small distilled models that solve mazes well alone may fail almost completely in certain pairings. We find that starting with the stronger agent often improves outcomes, motivating a “relay inference” approach where the stronger agent leads before handing off to the weaker one, closing much of the gap. Our findings argue for (1) collaboration-aware evaluation, (2) training strategies developed to enhance collaborative capabilities, and (3) interaction design that reliably elicits agents’ latent skills, guidance that applies to AI–AI and human–AI collaboration.", "tldr": "A large-scale empirical study into the collaborative capabilities of leading language models", "keywords": ["ai agents", "agentic ai", "collaboration", "evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c92f15a1f87f7c40f322c31b8271b52eb1fa4525.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the collaboration ability of multiple LLMs on tasks with communication challenges. The authors develop a maze environment. Two LLM agents involve in the system and each of them can only observe part of the maze information. They conduct experiments across multiple choices of open/closed-source LLM models, and suggests that a collaboration gap exists. The authors further provides more additional experiments on cases when two LLMs come from different model family and the relay inference setup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Multi-agent collaboration is indeed a more and more important topics. This paper contributes to identifying key potential issues occurred in LLM cooperation tasks by investigating the LLM performance on a maza task. The paper writing is easy to follow, and the results are explained clearly. The results indeed demonstrates the existence of collaboration gap. I found some experiment results are interesting (e.g. the relay inference part)."}, "weaknesses": {"value": "1. My main concern is that the contribution in this paper, although insightful, may not reach the threshold of the acceptable of this top-tier conference. The main contribution is limited in identifying the collaboration gap, but the authors did not make progress beyond that. I believe some contribution on algorithm design to close the collaboration gap would be helpful and makes the paper stronger.\n\n2. The experiment in this paper is limited in the maza task. It is enough to suggest \"the collaboration gap\" indeed exists but it may not be sufficient to suggest that such an issue appears widely in more practical domains where cooperation between LLMs are required. For example, a more practical task is applying LLM for software engineering, and it is also a valid scenario to evaluate LLM cooperations. It would be better to provide results in a more diverse range of tasks in the paper."}, "questions": {"value": "I think the magnitude of the gap depends on how well the prompt engineering on the system/user message is. The authors suggest that one challenge in collaboration (which leads to the gap) is that the LLMs need to exchange their messages and ground their understanding \"on the same page\". \nAt least in this maze task, I think this issue can be partially solved by providing more detailed instructions in system/user prompts, such as some unified symbols/rules for communication or a clear step-by-step guidance on how to exchange information. \n\nI'm curious about the potential of closing the gap through careful prompt engineering. Concretely, with appropriate system/user prompts, would it be possible to eliminate this collaboration gap? If the gap can be closed by prompt engineering, would it still be reasonable to claim the gap exists?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JSB21nhGkk", "forum": "s5bYgQwUJo", "replyto": "s5bYgQwUJo", "signatures": ["ICLR.cc/2026/Conference/Submission7988/Reviewer_gfd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7988/Reviewer_gfd8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760954965574, "cdate": 1760954965574, "tmdate": 1762919994780, "mdate": 1762919994780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment to All Reviewers and Chairs"}, "comment": {"value": "We thank all three reviewers for their time and valuable feedback. We have identified two core methodological critiques that we wish to address head-on, as they are central to our paper's thesis and contributions.\n\n### **On the Benchmark's Goal: Unconstrained Natural Language vs. Symbolic Multi-Agent Reinforcement Learning**\n\nA key question (raised by R-cd33) is how our benchmark relates to existing AI collaboration tasks like Hanabi or Overcooked. We must clarify that our work is not a competitor to MARL benchmarks, which test coordination via constrained, symbolic action spaces (e.g., \"play card\"). As noted in lines 65-66 of the Introduction, a key difference in agents powered by language models is the opportunity to use rich, natural language. However, this shift from communication through constrained, symbolic action spaces to natural language introduces novel collaboration challenges. Our benchmark is thus inspired by and builds upon a different body of literature: seminal studies on human-human collaboration (Section 2.1, e.g., Garrod & Anderson, 1987). Our work adapts this methodology to AI-AI collaboration at scale, focusing on a different and novel problem: evaluating emergent collaboration via **unstructured, open-ended natural language dialogue**. We will extend our review of related work to make this distinction clearer and properly attribute past efforts.\n  \n\n### **On the \"Gap\": A Failure of Collaboration, Not Perception or Task Design**\n\n- **Is it a Perception Failure?**\nOur “Solo baselines (beginning of section 3 and Fig 4.1)\" directly answer this. In this control, a solo agent is either given the full ASCII map (Full) or both partial ASCII maps (Distributed). In both cases the solo agent succeeds with a high solve-rate. This shows the studied models can parse the ASCII and can solve the pathfinding problem, even when information is distributed. The performance drop only occurs when they must communicate with a partner. This isolates the failure to collaboration, not perception. We will update Section 3 and the discussion of results shown in Sections 4.1 and 4.2 to make the purpose and outcomes of this control more apparent.\n\n\n- **Can it be \"Prompt-Engineered\" Away?**\nThis question (from R-gfd8, R-mYFX) is central to our paper's motivation. We are interested in adaptive, \"on-the-fly\" communication. In the open, unstructured real world, it is unrealistic to assume we will always have the a-priori knowledge required to pre-define a perfect communication protocol (like a JSON schema) for every novel task an agent might encounter. Additionally, we will not have control over the output format of any agents deployed by another party. The fact that current models fail in the absence of heavy scaffolding shows that collaboration is not an emergent property of solo capabilities. Our benchmark is the first to quantify this \"native\" collaboration gap, which has clear implications for further integration of AI agents into society.\n\n### **Our Contributions**\nHaving clarified our motivation and methodology, we wish to re-emphasize our contributions in the context of the datasets and benchmarks track. Our work provides a comprehensive empirical study (evaluating 32 leading language models) that identifies a critical, novel failure mode. The key findings include:\n- The formal identification and quantification of the \"collaboration gap\" in unconstrained natural language communication.\n- The discovery that this gap is especially severe in distilled models.\n- A robust analysis of heterogeneous collaboration, revealing that \"ordering effects\" (who speaks first) are a critical performance determinant.\n- A novel, practical mitigation strategy, \"relay inference,\" where a strong agent \"primes\" the interaction, which we show closes much of the gap displayed by weaker models.\n\nWe are confident these are significant contributions and will revise the paper to make these points clearer."}}, "id": "2ajjqGaeql", "forum": "s5bYgQwUJo", "replyto": "s5bYgQwUJo", "signatures": ["ICLR.cc/2026/Conference/Submission7988/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7988/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7988/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763146413566, "cdate": 1763146413566, "tmdate": 1763146413566, "mdate": 1763146413566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a collaborative maze-solving benchmark to evaluate AI-AI collaboration among heterogeneous agents with partial observability. The authors evaluated 32 open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Their primary finding is the \"collaboration gap\": models that perform well individually often experience a substantial performance drop when required to collaborate with an identical copy of themselves. They also observed that collaborative performance is heavily influenced by which agent starts the task. To address this, the paper introduces \"relay inference,\" a strategy where a stronger agent initiates the interaction to \"prime/ground\" the rollout before a weaker agent takes over, which was shown to significantly boost collaborative performance and close much of the gap. The key contributions are formally defining the collaboration gap, analyzing heterogeneous collaboration dynamics, and proposing the effective relay inference strategy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The analysis of homogeneous, same-family (different strengths), and cross-model heterogeneous collaboration provides cool and valuable insights into agent interaction dynamics.\n\n- The overall scope of the evaluations conducted appears quite comprehensive, covering many models and various collaboration settings.\n\n- The proposed collaborative maze-solving benchmark is novel, isolates collaborative capabilities, and imposes minimal output constraints, which is a strong methodological contribution."}, "weaknesses": {"value": "It is unclear whether LLM collaboration failures in this specific maze task translate to an inability to collaborate effectively in more naturalistic use cases, such as coding tasks.\n\nHuman performance on these specific maze tasks is missing, which makes it difficult to fully substantiate claims about the LLM \"collaboration gap.\"\n\nThe reliance on the autograder is questionable, as it may introduce systemic biases or errors compared to enforcing a deterministic output format for all models.\n\nThe complexity of the ASCII map visualization raises concerns about whether perceived \"collaboration failures\" are actually failures of perception that could be mitigated with better data representation (e.g., natural language or tool use).\n\nThe authors should clarify the methodology of \"at least 100 rollouts\" by standardizing the exact number of runs across all different model types and evaluation conditions.\n\nThe discussion section does not fully justify why mazes are a good lower bound for complex collaboration, especially since the gap might be closed with different prompting or tool-use strategies. In the discussion, the authors talk about how the gap might be wider in more complex cases, but those are cases we have data for, whereas mazes seem like an esoteric example that likely doesn’t appear in the training set that much. \n\nThe section detailing the \"relay inference\" strategy is conceptually confusing and requires additional clarification on the mechanism and implementation.\n\nThe authors should quantify some of the interesting qualitative representational details observed during the model dialogues."}, "questions": {"value": "- How do the authors think the collaboration failures observed in this esoteric maze might reflect failures in more representative, naturalistic language environments, such as collaborative coding or long-horizon planning tasks?\n- Can the authors conduct an ablation study where the ASCII map input is replaced with a more structured, natural language (or JSON) representation to determine if performance gains are due to improved collaboration or simply better perception/parsing of the environment?\n- Why not impose a strictly defined, parseable output format for the agents' moves (e.g., a specific JSON/YAML structure) to enable deterministic grading, thereby eliminating the reliance on an LLM autograder and its inherent biases?\n- Can the authors include a baseline measurement of human performance on this identical distributed maze-solving task to properly contextualize the LLM failure rates?\n- How many rollouts (e.g., exactly 100 or 150) were done for all model types and evaluation conditions, rather than the ambiguous \"at least 100 rollouts\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vXhj7YDT2D", "forum": "s5bYgQwUJo", "replyto": "s5bYgQwUJo", "signatures": ["ICLR.cc/2026/Conference/Submission7988/Reviewer_mYFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7988/Reviewer_mYFX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601449504, "cdate": 1761601449504, "tmdate": 1762919994379, "mdate": 1762919994379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new maze based collaborative test for RL algorithms and test it on LLMs. They show that their test correlates with model \"power\" and provide suggestions on how to make LLMs more collaborative."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "# originality\n\nEvaluating collaboration between AI agents has been an ongoing area of research for decades. This appears to be a variant of the overcooked test [1], but with lower complexity and partial information. As they do not discuss how this fit into the literature it is hard to evaluate the originality. \n\n\n# quality\n\nThe experiments appear to have been done well, and they test a large number of LLM variants. The use of an automated LLM as grader is concerning, but they discuss the issues with it and seem to have done things correctly. The lack of code release though means I cannot verify these statments.\n\n# clarity\n\nThe paper is clear, giving good examples and explains what is happening well. I think putting a full dialogue somewhere prominent (maybe a section in the appendix) would help as I had to dig around in the appendix to understand exactly what transpires in a run.\n\n# significance\n\nAs mentioned above I have concerns about how this paper engages with the literature. Taking it on face value there are better and much more established tests for AI agent collaboration (Hanabi, Diplomacy, Overcooked, ...), so I'm not sure if this adds much to the disucssion\n\n[1] Carroll, M., Shah, R., Ho, M.K., Griffiths, T., Seshia, S., Abbeel, P. and Dragan, A., 2019. On the utility of learning about humans for human-ai coordination. Advances in neural information processing systems, 32."}, "weaknesses": {"value": "The 6x6 maze seems very small, A* can solve that trivially.\n\nThe use of a grader AI adds an additional level of complexity to the experiment.\n\nThe authors don't appear to engage with the SOTA in collaborative AI, instead focusing on LLMs only.\n\nMost of my other concerns are in the other sections, I think if the paper significantly toned down it's claims it would be publishable."}, "questions": {"value": "I'm concerned about the reproducibility of this experiment. Why didn't the authors include the code with the submission? This is a complex simulation, code is need for other groups to reproduce the results. Will the the authors do a full code release if the paper is published? The reproducibility statement only talks about the license not access.\n\nI find the use of anthropomorphic language (e.g. on line 302 \"The stronger o3 immediately seeks to\") concerning. Are the authors arguing that LLMs have goals? I think discussing what is happening with more theoretically grounded language would improve the paper.\n\nHanabi[1] is the standard for collaborative text based AI research. Why is this maze approach better and how do results compare to Hanabi? More generally was AI collaboration in non-LLM settings discussed at all in the paper?\n\nHow do these models perform when paired with non-LLM partners? Training an RL agent to solve these mazes seems trivial.\n\nCan the models share their maps to each other? Was their any filtering of messages?\n\n[1]Bard, N., Foerster, J.N., Chandar, S., Burch, N., Lanctot, M., Song, H.F., Parisotto, E., Dumoulin, V., Moitra, S., Hughes, E. and Dunning, I., 2020. The hanabi challenge: A new frontier for ai research. Artificial Intelligence, 280, p.103216."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n2SdZMbRRA", "forum": "s5bYgQwUJo", "replyto": "s5bYgQwUJo", "signatures": ["ICLR.cc/2026/Conference/Submission7988/Reviewer_cd33"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7988/Reviewer_cd33"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985554561, "cdate": 1761985554561, "tmdate": 1762919994021, "mdate": 1762919994021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}