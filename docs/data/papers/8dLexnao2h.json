{"id": "8dLexnao2h", "number": 5251, "cdate": 1757877630267, "mdate": 1759897985408, "content": {"title": "A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration", "abstract": "In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels supplemented with a distributed framework for image registration at unprecedented scales. Image registration is an inverse problem fundamental to biomedical and life sciences, but algorithms have not scaled in tandem with image acquisition capabilities. Our framework complements existing model parallelism techniques proposed for large-scale transformer training by optimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding. We demonstrate unprecedented capabilities by performing multimodal registration of a 100μm ex-vivo human brain MRI volume at native resolution – an inverse problem more than 570× larger than a standard clinical datum in about a minute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art optimization and deep learning registration pipelines by upto 6 − 7× while reducing peak memory consumption by 20 − 59%. Comparative analysis on a 250μm dataset shows that FFDP can fit upto 64× larger problems than existing SOTA on a single GPU, and highlights both the performance and efficiency gains of FFDP compared to SOTA image registration methods.", "tldr": "we propose non-GEMM CUDA kernels and distributed primitives to scale multimodal image registration to arbitrary image sizes", "keywords": ["image registration", "distributed optimization", "CUDA kernels", "neuroanatomy"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16382d88db031655ef793f0e950ef32b9c94f292.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FFDP (Flash Fused Distributed Primitives) as a scalable and distributed framework for large-scale image registration. The main idea is to fuse non-GEMM operations using IO-aware kernels and distributed tensor sharding, enabling registration of multi-billion-voxel images. The proposed framework includes fused kernels for single-GPU efficiency and GridParallel plus RingSampler modules for distributed multi-GPU scalability without full-image allgather operations. Experimental results show that FFDP significantly accelerates registration and reduces memory consumption for both iterative optimization-based and deep learning-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. System contribution: \nFFDP elegantly adapts concepts like ring topology and sharded synchronization to a domain with boundary dependencies.\n\n2. Significant scalability results on iterative methods: \nDemonstrates unprecedented registration (11 B + parameters) within realistic compute budgets."}, "weaknesses": {"value": "1. The introduction part does not demonstrate the motivation for full-resolution registration:\nThe introduction does not sufficiently justify the necessity of performing registration at full resolution. While image acquisition capabilities have improved, in many biomedical and clinical applications registration on downsampled images is adequate. The authors should provide stronger evidence or references showing that downsampling significantly degrades registration accuracy or downstream analysis.\n\n2. Limited benefit for deep learning pipelines:\nThe proposed framework provides modest gains (16.5% and 24.7% memory reduction) for deep learning networks comparing with it in iterative methods. It would be helpful if the authors could analyze whether further optimizations could narrow this gap.\n\n3. Combined Introduction and Related Work:\nThe paper merges the introduction and related work into a single section, which reduces readability and makes it harder to follow the motivation versus prior context. \n\n4. Poorly organized experiments:\nThe experimental section is difficult to follow and lacks clear structure."}, "questions": {"value": "1. Could you provide evidence or references that shows the necessity of full-resolution registration?\n\n2. In Table 1, what do the three “Baseline” rows represent, what do “Top” and “Bottom” refer to, and which dataset was used to generate Table 1? Besides, Table 1 is not referenced in the main paper.\n\n3. Regarding the faux-OASIS dataset experiment, you register the images at a downsampled resolution and perform patchwise registration followed by mosaicing of the final deformation for deep learning methods. This setup may adversely affect their performance, as many hyperparameters proposed in their paper(such as the weighting of the smoothness term) are designed for whole-image registration rather than patchwise processing. Moreover, you did not report the performance of these methods on the original OASIS dataset. If the deep learning methods achieve better performance on the original dataset, it would suggest that your experimental setup is flawed.\n\n4. In Figure 5, you denote “fire-ants with the proposed method” as Ours and compare it with other methods. However, this comparison appears unfair since you did not include the performance of fire-ants itself. It is highly likely that fire-ants alone could outperform many of the other baselines. You should include the performance of the original fire-ants for a fair comparison. If memory limitations are a concern, you may apply the same two modifications used for the deep learning–based methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6U2AXFuyxo", "forum": "8dLexnao2h", "replyto": "8dLexnao2h", "signatures": ["ICLR.cc/2026/Conference/Submission5251/Reviewer_hXkU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5251/Reviewer_hXkU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761424836058, "cdate": 1761424836058, "tmdate": 1762917972916, "mdate": 1762917972916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a set of IO-aware fused kernels and a distributed framework enabling deformable image registration at extremely large scale. It significantly reduces the memory usage for interpolation, and 2 very common loss functions. Experimental results show significant acceleration for both classical iterative optimization and deep learning–based registration pipelines, and demonstrate the first native-resolution multimodal registration at extremely high spatial resolutions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Enables giga-voxel registration in its native resolution that previously was infeasible.\n2. Identifies and proposes solutions for major memory bottlenecks speeding up both iterative and learning based registration.\n3. Demonstrates performance on multimodal registration of 250μm to 100μm images completing in 1 minute running on 8 gpus.\n4. Ablates each components contribution.\n5. Benchmarks the performance of registration algorithms on clinical scale and very high resolution brain images.\n6. The language of the paper is clear. \n7. I really enjoyed reading the paper."}, "weaknesses": {"value": "**Weaknesses and questions**\n\n1. I was wondering why the affine transformations is denoted as φ(x)=Ax+t. Isn’t it the case normally that the translation is incorporated in A along with rotation and scaling? \n2. Although the scalability demonstrated on ex-vivo MRI is impressive, it is unclear why the method was not evaluated on multi-gigapixel histopathology datasets, which seem like an ideal target application given their fine-scale structural alignment requirements. Such experiments would directly validate the paper’s stated motivation and better highlight FFDP’s advantages over existing patch-based histology workflows.\n3. While the paper emphasizes improvements for multimodal registration, LNCC is widely used for monomodal scenarios too, so the proposed optimizations appear broadly applicable. I am therefore unsure why the authors chose to position their contribution primarily around multimodal registration and not a general purpose registration with more speedup in the multi-modal case.\n4. Not sure if I missed it, but are all methods (learning-based and iterative optimization ones) trained with the same objectives?\n5. Multi-resolution strategies are commonly used in high-resolution registration to manage memory and computation, and while they suffer from similar limitations as those discussed in the paper, acknowledging them would improve completeness and situate the work more clearly within existing practice.\n6. Regarding the learning-based baselines, their encoders are not designed for extremely high-resolution inputs, and while patchification is clearly suboptimal, it seems that alternative architectural designs (e.g., improved downsampling kernels in the feature space, larger capacity models, more spatially aware context handling) might yield stronger performance even if at higher memory cost. It would be helpful for the authors to comment on whether such adaptations were considered and how FFDP compares in those scenarios. I would be interested in the authors’ perspective on whether the limitations observed are inherent to deep approaches at scale or simply due to the baselines being used outside their intended operating regime.\n7. No codebase or link to code is available."}, "questions": {"value": "Please see the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tNf06oQYUa", "forum": "8dLexnao2h", "replyto": "8dLexnao2h", "signatures": ["ICLR.cc/2026/Conference/Submission5251/Reviewer_bohM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5251/Reviewer_bohM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733243441, "cdate": 1761733243441, "tmdate": 1762917972611, "mdate": 1762917972611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, we propose the FFDP framework, which aims to solve the problem of insufficient algorithmic scalability in ultra-high resolution multimodal image alignment.FFDP designs three I/O-aware non-GEMM fusion kernels (composite implicit grid sampler, implicit Parzen window mutual information estimation, and efficient fusion inter-correlation) as well as distributed architectures (grid parallelism, ring sampler, and distributed loss computation) to realize the fast processing of tasks at the level of hundreds of billions of voxels. Experiments show that FFDP outperforms existing methods on both synthetic and real datasets, increasing the runtime speed by 6-7 times, reducing peak memory by 20-59%, and significantly reducing GPU computational overhead while maintaining higher alignment accuracy (e.g., Dice coefficient of 89.5% at 250µm resolution)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Motivation is clear: It precisely targets the core contradiction between high-resolution acquisition capabilities and the insufficient scalability of existing algorithms in medical imaging, with real scenarios like 100µm ex-vivo human brain MRI clearly illustrating the necessity.\n- Experiments are rich and comprehensively prove the effectiveness of its method: It covers multiple types of datasets and various dimensions of experiments, from component-level ablation to system-level comparison with mainstream baselines, fully demonstrating the method's validity.\n- Performance improvement is significant"}, "weaknesses": {"value": "While this paper targets at improving efficiency, it would be better provide experimental evaluations on accuracy against counterparts."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cvp3ZOzdV5", "forum": "8dLexnao2h", "replyto": "8dLexnao2h", "signatures": ["ICLR.cc/2026/Conference/Submission5251/Reviewer_TnBN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5251/Reviewer_TnBN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850001513, "cdate": 1761850001513, "tmdate": 1762917972340, "mdate": 1762917972340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes large 3D medical image registration run fast and fit in GPU memory. \nKey idea the reviewer finds: real bottlenecks at giga-voxel scale are not matrix multiplies but ops like grid sampling, LNCC, MI. They write fused, IO-aware CUDA kernels plus a ring sampler so GPUs don’t need full image copies. Result: 6–7× speed, up to ~59% memory saved, and a demo of 11.8B-parameter multimodal brain registration in about a minute on 8×A6000. This work is quire impressive.\n\nThe reviewer is familiar with registration but not with optimization work. But this work appears impressive and practical. Therefore, the reviewer will assign a low confidence score but high rating."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Writing: Problem importance is well motivated: when the image is 100–1000× larger, registration algorithms did not keep up. It is an important gap.\n- Distributed story is novel for registration.\n- Empirical section is broad: classical (ANTs-like), learning (TransMorph, SynthMorph, VFA, UniGradICON), and purpose-built (CLAIRE, ITK-DReg). The 250µm setting, where every other method degrades while FFDP improves, is persuasive.\n- The accelerations for existing code (TransMorph training 6× faster, big memory drops, and also for FireANTs) mean this work is really a critical progress."}, "weaknesses": {"value": "- This work enables faster/larger registration, but it lacks a clear section on how to apply this. It does not specify which registration methods are directly supported, how to integrate the kernels, or which methods are not supported and why.\n- Only mentioned related work on GPU/system acceleration. The author does not survey other GPU acceleration outside registration."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SbPFIivEAG", "forum": "8dLexnao2h", "replyto": "8dLexnao2h", "signatures": ["ICLR.cc/2026/Conference/Submission5251/Reviewer_oLeD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5251/Reviewer_oLeD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994844006, "cdate": 1761994844006, "tmdate": 1762917972110, "mdate": 1762917972110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}