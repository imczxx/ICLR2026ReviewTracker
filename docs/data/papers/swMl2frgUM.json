{"id": "swMl2frgUM", "number": 18980, "cdate": 1758292531802, "mdate": 1759897069335, "content": {"title": "Watermarking Graph Neural Networks via Explanations for Ownership Protection", "abstract": "Graph Neural Networks (GNNs) are widely deployed in industry, making their intellectual property valuable. However, protecting GNNs from unauthorized use remains a challenge. Watermarking offers a solution by embedding ownership information into models. Existing watermarking methods have two limitations: First, they rarely focus on graph data or GNNs. Second, the \\emph{de facto} backdoor-based method relies on manipulating training data, which can introduce ownership ambiguity through misclassification and vulnerability to data poisoning attacks that can interrupt the backdoor mechanism. Our explanation-based watermarking inherits the strengths of backdoor-based methods (e.g., black-box verification) without data manipulation, eliminating ownership ambiguity and data dependencies. In particular, we watermark GNN explanations such that these explanations are statistically distinct from others, so ownership claims must be verified through statistical significance. We theoretically prove that, even with full knowledge of our method, locating the watermark is NP-hard. Empirically, our method demonstrates robustness to fine-tuning and pruning attacks. By addressing these challenges, our approach significantly advances GNN intellectual property protection.", "tldr": "", "keywords": ["Watermarking", "GNNs", "Ownership", "Verification", "Ownership Verification", "Explanation", "Graph"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e1d41ec2e898a43e15e06088a66aa58a8bf8077.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel method for watermarking Graph Neural Networks (GNNs) to protect their intellectual property. The authors identify key weaknesses in existing backdoor-based watermarking, namely ownership ambiguity (false claims on misclassified data) and vulnerability to data poisoning attacks. To overcome this, the paper introduces an explanation-based watermarking scheme. Instead of embedding a trigger into the model's predictions, this method embeds the watermark into the model's explanations. The core mechanism involves: (a) The model owner selecting multiple secret, private subgraphs. (b) Training the GNN with a dual-objective loss function: one term for the standard classification task and a second term (that forces the GNN's explanations of these secret subgraphs to align with a single, predefined watermark pattern. (c) The explanation itself is generated by a closed-form ridge regression method using Gaussian kernel matrices. A key contribution is the verification process. Verification is black-box and does not require knowledge of the ground-truth watermark. Instead, the owner presents the secret subgraphs as proof. The verifier then computes the explanations for these subgraphs and measures their statistical similarity to each other by counting the \"Matching Indices\" (MI). This MI score is compared against a baseline distribution of MIs from random subgraphs using a z-test. A highly significant p-value confirms ownership, as this high degree of similarity is statistically unlikely to occur by chance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- While explanation-based watermarking has been explored for DNNs , this paper's contribution is twofold: 1) it successfully adapts this concept to the complex, non-i.i.d. domain of graph data, and 2) it introduces a superior verification mechanism. This approach sidesteps the \"ground-truth watermark\" problem  and the need for a trusted third party.\n- The paper is exceptionally well-written. The problem and the limitations of prior art are clearly articulated.\n- This work tackles a critical and practical problem: the IP protection of valuable GNN models. It offers a complete, end-to-end solution that directly addresses the fundamental flaws (ownership ambiguity, data poisoning) of the current de facto standard (backdoor-based watermarking)."}, "weaknesses": {"value": "- The most significant weakness is admitted in Appendix F: the watermark is not robust to knowledge distillation attacks.\n- The verification process itself, while secure, appears computationally expensive. To perform a z-test, the verifier must first build the baseline distribution by running 1000 simulations.\n- The paper only provides empirical tests against pruning and fine-tuning. The claim of immunity to data poisoning is an assertion based on the method's design (using clean samples ) rather than an experiment. Other common attacks, such as model overwriting (retraining the final layers) or adversarial attacks designed to disrupt explanations, are not considered. Given the (untested) vulnerability to KD, the robustness claims may be incomplete."}, "questions": {"value": "- Your proposed defense in Appendix F involves periodically distilling the model during training. This sounds computationally massive, potentially more than doubling the already-high training cost. Could you elaborate on the feasibility of this defense? Have you considered or tested any simpler defenses? For example, would watermarking explanations at multiple, intermediate layers of the GNN provide any robustness against KD?\n- The entire method relies on a specific, global explanation method using ridge regression on Gaussian kernels. a) How crucial is this specific explainer to the method's success? b) Would the watermarking framework still be effective and robust if a different, perhaps more local or gradient-based, GNN explainer were used instead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s28ODMdGMJ", "forum": "swMl2frgUM", "replyto": "swMl2frgUM", "signatures": ["ICLR.cc/2026/Conference/Submission18980/Reviewer_YuUR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18980/Reviewer_YuUR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827158739, "cdate": 1761827158739, "tmdate": 1762931033482, "mdate": 1762931033482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a  for watermarking graph neural networks.\nIt is designed for  black-box ownership verification, i.e., ownership can be proven using only queries to the model, while adversaries can not (feasibly) construct a false proof of ownership.\n\nThe authors begin by identifying two key limitations of the most prevalent paradigm to black-box watermarking, which is based on adversarial backdoor attacks:\n* Misclassification may be misconstrued as proofs of ownership,\n* Data-dependent insertion of, and training on, backdoor triggers may be disrupted by poisoning attacks.\n\nThe authors instead propose to build upon an explanation-based work by Shao et al. (2024) [1] by:\n(1) Extending it to graph data and (2) resolving its dependence on a ground-truth watermark.\n\nOwnership verification requires: A set of \"watermarked subgraphs\" from the training set, an explanation method that maps subgraphs and model predictions to a feature-importance vector, a binary watermark vector, and a subset of indices into this watermark vector. Verification is performed by applying the model to each watermarked subgraph, computing the respective feature-importance vectors, binarizing them, and evaluating the number of matches with the watermark vector (in the subset of indices).\n\nAt training time, the watermark vector and indices are selected in a data-dependent but model-independent manner.\nThe model is then trained to simultaneously achieve high utility on the non-watermark-subgraph training data and a large number of matches with the watermark vector.\n\nAfter explaining that there is a combinatorially large number of possible subgraphs, which makes brute-force or random-search for the (secret) watermarkerd subgraphs intractable for an adversary,\nthe authors proceed to experimentally evaluating their method. They show that their method:\n* Is an effective watermark\n* The watermark cannot be easily removed by post-processing the model via pruning or fine-tuning\n* Brute-force or random search is indeed ineffective at finding the true (or similarly behaving) watermarked subgraphs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors do a great job of introducing watermarking and highglighting the limitations of prior work\n* The objectives of the paper and the considered threat model are very clearly defined\n* The proposed method does indeed achieve the goal of decoupling watermarking from the usual inference process, which would allow misclassifications to be misconstrued as proofs of ownership\n* Beyond the graph context, the work improves upon Shao et al. (2024) [1] by explicitly defining a procedure for constructing watermarks\n* The experimental evaluation is extremely well-motivated and aligned with previously specified objectives of the work. The experiments are targeted at answering specific research questions, as opposed to just running through various benchmarks"}, "weaknesses": {"value": "* Using an explanation-based approach is motivated by the fact that backdoor-based watermarking approaches are data-dependent and can thus be attacked via data poisoning. However, it appears like the proposed method could also be poisoned, as both the watermark embedding process (see Fig. 1) and the watermark construction (see Eq. 12) are data-dependent\n* The justification for why the proposed procedure is robust to verification-time adversaries as well as the corresponding experimental evaluation are weak. Yes, iterating over all possible watermarked subgraphs is obviously combinatorially hard. However, this does not preclude more effective search methods (e.g., via reconstruction attacks on the trained model).\n* Shao et al. (2024) [1] are critized for \"asssum[ing] a known ground-truth watermark\". This is somewhat misleading. It implies that there was only a specific valid watermark that may be intractable to construct/find (which would indeed be a major limitation). However, upon reading their paper, it appears like the method can indeed be used with arbitrary watermarks. The authors can only be critized for not detailing a specific procedure for choosing it.\n* If the above does indeed count as \"assuming[ing] a known ground-truth watermark\", then this work can be critized for assuming known ground-truth watermarked subgraphs. Afterall, no procedure for selecting the subgraphs $G^{wmk}_t$ is specified.\n* Aside from providing a procedure for constructing watermarks, this work appears like a direct application from Shao et al. (2024) [1] to graphs, and would fit directly into their Fig. 2 via the following mapping: Trigger sample = Union of watermarked subgraphs. Masked samples = Watermarked subgraphs. The embedding and verification procedure appears identical, aside from using a different explanation method taken from prior work. Therefore, novelty seems somewhat limited.\n\n### Minor weaknesses\n* The explanation method essentially maps nodes of $N$ graphs to a regression dataset with $N^2$ traininig samples. Thus, the method may have issues scaling to larger graphs\n* In Section 4.3, it is not clear what \"natural matching\" refers to. The justification for the watermark length selection is thus somewhat hard to follow\n* Fig.1 is too busy and small. It is also pixelated and should be replaced with a vector graphic.\n\n\n\n### Summary\nOverall, this submission is an overall well (and in some sections extremely well) presented, albeit incremental adaptation of an existing method to the graph domain.\nHowever, even though this is an integral part of the problem formulation in Section 3.3,\nthe paper fails to demonstrate the effectiveness of the proposed method in an adversarial context (either under data poisoning or  inference-time reconstruction of the watermark), \nThe discussion of the most directly related prior work is also somewhat questionable.\n\nFor now, I consider this a borderline reject, but I am willing to increase my score based on the authors' response to my questions below."}, "questions": {"value": "* Aside from specifying a concrete procedure for constructing the watermark, and masking graphs instead of masking images,\n how does the proposed method differ from Shao et al. (2024) [1] (specifically the procedure shown in their Fig. 2)\n* How are the watermarked subgraphs selected?\n* What exactly is meant with \"ground-truth\" watermark in the discussion of prior work?\n* Currently, the paper focuses on node-level transductive learning on a single graph. Could the method be applied to multiple graphs (e.g., by representing them as a large graph with multiple connected components)?\n* Could it be applied to tasks other than node classification (as long as there is an explanation method?)\n\n### Other comments (will not affect my score, no need to respond)\n* It seems like the proposed method work with arbitrary explanation methods. It might be nice to keep the generic discussion of explanation methods in background Section 3.2, and move the concrete instantion to methods Section 4. This greater generality could also be highlighted as a strength of this watermarking framework.\n* l. 294: There's a missing space after \"pad watermark length.\".\n\n### References\n[1] Shao et al. Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution. NDSS 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wbv5xRXJkp", "forum": "swMl2frgUM", "replyto": "swMl2frgUM", "signatures": ["ICLR.cc/2026/Conference/Submission18980/Reviewer_8etF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18980/Reviewer_8etF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843378388, "cdate": 1761843378388, "tmdate": 1762931032869, "mdate": 1762931032869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel explanation-based watermarking method for protecting the intellectual property of Graph Neural Networks (GNNs). The approach is designed to overcome the limitations of traditional backdoor-based watermarking, such as ownership ambiguity and vulnerability to data poisoning attacks.\nThe core idea is to embed the watermark by aligning the GNN's explanations (feature attributions) for a set of secret, owner-selected subgraphs with a predefined secret watermark pattern, w. This is achieved by adding a watermark loss term to the standard classification loss during training, creating a dual-objective optimization problem."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This novel approach bypasses the drawbacks of traditional backdoor methods, such as ownership ambiguity and vulnerability to data poisoning attacks.\n2. The design of the verification process is particularly clever. It does not directly compare against the secret watermark pattern. Instead, it tests whether the explanations of the subgraphs claimed by the owner exhibit a similarity that is statistically unlikely to occur naturally. This enables the verification process to proceed without knowledge of the secret watermark.\n3. It is theoretically proven that even in a worst-case scenario, where an attacker has full knowledge of the watermarking method, locating the watermark via brute-force search is computationally infeasible. This provides a strong guarantee for the method's undetectability."}, "weaknesses": {"value": "1. The calculation of explanations involves complex matrix operations. Consequently, scalability to larger-scale graphs remains a potential challenge. It is recommended that this is discussed more in-depth in the main body of the paper, beyond the brief mention in the appendix.\n2. There is limited sensitivity analysis for the balancing coefficient r. While the paper provides extensive ablation studies for the number of watermarked subgraphs (T) and their size (s), a similar analysis for r is absent.\n3. The introduction of the watermarking mechanism causes a slight drop in the model's test accuracy. This \"performance-security\" trade-off warrants further discussion."}, "questions": {"value": "The paper evaluates robustness against pruning and fine-tuning. What is the method's expected robustness against model merging?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tozLPmPtuY", "forum": "swMl2frgUM", "replyto": "swMl2frgUM", "signatures": ["ICLR.cc/2026/Conference/Submission18980/Reviewer_MyRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18980/Reviewer_MyRG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917722631, "cdate": 1761917722631, "tmdate": 1762931032362, "mdate": 1762931032362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the first explanation-based watermarking scheme for Graph Neural Networks (GNNs).\nUnlike traditional backdoor-based watermarking methods that modify training data and risk ownership ambiguity, this work embeds ownership into GNN explanations—that is, into feature attribution vectors derived from subgraphs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Strong theoretical support\n1. NP-hardness proof for watermark detection.\n2. Statistical verification (z-test) provides an interpretable and quantifiable ownership test.\n\n+Solid experiments\n1. Evaluations across multiple GNNs (GCN, SAGE, Transformer, etc.) and datasets (Photo, PubMed, CS, Reddit2)."}, "weaknesses": {"value": "- Limited comparison with other watermarking baselines\nThe paper references prior methods (e.g., Xu et al. 2023, Waheed et al. 2024) but does not empirically compare performance, robustness, or detection accuracy against them.\n\n- Simplified threat model\nThe assumed adversary is strong but lacks adaptive attack testing—e.g., adversaries who retrain explanations or randomize attributions could weaken the watermark."}, "questions": {"value": "refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LQj98V4Mdm", "forum": "swMl2frgUM", "replyto": "swMl2frgUM", "signatures": ["ICLR.cc/2026/Conference/Submission18980/Reviewer_FFY4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18980/Reviewer_FFY4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117354948, "cdate": 1762117354948, "tmdate": 1762931031585, "mdate": 1762931031585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}