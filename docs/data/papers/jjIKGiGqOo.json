{"id": "jjIKGiGqOo", "number": 1324, "cdate": 1756870963989, "mdate": 1763508820300, "content": {"title": "BrowseComp-Plus: A More Fair and Transparent Evaluation  Benchmark of Deep-Research Agent", "abstract": "Deep search agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results.\nEvaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep search agent methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs.\nTo address these challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus.\nEach query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation.\nThe benchmark is shown to be effective in distinguishing the performance of various deep search agents.\nFor instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86\\% accuracy, whereas the GPT-5 achieves 55.9\\%.\nIntegrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1\\% with fewer search calls.\nThis benchmark allows comprehensive evaluation and disentangled analysis of deep search agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in deep search agents.", "tldr": "", "keywords": ["deep research", "inforamtion retrieval", "large language model", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3ee4bc45560d5e4d1ce4c5078ea109621ed4dc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduced BROWSECOMP-PLUS, a deep research benchmark based on BROWSECOMP, with a provided retrieval corpus. Comparisons between multiple deep search systems and retrievers are also included."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposed a fairer and more transparent deep research benchmark.\n- Various LMs are included in the experiment.\n- The authors are willing to share their full execution traces of their expensive experiments."}, "weaknesses": {"value": "- The methodology and the dataset in the paper are not innovative enough. The queries in BROWSECOMP-PLUS is a subset of the queries in BROWSECOMP. It is also not clear if the methodology of building the document corpus can be applied to other scenarios.\n- Missing details about methodology and evaluation. Please see Questions for more."}, "questions": {"value": "I really appreciate the authors for the extensive evaluation of various LMs on deep search, particularly given that half of the models included in the experiments are closed-source. However, the paper would be strengthened by a more thorough justification of the **novelty** of the dataset itself and its construction methodology. The current discussion in the paper looks like simple application of existing methods to mine and append relevant and hard negative documents to the original BROWSECOMP dataset, supported by human annotation. Further clarification on the innovative aspects of this process is needed.\n\n## Questions about Methodology:\n\n* Section 3.2.1: Evidence documents were gathered by prompting an OpenAI model. This data collection method raises concerns about potential bias in the document corpus, favoring OpenAI models. The authors should address this potential bias.\n\n* Section 3.2.2: Regarding human annotation, the paper states that annotators were “instructed to revise … and search … for at least 20 minutes.” What is the justification for this specific 20-minute time requirement?\n\n* Section 3.2.2: The annotation of 1,005 queries, with each requiring at least 20 minutes, amounts to a minimum of 335 human hours for the verification step. The authors should provide details on the recruitment process for annotators and the quality assurance measures implemented to maintain high-quality annotations at this scale.\n\n* Section 3.3: Hard negative documents were collected using the Google Search API for sub-queries. What steps were taken to ensure that the search results did not contain false negatives?\n\n## Questions about Evaluation:\n\n* Line 298: The paper states, “86.5% of queries still contain the ground-truth answer in at least one of their gold documents.” Was this figure calculated based on direct string matching, or did it also account for the “implication” cases mentioned in Lines 244-245?\n\n* The authors have not specified how the LMs were integrated into the deep search pipeline. For instance, did the retrievers use the dataset's queries directly, or did they use search queries generated by the LMs? What factors determined the number of search calls?\n\n* The oracle retrieval experiment in Section 4.5 is a valuable inclusion; however, I cannot find the results in the main paper or the appendix."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "It is not clear if the crawled pages from Sections 3.2.1 and 3.3 have copyright issues or not. Also, the author did not discuss much about the human annotation process."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KCUIOu3XGV", "forum": "jjIKGiGqOo", "replyto": "jjIKGiGqOo", "signatures": ["ICLR.cc/2026/Conference/Submission1324/Reviewer_aAkT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1324/Reviewer_aAkT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760739886345, "cdate": 1760739886345, "tmdate": 1762915736189, "mdate": 1762915736189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BrowseComp-Plus, a new benchmark designed to provide a more fair and transparent evaluation environment for search agents. BrowseComp-Plus employs a fixed, curated corpus and includes human-verified supporting documents as well as challenging negative samples. Through controlled experiments, the benchmark demonstrates the ability to differentiate performance across deep-research systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important issue in evaluating search-based agents, namely the lack of fairness and transparency in existing benchmarks that rely on live web APIs. By introducing a fixed and curated corpus, the authors make a meaningful step toward reproducible and controlled evaluation. The inclusion of human-verified supporting documents and challenging negative samples strengthens the benchmark’s validity and difficulty, ensuring that evaluation results reflect true retrieval and reasoning capabilities."}, "weaknesses": {"value": "1. The term Deep Research Agent in the title and whole paper seems an overclaim for the presented benchmark. At most, they can be referred to as Search Agents. Deep Research is a much more complex concept that involves tool use, evidence search, and synthesizing a comprehensive report at the end. That belongs to the category of “deep research,” which is also the focus of products like those from  OpenAI and Google Gemini. What this paper presents is more like search agents that only handle relatively simple QA-style problems. Therefore, it is an overclaim. Besides, since this work is based on BrowseComp by OpenAI, and the original paper only refers to Browsing Agents, the naming here should more appropriately be Search Agents, as this paper fixes the corpus database. Therefore, the current title and terminology are somewhat overstated and should be revised.\n\n2. In the results section, it is suggested to add another type of baselines where each model is tested without using any external tools, relying only on the model’s own internal knowledge and skills. This would help observe whether there are other interesting phenomena and better understand the contribution of retrieval versus inherent model ability.\n\n3. I understand that the authors evaluated Search-R1 using the open-source checkpoint to show that “the benchmark is effective in distinguishing the performance of deep research systems.” However, this is somewhat strange, because Search-R1 was not trained with such complex data. In the BrowseComp paper, the authors clearly stated that “the Deep Research model is trained on data that specifically teach the model to be good at BrowseComp tasks.” Therefore, for a benchmark paper, it would be better for the community if it could provide some training data and verify on several models that these training data are effective."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bzvBCT9r5U", "forum": "jjIKGiGqOo", "replyto": "jjIKGiGqOo", "signatures": ["ICLR.cc/2026/Conference/Submission1324/Reviewer_4NMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1324/Reviewer_4NMg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530293202, "cdate": 1761530293202, "tmdate": 1762915736035, "mdate": 1762915736035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose BrowseComp-Plus, a benchmark for evaluating deep-research agents which can search and reason over web content.\n\nUnlike BrowseComp that rely on live web APIs, BrowseComp-Plus uses a fixed, human-verified corpus with both supporting and hard-negative documents for the goal of having fair and reproducible benchmark. The dataset is built by combining model-guided evidence retrieval, human verification, and query-based negative mining.\n\nExperiments test various agent–retriever pairs (like GPT-5, Qwen3-32B, BM25) on accuracy, recall, and efficiency. The authors aim was to show that BrowseComp-Plus offers a transparent, controlled benchmark for disentangling retrieval and reasoning in deep-research agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and address an important problem related to Deep Research.\n\nThe paper proposes a large corpus to address multihop question answering with human-verified supporting facts and hard-distractor facts.\n\nThe authors evaluate multiple agent–retriever combinations across many experiment configurations including ablations related to open and closed source models."}, "weaknesses": {"value": "It is not clear that this benchmark really measures deep research, as deep research question are more open-ended like \"How can global climate policy reconcile economic growth with ecological preservation?\". It feels more like a direct multi-hop question answering with closed form answers.\n\nThe idea and setup are very similar to Mind2Web2 (https://arxiv.org/abs/2506.21506), which also constructs a controlled environment for web reasoning and fact retrieval.\n\nThe paper does not explain how recall is handled when answers come from different websites that contain the same facts.\n\nThe use of LLM-as-a-judge may not be reliable. There should be some human evaluation to check if the model’s scores agree with human judgments (see this Deep Research Bench paper for inspiration https://arxiv.org/pdf/2506.11763).\n\nThere are no error bars or variance estimates, so we can’t tell if the differences between models are significant or just noise, and we can't tell if the llm-as-a-judge has large variance across runs.\n\nIt is not clear if the dataset covers diverse domains or if it is limited to a few common topics.\n\nThe finding that better retrieval gives higher accuracy is quite obvious as if the facts are not recalled it is impossible to achieve good solution. It is not clear why the llm is not able to answer the questions though if all the facts are recalled. This is something that needs to be studied.\n\nThe setup looks a lot like a retrieval-augmented generation (RAG) system. It’s unclear whether the agent really \"browses\" the web or just reads from a static collection of text files. Being able to navigate the web is a good challenge in itself. While the authors claim that web browsing is problematic due to changing content, the authors can always focus on timestamped posts that don't change over time and they can also use wayback machine (https://web.archive.org/).\n\nTrue deep research should handle different file types (like PDFs, tables, or HTML pages), but this work seems limited to plain text.\n\nThe paper does not compare how well humans perform on these tasks, so we don’t know how difficult the benchmark really is. See GAIA for inspiration on how and why this is done (https://arxiv.org/abs/2311.12983).\n\nThere is no clear analysis of task difficulty (usually there are 3 levels like in GAIA) or model failure cases, which would help us understand where the models struggle."}, "questions": {"value": "How do you distinguish deep research from multi-hop question answering in your benchmark, and what makes your tasks more complex than standard QA?\n\nHow is recall measured when multiple sources contain the same information are recalled if the website that the agent has acquired is not part of the groundtruth? and how do you deal with websites that have conflicting information, were all the items in the corpus checked for conflicts?\n\nDid you validate the LLM-as-a-judge results with human evaluations to confirm that the scoring model is reliable?\n\nCan you provide error bars or statistical tests to show whether the differences between models are significant?\n\nHow diverse are the domains and file types in your dataset, and could you expand the benchmark to include other content formats like PDFs, tables, or webpages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CsTHzp7tsp", "forum": "jjIKGiGqOo", "replyto": "jjIKGiGqOo", "signatures": ["ICLR.cc/2026/Conference/Submission1324/Reviewer_b7SQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1324/Reviewer_b7SQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154998652, "cdate": 1762154998652, "tmdate": 1762915735860, "mdate": 1762915735860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}