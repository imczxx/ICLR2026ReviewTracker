{"id": "MPMyROJvJV", "number": 6851, "cdate": 1757998235701, "mdate": 1759897888268, "content": {"title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs", "abstract": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization promises hardware-friendly, matmul-free inference by stacking binary ($\\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during Quantization-Aware Training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and crippling the model's expressive capacity. While prior work relies on heuristic workarounds (e.g., path freezing) that limit model capacity, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, ensuring each path corrects its predecessor's error. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a 4.49$\\times$ over full-precision models.", "tldr": "", "keywords": ["Large language models", "Low-bit quantization", "Binarization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7fdb3b40d77ac96d31a51a2f30892dcd7de21f8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a novel dual-binarization QAT framework, termed RaBiT. It updates the shared full-precision weights and dual scales, while dynamically deriving binarization matrices. The authors provide both theoretical and empirical evidence to elaborate on why this coupled QAT framework outperforms standard QAT. Additionally, they demonstrate the framework’s efficiency in both training and inference phases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical analysis of coupled QAT is interesting, with adequate details and analysis. Relevant experiments are also conducted to validate the theory.\n2. RaBiT exhibits impressive performance, hitting SOTA in 2-bit quantization with satisfactory speedup.\n3. Sufficient details on training settings and kernel design are provided, which may benefit the community for future research."}, "weaknesses": {"value": "1. This work’s primary contribution lies in the coupled QAT training framework; however, the residual binarization scheme and initialization method have been extensively explored in prior studies. This may constrain the novelty of the paper.\n2. It remains unclear whether the compared methods were trained on identical datasets with the same number of iterations. Additionally, the optimizers employed in these baselines differ from Muon. Considering that Muon might result in better performance, it would be preferable to conduct comparisons under consistent settings or perform ablation studies of RaBiT across different optimizers.\n3. The training overhead of RaBiT is several times that of EfficientQAT, which requires only 4.8 GPU hours on an A100 for a 7B model. It is uncertain whether RaBiT can still significantly outperform low-bit quantization schemes under full QAT settings."}, "questions": {"value": "1. The paper claims, \"Key hyperparameters, such as the learning rate and the I/O Channel Importance Scaling intensities (α_in, α_out), were fine-tuned for each specific model to achieve the best performance.\" Could the authors share results to show how sensitive performance is to different learning rates?\n2. Residual binarization schemes are also widely used in PTQ methods like BiLLM [1] and ARB-LLM [2]. It would be helpful if the authors could add a brief discussion about these works, to better illustrate the advantage of QAT over PTQ in residual binarization schemes.\n3. The dual-scale residual binarization format is also adopted in ARB-LLM, which proposes an alternating refined algorithm to determine dual scales. This algorithm could also be regarded as an initialization method. It is suggested that the authors might consider comparing it with Iterative Residual SVID, as such a comparison could help make the paper’s conclusions more comprehensive and robust.\n\n[1] BiLLM: Pushing the Limit of Post-Training Quantization for LLMs, ICML, 2024.\n\n[2] ARB-LLM: Alternating Refined Binarizations for Large Language Models, ICLR, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qNsO5DN9xN", "forum": "MPMyROJvJV", "replyto": "MPMyROJvJV", "signatures": ["ICLR.cc/2026/Conference/Submission6851/Reviewer_1dWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6851/Reviewer_1dWZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761135792559, "cdate": 1761135792559, "tmdate": 1762919108756, "mdate": 1762919108756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RaBiT, a novel Quantization-Aware Training (QAT) framework designed to overcome the \"inter-path adaptation\" problem in 2-bit residual binarization, where parallel paths learn redundant features. The core idea is \"on-the-fly residual coupling,\" which dynamically derives all binary paths from a single shared full-precision weight during training. This algorithmically forces each path to correct the residual error of its predecessor, preventing redundancy. Combined with a robust function-aware initialization, RaBiT achieves good results in terms of accuracy and latency."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written and easy to follow.\n- The paper aims to tackle a hot and important topic, which is to reduce the inference complexity of LLMs.\n- The paper demonstrates strong performance in accuracy and latency compared to the existing baseline, although it raised some concerns to the reviewer."}, "weaknesses": {"value": "- The paper’s main analysis and motivation focus on the decomposition of the MSE loss presented on page three. However, the experimental results contradict this hypothesis. The proposed framework combines KL divergence with intermediate MSE losses, yet the contribution of this complex knowledge distillation (KD) setup is never ablated. Notably, the authors disable this component for the Gemma models ($\\gamma=0$) to “avoid instability,” implying that the KD mechanism is sensitive and not universally beneficial. This observation suggests that the KL loss is the dominant factor, consistent with findings in prior literature. Moreover, in some experiments, the authors completely omit the MSE loss, leaving unclear how the so-called inter-path adaptation influences the KL loss. In addition, it is not clear how the MSE analysis holds for the case of 3 or more binary bases. The theoretical proofs, such as Proposition 1 and 2, which analyze inter-path adaptation and error correction, are based on simplified mathematical models. For instance, the proof for inter-path adaptation relies on the change in the Frobenius inner product between weights. This does not fully capture the complex, non-linear dynamics of a complete neural network.\n\n- The novelty of the paper is limited, as the parameterization using iterative SVID has already been proposed in the MBOK paper. The significance of maintaining the hierarchical structure of kernels and the observation that only a single set of binary weights needs to be trained were also thoroughly explored and ablated in MBOK. The authors attempt to adapt this approach by employing a single set of full-precision latent weights and justify it using the concept of inter-path adaptation; however, this reasoning is unsound, as discussed in the previous bullet point, and the evaluation may not be fair compared to prior baselines (to be clarified in the following points).\n\n- Moreover, the paper substantially overstates its novelty by claiming to redefine the 2-bit accuracy–efficiency frontier, despite considerable concerns about reproducibility. The core of this claim appears to depend on a custom CUDA kernel, which is only briefly described in the Appendix and lacks sufficient detail. Additionally, this design relies on packing and unpacking techniques that typically introduce non-negligible computational overheads.\n\n- The \"Coupled Forward Pass,\" which is the central mechanism of RaBiT, introduces a significant computational overhead during training that is not present during inference, especially for the case of using multiple binary bases. While the final inference pass is fast and parallel, the training pass is not. For every forward pass during training, the model must dynamically re-calculate the binary core matrices from the single full-precision shared weight. As a result, the training process is computationally expensive, requiring a significant number of training hours even on powerful hardware. For instance, training Llama2-7B took 39 hours on a node with four NVIDIA H100 GPUs. This high cost is a practical limitation for many researchers and practitioners.\n\n- The initialization process has several crucial hyperparameters, including the SVID iteration count ($T_{max}$) and the I/O channel importance scaling intensities ($\\alpha_{in}$, $\\alpha_{out}$. The paper shows these are determined via a \"comprehensive grid search\" for each model to minimize the initial KL loss. This adds a significant layer of model-specific tuning and computational overhead before the main training can even begin. For example, the optimal $\\alpha$ values for Llama2-7B were found to be (0.8, 0.65), while for Llama3-8B they were (0.85, 0.7).\n\n- During the backward pass, the gradient for the shared weight $W_{FP}$ is calculated using an \"effective-weight gradient\" that acts as a Straight-Through Estimator (STE) for the entire coupled derivation process. This is a strong simplification that ignores the complex, sequential relationship between the paths during derivation. The paper does not analyze the potential impact of this approximation on training stability or final performance.\n\n- The authors state they re-implemented two baselines (DB-LLM and MBOK) and created a custom, optimized CUDA kernel for another (DBF) to ensure a \"fair and robust comparison\". While laudable, this means the performance reported for these methods may not align with their original papers. More importantly, the paper details the extensive hyperparameter grid search for RaBiT's initialization but provides no such details for the baselines, leaving open the possibility that the competing methods were not as thoroughly tuned. For example,  all models were trained with the Muon optimizer. It is unclear how RaBiT would perform with a more conventional optimizer like AdamW. This makes it hard to know if the strong results are due, in part, to a non-standard optimizer choice.\n\n- In the appendix, the authors observe that RaBiT dramatically suppresses the high MSE peaks seen in the early layers of the network with Standard QAT. They admit they do not fully understand the cause, stating that a \"full investigation into how residual coupling imparts this stability is a compelling direction for future research\". This indicates a gap in the complete understanding of their own method's benefits."}, "questions": {"value": "- The paper claims to achieve \"state-of-the-art (SOTA) performance,\" but the data in Table 2 shows RaBiT is outperformed by QTIP on Llama2-13B in zero-shot accuracy (62.92% vs 62.10%) and by DBF on WikiText-2 PPL (5.11 vs 5.15). Similarly, on Gemma3-12B, QTIP performs better. Could you clarify the SOTA claim when RaBiT does not uniformly outperform competitors on all tested models?\nI would expect the authors to provide more experimental results on benchmarks such as GSM8k and MMLU(-Pro), as multiple pieces of evidence should demonstrate that different quantization methods will result in a significant performance gap on these challenging benchmarks.\n- The coupled forward pass dynamically re-calculates binary cores ($B_i$) and residuals ($R_i$) in a sequential manner for every training step. Have you benchmarked the wall-clock time and memory overhead of this training step compared to a standard QAT, and how does this additional cost scale?\n- The backward pass approximates the gradient for the shared $W_{FP}$ using a single Straight-Through Estimator (STE) for the entire coupled derivation process4. What is the impact of this simplification, and how does it compare to a more complex gradient that might better account for the sequential $R_{1} = W_{FP} - \\hat{W}_{1}$ step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "80IsQhs0Uv", "forum": "MPMyROJvJV", "replyto": "MPMyROJvJV", "signatures": ["ICLR.cc/2026/Conference/Submission6851/Reviewer_H1Ab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6851/Reviewer_H1Ab"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865965033, "cdate": 1761865965033, "tmdate": 1762919108293, "mdate": 1762919108293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RaBiT, a residual-aware binary training framework for LLMs that targets low-bit (2-3 bits) weight quantization. The key idea is to generate each binary path sequentially from the residual of the previous one, while maintaining a shared latent high-precision weight, thereby preventing the usual “inter-path adaptation” where multiple binary branches collapse into similar directions. RaBiT couples this with function-aware initialization and learnable per-path scales, demonstrating competitive or better perplexity than prior 2-bit/binary methods while remaining matmul-free at inference, supported by custom bit-packed kernels on the GPU."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies “inter-path adaptation” (multiple binary paths learn redundant features under shared gradients) as the key reason multi-binary LLMs underperform. This is a concrete, observable failure mode.\n2. The hypothesis is empirically supported. Switching from the “Standard QAT” to the “Coupled / residual-aware” training already recovers a large portion of the quantization gap, and further initialization helps.\n3. Simple inference-time form. After training, the model is reduced to a small number of binary paths and learnable scales. The FP latent weight is discarded, making deployment straightforward and eliminating the need for matrix multiplication.\n4. Well-studied with ablations.\n5. The paper implements an actual kernel for RTX 4090 and reports end-to-end inference speedups. That raises the practical value of the work."}, "weaknesses": {"value": "1. On the larger models, RaBiT does not clearly win zero-shot across tasks. For models of that size, zero-shot should be the headline, not only commonsense-style scores. (There’s also a mis-bolded PIQA number for LLaMA-13B in the appendix.)\n2. Results are single-run, no CIs, no multi-seed or alternative calibration subsets. So close numbers vs baselines are not conclusive and could flip with another seed.\n3. No long-context or instruction-tuned/chat evaluations. Given the close zero-shot numbers, this is something that can distinguish methods.\n4. The initialization ablation study primarily focuses on the initial reconstruction error. However, it doesn’t fully demonstrate how much of the final PPL improvement is attributed to initialization versus the coupled training itself.\n5. More recent QAT methods that are capable of very low-bit QAT(Like BitNet[Wang et al.], QuEST[Panferov et al.]) are not compared against. \n6. The method is limited to weight quantization. There’s no activation quantization.\n7. For MBOK (and similar), the paper used the same optimizer as for RaBiT. Fairer would be to run the baselines in their native, published setup first, then add as an extra comparison with your optimizer."}, "questions": {"value": "1. Each training step recomputes binary paths from the shared FP weight and applies KD on a 200M-token corpus. Can you quantify the wall-clock/step and total training time relative to a plain 2-bit QAT run without coupled residualization?\n2. All kernel and end-to-end throughput numbers are on a single RTX 4090. Do the same speedups hold on A100/H100 (with different memory hierarchies and Tensor Core priorities)?\n3. The “Standard QAT” baseline is somewhat underspecified. It’s not fully clear whether their baseline is just sign+STE or a stronger 1/2-bit QAT recipe (e.g., with per-channel scales, learned clipping). Could you please explain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g8sqHpNSRs", "forum": "MPMyROJvJV", "replyto": "MPMyROJvJV", "signatures": ["ICLR.cc/2026/Conference/Submission6851/Reviewer_QBGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6851/Reviewer_QBGj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946202543, "cdate": 1761946202543, "tmdate": 1762919107860, "mdate": 1762919107860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper begins by identifying a type of co-adaptation in residual binarization (called \"inter-path adaptation\"). Such co-adaptation appears if the network/method fails to break he correlation between loss paths. The authors show that a smaller/negative correlation directly improves the MSE loss value, and motivated by this, present RABIT, which achieves small/negative path correlations by design.\n\nThe key idea is to couple the weights of the stacked binary levels of a layer into a single full-precision matrix. Then the method is designed such that each level involves a binarized version of the residual so far + (learnable) per-channel scaling factors. The method is then combined with STE for backward pass and a tailored initialization method, and superior accuracy/efficiency is shown compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I generally think this is a solid paper, with a clear presentation and a well-motivated method. The experiments show good quality, and are also complemented by real runtime measurements."}, "weaknesses": {"value": "- Experiments limited to fine-tuning, which limits the impact of the paper.\n- QTIP seems to outperform RABIT in terms of accuracy/loss on larger models."}, "questions": {"value": "1. While the method shows strong performance for fine-tuning, and the initialization method is also tailored for fine-tuning, from my side, it remains an interesting question that whether or not RABIT would achieve similar level of quality in pre-training tasks. Do you have any insight/experiments for the pre-training setting?\n\n2. In both Tables 2 and 3, QTIP seems to outperform RABIT in the 2-bit regime on the largest models (>10B). This raises a concern regarding the scalability of RABIT to even larger scales (e.g., Llama-2 70B). Could the authors add a comparison with the main baselines for a larger model?\n\n3. It's mentioned that RABIT halves the memory footprint. Can the authors clarify that with respect to which methods the memory is halved? For example, a standard QAT method (with no binarization), would also keep a single weight matrix for each layer, hence if my understanding is correct, there's no memory saving there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ApezE4Lcjm", "forum": "MPMyROJvJV", "replyto": "MPMyROJvJV", "signatures": ["ICLR.cc/2026/Conference/Submission6851/Reviewer_7GpF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6851/Reviewer_7GpF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998549830, "cdate": 1761998549830, "tmdate": 1762935281722, "mdate": 1762935281722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}