{"id": "R1HcIN90A1", "number": 10417, "cdate": 1758170833289, "mdate": 1759897652029, "content": {"title": "ModalMix: Optimizing Multimodal Data Mixtures with Compute-Dependent Regression", "abstract": "Training large multimodal models requires optimizing the data mixture to balance cross-modal synergies against finite computational resources. \nHowever, existing heuristics for data mixing largely ignore the underlying cross-modal dynamics and their dependence on compute scaling. \nIn this work, we propose ModalMix, a framework that formalizes data mixture optimization by simultaneously modeling cross-modal interactions and compute-dependent scaling laws. ModalMix yields a predictive regressor for the optimal data mixture at any given computational budget. \nEmpirically, models trained with ModalMix achieve 1.4× faster convergence than those with a uniform data distribution, alongside a 47\\% better average rank over 17 downstream tasks. \nThe framework reveals that the optimal strategy is dynamic, not static: it initially prioritizes speech, then gradually shifts focus towards image-text data as compute increases, while maintaining stable use of text data. \nModalMix offers a flexible and principled solution to the data-mixing problem, bridging a critical gap between scaling theory and practical multimodal pretraining.", "tldr": "ModalMix optimizes data mixture for large multimodal model training (modeling cross-modal interactions and compute dependence) to find optimal data ratios, enabling 1.4× faster convergence, 47% better average rank across 17 tasks.", "keywords": ["Scaling Laws; Omni-modal Models; Data Mixture"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad5d4aa315a392f930f476486ef059a62eb5d956.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ModalMix, a regression-based framework that optimizes multimodal data mixtures by jointly modeling cross-modal interactions and compute-dependent scaling laws. The key idea is to predict the optimal data mixture for any given computational budget, addressing the limitations of static or heuristic data allocation strategies in multimodal learning.\n\nModalMix learns scaling laws that link model size, number of samples, and modality ratios to modality-specific losses, and it constrains optimization to avoid overfitting to any single modality. Experiments across text, image, and speech modalities (17 downstream tasks) demonstrate 1.4× faster convergence and a 47% improvement in average rank over baselines. The method generalizes well to unseen larger models (e.g., 7B parameters) and offers insights into how optimal mixtures evolve with scale—transitioning from speech-heavy at small scales to text-dominant at large scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies and formalizes a critical, under-explored challenge in multimodal learning: the joint optimization of data mixture while accounting for cross-modal dynamics and computational scale. This moves beyond unimodal and static mixture strategies.\n2. The scale of the experimental setup (1,620 runs across 3 model sizes and 27 mixtures) is impressive and provides a solid foundation for the model. The results are comprehensive, showing: (1) strong performance gains in convergence speed and downstream task performance；(2) Effective generalization to an unseen, larger (7B) model."}, "weaknesses": {"value": "1. Although the paper emphasizes compute-awareness, it lacks a clear quantitative comparison of the additional cost introduced by ModalMix (e.g., time to fit scaling parameters, overhead from mixture adjustment).\n2. The study is confined to text, image-text, and speech. The framework's applicability to other important modalities like video, 3D data, or tabular data remains an open question. A discussion on this limitation and potential for generalization would strengthen the paper.\n3. The scaling laws and optimal mixtures are derived and validated within a specific architecture family (LLaVA-like with Qwen backbones). It is unclear how sensitive the findings are to the underlying model architecture (e.g., the design of the vision projector, the speech tokenizer, or the choice of a purely autoregressive objective). The \"optimal\" mixture might be architecture-specific.\n4. The ablation studies focus on predictive performance but do not test robustness to noisy or imbalanced datasets. Real-world multimodal corpora often have high variance in quality and domain coverage."}, "questions": {"value": "1. The paper mentions using L-BFGS for parameter fitting and a search over the mixture space. Could you elaborate on the computational cost of this \"outer-loop\" optimization process itself, and how it compares to the cost of the training runs used for fitting?\n2. How sensitive is the ModalMix predictor to the initial dataset composition used for fitting scaling parameters? Would training on different data sources change the predicted optimal mixtures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dk4vIDkXF1", "forum": "R1HcIN90A1", "replyto": "R1HcIN90A1", "signatures": ["ICLR.cc/2026/Conference/Submission10417/Reviewer_ukzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10417/Reviewer_ukzq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403432631, "cdate": 1761403432631, "tmdate": 1762921726599, "mdate": 1762921726599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical and practical problem of optimizing data mixtures for large-scale multimodal pre-training. The authors propose ModalMix, a novel framework that extends scaling laws to the multimodal domain. The core of ModalMix is a regression model that predicts modality-specific training losses as a function of model size, data volume, and the data mixture ratio. By fitting this model on extensive experimental data, the authors obtain a predictive regressor that can efficiently find a compute-aware optimal data mixture. Empirical results on a comprehensive suite of 17 downstream tasks show that models trained with the ModalMix-derived mixture achieve significantly faster convergence and superior overall performance compared to strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Practical Significance and Strong Empirical Results**: The paper tackles a highly relevant, real-world problem in large-scale MLLM. The proposed framework demonstrates substantial gains across 17 diverse tasks. The strong performance, validated on models up to 7B parameters, underscores the practical utility of the work.\n\n**Principled Methodological Contribution**: The work moves beyond heuristics and proposes a principled, regression-based framework. The key innovations—explicitly modeling cross-modal interactions and incorporating a constraint to prevent a \"winner-takes-all\" outcome—are well-motivated and effective. \n\n**Valuable Insights for the Community**: The finding that the optimal mixture evolves from a speech-heavy curriculum for smaller models to a more balanced, image-text-focused strategy for larger ones (Figure 6) provides actionable guidance and deepens the community's understanding of multimodal training dynamics."}, "weaknesses": {"value": "**1. Questionable Generalizability and High Cost**: The primary concern lies with the framework's practical applicability, which is constrained by two factors.\n\n   - ***Parameter Stability***: The crucial interaction parameters ($\\gamma_{ij}$) are fitted on a specific combination of model architecture (LLaVa-Qwen-SPIRAL) and datasets. It is highly probable that these parameters lack stability and would not generalize to different architectural choices, necessitating a complete and costly re-fitting process.\n   - ***High Upfront Cost***: The framework's fitting process itself requires a massive computational investment (1,620 experimental runs are mentioned). The paper does not discuss this \"meta-cost,\" making it difficult to assess the efficient value.\n\n**2. Oversimplified Modeling Assumptions**: The mathematical formulation of ModalMix relies on simplifying assumptions that may not fully capture the problem's complexity.\n\n   - ***Interaction Form***: The assumption of an exponential form for cross-modal interactions is a strong one. It is unclear if this simple form is sufficient to model more complex, non-linear dynamics.\n   - ***Data Granularity***: The method treats each modality as a monolithic block, overlooking the crucial aspect of intra-modal diversity and data quality. This is a simplification, as intra-modal mixture optimization has been proven critical in unimodal settings.\n\n**3. A Disconnect Between Insight and Experimental Application**: While Figure 6 compellingly demonstrates that the optimal strategy is a dynamic curriculum that evolves throughout training, the main experiments in Tables 2 and 3 appear to use a single, static mixture ratio for the entire training run. This approach, while facilitating a fair comparison with static baselines, fails to showcase the full potential of the discovered dynamic strategy and weakens the direct empirical support for this key finding.\n\n**4. Positioning of the Novelty**: While the work is a successful and valuable application of scaling laws to a new and important domain, its contribution is arguably more of an extension and sophisticated application of an existing paradigm rather than the introduction of a fundamentally new one."}, "questions": {"value": "The quality and impact of this work could be further enhanced if the authors can provide detailed responses to the following questions:\n\n**1. On Generalizability and Cost**: Could the authors elaborate on the expected stability of the fitted interaction parameters ($\\gamma_{ij}$) across different model architectures and pre-training datasets? How much of the expensive fitting process needs to be repeated when changing a single component of the multimodal model?\n\n**2. On the Dynamic Curriculum**: The finding that the optimal data mixture is a dynamic curriculum (Figure 6) is one of the most exciting takeaways. Could the authors clarify why a static mixture was used for the main experiments in Tables 2 and 3? Can the authors provide any results or analysis on the performance difference between a model trained with the optimal static mix versus one trained with the truly dynamic curriculum proposed by the framework?\n\n**3. On Post-Training**: This work provides a clear framework for pre-training. Do the authors foresee any challenges or necessary modifications in applying a similar scaling-law-based optimization approach to the post-training phase (e.g., for SFT or RL)? How might the dynamics of data mixing differ in that context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hldf1BnxIL", "forum": "R1HcIN90A1", "replyto": "R1HcIN90A1", "signatures": ["ICLR.cc/2026/Conference/Submission10417/Reviewer_3KNk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10417/Reviewer_3KNk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761474245582, "cdate": 1761474245582, "tmdate": 1762921726262, "mdate": 1762921726262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ModalMix, a regression-based framework for optimizing the data mixture (text, image, speech) in multimodal large language model (MLLM) pre-training. The core innovation is a scaling law that models each modality's loss as a function of model size, dataset size, and the data mixture itself, with a key term to capture cross-modal interactions. The framework is used to predict a compute-dependent optimal mixture that prevents any single modality from being neglected. Extensive experiments (1,620 runs) demonstrate superior convergence speed and downstream task performance over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Important and Well-Defined Problem: The paper expertly identifies a significant bottleneck in modern MLLM training: moving beyond static, heuristic-based data mixing to a principled, compute-aware strategy.\nRigorous and Comprehensive Evaluation: The empirical validation is a standout feature. With 1,620 experimental runs, multiple model scales, and evaluation on 17 diverse downstream tasks, the claims are strongly supported. The ability to generalize to an unseen 7B model is particularly compelling.\nActionable Insights: The paper goes beyond just presenting a method. It delivers crucial insights for the community, such as:\nThe optimal mixture is dynamic and compute-dependent, shifting from speech-heavy to more text- and image-text-centric as scale increases.\nThe correlation between pre-training loss and downstream performance is highly modality-dependent (strong for perception, weak for reasoning)."}, "weaknesses": {"value": "The balancing constraint, which is critical to the method's performance, is a standard technique from multi-task learning.\n\nTable 2 and 3 show that the improvement is limited. \n\nThe entire framework is built upon data from 1,620 experimental runs. This is a staggering computational cost that renders the method impractical for most research institutions and contradicts the paper's goal of \"optimizing... against finite computational resources.\" A method that requires thousands of training runs to avoid a grid search is not a solution; it is an admission of failure. The framework is a post-hoc analysis of an extremely expensive hyperparameter sweep, not an efficient optimization algorithm.\n\nThe baselines are weak or misrepresented. Comparing against a Uniform mixture is insufficient. A more robust baseline would be a simple, computationally cheap dynamic scheduling policy (e.g., a curriculum learning based on loss slopes, which is essentially what M2-Omni does but done more simply). The paper does not demonstrate that the complexity of ModalMix is necessary to outperform simple, intuitive heuristics.\n\nThe paper fails to provide a meaningful analysis of the learned cross-modal parameters γᵢⱼ. These parameters are the key to the claimed \"modeling of cross-modal dynamics,\" yet they are not presented or interpreted. Without this, the model remains a black box, and the nature of the purported synergies and conflicts is merely speculative.\n\nThe \"key insight\"—that the optimal mixture is compute-dependent—is an expected phenomenon. It is unsurprising that a model's data diet should change as its capacity (model size) and training duration (number of samples) change. The paper dresses up this intuitive concept with complex machinery but fails to derive truly novel or surprising scientific knowledge from it.\n\nThe discussion on the correlation between training loss and downstream performance, while interesting, is preliminary and feels like a secondary observation rather than a core contribution."}, "questions": {"value": "Efficiency: The method requires 1,620 training runs for its initial fitting. Can you justify how this is a practical or efficient solution compared to existing methods, and can you provide an analysis of the total computational cost (including these runs) versus the baselines?\n\nInterpretability: You claim to model \"cross-modal dynamics,\" but you do not show the learned interaction matrix γ. Please provide this matrix and a detailed interpretation of its values. What specific, non-obvious cross-modal relationship did ModalMix discover that was not previously known?\n\nBaselines: Why was a stronger, simple dynamic baseline not implemented? For example, a scheduler that simply increases the proportion of the modality with the highest current loss—a common practice in multi-task learning—would be a more meaningful point of comparison.\n\nThe cross-modal interaction parameters γᵢⱼ are a central result. Could you provide a table or analysis of the learned γ values? Interpreting these (e.g., \"we find γ_speech→text is strongly positive, indicating synergy\") would greatly strengthen the narrative and help validate whether the model is learning intuitive relationships.\n\nThe constraint L_i ≤ (1+ε)L_i^{lb} is crucial. Was the tolerance ε=0.1 chosen via ablation? Can you comment on the sensitivity of the results to this value? A small ablation study would solidify this design choice.\n\nThe framework predicts a static optimal mixture for a given (N, D). However, Figure 2b suggests the optimal mixture should change during a single training run. How would you extend ModalMix to produce a dynamic scheduling policy, rather than a static mixture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4udVRnHR5u", "forum": "R1HcIN90A1", "replyto": "R1HcIN90A1", "signatures": ["ICLR.cc/2026/Conference/Submission10417/Reviewer_unVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10417/Reviewer_unVx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920407834, "cdate": 1761920407834, "tmdate": 1762921725742, "mdate": 1762921725742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ModalMix, a method for optimizing data mixtures for multimodal model pre-training. ModalMix achieves this by explicitly modeling cross-modal interactions (text, image-text, speech) and compute-dependent scaling dynamics. Instead of using fixed heuristic ratios, ModalMix builds a regression-based scaling law that predicts each modality’s loss as a function of model size, number of samples, and mixture ratios, including synergy/conflict terms across modalities. The framework then identifies a compute-optimal mixture that minimizes loss without sacrificing any modality’s capability."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper offers clear empirical evidence of cross-modal interactions. \n- It also provides a nice example that shows optimal data mixture is compute-dependent"}, "weaknesses": {"value": "- The core formulation (Eq. 1) resembles that of Ye et al. [1]. This raises questions on the novelty of the work. \n- The paper does not report error bars (in Tables 1 and 2). Given that many performance differences are small, knowing the scale of variability is necessary to draw any conclusion \n- Relatedly, the method requires extensive runs across 0.5B–3B scales to show modest gains at the 1.5B scale, and improvements at 7B are even narrower across benchmarks.\n- Computation cost is a critical dimension in optimizing data mixture, yet the paper does not provide compute cost for different methods. \n- The work does not discuss or compare against several relevant works for optimizing data mixture [1-6]. The baselines considered are quite limited. Coupled with all the points above, it is extremely difficult to tell whether ModalMix provides genuine gains over existing methods. \n\n[1] Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance\n\n[2] DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining\n\n[3] Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws\n\n[4] Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance\n\n[5] Data Mixture Optimization: A Multi-fidelity Multi-scale Bayesian Framework\n\n[6] ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization"}, "questions": {"value": "- How many 1.5B parameter models were trained for fitting RegMix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aOaKNclCPt", "forum": "R1HcIN90A1", "replyto": "R1HcIN90A1", "signatures": ["ICLR.cc/2026/Conference/Submission10417/Reviewer_f4MD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10417/Reviewer_f4MD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975665735, "cdate": 1761975665735, "tmdate": 1762921725276, "mdate": 1762921725276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ModalMix proposes a data mixing optimization method by modeling the patterns of cross modal interactions and computational dependencies, balancing cross modal synergistic effects in situations where computing resources are limited. The experiment shows that this method has significant improvements in training convergence speed and multitasking performance. Most importantly, ModalMix suggests that the optimal strategy for data mixing is dynamically changing, and the optimal data allocation ratio will also vary with different computing resources. This method provides a flexible and theory driven framework for multimodal training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Innovation: The ModalMix framework introduces a novel and highly relevant solution to optimizing multimodal data mixtures, addressing a gap in how the data mixture changes dynamically depending on computational budgets and model scale. The core idea of dynamically adjusting the mixture of modalities based on computational resources is both intuitive and innovative. This contrasts with prior heuristic-based approaches which often overlook the interdependencies between modalities and scaling dynamics. By using regression-based prediction, ModalMix can efficiently estimate the optimal data mixture for any given model and compute budget, offering a much-needed framework to optimize multimodal training processes.\n2.Experiment: The experimental validation is robust and comprehensive. The authors performed over 1,620 experiments across different model sizes and data mixture configurations. This empirical approach strengthens the framework's credibility, showing significant improvements in training convergence speed and downstream task performance. Additionally, the model is tested on unseen data and larger model scales, demonstrating its scalability and generalization ability.\n3. Writing: The paper is well-structured and clearly written, presenting complex ideas in an understandable manner. The background, methodology, experiments, and results are logically organized, making it easy for the reader to follow. The use of visualizations (such as Figure 1 and Figure 2) effectively illustrates the dynamic nature of optimal data mixtures, further supporting the paper's arguments."}, "weaknesses": {"value": "1. Insufficient related work: In the related work, Visual Large Models, especially SAM and DINO, should be mentioned. How do they process data, and are there any similarities in the processing methods between visual big models and language big models. \n2. Insufficient experiments: The experimental section of Part 4 mentions using pretrained models for testing, so the discussion on whether this strategy is applicable to non pretrained models needs to be added.\n3. Insufficient discussion: The paper discusses the mixing strategy of data in large models, and the discussion on whether this strategy can be extended to lightweight models needs to be added."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RVpclcBicj", "forum": "R1HcIN90A1", "replyto": "R1HcIN90A1", "signatures": ["ICLR.cc/2026/Conference/Submission10417/Reviewer_wtc7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10417/Reviewer_wtc7"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986943302, "cdate": 1761986943302, "tmdate": 1762921724513, "mdate": 1762921724513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}