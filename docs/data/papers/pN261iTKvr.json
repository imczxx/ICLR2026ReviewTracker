{"id": "pN261iTKvr", "number": 15800, "cdate": 1758255376981, "mdate": 1759897281256, "content": {"title": "Learning to Segment for Vehicle Routing Problems", "abstract": "Iterative heuristics are widely recognized as state-of-the-art for Vehicle Routing Problems (VRPs). In this work, we exploit a critical observation: a large portion of the solution remains stable, i.e., unchanged across search iterations, causing redundant computations, especially for large-scale VRPs with long subtours. To address this, we pioneer the formal study of the First-Segment-Then-Aggregate\n(FSTA) decomposition technique to accelerate iterative solvers. FSTA preserves stable solution segments during the search, aggregates nodes within each segment into fixed hypernodes, and focuses the search only on unstable portions. Yet, a key challenge lies in identifying which segments should be aggregated. To this end, we introduce Learning-to-Segment (L2Seg), a novel neural framework to intelligently\ndifferentiate potentially stable and unstable portions for FSTA decomposition. We present three L2Seg variants: non-autoregressive (globally comprehensive but locally indiscriminate), autoregressive (locally refined but globally deficient), and their synergy. Empirical results on CVRP and VRPTW show that L2Seg accelerates state-of-the-art solvers by 2x to 7x. We further provide in-depth analysis showing why synergy achieves the best performance. Notably, L2Seg is compatible with traditional, learning-based, and hybrid solvers, while supporting various VRPs.", "tldr": "", "keywords": ["Learning-Guided Optimization", "Vehicle Routing Problem"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38bdd43722be427fc4aca3ec51c63560467df04e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes First-Segment-Then-Aggregate (FSTA), a decomposition framework for large-scale VRPs, motivated by the observation that in iterative optimization methods, a large portion of the intermediate solution structure tends to remain stable across iterations.\nTo effectively identify which parts of the solution should be modified and which should remain fixed, the authors introduce a neural network–based module called Learning-to-Segment (L2Seg), which enables efficient segmentation within FSTA.\nThrough experiments on CVRP and TSPTW, the authors demonstrate that integrating L2Seg can accelerate existing iterative solvers by a factor of approximately 2×–7×, while maintaining or improving solution quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method successfully restricts the search space and achieves higher solution quality within the same computational budget on large-scale CVRP and TSPTW instances.\n\n- The framework is solver-agnostic and can be applied to multiple backbone iterative solvers, demonstrating its flexibility.\n\n- The experimental evaluation is comprehensive, including comparisons against diverse baselines from different methodological categories."}, "weaknesses": {"value": "- Since L2Seg is trained in a supervised manner, its performance may degrade when the distribution of intermediate solutions during inference differs significantly from that of the training data.\n\n- The behavior of L2Seg appears conceptually closer to search-space restriction rather than genuine problem decomposition. Consequently, there is a risk that the restricted search could lead to premature convergence to local optima, preventing the discovery of globally superior solutions.\n\n- The model is trained and evaluated on problem instances of the same size distribution, leaving it unclear whether L2Seg generalizes to unseen problem scales. If it fails to generalize, collecting sufficient training data for larger instances could become computationally prohibitive, limiting the practical applicability of the approach.\n\n- (Minor) Typo found at line 130.\n\n- (Minor) References in lines 73–75 should be enclosed in parentheses."}, "questions": {"value": "- How do the authors precisely define “iterative solvers” in the context of this paper?\n\n- The paper states that “a large portion of the solution remains stable.” Does this mean that most parts of the solution are not updated because they do not significantly contribute to objective improvement, or because only a small subset of solution elements is actually targeted for update by the iterative process?\n\n- Does L2Seg generalize to problem sizes that were not seen during training? If not, how feasible is it to train the model for significantly larger problem instances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pIlJjtQOYY", "forum": "pN261iTKvr", "replyto": "pN261iTKvr", "signatures": ["ICLR.cc/2026/Conference/Submission15800/Reviewer_L8xN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15800/Reviewer_L8xN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802186844, "cdate": 1761802186844, "tmdate": 1762926031820, "mdate": 1762926031820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Learning-to-Segment (L2Seg), a learning-based framework that accelerates iterative heuristics for Vehicle Routing Problems (VRPs). The key idea is to detect stable and unstable segments in existing solutions, aggregate stable portions into hypernodes via a First-Segment-Then-Aggregate (FSTA) decomposition, and then re-optimize only the unstable parts.\nThe authors design three neural variants (NAR, AR, and SYN)  and demonstrate empirical speedups over baseline solvers such as LKH-3, LNS, and L2D, on both CVRP and VRPTW benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The empirical study is well executed, covering multiple VRP variants, backbone solvers, and problem scales (1k–5k). Ablation studies, oracle comparisons, and visual analyses provide convincing evidence that the proposed framework accelerates iterative search.\n\n- The proposed framework is well documented, including pseudocode and architectural details. The authors have made strong efforts to ensure reproducibility and practical relevance."}, "weaknesses": {"value": "- Limited conceptual novelty: the proposed framework can be viewed as a natural neural extension of existing decomposition-based heuristics. While the idea of stability is interesting, the framework essentially replaces a search space of LNS.\n- Restricted theoretical contribution: the theoretical analysis is limited to the monotonicity of the FSTA reduction — that improving a reduced problem implies improving the original one. However, the paper lacks theoretical analysis regarding solution quality guarantees or learnability.\n- Dependence on heuristic supervision: the training labels rely on the lookahead procedure with heuristic solvers. This raises concerns about the correctness of the label."}, "questions": {"value": "- Are there any justifications of the label about stability? It is not entirely clear whether the notion of stability—as defined by differences between consecutive heuristic solutions—is reliable.\n- Are there any baselines using LNS with ML-based neighborhood selection? If such a baseline exists, the efficiency of proposed method can be more convincingly positioned as an improved (or generalized) version of ML-enhanced LNS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSjktJ8JxS", "forum": "pN261iTKvr", "replyto": "pN261iTKvr", "signatures": ["ICLR.cc/2026/Conference/Submission15800/Reviewer_Kw4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15800/Reviewer_Kw4B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823983659, "cdate": 1761823983659, "tmdate": 1762926031441, "mdate": 1762926031441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the problem of redundant computation in iterative VRP solvers. The authors observe that during the iterative search process, a large portion of the solution stabilizes and no longer changes, yet the solver still consumes computational resources on these stable parts. To address this, the paper proposes FSTA (First-Segment-Then-Aggregate), a formalized decomposition framework with a full theoretical proof, to identify stable segments in the solution, aggregating their nodes into fixed hypernodes and then focusing the search only on the reduced unstable parts. Furthermore, this paper proposes L2Seg (Learning-to-Segment), a novel neural network framework for segmentation identification, including network architecture, training, and inference processes. Empirical results show that L2Seg can achieve speedups of 2 to 7 times for existing classical and learning-based solvers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's proposed method of learning to identify and freeze stable segments to accelerate iterative search is intuitive and novel. The proposed method is promising and achieves SOTA performance on most of the testing cases.\n2. The FSTA framework is technically sound and empirically robust. It is formalized, and the authors provide theoretical proofs of its feasibility and monotonicity across various VRP variants (CVRP, VRPTW, VRPB, etc.).\n3. The authors tested L2Seg on three different and representative backbone solvers (LKH-3, LNS, L2D), demonstrating its flexibility and versatility. Ablation experiments strongly support the necessity of the learned component (compared to stochastic FSTA)."}, "weaknesses": {"value": "1. The L2Seg-SYN process seems a bit complicated. It is unclear how much time is consumed by the L2Seg-SYN prediction step. If the L2Seg-SYN prediction step itself is costly, the 2x-7x speedup may only be noticeable over long runs.\n2. While L2Seg has been successfully applied to LKH-3, LNS, and L2D, Appendix B.1.4 mentions that applying it to HGS (another top-level solver) requires modifying the HGS source code, which is left for future work. This is a reasonable limitation, but it means that L2Seg is not \"plug and play\" for all iterative solvers."}, "questions": {"value": "1. Would better look-ahead heuristics lead to a better L2Seg model and ultimately better solution quality?\n2. How does the time spent on the L2Seg-SYN prediction step compare to the time spent running the backbone solver in a single iteration?\n\nMinor:\n\n3. Some double quotes are different, such as the one in line 252.\n4. Lines 47-48 write: FSTA identifies stable segments and then aggregates them as fixed hypernodes. But lines 146-147 write: FSTA segments the VRP solutions by identifying unstable portions, and then groups them into hypernodes. It seems there is a typo."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y4JjsytCGa", "forum": "pN261iTKvr", "replyto": "pN261iTKvr", "signatures": ["ICLR.cc/2026/Conference/Submission15800/Reviewer_b73r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15800/Reviewer_b73r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832442923, "cdate": 1761832442923, "tmdate": 1762926031046, "mdate": 1762926031046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Learning-to-Segment (L2Seg), a novel learning-guided framework designed to accelerate iterative solvers for large-scale VRPs. It first formalizes a First-Segment-Then-Aggregate (FSTA) decomposition technique, which identifies stable segments in a solution and aggregates them into hypernodes, thereby reducing the problem size for re-optimization. It also employs neural models to predict unstable edges. The paper presents three variants of L2Seg: a non-autoregressive (NAR) model for global prediction, an autoregressive (AR) model for local precision, and a synergized (SYN) model that combines their strengths. Extensive experiments on CVRP and VRPTW show that L2Seg accelerates state-of-the-art solvers by 2x to 7x while maintaining or improving solution quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper is generally well-written and structured.\n\n(2) The novel FSTA framework and the specific problem of learning to segment for decomposition are a fresh perspective.\n\n(3) The FSTA framework is well-motivated, and its theoretical properties (feasibility and monotonicity) are formally proven for multiple VRP variants. \n\n(4) Experiments are comprehensive, testing on large-scale problems, multiple backbone solvers (classic, neural, hybrid), and various VRP types, demonstrating robust performance and generalizability."}, "weaknesses": {"value": "1. While the paper demonstrates broad applicability, a more explicit discussion of the boundaries of FSTA/L2Seg's effectiveness would be beneficial. For example, under what conditions (e.g., problem size, structure, solver type) might the overhead of segmentation and aggregation outweigh the benefits?\n2. It is unclear that the boundary of the acceleration, as I notice that the HGS and LKH3 only run for a short time (5m and 10m). If the solving time extends, how will the acceleration benefit change in terms of both solution quality and time?\n3. Some works are also based on hypergraphs, but this paper does not discuss [1, 2].\n\n\n[1] A hierarchical destroy and repair approach for solving very large-scale travelling salesman problem. https://arxiv.org/pdf/2308.04639.\n\n[2] Destroy and Repair Using Hyper-Graphs for Routing. https://ojs.aaai.org/index.php/AAAI/article/view/34018"}, "questions": {"value": "1. Can the author explain how to deal with the disconnected node after the insertion (shown as the initial node in the last subfigure in Figure 3)?\n\n2. Can the author explain why not evaluate the proposed L2Segment on another two problems, VRPB and 1-VRPPD, as the FSTA is already theoretically verified on these two problems?\n\n3. Please also see the weaknesses and clarify them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2uYQSvic6v", "forum": "pN261iTKvr", "replyto": "pN261iTKvr", "signatures": ["ICLR.cc/2026/Conference/Submission15800/Reviewer_SSqq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15800/Reviewer_SSqq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987139956, "cdate": 1761987139956, "tmdate": 1762926030697, "mdate": 1762926030697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}