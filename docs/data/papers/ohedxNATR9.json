{"id": "ohedxNATR9", "number": 2810, "cdate": 1757257460911, "mdate": 1759898125926, "content": {"title": "UnSAF: A Self-Assessment Framework of Uncertainty Awareness for Multimodal LLMs", "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate significant potential in addressing complex multimodal tasks, they often produce plausible yet incorrect responses that limit their practical deployment, highlighting the critical need for reliable uncertainty evaluation. Existing metrics for assessing model uncertainty typically require extensive labeled datasets and rely on token-level confidence, which might be inadequate for open-ended multimodal tasks. To address these issues, we propose an Uncertainty-Aware Self-Assessment Framework (UnSAF), which explicitly incorporates the key question—Do MLLMs know what they don’t know?—into the evaluation procedure. Specifically, UnSAF first prompts MLLMs to generate a set of both answerable and unanswerable questions, then requires the models to answer these self-generated questions. The responses are then categorized into four distinct types, namely true answerable, false answerable, true unanswerable, and false unanswerable, and this ultimately yields an interpretable and label-free uncertainty-aware F1 (UnF1) score. We conduct extensive studies across both open-source and commercial MLLMs based on UnSAF. Our experiments not only demonstrate the effectiveness of UnSAF compared to conventional metrics but also reveal intriguing observations. Notably, we identify a clear positive correlation between the UnF1 score and model scale, which motivates the use of knowledge distillation to enhance uncertainty awareness in open-source, smallerscale MLLMs. Unlike simply transferring question-answering ability from larger models, we incorporate uncertainty-aware question generation into the distillation framework by teaching the student model to generate both answerable and unanswerable questions in response to different types of instructions. Experiments show that distilling uncertainty-aware question generation capability markedly enhances MLLMs’ uncertainty awareness without degrading original task performance and also noticeably reduces hallucinations.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Uncertainty Evaluation", "Visual Question Answering", "Distillation"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf4f50d1acb63a04e705c504f703097412a5b54a.pdf", "supplementary_material": "/attachment/b9ae90599b506198187f6e4fb4205d58cf41eef2.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical limitation of MLLMs, their tendency to generate plausible yet incorrect responses (hallucinations), by proposing the Uncertainty-Aware Self-Assessment Framework (UnSAF). Unlike conventional uncertainty evaluation metrics that rely on extensive labeled datasets and struggle with open-ended multimodal tasks, UnSAF operates in two stages: first, prompting MLLMs to generate both answerable and unanswerable questions grounded in given images, then requiring models to answer these self-generated questions. Responses are categorized into four types (True Answerable, False Answerable, True Unanswerable, False Unanswerable) to compute an interpretable, label-free Uncertainty-aware F1 (UnF1) score. The paper further explores a knowledge distillation framework (with three variants: UnD-QA, UnD-IQ, UnD-Joint) to enhance uncertainty awareness in small-scale open-source MLLMs, showing that distilling uncertainty-aware question generation (UnD-IQ) yields significant UnF1 improvements without degrading task performance or increasing hallucinations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Label-free uncertainty evaluation paradigm. UnSAF addresses a critical gap in MLLM assessment by eliminating reliance on labeled datasets – a major limitation of metrics like Brier score and ECE. By framing uncertainty evaluation as a self-assessment task (generate questions → answer them → categorize responses), it naturally adapts to open-ended multimodal tasks where exact string matching or predefined labels fail (e.g., \"a man riding a bike\" vs. \"a person on a bicycle\").\n2. Robustness and consistency across scenarios. Extensive validity analysis shows UnSAF’s UnF1 score maintains stable model rankings across diverse datasets (MMBench, VQAv2, VizWiz, etc.) and is resilient to sample size variations – a stark contrast to conventional metrics (ECE, MCE) that exhibit high volatility, especially with small datasets. This makes UnSAF practical for low-data regimes common in multimodal research.\n3. Insightful empirical observations on MLLM behavior. The paper uncovers actionable patterns: (1) instruction-tuned MLLMs exhibit \"timid\" behavior (over-abstention) that reduces UnF1, (2) prompt engineering fails to balance audacity (over-answering) and timidity (over-abstention), and (3) model scale correlates positively with UnF1 (with a sharp improvement at a critical parameter threshold). These findings inform both MLLM evaluation and optimization.\n4. Practical distillation framework for open-source MLLMs. The proposed distillation strategies (UnD-QA, UnD-IQ, UnD-Joint) directly address the gap in uncertainty awareness between commercial (e.g., GPT-4o) and open-source models. Crucially, the paper identifies that distilling question generation (UnD-IQ) – not just question answering (UnD-QA) – drives the largest, most consistent UnF1 gains (12–17% vs. 2% from UnD-QA), providing a clear path to improving smaller, accessible MLLMs."}, "weaknesses": {"value": "1. Underspecified algorithmic details for reproducibility.\n2. The paper contrasts UnSAF with traditional metrics (Brier score, ECE) but omits direct comparisons to recent uncertainty-aware approaches for MLLMs, such as [1][2].\n3. The paper identifies that instruction-tuned MLLMs are \"more timid\" (higher abstention, lower timidity scores) but does not investigate why this occurs. Is it due to reinforcement learning from human feedback (RLHF) prioritizing avoiding errors over answering?\n4. While the paper claims distillation reduces hallucinations (via POPE and CHAIR), it does not define how hallucinations are linked to uncertainty awareness.\n\n[1] Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering, CVPR 2024.\n\n[2] Exploring response uncertainty in mllms: An empirical evaluation under misleading scenarios, EMNLP 2025."}, "questions": {"value": "1. Does a higher UnF1 score directly correlate with fewer object hallucinations (measured by POPE) or less semantic inconsistency (measured by CHAIR)?\n2. the positive correlation between model scale and UnF1 is observed but not explained, e.g., do larger models have better spatial/linguistic understanding, or do they simply learn more \"abstention triggers\"? \n3. if a model never abstains (FU=0, TU=0), how is timidity computed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PzIPohKCGJ", "forum": "ohedxNATR9", "replyto": "ohedxNATR9", "signatures": ["ICLR.cc/2026/Conference/Submission2810/Reviewer_3F9m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2810/Reviewer_3F9m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449919321, "cdate": 1761449919321, "tmdate": 1762916386373, "mdate": 1762916386373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UnSAF (Uncertainty-Aware Self-Assessment Framework), a label-free evaluation framework for assessing uncertainty awareness in Multimodal Large Language Models (MLLMs). The core innovation is a two-stage pipeline where MLLMs first generate both answerable and unanswerable questions, then attempt to answer them. Responses are categorized into four types (True/False Answerable/Unanswerable), yielding an interpretable UnF1 score. The paper demonstrates that larger models exhibit better uncertainty awareness, motivates knowledge distillation to improve smaller MLLMs, and shows that uncertainty-aware question generation is critical for effective distillation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Problem Formulation: The paper addresses a genuinely important problem—uncertainty awareness in MLLMs—with a creative, self-contained approach that doesn't require extensive labeled data or external metrics. The UnF1 score formulation is intuitive and interpretable.\n\n2. Comprehensive Empirical Study: The evaluation across 14 open-source and 3 commercial MLLMs is thorough. The consistency of findings regarding scaling effects and the effectiveness of UnD-Joint distillation strengthens the claims.\n\n3. Principled Approach to Knowledge Distillation: The paper goes beyond simple QA-based distillation by incorporating uncertainty-aware question generation. The finding that this contributes 15% improvement (vs. 2% for QA-only) is significant and well-motivated."}, "weaknesses": {"value": "1. Limited Theoretical Justification: While the UnF1 score is intuitive, the paper lacks deeper theoretical analysis of why this specific formulation effectively captures uncertainty. Why are the four categories optimal? How sensitive is the metric to the balance between answerable and unanswerable questions?\n\n2. Validation Concerns: \n   - The UnSAF validity analysis relies mainly on consistency across datasets and sampling stability, but lacks validation against ground-truth uncertainty measures\n   - The paper doesn't clearly validate that UnF1 scores actually correlate with downstream metrics (e.g., error rates in real applications)\n\n3. Dataset Annotation Quality: The distillation datasets are annotated using GPT-4o and peer MLLMs. How reliable are peer MLLM annotations? What's the inter-annotator agreement or quality assurance mechanism?\n\n4. Limited Analysis of Failure Cases: The paper doesn't deeply discuss when UnSAF might fail or produce misleading results. Are there types of MLLMs or tasks where the approach breaks down?"}, "questions": {"value": "1. How sensitive is UnF1 to the ratio of answerable vs. unanswerable questions generated in Stage 1?\n\n2. Can you provide correlation analysis between UnF1 scores and actual error rates across different model families?\n\n3. For the distillation approach, how were the hyperparameters (LoRA rank, learning rate, etc.) selected?\n\n4. The paper mentions instruction-tuned MLLMs adopt \"timid behavior.\" Can you elaborate on mechanisms causing this and whether other types of tuning lead to different behaviors?\n\n5. How does performance vary when applying UnSAF to models trained with different alignment procedures (e.g., DPO vs. RLHF)?\n\n6. Scalability Questions:\n   - How does computational cost scale with model size?\n   - What's the overhead of the two-stage pipeline compared to single-pass methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ayQyVq2uiZ", "forum": "ohedxNATR9", "replyto": "ohedxNATR9", "signatures": ["ICLR.cc/2026/Conference/Submission2810/Reviewer_CXVY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2810/Reviewer_CXVY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618432384, "cdate": 1761618432384, "tmdate": 1762916386079, "mdate": 1762916386079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UnSAF, a framework to evaluate the ability of MLLMs to provide self-assessment (without labelled data) of whether the models \"know what they don't know\" and correctly indicate whether a question is unanswerable or not. This is done via two stages: (1) the MLLM is prompted to generate answerable and unanswerable questions given images; (2) the MLLM is asked to answer the generated question and indicate whether a question is unanswerable. From these two stages, a confusion matrix of responses could be computed, and the precision/recall and F1 scores (audacity, timidity, and UnF1 respectively) are used as indication of the model's capability to abstain well. The authors showed that these scores show better consistency across datasets than other metrics, and observations such as UnSAF score trends with respect to model size and instruction-tuned models. The authors also developed a knowledge distillation framework for UnSAF (with QA pairs and Instruction-Question pairs), and results on how the distillation helps improve UnSF scores."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an important problem of multimodal LLM reliability and uncertainty quantification.\n- The content is well structured.\n- Experiments are run on a wide range of MLLMs.\n- The proposed perspective of using MLLMs to generate questions that it deem unanswerable and subsequently testing whether it can consistently assess if these questions are unanswerable seems novel."}, "weaknesses": {"value": "- The paper seems to mix up various notions of uncertainty, leading to claims and experimental designs that may not be well substantiated. The proposed framework does not measure uncertainty of model responses in a similar way as Brier Score or ECE, and hence it is a conceptual mismatch for the authors to compare them. Brier Score and ECE evaluates probability calibration for each task query, and hence requires ground truth data for evaluation, while UnSAF evaluates the model's consistency in abstaining from responding to questions it cannot answer. The experimental design in Sec 4.1 may not be useful -- it might not be the case that model ranking has to be preserved for Brier Score and ECE across datasets, e.g. some models may be better trained and calibrated in some tasks or domains than others. The authors may want to consider repositioning their paper to clearly distinguish what UnSAF is evaluating compared to the various other metrics and uncertainty quantification related work.\n\n- The proposed framework seems to specifically measure a given model's consistency, between question-generation of unanswerable questions and question-answering evaluation of unanswerable questions. This does not necessarily provide a clear indication of whether the proposed score will be useful independently in assessing a model's uncertainty in tackling various tasks. Unfortunately, most experimental results in the paper have been focused on consistency (including the distillation framework that is evaluated primarily only on the UnSAF metrics), without any other baselines or metrics. There is a mention of hallucination mitigation results at the end, but even then the results presented were relative (W/T/L counts) before and after distillation, rather than a more objective measure of the effectiveness of the proposed score.\n\n- While the distillation strategy and results make sense when evaluated against specifically the UnSAF metrics (consistency as described above), it remains to be seen whether the approach yield better calibrated models from other objective metrics. There are some relative win-loss count results in Table 2 for hallucination mitigation, but reporting absolute metrics and comparisons with other hallucination mitigation methods would help strengthen the claims of the paper.\n\n- The fidelity of the question generation process is unclear. Line 190 indicated that there seems to require manual verification of whether each set of options has exactly one unambiguous correct answer, raising questions on how the \"ground truth\" of generated questions are validated including for the open-ended questions. From the paper, it is unclear whether the framework expects that the model's assessment of what is answerable or unanswerable need to be correct from an objective perspective (as opposed to model's answerability), and whether it matters.\n\n- The prompts used in the experiments seem relatively complex with specific rules. It would be useful to report how sensitive the entire UnF1 score is to variations in the prompts (e.g. even with just paraphrasing of the prompt). The prompt engineering experiments in Sec 4.2 seem to show relatively large sensitivity for open-ended questions, indicating that ablations on prompts may be useful for other parts of the paper.\n\n- The paper positioned itself as related other multimodal LLMs, but the results seem to be restricted to just visual-language models. It would be useful for the authors to adjust the paper to be clearer about the scope and claims of the paper."}, "questions": {"value": "Please see points of concern and questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xlgszfMAGc", "forum": "ohedxNATR9", "replyto": "ohedxNATR9", "signatures": ["ICLR.cc/2026/Conference/Submission2810/Reviewer_nAFs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2810/Reviewer_nAFs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764879236, "cdate": 1761764879236, "tmdate": 1762916385940, "mdate": 1762916385940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of reliably evaluating uncertainty in MLLMs, as existing metrics often require extensive labeled data and perform poorly on open-ended tasks. The authors introduce the UnSAF, a novel, label-free evaluation method.  Experiments across 17 MLLMs show that the UnF1 score is more stable and consistent than traditional metrics. The research also reveals a positive correlation between model scale and uncertainty awareness. Based on this finding, the authors propose an uncertainty-aware distillation method. They demonstrate that teaching a smaller model to generate uncertainty-aware questions is more effective at improving its UnF1 score and reducing hallucinations than simply fine-tuning on question-answer pairs, without degrading task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed UnSAF framework is a methodologically novel, label-free approach. It uniquely uses the model's own generated answerable and unanswerable questions to create an internal evaluation benchmark.\n2. The paper rigorously validates its UnF1 metric. Figures 2 and 3 demonstrate superior stability across datasets and sample sizes compared to conventional metrics like ECE."}, "weaknesses": {"value": "1.  The method's core relies on the model's ability to self-generate answerable and unanswerable questions, yet the experimental section lacks a human evaluation of the quality of these questions. It is recommended to add a human assessment component to validate the fundamental premise of the UnSAF framework's effectiveness.\n2.  The uncertainty-aware distillation experiments only validated the UnF1 score improvement on the COCO test set, which shares the same distribution as the training data. It is suggested to evaluate the distilled models' UnF1 scores on more diverse, unseen datasets (e.g., MMBench, OKVQA) to demonstrate the generalizability of the enhanced uncertainty awareness.\n3.  While the paper proposes an effective UnD distillation strategy, it lacks direct experimental comparisons with other recent methods aimed at improving model uncertainty or abstention capabilities (such as those mentioned in Miyai et al., 2024). Adding such baseline comparisons is recommended to more comprehensively demonstrate the advantages of the proposed method.\n4.  The experimental results show that the UnD-QA strategy can lead to a decrease in the UnF1 score in some cases, but the authors fail to provide an in-depth analysis of this negative result. It is advisable to conduct a more detailed investigation into this phenomenon to explain why distilling only question-answer pairs might harm the model's uncertainty awareness."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sKQQHsig0B", "forum": "ohedxNATR9", "replyto": "ohedxNATR9", "signatures": ["ICLR.cc/2026/Conference/Submission2810/Reviewer_zwRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2810/Reviewer_zwRv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051167463, "cdate": 1762051167463, "tmdate": 1762916385796, "mdate": 1762916385796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}