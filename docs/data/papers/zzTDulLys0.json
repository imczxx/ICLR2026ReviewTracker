{"id": "zzTDulLys0", "number": 9335, "cdate": 1758119350110, "mdate": 1759897730817, "content": {"title": "vAttention: Verified Sparse Attention via Sampling", "abstract": "State-of-the-art sparse attention methods for reducing decoding latency fall into two main categories: approximate top-$k$ (and its extension, top-$p$) and recently introduced sampling-based estimation. However, these approaches are fundamentally limited in their ability to approximate full attention: they fail to provide consistent approximations across heads and query vectors and, most critically, lack guarantees on approximation quality, limiting their practical deployment. We observe that top-$k$ and random sampling are complementary: top-$k$ performs well when attention scores are dominated by a few tokens, whereas random sampling provides better estimates when attention scores are relatively uniform. Building on this insight and leveraging the statistical guarantees of sampling, we introduce vAttention, the first practical sparse attention mechanism with user-specified $(\\epsilon, \\delta)$ guarantees on approximation accuracy. These guarantees make vAttention a compelling step toward practical, reliable deployment of sparse attention at scale. By unifying top-k and sampling, vAttention outperforms both individually, delivering a superior quality–efficiency trade-off. Our experiments show that vAttention significantly improves the quality of sparse attention (e.g., $\\sim$4.5 percentage points for Llama-3.1-8B-Inst and Deepseek-R1-Distill-Llama-8B on RULER-HARD ), and effectively bridges the gap between full and sparse attention (e.g., across datasets, it matches full model quality at 10x–20x sparsity). We also demonstrate that it can be deployed in long-generation scenarios to achieve fast decoding without compromising model quality (e.g., vAttention achieves full model quality on AIME2024 at 10\\% sparsity with up to 32K token generations).", "tldr": "", "keywords": ["sparse attention"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08eba5df70e04615adf4427731eb39c1688adbb0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper combines top-k attention with a token sampling approach, seeking an interpolation of the two. It seeks provable and tunable approximation guarantees of the attention function and a practical method that can be deployed to save time and memory without sacrificing model quality. Numerous experiments are performed against many benchmarks to validate the method's performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, with the exception of some details. It is concise, to the point and effective at communicating its message. \n2. The paper tackles the important problem of achieving efficient attention in Transformers without sacrificing model quality. Improvements in this space undoubtedly have profound consequences on the landscape of AI. In this space, the paper contributes a method that offers a lot of practical promise by combining the existing approaches of top-k and sampling attention.\n3. The paper performs numerous experiments on a variety of benchmarks to support its claims. There are also lots of ablation studies with other attention methods. \n4. The paper quantifies the tradeoff between approximation quality and model decline, at least empirically. This is something very few works in this space do, so it is an important contribution by itself."}, "weaknesses": {"value": "1. The idea of combining sampling and top-k attention is not novel to this paper. The work of [1], for instance, seems to precisely propose the vAttention estimator in eq. (5). Furthermore, [1] also analyzes the approximation guarantees of their estimator rigorously. Beyond [1], numerous other works rigorously analyze the approximation quality of subquadratic attention mechanisms [2,3,4], making me feel uneasy about this paper's claim to  be the \"first\" algorithm to rigorously allow for quality-efficiency tradeoff control. I would say that the point of departure of this work from [1] and other such works seems to be mainly the fact that this work makes the sample size a tunable hyperparameter, and I worry that this is not a novel enough contribution. \n    * That being said, the experimental and empirical study provided by this paper are another one of its contributions. Prior works have not analyzed top-k attention in such an extent, so these insights are definitely valuable to the community. However, the paper is suggesting that it is the first to propose these methods and rigorously analyze them, which, in my opinion, is not accurate. \n2. The mathematical rigor of the paper has some notable issues:\n    * Lemma 4.1: The $\\Phi$ function in Lines 301-303 is applied to the entire term? It is a little hard to see what's happening here.\n    * Lemma 4.1: Why are $r_i$ considered random variables with some covariance matrix $\\Sigma$? The distribution on the scores is highly unknown and $\\Sigma$ is impossible to estimate. Yet the lower bound on $b$ depends on the trace of $\\Sigma$. The authors mention this and ultimately set the threshold arbitrarily, but the formal algorithm cannot be stated in terms of this $\\Sigma$.\n    * Lemma 4.1: The use of CLT here is a bit troublesome. The argument can only work in the limit $n_s \\to \\infty$, in which case I am also confused as to how the lower bound is ultimately proven. Hoeffding's inequality can salvage things and give a concrete bound, but the paper does not have this proof. Instead it is claimed that in practice the CLT-bound is superior, which again raises the question of how $\\Sigma$ is calculated. As a whole, Lemma 4.1 is a bit weak in my opinion.\n    * Lemma 4.2: This is the approximation guarantee. I don't think it parses very well: The probability is multiplied with $||N/D||_2$? Also, the statement is shown for $||N||_2/D$ instead in the appendix.\n\nOverall, the paper makes a compelling case for the use of top-k and sampling attention (or interpolation of these) in practice. However, I feel like it currently suffers from issues of originality and rigor.\n\n[1] Haris, Themistoklis. \"kNN Attention Demystified: A Theoretical Exploration for Scalable Transformers.\" The Thirteenth International Conference on Learning Representations.\n[2] Han, I., Jayaram, R., Karbasi, A., Mirrokni, V., Woodruff, D. P., & Zandieh, A. (2023). Hyperattention: Long-context attention in near-linear time. arXiv preprint arXiv:2310.05869.\n[3] Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L. and Belanger, D., 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794.\n[4] Alman, Josh, and Zhao Song. \"Fast rope attention: Combining the polynomial method and fast fourier transform.\" arXiv preprint arXiv:2505.11892 (2025)."}, "questions": {"value": "1. For the experimental section, it is mentioned that it is expensive to calculate the numerator so it is skipped? Why is it expensive? How can we afford to skip that approximation without suffering later on? This seems to be a different algorithm than what was argued earlier."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yzZyhoNCDS", "forum": "zzTDulLys0", "replyto": "zzTDulLys0", "signatures": ["ICLR.cc/2026/Conference/Submission9335/Reviewer_rduG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9335/Reviewer_rduG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761273137265, "cdate": 1761273137265, "tmdate": 1762920967984, "mdate": 1762920967984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces vAttention, a sparse attention mechanism that unifies deterministic top-k selection with sampling-based estimation and provides formal ($\\epsilon, \\delta$)\nguarantees on the approximation of full attention. Theoretical results show that the estimator achieves bounded error under Central Limit Theorem–based sampling, and experiments across Llama-3.1, DeepSeek, and Mistral models demonstrate strong quality-efficiency trade-offs, often outperforming existing top-k methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The mathematical derivation is clear and connects attention approximation to classical sum-estimation theory.\n\n2. Shows consistent empirical gains on long-context benchmarks and stable long-generation quality.\n\n3. The framework is compatible with existing top-k implementations.\n\n4. Writing and organization are clear, with well-motivated theoretical and experimental sections."}, "weaknesses": {"value": "1. No ablation on the relaxation that only approximates the denominator.\n\n2. No GPU runtime results or CUDA implementation to verify efficiency gains.\n\n3. Missing quantitative reporting on selected token counts and achieved sparsity.\n\n4. Comparison to oracle top-p and newer top-p-based methods is incomplete.\n\n5. Parameter selection procedure for $\\delta$ and $\\epsilon$ is heuristic and underexplained."}, "questions": {"value": "This paper is solidly motivated and theoretically sound, and the idea of providing formal guarantees for sparse attention is timely. However, several issues limit its empirical completeness and clarity. Below, I describe these concerns and follow with specific technical questions.\n\nThe relaxation that approximates only the denominator is a major simplification. While it is argued to control bias and match observed errors, there is no ablation comparing the full \n $(\\epsilon, \\delta)$ approximation to this relaxed version. The paper would benefit from quantifying how much quality or efficiency is gained or lost through this relaxation. In addition, since the method introduces user-defined $(\\epsilon, \\delta)$ parameters, it would be useful to know how these affect token selection and runtime. The paper does not clearly show how many tokens are chosen for a given accuracy on the benchmark compared to baselines.\n\nThe efficiency evaluation focuses on CPU-bound experiments and does not provide GPU results. Sparse attention acceleration is most relevant for GPU decoding workloads, so the lack of CUDA implementation or end-to-end throughput measurements makes it hard to judge the real benefit. Even a partial GPU prototype would help demonstrate scalability.\n\nThe comparison with oracle top-p is not conclusive. In some plots (Appendix A.1), vAttention performs worse than oracle top-p at the same density, but this is not discussed. Since top-p offers a different adaptive coverage mechanism, further comparison or integration could clarify the relative advantages.\n\nFinally, parameter tuning remains underexplained. The authors mention grid search over $(\\epsilon, \\delta)$ and sampling fractions but give no practical guideline for setting them. For deployment, it would be important to know if there exists a stable range that generalizes across models or if these need per-benchmark tuning.\n\n\nQuestions:\n\n1. How large is the empirical performance gap between the full numerator-denominator estimation and the denominator-only relaxation?\n\n2. In Eq. (5), the deterministic and stochastic terms are directly summed. Would importance weighting or probability normalization further reduce bias?\n\n3. Are there engineering challenges that prevent implementing the methods in CUDA? Could it reuse GPU primitives for top-k?\n\n4. What are the average token counts or densities achieved under different $(\\epsilon, \\delta)$ values, and how do they translate to runtime gains on GPU?\n\n5. When vAttention underperforms oracle top-p at fixed sparsity, is that due to attention distribution skewness, or to the approximation of only the denominator? Could vAttention be compatible with existing top-p methods?\n\n6. Why does vAttention sometimes outperform dense attention on AIME (Table 2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WwVr7SlkBd", "forum": "zzTDulLys0", "replyto": "zzTDulLys0", "signatures": ["ICLR.cc/2026/Conference/Submission9335/Reviewer_YGRV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9335/Reviewer_YGRV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631861024, "cdate": 1761631861024, "tmdate": 1762920967716, "mdate": 1762920967716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **vAttention**, a sparse attention mechanism with verifiable accuracy control. The key observation is that top-k and random sampling are complementary: top-k captures sharp, heavy-tailed score distributions while sampling better covers near-uniform regions. vAttention unifies them: it deterministically selects heavy-hitters (sink tokens, a local window, and approximate top-k) and then uniformly samples from the residual “long tail.” Users set \\\\((\\epsilon,\\delta)\\\\) and the method computes a per-query, per-head sampling budget via CLT-based estimates. Experiments show clear gains over strong baselines and narrow the gap to full attention, matching full-attention quality under high sparsity (≈10–15% density; ≈12% at 32K) in long-generation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Principled guarantees.** vAttention is presented as a practical algorithm that exposes user-controllable \\\\((\\epsilon,\\delta)\\\\) error targets. Empirically, the realized approximation error correlates strongly with the user tolerance \\\\(\\epsilon\\\\) (correlation \\\\(>0.99\\\\)).\n\n2. **Robust hybrid design.** The deterministic heavy-hitter stage plus stochastic long-tail sampling is well-motivated. A simple combination of oracle-top-k with random sampling consistently outperforms either component alone.\n\n3. **Strong empirical results.**\n   - **Accuracy:** Improves over HashAttention on RULER-HARD by ≈4.5 points (e.g., +4.6 for Llama-3.1-8B and +4.3 for DeepSeek-R1-Distill-Llama-8B).\n   - **Oracle comparison:** vAttention + oracle-top-k can outperform oracle-top-p on RULER-32K, suggesting limits of pure top-k/top-p schemes.\n   - **Long generation:** Matches full-attention quality on AIME@32K at ≈12% average density; at ≈16K, densities around 10–15% are reported."}, "weaknesses": {"value": "1. **Theory–practice relaxation.** The paper proves joint \\\\((\\epsilon,\\delta)\\\\) guarantees for **both** SDPA numerator \\\\(N\\\\) and denominator \\\\(D\\\\) (Theorem 4.3), leveraging a composition of bounds (with Lemma 4.2 for separate approximations). Computing the full budget is reported as expensive. Consequently, **all experiments** adopt a relaxation that provides an \\\\((\\epsilon,\\delta)\\\\) guarantee **only for the denominator** \\\\(D\\\\). The “verified” claim in practice is therefore supported by the strong empirical correlation \\\\(>0.99\\\\) rather than the full \\\\(N\\\\)&\\\\(D\\\\) guarantee.\n\n2. **Budget/latency overhead under-measured on GPU.** Speedups are shown in memory-bound regimes with KV on CPU and with a naive PyTorch implementation. A CUDA kernel is left to future work. The end-to-end latency trade-off when KV fits in HBM is not fully benchmarked, so the actual GPU-resident benefit remains uncertain."}, "questions": {"value": "1. **Cost of full \\\\(N\\\\)&\\\\(D\\\\) guarantee:** What is the empirical overhead and final density increase when enforcing the full Theorem 4.3 guarantee versus the denominator-only relaxation? Is the expense dominated by estimating \\\\(\\mathrm{Tr}(\\Sigma)\\\\) for the vector-valued numerator, and can that be approximated efficiently without breaking the guarantee?\n\n2. **Budget partitioning guidance:** The method composes sink/local/top-k (deterministic) with uniform sampling (stochastic). Experiments grid-search the fractions \\\\((f_s,f_l,f_t)\\\\). Is there any theoretical prescription for allocating budget between deterministic heavy-hitters and stochastic sampling to minimize total density for a target \\\\((\\epsilon,\\delta)\\\\)?\n\n3. **Early-token behavior:** For very early tokens in long generation, when the residual set size \\\\(n_s\\\\) is small, CLT assumptions are weaker. Does Algorithm 2 switch to heavier reliance on the deterministic set \\\\(\\mathcal{I}_f\\\\) or adaptive rules for minimum base samples to keep the guarantees meaningful in this regime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EaRShDo3nB", "forum": "zzTDulLys0", "replyto": "zzTDulLys0", "signatures": ["ICLR.cc/2026/Conference/Submission9335/Reviewer_5pmM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9335/Reviewer_5pmM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649833397, "cdate": 1761649833397, "tmdate": 1762920967041, "mdate": 1762920967041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces vAttention, a verified sparse attention method designed to address the memory and computational bottlenecks of standard scaled dot-product attention (SDPA) in large language models (LLMs) when processing long contexts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*   **Hybrid & Adaptive:** Intelligently combines deterministic \"heavy-hitter\" tokens (sinks, local, top-k) with stochastic sampling of the tail, adapting per head and query.\n*   **State-of-the-Art Performance:** Outperforms strong baselines (e.g., HashAttention, oracle top-p), often matching full-model accuracy with high sparsity (10-15%).\n*   **Efficient Long-Context Inference:** Effectively reduces the memory and computational bottleneck of large KV caches, enabling faster generation, especially when the cache is offloaded to CPU.\n*   **Practical & Versatile:** Can be integrated with existing approximate top-k methods and works well on diverse, challenging long-context benchmarks."}, "weaknesses": {"value": "*   **Relaxation of Theoretical Guarantees:** The core theoretical contribution provides an \\((\\epsilon, \\delta)\\) guarantee for the *entire* attention output. However, the authors explicitly state that in practice, they use a **relaxation that only guarantees the denominator**. While they provide empirical justification (strong correlation with final error), this significantly weakens the formal \"verified\" claim. The method is no longer provably guaranteed for the final output, but rather for an intermediate component.\n*   **Dependence on CLT and Large Sample Assumptions:** The theoretical bounds rely on the Central Limit Theorem (CLT), which holds for \"large enough\" sample sizes. The paper provides an empirical analysis (Appendix E) showing CLT is tighter than Hoeffding's bound, but the validity of the CLT approximation for *all* layers and heads, especially with small budgets, is not rigorously proven. This makes the \"verification\" approximate rather than exact.\n*   **Circular Dependency in Budget Calculation:** To compute the required sample size \\(b\\), the algorithm needs to know population statistics like the covariance matrix \\(\\Sigma\\) and the norm \\(||N||_2\\). Since these are unknown, the method uses a **base sample** (governed by \\(f_b\\)) to estimate them. The accuracy of the initial estimate directly impacts the final guarantee, creating a potential circularity that is not fully addressed theoretically.\n*   **Sensitivity to Hyperparameters:** The method introduces several new hyperparameters (\\(f_s, f_l, f_t, f_b, \\epsilon, \\delta\\)). While the paper shows a search can find good values, this adds complexity for practitioners compared to simpler methods like top-\\(k\\). The \"natural configuration\" used in AIME2024 is promising but requires validation across more diverse tasks.\n\n* **Computational Overhead of Budget Calculation:** The process of calculating the adaptive budget for each head and query, including drawing a base sample and estimating statistics, introduces **non-trivial overhead**. The paper admits this is done in \"naive PyTorch\" and that a CUDA kernel is future work. This overhead could offset the speed gains from sparse attention, especially for shorter sequences or GPU-hosted KV caches.\n*   **Dependence on Approximate Top-\\(k\\) Methods:** vAttention's performance is not standalone; it's a framework that incorporates an approximate top-\\(k\\) method (e.g., HashAttention). The results show that **\"more accurate top-\\(k\\) methods are essential for the overall quality.\"** Therefore, vAttention's weaknesses are, in part, the weaknesses of its underlying top-\\(k\\) component. If the top-\\(k\\) method fails to identify crucial \"heavy hitters,\" vAttention's sampling-based tail approximation may not be sufficient to recover.\n*   **No End-to-End Speed Evaluation:** The efficiency claims are primarily supported by a model showing near-linear speedup when the KV cache is on the CPU (Figure 5). There is **no comprehensive evaluation of end-to-end latency or throughput** (tokens/second) comparing vAttention to baselines under equal hardware and sparsity budgets. The gains for GPU-resident KV caches are stated but not demonstrated."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "59jBt6aG3i", "forum": "zzTDulLys0", "replyto": "zzTDulLys0", "signatures": ["ICLR.cc/2026/Conference/Submission9335/Reviewer_VSCh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9335/Reviewer_VSCh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710802616, "cdate": 1761710802616, "tmdate": 1762920966171, "mdate": 1762920966171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}