{"id": "8bJs2ElHGM", "number": 18415, "cdate": 1758287486280, "mdate": 1763001038805, "content": {"title": "PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction", "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality, real-time novel-view synthesis from multi-view images. However, most existing methods assume the object is captured in a single, static pose, resulting in incomplete reconstructions that miss occluded or self-occluded regions. We introduce PFGS, a pose-aware 3DGS framework that addresses the practical challenge\nof reconstructing complete objects from multi-pose image captures. Given images of an object in one main pose and several auxiliary poses, PFGS iteratively fuses each auxiliary set into a unified 3DGS representation of the main pose. Our pose-aware fusion strategy combines global and local registration to merge views effectively and refine the 3DGS model. While recent advances in 3D foundation models have improved registration robustness and efficiency, they remain limited by high memory demands and suboptimal accuracy. PFGS overcomes these challenges by incorporating them more intelligently into the registration process: it leverages background features for per-pose camera pose estimation and employs foundation models for cross-pose registration. This design captures the best of both approaches while resolving background inconsistency issues. Experimental results demonstrate that PFGS consistently outperforms strong baselines in both qualitative and quantitative evaluations, producing more complete reconstructions and higher-fidelity 3DGS models.", "tldr": "3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction", "keywords": ["3D Gaussian Splatting", "Object Reconstruction", "Multi-Pose Object Capture"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4e421293462119be3510b809dd5e99c257a6e9ab.pdf", "supplementary_material": "/attachment/8873d5fde5f026a6b0c378a9dd714f21a5df9acd.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents PFGS (Pose-Fused 3D Gaussian Splatting), a pose-aware framework for reconstructing complete 3D objects from multi-pose image captures. Unlike conventional 3DGS approaches that assume a static object pose, PFGS handles scenarios where an object must be reoriented to reveal occluded regions.\n\nThe method iteratively fuses each auxiliary-pose sequence into a unified 3DGS representation of a main pose through three stages:\n\n1. Global registration via a mixed-pose subset processed by foundation models (Fast3R, VGGT) and refined through a silhouette-consensus alignment.\n2. Gradient-based local registration using silhouette and RGB consistency.\n3. 3DGS completion through balanced sampling and fine-tuning.\n\nQuantitative results on synthetic and real datasets demonstrate substantial improvements in pose accuracy (orders of magnitude lower error than VGGT/Fast3R) and enhanced 3D reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposing problem “Multi-pose reconstruction” seems challenging and interesting research direction.\n- Novel framework to unify the poses of multiple sets of frames with efficient usage of foundation models\n- Evaluations on well-curated synthetic and real datasets followed by ablation studies on each components."}, "weaknesses": {"value": "1. **Illumination inconsistency ignored:** When an object’s pose changes, surrounding illumination directions and shading patterns also change. Fusing such data into a single 3DGS inevitably introduces appearance inconsistency. However, the method focuses on “photorealistic, view-consistent” results without modeling this radiometric gap. \n2. **Insufficient analysis of baseline performance drop:** Table 1 shows VGGT and Fast3R failing dramatically on masked objects, but the authors do not analyze why (e.g., loss of background context or inconsistent normalization). A discussion on this could provide insight into the necessity of the fusion method.\n3. **Limited scalability evidence:** While synthetic and real datasets are informative, all objects are relatively small-scale with moderate pose changes. It remains unclear whether PFGS generalizes to complex objects (e.g., articulated or deformable subjects) with extreme pose changes."}, "questions": {"value": "1. How does the varying illumination by the pose change the impact the reconstruction result iof PFGS?\n2. Why do VGGT and Fast3R perform much worse on masked (foreground-only) images?\n3. How does PFGS handle cases where auxiliary poses differ by extreme rotations (e.g., >120°)?\n4. Runtime comparison: How does the 22.6 min average compare to pure COLMAP or foundation-model-only approaches under the same GPU setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ISEnhwkMoq", "forum": "8bJs2ElHGM", "replyto": "8bJs2ElHGM", "signatures": ["ICLR.cc/2026/Conference/Submission18415/Reviewer_Df9k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18415/Reviewer_Df9k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725628250, "cdate": 1761725628250, "tmdate": 1762928117037, "mdate": 1762928117037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank all the reviewers for their thoughtful comments, constructive suggestions, and the time they invested in evaluating our submission. We deeply appreciate the insights provided, many of which align with limitations we are already aware of and actively working to address. After careful consideration, we have concluded that we will not be able to adequately resolve all identified weaknesses within the rebuttal period. Therefore, we have decided to withdraw the paper from ICLR and continue refining it for submission to the next venue."}}, "id": "wtCLQDSlVQ", "forum": "8bJs2ElHGM", "replyto": "8bJs2ElHGM", "signatures": ["ICLR.cc/2026/Conference/Submission18415/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18415/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763001037262, "cdate": 1763001037262, "tmdate": 1763001037262, "mdate": 1763001037262, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PFGS to solve the problem of 3D reconstructions with multiple-object-pose images. Given the main-pose and auxiliary-pose images, the authors propose a global registration stage that first find the visually overlapping views across poses, estimate the camera pose of mixed-pose images, then align auxiliary-pose views to main-pose coordinate system using silhouette-consensus fusion strategy. Gradient-based local registration optimization refines the camera poses, and a complete 3DGS model is trained with all poses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a setting where the center object is at different poses to solve the occlusion and limited physical access issues comparing to a single-pose setting.\n2. PFGS achieves better registration accuracy compared to VGGT, Fast3r, and COLMAP.\n3. PFGS achieves better novel view synthesis compared to COLMAP-registered results and main-pose results."}, "weaknesses": {"value": "1. The main technical contribution is the pose alignment. Similar topics discussed in Instant-Splat[1] and SPARS3R[2] regarding the train-test pose alignment. \n2. A qualititative comparison with COLMAP-registered results would help understand the effectiveness of the proposed approach, as the results in Table 2 across different objects.\n\n[1] Fan, Z., Cong, W., Wen, K., Wang, K., Zhang, J., Ding, X., ... & Wang, Y. (2024). Instantsplat: Sparse-view gaussian splatting in seconds. arXiv preprint arXiv:2403.20309.\n[2] Tang, Y., Guo, Y., Li, D., & Peng, C. (2025). SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 26810-26821)."}, "questions": {"value": "1. Is there a need of registration of visually overlapping views across poses and then align instead of directly register the overlapping auxillary views to main-pose? If so, will using COLMAP for the registration of visually overlapping views lead to more accurate results, as the registration error from VGGT or Fast3r seem large in Table 1. \n2. I don't entire understand the 3DGS-rendered the mask and SAM2 mask and how they contribute to the alignment accuracy. In figure 3 caption, it says \"the lowest average IoU is selected\".\n3. Are the position units in synthetic dataset meters?\n4. minor typos such as line 208 \"we alignment\". the draft merits a carefull proof-reading."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nLkN3TBFx3", "forum": "8bJs2ElHGM", "replyto": "8bJs2ElHGM", "signatures": ["ICLR.cc/2026/Conference/Submission18415/Reviewer_2Z7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18415/Reviewer_2Z7k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970922234, "cdate": 1761970922234, "tmdate": 1762928116579, "mdate": 1762928116579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PFGS, a framework for reconstructing 3D objects from multi-pose image sets consisting of a main set and an auxiliary set. First, representative M and N images are selected from the main and auxiliary sets, respectively, using DINO feature similarity and geometric verification. These selected images form the initial mixed-pose set and are fed into Fast3R to estimate their camera poses. The poses of the mixed-pose images are aligned to the COLMAP poses of the main set by checking silhouette IoU. Similarly, the COLMAP poses for the auxiliary set are aligned to those of the transformed mixed-pose set via a similarity transform, followed by photometric refinement. Finally, the 3D Gaussians are further optimized using all images for improved reconstruction quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The task of reconstructing 3D objects from multi-pose image sets appears novel, as the background is no longer a reliable cue for registering images across different sets.\n2. The proposed method is straightforward, and the experiments demonstrate its effectiveness both quantitatively and qualitatively.\n3. The ablation studies confirm the contribution from each component."}, "weaknesses": {"value": "1. The current method operates in a relatively easy setting:\n(1) Both the main and auxiliary sets contain abundant images (e.g., more than 100) for COLMAP to function effectively. When the number of available images decreases, COLMAP may produce noisy poses or even fail entirely. It seems that the current system cannot handle such pose estimation failures. The reviewer is also curious about an ablation study where the number of images is reduced (e.g., fewer than 20). In such cases, learning-based approaches might perform better. \n(2) The current formulation assumes static rigid objects and does not handle cases where the object exhibits articulation or non-rigid deformation across image sets.\n2. The authors may consider discussing 3DGS-based object reconstruction with generative priors [1, 2], which can address challenges in sparse-view settings. Moreover, Zhao et al. [2] assume noisy camera poses as input and are able to handle outliers.\n\nReferences:\n\n[1] Tang et al., DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation, ICLR 2024.\n\n[2] Zhao et al., Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis, NeurIPS 2024."}, "questions": {"value": "1. In the final stage (Sec. 3.3), are the camera poses optimized for all views to further improve alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yPc39101ZW", "forum": "8bJs2ElHGM", "replyto": "8bJs2ElHGM", "signatures": ["ICLR.cc/2026/Conference/Submission18415/Reviewer_3Dzw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18415/Reviewer_3Dzw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971363200, "cdate": 1761971363200, "tmdate": 1762928116076, "mdate": 1762928116076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of reconstructing a 3D object from images captured under multiple object poses by introducing PFGS, a pipeline that fuses multi-pose camera tracks into a single 3DGS model. The system combines COLMAP initialization, DINO-based view similarity filtering, Fast3R for multi-view pose estimation, and a silhouette-guided similarity transform followed by RGB refinement to align poses across different capture sessions. Once aligned, 3DGS is fine-tuned to produce a unified representation. Experiments on a synthetic dataset and five real objects show improved completeness and pose accuracy relative to COLMAP, VGGT, and Fast3R. However, the method is heavily procedural, combines well-known building blocks, and provides only modest improvements over a strong COLMAP baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Motivation and relevant problem setting.** - The paper addresses a realistic case where multi-pose captures are needed due to occlusions or physical constraints, which is a practical gap in existing 3DGS pipelines. The problem formulation of aligning auxiliary-pose views to a main-pose reconstruction is clear and grounded in real capture workflows.\n\n- **Reasonable pipeline design with careful alignment of modules.** - The use of mixed-pose selection, global Fast3R alignment, and silhouette-based similarity transform makes the pipeline robust when naive COLMAP fails across poses. Ablations support that mixed-pose selection and local refinement meaningfully improve alignment quality."}, "weaknesses": {"value": "- **Method is largely an engineering assembly of known components** - The pipeline strings together COLMAP, DINOv2, Fast3R, SAM2 masks, and 3DGS - each doing exactly what they were designed for. The silhouette-consensus fusion step is essentially a pose-space similarity alignment driven by mask IoU, which is useful but not conceptually novel.\n- **Extremely high complexity and runtime.** The global and local registration pipeline averages 22.6 minutes on a 4090 (for ~300 images), which is expensive given that COLMAP can already handle many cases. The method delivers incremental improvements at a substantially higher computational cost, raising concerns about practicality.\n- **Heavy reliance on accurate segmentation masks.**  Silhouette-guided registration/least-squares hinges on SAM2-style masks, leakage, thin structures, or reflectance will degrade both global alignment and local refinement. The paper does not analyze failure modes tied to mask quality.\n- **Results are not strongly convincing relative to strong baselines.** - In several cases, COLMAP already provides near-GT camera accuracy, and improvements in NVS metrics are small (often ~0.3–0.8 dB PSNR).\n- **Narrow and small-scale evaluation.** The core synthetic set covers five objects, and the real data also spans five objects (mostly controlled/turntable captures), which is too small to substantiate robustness claims. Improvements over baselines in NVS are modest and not statistically analyzed.\n- COLMAP is used for initial main/auxiliary camera poses and also as a baseline; the pipeline explicitly relies on COLMAP and Fast3R, undermining the narrative that PFGS is a principled replacement rather than a wrapper.\n- With the pace of 3D foundation models rapidly increasing (e.g., MAST3R/Spann3R/Fast3R improvements), the engineering-heavy nature of this pipeline risks becoming obsolete quickly. The paper positions itself as incremental bridge work, not actual progress."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Za5XjGkuk0", "forum": "8bJs2ElHGM", "replyto": "8bJs2ElHGM", "signatures": ["ICLR.cc/2026/Conference/Submission18415/Reviewer_bk66"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18415/Reviewer_bk66"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028053468, "cdate": 1762028053468, "tmdate": 1762928115592, "mdate": 1762928115592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}