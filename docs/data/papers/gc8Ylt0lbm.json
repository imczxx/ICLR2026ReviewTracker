{"id": "gc8Ylt0lbm", "number": 13071, "cdate": 1758213290965, "mdate": 1759897467370, "content": {"title": "DynaGuard: A Dynamic Guardian Model With User-Defined Policies", "abstract": "Guardian models play a crucial role in ensuring the safety and ethical behavior of user-facing AI applications by enforcing guardrails and detecting harmful content. While standard guardian models are limited to predefined, static harm categories, we introduce DynaGuard, a suite of dynamic guardian models offering novel flexibility by evaluating text based on user-defined policies, and DynaBench, a dataset for training and evaluating dynamic guardian models. Our models provide both rapid detection of policy violations and a chain-of-thought reasoning option that articulate and justify model outputs. Critically, DynaGuard not only surpasses static models in detection accuracy on traditional safety categories, but is competitive with frontier reasoning models on free-form policy violations, all in a fraction of the time. This breakthrough makes DynaGuard a critical tool for language model guardrails.", "tldr": "We demonstrate the ability to perform content moderation for guardrails using only user-defined policies in a model's context window.", "keywords": ["Safety", "Guardrails", "Content Moderation", "Compliance"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/825854e708f74489d16118c56fb880126b7222d3.pdf", "supplementary_material": "/attachment/c6aa7093c7a15cbd75591314aa26dc1603a8f826.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces DynaGuard, a dynamic guardian LLM that (1) enforces user-defined policies rather than a fixed taxonomy, (2) produces interpretable rationales for its judgments, and (3) supports a token-efficient classification-style inference path for low latency. Training uses a mixed recipe—SFT on policy-conditioned, reasoned exemplars plus GRPO for reinforcement—and a new dataset, DynaBench, comprising 61.5k training samples paired with policies and a 543-example human-constructed test set. The model family (1.7B/4B/8B) reports strong F1 on WildGuardMix, HarmBench, Safe-RLHF, XS-Test, and especially its own DynaBench that particularly enforces and tests custom policies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper introduces DynaBench, a policy-conditioned safety dataset that covers real-world adaptive policy settings such as corporate policies. This enables programmable moderation possible with in-context learning for applying enterprise or jurisdiction-specific rulebooks, making the benchmark directly applicable to customized real-world guard-model deployments.\n2. The paper shows that pre-existing guards transfer poorly to policy-conditioned evaluation on DynaBench (Llama-Guard F1 13.1% even with zero-shot rule adaptation capabilties), underscoring limited flexibility for guard models to be applied in practical situations. The proposed model DynaGuard, trained to follow policies, closes much of this gap with F1 73.1% on DynaBench, demonstrating substantially better coverage for customized policies."}, "weaknesses": {"value": "1. Although the paper shares the same core idea with Constitutional Classifiers [1] in defining policies and generating datasets based on the policies for training classifiers, it does not mention the work. Please position this work with regard to Constitutional Classifiers.\n2. Most of the other baseline models are non-CoT, leaving Table 3 misleading since it mentions non-CoT only in DynaGuard and GuardReasoner. \n3. The core motivation of guard models is that it has a better compute/defense tradeoff than language models, thus making it reliable for real-world production. DynaGuard tends to use CoT for better scores, but does not take into account the increased computation. For example, HarmAug[2] shows that their model serves as an attractive practical candidate by comparing FLOPs, latency, peak memory, and even monetary cost with pre-existing guard models.\n4. The model performance benchmark only reports F1 score, which is sensitive to thresholds, which is not reported in the paper. For more reliable results, reporting AUPRC as well would be a good alternative.\n\n[1] Sharma, M., Tong, M., Mu, J., Wei, J., Kruthoff, J., Goodfriend, S., ... & Perez, E. (2025). Constitutional classifiers: Defending against universal jailbreaks across thousands of hours of red teaming. arXiv preprint arXiv:2501.18837.\n\n[2] Lee, S., Seong, H., Lee, D. B., Kang, M., Chen, X., Wagner, D., ... & Hwang, S. J. (2024). Harmaug: Effective data augmentation for knowledge distillation of safety guard models. ICLR 2025."}, "questions": {"value": "1. The CoT version and the non-CoT version of DynaGuard seems to be trained as a separate model. Is that correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7taUkMNg22", "forum": "gc8Ylt0lbm", "replyto": "gc8Ylt0lbm", "signatures": ["ICLR.cc/2026/Conference/Submission13071/Reviewer_G73o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13071/Reviewer_G73o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642855748, "cdate": 1761642855748, "tmdate": 1762923798610, "mdate": 1762923798610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Thank you all for your thoughtful and constructive feedback. The time you invested makes this a better paper and we appreciate your contribution.\n\nWe are actively working through the points you raised. Rather than waiting to address everything at once, we plan to post responses incrementally as we complete each set of experiments and analyses. This will allow us to share results with you more promptly while maintaining the detail and rigor each point deserves."}}, "id": "CufPTpF6Pe", "forum": "gc8Ylt0lbm", "replyto": "gc8Ylt0lbm", "signatures": ["ICLR.cc/2026/Conference/Submission13071/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13071/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13071/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763129782933, "cdate": 1763129782933, "tmdate": 1763129782933, "mdate": 1763129782933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a semi-synthetic data generation pipeline and a resulting dataset (called DynaBench) of guardrail relevant data. The dataset is split into:\n- **Generated training set** of 40K guardrail policies, each accompanied by simulated chatbot conversations reflecting adherence and violation. \n- **An evaluation dataset** of 543 additional, hand-written examples.\n\nBased on the introduced dataset, the authors post-train Qwen3 models into guardian models using a mix of SFT and GRPO. Finally, the authors demonstrate that the trained guardrail models outperform existing models on the introduced DynaBench while showing competitive performance on existing datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Principled Dataset Construction Pipeline**: To ensure a wide diversity in the dataset, the authors leverage a hand-written set of attribute seeds for user and agent personas, as well as a curated bank of rules. Based on this, they use LLMs to generate persona profiles, paraphrase rules, generate multi-turn dialogues and finally to label the data.\n- **Extensive Evaluation across Multiple Benchmarks**: The authors evaluate on a wide range of benchmarks showing improved accuracy scores, and present various training and model ablations, as well as closer examination of failure modes.\n- **Effectiveness Ablation Across Model Families:** The author demonstrate the effectiveness of the generated data across multiple models (section 4.1.)"}, "weaknesses": {"value": "- **Limited Technical Novelty:** While it’s definitely a solid dataset contribution in the field of guardrails, the technical novelty of the proposed approach is limited. Similar dataset construction pipelines were already proposed in many different areas. To post-train the models, authors follow a relative straightforward post-training procedure.\n- **DynaGuard Prompt is not provided**: Could you please provide the DynaGuard prompt that you also used for the API models? \n- **Prompt Impact:** Did the authors analyze the impact of the prompt (for example comparing a baseline prompt vs. DynaGuard prompt on the API models)? This is especially important as most baseline models have been trained on a fixed set of safety categories, which are also reflected in the prompt."}, "questions": {"value": "- What's the prompt used for DynaGuard?\n- Could you also add other baseline models to the analysis in Figure 3 (left part; accuracy over rules, tokens, turn and hops)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CcGYm3wDfq", "forum": "gc8Ylt0lbm", "replyto": "gc8Ylt0lbm", "signatures": ["ICLR.cc/2026/Conference/Submission13071/Reviewer_ffXw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13071/Reviewer_ffXw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661988107, "cdate": 1761661988107, "tmdate": 1762923797731, "mdate": 1762923797731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce DynaGuard, a dynamic guardian model designed to perform safety and compliance for LLMs under user-defined policies. Unlike static Guardrail system, DynaGuard conditions on an explicit policy context provided by the user. The authors also release DynaBench, a new dataset containing both standard safety categories and custom policy scenarios. Experiments show that DynaGuard maintains strong performance on standard harms and substantially outperforms static guard models when evaluating against novel or user-specific policies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This is a real deployment gap, this paper adapts safety filter for dynamic policies. \n2.  Provides a valuable benchmark for studying policy-driven moderation; could become a standard resource.1"}, "weaknesses": {"value": "1. Real-world policies are often long, ambiguous, or inconsistent; paper does not evaluate robustness to noisy or underspecified policies.\n2. Unclear how well DynaGuard scales to highly domain-specific or legalistic policies beyond the dataset’s scope (e.g., finance regulations, medical privacy).\n3. Out-of-domain evaluations can strengthen the paper."}, "questions": {"value": "Please address weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "x1kuPkOT3X", "forum": "gc8Ylt0lbm", "replyto": "gc8Ylt0lbm", "signatures": ["ICLR.cc/2026/Conference/Submission13071/Reviewer_32vv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13071/Reviewer_32vv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868461713, "cdate": 1761868461713, "tmdate": 1762923797139, "mdate": 1762923797139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DynaBench, a comprehensive benchmark designed to evaluate the safety and alignment capabilities of guardian models across various failure modes and real-world scenarios, and proposes DynaGuard, a set of models optimized for safety through fine-grained guardrails. The authors demonstrate that DynaBench effectively captures diverse safety challenges, with high annotation consistency and meaningful differentiation among models. Experimental results show that DynaGuard models outperform existing open-weight safety models, achieving lower error rates and improved robustness across multiple failure modes, while maintaining competitive overall accuracy. These contributions advance the development of safer, more controllable language models by providing a valuable evaluation framework and specialized models tailored for safety-critical applications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "A novel dataset, DynaBench, for training and evaluation of guardian models is proposed. It contains multi-turn conversations and adversarial jailbreaking prompts. It reveals the weakness of existing guardian models.\n\nA novel guardian model, DynaGuard, is proposed, which is trained using DynaBench and exhibits promising performance on several benchmarks. DynaGuard is claimed to be the first model addressing the four desired characteristics of guardian models listed in Table 1."}, "weaknesses": {"value": "It was not clear to me how the DynaGuard is technically different from the existing guardian models. In Sections 1 and 2, the difference in (resulting) characteristics among guardian models are mentioned. However, the technical differences between guardian models such as the difference in training scheme, system models, etc., are not clearly mentionedin Sections 3.4 and 3.5. Therefore, it is not clear to me which parts contributed to the good performance of DynaGuard.\n\nNo statistical evaluation is provided for the results. If I understand correctly, the result is reported for a single training result for each method. Because the standard deviation or related variation metrics is not provided, I can not judge whether the difference is meaningful or not."}, "questions": {"value": "Please answer to the points mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mDRGzIm5SY", "forum": "gc8Ylt0lbm", "replyto": "gc8Ylt0lbm", "signatures": ["ICLR.cc/2026/Conference/Submission13071/Reviewer_pgrp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13071/Reviewer_pgrp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955865320, "cdate": 1761955865320, "tmdate": 1762923796767, "mdate": 1762923796767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}