{"id": "NDryJf9DSL", "number": 5545, "cdate": 1757919386357, "mdate": 1759897968527, "content": {"title": "Aryabhata: An exam-focused language model for JEE Math", "abstract": "We present Aryabhata 1.0, a compact 7B parameter math reasoning model optimized for the Indian academic exam, the Joint Entrance Examination (JEE). Despite rapid progress in large language models (LLMs), current models often remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong open-weight reasoning models, followed by supervised fine-tuning (SFT) with curriculum learning on verified chain-of-thought (CoT) traces curated through best-of-$n$ rejection sampling. To further boost performance, we apply reinforcement learning with verifiable rewards (RLVR) using A2C objective with group-relative advantage estimation alongwith novel exploration strategies such as Adaptive Group Resizing and Temperature Scaling. Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and efficiency, while offering pedagogically useful step-by-step reasoning. We release Aryabhata as a foundation model to advance exam-centric, open-source small language models..", "tldr": "", "keywords": ["reinforcement learning with verifiable rewards", "generative language model", "model merging", "rejection sampling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d10e2edf15a4a91b88ed779e72f851a021d131ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tries various existing model technologies and combines them into a training solution for JEE, then test their model on JEE Main 2025, MATH500 and GSM8K."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "A model was trained for Joint Entrance Examination (JEE), maybe useful for Indian academic exam."}, "weaknesses": {"value": "This is more like an experiment report using a variety of techniques, and the combination of these techniques is not informative.\nLacks ablation experiments, for example, does not fuse model weights but uses a single model, and the necessity of the SFT stage.\nThe dataset used for model training is not publicly available, making it unclear whether the results are reproducible.\nDuring the evaluation phase, verification was performed only on GSM8K and MATH500. The benchmark itself is not difficult, and no general applicability testing was performed."}, "questions": {"value": "Q1: Why not use open source data to verify the method? It is impossible to know whether the improvement in model performance comes from the data itself rather than the training method.\n\nQ2: What is the innovation of this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IPwDDw3k1t", "forum": "NDryJf9DSL", "replyto": "NDryJf9DSL", "signatures": ["ICLR.cc/2026/Conference/Submission5545/Reviewer_x2XF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5545/Reviewer_x2XF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572898983, "cdate": 1761572898983, "tmdate": 1762918124150, "mdate": 1762918124150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Aryabhata 1.0, a compact 7B parameter math reasoning model optimized for the Indian academic exam. The method consists of merging strong open-weight reasoning models, SFT and RLVR. The final model outperforms existing models in accuracy and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. It presents an engineering-oriented approach to training a reasoning model for India’s Joint Entrance Examination (JEE), combining model merging, supervised fine-tuning, and reinforcement learning in a clear and organized manner."}, "weaknesses": {"value": "1. The proposed methodology is mainly engineering-oriented, without addressing a clear research challenge or presenting substantial methodological novelty. The techniques described, such as group-relative advantage estimation and exploration strategies like adaptive group resizing and temperature scaling, have been explored in prior works and appear incremental.\n\n2. More importantly, the paper lacks ablation studies to validate the effectiveness of the proposed components. Without such analysis, it is difficult to assess the contribution of each part of the training pipeline and the overall necessity of the proposed approach."}, "questions": {"value": "See the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mn8UEuNH3P", "forum": "NDryJf9DSL", "replyto": "NDryJf9DSL", "signatures": ["ICLR.cc/2026/Conference/Submission5545/Reviewer_BitB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5545/Reviewer_BitB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804670322, "cdate": 1761804670322, "tmdate": 1762918123308, "mdate": 1762918123308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Aryabhata, a compact 7B-parameter language model specifically designed to solve JEE math problems. This work used a multi-stage pipeline: merging three specialized 7B models; curating a 130K-question JEE-aligned dataset; fine-tuning via sft, reject sampling and reinforcement learning. The model achieves state-of-the-art performance on JEE 2025."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. End-to-end. This paper covers various mainstream technologies, from model merging, to data acquisition, and then to different training methods such as SFT and RL."}, "weaknesses": {"value": "1. This paper does not address any scientific questions. Although it employs a variety of techniques, it reads more like a technical report, and the scientific motivation for using LLMs to solve JEE problems is not clearly articulated.\n2. The paper lacks baseline methods. It does not present the performance after model merging or the results at each training stage, making it unclear which techniques are actually effective.\n3. The papepr does not conduct ablation studies on the methods used. The proposed 'Adaptive Group Sizing' method lacks experimental evidence to demonstrate its effectiveness.\n4. GSM8K and MATH cannot serve as out-of-domain benchmarks. They are too simple for today's models.\n5. Poor writing quality and presentation: The overall writing is very poor, including formatting, length. and it is Verbose and Unfocused."}, "questions": {"value": "1. Can you provide more clear motivation for solving JEE problems?\n2. The authors need to provide more ablation results to show the effectness of proposed methods.\n3. What is the key factor to obtain the sota performance of JEE problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MRmUibkZaO", "forum": "NDryJf9DSL", "replyto": "NDryJf9DSL", "signatures": ["ICLR.cc/2026/Conference/Submission5545/Reviewer_4sGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5545/Reviewer_4sGY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941658239, "cdate": 1761941658239, "tmdate": 1762918122580, "mdate": 1762918122580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **Aryabhata 1.0**, a 7B-parameter small language model (SLM) specifically designed for mathematical reasoning in India’s high-stakes Joint Entrance Examination (JEE).\nThe authors train the model through a sequential pipeline involving **Model Merging**, **Supervised Fine-Tuning (SFT)**, and **Reinforcement Learning with Verifiable Rewards (RLVR)**.\nOn the MATH500 benchmark, Aryabhata 1.0 outperforms several open-source 7B models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper adopts a standard fine-tuning (post-training) pipeline, which is a reasonable approach for developing a domain-expert model tailored to a specific application scenario.\n2. The model demonstrates superior performance on the JEE benchmark compared to other existing models."}, "weaknesses": {"value": "1. The paper does not introduce any new methods or insights; it merely combines existing techniques.\n2. The paper lacks comprehensive ablation studies to analyze the contribution of each component. For instance, it remains unclear how much of the **performance gain comes from** Model Merging, SFT, or RLVR. Questions such as how the model would perform without Model Merging or without SFT are left unanswered.\n3. The paper does not evaluate the model beyond mathematics, despite claiming that “we release Aryabhata as a foundation model to advance exam-centric applications.” In practice, small models often suffer from catastrophic forgetting after domain-specific fine-tuning. The authors are encouraged to test Aryabhata on broader benchmarks such as MMLU-Pro to verify its general capabilities.\n4. The model’s performance on MATH500 is not the best; for example, it does not surpass DeepSeek-R1-Distill-Qwen-7B. Moreover, GSM8K is too simple for higher-education-level evaluation, and its score has limited significance.\n5. The paper suffers from presentation issues: Table 1 is overly large for its limited importance, Figure 1 has text that is too small, and the authors’ own model is not highlighted in bold for easy comparison."}, "questions": {"value": "I am particularly curious about the second weakness, i.e., the lack of ablation studies. Specifically:\n\n1. What is the model’s performance after each of the three stages: Model Merging, SFT, and RLVR?\n2. How would the final performance degrade if any of these stages were skipped?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1rYfWEx6XE", "forum": "NDryJf9DSL", "replyto": "NDryJf9DSL", "signatures": ["ICLR.cc/2026/Conference/Submission5545/Reviewer_gZxq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5545/Reviewer_gZxq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982684402, "cdate": 1761982684402, "tmdate": 1762918122119, "mdate": 1762918122119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}