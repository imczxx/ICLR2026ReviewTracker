{"id": "65Ai8mLfjI", "number": 18902, "cdate": 1758291864590, "mdate": 1759897074386, "content": {"title": "Revisiting Global Text Conditioning in Diffusion Transformers", "abstract": "Diffusion transformers typically incorporate textual information via (i) attention layers and (ii) a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention.\nIn this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective—serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.", "tldr": "", "keywords": ["diffusion models", "image and video generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cced972c802ab152e4315d19467e5bf5541ccd95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the impact of modulation-based text conditioning on text-to-image diffusion models. The authors demonstrate that this technique is an important factor in increasing the quality of generated images. They introduce a simple, training-free method that improves performance across various diffusion models without imposing any additional computational burden at inference time."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, presenting its concepts and results with clarity.\n- The proposed approach is easy to apply, computationally inexpensive, and demonstrably improves generation quality.\n- The method is validated through a sound evaluation on state-of-the-art models, confirming its effectiveness.\n- A significant advantage is the method's broad applicability, as it can be used even with models that do not rely on a CLIP text encoder."}, "weaknesses": {"value": "- Based on the observation presented in Table 1 that adding CLIP embeddings can increase the quality of images generated from short prompts, it would be beneficial if the authors would also separate their evaluation based on the criteria of prompt length, to demonstrate that modulation can increase generation quality even with long prompts. A more detailed analysis would strengthen the method's reliability.\n- The evaluation lacks a comparison to common, practical methods for quality enhancement. For example, many users simply add phrases like \"good quality image\" or \"very detailed image\" to their prompts or use negative guidance, which is a standard feature in many diffusion model interfaces.\n- A potential trade-off between the enforced modulation and prompt fidelity is not explored. If the aesthetic qualities introduced by the modulation contradict the user's explicit request in a prompt, it could negatively impact prompt-following. An exploration of this dynamic would strengthen the submission."}, "questions": {"value": "- The quality improvement shown in Figure 5 appears to stem largely from guiding the outputs to look more photographic, particularly with the introduction of blurred backgrounds. While this is often desirable, it raises a question about the trade-off between this aesthetic guidance and prompt fidelity. For example, if a user explicitly asks for a plain background, will the modulation override this request to produce a more \"image-like\" result with depth of field? Have the authors evaluated this trade-off?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VNvvap7JTv", "forum": "65Ai8mLfjI", "replyto": "65Ai8mLfjI", "signatures": ["ICLR.cc/2026/Conference/Submission18902/Reviewer_DbAo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18902/Reviewer_DbAo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761310631361, "cdate": 1761310631361, "tmdate": 1762930877750, "mdate": 1762930877750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of the CLIP modulation component within Text-to-Image (T2I) diffusion models. The authors begin by questioning its necessity, demonstrating through ablation that its removal has a minimal impact on overall generation performance. Despite this finding, the authors argue that this component enable controllable shifts in the generation process. They propose a novel method that involves altering the modulation guidance at different blocks of the diffusion transofmer. This claim is supported by a thorough quantitative analysis across several T2I models and is further extended to text-to-video models. The authors show that their method improves generation quality in terms of aesthetics and complexity. They also demonstrate that it mitigates common generation failures, such as incorrect object counts and anomalous finger generation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is clearly written and well-structured. A primary strength lies in its comprehensive experimental validation. The experiments are thorough and are conducted on 4 T2I models that are trained with CLIP modulation, and even included additional model that was not using CLIP, training it to incorporate CLIP modulation. Furthermore, they include text-to-video models, thereby broadening the applicability of their findings."}, "weaknesses": {"value": "The primary weakness is the limited novelty of the method. This method (with the exception of choosing the dynamic modulation strategies) was already presented in [1] as a naive approach (Equation 2). If the authors disagree, I would be happy to discuss and understand the novelty better.\n\nA second, smaller weakness, concerns the justification for the proposed dynamic modulation strategies. These strategies are heuristically derived from observed attention patterns within the model. This reliance is a potential weakness, as attention weights are not always a reliable or faithful indicator [2] of a model's internal semantic processing at different hierarchical levels. While the authors attempt to validate these attention-driven heuristics through an ablation study, the results appear inconclusive and fail to provide a definitive justification for the chosen strategies.\n\n[1] TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space (Garibi and Yadin et al. 2025)\n[2] Attention is not Explanation. (Jain et al. 2019)"}, "questions": {"value": "- The analysis suggests that CLIP modulation is more influential for short prompts compared to long prompts. Do the authors have a hypothesis for this observed phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DUaPJaHQ6m", "forum": "65Ai8mLfjI", "replyto": "65Ai8mLfjI", "signatures": ["ICLR.cc/2026/Conference/Submission18902/Reviewer_rF5b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18902/Reviewer_rF5b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761314050974, "cdate": 1761314050974, "tmdate": 1762930876775, "mdate": 1762930876775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the role of global text conditioning in diffusion transformers. In response to the prevailing trend of abandoning modulation mechanisms in favor of attention-only approaches, the authors demonstrate through analysis that while conventionally used pooled text embeddings contribute limited benefits, repurposing them as a guidance mechanism can effectively adjust the diffusion trajectory toward more desirable attributes. This approach, termed modulation guidance, is training-free, straightforward to implement, and enhances performance across multiple tasks including text-to-image, text-to-video generation, and image editing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The revisiting and discovery that global text conditioning can be leveraged as a powerful control signal—rather than being merely a passive input—is novel. The proposed dynamic modulation guidance demonstrates a clear ability to address classic and stubborn challenges in T2I generation, such as hand synthesis and object counting, which is a significant finding.\n\n2. The paper is impressive in its extensive experimental scope, demonstrating effectiveness across a diverse set of tasks—including text-to-image, text-to-video, and instruction-guided editing—and model architectures, encompassing transformer-based DMs and the CLIP-free COSMOS model.\n\n3. The paper is well-structured and easy to follow."}, "weaknesses": {"value": "I have the following two major questions:\n\n1. I noticed that different hyperparameters are used for different tasks and generation types/styles In Tab.5. Could the authors provide more detailed guidance on the process of selecting the appropriate strategy and its associated hyperparameters for a **new, unseen task**? Is this process largely heuristic, requiring manual search for each new situation, or are there general principles or a methodology that can be derived from the observations in Figure 3 to make this selection more systematic?\n\n2. **Connection and Distinction to h-space Methods:** The work [1] demonstrates that diffusion models possess a semantic latent space (h-space) and that rescaling the difference in latent features ($\\Delta h$) can control attribute strength. Could the authors discuss the primary distinction between their method and this prior work? Specifically, is it possible to achieve a similar guidance effect by simply rescaling $\\Delta h$, analogous to Equation 3 in this paper, instead of explicitly using the CLIP embedding to compute $y(p^+, t) - y(p^-, t)$?\n\n[1] Mingi Kwon, Jaeseok Jeong, Youngjung Uh. \"Diffusion Models Already Have a Semantic Latent Space\". *ICLR*, 2023."}, "questions": {"value": "Please see my **Weaknesses** part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0j9v8PmFBs", "forum": "65Ai8mLfjI", "replyto": "65Ai8mLfjI", "signatures": ["ICLR.cc/2026/Conference/Submission18902/Reviewer_Tihg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18902/Reviewer_Tihg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410557721, "cdate": 1761410557721, "tmdate": 1762930875774, "mdate": 1762930875774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the role of global (pooled) text conditioning in diffusion transformers, which has recently been discarded in favor of attention-only conditioning. The authors first demonstrate through empirical analysis that the pooled CLIP embedding contributes little to generation quality in several state-of-the-art models (e.g., FLUX, HiDream-Fast), especially with long prompts. However, they propose a novel perspective: repurposing the pooled embedding for modulation guidance—a training-free, plug-and-play technique that steers the diffusion process toward desirable visual properties (e.g., aesthetics, complexity, hand realism) by extrapolating in the modulation space using positive/negative prompt pairs. The method is simple, incurs negligible overhead, works with or without classifier-free guidance (CFG), and can be retrofitted into models that originally lack pooled embeddings. Extensive experiments across text-to-image, text-to-video, and image editing tasks show consistent improvements in human evaluations and automatic metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper provides a clear and convincing empirical investigation into why global text conditioning appears ineffective in current models, filling an important gap in understanding.\n\n2. Modulation guidance is training-free, easy to implement, computationally lightweight, and broadly applicable across architectures and tasks."}, "weaknesses": {"value": "1. The core idea of using pooled embeddings for guidance resembles prior work on semantic directions in GANs (e.g., StyleGAN) and recent methods like TokenVerse or Concept Sliders, though the application to modulation space in diffusion transformers is new.\n\n2. The method introduces new hyperparameters (guidance scale w, layer indices), requiring tuning for different tasks—though ablations help, this adds complexity compared to plug-and-play baselines."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jfW3PnDEAA", "forum": "65Ai8mLfjI", "replyto": "65Ai8mLfjI", "signatures": ["ICLR.cc/2026/Conference/Submission18902/Reviewer_Jrir"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18902/Reviewer_Jrir"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814589134, "cdate": 1761814589134, "tmdate": 1762930874401, "mdate": 1762930874401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}