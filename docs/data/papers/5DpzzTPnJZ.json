{"id": "5DpzzTPnJZ", "number": 3146, "cdate": 1757342531483, "mdate": 1759898106355, "content": {"title": "The Rank and Gradient Lost in Non-stationarity: Sample Weight Decay for Mitigating Plasticity Loss in Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) suffers from plasticity loss severely due to the nature of non-stationarity, which impairs the ability to adapt to new data and learn continually. Unfortunately, our understanding of how plasticity loss arises, dissipates, and can be dissolved remains limited to empirical findings, leaving the theoretical end underexplored. To address this gap, we study the plasticity loss problem from the theoretical perspective of network optimization. By formally characterizing the two culprit factors in online RL process: the non-stationarity of data distributions and the non-stationarity of targets induced by bootstrapping, our theory attributes the loss of plasticity to two mechanisms: the rank collapse of the Neural Tangent Kernel (NTK) Gram matrix and the Θ(1/k) decay of gradient magnitude. The first mechanism echoes prior empirical findings from the theoretical perspective and sheds light on the effects of existing methods, e.g., network reset, neuron recycle, and noise injection. Against this backdrop, we focus primarily on the second mechanism and aim to alleviate plasticity loss by addressing the gradient attenuation issue, which is orthogonal to existing methods. We propose Sample Weight Decay (SWD) --- a lightweight method to restore gradient magnitude, as a general remedy to plasticity loss for deep RL methods based on experience replay. In experiments, we evaluate the efficacy of SWD upon TD3, SAC with SimBa architecture in MuJoCo and DeepMind Control Suite tasks. The results demonstrate that SWD effectively alleviates plasticity loss and consistently improves learning performance across various configurations of deep RL algorithms, UTD, network architectures, and environments, achieving SOTA performance on challenging DMC Humanoid tasks.", "tldr": "", "keywords": ["Reinforcement Learning", "non-stationary data distributions", "plasticity loss"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c8cb1e4e4c09e6b36f142b53403d7a4617ace56.pdf", "supplementary_material": "/attachment/d7c78e3e3424abbe0a6cc447d3455012e9a3853f.zip"}, "replies": [{"content": {"summary": {"value": "Studies plasticity loss in deep RL under non-stationarity. The theory isolates two mechanisms: (i) NTK rank collapse across sequential warm-starts and (ii) a $\\Theta (1/k)$ decay of the initial gradient each round. Motivated by (ii), the paper proposes Sample Weight Decay (SWD)—a lightweight recency-weighted replay scheme—to restore gradient magnitude. Experiments on TD3 (MuJoCo) and SAC with SimBa (DMC Humanoids/Dog) show consistent gains with reliable aggregate metrics (IQM/median/mean, bootstrap CIs) and a reverse ablation (SWA) that up-weights old samples and underperforms, alongside GraMa analyses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Identifies a crisp cause of plasticity loss and links it to a tractable remedy (recency weighting)\n- Very simple algorithm (SWD) with negligible overhead, orthogonal to architectural methods (ReDo, Plasticity injection, etc.)\n- Consistent empirical improvements across TD3 and SimBa-SAC; reverse ablation (SWA) plus GraMa trends support the mechanism.\n- Uses reliable RL reporting (IQM/median/mean + stratified bootstrap CIs)."}, "weaknesses": {"value": "- Theory scope. Main results are derived for FQI-style/population losses; transfer to fully practical bootstrapped targets with representation drift is not fully established.\n- Breadth. Evaluation is confined to continuous control; adding a pixel-based or sparse-reward task (e.g., DMControl pixels, AntMaze) would test generality.\n- Over-edited text (LLM side-effects). While LLM assistance can improve flow/grammar, several sentences become awkward or semantically off and harm readability—for example, the abstract’s “How plasticity loss arises, dissipates and can be dissolved.” A careful human pass is needed to fix misuses and improve readability.\n- Related work is too narrowly framed (over-emphasis on resets). The section concentrates on reset-style approaches while under-representing other relevant families—particularly **churn-reduction methods** and **auxiliary-loss–based representation stabilisation**. Please discuss these lines of work and clarify how SWD differs or complements them (see, e.g., churn-reduction: [https://arxiv.org/abs/2506.00592](https://arxiv.org/abs/2506.00592); auxiliary losses: [https://arxiv.org/abs/2405.00662](https://arxiv.org/abs/2405.00662))."}, "questions": {"value": "- How sensitive are results to **linear vs. exponential** decay and to $((w_{\\min}, T))$? Any failure modes with small buffers or rapid behaviour-policy shifts? \n- Do SWD’s benefits persist with **pixel observations** or **sparse rewards**? (A small add-on task would suffice.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SxoeH4TwpM", "forum": "5DpzzTPnJZ", "replyto": "5DpzzTPnJZ", "signatures": ["ICLR.cc/2026/Conference/Submission3146/Reviewer_2bzK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3146/Reviewer_2bzK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896505472, "cdate": 1761896505472, "tmdate": 1762916571473, "mdate": 1762916571473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies plasticity loss in deep RL through an optimization lens, identifying two mechanisms: (i) NTK rank degeneration and (ii) gradient attenuation that scales as Θ(1/k) under non-stationary data/targets with experience replay. Building on this analysis, the authors propose Sample Weight Decay (SWD)—a simple, plug-and-play, age-based sampling scheme for replay buffers that linearly down-weights older samples to counteract gradient decay and restore gradient magnitude. Experiments on MuJoCo (TD3) and DMC (SimBa-SAC) show consistent gains—including strong results on Humanoid—plus robustness across UTD ratios; ablations (including a reverse “SWA” variant) and GraMa measurements support the mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear theoretical framing with actionable takeaways. The paper formalizes how distribution/target non-stationarity yields NTK rank issues and a Θ(1/k) gradient-magnitude decay, then links performance to Bellman-residual control via a suboptimality bound—cleanly motivating data-weighting interventions.\n\n2. Simple, general, and orthogonal method. SWD is an easy drop-in change to replay sampling, compatible with TD3/SAC (and, in principle, other replay-based methods) and positioned as orthogonal to architectural “plasticity-injection” tricks. The paper claims minimal overhead and plug-and-play practicality.\n\n3. Compelling empirical evidence. Consistent improvements across MuJoCo and DMC (including Humanoid), robustness to varying UTD, and reverse validation via SWA plus GraMa analysis strengthen the causal story beyond raw scores."}, "weaknesses": {"value": "1. Missing related work / plasticity literature coverage.\n\nThe related work should more deeply connect to recent plasticity and replay-weighting literature. Please discuss and contrast with, e.g.:\n\n- Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning (ICML’24).\n- Disentangling the causes of plasticity loss in neural networks (CoLLA’24).\n- Hyperspherical Normalization for Scalable Deep RL (ICML’25).\n- Mitigating Plasticity Loss in Continual RL by Reducing Churn (ICML’25).\n- A Forget-and-Grow Strategy for Deep RL Scaling in Continuous Control (ICML’25).\n\n2. Overlapping idea; contribution clarity vs ER-decay.\n\nConceptually, SWD (linear, age-based down-weighting) looks very close to prior ER-decay heuristics, with differences seemingly in coefficients/schedules. The paper does offer more formalism, but the delta in contribution should be crystal clear: which parts are novel theory, which are new algorithmic prescriptions beyond a tuned decay, and what guarantees (if any) distinguish SWD from ER-decay? I’m open to a high score even if the mechanism is similar—provided the theoretical backup and empirical analysis are meticulous and make the case for why this instantiation is principled/non-equivalent.\n\n3. Implementation and efficiency details are thin.\n\nA per-sample weight update naively done every step can be costly. Please:\n- Describe the efficient implementation (lazy updates? piecewise-linear buckets? periodic renormalization?).\n- Report end-to-end wall-clock and GPU-hour costs on standard hardware for SimBa vs SimBa+SWD;"}, "questions": {"value": "1. Beyond sample-efficient regimes. \n\nAuthors primarily test in sample-efficient settings. What happens with very long training (e.g., 10M env steps with a 1M buffer) where fresh samples repeatedly overwrite older ones? Do you observe more catastrophic forgetting under high churn, and does SWD still mitigate it or saturate? (This is important because small buffers + long horizons can exacerbate plasticity.)\n\n\n\nI’m open to increasing the score if the authors adequately address the weaknesses—particularly by deepening related work discussion, clarifying the novelty relative to ER-decay, and detailing the efficiency and long-horizon robustness experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xL0rDYZPtK", "forum": "5DpzzTPnJZ", "replyto": "5DpzzTPnJZ", "signatures": ["ICLR.cc/2026/Conference/Submission3146/Reviewer_8vUF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3146/Reviewer_8vUF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956432917, "cdate": 1761956432917, "tmdate": 1762916571252, "mdate": 1762916571252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates plasticity loss in deep reinforcement learning from a theoretical optimization perspective. The authors identify two mechanisms causing plasticity loss: (1) rank collapse of the Neural Tangent Kernel (NTK) Gram matrix, and (2) Θ(1/k) decay of gradient magnitude during training. Based on this analysis, they propose Sample Weight Decay (SWD), a lightweight sampling strategy that assigns higher probabilities to more recent samples in the replay buffer to counteract gradient attenuation. Experiments on MuJoCo and DeepMind Control Suite tasks with TD3 and SAC algorithms demonstrate consistent performance improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- SWD is remarkably simple to implement with minimal computational overhead, making it easily applicable across different RL algorithms and architectures as a plug-and-play solution, which is practically valuable compared to more invasive methods.\n- The experimental validation is comprehensive, including evaluation on two benchmark suites, comparison with PER, reverse validation through SWA ablation, plasticity measurement using GraMa metrics, and robustness analysis across different UTD ratios, all showing consistent improvements."}, "weaknesses": {"value": "1. The linear decay design of SWD (w_i = max(w_min, 1 - age_i/T)) appears somewhat arbitrary without clear theoretical justification for why this particular weighting scheme optimally compensates for the 1/k gradient attenuation. Sensitivity analysis on decay schedules and hyperparameters is insufficient.\n\n2. Recency-based sampling is not novel conceptually, and the paper lacks direct comparisons with recent plasticity-preserving methods (ReDo, ReGraMa, Plasticity Injection). The claimed \"SOTA performance\" is primarily against uniform sampling and PER, making it difficult to assess the true contribution relative to the current state of the art."}, "questions": {"value": "1. Can you provide rigorous analysis showing how the gradient dynamics derived for FQI extend to deep RL algorithms with experience replay and bootstrapping? How does the 1/k decay manifest in TD3/SAC specifically?\n2. How sensitive is SWD to hyperparameters T and w_min? Tables 5-6 show different T values—how were these chosen? Have you systematically compared different decay schedules (exponential, polynomial, etc.)?\n3. Since you claim SWD is orthogonal to existing methods, have you tested combinations with network reset or neuron recycling? Can you provide direct experimental comparisons with ReDo, ReGraMa, and Plasticity Injection to substantiate the SOTA claim?\n\nI will consider increasing the score if the author responds to these questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BSIbMCRqsy", "forum": "5DpzzTPnJZ", "replyto": "5DpzzTPnJZ", "signatures": ["ICLR.cc/2026/Conference/Submission3146/Reviewer_mmd4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3146/Reviewer_mmd4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762246678605, "cdate": 1762246678605, "tmdate": 1762916571011, "mdate": 1762916571011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies plasticity loss in deep RL and attributes it to two mechanisms induced by non-stationarity: (i) NTK Gram rank collapse, and (ii) attenuation of gradient magnitudes that decays as $\\Theta(1/k)$ over training iterations $k$. Motivated by the second mechanism, the authors propose Sample Weight Decay (SWD): linearly down-weight older transitions in the replay buffer so that recent data counteracts the $\\Theta(1/k)$ decay and restores effective gradient scale. SWD is plug-and-play for replay-based algorithms.  \nEmpirically, SWD is added to TD3 and SimBa-SAC on MuJoCo and DMC tasks, with strong gains on DMC Humanoid. A “reverse” ablation (SWA, which up-weights old data) predictably reduces gradient norms and hurts returns, supporting the causal story."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Tight theory to method link: The $\\Theta(1/k)$ gradient attenuation analysis (Theorem 3) cleanly motivates SWD’s linearly decaying replay weights; the SWA “reverse” ablation strengthens causal plausibility.  \n2. Simple and orthogonal: SWD lives at the sampling layer, is easy to add to any replay-based RL algorithm, and should compose with model-level plasticity fixes.  \n3. Compelling results on hard control: Consistent improvements, with standout gains on DMC Humanoid, suggest the effect is meaningful, not a small-n artifact."}, "weaknesses": {"value": "1. Unprobed NTK mechanism: The paper posits NTK rank collapse but provides no spectrum or conditioning measurements; half of the causal story remains speculative.  \n2. Missing SOTA positioning: No head-to-head (or composition) with recent plasticity remedies such as ReGraMa, Plasticity Injection, or ReDo, leaving SWD’s incremental or additive value unclear.  \n3. Practical knobs under-explored: No sensitivity for $T$ (linear decay steps) and $w_{\\min}$; interactions with PER are not clarified (do they stack or conflict?); runtime or throughput impact is unreported."}, "questions": {"value": "1. NTK evidence: Can you track NTK Gram eigenvalues (or condition number) on fixed probe batches across training, ideally under “sequential initialization,” to confirm or quantify rank collapse?  \n2. Comparisons or compositions: How does SWD compare to and combine with ReGraMa or Plasticity Injection on a small DMC subset (including an SWD+X variant) to establish additivity?  \n3. Sensitivity and heuristics: Please add curves for $T$ and $w_{\\min}$. Is a simple heuristic (for example, $T$ as a fraction of buffer capacity or tied to a gradient-norm half-life) a reasonable default for new domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QVzmltipsN", "forum": "5DpzzTPnJZ", "replyto": "5DpzzTPnJZ", "signatures": ["ICLR.cc/2026/Conference/Submission3146/Reviewer_HaDB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3146/Reviewer_HaDB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248412831, "cdate": 1762248412831, "tmdate": 1762916570724, "mdate": 1762916570724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}