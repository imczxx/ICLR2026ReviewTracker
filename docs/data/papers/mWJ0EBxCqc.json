{"id": "mWJ0EBxCqc", "number": 24335, "cdate": 1758355581979, "mdate": 1759896770960, "content": {"title": "See both ways: A bidirectional evaluation of Multimodal Language Models and Human Spontaneous Speech for Image Captioning", "abstract": "Multimodal large language models (MLLMs) have achieved notable success in image captioning, yet systematic comparisons with human-generated references remain underexplored. In this work, we present a novel study on understanding the alignment between captions generated by multimodal models and spontaneous human speech captions. To this end, we introduce a human–machine bidirectional evaluation framework, extending a recently proposed image-caption evaluation metric. This evaluation is performed by comparing crowd-sourced audio-based captions of images with model-generated captions from various MLLMs. Our detailed analysis reveals that, (i) humans are more selective than models in describing specific aspects of the image rather than providing a comprehensive summary, (ii) scores with human reference and model targets are significantly higher those computed with model reference and human response, and (iii) images from specific categories like \"nature'' and \"educational'' evoke more human imagination during the description task, compared to other categories. Together, these findings reveal a clear divergence in human vs. model captioning.", "tldr": "We present a novel study on understanding the alignment between captions generated by multimodal models and  spontaneous human speech captions using a bidirectional evaluation framework.", "keywords": ["Multimodal large language models", "Image captioning", "Human–AI comparison", "Model Evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec65125b783d3d60155feb2292272499b63ef8e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper centers on the visually-grounded speech dataset, Vaani.\nThis dataset consists of images and associated spoken captions collected from people in India speaking various Indic languages.\nThe paper compares the human captions to captions generated by four vision-language models.\nAs a primary analysis tool, the authors propose applying the DCScore metric bidirectionally (using humans as reference and models as reference).\nThe main conclusions reveal that spoken captions are shorter, more casual, and incomplete, whereas model outputs provide more complete descriptions; notably, the latter are also preferred by human raters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- I am sympathetic to the general task of working with realistic visually-grounded speech datasets, and I applaud the effort to work with the challenging Vaani data. This type of data holds significant potential for developing technology for low-resource languages (such as Indic languages).\n- The human evaluation study (Section 4.3) is valuable. Beyond validating the results, the collected ratings could serve as a useful resource for identifying or filtering unreliable human captions within the Vaani dataset, thereby improving its utility for downstream multimodal learning tasks."}, "weaknesses": {"value": "- The paper claims to compare VLMs with human captioning, but this comparison is conducted only in a very specific setting on a single dataset. It is unclear whether the conclusions would generalize to other datasets. For example, as the authors observe, \"several subjects mentioned that human captions at times appeared chopped, grammatically inconsistent or included slang variations.\" I believe this reflects challenges inherent to this dataset (e.g., data collection in the wild, demographics) rather than general characteristics of spoken captions. Note that other spontaneously spoken caption datasets exist, such as Places Audio Captions (Harwath et al., 2016) and its extensions to Hindi (Harwath et al., 2018) and Japanese (Ohishi et al., 2020).\n- Another central claim of the paper is the introduction of a new evaluation framework (the bidirectional DCScore). But this contribution is never compared to existing metrics (BLEU, ROUGE, CIDEr, METEOR, or other model-based metrics). Do we really need this new machinery? Would we reach similar conclusions using other metrics?\n- All in all, I think the true contribution of the paper is an analysis of the Vaani dataset. The insights are valuable, but narrow in scope, as the conclusions rely heavily on the specific characteristics of this dataset.\n\nReferences:\n- Harwath, David, Antonio Torralba, and James Glass. \"Unsupervised learning of spoken language with visual context.\" _NeurIPS_, 2016.\n- Harwath, David, Galen Chuang, and James Glass. \"Vision as an interlingua: Learning multilingual semantic embeddings of untranscribed speech.\" _ICASSP_, 2018.\n- Ohishi, Yasunori, et al. \"Trilingual semantic embeddings of visually grounded speech with self-attention mechanisms.\" _ICASSP_, 2020."}, "questions": {"value": "- As I understand it, the evaluation is conducted in Hindi. Could it also be performed in English? That is, could the human captions be translated from Hindi to English and have the VLMs output in English? Would the results look similar?\n- Related to the above: Doesn't working in Hindi (a less-resourced language than English) introduce errors in the DCScore pipeline (decomposition, matching, verification)?\n- In Figure 4, I would be curious to see the human hallucination plots. Are those results as low as the model hallucinations?\n- The paper might benefit from a discussion of the literature on spoken versus written image descriptions (e.g., van Miltenburg et al., 2018), which explores how modality can influence descriptive style (adverb use, hedging, verbosity).\n- The paper might also benefit from discussing the literature on modeling what humans tend to mention in images (e.g., Berg et al., 2012), which examines how humans prefer certain image elements when describing scenes (although this work focuses on written rather than spoken captions).\n\nReferences:\n- Berg, Alexander C., et al. \"Understanding and predicting importance in images.\" _CVPR_, 2012.\n- Van Miltenburg, Emiel, Ruud Koolen, and Emiel Krahmer. \"Varying image description tasks: spoken versus written descriptions.\" _Workshop on NLP for Similar Languages, Varieties and Dialects_. 2018."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "szARPLFpnQ", "forum": "mWJ0EBxCqc", "replyto": "mWJ0EBxCqc", "signatures": ["ICLR.cc/2026/Conference/Submission24335/Reviewer_u9Nu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24335/Reviewer_u9Nu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644359166, "cdate": 1761644359166, "tmdate": 1762943047348, "mdate": 1762943047348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel bidirectional score (HAR and MAR) for image captioning datasets to evaluate the similarity between human-generated and model-generated captions. Rather than English-only captions, this paper explores more of Hindi speech with large-scale Vanni dataset. The experiments on both close-source and open-source models are comprehensive and thorough."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe motivation of this paper is interesting.\n2.\tThe introduction of spontaneous and image-prompted speech is novel for image caption evaluation. \n3.\tComprehensive experimental results demonstrate the rationality and effectiveness of proposed HAR score and MAR score."}, "weaknesses": {"value": "1.\tOnly DCScore is used to evaluate the model-generated caption and human-generated caption, while some reference-based metrics BLEU, METEOR, and BERTScore are not exploited. \n2.\tMultimodal models typically tend to generate complex clauses with more than 50 words given an image, while humans usually deliver simple sentences. Does this phenomenon affect the final score?\n3.\tSome mistakes in the paper, such as Line 356. \n4.\tThe format of this paper seems different from other ICLR papers."}, "questions": {"value": "Please see the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jpRsrxVPdk", "forum": "mWJ0EBxCqc", "replyto": "mWJ0EBxCqc", "signatures": ["ICLR.cc/2026/Conference/Submission24335/Reviewer_3M5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24335/Reviewer_3M5F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705871651, "cdate": 1761705871651, "tmdate": 1762943046871, "mdate": 1762943046871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the image captioning capabilities of existing MLLMs from the perspective of human-speech transcription. The authors use the Vaani dataset as the source for human-speech transcription of images and analyze image captions generated by MLLMs such as Gemini 2.5 Pro, GPT-4o, Gemma-3-12B, and Llama-4-scout-17b-16e-instruct. The experiments using the evaluation metrics proposed by a recent study show that humans are more selective than models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is clear and well-organized overall.\n2. The attempt to compare human spoken transcriptions with model-generated image captions is interesting."}, "weaknesses": {"value": "1. Why do we need to compare human spoken transcriptions with model-generated image captions? What is the motivation of this study?\n2. Why do we use precision and recall scores? Are these metrics well aligned with the motivation?\n3. Section 3.2 describes how to evaluate generated captions using human transcriptions as references. However, Section 3.3 defines a bidirectional evaluation framework without providing its context. This confuses understanding the following sections."}, "questions": {"value": "(Copied)  \n1. Why do we need to compare human spoken transcriptions with model-generated image captions? What is the motivation of this study?\n2. Why do we use precision and recall scores? Are these metrics well aligned with the motivation?\n3. Why do we use a bidirectional evaluation framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "40lqAUkB6l", "forum": "mWJ0EBxCqc", "replyto": "mWJ0EBxCqc", "signatures": ["ICLR.cc/2026/Conference/Submission24335/Reviewer_Mz2K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24335/Reviewer_Mz2K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975691519, "cdate": 1761975691519, "tmdate": 1762943046667, "mdate": 1762943046667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript aims to investigate the relationship between model-generated captions and human spontaneous speech descriptions, using the Hindi subset of the VAANI dataset. The authors introduce a “bidirectional” evaluation framework, using Human-as-Reference (HAR) and Model-as-Reference (MAR) metrics, derived from DCScore. The study benchmarks several multimodal large language models (MLLMs) such as Gemini 2.5 Pro, GPT-4o, Gemma-3-12B, and LLaMA-4-scout.\n\nWhile the topic may be of mild interest to researchers working on multimodal evaluation or cross-lingual captioning, it does not fit within ICLR’s scope. The paper presents no new learning representations, algorithms, or optimization frameworks. Its contributions are descriptive and incremental at best, with no methodological novelty or theoretical advancement."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Provides an empirical analysis of MLLM-generated captions versus spontaneous human speech in Hindi, an underrepresented language.\n- The bidirectional evaluation idea is straightforward and may help analyze asymmetries between human and model captions.\n- The writing is relatively clear and the experimental setup is well documented.\n- The dataset (VAANI) is interesting from a sociolinguistic and inclusivity standpoint."}, "weaknesses": {"value": "- The work does not propose any new learning paradigm, model, or representation. It focuses entirely on empirical evaluation, making it unsuitable for ICLR. The analysis remains within the scope of applied benchmarking rather than representation learning.\n- The so-called bidirectional evaluation merely swaps the roles of reference and prediction in an existing metric (DCScore). This is a trivial extension that does not constitute a meaningful methodological or conceptual advance.\n- The study provides no analysis of internal representations, cross-modal alignment, or learning dynamics. Thus, it offers no new understanding of how multimodal models learn or encode information.\n- The reliance on GPT-4.1 for decomposition, matching, and verification introduces uncontrollable biases and threatens reproducibility.\n- There is no verification that GPT-4.1 judgments correlate with human assessments beyond anecdotal claims.\n- The human evaluation is too small (22 samples, 16 raters) to support any statistically robust conclusions.\n- Statistical tests (Welch t-tests) are misapplied given the small and non-independent samples.\n- The use of a single dataset (VAANI Hindi subset) restricts generalization.\n- The paper does not justify why DCScore is an appropriate metric for spontaneous speech descriptions.\n- There is no comparison to established captioning metrics (CIDEr, SPICE, CLIPScore, BLIPScore), which weakens the methodological soundness.\n- The conclusions—e.g., humans are more selective while models are more exhaustive—are well known and uninformative. The analysis reiterates established observations without new theoretical framing or insight.\n- The authors draw sweeping claims about “human imagination” and “model verbosity” without sufficient grounding in cognitive or psycholinguistic evidence."}, "questions": {"value": "- How is your proposed bidirectional framework conceptually distinct from computing DCScore in both directions?\n- Did you obtain ethical clearance or participant consent for using the VAANI data?\n- How do you ensure the reproducibility and fairness of GPT-4.1-based evaluation steps, given its proprietary nature?\n- Can your conclusions about “human imagination” and “model selectivity” be supported empirically, or are they speculative?\n- Why are standard captioning metrics omitted from comparison?\n- How does this work advance representation learning, which is the core theme of ICLR?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Use of human speech data:\nThe paper heavily relies on human audio data from the VAANI dataset, yet provides no ethical discussion of data provenance, consent, or participant anonymization. The VAANI dataset includes demographic metadata (gender, district, and linguistic background), raising potential risks for re-identification or misuse. These ethical issues are not acknowledged or mitigated.\n\nLanguage and cultural sensitivity:\nThe work frames Hindi speech data as a “testbed for non-Western contexts” but does not engage with potential biases, representational harms, or power asymmetries in using Indian participants to evaluate Western-developed models. There is no reflection on cross-cultural implications or the fairness of comparing spontaneous human speech to machine-generated text.\n\nDependence on proprietary APIs (e.g., GPT-4.1):\nThe use of closed-source systems for both generation and evaluation compromises reproducibility and raises transparency concerns. The authors do not address how they ensured consistency, consent, or ethical compliance when using commercial systems to process sensitive linguistic data.\n\nAbsence of ethical approval:\nAlthough the study involves human-generated speech data, no mention is made of IRB approval, consent mechanisms, or ethical oversight. This omission is unacceptable for a paper that processes human subject data."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4bdYQys74M", "forum": "mWJ0EBxCqc", "replyto": "mWJ0EBxCqc", "signatures": ["ICLR.cc/2026/Conference/Submission24335/Reviewer_KAyz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24335/Reviewer_KAyz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120649214, "cdate": 1762120649214, "tmdate": 1762943046420, "mdate": 1762943046420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}