{"id": "SBmZWDntTv", "number": 10911, "cdate": 1758184576568, "mdate": 1763119967950, "content": {"title": "FAST AND SCALABLE INVERSION OF CONVOLUTION LAYERS", "abstract": "Data inversion in neural networks allows to map intermediate network variables\nto their input source. Inversion of convolutional layers is not straightforward\nand is often performed approximately by training additional inversion networks.\nApproaching this as a linear operator inversion problem requires extremely large\ncomputational and memory resources, as huge matrices are involved. In this work\nwe present Scalable TRimmed Iterative Projections (STRIP), a fast and sparse\nlinear solver dedicated to the convolutional inversion problem.\nWe take advantage of the neural convolution structure to design a series of very\nfast projections (following the block Kaczmarz paradigm). We prove conditions for\nconvergence for the two-strip case and propose a measure to estimate the rate of\nerror reduction for the general case. In practice, we show that a single pass over\nthe inversion matrix by STRIP can almost perfectly solve the inversion problem.\nOur algorithm is fast, low on memory and can scale to very large matrices. We do\nnot have to store the linear matrix to be inverted, hence can surpass by 3 orders of\nmagnitude linear sparse solvers, such as conjugate gradient. Extensive experiments\ndemonstrate that our method considerably outperforms the best competing solvers\nby both speed and memory footprint. We further show that a single STRIP iteration\nis more accurate than transposed convolutions, motivating the use of such methods\nin U-Net architectures.", "tldr": "A fast sparse linear solver dedicated to the convolutional layer inversion problem", "keywords": ["inversion", "convolution layers", "optimization", "sparse linear solvers", "block Kaczmarz", "transposed convolution"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/edb88391bd8a7d61a0b0a936d4b5bbe861995608.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces STRIP, a method for convolution inversion. The authors formulate convolution as a linear system of equations and employ the block Kaczmarz (BK) algorithm to approximate the solution. The method leverages toeplitz structure of the coefficient matrix in this formulation and proposes an efficient row selection mechanism required by the BK algorithm. Specifically, the authors rearrange the rows of the coefficient matrix so that the convolution kernel repeats in blocks. As a result, the necessary computations for the BK algorithm can be performed on a single block and then reused across the others. The paper analyzes this mechanism in the context of the BK algorithm and compares it against standard iterative baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is overall easy to follow, and the proposed row selection mechanism for the BK algorithm is novel.\n\n- The use of the BK algorithm for convolution inversion is, to my knowledge, new.\n\n- The theoretical analysis is thorough, and the proposed method is shown to be computationally efficient under certain conditions."}, "weaknesses": {"value": "- The method is not well motivated, and its practical application is unclear. The authors argue that it can be used in U-Nets; however, they do not showcase such a use case in their experiments. Therefore, its application appears to be limited to computing convolution inverses in non-learning scenarios, without offering clear practical benefits for machine learning domain.\n\n- The paper’s main contribution, the select_rows function in Algorithm 1, is only described intuitively in the text. It would be clearer and more useful if this mechanism were presented as pseudocode.\n\n- While the mathematical analysis of the method is solid, it does not provide much insight for practical use. In particular, the connection between the results in Theorem 2 and the convolution kernel or its parameters is non-intuitive. What is the key takeaway from Theorem 2, and what are its implications for using the proposed method?\n\n- I am not familiar with existing works on convolution inversion; however, the baselines used in the experiments do not appear to be designed specifically for convolutions. Therefore, it is not surprising that these baselines perform much worse, as shown in Tables 1 and 2. The authors should include related baselines, and if no such methods exist, at least provide a naive version of the BK algorithm: how do the other row arrangements shown in Figure 4 perform?\n\n- The experiment comparing the method with transposed convolution is not clearly described and is very limited.\n\n- The writing also needs improvement and is somewhat messy in its current form. For example, some figures are not referenced in the text, section 5 could be compressed by moving unnecessary details to the appendix, and the numbering of elements (theorem, figures, etc) needs to be fixed."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SLYQdDEaVt", "forum": "SBmZWDntTv", "replyto": "SBmZWDntTv", "signatures": ["ICLR.cc/2026/Conference/Submission10911/Reviewer_NLxa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10911/Reviewer_NLxa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761197587870, "cdate": 1761197587870, "tmdate": 1762922114371, "mdate": 1762922114371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "c600rNDN5I", "forum": "SBmZWDntTv", "replyto": "SBmZWDntTv", "signatures": ["ICLR.cc/2026/Conference/Submission10911/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10911/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119966341, "cdate": 1763119966341, "tmdate": 1763119966341, "mdate": 1763119966341, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel method for inversion of convolutional layers. Their method is augmented variant of the block Kaczmarz algorithm, which exploits the structure of convolutional layers. The authors showcase the application of their method on a variety of different datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The authors showcase that their method significantly improves on computation time as well as PSNR and MSE. The authors evaluated this work on multiple datasets."}, "weaknesses": {"value": "1. Limited applicability. The authors demonstrate their method in a highly artificial and non-practical context. The inversion of a single convolutional layer is not a realistic or meaningful application. The paper does not present any practical use cases, only citing the U-Net architecture, which uses learnable transposed convolutions. The closest real-world scenario involving convolutional inverses is in normalizing flows; however, even there, strict constraints are imposed to ensure the Jacobian determinant can be computed efficiently.\n\n2. Limited experiments. The authors conduct experiments using only 8 or 16 channels, whereas typical architectures employ 256–512 channels. Under such realistic conditions, the proposed method would likely be computationally infeasible.\n\n3. Computation time. Although the authors report that their method takes 1.8275 seconds compared to 0.0033 seconds for an up-convolution (upconv), this makes the approach practically unusable. Such computational overhead would become a major bottleneck in any real-world setting.\n\n4. Incomplete comparison with upconv. The authors claim higher PSNR compared to upconv but provide no details regarding the experimental setup. The main advantage of upconv is that it can be trained—so why not train it? Running even a few training iterations (within the same 2-second budget) could yield better results while also producing a faster inverse operation.\n\n5. Memory experiments. Almost all experiments showcase 0 memory consumption, which is doubtfully true."}, "questions": {"value": "1. Could you comment on the weaknesses that I pointed? \n2. What is the main potential application of the method?\n3. What are the matrix constraints we need to impose in order to have a stable inverse? During training a NN weights of the conv layers shift, and the condition number can change, and the inverse might be infeasible due to aggregate numerical error."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eZrU7FG7ok", "forum": "SBmZWDntTv", "replyto": "SBmZWDntTv", "signatures": ["ICLR.cc/2026/Conference/Submission10911/Reviewer_ah6f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10911/Reviewer_ah6f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783400787, "cdate": 1761783400787, "tmdate": 1762922113875, "mdate": 1762922113875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the inversion of the convolution layer (linear). It differs from the learning-based inverse to derive mathematical analysis on the convolutional layer matrices. The proposed algorithm is related to the classic pseudoinverse, but in a partitioned manner to have fast computation and convergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem scope is made clear, allowing for a fast inverse computation for convolutional layers without additional training.\n\nThe partitioning idea based on pseudoinverses to reduce memory and computation sounds like the right track and is easy to follow.\n\nThe experiments comparing the proposed method with UPCONV and other solvers show an obvious improvement."}, "weaknesses": {"value": "For the target problem, the proposed method seems to apply only to a per-layer linear inverse. Is it true? Or are the compositions across layers considered? If stacking is intended, what breaks or changes (conditioning, caching pseudoinverse of A, stability)?\n\nAs images are mainly used, a natural question is whether the convolution inversion is trivial or ill-conditioned. Conceptually, images may contain information redundancy, and convolution layers are often used to compress and extract information. In contrast, inverse operations need to preserve information without any loss, as it is trivial to trace back to the origin. The paper also argues classic deconvolution is ill-conditioned and mismatched to multi-channel conv. Is it possible to quantify the conditioning of the actual convolution operators for inversion?\n\nWhat is the use case of the analyzed linear inverse, e.g., to replace transposed conv in decoders? While the Related Work mentions that learning an additional inverse model lacks explicitness and is computationally heavy, the reviewer wonders about the performance comparison of the proposed analytical inversion with inverse modeling. Inverse modeling also includes analytically invertible models and numerical inverse approximation. Will the proposed method be more accurate?"}, "questions": {"value": "Please see my concerns and questions in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pFj1T2O2hN", "forum": "SBmZWDntTv", "replyto": "SBmZWDntTv", "signatures": ["ICLR.cc/2026/Conference/Submission10911/Reviewer_4pZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10911/Reviewer_4pZV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989280836, "cdate": 1761989280836, "tmdate": 1762922113132, "mdate": 1762922113132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}