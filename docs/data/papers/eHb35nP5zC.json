{"id": "eHb35nP5zC", "number": 11478, "cdate": 1758200016267, "mdate": 1759897573038, "content": {"title": "ZipperQuant: Bit-Based Inlier–Outlier Disaggregation for 4-Bit LLMs on GPU–CPU", "abstract": "Quantization has become a key technique for reducing the memory footprint and accelerating the inference of large language models (LLMs), especially as modern GPUs provide native INT4 compute units. However, quantizing both weights and activations to low bit-width often causes substantial accuracy degradation, since the limited representational range cannot simultaneously capture common values (\\emph{inliers}) and rare large-magnitude values (\\emph{outliers}). To address this challenge, we propose \\textbf{ZipperQuant}, a novel 4-bit quantization paradigm for LLMs that disaggregates the computation of inliers and outliers across GPU and CPU. A key limitation of naive value-based disaggregation—offloading entire outlier values to the CPU—is that it suffers from the large performance gap between GPUs and CPUs and the high overhead of inter-device data transfer. ZipperQuant instead introduces a bit-based disaggregation strategy. Using smoothing, activation outliers are first absorbed into the weights, which are then split into low-order and high-order components. The GPU executes all inliers together with the low-order bits of outliers in low precision, while only the sparse high-order bits are offloaded to the CPU and multiplied with activations at high precision. These high-order CPU computations are further accelerated by a specialized lookup-table mechanism: since only a small set of bit patterns occurs, their results can be precomputed and reused, replacing costly multiplications with lightweight table lookups and accumulation, while also eliminating dequantization overhead. Extensive experiments demonstrate that ZipperQuant preserves near-FP16 accuracy while achieving up to $3.01\\times$ speedup over a W4A16 baseline on an RTX 4090 with INT4 precision.", "tldr": "4-Bit Post-Training Quantization for Hybrid GPU–CPU LLM Inference", "keywords": ["Quantization", "LLM", "Inference Acceleration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a46a46221428ef4a3640e20ff77dc79ee82a7b55.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces **ZipperQuant**, a 4-bit quantization paradigm for Large Language Models (LLMs). It addresses the significant accuracy degradation that occurs in low-bit quantization due to the challenge of simultaneously representing both high-frequency \"inlier\" values and rare, large-magnitude \"outlier\" values. The proposed solution is a hybrid GPU-CPU approach based on **bit-based disaggregation**. The method first uses smoothing to migrate activation outliers into the weights. Then, these weights are decomposed into low-order and high-order bit components. The GPU executes the dense computations, (all inliers and the low-order bits of outliers) using low-precision INT4 units. Concurrently, the sparse high-order bits are offloaded to the CPU for high-precision computation. To mitigate the performance gap between the GPU and CPU, the authors develop a specialized **lookup-table (LUT) mechanism** that accelerates the CPU-side computation by replacing multiplications with efficient table lookups and accumulation. The primary contributions are this bit-based disaggregation strategy, the co-designed Zipper engine with its LUT-based acceleration, and experimental results showing that the method achieves near-FP16 accuracy for W4A4 quantization, along with significant speedups (up to 3.01x) over W4A16 baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Novel and well-motivated disaggregation strategy**\n    * The paper clearly articulates the problem with naive *value-based* offloading, citing the 10-100x GPU-CPU performance gap and PCIe transfer bottlenecks as critical flaws. This provides a strong motivation for a new approach.\n    * The core idea of *bit-based* disaggregation is novel. It intelligently separates the computation, keeping the dense, low-order bits on the GPU to leverage low-precision hardware (INT4 GEMM) while moving only the sparse, high-order bits to the CPU for high-precision handling. This is a clever and practical way to balance precision requirements with computational efficiency.\n    * This design choice is empirically justified by analysis showing that the high-order bits of weights are indeed highly sparse across layers (Figure 3b), making them suitable for the proposed sparse computation on the CPU.\n    * The proposal is not purely theoretical; it includes a \"Zipper engine\" and a detailed LUT-based workflow (Figure 4b), demonstrating a clear and practical path to implementation.\n\n* **Strong empirical results for W4A4 quantization**\n    * The primary accuracy results in Table 1 are compelling.ZipperQuant (W4A4) consistently and significantly outperforms other W4A4 methods, including RTN, SmoothQuant, QuaRot, and SpinQuant, in terms of both WikiText-2 perplexity and average zero-shot accuracy.\n    * In many cases, the W4A4 ZipperQuant results are competitive with or even exceed W4A8 and W4A16 baselines (Table 1). For instance, on LLaMA-3.2 3B, ZipperQuant (W4A4) achieves a 60.6 average accuracy, closing much of the gap to the W16A16 baseline (66.2) and outperforming W4A4 SpinQuant (57.5).\n    * The method is shown to be complementary to other SOTA techniques. Table 2 demonstrates that integrating the Zipper engine with SpinQuant (\"Spin+Zipper\") further improves accuracy over both methods individually. This suggests the bit-based disaggregation concept has broad applicability.\n\n* **Comprehensive performance analysis and ablation studies**\n    * The paper provides a detailed end-to-end speedup analysis for both prefill and decoding (Figure 5). The reported speedups over a W4A16 baseline (up to 3.01x prefill, 1.90x decoding) are significant and demonstrate the practical benefit of enabling W4A4 computation without sacrificing accuracy.\n    * The ablation study on the CPU computation strategy (Figure 7) is essential. It provides clear evidence that a \"Naive GEMM\" implementation on the CPU is prohibitively slow (up to 101x slowdown) and validates that the proposed sparse LUT mechanism is critical for achieving the reported performance.\n    * The accuracy ablations in Table 3 effectively justify the complete pipeline. They show that \"Naive 4-bits\" and \"Smoothing\" alone are insufficient, and critically, that applying ZipperQuant *without* smoothing also fails. This confirms that the combination of smoothing and bit-disaggregation is necessary.\n    * The inclusion of memory footprint analysis (Figure 6) confirms the practical memory-saving benefits of the W4A4 approach compared to W4A16 and FP16 models."}, "weaknesses": {"value": "* **Clarity and correctness of mathematical formulations**\n    * The proof for Theorem 4.3 (ZipperQuant error bound) in Appendix B.4 appears to be inconsistent with the main text. The proof steps use $Q_{k_1}(\\hat{W})$ and $Q_{k_2}(\\hat{W})$, but the theorem statement in Equation 8 and Equation 11 relies on $U_{k_1}(\\hat{W})$ and $2^{k_1}Q_{k_2}(\\hat{W})$. Furthermore, the first line of the proof defines the error relative to a decomposition that does not seem to match the one proposed in Equation 7. This discrepancy makes it difficult to verify the correctness of the theoretical error bound.\n    * The notation in Equation 6 is ambiguous. It defines $\\aleph_{K}(\\hat{W}) = s_K^W Q_K^W = \\dots = U_{k_1}(\\hat{W}) + 2^{k_1}Q_{k_2}(\\hat{W})$. This structure implies that both the low-order unsigned component $U_{k_1}(\\hat{W})$ and the high-order signed component $Q_{k_2}(\\hat{W})$ are derived using the *same* scaling factor $s_K^W$ (from the K-bit quantization). This seems counterintuitive, and it's not clear how $U_{k_1}(\\hat{W})$ is defined. This ambiguity obscures the precise mathematical definition of the decomposition.\n    * The notation for quantized activations in the proof of Theorem 4.3 is confusing. The proof introduces $Q_{k_1}(\\hat{X})$ and even $Q_{k_2}(\\hat{X})$, but the (k1, k2) bit-slicing scheme is defined for weights, not activations. This makes the proof difficult to follow and its validity unclear.\n\n* **Ambiguity in implementation details**\n    * The paper states the bit split is $K=6$, $k_1=4$ (GPU), and $k_2=2$ (CPU). However, Figure 3c illustrates the split as \"Highest INT1\" (CPU), \"2nd Highest INT1\" (CPU), and \"UINT4\" (GPU). It is not clear how these two sparse 1-bit components are combined and processed as the single $k_2=2$ bit signed term ($Q_{k_2}^W$) described in Equations 6-7.\n    * The \"sign restoration\" mechanism is critical but confusingly explained. The text mentions \"subtracting 16 at those positions on the GPU\" for negative entries, which is non-trivial for a UINT4 representation. The relationship between this step and the \"dividing GPU weights by 2\" step mentioned immediately after is also unclear.\n    * The LUT mechanism's performance (Figure 4b) depends on the group size $g$ (example $g=3$ is given). This hyperparameter, which dictates the LUT size ($2^g-1$) and computational trade-off, is not specified for the main experiments nor is its impact ablated.\n    * Equation 7 implies a potential overhead: the GPU computes $Q_{k_1}(\\hat{X})U_{k_1}(\\hat{W})$ (using quantized activations) while the CPU computes $\\hat{X} \\cdot 2^{k_1}Q_{k_2}(\\hat{W})$ (using high-precision activations $\\hat{X}$). This suggests activations $\\hat{X}$ must be quantized for the GPU path *and* simultaneously sent in high precision to the CPU for the LUT. This cost is not explicitly analyzed.\n\n* **Limited performance comparisons and sensitivity analysis**\n    * The main speedup comparisons (Figure 5) are against W16A16 and W4A16 (AWQ) baselines. While this is useful, the paper's core contribution is a W4A4 method. Direct, quantitative speedup comparisons against the other SOTA W4A4 methods (QuaRot, SpinQuant) are missing. The paper notes their \"online conversion overhead\" but does not provide data to assess the practical performance trade-off.\n    * The latency analysis in Figure 4a is shown for \"QKV projection\". It is not guaranteed that this favorable latency holds for the much larger FFN layers. The paper does not provide a layer-wise latency breakdown to confirm that the CPU path does not become a bottleneck elsewhere.\n    * The method's efficiency hinges on the assumption of high sparsity for high-order bits. Figure 3b shows this for K-projection in one model. This assumption needs to be validated more broadly (e.g., for FFN layers, other models). The paper lacks a sensitivity analysis showing how performance (latency, data transfer) degrades if this sparsity assumption weakens (e.g., if sparsity drops from >99.5% to 95% or 90%).\n    * All experiments are on an RTX 4090. The performance of a hybrid GPU-CPU system is highly dependent on the specific CPU, GPU, and interconnect (PCIe). The conclusions drawn may not generalize to server-grade systems (e.g., H100 with NVLink) or on-device platforms with integrated memory, which have very different performance characteristics."}, "questions": {"value": "* **Regarding mathematical formulations:**\n    * Could the authors please clarify the derivation in Appendix B.4? Specifically, how does the proof correctly map to the decomposition in Equation 7 and the error bound in Equation 8, given the apparent mismatches in terms?\n    * Could the authors please elaborate on the scaling factors in Equation 6? Are the low-order $U_{k_1}(\\hat{W})$ and high-order $Q_{k_2}(\\hat{W})$ components truly scaled by the same factor $s_K^W$? If not, could the notation be corrected to reflect the actual implementation?\n    * Could the authors please review the notation for activations in the proof in Appendix B.4 (e.g., $Q_{k_2}(\\hat{X})$) and clarify its meaning, as the k1/k2 split is defined for weights?\n\n* **Regarding implementation details:**\n    * Could the authors please explain how the $k_2=2$ bits for the CPU are implemented? Specifically, how are the \"Highest INT1\" and \"2nd Highest INT1\" (from Figure 3c) combined and processed to match the single $Q_{k_2}(\\hat{W})$ term in Equation 7?\n    * Could the authors provide a more concrete explanation or pseudo-code for the \"sign restoration\" mechanism? How is \"subtracting 16\" implemented on a UINT4 GPU value, and what is its precise interaction with the \"dividing by 2\" step?\n    * What was the group size $g$ used for the LUT in the main experiments? Could the authors provide a sensitivity analysis on how $g$ affects end-to-end latency?\n    * Does the ZipperQuant pipeline indeed require both quantizing $\\hat{X}$ for the GPU and sending the full-precision $\\hat{X}$ to the CPU, as implied by Equation 7? If so, what is the measured overhead of this dual requirement?\n\n* **Regarding performance and bottlenecks:**\n    * Could the authors provide a direct performance (latency/throughput) comparison against other W4A4 methods like QuaRot and SpinQuant, even if it requires discussing the impact of their respective overheads?\n    * Does the low latency of the CPU path, shown for QKV projections in Figure 4a, also hold for the larger FFN layers? Can the authors confirm the CPU offload does not become a bottleneck in any part of the models tested?\n    * How robust is the method's performance to the high-order bit sparsity assumption? Does this high sparsity (Figure 3b) hold for FFN layers as well? What is the performance impact if sparsity decreases?\n    * Do the authors have any insights or preliminary data on how this GPU-CPU disaggregation strategy might perform on different hardware architectures, such as server-grade GPUs or edge devices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TDrZOj2ikK", "forum": "eHb35nP5zC", "replyto": "eHb35nP5zC", "signatures": ["ICLR.cc/2026/Conference/Submission11478/Reviewer_QWVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11478/Reviewer_QWVN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984840606, "cdate": 1761984840606, "tmdate": 1762922585570, "mdate": 1762922585570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new quantization method to achieve accurate, efficient LLM inference via finer-grained bit-level disaggregation across GPU and CPU. A LUT-based approach is further proposed to accelerate the high-order bit computation on CPU. Experimental results show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ finer-grained bit-level disaggregation across GPU and CPU to achieve accurate, efficient LLM inference\n+ LUT-based approach is further proposed to accelerate the high-order bit computation on CPU"}, "weaknesses": {"value": "- accuracy comparison seems unfair as ZipperQuant does 6-bit quantization rather than 4-bit one\n- no direct comparison with existing disaggregation approaches"}, "questions": {"value": "It looks ZipperQuant does 6-bit quantization. Is it fair to compare with 4-bit quantization for accuracy? For latency, how does ZipperQuant compare with existing disaggregation-based approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gU16xcQrK0", "forum": "eHb35nP5zC", "replyto": "eHb35nP5zC", "signatures": ["ICLR.cc/2026/Conference/Submission11478/Reviewer_kTRi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11478/Reviewer_kTRi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997897962, "cdate": 1761997897962, "tmdate": 1762922585085, "mdate": 1762922585085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ZipperQuant, a post-training 4-bit quantization and execution scheme that splits work across GPU and CPU. After a smoothing step that “absorbs” activation outliers into the weights, each weight tensor is decomposed into low-order and high-order bit components. The GPU executes inliers and the low-order bits with INT4 kernels, while only the sparse high-order bits are offloaded to the CPU and applied at higher precision. To reduce CPU cost, the authors precompute a lookup-table (LUT) over the small set of high-order bit patterns and replace multiplications with table lookups plus accumulation. Experiments on an RTX 4090 claim near-FP16 accuracy and speedups over a W4A16 baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear systems idea: The bit-level disaggregation is an interesting twist on prior value-based outlier handling, with a plausible path to better overlap and lower transfer volume.\n\n2. Hybrid deployment perspective: Embraces real-world heterogeneity (CPU+GPU) rather than assuming an all-GPU pipeline; the paper surfaces important engineering considerations (transfer, overlap, cache effects)."}, "weaknesses": {"value": "1.  The core contribution is mainly a systems design/implementation choice (bit-wise split + LUT), while the “smoothing” component appears conceptually close to prior activation-aware methods (e.g., SmoothQuant-style weight–activation rebalancing). The paper does not articulate a clearly new quantization algorithm beyond the execution strategy.\n\n2. The headline comparison is against a W4A16 baseline, which may be weak. It’s unclear how ZipperQuant fares against strong end-to-end stacks with optimized INT4 paths and outlier handling (e.g., TensorRT-LLM, vLLM with optimized kernels, AWQ/OmniQuant/ZeroQuant variants).\n\n3. The claimed gains are reported on RTX 4090; the approach’s benefit likely depends on GPU:CPU FLOP ratio, memory bandwidth, cache sizes, and interconnect (PCIe 4/5, NVLink). There is no systematic study showing that the speed-accuracy trade-off is robust across different CPU classes (e.g., AVX2 vs AVX-512/AMX), GPUs, or buses."}, "questions": {"value": "How does ZipperQuant’s end-to-end throughput/latency compare with TensorRT-LLM, highly tuned vLLM INT4 kernels, and recent activation-aware 4-bit baselines (e.g., AWQ, OmniQuant, SmoothQuant)? Please include same hardware, same batch/seq settings, and decode vs prefill regimes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ra3K2QrMiL", "forum": "eHb35nP5zC", "replyto": "eHb35nP5zC", "signatures": ["ICLR.cc/2026/Conference/Submission11478/Reviewer_PfpY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11478/Reviewer_PfpY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998452215, "cdate": 1761998452215, "tmdate": 1762922584525, "mdate": 1762922584525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ZipperQuant, a hybrid GPU–CPU execution scheme for 4-bit PTQ LLM inference.\nThe method splits  modelsweights per bit-plane after smoothing, low-order bits → GPU int4 path (dense, fast) while sparse high-order bits → CPU LUT path (lookup w/o multiply)\nso the “inlier” part of outliers stays on GPU, and only the sparse “heavy” bits get offloaded to CPU."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a real deployment bottleneck by leveraging the sparsity of high-order bit planes.\n\nThe proposed method demonstrates improvements over baselines"}, "weaknesses": {"value": "1. Wrong reference year: Some method has wrong year, like AWQ is a 2023 method\n2. Weak baselines: The paper seems to compare with the result of SpinQuant without Hadamard matrix, which is much weaker than their version using Hadamard. Also, there are stronger baselines like FlatQuant that need to be compared. Additionally, why do the author compare the speed up and memory with only AWQ, which is kinda old?\n\n3. Technical novelty: The novelty feel weak to me, as they seems to be engineering trick to improve efficiency, but the results actually show improvement, so what is the key behind the performance improvement over spinQuant?"}, "questions": {"value": "please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M4eszWvYrg", "forum": "eHb35nP5zC", "replyto": "eHb35nP5zC", "signatures": ["ICLR.cc/2026/Conference/Submission11478/Reviewer_xBns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11478/Reviewer_xBns"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183026740, "cdate": 1762183026740, "tmdate": 1762922583775, "mdate": 1762922583775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to represent the larger bit-width quantization with a smaller bit-width dense tensor container the lower-bits and a sparse tensor containing the higher-bits (i.e., 4+2 in its experiments). It illustrates that larger bit-width is important for quantization with a set of equations. The higher-bits tensor is computed on the GPU, and the lower-bit sparse tensor is computed on the CPU with LUT method (with necessary D2H communication of the activation) and then transferred back to GPU for reduction. It shows better accuracy with its W4A4 (actually should be W6A4?) method than the rotation-based works, and better system efficiency than FP16 and AWQ baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It focuses on the important problem of LLM serving, the post-training quantization.\n2. Splitting the tensor into a dense one and a sparse one at bit-level seems new."}, "weaknesses": {"value": "1. The decision of using 6-bits for weight and 4-bits for activation is not well discussed/evaluated. The advantage of \"4-bit on GPU and 2-bit on CPU\" than \"the full 6-bits on GPU\" is not clear (e.g., Blackwell supports the 6-bits datatype).\n2. Lacks the comparison of some recent works.\n3. The system overhead is not clear.\n4. Some statement is not accurate."}, "questions": {"value": "1. Why use larger bits for weight and smaller bits for activation, i.e., 4+2 for weight and 4 for activation? Given that LLM inference is easily bound by the memory access of large weight tensor (QServe [1]), the state-of-the-art solution is to use smaller weights and slightly larger activation as for the bit-width. Even with smoothing, the loss coming from the 4-bit activation can be too large.\n2. Given that this paper uses 6-bits for the weight quantization, 4-bit on the GPU and 2-bits on the CPU, why not using the Blackwell FP6 datatype directly? It only saves 1/3 GPU memory of using 4-bits when compared to the full 6-bits. The current model serving solutions are not bound by this portion of GPU memory (especially for the small models evaluated in this paper). Besides, the complexity of 4+2 has introduced a lot of CPU and communication overhead.\n3. It could be better to compare this work with more related works (e.g., QServe[1] and more mixed-precision quantization works). Besides, it claims W4, but it is actually W6 in the evaluation. This can be misleading.\n4. As for the system efficiency, it should have a comparison with the state-of-the-art W4A4 and W4A8 systems, rather than the relatively old system of AWQ and the full precision.\n5. The system overhead is not studied. What is the overhead of the H2D communication of the activation and D2H communication of the partial results? How long is the W4A4 GPU kernel and how long is the end-2-end W6A4 execution including the CPU and communication part? Without this breakdown, it is hard to believe this system is practical due to the potential overhead.\n6. The equations in section 4.1 are good! But they seem only used for the illustration that the bit-width is important. This is not a problem though.\n7. The statement \"Q_X and Q_W typically adopt the same bit-width\" in L157 is not accurate, e.g., QServe uses W4A8.\n8. It would be good to add space between the columns of Table 1. It is hard to differ different columns for its row-1.\n9. It seems have not present the ratio of higher bits and lower bits in the evaluated models and tasks. What is the ratio of the higher bits in each portion of the weight?\n\n[1] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p8T1jCxkEn", "forum": "eHb35nP5zC", "replyto": "eHb35nP5zC", "signatures": ["ICLR.cc/2026/Conference/Submission11478/Reviewer_ehPS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11478/Reviewer_ehPS"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241645847, "cdate": 1762241645847, "tmdate": 1762922583407, "mdate": 1762922583407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}