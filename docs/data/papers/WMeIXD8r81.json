{"id": "WMeIXD8r81", "number": 19853, "cdate": 1758300024511, "mdate": 1759897015750, "content": {"title": "Learning to Remember, Learn, and Forget in Attention-Based Models", "abstract": "The ability to perform learning during inference,  i.e. in-context learning (ICL) is a core feature of self-attention in transformers. ICL acts like an online associative memory and is believed to underpin transformers' capabilities in complex sequence processing tasks. \nIn some cases, ICL was shown to simulate online gradient descent of a local loss function on an input sequence.\nIn this work, we view ICL as a continual learning problem that may suffer from memory interference and requires a solution to a plasticity--stability dilemma. \nWe examine here the memory consolidation properties of ICL and propose a Bayesian continual learning framework to solve this dilemma, leading to a new attention model. \nOur framework builds on the idea of metaplasticity in neuroscience, where the level of plasticity of each synapse is tied to an importance measure grounded by a Bayesian prior distribution capturing previously learned knowledge.\nOur approach explains several gated linear attention models in the literature, identifying the respective assumptions from a Bayesian learning perspective.\nFurthermore, our Bayesian continual learning approach provides a principled approach to forgetting, enabling the design of attention layers with a desired memory horizon.\nOur experiments achieve competitive performances on  synthetic benchmarks. Additionally, we experiment on several commonsense reasoning benchmarks where small models benefit from consolidated synapses, outperforming strong baseline like Gated Delta Networks.", "tldr": "A Bayesian continual-learning framework for attention based model balances plasticity and stability, to achieve superior in-context learning.", "keywords": ["Attention Based Model", "In-Context Learning", "Continual Learning", "Bayesian Inference", "Metaplasticity."], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5bec75c1a2f18665c07efc0ac4bdc88cdf46c712.pdf", "supplementary_material": "/attachment/189b384250843761ee43607a313ca5a79d5b7a9d.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Palimpsa, a new attention mechanism that treats memory updates as Bayesian inference. Inspired by metaplasticity in neuroscience, Palimpsa maintains both a memory mean and an uncertainty parameter for each attention state. The paper hypothesizes that this allows the model to adaptively balance learning new information, retaining important past content, and forgetting outdated inputs. The authors derive closed-form update rules from a variational free-energy objective, unifying prior models like Mamba2 and MesaNet as special cases in this framework. Experiments on synthetic memory benchmarks and language tasks show that Palimpsa achieves comparable performance to previous baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strength is that the paper provides a compelling Bayesian reinterpretation of attention and state-space models, showing that many recent architectures (e.g., Gated DeltaNet, Mamba2, MesaNet) can be viewed as special cases of a single variational inference framework. This theoretical unification helps clarify connections between disparate model classes and gives principled meaning to heuristic gating mechanisms used in prior work."}, "weaknesses": {"value": "1. Some parts of the paper are not well-presented. In Section 2.2, β is not defined until several paragraphs later and initially reads as if it comes directly from the input. The notation of placing diag(β) in a subscript is also unfamiliar to me and likely to other readers. It was not immediately clear why or how we aim to maximize the objective on line 245 — if I understand correctly, the goal is to update S to accommodate the new key–value pair (k, v). On line 265, what is x and how is it linked to k and v? The variational approximation introduced on line 273 also lacks context; at minimum, the authors should mention that p is intractable. I suggest clarifying these points, as they appear to form the core of the proposed method.\n\n2. The model’s performance does not fully convince me that Palimpsa is a stronger method. On the MAD benchmark, Palimpsa performs worse than Gated Linear Attention and MesaNet. For language modeling, the set of baselines appears somewhat limited. Moreover, the performance gain reverses for larger networks, as the authors themselves acknowledge.\n\n3. The core idea of Palimpsa is to adaptively update the attention state based on the current input. However, it remains unclear how memory and forgetting actually operate in practice. Including visualizations, even on toy tasks, could help illustrate these dynamics. The paper also lacks ablation studies, making it difficult to assess under what conditions the proposed update rule is effective.\n\n4. The link to metaplasticity in neuroscience is mainly conceptual. Context-modulated plasticity is not new and has previously been explored in recurrent neural networks [1, 2].\n\n[1] Backpropamine: Training self-modifying neural networks with differentiable neuromodulated plasticity\n[2] Hebbian and Gradient-based Plasticity Enables Robust Memory and Rapid Learning in RNNs"}, "questions": {"value": "1. How important is the adaptive forgetting and momorization mechanism (metaplsticity) in Palimpsa? If we just use a constant $\\gamma$, or use a constant learnable vector $\\beta$ that does not change for each step $t$, how does that affect empirical performance?\n\n2. While the comparisons in Table 1 is very interesting, it is not clear to me what are the weaknesses of previous models that Palimpsa attempt to address.\n\n3. The authors mention that Palimpsa uses the simplification that uses a diagonized stability matrix, could the authors discuss the implication for such simplification?\n\n4. How does Palimpsa decide when to forget versus consolidate? Is the forgetting gate driven purely by input statistics, or does it evolve based on longer-term task structure?\n\n5. How would you interpret the inconsistent results on different MAD subtasks? For example, why would Palimpsa less well on Fuzzy recall?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wcjcvL7cOT", "forum": "WMeIXD8r81", "replyto": "WMeIXD8r81", "signatures": ["ICLR.cc/2026/Conference/Submission19853/Reviewer_HigJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19853/Reviewer_HigJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509175401, "cdate": 1761509175401, "tmdate": 1762932026455, "mdate": 1762932026455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors frame in-context learning as an online continual learning problem – with the concomitant tension between retaining past and absorbing new information – and propose a Bayesian framework with which to understand the trade-off. They apply the framework to several linear-gated transformers and use it to derive their own metaplasticity-inspired layer. Results on language modeling, commonsense reasoning, and the MAD benchmark are presented. The authors state that they have implemented custom kernels to speed up the computation of their layer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The attempt to formally unify linear-gated transformers is admirable (see the Appendix) and a common area of recent work. This reviewer finds the framing of in-context learning as a continual learning problem compelling. The quality of the writing is above average and the arguments are typically clear."}, "weaknesses": {"value": "This reviewer believes the evaluation and results of the authors’ layer, Palimpsa, could be much stronger. The language modeling and commonsense reasoning evaluation only compares to Gated DeltaNet. We have observed that results on those tasks tend to be much better with Gated DeltaNet-H2. While the authors give DeltaNet and Gated DeltaNet special treatment in the Appendix, that should not preclude the use of a different model – one closer to state-of-the-art – in their evaluation. The results on the synthetic MAD benchmark place Palimpsa in the middle of the pack (itself not necessarily a problem) and, sadly, the results on Fuzzy Recall and Memorize – tasks on which one would expect Palimpsa to excel due to the metaplasticity-inspired design of the layer – are underwhelming. I would find this paper more compelling if the evaluation contained, say, a substantial amount of error analysis or other phenomenology that would demonstrate the failure modes of Palimpsa. Such analysis can bring substantial value to a report when the results are not state of the art and, especially, when the results are not state of the art on tasks on which the model should be quite superior.\n\nFootnote 2 states that the code (or the link thereto) has been omitted to ensure double-blind review. This reviewer reminds the authors that sites such as https://anonymous.4open.science/ support anonymous repositories.\n\nThere are a number of strange paragraph breaks in the paper. I would encourage the authors to track down the cause."}, "questions": {"value": "This reviewer invites the authors to respond to any of the points I have raised in the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E38hM5pKZQ", "forum": "WMeIXD8r81", "replyto": "WMeIXD8r81", "signatures": ["ICLR.cc/2026/Conference/Submission19853/Reviewer_jHq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19853/Reviewer_jHq3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699697300, "cdate": 1761699697300, "tmdate": 1762932026043, "mdate": 1762932026043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Palimpsa, a dual‑state, fixed‑memory token mixer that casts self‑attention as online Bayesian inference with metaplasticity. Concretely, it maintains a per‑state Gaussian posterior over the attention memory with a diagonal precision vector  I_t, yielding closed‑form, per‑token updates that gate input, stability, and forgetting; forgetting is tied to input, so no forgetting results in no input. The authors argue this framework unifies several gated models and show Mamba2 as an asymptotic special case under strong forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Principled derivation. The free‑energy view leads to exact, closed‑form update rules (Eq. 3) for both the mean and the per‑synapse importance, providing a clean, test‑time learning interpretation of attention that goes beyond heuristic gating.\n2. Unifying lens on gated models. Table 1 methodically maps several models into the same objective, and shows Mamba2 as a limiting case. This is an interesting contribution."}, "weaknesses": {"value": "1. Scaling story is mixed. At 760M, Palimpsa underperforms Gated DeltaNet on the averaged suite, weakening the claim that metaplastic updates are broadly advantageous; an analysis isolating why performance flips with scale is needed.\n2. Fairness of comparisons needs tightening. Table 5 shows Palimpsa uses more layers yet half state size to keep state budgets comparable. Please provide compute‑matched and parameter‑matched comparisons (same layers/expansions).\n3. Limited long‑context evaluation. Claims about a tunable memory horizon would be more convincing with established long‑range tasks.\n4. There is a lack of discussion on TTT/Titans models. The emperical baselines are borrowed from Titans, so why these baselines are missing?"}, "questions": {"value": "See weaknesses,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DAF2aG9b5O", "forum": "WMeIXD8r81", "replyto": "WMeIXD8r81", "signatures": ["ICLR.cc/2026/Conference/Submission19853/Reviewer_dUCE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19853/Reviewer_dUCE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893486822, "cdate": 1761893486822, "tmdate": 1762932025717, "mdate": 1762932025717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes what could be considered an extension of LongHorn or MesaNet, adding a Bayesian framework inspired by Bonnet et al. that explicitly allows forgetting, in an input-dependent way. It proposes a new token mixer in state space models, addressing in-context catastrophic forgetting and catastrophic remembering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. It's interesting to see a Bayesian scheme for meta-plasticity, combining ideas from LongHorn and Mesanet.\n2. The authors attempt to structure the field a bit, e.g. by showing methods like MAMBA2 are a special case of their work. Table 1 is interesting in that respect, but could be made clearer.\n3. They achieve competitive results, especially for 'smaller' models."}, "weaknesses": {"value": "1. The paper needs significant restructuring and rewriting, in my opinion. \n- Half of the main paper is spent on contextualization and related work. They start from a too broad context of self-attention and in-context learning, while actually their work applies to state space models. Stating that explicitly from the start, would have made the actual contributions a lot clearer. \n- A lot of the relevant content of the paper, related to the actual method, is moved to the supplementary material. \n- Supplementary material is not well structured. It feels like the authors abuse the supplementary material to bypass the page limit. Text in the main paper is too verbose and not focusing on the essence. At the same time, technical descriptions in the main paper are vague and handwavy, with important details left out. Instead of pointing to a specific section in the appendix where those details can be found, it's left to the reader to go dig in the supplemental material to try and find them. \nThis makes it hard for the reader to really grasp the technical details. \n\n2. Poor analysis of the proposed method\nThe experimental results are limited to 2 tables only, basically comparing the method as a whole against competing methods, once on the MAD benchmark and once on the fineweb-edu dataset. \n- There is no ablation study.\n- There is no sensitivity analysis on hyperparameters. \n- There are no further analyses trying to understand the behavior of the model. \nIn particular, I would be interested to know when the model decides to forget. Even if only anecdotal, some examples or visualizations could give some insights. In that context, I've always found it weird the forgetting and input gates depend on the input, not on a combination of input and state. \nAlso interesting would be some insights in the distribution of the gate values. \n\n3. Difficult to reproduce.\nI would have a really hard time reproducing the method, based on the information provided (even including the supplementary materials). There's no mention in the paper about the authors planning to make code or pretrained models publicly available."}, "questions": {"value": "1. The paper states some explicit differences of the proposed method compared to MesaNet (end of related work section). Which of these differences are most important to improve the results ? Is it the Bayesian framework per se ? Or the non-scalar beta ? Or the linking of the input gate to the forgetting gate ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5xlZB8o2jd", "forum": "WMeIXD8r81", "replyto": "WMeIXD8r81", "signatures": ["ICLR.cc/2026/Conference/Submission19853/Reviewer_etLB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19853/Reviewer_etLB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998363300, "cdate": 1761998363300, "tmdate": 1762932024948, "mdate": 1762932024948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}