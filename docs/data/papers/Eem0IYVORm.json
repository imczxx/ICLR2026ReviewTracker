{"id": "Eem0IYVORm", "number": 14630, "cdate": 1758240524015, "mdate": 1763737357388, "content": {"title": "Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation", "abstract": "Reasoning distillation, a cost-effective approach for enhancing student model performance, has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. \nHowever, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models.\nTo analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into four categories: (i) teacher-originated actions, (ii) student-originated actions, (iii) pre-existing actions in both models not enhanced by distillation, and (iv) pre-existing actions boosted through distillation. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics (e.g., selecting data most aligned with the student's original distribution), our method directly compares teacher–student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models (Deepseek-R1-671B, QwQ-32B, GPT-OSS-120B) and diverse student models (Qwen2.5-7B-Instruct, Qwen4-4B-Base, Qwen3-8B-Base, Qwen3-4B-Instruct-2507). The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing, along with our insights into reasoning distillation, with the community.", "tldr": "", "keywords": ["Reasoning Distillation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fd23b24d5dfb360fc45c97085c41c925947fda6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission studies reasoning distillation, where a smaller student model is fine-tuned on reasoning paths generated by a larger teacher model. It first proposes a method called Reasoning Distillation Provenance Tracing to categorize each reasoning step (\"action\") produced by a distilled model, based on the probabilities assigned to it by the distilled model, teacher model, and student model before distillation. The aim is to determine whether a reasoning step is more likely to originate from the teacher model, from the student before distillation, or be \"boosted\" by distillation even though the teacher does not assign it high probability. The authors present a study using Reasoning Distillation Provenance Tracing on open-source distilled models and reasoning datasets. The second part of the submission builds on the first by selecting examples for distillation where the probability of the example's actions being teacher-generated is higher. Experiments show that this data selection strategy outperforms random selection and a baseline called GRAPE in terms of the distilled model's performance on reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This work aims to go beyond performance on reasoning benchmarks to understand the effects that distillation has on a granular, step-by-step level. The proposed categorization of actions appears simple and reasonable.\n- The work also aims to close the loop by using the categorization to select more effective examples for distillation, with encouraging results.\n- An effort is made to experiment on multiple student models, teacher models, distillation datasets, and evaluation datasets."}, "weaknesses": {"value": "1. The paper claims that distilled models reproduce teacher-originated actions in \"entirely new contexts,\" and the newness of test contexts is emphasized throughout. However, this newness is not substantiated. Moreover, newness as a binary notion misses the fact that similarity to training contexts lies on a spectrum. I think it would be better to quantify somehow the dissimilarity of test contexts from training contexts, at both the sample and dataset levels, and analyze results in terms of this dissimilarity.\n1. Figures are unclear, to the point of obscuring evidence for main claims of the paper (see especially the first point below).\n    - Figures 3 and 4: The solid and dashed lines are hard to distinguish. This makes it hard to be convinced of finding (3) in Section 3.3 in particular, i.e., the correlation between Teacher Sentences and correctness. Can this correlation be quantified so that the evidence is not just visual comparison of two curves? It is also hard to understand finding (2) in Section 3.3 from the current figures.\n    - Legends, tick labels, etc. are too small: This matters especially in Figures 3 and 4 because the line colors and line types are not described in the text.\n    - Figure 6: The x-axis is not labelled (and the tick labels are very small). This does not help in explaining the selection of $\\beta$ (see next point).\n    - Figure 7: Since the estimates are statistically reliable only for the first 800 action indices, a suggestion is to restrict the plot to this range so that it can be clearer.\n1. The selection of threshold $\\beta$ is described only in Appendix A.2. Given that $\\beta$ is needed in both Section 3 (provenance tracing) and 4 (data selection), and given the claim (in bold) about the selection of $\\beta$ (lines 442-445), this description should be in the main paper.\n    - As a secondary point, the selection of threshold $\\alpha$ is also unclear, specifically what is meant by \"manual binary classification\" (lines 243-244). What is the classification task? Why not set $\\alpha$ larger than 0.1 if \"the average probability difference for Shared\nSentence was found to be 0.097\"?\n1. Main results for data selection: With 3 teacher models x 4 student models x 2 data sources, there are 24 combinations. However, the main Table 1 shows only 4 combinations, and I did not find more in the appendix. There is thus no assurance that the combinations in Table 1 were not cherry-picked.\n1. Some sentences are unclear or should otherwise be improved. Below I list only ones where I am quite unsure of what is meant. That said, I consider this point to be less important than the other presentation issues above.\n    - Lines 155-156, \"minimal sentence level\": Why is it \"minimal\"? It is also not clear at this point that each reasoning step corresponds to a sentence.\n    - Lines 253-254, \"the Teacher Sentence curve in the figure\": Which figure?\n    - Lines 267-268, \"Figure 2 and Figure 10 further illustrate this point\": I do not see how these figures show emergence of other actions after a few steps.\n    - Lines 320-327: This paragraph about LIMO-v2 is unclear to me, especially \"LIMO-v2 contains only 800 samples.\" It appears that LIMO-v2 refers to both a model as well as a dataset. I do not understand the statements about the work being limited to models of $<8$B parameters. A quick look at LIMO-v2's GitHub page suggests the LIMO-v2 model has 32B parameters.\n    - Lines 360-361, \"for each question with multiple responses from teacher models\": This is confusing because it suggests that multiple teachers give responses to the same question, and the selection is between teachers, which does not seem to be the case later on.\n    - Section 4.2.2 ablation (3): What is the domain of the two main datasets, AceReason-1.1-SFT and OpenThought3-1.2M, if science is considered a different domain?"}, "questions": {"value": "In order of importance:\n- Regarding weakness 4 above, how were the 4 combinations in Table 1 chosen? \n- I would appreciate responses to the clarity issues identified above.\n- Did the authors try splitting sentences using a natural language processor such as NLTK or spaCy? These may be more robust than the regex pattern used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aK5pd2ThZa", "forum": "Eem0IYVORm", "replyto": "Eem0IYVORm", "signatures": ["ICLR.cc/2026/Conference/Submission14630/Reviewer_E2xa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14630/Reviewer_E2xa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761175732024, "cdate": 1761175732024, "tmdate": 1762925006697, "mdate": 1762925006697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the origin of student model behaviors in reasoning distillation and introduces a framework called Reasoning Distillation Provenance Tracing (RDPT). Instead of only measuring performance, the authors analyze where each generated sentence (action) in a distilled model comes from—whether it originates from the teacher model, the student model, both, or is newly boosted through distillation. By comparing predictive probabilities from the teacher, student, and distilled models under the same context, they categorize actions into four provenance types. Using this analysis, the paper shows that distilled models indeed generate “teacher-originated” actions even in unseen contexts, and these actions are correlated with correctness, offering an interpretable explanation for distillation’s generalization benefits.\nBuilding on this, the authors propose a teacher-guided data selection strategy that prioritizes examples with higher teacher–student divergence. Experiments across multiple teacher–student pairs (e.g., DeepSeek-R1, QwQ-32B, GPT-OSS-120B) and reasoning benchmarks (AIME24/25, GPQA-D, MATH500) demonstrate consistent performance improvements over prior heuristics such as GRAPE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a novel analytical framework (RDPT) that quantitatively traces the provenance of reasoning actions, offering interpretability to the distillation process.\n- Empirical evidence that distilled models reproduce teacher-originated behaviors in unseen contexts, explaining why reasoning distillation generalizes.\n- The proposed teacher-guided data selection is simple yet principled, improving performance across diverse teacher–student configurations and datasets.\n- Strong experimental design and evaluation, with systematic analysis on multiple benchmarks and detailed ablation studies.\n- Offers insightful observations about how teacher- and student-originated patterns emerge and interact during reasoning, bridging the gap between empirical distillation and interpretability analysis."}, "weaknesses": {"value": "- The proposed framework (RDPT) is primarily analytical and diagnostic rather than methodological. It provides interpretation of distillation outcomes but does not introduce new mechanisms that improve reasoning capability.\n- The provenance classification relies on manually set thresholds (α, β) and simple probability gaps between teacher and student. This rule-based design is heuristic and potentially unstable across datasets or model scales.\n- The definition of “teacher-originated” actions is superficial and token-level, lacking semantic or reasoning-structure grounding. High teacher probability does not necessarily imply genuine reasoning transfer.\n- Experimental coverage is limited to ≤8B models and a narrow set of teacher–student pairs; it remains uncertain whether the findings generalize to stronger or cross-architecture settings.\n- The teacher-guided data selection method is incremental over GRAPE, and its ~2% average gain is modest given the extra computational cost of obtaining teacher logits.\n- The analysis focuses entirely on quantitative trends, without qualitative or behavioral validation to substantiate the claim that “teacher-originated” sentences truly reflect reasoning inheritance.\n- The framework assumes access to teacher probabilities for all samples, which restricts scalability and practical applicability in closed-source or large-model scenarios."}, "questions": {"value": "The experiments focus exclusively on mathematical reasoning tasks (AIME, MATH500, OlympiadBench, etc.), while reasoning distillation is also widely applied to other domains such as common-sense reasoning and natural language inference. Could the authors clarify why the analysis and validation were limited to math reasoning? Are there specific challenges that make extending the proposed provenance-tracing framework to broader reasoning domains difficult or less meaningful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CZj9IXSfpC", "forum": "Eem0IYVORm", "replyto": "Eem0IYVORm", "signatures": ["ICLR.cc/2026/Conference/Submission14630/Reviewer_GDUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14630/Reviewer_GDUf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964654691, "cdate": 1761964654691, "tmdate": 1762925006350, "mdate": 1762925006350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reasoning Distillation Provenance Tracing, a novel analytical framework for understanding where the behaviors of a distilled reasoning model originate, whether from the teacher model, the student’s prior patterns, or both. Building on these findings, they propose a teacher-guided data selection strategy, experiments across multiple teacher–student pairs show consistent improvements."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well organized and good written\n - RDPT introduces an interpretable framework for tracing the origin of reasoning steps, bridges the gap between explainability and distillation efficiency.\n - Proposes a simple yet effective teacher-guided data selection method."}, "weaknesses": {"value": "- Lack of validation on larger-scale (e.g., 70B+) models questions scalability\n - The proposed teacher-guided selection requires re-feeding large corpora through both models for probability extraction, cost analysis is needed\n - The method is primarily validated on short- to medium-length reasoning traces, leaving its effectiveness in long-range or multi-step reasoning scenarios uncertain."}, "questions": {"value": "Does producing more Teacher Sentences **cause** better reasoning, or merely co-occurs with it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GVPBjdsKVR", "forum": "Eem0IYVORm", "replyto": "Eem0IYVORm", "signatures": ["ICLR.cc/2026/Conference/Submission14630/Reviewer_8GhT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14630/Reviewer_8GhT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988992339, "cdate": 1761988992339, "tmdate": 1762925005828, "mdate": 1762925005828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of tracing provenance in LLM reasoning distillation. The paper proposes Reasoning Distillation Provenance Tracing, a method that quantifies the sources of a distilled model's capabilities: whether it is from the teacher, from the student, or both. Based on this, the paper then proposes a new teacher-guided data selection method, showing its efficacy across various teacher/student models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A timely, yet interesting topic\n- Well written\n- Several qualitatively interesting experimental results, and promising results for the newly proposed data selection method"}, "weaknesses": {"value": "- Although I'm not an expert in this field, it is still clear that the paper is lacking discussions of prior literature on knowledge distillation and model auditing/provenance [1,2]. Additionally, core literature on distilling LLM reasoning capabilities [3,4] is lacking.\n- Can the proposed provenance methodology be used to identify which teacher model distilled the reasoning capability?\n\n\n[1] https://dl.acm.org/doi/10.1145/3292500.3330885\n\n[2] https://openreview.net/forum?id=TatRHT_1cK\n\n[3] https://aclanthology.org/2023.acl-long.830/\n\n[4] https://aclanthology.org/2023.findings-acl.507/"}, "questions": {"value": "1. All the provenance analyses are done with open-source, already-distilled models only. What are the effects of different distillation methods? Do they lead to different provenance?\n2. If one utilizes distillation methodologies that explicitly matches sequential-level probabilities [5,6], then would this lead to more homogeneous provenance and potentially even better data selection? Some discussions on this and references therein would be helpful as well.\n\n\n\n[5] https://proceedings.mlr.press/v235/ko24c.html\n\n[6] https://proceedings.mlr.press/v267/ko25a.html"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1unyI8QXis", "forum": "Eem0IYVORm", "replyto": "Eem0IYVORm", "signatures": ["ICLR.cc/2026/Conference/Submission14630/Reviewer_CXTo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14630/Reviewer_CXTo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762440908945, "cdate": 1762440908945, "tmdate": 1762925005196, "mdate": 1762925005196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}