{"id": "YcdfEa1355", "number": 16569, "cdate": 1758266218185, "mdate": 1759897232122, "content": {"title": "LLMSELECTOR: Towards Model Selection Optimization for Compound AI Systems", "abstract": "Compound AI systems that combine multiple LLM calls, such as Self-Refine and Multiagent-Debate, are increasingly critical to AI advancements. Perhaps surprisingly, we find empirically that choosing different models for different modules has a substantial effect on these systems’ performance. Thus, we ask a core question in compound AI systems: for each LLM call or module in the system, how should one decide which LLM to use? As a first step, we formally show that the model selection problem (MSP) is computationally intractable. Next, we propose LLMSELECTOR, a principled framework that learns LLMs’ strengths and weaknesses across different modules through an LLM evaluator and then performs an efficient optimization to select which models to use in any given compound system with a bounded number of modules. Our theoretical analysis gives mathematical conditions under which LLMSELECTOR only requires LLM calls scaling linearly with the number of modules and the number of LLMs to identify the optimal model selection. Extensive experiments across diverse tasks, including multimodal question answering, health knowledge comprehension, and advanced reasoning challenges, demonstrate that LLMSELECTOR achieves up to 79% gains for compound AI systems like Self-Refine, Multiagent-Debate, and Majority-Vote with frontier reasoning models including GPT-5 and Gemini 2.5 Pro. Similarly, LLMSELECTOR unlocks up to 73% performance improvements as well when using general-purpose models such as GPT-4o and Claude 3.5 Sonnet.", "tldr": "We present an algorithmic framework to optimize model selection for compound AI systems, offering 2%-79% performance gains over using any single models including GPT-5, Gemini 2.5 Pro, and Claude Sonnet 4.", "keywords": ["LLM; compound AI systems; agents; model selection"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8665bc9b95a7e0882c5f185aac95c18a1d4815ef.pdf", "supplementary_material": "/attachment/ae4e0901b3adc3c96307099a272db9ca20d37fd1.pdf"}, "replies": [{"content": {"summary": {"value": "This paper addresses the Model Selection Problem in compound AI systems, which involve multiple LLM calls or modules. The authors observe that selecting different LLMs for different modules significantly impacts overall system performance. The MSP is formally shown to be NP-Hard due to its exponentially large search space. To tackle this, the paper proposes LLMSELECTOR, a framework that efficiently learns each model's strengths per module via an LLM evaluator and performs optimized model selection. Theoretical guarantees are provided under which LLMSELECTOR finds the optimal selection with linear complexity. Extensive experiments demonstrate performance gains of up to 79% on various tasks using both frontier and general-purpose LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies and addresses the under-explored but critical problem of model selection in compound AI systems, establishing a new research direction complementary to prompt optimization and module interaction design.\n2. The paper offers a solid theoretical foundation, including the NP-Hardness proof of MSP and mathematical conditions under which LLMSELECTOR achieves linear complexity and optimality.\n3. Extensive experiments on multiple tasks, systems, and model types robustly validate the method's effectiveness and generality.\n4. The paper demonstrates that LLMSELECTOR requires up to 75% fewer LLM calls than exhaustive search, making it highly efficient and scalable for potential real-world applications."}, "weaknesses": {"value": "1. I think the expression in Figure 1 is not clear enough. It should more explicitly demonstrate, in a Self-Refine process, a comparison between two approaches: one using manually selected models and the other dynamically learning and autonomously selecting models, rather than presenting fixed examples. This initially may mislead readers to question, for example, why vanilla self-refine consistently selects one type of model while LLMSELECTOR chooses three different models. It would be better to even show all three mentioned tasks (Self-Refine, Multiagent-Debate, Majority-Vote) to make a full comparison and provide a whole picture for the readers.\n\n2. In Section 1, the authors state that different models are better at different modules. However, what is the basis for this conclusion? What underlying reasons allow different models to excel at solving tasks in different modules? Since the method in this paper is built upon this assertion, I believe a more in-depth analysis and explanation are necessary.\n\n3. The MODELSELECTOR method proposed in the paper may have limitations in practical scenarios. This method primarily targets AI systems with fixed modules. In practical applications, however, a highly autonomous intelligent agent system might dynamically plan its modules based on different inputs, thereby generating a non-fixed multi-module process. Additionally, the modules may not necessarily execute sequentially but could operate in parallel. In such cases, is this method applicable? What challenges remain to be addressed?\n\n4. The complexity of the MODELSELECTOR method proposed in the paper is linear. However, the number of models available for selection in reality remains enormous (considering open-source model hubs such as Hugging Face or ModelScope). The experiments in the paper only considered eight general-purpose LLMs. If this method is extended to open-source model platforms, would it still be applicable? Would the complexity be too high? What technical improvements would still be needed?\n\n5. The paper provides an explanation of quality learning, but it lacks a detailed implementation process on how to enable an LLM to learn to evaluate allocation performance, making it difficult for readers to understand how this stage is completed.\n\n6. The author presents the performance of various tasks across eight different datasets, but there are significant differences in performance across different tasks and datasets. For example, the performance advantage in the Locate-Solve task is much greater than in the Multiagent-Debate task. What is the reason for this phenomenon? What types of tasks benefit more from model selection? This requires further analysis."}, "questions": {"value": "See the second, the third, the fourth, and the sixth points in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jVCixR7rin", "forum": "YcdfEa1355", "replyto": "YcdfEa1355", "signatures": ["ICLR.cc/2026/Conference/Submission16569/Reviewer_T1GQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16569/Reviewer_T1GQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760494864882, "cdate": 1760494864882, "tmdate": 1762926651499, "mdate": 1762926651499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the model selection problem (MSP) in compound AI systems composed of multiple LLM modules.\n\nHowever, the biggest weakness of this article is its lack of comparison with state-of-the-art (SOTA) multi-agent systems. If the authors could provide relevant model comparisons, I would consider raising my score."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Visualizations and tables are clear, providing both quantitative performance gains and qualitative insights.\n\nThe modular ascent/design is somehow sound."}, "weaknesses": {"value": "1. The article does not specify the training and inference costs. Following the three-stage approach described in the article to determine the optimal model combination step-by-step would theoretically result in extremely high costs.\n\n2. There is a lack of comparison with state-of-the-art MAS systems, and all the models used are closed-source. The authors should consider adding open-source models to facilitate the reproduction of experimental results.\n\n3. The proof of NP-hardness and several key algorithmic details are deferred to the Appendix but are critical for assessing validity. While their correctness does not need to be independently checked, the lack of a summary or sketch in the main text undermines transparency."}, "questions": {"value": "1. How sensitive is LLMSELECTOR to the reliability/noise of LLM-based estimators? Are there any empirical studies on its robustness or its ablation effect on estimator accuracy?\n\n2. Could the authors specify which article TableArithmetic comes from, or how their own benchmark was constructed, and how they ensured fairness?\n\n3. Why were all recent research results directly related to multi-agent systems and model selection not included in the experimental comparison? If the comparison is only against a baseline model, it is inherently fair, since your single response requires multiple LLMs for correction and refinement. To demonstrate the superiority of the method, the authors must supplement this with performance comparisons against state-of-the-art MAS/or LLM-as-judge systems on publicly available datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pXFnQI739S", "forum": "YcdfEa1355", "replyto": "YcdfEa1355", "signatures": ["ICLR.cc/2026/Conference/Submission16569/Reviewer_A9Bg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16569/Reviewer_A9Bg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809281565, "cdate": 1761809281565, "tmdate": 1762926650956, "mdate": 1762926650956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that for compound AI systems (like Self-Refine ), using different LLMs for different modules is better than using one model for everything. They propose LLMSELECTOR, a two-stage method: 1) Use an \"LLM evaluator\" to find a decent starting allocation (\"quality learning\") , and 2) Run a greedy search (\"module-wise ascent\") 。"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.As systems get more complex, we need to know where to use our best (and most expensive) models.\n\n2.Clear Motivation:** The case study in Figure 4 (TableArithmetic) is excellent. It shows a task where all single models fail (0-2% accuracy), but their method gets 96% by picking the right model for each step (e.g., GPT-5 for \"Locate\", Gemini 2.5 Flash Lite for \"Solve\")."}, "weaknesses": {"value": "---\n\n1.   The method seems to be just a combination of two known things: \"LLM-as-a-judge\" (Stage 1) and standard \"coordinate ascent\" or greedy search (Stage 2's \"module-wise ascent\"). This isn't a new algorithm.\n\n2.  The linear-time guarantee (Thm. 4.2) relies on an inter-monotone assumption—that improving one module never hurts another—which rarely holds in practice (e.g., a stronger generator can make the critic’s job harder).\n\n3. Stage 1 (\"quality learning\") is the most \"novel\" part. The paper *never* says which LLM they used as the evaluator, how they prompted it, or how robust it is. The theory assumes a *perfect* evaluator, which is impossible.\n\n4. The large reported gains (0→96%) come mainly from synthetic datasets like TableArithmetic, while on standard benchmarks such as MathVQA the improvement is minor (56→60%).\n\n\n\n**This gap makes the results look dataset-specific and raises doubts about the method’s real-world relevance.**"}, "questions": {"value": "---\n\nQ1.   To justify Stage 1 (quality learning), can you please provide a key ablation: comparing LLMSELECTOR to \"just running Stage 2 (module-wise ascent) from *multiple random* starting allocations\"? This is crucial to prove Stage 1 is more than just a good initialization.\n\n\n\n\n\nQ2.   Please specify the *exact model* and *the prompts* used for the \"LLM evaluator\" in Stage 1. This is essential for reproducibility. (b) Can you provide a sensitivity analysis? How much does final performance drop if you use a weaker model (e.g., Claude 3.5 Haiku) as the evaluator?\n\n\n\nQ3.  How do you explain the huge performance gap between synthetic tasks (e.g., TableArithmetic, 0% $\\rightarrow$ 96%) and standard benchmarks (e.g., MathVQA, 56% $\\rightarrow$ 60%)? Does this not imply the method's utility is limited to tasks with extreme, clear modular specialization?\n\n\n\n\nQ4. Your theory (Thm 4.2) relies on the \"inter-monotone\" assumption, which you admit \"is not always satisfied\". Can you provide *empirical evidence* (e.g., from your Self-Refine experiments) showing *how often* this assumption is violated in practice? How badly is convergence affected when it is?\n\n\n\nQ5. Stage 2 (module-wise ascent) requires many end-to-end system evaluations. Can you report the *total cost* (e.g., total API calls) of LLMSELECTOR in practice? The Figure 4(f) case study is small ($|V|=2, |M|=8$). How does this cost scale when $|V|$ is larger (e.g., in Multiagent-Debate, $|V|=6$)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ifPFi30hKi", "forum": "YcdfEa1355", "replyto": "YcdfEa1355", "signatures": ["ICLR.cc/2026/Conference/Submission16569/Reviewer_ujw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16569/Reviewer_ujw1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824596098, "cdate": 1761824596098, "tmdate": 1762926648896, "mdate": 1762926648896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the Model Selection Problem (MSP) in compound AI systems, where frameworks such as Self-Refine and Multiagent-Debate that combine multiple large language model (LLM) calls to solve complex tasks. The authors first formalize MSP and prove that finding the optimal model allocation is NP-hard. To address this challenge, they propose LLMSelector, a two-stage optimization framework that (1) learns each model’s module-wise performance using an LLM evaluator through a quality learning phase, and (2) performs module-wise ascent to efficiently identify near-optimal model allocations. Under mild assumptions, LLMSelector achieves linear computational complexity with respect to the number of models and modules. Experiments across diverse systems and tasks show that LLMSelector delivers substantial performance gains over fixed single-model baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper systematically studies the model selection problem in compound AI systems, which is important and underexplored beyond traditional prompt or interaction optimization. Its efforts on handling the exponentially large search space of multi-module LLM pipelines is both novel and highly relevant for advancing automated AI pipeline optimization.\n2. The proposed LLMSelector framework is theoretically grounded and achieves linear computational complexity under reasonable assumptions. Its design, combining quality learning with module-wise ascent, is effective and computationally efficient for compound systems.\n3. The experiments, conducted across multiple tasks, datasets, and both frontier and general-purpose LLMs, demonstrate consistent and substantial performance improvements.\n4. The paper is well-written and well-organized. The figures, examples, and case studies clearly convey the workflow, motivation, and contributions."}, "weaknesses": {"value": "1. The efficiency and optimality proofs depend on strong monotonicity assumptions that may not hold in real-world settings. The paper should discuss sensitivity to assumption violations and expected performance impacts.\n2. More ablation studies and comparisons under varied experimental setups are needed to clearly demonstrate advantages over existing compound system optimization methods.\n3. The current framework is limited to pre-defined static architectures. It is interesting to explore dynamic or adaptive compound systems, where module structures or interactions evolve during inference."}, "questions": {"value": "1. It is better to provide more implementation details on the quality learning phase.\n2. It would be helpful to include a clearer analysis of the LLM call cost and computational overhead involved in the LLMSelector optimization process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZucZpWg7IO", "forum": "YcdfEa1355", "replyto": "YcdfEa1355", "signatures": ["ICLR.cc/2026/Conference/Submission16569/Reviewer_m2vk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16569/Reviewer_m2vk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981321107, "cdate": 1761981321107, "tmdate": 1762926648192, "mdate": 1762926648192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}