{"id": "6KHGfmFm4A", "number": 16606, "cdate": 1758266678365, "mdate": 1759897229999, "content": {"title": "Temporal Reasoning for Vision-Language Models via Chain of Draft", "abstract": "Large Vision Language Models (LVLMs) like Qwen VL have demonstrated remarkable capabilities in understanding and reasoning about visual content, particularly for static images. However, their application to video reasoning tasks remains computationally intensive, with significant latency and token usage when prompted using traditional Chain-of-Thought (CoT) methods. In this paper, we propose the integration of Chain of Draft (CoD) methodology with Qwen VL for efficient video reasoning. CoD is a prompting technique that encourages models to generate concise, essential intermediate thoughts rather than verbose reasoning steps. We adapt this approach specifically for video understanding tasks, demonstrating that our method achieves comparable or superior accuracy to CoT while significantly reducing token consumption (by up to 78%) and inference latency (by up to 65%). We evaluate our approach on multiple video reasoning benchmarks, including MVBench and EgoSchema, and demonstrate its effectiveness across various video understanding tasks. Our contributions include: (1) a novel adaptation of Chain of Draft for video reasoning tasks; (2) a comprehensive evaluation framework for video reasoning efficiency; (3) a theoretical analysis providing time complexity guarantees; and (4) empirical evidence of significant computational benefits without sacrificing accuracy. This work has important implications for deploying efficient video reasoning capabilities in resource-constrained environments and real-time applications.", "tldr": "", "keywords": ["chain of draft; temporal reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0effa335630f68e88d017699088df6375cb7488a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes adapting Chain of Draft (CoD), a prompting style that encourages concise intermediate thoughts, for video reasoning with large vision–language models (LVLMs), specifically Qwen VL. The method adds three ideas for temporal data: a minimal temporal indexing to reference moments, visual abstraction to compress object–relation descriptions, and event compression to remove redundant events. Experiments show similar or slightly better accuracy than CoT while reducing tokens and latency."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The core question of how to keep the benefits of deliberate reasoning without paying CoT’s heavy token/latency cost on videos is timely. The paper gives a clear, lightweight interface for concise intermediate steps and provides large efficiency gains with small accuracy deltas across four benchmarks."}, "weaknesses": {"value": "The novelty beyond prompt engineering is modest and the attribution of gains to each proposed component is under-evidenced. The three add-ons (temporal indexing, visual abstraction, and event compression) are introduced with definitions and notations, but there is no ablation that removes each piece independently under the same training/inference budget. \n\nThe feasibility of the event-compression rule depends on quantities that are not accessible at inference time. Eq. (5) uses a conditional mutual information criterion $I(e;q,a\\mid e')>\\epsilon$ to decide which events to keep, but the paper does not explain how $I(\\cdot)$ is estimated online without access to $a$ or a surrogate. A practical remedy is to replace $I(\\cdot)$ with a calibrated proxy (e.g., a small learned scorer over $q$ and draft logits) and then show correlation with the theoretical $I(\\cdot)$ on a labeled slice. Please clarify the estimator or restate Eq. (5) as an offline data construction heuristic. \n\nThere is an inconsistency between the pitch of “prompting technique” and the actual training recipe. Section 2.4 introduces two fine-tuning phases, including a video fine-tuning objective with CoD-formatted chains, which departs from a purely training-free narrative in the introduction and abstract. I think the paper should explicitly separate zero-shot prompting from the fine-tuned variant and report both under identical evaluation harnesses.  \n\nThe theoretical section largely restates token-count proportionality without validating assumptions on $m$, $\\bar z$, and $\\bar d$. The 80% reduction arises from a fixed 5-word cap and an assumed 25-word CoT step, but there is no empirical check that $m$ does not increase under CoD (e.g., via more, shorter steps). Please add a controlled study reporting step count $m$, average length, and total tokens for matched accuracy at multiple temperatures; Table 5 gives some stats but not for accuracy-matched settings.  \n\n\nThe Qwen-specific integration details need hyperparameter transparency to reproduce results. Section 2.3 defines M-RoPE and the generation factorization, but important settings are missing: dynamic frame sampling schedule, max frames per clip, temperature/top-p, and any decoding constraints for drafts. Please provide these in an appendix table and report seed variance for Tables 1–4. \n\nThe efficiency analysis would benefit from accuracy-matched comparisons. Table 2 reports large token and latency reductions but at slightly different accuracies; without accuracy-matching (by adjusting temperature or step limits) the headline percentages can overstate the efficiency gain. \n\nThe paper’s LaTeX template appears modified and fonts differ from the ICLR guidelines. This does not block understanding but may violate submission requirements. \n\nThe citation style is consistently incorrect in-text. For example, “Benchmarks like MVBench Li et al. (2023b), EgoSchema Mangalam et al. (2023), and MTVQA Wang et al. (2022) …” should instead be “Benchmarks like MVBench (Li et al., 2023b), EgoSchema (Mangalam et al., 2023), and MTVQA (Wang et al., 2022) …”."}, "questions": {"value": "How much of the gain comes from each of the three components? Please provide a clean ablation removing temporal indexing, visual abstraction, and event compression one at a time under CoD. \n\nHow is the conditional mutual information in Eq. (5) estimated in practice during inference, and what proxy is used if $a$ is not known yet? A small validation showing proxy–$I(\\cdot)$ correlation would clarify feasibility. \n\nCan you report zero-shot CoD prompting (no fine-tuning) next to the fine-tuned CoD, and also a fine-tuned CoT baseline under the same training data and budget? This would isolate prompting vs. training effects."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zz5EPf4hIM", "forum": "6KHGfmFm4A", "replyto": "6KHGfmFm4A", "signatures": ["ICLR.cc/2026/Conference/Submission16606/Reviewer_ci29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16606/Reviewer_ci29"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761160975034, "cdate": 1761160975034, "tmdate": 1762926678970, "mdate": 1762926678970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an adaptation of chain-of-draft (CoD) for vision language models for video reasoning tasks.\nThe idea prompts MLLMs to generate concise reasoning steps for token efficiency, and the paper introduces several task-specific rules for draft generation (e.g., shortening timestamps, event mapping). Experiments show that the proposed method achieves better performance than CoT while using significantly fewer tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is straightforward and the token reduction is important for practical usage for MLLMs."}, "weaknesses": {"value": "- Limited evaluation: as the paper's focus is on improving video temporal reasoning tasks, it is highly recommended to evaluate the method on long video benchmarks such as VideoMME, MLVU, and LongVideoBench, as well as temporal-oriented diagnosis benchmarks such as TempCompass and VITATECS. The current evaluation benchmarks are mainly short and general videos.\n\n- Unclear training details: It remains unclear which dataset is used for `fine-tuning on examples of efficient video reasoning` (Line 263), which is important for understanding the benefits gained.\n\n- Generalizabilities: As only the Qwen-VL series is adapted, it is unknown whether the method can generalize to other MLLMs such as LLaMA-V and LLaVA. Is there any requirements for the underlying backbone models?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8wDjbSsN0v", "forum": "6KHGfmFm4A", "replyto": "6KHGfmFm4A", "signatures": ["ICLR.cc/2026/Conference/Submission16606/Reviewer_Fn3H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16606/Reviewer_Fn3H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881787657, "cdate": 1761881787657, "tmdate": 1762926678655, "mdate": 1762926678655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends Chain of Draft (CoD), a minimalist form of intermediate reasoning to video. At inference time, it employs temporal indexing, visual abstraction, and event compression to produce short, symbolic draft chains, and uses a two-stage fine-tuning protocol to enable an MLLM to generate such minimalist reasoning traces."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Proposes an efficient video reasoning framework that markedly reduces token usage and latency without sacrificing accuracy."}, "weaknesses": {"value": "First, the submission appears to deviate from the ICLR format (e.g., font), which may warrant a desk reject. While this is ultimately an AC decision, I provide my technical assessment below.\n\n1. The writing is unusual. The main text does not reach nine pages, which could be acceptable for concision, but substantial space is devoted to content unrelated to the proposed method. For instance, the “Integration with Qwen VL” section discusses many Qwen2-VL specifics, whereas Table 1 indicates the primary base MLLM is Qwen2.5-VL. Moreover, the method seems general-purpose—why is it tightly coupled to the Qwen2-VL backbone? What results would it yield on other models?\n2. The approach of annotating video content into short, symbolic draft chains and fine-tuning the base MLLM appears ineffective empirically. After two-stage fine-tuning, CoD improves performance by less than 1% over simple prompting (e.g., “Think step by step ...”) to induce CoT. Although CoD substantially reduces reasoning length, its marginal gains over the weakest baseline (prompt-only) limit practical utility. The authors should compare against video MLLMs explicitly trained with CoT, or take such a base model and further fine-tune it to produce short, symbolic draft-chain reasoning to substantiate effectiveness. The current CoT baseline averages only ~200 tokens per response, which may not constitute a sufficiently strong reasoning baseline.\n3. Missing comparisons with related CoT compression methods. Recent work such as tokenskip [1] similarly compresses and streamlines CoT to improve reasoning efficiency.\n4. Baselines are weak. The base MLLM is the strong Qwen2.5-VL (Table 1), but comparisons are made against outdated models like GPT-4V. At minimum, comparisons should include commonly used gpt-4o and several reasoning-oriented MLLMs; otherwise, the evaluation is unfair.\n\n[1] TokenSkip: Controllable Chain-of-Thought Compression in LLMs. EMNLP 2025."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3djwhjwG2G", "forum": "6KHGfmFm4A", "replyto": "6KHGfmFm4A", "signatures": ["ICLR.cc/2026/Conference/Submission16606/Reviewer_gxzA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16606/Reviewer_gxzA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178203110, "cdate": 1762178203110, "tmdate": 1762926678324, "mdate": 1762926678324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}