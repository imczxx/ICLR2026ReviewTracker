{"id": "RsletK8757", "number": 18831, "cdate": 1758291235409, "mdate": 1759897078958, "content": {"title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models", "abstract": "Recent advances in text-to-video (T2V) generation have enabled visually compelling outputs, but models still struggle with everyday physical commonsense, often producing videos that violate intuitive expectations of causality, object behavior, and tool use. We introduce PhysVidBench, a human-validated benchmark for assessing physical reasoning in T2V models. It comprises carefully curated prompts spanning seven dimensions of physical interaction, from material transformation to temporal dynamics, offering broad, multi-faceted coverage of scenarios where physical plausibility is critical. For each prompt, we generate videos using diverse state-of-the-art models, and evaluate them through a three-stage pipeline: grounded physics questions are derived from each prompt, generated videos are captioned with a vision–language model, and a language model answers the questions using only the captions. This strategy mitigates hallucination and produces scores that align closely with human judgments. Beyond evaluation, PhysVidBench also serves as a diagnostic tool, enabling feedback-driven refinement of model outputs. By emphasizing affordances and tool-mediated actions, areas often overlooked in existing benchmarks, PhysVidBench provides a structured, interpretable framework for assessing and improving everyday physical commonsense in T2V models.", "tldr": "", "keywords": ["benchmarks", "video generation", "physical commonsense", "text-to-video", "generative models", "evaluation", "multimodal reasoning", "synthetic video", "model assessment"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2877ef59caff200fa215d16993f6babb88071168.pdf", "supplementary_material": "/attachment/3976b5069c0797816fd1af97929f4e2725c75d28.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a multi-stage, structured benchmark and evaluation methodology for assessing the physical reasoning capabilities of text-to-video generation models. The proposed framework includes a multi-phase prompt construction process and a multi-stage evaluation pipeline that jointly enable fine-grained analysis across seven dimensions of physical commonsense. The authors evaluate a range of state-of-the-art models, including Sora and Veo-2, on the proposed PhysVidBench, and further introduce an iterative, error-guided refinement strategy for improving video generation quality based on their evaluation framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The evaluation methodology is relatively novel and interesting, offering a new perspective on assessing the physical understanding of video generation models.\n2. The division of evaluation into seven commonsense dimensions appears comprehensive and covers a wide range of physical reasoning aspects.\n3. The proposed iterative, error-guided prompt refinement approach for video generation is potentially inspiring for future research on video generation agents."}, "weaknesses": {"value": "1. The presentation of the paper is not very good. For example, in Figure 3, the text in Stage 4 is incomplete and partially covered. The classification in Figure 1 into Real-World Videos and Synthetically Generated Videos is unclear in purpose. Also, Figure 4 looks more like a slide for a report rather than a figure suitable for an academic paper.\n2. I have some concerns about the details of the evaluation process. In evaluation Step 1, the LLM generates ground-truth \"yes\" questions purely based on textual information. Would this introduce bias? It might also mislead the judgment in Stage 3 when another LLM is used, similar to the VLM collapse phenomenon mentioned in Table 5.\n3. It seems that all the video content and elements are processed in Stage 2 of the evaluation pipeline. However, only one model (e.g., AuroraCap in the paper) is used in this critical step. What is the reason for choosing this model, and how accurate are the generated captions? This is crucial because it heavily influences the Stage 3 judgments, and using a single model seems insufficient. In addition, since this model generates eight captions for each video, could there be contradictions among them? If so, how are such conflicts avoided?"}, "questions": {"value": "* The generated videos from T2V models may sometimes be ambiguous, making certain yes/no questions difficult to classify or judge. Did the authors observe such cases during the experiments? If so, how were these situations handled or evaluated?\n* Other questions are mentioned in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p4j2f1a51S", "forum": "RsletK8757", "replyto": "RsletK8757", "signatures": ["ICLR.cc/2026/Conference/Submission18831/Reviewer_Gav7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18831/Reviewer_Gav7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600923699, "cdate": 1760600923699, "tmdate": 1762930801101, "mdate": 1762930801101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PhysVidBench, a new benchmark designed to evaluate physical commonsense understanding in T2V models. They argue that although recent T2V models demonstrate impressive visual realism, they frequently violate intuitive physics—showing implausible object motions, misuse of tools, or broken causal sequences. PhysVidBench is derived from the PIQA dataset and contains 383 base prompts plus 383 “upsampled” prompts, emphasizing tool use and affordance reasoning in everyday physical tasks. Evaluation is conducted via a three-stage pipeline: Generating physics-grounded yes/no questions; Producing dense video captions; Asking an LLM to answer the questions using only the captions. \nExperiments across major T2V models show that current systems achieve modest scores and fail most often in spatial reasoning and temporal dynamics. They further demonstrate human–model agreement and propose a lightweight prompt refinement loop that improves generation quality without retraining the model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tClear motivation and real-world relevance\nThe paper targets an underexplored yet essential capability: everyday physical commonsense. Unlike prior work focusing on abstract physics laws or motion smoothness, PhysVidBench tests realistic, goal-oriented interactions involving tool use, and material behavior.\n2.\tComprehensive and systematic evaluation\nThe benchmark spans seven reasoning dimensions (force, motion, affordance, material transformation, etc.) and incorporates a difficulty-based stratification. The analysis includes base vs. upsampled prompts, scale effects, and cross-model comparisons, offering a holistic diagnostic view.\n3.\tDiagnostic and iterative refinement utility\nThe proposed “error-guided prompt refinement” demonstrates that the same benchmark can be repurposed to guide model improvement. The iterative procedure yields consistent gains without model updates, proving its usefulness beyond static evaluation."}, "weaknesses": {"value": "1.\tEvaluation of realism and ceiling\nAlthough the caption-based QA pipeline reduces hallucination, it evaluates textual rather than visual physical understanding. The final judgment depends on the only one captioner’s recall and Gemini’s internal physics priors. A single model may have deviations. The pipeline measures consistency within the text–caption–QA chain, not direct perception–reasoning, may introduce biases. \n2.\tLack of statistical significance reporting\nPerformance differences (e.g., base vs. upsampled prompts, difficulty tiers) are presented without confidence intervals or hypothesis testing. Including bootstrap-based confidence intervals or paired-sample tests would strengthen claims about robustness and consistency.\n3.\tLimited benchmark scale and coverage\nWhile high in quality, 383 prompts are relatively small compared to existing video benchmarks. The benchmark lacks broader coverage of outdoor or complex fluid–solid interactions, or other based datasets similar to PIQA.\n4.\tLack of granular failure analysis\nThe discussion identifies spatial and temporal reasoning as weak dimensions but does not quantify error categories (e.g., occlusion, contact, support relations, deformation). A detailed failure analysis would enhance the diagnostic value of the benchmark."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r5rhJE1PIi", "forum": "RsletK8757", "replyto": "RsletK8757", "signatures": ["ICLR.cc/2026/Conference/Submission18831/Reviewer_8iig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18831/Reviewer_8iig"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558109649, "cdate": 1761558109649, "tmdate": 1762930800265, "mdate": 1762930800265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PhysVidBench, a new benchmark for evaluating physical commonsense reasoning in text-to-video (T2V) models. The authors argue that while existing models produce visually compelling outputs, they struggle with everyday physical interactions like tool use and causality. The benchmark consists of 383 curated prompts derived from the PIQA dataset, focusing specifically on tool-mediated actions and object affordances. The evaluation employs an innovative three-stage pipeline: \"always-yes\" physics questions are generated from prompts, generated videos are densely captioned by a VLM, and an LLM answers the questions based only on the captions. This caption-based approach is shown to mitigate hallucinations and align closely with human judgment , revealing significant gaps in spatial and temporal reasoning even in state-of-the-art models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The most significant strength is the novel evaluation pipeline. By decoupling perception (VLM dense captioning ) from reasoning (LLM-as-a-judge ), the method cleverly avoids the hallucination and \"response collapse\" pitfalls common in direct video-QA.\n2. The benchmark itself is well-designed and highly focused.\n3. The benchmark offers strong diagnostic utility beyond simple scoring. It provides a fine-grained breakdown of model failures across seven distinct physical dimensions (Table 2)"}, "weaknesses": {"value": "1. The evaluation is entirely dependent on the AuroraCap VLM. If the captioner fails to observe a correctly generated physical detail, the T2V model is unfairly penalized for a failure in the evaluation pipeline itself.\n\n\n2. The benchmark's questions are all designed to have \"Yes\" as the ground truth. This only tests a model's ability to produce a correct phenomenon and fails to test if it can avoid producing a physically implausible one.\n\n\n3. All 383 prompts are adapted from the PIQA dataset. While this focuses on tool use, this single source may not be diverse enough to represent the full spectrum of everyday physical commonsense scenarios\n\n4. The user study validating the evaluation pipeline, while positive, was relatively small, with 15 participants evaluating 120 videos. A larger study would provide more robust statistical confidence in its alignment with human judgment.\n\n5. There is a lack of discussion in related work on improving the physical consistency of video generation, such as PhyT2V[1], WISA[2], VIdeoREPA[3]\n\n[1] Phyt2v: Llm-guided iterative self-refinement for physics-grounded text-to-video generation\n[2] Wisa: World simulator assistant for physics-aware text-to-video generation\n[3] VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models"}, "questions": {"value": "1. Have you conducted an analysis to quantify the \"false negative rate\" of AuroraCap for this specific task? That is, how often does it fail to describe a key, subtle physical phenomenon that was verifiably present in the generated video?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nan"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8TSQuUOYFN", "forum": "RsletK8757", "replyto": "RsletK8757", "signatures": ["ICLR.cc/2026/Conference/Submission18831/Reviewer_VJxa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18831/Reviewer_VJxa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889111332, "cdate": 1761889111332, "tmdate": 1762930799663, "mdate": 1762930799663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}