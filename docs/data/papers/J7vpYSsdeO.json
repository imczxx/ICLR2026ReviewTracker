{"id": "J7vpYSsdeO", "number": 17864, "cdate": 1758281376674, "mdate": 1759897149393, "content": {"title": "Pretraining LLM with Latent Thoughts in Continuous Space", "abstract": "The remarkable success of Chain-of-Thought (CoT), which enhances performance by scaling generation steps at test-time, inspires us to ask: can we leverage a similar scaling of computational steps during pretraining to improve the generation of each individual token? To address this, we propose a novel pre-training methodology: Pretraining Language Models with Latent Thoughts. Our approach pretrains a language model (LM) to first generate an intermediate latent thought—the last hidden\nstate of the current position—which is then used as input to predict the actual subsequent token. This additional computational step enables the LM to refine its prediction within unconstrained continuous space. Our experiments demonstrate that, at an identical inference cost, a LM that generates one additional latent thought per token outperforms a standard model with double the parameters. For instance, ours-1.4B (Pythia Arch), pretrained on 300B tokens from the Pile, significantly surpasses the vanilla Pythia-2.8B trained on the same data on both language modeling and a range of general downstream tasks. Furthermore, increasing the number of latent thoughts generated before each actual token—forming a chain analogous to CoT—consistently improves the model's performance.", "tldr": "", "keywords": ["Pretraining", "Latent Thoughts", "Continuous Space", "language modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e94e044e123986964e6b64634dfcc6305f5acb0f.pdf", "supplementary_material": "/attachment/b760d817aeab1e6d2fa3dd9f6a0a2d8a1b9ed9f1.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a technique to interleave input tokens of an LLM and their corresponding hidden states during pre-training and inference. With such a horizontal scaling approach, the authors show that Pythia-1.4B models can be trained with a relatively smaller amount of tokens from the Pile dataset and attain similar validation loss as the Pythia-2.8B model. For the same computation budget of 300B tokens, the proposed pre-training approach and inference leads to better downstream performance across a variety of tasks. Similar experiments were conducted for GPT2 and Llama models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The technique is quite simple, yet well presented with extensive experiments, and is also shown to present a new avenue for scaling with continuous/hidden layer features."}, "weaknesses": {"value": "FLOP utilization during pre-training with the new approach is not discussed at all. This is important since the Jacobi iteration needs to be performed at least 2 or 3 times for hidden state convergence of every input sequence. This means 2/3 forward passes per sequence with double the sequence length (input tokens + hidden states). Additionally, pre-training resources, compute budget, and time are not discussed. Claims regarding the comparison with bigger models will have to be revisited based on the FLOP analysis. The details about the context lengths for inference are currently missing, so I am unsure about the claims on horizontal scalability/ complementing existing CoT prompting approaches."}, "questions": {"value": "1. In Figure 1 (b), if the Pythia-1.4B model trained with the proposed approach for 62\\% fewer tokens achieves the same loss as the vanilla pre-training, how does the downstream performance of this checkpoint compare with fully trained checkpoint?\n\n2. In section 4.2.2, was instruction finetuning done with the proposed approach or vanilla?\n\n3. How does the approach complement standard chain-of-thought? It seems that long chains will be a bottleneck for this approach. More importantly, can you provide details about the sequence lengths for training/inference? How long are the outputs for the results in (say) Figure 6?\n\n4. What happens if we train with the proposed approach and a varying number of latent thoughts, but try to avoid latent thoughts during inference and leverage the standard chain of thought? This tradeoff is important to consider when reporting the results on math/reasoning tasks, as the steps can be lengthy and the proposed approach might result in fewer discrete tokens, potentially failing to provide the answer within a reasonable inference token budget."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GcFr6RrosG", "forum": "J7vpYSsdeO", "replyto": "J7vpYSsdeO", "signatures": ["ICLR.cc/2026/Conference/Submission17864/Reviewer_bYzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17864/Reviewer_bYzv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635391739, "cdate": 1761635391739, "tmdate": 1762927692814, "mdate": 1762927692814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a \"continuous\" pretraining technique for language models where, before predicting each token, the model generate intermediate \"latent thoughts\" hidden states in continuous space. It is essentially a pretraining extension of chain-of-continuous thought (coconut) paper. \n\nTo make the sequential refinement of these hidden states efficient during training, the authors introduce a method that allows for parallel updates. \n\nTheir results show that a 1.4B parameter model achieves performance comparable to a conventional 2.8B parameter model trained on the same data. This demonstrates that \"horizontal thinking\", or doubling the reasoning steps per token can help."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Presents novel pretraining method, extending coconut to pretraining\n- Parallel training procedure is nice\n- Baseline comparisons are good (i have a few questions in later section)\n- The results seem strong"}, "weaknesses": {"value": "- It's unclear the difference between parallel training and sequential inference\n- Fig 1 shows similar results at lower #params, but double the compute. We know that compute can be more beneficial than #params\n- Lacking in ablations and discussion on why this method outperforms the baselines\n- Doesn't compare to training a model with double the amount of layers. That's a more proper comparison than just double param count. \n- It would be nice to see larger models or some math evals, but these are somewhat minor."}, "questions": {"value": "- Training uses parallel, but inference uses sequential (feedback). Why isn't there a train/test mismatch?\n- For the models that use double the amount of parameters, they don't use double the amount of layers, correct? That seems to be the proper comparison\n- It's a bit surprising how much better your method is than other methods in table 3. Do you have insights as to why this is the case?\n- The Pythia-1.4B in table 2 is not pretrained by you, correct? It uses exactly all of the same hyperparams though?\n- All of the models in Table 3 are trained by you?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "byyuRxffsU", "forum": "J7vpYSsdeO", "replyto": "J7vpYSsdeO", "signatures": ["ICLR.cc/2026/Conference/Submission17864/Reviewer_2jAG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17864/Reviewer_2jAG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870566632, "cdate": 1761870566632, "tmdate": 1762927692320, "mdate": 1762927692320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new pretraining methodology, where language models at each step are used to first generate hidden states in continuous space and then resend as input to generate the next token. Training uses Jacobi iteration to update hidden states iteratively. Results show improved performance in general LM benchmarks, as compared to baseline models of similar sizes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Authors propose a conceptually lightweight method to add computational depth per token.\n* Jacobi iteration enables efficient parallel training despite the inherently sequential inference process.\n* Experiments indicate better performance across benchmarks, as well as in parameter and data efficiency."}, "weaknesses": {"value": "* The name \"latent thought\" is potentially an overclaim. The term \"latent thought\" suggests high-level reasoning or abstraction, but the method simply generates one additional hidden state per token without evidence of global deliberation or multi-token planning processes. It is also unclear from the experiment what role the hidden states actually play in reasoning.\n* The method requires 2N KV cache positions for N tokens, effectively cutting the usable context window to half. The paper lacks discussion on how the model can handle longer context, which is what modern LLM applications increasingly rely on.\n* Authors apply Jacobi iteration to approximate hidden states, but there is no analysis of convergence conditions, fixed-point existence guarantees, or motivation-wise, why this approximation should work for learned non-linear transformations.\n* Ablation studies in Figure 8 use Pythia-70M which is 20× smaller than main experiments, and it is expected that additional computation helps more. It is unclear whether the scaling behavior can transfer to larger models."}, "questions": {"value": "* It is unclear what the hidden states are actually learning through iterations. Can the authors provide some experiments or analysis of what \"latent thoughts\" compute. For instance, probing studies, attention pattern visualizations, or examples showing how thoughts evolve through iterations)\n* How does performance scale with longer thought chains (K > 3)? Is it influenced by different task types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J0AbHJaZug", "forum": "J7vpYSsdeO", "replyto": "J7vpYSsdeO", "signatures": ["ICLR.cc/2026/Conference/Submission17864/Reviewer_GTwo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17864/Reviewer_GTwo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947043138, "cdate": 1761947043138, "tmdate": 1762927691895, "mdate": 1762927691895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a novel method to incorporate continuous thoughts into the model parameterization without changing the model architecture or number of parameters. The core idea is to pass the last hidden state of the model as input embedding at the next time step and repeat the process N times. This way model can reflect on its own representation multiple times before predicting the next token. \nAuthors proposed an elegant way of speeding up the training process by using jacobian iterations which allowed them to keep the parallel training of transformer by only repeating the forward pass N times according to N continuous extra steps.\nThey conducted a set of experiments including pretraining, mid-training and evaluated resulted models across different benchmarks. They compared their models to known baselines that involved both discrete and continuous thoughts. Their experiments confirm that their approach is superior to related work as well as their model at smaller size surpass larger models trained without continuous thoughts."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The effectiveness of the method will allow community to use it as a mid training scenario in many future work.\n2. Ability to keep the efficiency of training makes it especially attractive."}, "weaknesses": {"value": "1. Novelty is not extreme given related work, but good execution here compensate that making it a solid contribution to the community."}, "questions": {"value": "1. I am very curious to see how models with continuous thoughts show itself with different test time scaling approaches i.e. majority voting or combined with external reward models. If authors can perform a simple experiment with that, that would be awesome. For instance, how much different decoding hyper parameters affect results with these models (are these models more collapsed in their distributions or not)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uakuE4nAae", "forum": "J7vpYSsdeO", "replyto": "J7vpYSsdeO", "signatures": ["ICLR.cc/2026/Conference/Submission17864/Reviewer_gUCx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17864/Reviewer_gUCx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023789976, "cdate": 1762023789976, "tmdate": 1762927691427, "mdate": 1762927691427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new pretraining methodology where the language model performs an extra computation step to generate an intermediate \"latent thought\" (its last hidden state) before using that state to predict the next token. This \"think-before-you-speak\" mechanism is applied to every token. To make this sequential process trainable, the authors use a Jacobi iteration to parallelize the computation, approximating the \"latent thought\" in a fixed number of steps. The authors claim this method produces models that are far more efficient, with their 1.4B parameter model, which runs two passes per token, outperforming a standard 2.8B parameter model at a comparable inference throughput."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-Motivated Problem: The goal of creating smaller, more compute-efficient models that can match the performance of larger models is highly valuable and a key area of research.\n\n2. Strong Empirical Results: Taken at face value, the results are impressive. Table 3, for instance, shows the proposed 1.4B LLaMA model outperforming not only a 2.8B LLaMA baseline but also other \"increased-compute\" methods like PonderLM and Looped Transformer, even when those are given a larger (4x) compute budget.\n\n3. Broad Applicability: The method is demonstrated on three different architectures (Pythia, LLaMA, GPT-2) and is also shown to be effective as a continual pretraining technique for existing models (LLaMA-3-3B), suggesting it is a potentially general-purpose method."}, "weaknesses": {"value": "1. The paper's claims of efficiency are one-sided and misleading. It completely omits an analysis of training cost, which appears to be substantially higher (e.g., 6-8x per step) due to the 2x sequence length and K=2-4 passes. The paper is therefore missing the most critical baseline: a vanilla model trained for the same total training FLOPs. Without this, it's impossible to know if the proposed method is superior, or if the authors simply trained their model with far more total compute.\n\n2. The core training method is presented without theoretical or empirical justification. The Jacobi iteration is a pragmatic hack, but the paper provides no analysis of its approximation error or why this fixed-point iteration should converge to a meaningful linguistic representation. Furthermore, key design choices, like sampling K from {2,3,4}, are arbitrary and presented without any ablation or analysis.\n\n3. Also the choice of thought tokens (the last hidden unit and the interleaved sequence) needs to be justified over other choices like PonderLM. The training cost seems to be significantly enhanced. It is unclear if the benefit comes from the Jacobi training, the choice of the hidden state as the thought tokens, or the specific interleaving strategy."}, "questions": {"value": "1. Can you clarify the inference memory requirements? The two-pass generation process at inference suggests the KV-cache size may be doubled."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z47ebFW26u", "forum": "J7vpYSsdeO", "replyto": "J7vpYSsdeO", "signatures": ["ICLR.cc/2026/Conference/Submission17864/Reviewer_TxRr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17864/Reviewer_TxRr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762552528018, "cdate": 1762552528018, "tmdate": 1762927690547, "mdate": 1762927690547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}