{"id": "OndDxNGrqJ", "number": 19704, "cdate": 1758298542055, "mdate": 1763523542543, "content": {"title": "Modeling Student Learning with 3.8 Million Program Traces", "abstract": "As programmers write code, they often edit and retry multiple times, creating rich “interaction traces” that reveal how they approach coding tasks and provide clues about their level of skill development. For novice programmers in particular, these traces reflect the diverse reasoning processes they employ to code, such as exploratory behavior to understand how a programming concept works, re-strategizing in response to bugs, and personalizing stylistic choices. In this work, we explore what can be learned from training language models on such reasoning traces: not just about code, but about coders, and particularly students learning to program. We introduce a dataset of over 3.8 million programming reasoning traces from users of PENCIL CODE, a free online educational platform used by students to learn simple programming concepts. Compared to models trained only on final programs or on synthetically-generated traces, we find that models trained on real traces are stronger at modeling diverse student behavior. Through both behavioral and probing analyses, we also find that many properties of code traces, such as goal backtracking or number of comments, can be predicted from learned representations of the students who write them. Building on this result, we show that we can help students recover from mistakes by steering code generation models to identify a sequence of edits that will results in more correct code while remaining close to the original student’s style. Together, our results suggest that many properties of code are properties of individual students and that training on edit traces can lead to models that are both more steerable and predictive of student behavior, even when evaluated solely on final program states.", "tldr": "", "keywords": ["education", "coding", "interactive"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f831956be62f2b21643b08d540d3a2721f63daa6.pdf", "supplementary_material": "/attachment/a4608b9293806076576b402e782730659eafff16.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces a dataset of over 3.8 million programming reasoning traces from a free online educational platform. The authors develop and compare five model variants trained on this dataset: the trace model, last model, synthetic model, trace downsampled model, and synthetic downsampled model, which are evaluated from both behavioral and representational perspectives. They demonstrate that models trained on full traces acquire stronger representations of student coding behavior compared to models trained solely on synthetically generated traces or final program submissions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-motivated, and a decent amount of technical details are given.\n2. The idea of modeling students' coding behavior through intermediate traces is both interesting and practical."}, "weaknesses": {"value": "1. Insufficient dataset presentation\n2. Missing discussion of related work and evaluation metric\n3. Lacks user study\n4. Limited evaluation to outdated model (GPT-2)\n5. Code not provided"}, "questions": {"value": "1\\. **Concerns about the dataset presentation**\n\nA key contribution of this paper is the presented programming reasoning traces dataset. I suggest the authors add a dedicated section within the main text to thoroughly introduce the dataset's features and characteristics rather than placing this important information in the appendix. Additionally, providing an illustrative visualization of the dataset structure would help readers better grasp its organization and content.\n\n2\\. **Missing discussion of related work and evaluation metrics**\n\nFor the behavioral evaluation, the authors compare generated samples against actual student-written code. This objective semms similar to the work \"Open-ended Knowledge Tracing for Computer Science Education\" (EMNLP, 2022), which should be cited and discussed. Also, I suggest adopting CodeBLEU—a variant of the traditional BLEU metric specifically adapted for code—as suggested by this related work, as it would allow for a more accurate assessment of similarity between the predicted and actual student code.\n\n3\\. **User study**\n\nThe authors demonstrate that their trace model can help students recover from errors. I suggest that the authors conduct a user study in real educational settings to further validate this claim. Such an evaluation would provide valuable empirical evidence for the practical effectiveness of the proposed model.\n\n4\\. **Clarification on Figure 6 results**\n\nIn Figure 6, as the number of fine-tuning traces increases, the performance on trace generation appears to be lower compared to final program generation. Could the authors provide a more detailed analysis or explanation of this phenomenon?\n\n5\\. **Evaluation on more advanced language models**\n\nThe authors conduct experiments using base GPT-2 and OLMo-2 models. Given that GPT-2 is somewhat outdated, I suggest extending the evaluation to include more advanced models, such as those from the Llama series or other state-of-the-art LLMs, to further strengthen the generalizability of the findings.\n\n6\\. **Code and reproducibility**\n\nThe authors are encouraged to release the code to facilitate reproducibility and benefit the research community.\n\n7\\. **Typo**\n\nPage 4, line 202: \"a an\" → \"an\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The authors have adequately discussed the protection of student information in the manuscript, and there appear to be no ethical concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AAnVu8l3UR", "forum": "OndDxNGrqJ", "replyto": "OndDxNGrqJ", "signatures": ["ICLR.cc/2026/Conference/Submission19704/Reviewer_ncN7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19704/Reviewer_ncN7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761478256332, "cdate": 1761478256332, "tmdate": 1762931546650, "mdate": 1762931546650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a 3.8M programming trace dataset from Pencil Code and trains language models to capture student coding behavior, comparing models trained on real traces, synthetic traces, and final programs only. The focus on modeling \"how\" students code rather than just \"what\" they produce is interesting, but the scope is limited and the experimental section needs significant reorganization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The focus on \"how\" students code instead of just \"what\" they produce is a valuable perspective shift for modeling programming behavior.\n* The 3.8M trace dataset from real students over 9 years is substantial and could benefit the education and code generation community."}, "weaknesses": {"value": "* The base models (GPT-2 124M, OLMo-2 1B) are outdated. Modern models (such as qwen3, starcoder) would be more convincing baselines.\n* Line 132 mentions \"reported in Table 3\" but I cannot find Table 3 anywhere in the paper.\n* The entire work is based on one platform (Pencil Code) teaching \"simple programming concepts\" with visual blocks. This feels too narrow for ICLR. There is no evidence the findings generalize to other languages, platforms, or more complex programming tasks.\n* The citation format does not follow ICLR style. Please check the formatting guidelines.\n* Figure 3 has overlapping numbers that make it hard to read. Please fix the visualization.\n* The experimental results section is very hard to follow. There are too many sub-research questions (5 major sections, each with multiple questions) but they are not well-justified. For example, Section 4.1 asks \"Can models generate code that reflects real student programming behavior?\" but I don't understand why this matters. The model is still just generating programs, so what is the point? The later experiments on probing and adapting to new students are more interesting, but they get buried.\n* The paper tries to answer too many research questions at once. The authors should narrow down to 2-3 core questions and go deeper on those instead of spreading thin across many shallow analyses. More research questions does not equal a better paper."}, "questions": {"value": "see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4hy70TZdYu", "forum": "OndDxNGrqJ", "replyto": "OndDxNGrqJ", "signatures": ["ICLR.cc/2026/Conference/Submission19704/Reviewer_ngbA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19704/Reviewer_ngbA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723217160, "cdate": 1761723217160, "tmdate": 1762931545887, "mdate": 1762931545887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work involves a set of model training experiments on a large programming traces dataset from Pencil Code. Specifically, they used five models to model students' behaviors, and also investigated the representations of both code and students."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ It is a major contribution to show that, at a large scale, programming traces are useful for modeling students' programming."}, "weaknesses": {"value": "- The contribution of this paper is unclear. One key issue is that while it comes from an educational discipline, it does not have any task involving actual educational goals. The five models are all about student behaviors or representations of students or code, but what is the next step? There is almost no educational implication discussed in the work.\n- There are some key claims counterintuitive for general machine learning tasks. One of the biggest issues is about student embedding -- how exactly can we expect a model learned with student IDs to be generalizable for future new students? There are discussions about the result when new students are involved, and the result says \"generalization is still difficult\" -- this is almost certain, even for a large language model now trained with a lot of data. If you ask GPT-4 who a student is, it likely won't give you any good idea. The power of generalizability cannot help with tasks like overfitting to IDs."}, "questions": {"value": "- Line 89: Why is this large dataset suitable for language model training? For small language models, smaller datasets could also work, especially if we want to create models for specific contexts.\n- Line 94: The requirement and context of learning will be very important for the final program states. In classroom settings, students' final program states are almost all correct, while in situations of informal learning, there's often a lack of motivation for students to finish programming for many. This is actually not a minor issue -- context is very important for educational applications and this is missing.\n- Line 99: We cannot train from IDs, but can check about certain classes or sessions.\n- Line 118: So what is the goal? Education happens in a certain context, and it will need to show it surpasses small models in their own context to make sense. Otherwise we can always use smaller models trained in specific contexts.\n- Line 130: While training LMs are important, it is still important to show what exactly will be a good downstream educational task.\n- Line 140: I don't get this -- student IDs are involved in training and in this case, any new student will be an unknown input."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CxWpoAGkgC", "forum": "OndDxNGrqJ", "replyto": "OndDxNGrqJ", "signatures": ["ICLR.cc/2026/Conference/Submission19704/Reviewer_fQu1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19704/Reviewer_fQu1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753553728, "cdate": 1761753553728, "tmdate": 1762931545179, "mdate": 1762931545179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an analysis of a dataset of 3.8 million code editing traces.\nThese traces are taken from PencilCode, which is a web-based code editing\nplatform focused on education. PencilCode allows the user to read and edit code\nin both textual and graphical form, and seamlessly switch between the two.\nHowever, this paper focuses on the textual representation.\n\nThe paper performs continuous pre-training or fine-tuning (you can argue which)\nof GPT2-124M and Olmo-1B using the trace dataset. Each training item is a\nsequence of code edits, along with certain metadata such as student ID. The\npaper ablates the training data format: using synthetic traces (assuming each\nstep adds a line), using the ground-truth traces, and using just the final\nprogram. The natural traces perform best on several days. The tasks considered\ninclude getting the trained models to correct errors in student traces (i.e.,\ncompleting a student trace to be correct), predicting the program title from the\ntrace, etc."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents a dataset that is potentially very interesting. However, I\n  believe there is no plan to release the dataset publicly."}, "weaknesses": {"value": "The primary weakness of this paper is that it is missing several obvious\nbaselines that involve prompting pretrained models (e.g., any open-weight model\nthat is 32B+ or even a proprietary model). Since the traces involve program\nexecution in JavaScript and CoffeeScript, I imagine that a reasonable pretrained\nmodel will pick-up enough in-context signals given the trace and a reasonable\nprompt. I thought the most interesting task in the paper was on L428, where the\nfine-tuned model completes a prefix of a student-written trace that ended in\nfailure with a successful trace 60% of the time. I expect that if you give a\nbroken program or trace to a reasonable pretrained model, it will identify and\nfix the bug at least as well. I don't expect a pretrained model to be good at\nprobing student representations, but it's worth asking if they can do the other\ncode representation tasks. E.g., asking \"will a student backtrack\" is similar to\nasking \"is there a bug\".\n\nI also think this paper needs to do a better job engaging with related work.\nThere is enormous interest in studying how students learn to code, with\nand without LLMs:\n\n- BlueJ Blackbox (ICER 2018) has very detailed logs of edit actions. The\n  ICER papper lists 18 papers that use the dataset.\n- FalconCode: https://huggingface.co/datasets/koutch/falcon_code\n- StudentEval (this is LLM related): https://huggingface.co/datasets/wellesley-easel/StudentEval\n\nThe datasets above are either open or relatively easy to get access to."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BFhvBGW109", "forum": "OndDxNGrqJ", "replyto": "OndDxNGrqJ", "signatures": ["ICLR.cc/2026/Conference/Submission19704/Reviewer_s2Vq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19704/Reviewer_s2Vq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920228350, "cdate": 1761920228350, "tmdate": 1762931544628, "mdate": 1762931544628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}