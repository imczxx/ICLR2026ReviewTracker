{"id": "r2g8TV4nJy", "number": 12098, "cdate": 1758205675047, "mdate": 1763662276105, "content": {"title": "DriveAgent-R1: Advancing VLM-based Autonomous Driving with Active Perception and Hybrid Thinking", "abstract": "The advent of Vision-Language Models (VLMs) has significantly advanced end-to-end autonomous driving, demonstrating powerful reasoning abilities for high-level behavior planning tasks. However, existing methods are often constrained by a passive perception paradigm, relying solely on text-based reasoning. This passivity restricts the model’s capacity to actively seek crucial visual evidence when faced with uncertainty. To address this, we introduce DriveAgent-R1, the first autonomous driving agent capable of active perception for planning. In complex scenarios, DriveAgent-R1 proactively invokes tools to perform visual reasoning, firmly grounding its decisions in visual evidence, thereby enhancing both interpretability and reliability. Furthermore, we propose a hybrid thinking framework, inspired by human driver cognitive patterns, allowing the agent to adaptively switch between efficient text-only reasoning and robust tool-augmented visual reasoning based on scene complexity. This capability is cultivated through a three-stage progressive training strategy, featuring a core Cascaded Reinforcement Learning (Cascaded RL) phase. Extensive experiments on the Drive-Internal dataset, which is rich in long-tail scenarios, and the public nuScenes dataset show that, with only 3B parameters, DriveAgent-R1 achieves competitive performance comparable to top closed model systems such as GPT-5 and to human driving proficiency while remaining deployment-friendly, offering a proven path toward building more intelligent autonomous driving systems.", "tldr": "", "keywords": ["Autonomous Driving", "Vision-Language Models", "Hybrid Thinking", "Visual Reasoning", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4825ebd1a5162a904fecbd7e7f156011ed4249f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DriveAgent-R1, a vision-language-reasoning framework designed to handle complex, long-tail driving scenarios through hybrid thinking. The core idea is to let the model actively retrieve task-relevant information from tools or external sources to enhance its reasoning about rare or unseen driving cases.\n\nThe paper also presents a new QA dataset focused on long-tail driving events, covering diverse edge cases such as unusual pedestrian behavior, obstructed lanes, or ambiguous traffic signals. The authors show that the model’s ability to retrieve external information makes a big difference: without it, performance drops notably (as seen in Table 5)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a new hybrid-thinking framework, which combines internal reasoning with external information retrieval. The experiments clearly show that the active retrieval step helps — when it’s removed, performance drops quite a bit (Table 5).\n\nThe authors also built a large QA dataset targeting long-tail driving cases. Once released, this dataset should be valuable to the research community, since long-tail reasoning data is still pretty rare for driving tasks."}, "weaknesses": {"value": "The biggest issue here is the choice of baselines. The paper only compares against generalist LLMs like GPT, Gemini, and Qwen. These are not trained for driving at all, so this is really an out-of-domain test for them. DriveAgent-R1, on the other hand, is trained on driving-specific data, so it’s expected to do better. The comparison doesn’t tell us how much progress this paper actually makes within the driving domain.\nIn DriveVLM, for instance, the authors showed that a domain-specific model can outperform GPT by around 90% — so the marginal improvements here aren’t that surprising. For a fair evaluation, DriveAgent-R1 should be compared against specialized models like DriveLM, DOLPHINS, DriveVLM, or similar. Without that, it’s hard to judge the real impact of this model or dataset.\n\nThe evaluation focus is also a bit off. The stated goal is to improve driving and planning, but the experiments don’t report any driving metrics like Displacement Error (DE) or Collision Rate (CR). Other VLM-based systems such as DriveVLM, OpenDriveVLA, and OmniDrive all evaluate directly on these metrics to show actual improvements in control or planning.\nHere, DriveAgent-R1 is mostly tested on reasoning or QA accuracy, not on whether it leads to better driving behavior. That leaves a big gap — it’s unclear if this approach really translates into improved driving performance."}, "questions": {"value": "Are there any plans to evaluate DriveAgent-R1 on driving metrics like DE or CR? Without that, it’s hard to tell whether the improvements in reasoning actually help with driving.\n\nCan the authors provide comparisons with domain-specific baselines (e.g., DriveVLM, DriveLM, DOLPHINS)? That would make the results much more meaningful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HUwZBTY9nT", "forum": "r2g8TV4nJy", "replyto": "r2g8TV4nJy", "signatures": ["ICLR.cc/2026/Conference/Submission12098/Reviewer_LB8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12098/Reviewer_LB8N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761248832761, "cdate": 1761248832761, "tmdate": 1762923065376, "mdate": 1762923065376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DriveAgent-R1, an autonomous driving agent that incorporates active perception and a hybrid-thinking framework for high-level driving planning. In contrast to previous Vision-Language Models (VLMs), which are limited by passive perception, DriveAgent-R1 dynamically interacts with visual tools to resolve uncertainties in complex driving scenarios. DriveAgent-R1 achieves competitive performance with top models like GPT-5, human drivers, and other VLM-based systems, using only 3B parameters while maintaining deployment efficiency. It outperforms models using passive perception in scenarios requiring active visual inspection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces active perception to autonomous driving planning, an important advancement beyond the passive perception typically used in VLM-based systems. The integration of tool-based M-CoT for real-time visual reasoning represents a novel approach in this field.\n2. The methodology and results are presented in a clear, structured manner, with helpful diagrams and an effective explanation of the agent's reasoning process. The use of visual tools (such as the Vision Toolkit) for active perception is well-described and easy to follow.\n3. The hybrid-thinking framework is well-designed, allowing the agent to switch between text-based reasoning and tool-augmented reasoning. The progressive training strategy is a strong contribution, effectively training the agent to adapt its thinking mode based on scene complexity."}, "weaknesses": {"value": "1. There is insufficient statistical validation such as confidence intervals or multiple trials. The reported improvements (e.g., 6.07% gain) would be more convincing with more rigorous statistical analysis.\n2. While the Cascaded RL strategy is an interesting approach, the reward design and training stages could be more clearly explained, especially for readers unfamiliar with GRPO. Additionally, the computational cost of such a training strategy could be a concern for real-world deployment."}, "questions": {"value": "1. The performance drops significantly without access to visual tools. How would DriveAgent-R1 handle sensor failures, such as loss of camera feed or occlusions? How does the agent perform when only partial visual inputs are available?\n2. While the results on the Drive-Internal and nuScenes datasets are promising, do you have plans to evaluate the model on other benchmarks or real-world driving scenarios to further validate its generalization capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "82Su8DRBtF", "forum": "r2g8TV4nJy", "replyto": "r2g8TV4nJy", "signatures": ["ICLR.cc/2026/Conference/Submission12098/Reviewer_YkZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12098/Reviewer_YkZV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838331279, "cdate": 1761838331279, "tmdate": 1762923064542, "mdate": 1762923064542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DriveAgent-R1, an autonomous driving agent that incorporates active perception and a hybrid-thinking framework for high-level driving planning. In contrast to previous Vision-Language Models (VLMs), which are limited by passive perception, DriveAgent-R1 dynamically interacts with visual tools to resolve uncertainties in complex driving scenarios. DriveAgent-R1 achieves competitive performance with top models like GPT-5, human drivers, and other VLM-based systems, using only 3B parameters while maintaining deployment efficiency. It outperforms models using passive perception in scenarios requiring active visual inspection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces active perception to autonomous driving planning, an important advancement beyond the passive perception typically used in VLM-based systems. The integration of tool-based M-CoT for real-time visual reasoning represents a novel approach in this field.\n2. The methodology and results are presented in a clear, structured manner, with helpful diagrams and an effective explanation of the agent's reasoning process. The use of visual tools (such as the Vision Toolkit) for active perception is well-described and easy to follow.\n3. The hybrid-thinking framework is well-designed, allowing the agent to switch between text-based reasoning and tool-augmented reasoning. The progressive training strategy is a strong contribution, effectively training the agent to adapt its thinking mode based on scene complexity."}, "weaknesses": {"value": "1. There is insufficient statistical validation such as confidence intervals or multiple trials. The reported improvements (e.g., 6.07% gain) would be more convincing with more rigorous statistical analysis.\n2. While the Cascaded RL strategy is an interesting approach, the reward design and training stages could be more clearly explained, especially for readers unfamiliar with GRPO. Additionally, the computational cost of such a training strategy could be a concern for real-world deployment."}, "questions": {"value": "1. The performance drops significantly without access to visual tools. How would DriveAgent-R1 handle sensor failures, such as loss of camera feed or occlusions? How does the agent perform when only partial visual inputs are available?\n2. While the results on the Drive-Internal and nuScenes datasets are promising, do you have plans to evaluate the model on other benchmarks or real-world driving scenarios to further validate its generalization capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "82Su8DRBtF", "forum": "r2g8TV4nJy", "replyto": "r2g8TV4nJy", "signatures": ["ICLR.cc/2026/Conference/Submission12098/Reviewer_YkZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12098/Reviewer_YkZV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838331279, "cdate": 1761838331279, "tmdate": 1763693528050, "mdate": 1763693528050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DriveAgent-R1, a 3 B-parameter vision-language agent that introduces two main innovations for end-to-end autonomous-driving reasoning:\n(1) an active-perception framework, where the model invokes a Vision Toolkit (Retrieve View, RoI Inspection, Depth Estimation, 3 D Object Detection) to gather additional visual evidence when uncertainty arises; and (2) a hybrid-thinking mechanism that adaptively switches between text-only reasoning and tool-augmented multimodal reasoning.\nThe system is trained through a three-stage progressive pipeline – Dual-Mode SFT, Forced Contrastive Mode RL (FCM-RL), and Adaptive Mode Selection RL (AMS-RL) – using a Drive-Internal dataset (35 k clips) and nuScenes.\nExperiments show that DriveAgent-R1 achieves human- and GPT-5-level planning accuracy with lower latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a novel framework where the agent proactively gathers visual evidence via a Vision Toolkit, improving interpretability and robustness.\n\n2. Proposes an adaptive reasoning mechanism and a well-designed three-stage training pipeline that balances text and tool reasoning effectively.\n\n3. Achieves near GPT-5 and human-level performance with only 3B parameters, supported by thorough ablations and clear presentation.\n\n4. The paper is clearly written with reasonable experimental design, the overall results show significantly improvement. And the adaptive method for reasoning and calling tools is also novel."}, "weaknesses": {"value": "1. **Possible over-claiming of novelty.** Several contemporaneous works already explore tool-augmented or hybrid multimodal CoT reasoning (e.g., AgentThink (Qian et al., 2025), DeepEyes (Zheng et al., 2025)). While DriveAgent-R1 applies this to high-level driving planning, the “first active-perception framework” claim should be softened.\n\n2. **Limited evaluation scope.**  Tests focus on two datasets (Drive-Internal and nuScenes); closed-loop or real-world driving performance is not analyzed. Comparisons to other active-sensing or attention-based perception approaches (e.g., DriveLM, LightEMMA) are missing.\n\n3. **Data and metric ambiguities.**  The construction of Drive-Internal (35 k clips) and the VQA dataset (530 k QA pairs) is only partly described; potential data leakage or bias is unexamined."}, "questions": {"value": "1. Could authors provide quantitative examples where hybrid thinking (adaptive mode) clearly outperforms either forced text or forced tool modes?\n\n2. How is scene complexity estimated before choosing \"think_text\" vs \"think_tool\"? Is it a learned token decision or a heuristic?\n\n3. What are the inference-time compute costs of the Vision Toolkit relative to full multi-view pipelines?\n\n4. How are erroneous or redundant tool calls penalized during RL training – is R_tool margin tuned empirically?\n\n5. Can the authors clarify what “human-driver accuracy ≈ 50 %” represents (expert annotation or real trajectories)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b7yHCbOufu", "forum": "r2g8TV4nJy", "replyto": "r2g8TV4nJy", "signatures": ["ICLR.cc/2026/Conference/Submission12098/Reviewer_ZEQw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12098/Reviewer_ZEQw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037900580, "cdate": 1762037900580, "tmdate": 1762923064121, "mdate": 1762923064121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DriveAgent-R1, a 3 B-parameter vision-language agent that introduces two main innovations for end-to-end autonomous-driving reasoning:\n(1) an active-perception framework, where the model invokes a Vision Toolkit (Retrieve View, RoI Inspection, Depth Estimation, 3 D Object Detection) to gather additional visual evidence when uncertainty arises; and (2) a hybrid-thinking mechanism that adaptively switches between text-only reasoning and tool-augmented multimodal reasoning.\nThe system is trained through a three-stage progressive pipeline – Dual-Mode SFT, Forced Contrastive Mode RL (FCM-RL), and Adaptive Mode Selection RL (AMS-RL) – using a Drive-Internal dataset (35 k clips) and nuScenes.\nExperiments show that DriveAgent-R1 achieves human- and GPT-5-level planning accuracy with lower latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a novel framework where the agent proactively gathers visual evidence via a Vision Toolkit, improving interpretability and robustness.\n\n2. Proposes an adaptive reasoning mechanism and a well-designed three-stage training pipeline that balances text and tool reasoning effectively.\n\n3. Achieves near GPT-5 and human-level performance with only 3B parameters, supported by thorough ablations and clear presentation.\n\n4. The paper is clearly written with reasonable experimental design, the overall results show significantly improvement. And the adaptive method for reasoning and calling tools is also novel."}, "weaknesses": {"value": "1. **Possible over-claiming of novelty.** Several contemporaneous works already explore tool-augmented or hybrid multimodal CoT reasoning (e.g., AgentThink (Qian et al., 2025), DeepEyes (Zheng et al., 2025)). While DriveAgent-R1 applies this to high-level driving planning, the “first active-perception framework” claim should be softened.\n\n2. **Limited evaluation scope.**  Tests focus on two datasets (Drive-Internal and nuScenes); closed-loop or real-world driving performance is not analyzed. Comparisons to other active-sensing or attention-based perception approaches (e.g., DriveLM, LightEMMA) are missing.\n\n3. **Data and metric ambiguities.**  The construction of Drive-Internal (35 k clips) and the VQA dataset (530 k QA pairs) is only partly described; potential data leakage or bias is unexamined."}, "questions": {"value": "1. Could authors provide quantitative examples where hybrid thinking (adaptive mode) clearly outperforms either forced text or forced tool modes?\n\n2. How is scene complexity estimated before choosing \"think_text\" vs \"think_tool\"? Is it a learned token decision or a heuristic?\n\n3. What are the inference-time compute costs of the Vision Toolkit relative to full multi-view pipelines?\n\n4. How are erroneous or redundant tool calls penalized during RL training – is R_tool margin tuned empirically?\n\n5. Can the authors clarify what “human-driver accuracy ≈ 50 %” represents (expert annotation or real trajectories)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b7yHCbOufu", "forum": "r2g8TV4nJy", "replyto": "r2g8TV4nJy", "signatures": ["ICLR.cc/2026/Conference/Submission12098/Reviewer_ZEQw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12098/Reviewer_ZEQw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037900580, "cdate": 1762037900580, "tmdate": 1763667340430, "mdate": 1763667340430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DriveAgent-R1, an autonomous driving agent with active perception for planning. It can dynamically switch between text-only reasoning and tool-augmented visual reasoning based on scene complexity, improving interpretability and reliability. Experiments show it achieves promising performance with only 3B parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n\n\n2. The experimental results are promising"}, "weaknesses": {"value": "1. One concern is the missing of any closed-loop experiments. Since the entire pipeline (Stages 1–3) is trained on a specific dataset, once a scenario is classified as “not needing tools,” the model will no longer invoke them, regardless of the ego’s subsequent actions. I wonder if the model would make different decisions (e.g., switching between using and not using tools) if the ego took alternative maneuvers.\n\n2. I suggest that the authors include a discussion of recent studies on VLM-generated datasets for autonomous driving. For example, works[1][2] use VLMs to produce new annotations through predefined prompts or questions, which are closely related to this paper’s dataset generation strategy.\n\n[1] Y. Xu et al., “VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision,” CoRL, 2025.  \n[2] Z. Zhou et al., “AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning,” NeurIPS, 2025.\n\n3. Another concern relates to the use of different tools described in Section 2.1. These tools seem to offer no new information, they mainly act as passive “visual prompts,” such as cropping or highlighting key regions. How does this differ from simply providing the original image and prompting the model with instructions like “Please focus on the top-right region” or “Please focus on the grey van in the center”? It would be interesting to see a comparison of these approaches, as it might reduce the need for a complex tool-selection module.\n\n4. The paper uses only the front-view image in its pipeline. While this may be sufficient for relatively simple scenarios like those in nuScenes (which rarely include U-turns, reverse driving, or complex lane changes), it could limit generalization to more challenging cases. Without additional camera views, even human drivers would struggle with tasks such as merging or lane changing."}, "questions": {"value": "1. How was the quality of the new dataset ensured? In related works, human evaluation or questionnaires are often used to validate annotations.\n\n\n2. Did the authors encounter imperfect labels during annotation? In my experience, VLMs often struggle with temporal grounding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pG4ikFs3be", "forum": "r2g8TV4nJy", "replyto": "r2g8TV4nJy", "signatures": ["ICLR.cc/2026/Conference/Submission12098/Reviewer_XEzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12098/Reviewer_XEzY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159820396, "cdate": 1762159820396, "tmdate": 1762923063771, "mdate": 1762923063771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DriveAgent-R1, an autonomous driving agent with active perception for planning. It can dynamically switch between text-only reasoning and tool-augmented visual reasoning based on scene complexity, improving interpretability and reliability. Experiments show it achieves promising performance with only 3B parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n\n\n2. The experimental results are promising"}, "weaknesses": {"value": "1. One concern is the missing of any closed-loop experiments. Since the entire pipeline (Stages 1–3) is trained on a specific dataset, once a scenario is classified as “not needing tools,” the model will no longer invoke them, regardless of the ego’s subsequent actions. I wonder if the model would make different decisions (e.g., switching between using and not using tools) if the ego took alternative maneuvers.\n\n2. I suggest that the authors include a discussion of recent studies on VLM-generated datasets for autonomous driving. For example, works[1][2] use VLMs to produce new annotations through predefined prompts or questions, which are closely related to this paper’s dataset generation strategy.\n\n[1] Y. Xu et al., “VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision,” CoRL, 2025.  \n[2] Z. Zhou et al., “AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning,” NeurIPS, 2025.\n\n3. Another concern relates to the use of different tools described in Section 2.1. These tools seem to offer no new information, they mainly act as passive “visual prompts,” such as cropping or highlighting key regions. How does this differ from simply providing the original image and prompting the model with instructions like “Please focus on the top-right region” or “Please focus on the grey van in the center”? It would be interesting to see a comparison of these approaches, as it might reduce the need for a complex tool-selection module.\n\n4. The paper uses only the front-view image in its pipeline. While this may be sufficient for relatively simple scenarios like those in nuScenes (which rarely include U-turns, reverse driving, or complex lane changes), it could limit generalization to more challenging cases. Without additional camera views, even human drivers would struggle with tasks such as merging or lane changing."}, "questions": {"value": "1. How was the quality of the new dataset ensured? In related works, human evaluation or questionnaires are often used to validate annotations.\n\n\n2. Did the authors encounter imperfect labels during annotation? In my experience, VLMs often struggle with temporal grounding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pG4ikFs3be", "forum": "r2g8TV4nJy", "replyto": "r2g8TV4nJy", "signatures": ["ICLR.cc/2026/Conference/Submission12098/Reviewer_XEzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12098/Reviewer_XEzY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159820396, "cdate": 1762159820396, "tmdate": 1763677253555, "mdate": 1763677253555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}