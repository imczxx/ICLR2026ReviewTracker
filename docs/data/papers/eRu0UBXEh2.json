{"id": "eRu0UBXEh2", "number": 23815, "cdate": 1758348813215, "mdate": 1759896795704, "content": {"title": "Hypergraph-Native Message Passing: An Incidence-Centric Perspective", "abstract": "While hypergraphs encapsulate higher-order interactions among entities and transcend the pairwise connections characteristic of traditional graphs, their prevailing learning approaches predominantly inherit from graph neural networks, adhering to the established message passing paradigm.\nThese methods frequently conceptualizes hyperedges as special nodes, facilitating the transmission of aggregated messages through hyperedges instead of direct messages between adjacent nodes.\nSuch a paradigm is prone to information loss, especially in the context of large hyperedges that bridge a heterophilic array of nodes.\nTo mitigate this shortcoming and enhance high-order message passing, we propose the Hypergraph-native Message Passing (HMP) framework, which leverages full-rank interactions among the incidences along the underlying hypergraph and its dual.\nIn contrast to the conventional node-centric approaches, this incidence-centric perspective adeptly manages incidence-level tasks, such as hyperedge-dependent labelling, and seamlessly integrates virtual incidences for both hyperedge- and node-level tasks.\nEmpirical evaluations demonstrate that HMP achieves a substantial improvement over state-of-the-art methods on 6 hyperedge-dependent labelling benchmarks, with an increase in accuracy ranging from 2.3% to 28.9%, while also delivering competitive results on 13 node classification benchmarks.\nCode to reproduce all our experiments is available.", "tldr": "A state-of-the-art hypergraph learning method that exploits the high-order structures of hypergraphs for incidence-, hyperedge-, and node-leval tasks", "keywords": ["Hypergraph Neural Networks", "Message Passing", "Hyperedge-Dependent Labelling", "Node Classification"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bfba690ddc4f7fc4dfd15ecdb0e80e657cc1af2c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Hypergraph-native Message Passing (HMP) framework for learning on hypergraphs. The central motivation is to overcome the \"information squashing\" problem that affects many existing Hypergraph Neural Networks (HNNs) based on the star expansion. The authors substantiate the framework's efficacy with both theoretical proofs and empirical validation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe manuscript is well-written. The intuition of the \"information squashing\" problem is explained clearly, which makes the paper's goals easy to understand.\n\n2.\tExperimental validation demonstrates that the proposed method can outperform many baseline methods on the selected benchmarks."}, "weaknesses": {"value": "1.\tMy primary concern is regarding the claimed novelty of the \"incidence-centric\" idea, which appears to have significant overlap with established previous works. Specifically, the core concept strongly resembles the line expansion (LE) approach [1] and, even more so, the co-representation learning (CoNHD) framework [2]. The paper's dismissal of LE for treating the converted graph as \"homogeneous\" seems superficial, as LE constructs a well-defined graph of (vertex, hyperedge) pairs with a clear structure. Furthermore, HMP shares nearly identical motivations and high-level solutions with CoNHD, which also models interactions as \"multi-input multi-output functions\" to avoid information loss. The paper's main attempt to distinguish itself, by critiquing CoNHD's specific SetTransformer implementation as less adaptive, focuses on a low-level implementation choice rather than a fundamental difference in the learning paradigm. This positions HMP less as a novel framework and more as an alternative (e.g., self-attention-based) implementation of the CoNHD paradigm, which diminishes the claimed conceptual contribution.\n\n2.\tFollowing the first point, the paper's central claim of achieving \"Adaptive Dimensions\" as a key advantage over CoNHD is vague and not well-substantiated. The authors contend that their self-attention mechanism is inherently more adaptive to hyperedge size than CoNHD's SetTransformer. However, this argument seems flawed, as standard multi-head self-attention (used in HMP) also aggregates inputs into a fixed-size output representation, with its 'adaptiveness' coming from learned attention weights. This weighting principle is also central to the SetTransformer. It is therefore unclear how HMP offers a fundamentally more adaptive representation capacity, and this core claimed benefit remains unconvincing.\n\n3.\tThe empirical analysis feels incomplete and would be strengthened by addressing two key omissions. First, while Theorem 2 commendably identifies the high computational complexity per hyperedge, the proposed solution, referencing efficient transformers, is presented without any empirical validation. The paper would be much more convincing if it included experiments on runtime and scalability to demonstrate the practical viability of this approach. Second, the paper reports that HMP underperforms on key homophilic benchmarks (Cora, Citeseer, Pubmed) but then offers no analysis for this important negative result. A detailed discussion is required to explain why the proposed message-passing mechanism struggles in these standard settings, as this insight is crucial for understanding the model's limitations and applicability.\n\n[1] Yang, C., Wang, R., Yao, S., & Abdelzaher, T. (2022, October). Semi-supervised hypergraph node classification on hypergraph line expansion. In Proceedings of the 31st ACM international conference on information & knowledge management (pp. 2352-2361).\n\n[2] Zheng, Y., & Worring, M. (2024). Co-Representation Neural Hypergraph Diffusion for Edge-Dependent Node Classification. arXiv preprint arXiv:2405.14286."}, "questions": {"value": "1.\tCould you clarify the fundamental theoretical difference between HMP's learning paradigm and that of LE and CoNHD, beyond the specific implementation choice?\n\n2.\tGiven that both multi-head self-attention and SetTransformers aggregate inputs to a fixed-size output, what is the specific mechanism in HMP that enables a fundamentally more \"adaptive\" representation capacity?\n\n3.\tCan you provide the empirical runtime and scalability experiments that validate your claim that efficient transformers are a practical solution to the complexity identified in Theorem 2?\n\n4.\tWhat is your analysis for why the proposed HMP message-passing mechanism underperforms on key homophilic benchmarks like Cora, Citeseer, and Pubmed?\n\n5.\tCould you elaborate on how your critique of LE as \"homogeneous\" holds, given its well-defined bipartite structure of (vertex, hyperedge) pairs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VTTgJxWm06", "forum": "eRu0UBXEh2", "replyto": "eRu0UBXEh2", "signatures": ["ICLR.cc/2026/Conference/Submission23815/Reviewer_RCRj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23815/Reviewer_RCRj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709348088, "cdate": 1761709348088, "tmdate": 1762942818796, "mdate": 1762942818796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interesting way of thinking for hypergraph learning.  Instead of the usual star expansion, hypergraph native message passing is introduced to pass messages directly between node hyperedge pairs, to avoid information squashing, which is further shown to be generalisation of prior work like AllSet, with gains on extensive tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core idea is interesting. The incidence-centric perspective is more natural fit for hypergraphs. The experiment results are promising."}, "weaknesses": {"value": "The theory says complexity scales with the size of the largest hyperedge, what will happen in practice with a dataset that has massive hyperedges? A short discussion or experiment on this would be more convincing."}, "questions": {"value": "Could you provide practical guidance on when to use virtual node incidences or hyperedge incidences?\n\nWill the model performance degreade as the hyperedge become extremely large and if there is a point that self attention within a hyperedge become the bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t7aw2kTMqy", "forum": "eRu0UBXEh2", "replyto": "eRu0UBXEh2", "signatures": ["ICLR.cc/2026/Conference/Submission23815/Reviewer_a6qN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23815/Reviewer_a6qN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827684853, "cdate": 1761827684853, "tmdate": 1762942818572, "mdate": 1762942818572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A1. Common Response: Detailed Differences Between HMP and Line Expansion (LE) [1]"}, "comment": {"value": "We note that \"LE treats the converted graph as homogeneous\" (as stated in line 294 of our Related Works) is directly supported by LE's original description, specifically the third paragraph in the Introduction section of [1]:\n\n> We propose line expansion (LE) for hypergraphs, which is a powerful bijective mapping from a hypergraph structure to a **homogeneous** graph structure.\n\nThe reason for this homogeneity is explained in the fourth paragraph of [1]:\n\n> “edge” between two \"node\"s are constructed if two \"node\"s share the same vertex **or** hyperedge.\n\nTo illustrate, consider Figure 2c in our manuscript, which uses incidences as nodes. The red dashed lines (between incidences sharing hyperedges) and black solid lines (sharing nodes) are two distinct types of edges. Recent methods such as AllSet, CoNHD [2], and our proposed HMP all distinguish between these two edge types, for example, by using different rules to propagate information from incidence c2 to incidences c1 and a2, respectively. However, LE fails to distinguish between incidences that share nodes and those that share hyperedges, leading to irreversible loss of hypergraph structural information during message passing. (This structural information is stored in an additional vertex projection matrix but does not participate in message passing.) This is also reflected in our experimental results on Hyperchains, where the LEGCN model performs the worst.\n\n> * [1] Yang, C., Wang, R., Yao, S., & Abdelzaher, T. (2022, October). Semi-supervised hypergraph node classification on hypergraph line expansion. In Proceedings of the 31st ACM international conference on information & knowledge management (pp. 2352-2361).\n> * [2] Zheng, Y., & Worring, M. (2024). Co-Representation Neural Hypergraph Diffusion for Edge-Dependent Node Classification. arXiv preprint arXiv:2405.14286."}}, "id": "Rh9E07w4a8", "forum": "eRu0UBXEh2", "replyto": "eRu0UBXEh2", "signatures": ["ICLR.cc/2026/Conference/Submission23815/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23815/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23815/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763096606854, "cdate": 1763096606854, "tmdate": 1763096606854, "mdate": 1763096606854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Hypergraph-native Message Passing (HMP), a novel framework for learning on hypergraphs. The authors argue that existing Hypergraph Neural Networks (HNNs), which often adapt graph neural network (GNN) paradigms, suffer from information squashing. HMP proposes an incidence-centric perspective, instead of the traditional 'node-hyperedge-node' path."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper clearly identifies a key limitation in existing HNNs, i.e., the \"information squashing\" bottleneck. Doing the same kind of incidence exchange on the dual hypergraph is elegant and makes the framework naturally handle hyperedge-centric signals as well. The synthetic hyperchains are set up specifically to test whether a method can preserve higher-order paths without over-squashing. HMP is indeed the most robust"}, "weaknesses": {"value": "The primary concern is computational cost. While they mention using linear-complexity attention and parallelization, this feels like a partial solution. For hypergraphs with very large hyperedges, this quadratic cost within every hyperedge, every layer, could be prohibitive compared to the simple aggregation in methods like AllSet."}, "questions": {"value": "Could the authors elaborate on the practical runtime/memory trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "evxXOEoRHu", "forum": "eRu0UBXEh2", "replyto": "eRu0UBXEh2", "signatures": ["ICLR.cc/2026/Conference/Submission23815/Reviewer_N534"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23815/Reviewer_N534"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948831007, "cdate": 1761948831007, "tmdate": 1762942818326, "mdate": 1762942818326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A2. Common Response: Detailed Differences Between HMP and CoNHD [2]"}, "comment": {"value": "HMP and CoNHD are independent concurrent works. HMP breaks through the fixed-dimensional limitation of CoNHD through dynamic self-attention and constructs a complete research system covering \"mechanism-framework-extension\".\n\n**First**, HMP and CoNHD are concurrent works. CoNHD was accepted by CIKM 2025 and released its source code in August this year, remaining in arXiv preprint status before that. HMP was submitted to KDD in February this year with simultaneous open-sourcing of code (as evidenced by the anonymous repository in our abstract, where our complete code was made open-source as early as February 13, 2025), at which point we were unaware of the preprint CoNHD. Later, following reviewer suggestions, we added comparisons with CoNHD and extensive theoretical analyses before submitting to ICLR 2026. Thus, the research timelines and code release dates of both works confirm that HMP's core ideas were independently developed. The overlap with CoNHD is a natural coincidence in academic exploration and does not affect its innovativeness.\n\n**Second**, the self-attention mechanism in HMP is not a trivially replaceable module; its implementation is crucial to the viability of the incidence-centric idea, whereas CoNHD's implementation (e.g., ISAB) is insufficient to support this idea. CoNHD's implementation details can be found in its [Appendix C](https://arxiv.org/pdf/2405.14286v2): \n\n> ISAB utilizes a fixed number of inducing points $\\mathbf{I} \\in \\mathcal{R}^{k \\times d}$ to reduce the quadratic complexity in self-attention to linear complexity.\n\nISAB is formally expressed as:\n\n$$\\begin{aligned}\n\\text{ISAB}(X) &= \\text{MAB}(X, \\text{MAB}(\\mathbf{W}^I, X)) \\\\\\\\\n\\text{MAB}(Q, K) &= \\text{LN}(M + \\text{REF}(M)) \\\\\\\\\nM &= \\text{LN}(Q, \\text{MultiHead}(Q, K, K))\n\\end{aligned}$$\n\nIf we regard the inducing points $\\mathbf{I}$ as $k$ added nodes in the hypergraph, $\\text{MAB}(\\mathbf{W}^I, X)$ propagates information from the considered nodes (with $X$) to the $k$ inducing nodes, and the second MAB in ISAB propagates information back from the $k$ inducing nodes to the considered nodes, resulting in a message passing paradigm as shown in the following figure (when $k=4$ as illustrated):\n\n![](https://github.com/user-attachments/assets/7df9a0cc-0424-4dae-b24e-c0670f3296b8)\n\nHere, the first MAB propagates messages from $v_1, v_2, v_3, v_4$ to the $k=4$ inducing nodes $I_1, I_2, I_3, I_4$, and the second MAB propagates messages from $I$ back to $v_1, v_2, v_3, v_4$. As noted in our Related Works (line 299), while ISAB alleviates the information bottleneck issue, its use of a fixed number of inducing points (with $k$ being a global hyperparameter, defaulting to only 4 according to its [code](https://github.com/zhengyijia/CoNHD/blob/main/model/CoNHD_ADMM.py#L86)) actually undermines its claimed adaptiveness, as the representation size cannot adjust with hyperedge size. Moreover, message passing on star expansion (MP on SE) methods only require increasing the intermediate representation size (e.g., expanding the representation size of node $a$ in Figure 1b to match the $k \\times d$ dimension of inducing points) to achieve results comparable to CoNHD with ISAB. Thus, CoNHD with ISAB is not more expressive than AllSet or HMP. In summary, although CoNHD, like HMP, identifies the information squashing problem in MP on SE, its implementation still reverts to the MP on SE framework. In contrast, HMP truly realizes an incidence-centric solution, achieving significantly better performance than CoNHD in incidence-level tasks (Table 1).\n\n**Third**, unlike CoNHD, which focuses on simply solving hyperedge-dependent labelling tasks, HMP conducts comprehensive research around the incidence-centric idea, including:\n\n1. Providing theoretical and empirical support for the rationality of the incidence-centric paradigm through s-walk analysis and the introduction of hyperchain datasets.\n2. Innovatively unifying message exchanging modelling on two types of incidence interactions by introducing hypergraph duality (\"elegant and natural\" — Reviewer N534).\n3. Proposing virtual incidences to extend HMP to more downstream tasks (e.g., hyperedge-/node-level tasks) without modifying the HMP model, and demonstrating the broad applicability of the incidence-centric paradigm through node classification experiments.\n\nIn conclusion, compared to CoNHD's partial improvements, HMP's research forms a complete logical chain from \"why it works\" (mechanism) to \"how to model it\" (framework) and \"how to apply it\" (extension). It not only addresses key scientific questions in the incidence-centric paradigm but also provides reusable methodologies, representing a systematic contribution to the field.\n\n> * [2] Zheng, Y., & Worring, M. (2024). Co-Representation Neural Hypergraph Diffusion for Edge-Dependent Node Classification. arXiv preprint arXiv:2405.14286."}}, "id": "uO3MCW0N92", "forum": "eRu0UBXEh2", "replyto": "eRu0UBXEh2", "signatures": ["ICLR.cc/2026/Conference/Submission23815/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23815/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23815/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763096940078, "cdate": 1763096940078, "tmdate": 1763096940078, "mdate": 1763096940078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Hypergraph-native Message Passing (HMP) framework, a novel approach for representation learning on hypergraphs. The key motivation is to address the \"information squashing\" problem prevalent in existing Hypergraph Neural Networks (HNNs), which often rely on a star-expansion paradigm where messages from multiple nodes are aggregated into a single, fixed-size hyperedge representation. This bottleneck is particularly detrimental in large or heterophilic hyperedges. The authors demonstrate both theoretical and empirical results to support the effectiveness of the proposed HMP framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript is clearly written and logically organized.\n- The proposed method demonstrates strong empirical performance, outperforming many baseline methods on the selected benchmarks.\n- The paper provides a useful theoretical contribution, offering theorems that prove HMP can be seen as a generalization of AllSet and, by extension, many other HNNs."}, "weaknesses": {"value": "- **Incremental Novelty:** The paper's claim to propose a \"pioneering learning paradigm\" is unsubstantiated. The core \"incidence-centric\" idea is predated by frameworks like Line Expansion (LE) [1], which is not adequately discussed. HMP's novelty over LE appears to be the application of self-attention, not the paradigm itself. Similarly, the paper claims novelty over CoNHD [2] by handling adaptive hyperedge representations, but this overlooks the fact that adaptive representation sizes are already a key contribution of CoNHD. Consequently, the novelty appears incremental.\n- **Insufficient Motivation:** The paper is motivated as a solution to the \"information squashing\" problem of HNNs, but this issue is primarily demonstrated in the context of star-expansion. The paper fails to establish that this problem even exists in more relevant, related works like LE and CoNHD. If these prior methods already resolve information squashing, the paper's core motivation is not strong enough.\n- **Lack of Theoretical Analysis:** Given that related works (LE, CoNHD) also appear to mitigate the information squashing problem, the paper must provide a more rigorous theoretical analysis to differentiate its contribution. The authors should formally prove what advantages HMP offers in terms of expressive power or other theoretical properties when compared directly against these existing methods.\n- **Insufficient Empirical Analysis:** The paper's empirical analysis is incomplete. It reports that HMP underperforms on key homophilic benchmarks (Cora, Citeseer, Pubmed) but offers no analysis for this important negative result. An analysis is required to explain why HMP's message-passing mechanism fails in these settings. For example, the paper should investigate the trade-off between HMP's flexible attention and the strong, beneficial smoothing bias of the baseline methods that outperform it.\n\n[1] Yang et al., \"Semi-supervised hypergraph node classification on hypergraph line expansion,” Proceedings of the 31st ACM international conference on information and knowledge management, 2022.\n\n[2] Zheng and Worring, ”Co-Representation Neural Hypergraph Diffusion for Edge-Dependent Node Classification,” arXiv, 2024."}, "questions": {"value": "- Could you please clarify the theoretical difference between the proposed 'incidence-centric' paradigm and the Line Expansion (LE) framework [1]?\n- Given the similarity to LE, how would you redefine HMP's core novelty beyond the specific application of self-attention?\n- Since CoNHD [2] also features an adaptive representation mechanism, could you provide a direct comparison to demonstrate what unique advantages HMP's mechanism offers?\n- Can you provide theoretical or empirical evidence that the information squashing problem persists in more advanced frameworks like LE and CoNHD?\n- Could you provide a formal analysis of expressive power that theoretically establishes the advantages of HMP's self-attention mechanism over the schemes used in LE and CoNHD?\n- Could you provide a detailed analysis for the negative result that HMP underperforms on key homophilic benchmarks like Cora and Citeseer?\n\n[1] Yang et al., \"Semi-supervised hypergraph node classification on hypergraph line expansion,” Proceedings of the 31st ACM international conference on information and knowledge management, 2022.\n\n[2] Zheng and Worring, ”Co-Representation Neural Hypergraph Diffusion for Edge-Dependent Node Classification,” arXiv, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "14xtVX9S92", "forum": "eRu0UBXEh2", "replyto": "eRu0UBXEh2", "signatures": ["ICLR.cc/2026/Conference/Submission23815/Reviewer_2FbW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23815/Reviewer_2FbW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007726326, "cdate": 1762007726326, "tmdate": 1762942818063, "mdate": 1762942818063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A3. Common Response: Further Analysis of HMP's Performance on Homophilic Hypergraphs"}, "comment": {"value": "As noted in line 473 of our manuscript, HMP underperforms in node classification tasks on homophilic hypergraphs (Cora, Citeseer, Pubmed) because baselines such as ED-HNN and PhenomNN adopt diffusion-based propagation rules, whose inherent homophilic inductive bias aligns well with the structural characteristics of these datasets. In contrast, HMP is designed to adapt to broader scenarios (including heterophily) without relying on such task-specific biases, explaining its competitive advantages in heterophilic node-level tasks.\n\nTo elaborate, we verify that HMP can learn homophilic inductive bias from data as the hypergraph scale increases. The table below compares HMP with state-of-the-art methods on datasets with CE Homophily > 0.8:\n\n|              | Citeseer | Cora-CA | Cora  | Pubmed | ModelNet40 | DBLP-CA |\n|--------------|----------|---------|-------|--------|------------|---------|\n| CE Homophily | 0.893    | 0.803   | 0.897 | 0.952  | 0.853      | 0.869   |\n| #incidences  | 3k       | 5k      | 5k    | 35k    | 62k        | 100k    |\n| SotA         | 75.10    | 85.81   | 82.29 | 89.56  | 98.66      | 91.93   |\n| HMP          | 73.82    | 84.61   | 80.35 | 88.45  | 98.54      | 91.87   |\n| $\\| \\Delta \\|$   | 1.28     | 1.2     | 1.94  | 1.11   | 0.12       | 0.06    |\n\nNotably, the performance gap ($|\\Delta|$) shrinks significantly with increasing incidence count, decreasing from 1.x to 0.06. This aligns with the empirical law of GNNs on conventional graphs [3]: as dataset scale grows, models with flexible learning capabilities (e.g., GAT) narrow the gap with bias-aligned baselines (e.g., GCN/GraphSAGE) on homophilic data:\n\n|               | Cora      | CiteSeer  | PubMed    | CS        | Photo     | WikiCS    | Computer  | Physics   |\n|---------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| #edges        | 5k        | 5k        | 44k       | 82k       | 119k      | 216k      | 246k      | 248k      |\n| GCN/GraphSAGE | **85.10** | **73.14** | **81.12** | **96.38** | **96.78** | 80.69     | 93.99     | **97.46** |\n| GAT           | 84.46     | 72.22     | 80.28     | 96.21     | 96.60     | **81.07** | **94.09** | 97.25     |\n\nIn summary, baselines with homophilic inductive bias may outperform HMP on small-scale homophilic hypergraphs, but this advantage diminishes as data scale increases due to HMP's strong learning ability. Meanwhile, HMP maintains superior performance on heterophilic hypergraphs, demonstrating its broader applicability.\n\n> * [3] Luo, Y., Shi, L., & Wu, X. M. (2024). Classic gnns are strong baselines: Reassessing gnns for node classification. Advances in Neural Information Processing Systems, 37, 97650-97669."}}, "id": "nt9XUFW3HF", "forum": "eRu0UBXEh2", "replyto": "eRu0UBXEh2", "signatures": ["ICLR.cc/2026/Conference/Submission23815/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23815/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23815/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763097144789, "cdate": 1763097144789, "tmdate": 1763097144789, "mdate": 1763097144789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A4. Common Response: Detailed Efficiency Analysis of HMP"}, "comment": {"value": "HMP uses self-attention as its message exchanger and leverages existing advancements in attention mechanisms (e.g., Performer [4]) to improve efficiency. Thus, runtime/memory trade-offs are not the core innovation or primary challenge of this work, but their feasibility has been verified. For example, HMP can stably operate on the large-scale Coauth-AMiner dataset (containing 5 million incidences) while achieving state-of-the-art performance.\n\nTo address reviewers' concerns, we analyzed the runtime and memory usage of HMP on hypergraphs with varying hyperedge sizes. We trained AllSet (AllDeepSets), ED-HNN, and HMP (with one attention head) for 1000 epochs on Hyperchains datasets (#classes=5) with a fixed length of 10 and varying widths (and thus varying hyperedge sizes). All models have 10 layers and adaptive hidden dimensions, resulting in approximately 50k learnable parameters. The following table reports the \"script time (in seconds) & peak GPU memory (in MB)\" using a NVIDIA GeForce RTX 3060 GPU:\n\n|         | #params | width=2    | width=4    | width=6    | width=8     | width=10    |\n|---------|---------|------------|------------|------------|-------------|-------------|\n| AllSet  | 49028   | 222 & 542  | 389 & 946  | 535 & 1363 | 693 & 1760  | 856 & 2169  |\n| **HMP** | 50885   | 333 & 855  | 482 & 1710 | 667 & 2565 | 826 & 3392  | 1076 & 4289 |\n| ED-HNN  | 50055   | 302 & 1716 | 584 & 3392 | 863 & 5120 | 1134 & 6776 | 1424 & 8481 |\n\nSpecifically, except for the highest time consumption on 2-walk hyperchains, HMP's time and space usage fall between AllSet and ED-HNN as the width increases, demonstrating moderate practical efficiency. Overall, the ratio of time and space consumption between HMP and AllSet remains stable within 2 as the hyperedge size (width) increases, validating our complexity analysis that HMP, after efficiency improvements, has the same complexity as AllSet.\n\n> * [4] Krzysztof, C., Valerii, L., David, D., Xingyou, S., Andreea, G., Tamas, S., ... & Adrian, W. (2021). Rethinking attention with performers. Proceedings of ICLR."}}, "id": "KemAZ7JofN", "forum": "eRu0UBXEh2", "replyto": "eRu0UBXEh2", "signatures": ["ICLR.cc/2026/Conference/Submission23815/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23815/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23815/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763097350173, "cdate": 1763097350173, "tmdate": 1763097350173, "mdate": 1763097350173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}