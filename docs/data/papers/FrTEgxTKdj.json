{"id": "FrTEgxTKdj", "number": 4917, "cdate": 1757805429557, "mdate": 1763483674276, "content": {"title": "Feedback to Reasoning: LLM-Assisted Molecular Optimization with Domain Feedback and Historical Reasoning", "abstract": "The remarkable success of large language models (LLMs) across diverse fields demonstrates their transformative potential in science, with molecular optimization representing a promising frontier. Traditionally, molecular optimization involves iterative discussions with domain experts, who progressively refine molecules with feedback until the desired properties are achieved. This interactive and feedback-driven process aligns well with the inherent strengths of LLMs, positioning them as promising tools for this task. **As an experience-driven task, molecular optimization depends critically on the domain feedback and accumulation of historical knowledge. However, none of the existing methods fully leverages such feedback and historical knowledge; especially, the reasoning trace and chemical insights that have led to successful optimization.** In this work, we propose **F2R**: Feedback to Reasoning, a conversational molecular optimization pipeline that allows LLMs to dynamically accumulate and retrieve historical knowledge about prior actions, rationales, and feedback. Moreover, just like humans whose reasoning is not always correct or precise, LLMs can also produce imperfect reasoning traces; F2R is the first work to leverages detailed domain feedback to critically reflect on and improve this reasoning. In this way, LLMs can evolve from passive language processors to agentic experts that emulate human experts in learning both actions and reasoning from experience. F2R is also the first work that leverages historical optimization results and reasoning traces from historical feedback. Consequently, F2R shows remarkable performance.", "tldr": "", "keywords": ["Molecular Optimization", "LLM for Science"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0712850276446180717eaecf58d902e98e329d38.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission focuses on the molecular optimization problem, which requires iterative refinement based on domain-specific feedback on the proposal. The form of iterative improvement aligns the key strengths of LLMs, their capacity for interactive dialogue, and incorporates feedback. However, prior LLM-based methods overlook the importance of historical knowledge accumulation during refinement and utilize static, single-action suggestions from a limited action space. To address this limitation, the submission proposes feedback to reasoning (F2R), an agent framework that cooperatively guided self-reflection, domain-specific feedback, and memory mechanisms to enhance the quality of the proposed molecules. The submission conducts extensive empirical studies and justifies the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The submission focuses on the trendy LLM reasoning problem. The motivation is clear: utilizing historical information is crucial for the iterative optimization tasks.\n- The submission is generally well-written, with clear illustrations and tables.\n- The conducted experiments provide empirical evidence on the effectiveness of the proposed method compared to the baselines."}, "weaknesses": {"value": "- The submission positions F2R as the first framework to incorporate historical feedback and reasoning trajectories. However, such a memory and feedback mechanism already appears in prior agentic frameworks like OctoTools [1] and LangGraph. In addition, the F2R's memory structure seems a direct adaptation of existing agentic memory paradigms, adapting to the molecular domain with detailed prompt design and searching metric. \n- The submission provides a limited ablation study on the proposed method. There is no quantitative breakdown of how much each component, including feedback, SAR knowledge, or memory, contributes to the overall performance. In addition, the submission does not examine the influence of memory size, retrieval count during reasoning, or similarity metrics, which are critical to validate claims of “dynamic knowledge accumulation.”\n- Despite repeatedly using terms like “guided self-reflection”, the F2R does not demonstrate actual reasoning improvement over time. There is no evidence that the model’s reasoning quality changes or that stored reasoning traces meaningfully influence future behavior.\n\n[1] OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning. In arXiv, 2025."}, "questions": {"value": "1. Could you compare and discuss the model's reasoning behavior when cooperating with the F2R and other frameworks?\n2. Could you provide ablation studies on the F2R's components, including feedback, SAR knowledge, and memory?\n3. Could you discuss the difference between F2R and other agent frameworks with the memory component and the feedback mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MGZXCgdbRb", "forum": "FrTEgxTKdj", "replyto": "FrTEgxTKdj", "signatures": ["ICLR.cc/2026/Conference/Submission4917/Reviewer_9atv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4917/Reviewer_9atv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566434936, "cdate": 1761566434936, "tmdate": 1762917765288, "mdate": 1762917765288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission focuses on the molecular optimization problem, which requires iterative refinement based on domain-specific feedback on the proposal. The form of iterative improvement aligns the key strengths of LLMs, their capacity for interactive dialogue, and incorporates feedback. However, prior LLM-based methods overlook the importance of historical knowledge accumulation during refinement and utilize static, single-action suggestions from a limited action space. To address this limitation, the submission proposes feedback to reasoning (F2R), an agent framework that cooperatively guided self-reflection, domain-specific feedback, and memory mechanisms to enhance the quality of the proposed molecules. The submission conducts extensive empirical studies and justifies the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The submission focuses on the trendy LLM reasoning problem. The motivation is clear: utilizing historical information is crucial for the iterative optimization tasks.\n- The submission is generally well-written, with clear illustrations and tables.\n- The conducted experiments provide empirical evidence on the effectiveness of the proposed method compared to the baselines."}, "weaknesses": {"value": "- The submission positions F2R as the first framework to incorporate historical feedback and reasoning trajectories. However, such a memory and feedback mechanism already appears in prior agentic frameworks like OctoTools [1] and LangGraph. In addition, the F2R's memory structure seems a direct adaptation of existing agentic memory paradigms, adapting to the molecular domain with detailed prompt design and searching metric. \n- The submission provides a limited ablation study on the proposed method. There is no quantitative breakdown of how much each component, including feedback, SAR knowledge, or memory, contributes to the overall performance. In addition, the submission does not examine the influence of memory size, retrieval count during reasoning, or similarity metrics, which are critical to validate claims of “dynamic knowledge accumulation.”\n- Despite repeatedly using terms like “guided self-reflection”, the F2R does not demonstrate actual reasoning improvement over time. There is no evidence that the model’s reasoning quality changes or that stored reasoning traces meaningfully influence future behavior.\n\n[1] OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning. In arXiv, 2025."}, "questions": {"value": "1. Could you compare and discuss the model's reasoning behavior when cooperating with the F2R and other frameworks?\n2. Could you provide ablation studies on the F2R's components, including feedback, SAR knowledge, and memory?\n3. Could you discuss the difference between F2R and other agent frameworks with the memory component and the feedback mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MGZXCgdbRb", "forum": "FrTEgxTKdj", "replyto": "FrTEgxTKdj", "signatures": ["ICLR.cc/2026/Conference/Submission4917/Reviewer_9atv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4917/Reviewer_9atv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566434936, "cdate": 1761566434936, "tmdate": 1763520646614, "mdate": 1763520646614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a molecular optimization pipeline based on LLMs, utilizing domain feedback and summarizing historical knowledge to iteratively optimize results. The manuscript is well-organized. The proposed pipeline exceeds baselines in both small molecule and peptide optimization tasks. As for its novelty, I cannot precisely assess its originality, but the pipeline seems rather straightforward and may lack technical depth, potentially making it better suited for conferences like *CL Findings. \n\nRegarding experimental results, although the proposed F2R achieves impressive performance, such as a 99% success rate in molecular solubility optimization, there are notable deficiencies in key metrics and analyses. For instance, it is crucial to examine the similarity between optimized and original molecules, as well as the overall similarity among optimized molecules, given the use of historical knowledge which may result in the generation of identical or very similar molecules. Additionally, the diversity of optimized molecules should be assessed for the same reason. Visualizing some pre- and post-optimization pairs would also be highly beneficial. Moreover, examples where certain LLMs initially made incorrect inferences but corrected them based on provided feedback are crucial. Based on my experience, even models like GPT-4.1 encounter challenges in adjusting reasoning appropriately to align with detailed domain-specific feedback."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work introduces a molecular optimization pipeline based on LLMs, utilizing domain feedback and summarizing historical knowledge to iteratively optimize results. The manuscript is well-organized."}, "weaknesses": {"value": "The pipeline seems rather straightforward and may lack technical depth. Deficiencies in metrics and analyses.  For instance, it is crucial to examine the similarity between optimized and original molecules, as well as the overall similarity among optimized molecules, given the use of historical knowledge which may result in the generation of identical or very similar molecules. Additionally, the diversity of optimized molecules should be assessed for the same reason. Visualizing some pre- and post-optimization pairs would also be highly beneficial. Moreover, examples where certain LLMs initially made incorrect inferences but corrected them based on provided feedback are crucial."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8oYmNhDG6n", "forum": "FrTEgxTKdj", "replyto": "FrTEgxTKdj", "signatures": ["ICLR.cc/2026/Conference/Submission4917/Reviewer_XS5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4917/Reviewer_XS5Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805642014, "cdate": 1761805642014, "tmdate": 1762917764261, "mdate": 1762917764261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces F2R, a framework that enhances LLMs for molecular optimization by integrating iterative domain feedback and memory-based reasoning. Unlike existing models that perform one-shot edits, F2R allows the LLM to receive structured feedback and self-reflect before proposing the next molecule. It also accumulates structure-activity relationship patterns from past tasks to inform future reasoning, functioning like an experience-driven agent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a feedback loop to enable self-correction.\n\n2. The approach employs an agentic memory that extracts reusable structure-activity relationship insights.\n\n3. The results show significant gains over ChatDrug, ChemReasoner, and RL-Guider.\n\n4. The work can generalize to other scientific reasoning tasks such as peptides and proteins."}, "weaknesses": {"value": "1. The framework heavily relies on LLMs' reasoning quality, which may not generalize to weaker models.\n\n2. The feedback and retrieval loops increase computational cost.\n\n3. Knowledge reuse depends on task similarity, so the performance on unseen targets is unclear."}, "questions": {"value": "1. Are there observed cases where the model produces the right answer with wrong reasoning or plausible reasoning leading to an incorrect molecule? If so, how does the feedback loop address such inconsistencies? Have the authors performed any systematic error analysis to categorize types of reasoning failures or feedback misinterpretations across tasks? Could accumulated reasoning errors propagate in memory?\n\n2. F2R currently optimizes a single target property at a time, using feedback such as “increase solubility” or “improve binding affinity.” In realistic molecular design, however, optimization often involves multiple competing objectives. For example, we ask the model to simultaneously improve potency, solubility, and stability, or to balance drug-likeness with binding affinity.\n\nHow does F2R scale to such multi-property or high-dimensional optimization settings, where feedback must capture trade-offs between properties rather than a single scalar goal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MvsouIZhlx", "forum": "FrTEgxTKdj", "replyto": "FrTEgxTKdj", "signatures": ["ICLR.cc/2026/Conference/Submission4917/Reviewer_Ayw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4917/Reviewer_Ayw5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900933258, "cdate": 1761900933258, "tmdate": 1762917763924, "mdate": 1762917763924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes F2R, a framework for improving molecular design using feedback from experts in a loop, with a mechanism for self-reflection on mistaken actions. the framework leverages this to create historical feedback. \nThis feedback is automatically generated with another LLM. This historical feedback is then retrieved during inference-time, based on the similarity between the generated molecules."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using accumulated expert feedback to reason towards improving some objective is fundamentally attractive as it tackles a key problem of learning algorithms like RL, which is that \"errors\" and \"successes\" are not reflected upon, making the rewards very sparse.\nThe authors make a good case comparing with other similar works in the field like ChemReasoner and ChatDrug, and show that their system F2R performs the best on most tasks."}, "weaknesses": {"value": "Code and data are not given for review.\nmany ablations missing\nexamples of execution would be great for understanding. right now it's not very transparent\nresults on the metric look suspiciously high, and there is no analysis of this. This seems to imply that the task has been technically solved, which is not the case. If the benchmarks have been saturated, the authors should either evaluate on a harder benchmark, or if not available select more advanced cases of the task, and see if the performance still holds. Potentially also adding some case-studies where you analyze how the system performs, what it gets correct and what it gets wrong, and how it compares against other methods.\nAlthough the idea is intellectually appealing, the authors do not show any examples of how the feedback looks like, and if there are any specific insights that the system has leveraged towards completing the task.\nIt is a bit sad that a paper is written entirely about molecular generation and optimization, and yet no single depiction of a molecule is shown. I think it's important to show examples, to demonstrate what the solutions given by your system really look like, if they are any good. Simply showing a bunch of results on a table is not enough and at minimum lacks transparency."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4gsPbjIcfL", "forum": "FrTEgxTKdj", "replyto": "FrTEgxTKdj", "signatures": ["ICLR.cc/2026/Conference/Submission4917/Reviewer_RJqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4917/Reviewer_RJqH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762108839425, "cdate": 1762108839425, "tmdate": 1762917763656, "mdate": 1762917763656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}