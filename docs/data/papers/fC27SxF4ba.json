{"id": "fC27SxF4ba", "number": 3148, "cdate": 1757342817537, "mdate": 1759898106132, "content": {"title": "MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning", "abstract": "Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent’s transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines.", "tldr": "We introduce a visual reasoning method as a hierarchical automaton with learning a high-level LLM-based transition policy over collaborating and competing agents to deliver interpretable, robust visual reasoning.", "keywords": ["Visual Reasoning", "Agent", "Neuro-Symbolic"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/155d9336cf2ba2e7d7a8de6f1c75540fc02a59cc.pdf", "supplementary_material": "/attachment/488d55174aecaa8d0de48c8e030800574e466639.pdf"}, "replies": [{"content": {"summary": {"value": "The paper presents a multi-agent framework for visual reasoning tasks, where a learnable hyper agent coordinates three types of reasoners. During training, only the hyper agent is optimized using synthetic data. Experiments are conducted on two VQA benchmarks (GQA and OK-VQA) and several referring expression comprehension (REC) datasets.\n\n\nThe general idea of multi-agent collaboration has been extensively explored in prior works such as [1] and [2]. While this paper introduces a slightly different collaboration mechanism, the overall conceptual novelty remains limited. Therefore, the main contributions appear to be: (1) applying multi-agent collaboration to visual reasoning; and (2) learning the hyper agent using synthetic data.\n\n\nRegarding (1), the technical contribution seems incremental. The framework largely mirrors existing multi-agent reasoning pipelines, with LLMs replaced by VLMs and visual tools. Moreover, the experimental evaluation is restricted to GQA and OK-VQA, which are insufficient to demonstrate general effectiveness on broader visual reasoning tasks.\n\n\nRegarding (2), based on Table 5, the results show minimal difference between models trained with and without SFT. This weak empirical signal makes it difficult to conclude that the proposed learning scheme provides substantial benefits.\n\n\nOverall, given the limited technical novelty and unconvincing empirical validation, I lean toward a negative recommendation for this submission.\n\n[1] Weize Chen et al., AGENTVERSE: FACILITATING MULTI-AGENT COLLABORATION AND EXPLORING EMERGENT BEHAVIORS\n\n[2] Sirui Hong et al., METAGPT: META PROGRAMMING FOR A MULTI-AGENT COLLABORATIVE FRAMEWORK"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A multi-agent framework for visual reasoning\n2. Empirical verification of the effectiveness of the proposed framework"}, "weaknesses": {"value": "See the comments in \"Summary\""}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cUw5WHwhVg", "forum": "fC27SxF4ba", "replyto": "fC27SxF4ba", "signatures": ["ICLR.cc/2026/Conference/Submission3148/Reviewer_TrxZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3148/Reviewer_TrxZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760791256947, "cdate": 1760791256947, "tmdate": 1762916573148, "mdate": 1762916573148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MATA introduces a hierarchical finite-state automaton for visual reasoning where multiple specialized agents (specialized perception, stepwise reasoning, oneshot reasoning) collaborate and compete. The key innovation is a trainable hyper agent that learns transition policies between agents using supervised fine-tuning on a generated dataset (MATA-SFT-90K). The dataset is created by expanding transition trajectory trees, scoring leaf nodes based on task performance, and generating memory-to-next-state training pairs. MATA improves over the base internvl model on GQA, OK-VQA, RefCOCO/RefCOCO+/RefCOCOg, and Ref-Adv benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The combination of trainable high-level transitions with rule-based sub-automata is elegant, focusing learning on the ambiguous agent selection problem while preserving reliable execution within agents.\n- The paper provides experiments across multiple benchmarks (VQA and visual grounding), demonstrating consistent improvements over the base model.\n- The transition trajectory tree expansion provides a principled approach to generating supervision for the hyper agent, though scalability concerns remain."}, "weaknesses": {"value": "- The gains appear modest (e.g., 75.2% base internvl25 used as vlm vs 76.5% theirs on AOKVQA) considering the 90K in-domain training examples generated. The paper doesn't isolate whether improvements come from multi-agent collaboration or simply additional task-specific training data.\n- Table 5 reveals that removing SFT causes performance to drop below the base internvl25 model, suggesting the architecture itself may be detrimental without training. A crucial missing experiment is training a monolithic model on the same MATA-SFT-90K data to isolate the architectural contribution.\n- The paper claims zero-shot generalization but the base models may have been pre-trained on these datasets. This undermines claims about generalization capabilities.\n- The authors admit their trajectory tree expansion becomes intractable as agents increase, yet provide no computational overhead analysis or comparison with simpler methods.\n- While claiming to address competition between functionally overlapping agents, the paper only uses three distinct agents with clearly separated roles, not truly competitive alternatives."}, "questions": {"value": "- What happens when you train InternVL2.5-8B directly on MATA-SFT-90K to output answers without the multi-agent architecture? This would isolate the contribution of the architecture versus the training data. Or did distillation of a large VLM to internvl2.5?\n- What is the computational overhead (inference time, memory) compared to monolithic models and single-agent methods like HYDRA?\n- How does performance scale when adding more agents? Given the acknowledged exponential growth in trajectory trees, is this approach practical beyond 3-4 agents?\n- Can you provide evidence that the models used (InternVL2.5, Florence2-L) were not trained on GQA/OK-VQA/RefCOCO to substantiate zero-shot claims?\n- Why does the architecture without SFT perform worse than the base VLM used (internvl2.5)? Why not use a VLM instead of an LLM for the state controller?\n- Have you considered more efficient alternatives to near-exhaustive tree expansion, such as Monte Carlo tree search or learned value functions to prune unpromising branches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NSlbdjDDel", "forum": "fC27SxF4ba", "replyto": "fC27SxF4ba", "signatures": ["ICLR.cc/2026/Conference/Submission3148/Reviewer_z1Pw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3148/Reviewer_z1Pw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942307197, "cdate": 1761942307197, "tmdate": 1762916572934, "mdate": 1762916572934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **MATA (Multi-Agent Trainable Automaton)**, a hierarchical framework for **multi-agent visual reasoning**. Instead of executing a fixed modular pipeline, MATA organizes several reasoning agents (one-shot, step-wise, and specialized perception agents) as states of a **finite-state hyper-automaton**. A **trainable hyper-agent** which is an LLM-based controller then learns to select the next agent state from a shared memory, enabling dynamic collaboration and competition among agents.\n\nTo supervise this transition policy, the authors construct **MATA-SFT-90K**, a dataset of memory-to-next-state pairs extracted from expanded transition-trajectory trees across visual reasoning datasets (GQA, OK-VQA, RefCOCO series). The learned controller yields interpretable reasoning traces and achieves state-of-the-art results across multiple visual reasoning and grounding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "– **Clear conceptual motivation:** The paper identifies an important limitation of existing VLMs and compositional systems that the lack of a learned, flexible orchestration mechanism among reasoning agents and recasts it elegantly as a finite-state automaton control problem.\n\n– **Novel hierarchical formulation:** Treating each agent as a sub-automaton and learning high-level transitions through a hyper-agent is a conceptually clean, interpretable design that unifies rule-based micro-control with data-driven macro-control.\n\n– **Strong empirical results:** MATA attains new SOTA accuracy on GQA, OK-VQA, and RefCOCO series, outperforming both monolithic VLMs and compositional baselines.\n\n– **Generalizability and ablation rigor:** Cross-dataset transfer (Table 6) shows < 1 % drop in zero-shot settings; ablations confirm that supervised fine-tuning of the hyper-agent contributes most gains (Table 5)."}, "weaknesses": {"value": "– **Incremental algorithmic novelty:** While the integration is elegant, many components (agent orchestration, SFT, trajectory trees) extend known concepts from HYDRA and NAVER. The work’s originality lies more in *system design* than in theoretical innovation.\n\n– **Limited discussion of scalability:** The near-exhaustive transition expansion is tractable for 3 agents but may explode combinatorially as more states are added.\n\n– **Computational cost analysis:** Wall-clock training times and GPU usage are only qualitatively stated; quantitative comparisons would clarify efficiency relative to HYDRA or DWIM."}, "questions": {"value": "Please see the above weakness.\n\n1. Do you encounter issues with the shared memory growing unboundedly during long reasoning sequences? If so, how is this mitigated?\n2. It would also be helpful if the authors could provide qualitative visualizations comparing MATA’s reasoning paths against other systems, to better illustrate how its hierarchical controller differs in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rqgES7fYna", "forum": "fC27SxF4ba", "replyto": "fC27SxF4ba", "signatures": ["ICLR.cc/2026/Conference/Submission3148/Reviewer_6jZh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3148/Reviewer_6jZh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943997143, "cdate": 1761943997143, "tmdate": 1762916572656, "mdate": 1762916572656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key challenge in current multi-agent systems: how to train a policy to select among multiple sub-agents, rather than relying on manually handcrafted pipelines. To tackle this, the authors propose MATA (Multi-Agent hierarchical Trainable Automaton), a novel system for visual reasoning that organizes inference as a hierarchical finite-state automaton. The authors construct a transition-trajectory dataset, MATA-SFT-90K, which is used for supervised training and evaluation. The method demonstrates strong performance on a range of visual reasoning tasks.\n\nOverall, this is a solid paper with clear motivation. The method is well designed, and the experiments are conducted rigorously. One major concern is that the three-agent design is somewhat simplistic and limited in scope, as the authors themselves acknowledge in the limitations section."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear motivation**: The paper addresses an important limitation in current multi-agent systems: leveraging the power of multiple agents typically requires manual pipelining, which becomes unwieldy as task complexity grows. The proposal to learn a hyper-policy for agent selection is both reasonable and interesting.\n- **Principled and extensible design**: MATA’s architecture is well aligned with its motivation. It is technically sound and, importantly, not narrowly restricted to the specific visual reasoning setting or the particular sub-agents used in this paper. In principle, the approach could extend to other tasks and larger agent pools, opening up many potential research directions.\n- **Rigorous experiment design and evaluation**: The experiments are well designed and executed. For example, the three SFT configurations and their results convincingly demonstrate the benefits of the proposed method and its generalization ability, rather than mere overfitting."}, "weaknesses": {"value": "### Major\n\n- **Limited applicability**: As noted in the limitations, the use of only three agents is a restricted setting. There is also a lack of detail regarding how these three agents were selected and the rationale behind their design.\n- **Unclear attribution of performance gains**: It is not clear whether the learned state transition policy is truly responsible for the observed performance improvements. For example, if all three agents were simply called exhaustively, would performance improve regardless, making the learned policy less critical?\n- **Competition mechanism not fully justified**: While the collaborative aspect of the system is well motivated, the competitive aspect is less convincing. The paper claims that “a competition mechanism where functionally overlapping agents for the same subtask work together is under-explored,” but in the current three-agent setup, the agents seem to serve distinct roles with little actual overlap. It would be more compelling to see experiments with a larger pool of agents, including multiple agents with overlapping capabilities, to better demonstrate the value of competition.\n\n### Minor\n- Line #352: “... target dataset for evaluated; ...” should be “... target dataset for evaluation; ...”"}, "questions": {"value": "- How were the three agents chosen, and what was the rationale behind their selection?\n- How is the competition aspect justified, given that the three agents in the current setup do not appear to have significant functional overlap?\n- Although the exponential growth of the transition search space is discussed as a limitation, do the authors have any thoughts on extending the approach to more complex scenarios with significantly more agents?\n- Would it be possible that we would observe the same performance gains if the three agents are called in an exhaustive way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TujjujK6z0", "forum": "fC27SxF4ba", "replyto": "fC27SxF4ba", "signatures": ["ICLR.cc/2026/Conference/Submission3148/Reviewer_mEnL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3148/Reviewer_mEnL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960105670, "cdate": 1761960105670, "tmdate": 1762916572356, "mdate": 1762916572356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}