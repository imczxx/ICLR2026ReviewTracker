{"id": "8IsxSZ4CRW", "number": 22680, "cdate": 1758334395004, "mdate": 1759896852651, "content": {"title": "Beyond Similarity for Personalization: User Memory Selection via Response-Utility Optimization", "abstract": "A common approach to personalization in large language models (LLMs) is to incorporate a subset of the user memory into the prompt at inference time to guide the model's generation. Existing methods to select these subsets primarily rely on similarity between user memory items and input queries, ignoring how these items actually affect the model's predictive distribution. We propose **R**esponse-**U**tility  optimization for **M**emory **S**election (RUMS), a novel user memory selection method, inspired by Bayesian Optimal Experimental Design, that directly quantifies how much each memory item reduces uncertainty in the model's response distribution. RUMS measures mutual information between a subset of user memory and model outputs to identify items that sharpen predictions beyond semantic similarity. Even more, RUMS, by design, automatically selects if personalization is beneficial at all. We demonstrate that this information-theoretic foundation enables more principled user memory selection that aligns more closely with human selection compared to state-of-the-art methods, and models $400$x bigger. Additionally, we show that memory items selected using RUMS result in better response quality compared to existing approaches, while having up to 95\\% reduction in cost.", "tldr": "We introduce a response-utility optimized method for memory selection for personalization.", "keywords": ["Personalization Method", "Efficient Methods", "Information-Theoretics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31ec00b7167e5866b4ab645151bc08185ecae7d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors outline existing problems with personalization systems for LLMs: either you include the entire memory (lots of irrelevant info), or you do semantic similarity with the queries, which surfaces similar memory elements but not necessarily *what should be included to improve the model's outputs*. Authors outline a better approach using Bayesian Optimal Experimental Design: the best memory elements to include are those that reduce the uncertainty in the model's output. The issue is that the true utility metric is intractable as it requires testing every possible subset of memory elements -- authors get around this by training models to proxy the utility. Authors show improvement in response quality against baselines and much higher cost efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- problem with existing personalization systems is well-motivated\n- solution presented is intuitive and understandable\n- solution has concrete and significant improvements compared to existing methods (cost, effectiveness)\n- solution is well-grounded in theory\n- authors do human eval as well"}, "weaknesses": {"value": "- small memory set in experiments - 50 static memory elements? this seems somewhat limited, some experiments on larger memory stores to see how the experiment scales\n- human eval is great but still somewhat limited at 64 samples\n- proxy is very black-box: some analysis of what the RUMS-Models are actually learning would strengthen the paper considerably"}, "questions": {"value": "I'm curious about interactions between memory elements -- the interactions are weakly modelled as you are looking at subsets, but what happens for example when there is contradictory information? What happens if memory elements are related to each other in a hierarchy? etc. I feel like interactions between elements need to be modelled more explicitly. Thoughts on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mDEQC1nM1p", "forum": "8IsxSZ4CRW", "replyto": "8IsxSZ4CRW", "signatures": ["ICLR.cc/2026/Conference/Submission22680/Reviewer_9ey2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22680/Reviewer_9ey2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949312369, "cdate": 1761949312369, "tmdate": 1762942333580, "mdate": 1762942333580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RUMS, a novel method for user memory selection in LLM personalization. Unlike existing approaches that rely on semantic similarity between user memory and queries, RUMS uses an information-theoretic utility function to select memory items that directly reduce uncertainty in the model’s response distribution. RUMS can also abstain from personalization when it is not beneficial. The method is made efficient for inference by training a lightweight classifier to approximate the utility-based selection. Experiments on synthetic and real-world datasets show that RUMS better matches human judgments, improves response quality, and reduces computational cost compared to strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Proposes a principled, response-aware criterion for memory selection, moving beyond surface-level similarity.\n\n* RUMS can automatically decide when personalization is helpful, reducing unnecessary context and noise.\n\n* The approach is efficient at inference time, with up to 95% cost reduction over baselines.\n\n* Empirical results show improved alignment with human judgments and better response quality than both similarity-based and LLM-prompting baselines, including much larger models."}, "weaknesses": {"value": "* The initial utility computation (entropy reduction) is computationally expensive, though mitigated by offline training.\n\n* The method assumes access to high-quality user profiles; the impact of noisy or sparse profiles is not fully explored.\n\n* Human evaluation for alignment is relatively small-scale, which may limit generalizability.\n\n* Applicability to multi-turn or more complex dialog scenarios is not demonstrated."}, "questions": {"value": "* In the candidate reduction step, GPT-4 is used to filter memory items before utility computation. Could the authors clarify how this step affects fairness and reproducibility, especially if the candidate set varies across users or queries?\n\n* The utility threshold for abstaining from personalization is tuned on validation data. How robust is the system to this threshold in practice? \n\n* Have the authors observed cases where the threshold leads to under- or over-personalization, and how might this be mitigated?\n\n* For user profiles with missing or conflicting attributes, how does RUMS handle such cases during both training and inference? Would the authors consider integrating uncertainty estimation or imputation strategies?\n\n* The cost analysis is compelling, but could the authors provide more details on the wall-clock latency and memory usage of RUMS-Model inference compared to the strongest baselines in a real deployment scenario?\n\n* In Table 5, the selected memory items sometimes differ from human annotation. Could the authors share more qualitative examples or error analysis to help understand typical failure modes or edge cases?\n\n* The current evaluation focuses on single-turn queries. Do the authors see a path to extending RUMS to multi-turn or session-based personalization, and what challenges might arise in that setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O5blsSXqdp", "forum": "8IsxSZ4CRW", "replyto": "8IsxSZ4CRW", "signatures": ["ICLR.cc/2026/Conference/Submission22680/Reviewer_ksBw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22680/Reviewer_ksBw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957368354, "cdate": 1761957368354, "tmdate": 1762942333298, "mdate": 1762942333298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a method for memory selection in LLM personalization at inference time, which selects user memory items that improve response utility, rather than relying on similarity between memory item and user query. The key idea is to frame personalization as an information-theoretic optimization inspired by Bayesian Optimal Experimental Design. Experiments on both synthetic (PersonaFeedback, FreebaseQA) and real-world datasets (WildChat) validate the performance of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of shifting personalization from semantic retrieval to response-driven utility estimation is interesting. The motivation is valid in studying personalization from an information-theoretic perspective in terms of entropy. In particular,  the utility function is well-motivated and clearly formalized through predictive entropy reduction.\n\n2. By amortizing the expensive utility computation into an efficient DeBERTa-based selector, RUMS provides a deployable solution for large-scale user adaptation."}, "weaknesses": {"value": "1. Although the BOED analogy is appealing, the adaptation is heuristic rather than formally justified. Eq. 3 equates personalization utility with predictive entropy reduction, but there is no discussion of conditions under which this correlates with user-level response quality \nThe lack of any regret or generalization bound limits the claimed theoretical rigor.\n\n2. There is an approximation gap between utility and learned model. The RUMS-Model learns from utility-labeled data generated offline, but the paper provides no quantitative analysis of approximation error, e.g., how often the DeBERTa predictor agrees with true RUMS-Utility decisions, or how this affects downstream response quality. Without this, it is unclear how much benefit comes from the theoretical utility versus simply supervised correlation learned during training.\n\n3. The experiments rely on synthetic user profiles and GPT-4-simulated human judgments. While this is reasonable for development, the central claim of “aligning with human personalization preferences” remains speculative without real human evaluation beyond annotation agreement on static queries.\n\n4. Prior works (e.g., PEARL 2024; Context Steering 2025; Bayesian Preference Elicitation 2024) already explore information-theoretic or uncertainty-aware retrieval for personalization. Authors could better clarify how RUMS fundamentally differs, beyond using entropy reduction as the scoring signal."}, "questions": {"value": "1. I am confused by the equivalence between $H_\\theta(y|x)$ in Proposition 1 and $\\hat{H}\\_\\theta(y|x)$ in Eq. 4. If I understand correctly,  $H_\\theta(y|x)$ quantifies the sequence-level entropy, while Eq. 4 quantifies the token-level entropy. In particular, the original expectation is taken w.r.t. $p_{\\theta}(y|x)$, using MC sampling only requires computing empirical mean over $N$, why $1 / T$ is needed in Eq.4? Do we require each sample to have the same sequence length?\n\n2. How sensitive are personalization decisions to the threshold $\\tau$? Could it be adaptively chosen per user or query using uncertainty calibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nH0qWOlW75", "forum": "8IsxSZ4CRW", "replyto": "8IsxSZ4CRW", "signatures": ["ICLR.cc/2026/Conference/Submission22680/Reviewer_wM1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22680/Reviewer_wM1E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980879585, "cdate": 1761980879585, "tmdate": 1762942333110, "mdate": 1762942333110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a method for memory selection in LLM personalization at inference time, which selects user memory items that improve response utility, rather than relying on similarity between memory item and user query. The key idea is to frame personalization as an information-theoretic optimization inspired by Bayesian Optimal Experimental Design. Experiments on both synthetic (PersonaFeedback, FreebaseQA) and real-world datasets (WildChat) validate the performance of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of shifting personalization from semantic retrieval to response-driven utility estimation is interesting. The motivation is valid in studying personalization from an information-theoretic perspective in terms of entropy. In particular,  the utility function is well-motivated and clearly formalized through predictive entropy reduction.\n\n2. By amortizing the expensive utility computation into an efficient DeBERTa-based selector, RUMS provides a deployable solution for large-scale user adaptation."}, "weaknesses": {"value": "1. Although the BOED analogy is appealing, the adaptation is heuristic rather than formally justified. Eq. 3 equates personalization utility with predictive entropy reduction, but there is no discussion of conditions under which this correlates with user-level response quality. As this approach borrows the concept from Bayesian Optimization, if authors can make effort to provide some theoretical guarantees, e.g. regret bound, it would help improve the rigorousness of the method significantly.\n\n2. There is an approximation gap between utility and learned model. The RUMS-Model learns from utility-labeled data generated offline, but the paper provides no quantitative analysis of approximation error, e.g., how often the DeBERTa predictor agrees with true RUMS-Utility decisions, or how this affects downstream response quality. Without this, it is unclear how much benefit comes from the theoretical utility versus simply supervised correlation learned during training.\n\n3. Prior works already explore information-theoretic or uncertainty-aware retrieval for personalization. Authors could better clarify how RUMS fundamentally differs, beyond using entropy reduction as the scoring signal."}, "questions": {"value": "1. I am confused by the equivalence between $H_\\theta(y|x)$ in Proposition 1 and $\\hat{H}\\_\\theta(y|x)$ in Eq. 4. If I understand correctly,  $H_\\theta(y|x)$ quantifies the sequence-level entropy, while Eq. 4 quantifies the token-level entropy. In particular, the original expectation is taken w.r.t. $p_{\\theta}(y|x)$, using MC sampling only requires computing empirical mean over $N$, why $1 / T$ is needed in Eq.4? Do we require each sample to have the same sequence length?\n\n2. How sensitive are personalization decisions to the threshold $\\tau$? Could it be adaptively chosen per user or query using uncertainty calibration?\n\n3. The experiments rely on synthetic user profiles. It might be case that I overlook some details. How did you obtain the human judgetment? Did you use LLM simulated judgement or recruit human annotators? It would be best if the annotation process can be described."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nH0qWOlW75", "forum": "8IsxSZ4CRW", "replyto": "8IsxSZ4CRW", "signatures": ["ICLR.cc/2026/Conference/Submission22680/Reviewer_wM1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22680/Reviewer_wM1E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980879585, "cdate": 1761980879585, "tmdate": 1763771658954, "mdate": 1763771658954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using information gain to quantify the extent to which memory reduces model uncertainty, selecting relevant memory.\nThree challenges are identified\n* The aim is to optimize predictive distribution rather than inferring latent parameters\n* LLM's large output space makes computations intractable\n* It requires to detect whether personalization can improve responses\n\nTo address these problems\n* A novel utility function is introduced to reduce predictive entropy\n* Sequence-level entropy is decomposed to token-level and Monte Carlo sampling is employed for estimation\n* A threshold is set to filter irrelevant information, preventing degrading response quality\n\nSince computing entropy reductions for all candidates is prohibitive, a classifier is trained to predict the utility at inference time.\nEmpirical study is performed to compare the proposed method with prompting and retrieval on Personal Feedback, FreebaseQA, and WildChat datasets.\nConclusions include but not limited to\n\n* It is statistically significant that RUMS-utility can distinguish personalized from non-personalized inputs\n* RUMS improves response quality and reduces cost"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper has a good structure.\nIn methodology, main challenges are identified and solutions are proposed for each one.\nIn experiments, research questions are clearly stated, and analysis is conducted for each question with conclusion provided.\nThis makes the paper has a clear logic and easy to understand."}, "weaknesses": {"value": "My primary concern is that the solutions proposed to address the main challenges seem trivial to me.\nI do not deem this paper have technical novelty of substantial significance, but rather as a practical implementation of a technical solution, so I lean to reject the paper.\nI would like to raise my score if my concern is well addressed.\n* It is claimed that a novel utility function is proposed to shape the distribution, rather than inferring parameters.\nI deem that the model output $y$ can be viewed as a discrete parameter to be inferred.\nThe goal of BOED, *i.e.*, reducing the uncertainty of parameters, then naturally aligns with the goal of this work.\n* I deem that it is trivial to estimate parameters using Monte Carlo method unless any variance reduction technique is involved.\n* Setting filtering threshold is general and not specific to the proposed method.\nFor example, threshold can be also adopted by retrieval.\nSuch method also typically rely on engineering tuning rather than algorithm design."}, "questions": {"value": "* The scope of the paper is limited to memory selection.\nWhether the proposed method can be extended to broader conditioned generation, *e.g.*, retrieval-augmented generation?\n* I understand that the retrieval encoder used in the paper is pre-trained and not particularly fine-tuned on the specific experimental datasets.\nCould this lead to an unfair comparison?\n* L206: Why $T\\equiv T_N$ if token sizes are not fixed per sample?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "spKGGtDGX0", "forum": "8IsxSZ4CRW", "replyto": "8IsxSZ4CRW", "signatures": ["ICLR.cc/2026/Conference/Submission22680/Reviewer_48sG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22680/Reviewer_48sG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762170975834, "cdate": 1762170975834, "tmdate": 1762942332900, "mdate": 1762942332900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}