{"id": "viBAbg9ihM", "number": 7971, "cdate": 1758046879542, "mdate": 1759897818952, "content": {"title": "Eliciting Harmful Capabilities by Fine-Tuning on Safeguarded Outputs", "abstract": "Model developers apply safeguards to frontier models to resist misuse by adversaries; for example, they fine-tune models to refuse harmful requests and filter harmful outputs with classifiers. In this work, we show that we can partially circumvent even idealized versions of these safeguards with \\emph{elicitation attacks}. Our elicitation attacks use only benign capabilities of a frontier model to elicit harmful capabilities from open-source models. Specfically, they (i) generate harmless prompts that are adjacent to target harmful tasks, (ii) generate outputs to these prompts with frontier models, then (iii) fine-tune open-source models on these prompt-output pairs. \nUsing dangerous chemical synthesis and processing as a case study, we find that our elicitation attacks enable \\emph{harmless-to-harmful generalization}: our fine-tuned models recover \\textasciitilde40\\% of the performance gap between the original open-source model and the frontier system without safeguards, despite only using harmless outputs from the frontier system. \nWe then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. \nOur work underscores the challenge of output-level safeguards and the pressing need for better distributed threat modeling.", "tldr": "", "keywords": ["adversarial robustness", "LLMs", "machine learning", "distillation", "jailbreaks", "classifier guarded system", "adversarial attacks", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd7df454ebb2c2327906f2d7fc2fb261c6b27f6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates elicitation attacks, a novel method of circumventing safeguards on frontier models. Instead of directly extracting harmful knowledge, the authors use frontier models to generate harmless but adjacent outputs, and then fine-tune open-source models on these outputs. The experiments, conducted primarily on chemical synthesis and processing tasks, show that fine-tuned open-source models regain up to ~40% of the performance gap relative to unguarded frontier models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a less explored but realistic pathway for adversaries: leveraging benign outputs to indirectly reconstruct harmful capabilities.\n2. The authors evaluate the proposed method across multiple open-source models."}, "weaknesses": {"value": "1. The work convincingly shows uplift in chemical synthesis, but it is not entirely clear how an adversary would scale this approach to more complex or diverse malicious domains in practice, such as cybersecurity, disinformation, and bioterrorism. The paper would benefit from a stronger justification that such attacks are not just proof-of-concept but present a practical risk.\n\n2. The paper argues that rubric evaluation based on keyword matching is an unreliable measure, and introduces structured comparison evaluation as a more fine-grained alternative. However, this method still primarily measures similarity to reference answers rather than the ability to successfully complete harmful tasks. As a result, the evaluation outcomes are heavily influenced by the quality and biases of the frontier models used to generate those references. Consequently, it remains unclear how much of the reported “40% performance gap recovery” actually translates into genuine security threats."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k9b0B9uh79", "forum": "viBAbg9ihM", "replyto": "viBAbg9ihM", "signatures": ["ICLR.cc/2026/Conference/Submission7971/Reviewer_d887"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7971/Reviewer_d887"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760512164510, "cdate": 1760512164510, "tmdate": 1762919983421, "mdate": 1762919983421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces elicitation attacks, which aim to elicit harmful capabilities from open-source models. This is achieved by fine-tuning open-source models on benign but adjacent outputs from safeguarded frontier models. Using dangerous chemical synthesis and processing as a case study, the authors show that these elicitation attacks can recover up to ~40% of the performance gap between the original open-source model and the unsafeguarded frontier system."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work explores a new and interesting security risk, whether open-source models can pick up harmful skills.\n2. The attack method is simple but effective.\n3. The results are consistent across different open-source models and measuring metrics."}, "weaknesses": {"value": "1. Lack of mechanistic insight: The paper doesn’t explain why fine-tuning on benign, adjacent chemical tasks enables a weak model to gain harmful capabilities.\n2. Limited scalability to new and more complex harmful knowledge: The approach relies on fixed, manually designed benign queries, while frontier models will continue to advance and acquire more sophisticated or creative harmful capabilities. However, assuming there is a fixed ground truth for these benign questions, then the benign prompts and outputs remain static, which prevents open-source models from learning genuinely new or more complex harmful knowledge.\n3. Style learning or technical learning: Based on the above two weaknesses, it remains unclear why the observed improvement occurs. I suspect that the improved performance may reflect imitation of the frontier model’s style rather than genuine understanding of harmful chemical synthesis. The evaluation might therefore capture stylistic similarity instead of true technical competence.\n4. Poor transferability: The method is only tested on chemical weapon tasks, which rely on strong domain-specific prior knowledge and extensive manual design. It remains unclear whether the same approach would work for other harmful domains, since the paper never defines or automates what qualifies as “adjacent” benign prompts. For example, if an attacker wanted to learn how to build a bomb, what would the corresponding adjacent benign questions be?\n5. Unclear evaluation procedure: The evaluation method is difficult to understand and interpret. In the provided example (Figure 1), the judgment that “125–135 °C causes decomposition” seems to rely on the evaluator model’s own reasoning rather than on a reference solution. Which of R1 or R2 represents the reference solution? If m is the structured comparison function, could you clarify how m(W) is calculated, as shown in Figure 1?"}, "questions": {"value": "Please refer to the questions in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ta1oqaCewI", "forum": "viBAbg9ihM", "replyto": "viBAbg9ihM", "signatures": ["ICLR.cc/2026/Conference/Submission7971/Reviewer_g5qm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7971/Reviewer_g5qm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761145771788, "cdate": 1761145771788, "tmdate": 1762919983090, "mdate": 1762919983090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new elicitation attack paradigm, showing that benign content generated by frontier models can be used to finetune de-safeguarded open source models, which then exhibit harmful domain performance uplift. The authors validate this on chemical synthesis and processing tasks. \n\nThe attack first constructs harmless <prompt, output> pairs by (1) prompts derived from benign compounds in “Compounds” database and then frontier models generate answers, or (2) using relevant topic-based prompting to frontier models that bypass the Constitutional Classifier. Then these harmless pairs are used to fine-tune the open source abliterated models via QLoRA. To quantify the quality of answers,  the authors employ rubric scores and additionally propose the structured comparison. It uses frontier models to generate references and a separate frontier model as a judge to compare tested outputs against references along weighted subgoals. The reliability of this structured comparison is further evaluated by human evaluation, deliberate error detection, and ground truth rating comparison.\n\nExperiments show harmful uplift across multiple open-source models. Ablations examine task type, frontier capability, data scale, and training-domain adjacency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The motivation of this paper is clear and the intuition is straightforward.\n- The discussions from the angles of harmless-to-harmful generalization and ecosystem attacks (i.e., even if both the input and output of the frontier are filtered, a malicious player can still use benign output to train another unguarded open-source model, circumventing guarding strategies at the “system level”) are very insightful to the community. Thanks to the authors for this interesting work!\n- This paper is technically sound with comprehensive and systematic evaluations. Dataset collection is also high-quality and thorough."}, "weaknesses": {"value": "- Limited algorithmic novelty, as the attack applies standard SFT/QLoRA on novel data and the evaluation design is regular. The main contribution is problem formulation, data construction, and evaluation framing, rather than a new attack technique.\n- Results are conducted on abliterated open-source models. It remains unclear whether common safeguard techniques would partially persist under this attack.\n- The role of the frontier model in this attack paradigm is unclear.\n    - There is no discussion on why only using the <prompt, output> pairs generated by frontier models can have the attack success. Specifically, what are the fundamental differences between LibreChem baseline and the method? If using a language model to rewrite the original LibreChem dataset into <prompt, output> template, how is the performance? I suggest that the authors address this during rebuttal, as currently, it’s hard to tell whether APGR gain comes from frontier quality, instructional formatting, task targeting, or all of the above.\n    - Similarly, why newer/stronger frontier models can yield larger harmful uplift would help to understand this question.\n- The authors acknowledge key limitations of structured comparison, and it does affect the evaluation fairness. In the current setup, it is still unclear how much of APGR comes from true procedural correctness versus format/length/style alignment to the references.\n- It is appropriate to avoid publishing any sensitive content; however, the paper could still provide aggregate, non-sensitive information of the testing benchmark (e.g., numbers of samples, high-level category balance, source provenance, etc.) to support claims of coverage and fairness.\n- Compared to the breadth of the attack analysis, the paper offers rather limited discussion from the defensive perspective."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper demonstrates strong safety awareness and deliberately avoids releasing sensitive operational information. Still, given the topic and substantial amount of materials, it would be nice to have an individual ethics/safety review."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ImTJglk3Jg", "forum": "viBAbg9ihM", "replyto": "viBAbg9ihM", "signatures": ["ICLR.cc/2026/Conference/Submission7971/Reviewer_R5gC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7971/Reviewer_R5gC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971652403, "cdate": 1761971652403, "tmdate": 1762919982640, "mdate": 1762919982640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}