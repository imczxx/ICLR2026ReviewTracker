{"id": "gcwzqA8iWa", "number": 9410, "cdate": 1758121514619, "mdate": 1759897726443, "content": {"title": "Towards real-time BCI for speech with Whisper-based decoding of neural activity", "abstract": "Decoding continuous speech from neural activity is a central challenge for brain–computer interfaces (BCIs), with major implications for restoring communication in individuals with paralysis. While recent work has achieved impressive performance using recurrent neural decoders trained on multi-electrode array (MEA) recordings, these models remain brittle, data-hungry, and struggle to generalize across sessions or participants. In this work, we introduce Whisper-BCI, the first neural speech decoder to integrate high-resolution MEA recordings with a large pretrained automatic speech recognition (ASR) model. Our approach leverages interpretability findings showing that Whisper’s encoder layers learn phoneme-selective representations with localized attention. Building on this insight, we adapt Whisper to predict phoneme embeddings from neural signals into the third layer of Whisper's encoder and fine-tune the model end-to-end with a hybrid objective combining CTC loss on phoneme alignments and cross-entropy loss on word tokens. We further introduce domain-informed modifications including windowed self-attention to capture articulatory continuity, day-specific low-rank projections to address non-stationarity and reduce computational complexity, and subject-specific input embedders for cross-subject training. Evaluated on Card et al. and Brain-to-Text '25 data, Whisper-BCI performs on par with or outperforms baselines relative to prior MEA decoders, and achieves cross-subject generalization, opening the door to robust decoding with limited resources. Post-processing with rescoring and grammar-guided correction yields an additional relative improvement, and the use of windowed attention has the potential to significantly reduce latency, enabling near-real-time online decoding. Our results demonstrate that pretrained ASR models can serve as effective language backbones for neural decoding and suggest a scalable path toward foundation models for speech BCIs.", "tldr": "Fast sentence decoding from neural activity using Whisper", "keywords": ["BCI", "neuroAI", "language decoding", "invasive neural recordings"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/064e85447c17a091fae405db4f89b4f928f1400f.pdf", "supplementary_material": "/attachment/d61966bfe0464c236ec62ec17b68340b1fbd3828.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Whisper-BCI, a neural speech decoder that maps MEA recordings into a pretrained ASR model (Whisper) to improve robustness and cross-subject generalization. Evaluations are performed to show effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* a) This paper is well organized and easy to follow."}, "weaknesses": {"value": "**(a) Limited novelty**  \nThe proposed method builds directly on the  Whisper architecture. The proposed architecture still follows the CTC loss training with beam search for inference, with an additional CE loss for as the token-level loss being the only difference from Willett et al. As such, the work primarily represents an engineering integration of existing approaches rather than introducing new insights from either the neuroscience or the machine learning perspective.\n\n**(b) Incremental improvement**  \nThe reported performance in Table 1 is only marginally better than the baseline models used in prior work, and these gains appear modest relative to the complexity added by the proposed approach. This raises questions about the practical advantage of the method over simpler existing techniques.\n\n**(c) Insufficient Ablation Study**  \nAblation should at least be conducted to demonstrate the effectiveness of the proposed approach compared to the base model proposed by Willett et al. to show solidness."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LWCtEMl3wH", "forum": "gcwzqA8iWa", "replyto": "gcwzqA8iWa", "signatures": ["ICLR.cc/2026/Conference/Submission9410/Reviewer_4krM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9410/Reviewer_4krM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569034844, "cdate": 1761569034844, "tmdate": 1762921015976, "mdate": 1762921015976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose fine-tuning a Whisper model on intracranial MEA recordings to decode speech from the brain. They train their model to provide two decoding pathways: (1) where phoneme representations are rescored via WSFT, and (2) a more efficient pathway that leverages the Whisper decoder as an implicit language model and uses a small beam search to produce fast transcriptions. The authors run experiments where they jointly train with the Willett and Card data and show cross-subject generalisation and comparable performance to the current state-of-the-art on the respective datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Promising idea to combine both phoneme representations with word token predictions as they likely leverage complementary neural signals\n- Aggregating all publicly available MEA datasets is a sensible step for scaling intracranial decoding\n- Hierarchical normalization via month- and day-specific projections to account for representation drift is well-motivated\n- Lightweight ASR approach by skipping the WSFT is sensible as the field moves towards real-time intracranial BCIs"}, "weaknesses": {"value": "- Improvements over Card and Willett are minor, if any, and are difficult to assess without error bars or a sense of statistical significance. Is this possible to gather via the competition platform?\n- I would like to see a more robust analysis of subject generalisation as this could be an important aspect of this work. How does zero-shot subject transfer look? Does training on X% of Willett yield the same gain on Card as training on X% of Card?\n- The authors have not made comparisons to the relevant baselines they discuss (e.g. LISA) and others described in the original Willett competition reflections paper [A]\n- Minor: Line 105; NeuSpeech did not re-train Whisper on MEG recordings but rather fine-tuned the model with MEG.\n\nI am open to raising my score if the authors can satisfactorily address the above weaknesses.\n\n[A] Willett, F.R., Li, J., Le, T., Fan, C., Chen, M., Shlizerman, E., Chen, Y., Zheng, X., Okubo, T.S., Benster, T. and Lee, H.D., 2024. Brain-to-Text Benchmark'24: Lessons Learned. arXiv preprint arXiv:2412.17227."}, "questions": {"value": "Questions\n- Line 124-127: I understand how month- and day-specific projections help with representation drift across time, but how does it improve generalisation “across subjects”?\n- Line 239-240: Although local window attention is a smart choice for computational efficiency, articulatory dynamics are not the only factor that may influence phoneme prediction. Low-frequency long-range semantic signals could inform and improve these predictions, too. Did the authors try training with full attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jhtmv0Y9aJ", "forum": "gcwzqA8iWa", "replyto": "gcwzqA8iWa", "signatures": ["ICLR.cc/2026/Conference/Submission9410/Reviewer_SXv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9410/Reviewer_SXv4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910523073, "cdate": 1761910523073, "tmdate": 1762921015627, "mdate": 1762921015627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Whisper-BCI, a brain–computer interface system that decodes speech directly from Brain-to-Text 2025 recordings using a modified version of the Whisper automatic speech recognition (ASR) model. The authors project neural features into Whisper’s encoder and optimize it with a hybrid loss that combines phoneme-level CTC loss and token-level cross-entropy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of utilizing some proven models in speech to text generation and migrate these models into brain signal to text generation makes sense. In this area, we've observed quite a bit papers following the same phylosophy."}, "weaknesses": {"value": "- The experiments are extremely limited, where only one set of experiment reported and the performance is even not close to baseline. Is this results just to illustrate whisper model could reach slightly worse results compared to the baseline? Also, in the original brain-to-text 2025 repo, there are already some wav2vec/whisper similar structures. \n\n- Utilizing whisper model into brain to text decoding is not new as well. What is the difference between this paper and NeuSpeech paper using MEG data? Merely the data sampling rate and the shape difference won't bring too much novelty."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9eEkEdSB0M", "forum": "gcwzqA8iWa", "replyto": "gcwzqA8iWa", "signatures": ["ICLR.cc/2026/Conference/Submission9410/Reviewer_1X1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9410/Reviewer_1X1X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072129980, "cdate": 1762072129980, "tmdate": 1762921014592, "mdate": 1762921014592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}