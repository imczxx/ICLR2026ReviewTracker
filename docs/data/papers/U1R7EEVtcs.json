{"id": "U1R7EEVtcs", "number": 14668, "cdate": 1758241268222, "mdate": 1759897356125, "content": {"title": "Transductive and Learning-Augmented Online Regression", "abstract": "Motivated by the predictability of real-life data streams, we study online regression when the online learner has access to predictions about future examples. In the extreme case, called transductive online learning, the sequence of examples is revealed to the learner before the game begins. Here, we fully characterize the expected regret by the fat-shattering dimension, establishing a separation between transductive online regression and online regression, akin to that between online and transductive online classification. Then, we generalize this setting by allowing for noisy or \\emph{imperfect} predictions about future examples. Using our results for the transductive online setting, we develop an online learner whose expected regret matches the worst-case regret, improves smoothly with prediction quality, and significantly outperforms the worst-case regret when future example predictions are precise, achieving performance similar to the transductive online learner. This enables learnability for previously unlearnable classes under predictable examples, aligning with the broader learning-augmented model paradigm.", "tldr": "We show that having access to good predictions about future examples can lead to better regret bounds in online regression.", "keywords": ["Online Learning", "Transductive Online Learning", "Algorithms with Predictions"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20f623c46489c6545a99cc74be23551cdad36b13.pdf", "supplementary_material": "/attachment/eca4e8d58c3994bbb101616c35e57563cf62ac1a.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies online learning under two related settings: the *transductive* setting, where the entire sequence of features or contexts is revealed to the learner in advance, and the *learning-augmented* setting, where the learner has access to a predictor that provides (possibly noisy) forecasts of future examples. The authors analyze two types of prediction errors—the zero-one error and the $\\epsilon$-ball error defined with respect to a metric on the context space. The main contributions are a minimax-optimal bound for the transductive setting and upper bounds for online learning with imperfect predictions, where the latter scale with the transductive regret multiplied by a term that depends on the predictor’s error."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The main contribution, as I understand it, is the learning-augmented setting, which extends the classification scenario from Raman & Tewari (2024) to regression.  \n\n2. The paper introduces some new techniques for handling the regression case and connects it to the concept of transductive regret."}, "weaknesses": {"value": "1. I have a major concern regarding the claimed novelty of the minimax regret result and its connection to the fat-shattering dimension in transductive regression. This connection, in my understanding, was already established in Lemma 11 and Lemma 12 of A. Rakhlin, O. Shamir, and K. Sridharan, *“Relax and Randomize: From Value to Algorithms,”* NeurIPS 2012. Their work derived a minimax-optimal regret bound for the transductive setting expressed via the (classical) Rademacher complexity, and it is a standard argument to convert such Rademacher-based bounds into equivalent statements in terms of the fat-shattering dimension, as done in the present paper. Moreover, Rakhlin et al. not only established this characterization but also provided an oracle-efficient algorithm achieving the optimal regret, whereas the current submission only re-derives the bound without offering a constructive realization.\n\n2. Given point 1 above, the actual contribution of the paper lies in the learning-augmented setting, where the authors establish only an upper bound. However, it remains unclear how tight this bound is with respect to both the prediction error and the transductive regret. Moreover, the overall results seems (conceptually) largely follows Raman & Tewari (2024)."}, "questions": {"value": "Could you clarify the claimed novelty of your transductive minimax regret result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wOwttfg62N", "forum": "U1R7EEVtcs", "replyto": "U1R7EEVtcs", "signatures": ["ICLR.cc/2026/Conference/Submission14668/Reviewer_JgLE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14668/Reviewer_JgLE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606158064, "cdate": 1761606158064, "tmdate": 1762925039094, "mdate": 1762925039094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the online regression in both the transductive and learning augmented settings.  In the transductive setting, it establishes the near-optimal regret bound for $\\ell_1$ loss. In the online regression with predictions setting, it proposes the algorithms that adapt smoothly to the quality of the Predictor. They also conduct numerical experiments to verify the effectiveness of their methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well organized and easy to follow. The authors also provide several applications of the online regression setting.\n* The theoretical results resolve an open problem left by previous studies.\n* The work presents a unified and elegant theoretical framework.\n* The paper also includes numerical experiments to support the theoretical claims."}, "weaknesses": {"value": "**W1.** I am confused about the motivation of this paper, especially in Lines 80–83. Why do the authors investigate the use of future examples to obtain better-than-worst-case regret bounds?\n\n**W2.** In Line 131, the authors note that the setting where the learner has access to samples is unrealistic. Therefore, I wonder whether the contribution of the first results is actually limited.\n\n**W3.** What is the valid range of $p$ in Corollary 1.3?\n\n**W4.** The equation in Lines 252–255 exceeds the column width.\n\n**W5.** The algorithm design and analysis of this paper are largely based on Raman & Tewari (2024), so I have serious concerns about the novelty of this work. I hope the authors can highlight what specific challenges arise in the online regression setting compared with prior work.\n\n**W6.** Can the authors conduct experiments on real-world datasets to further validate their method?\n\n**W7.** The authors place the pseudocode of the algorithm in the appendix, but I believe it should be included in the main text for better readability."}, "questions": {"value": "Refer to __Weakness__"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tD3ZI8231b", "forum": "U1R7EEVtcs", "replyto": "U1R7EEVtcs", "signatures": ["ICLR.cc/2026/Conference/Submission14668/Reviewer_AyX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14668/Reviewer_AyX2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814472989, "cdate": 1761814472989, "tmdate": 1762925038506, "mdate": 1762925038506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies online regression when the learner has access to predictions (ranging from full knowledge of the unlabeled input sequence to noisy/evolving forecasts). It proves that in the transductive setting the minimax expected regret is governed by the fat shattering dimension, allowing learning and yielding tighter rates for classes like bounded variation. It then designs learning augmented algorithms with restarts and multiplicative weights over interval granularities so that regret interpolates between worst case and transductive bounds as predictor quality improves. A small synthetic experiment qualitatively illustrates the gap."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear theoretical separation: transductive regret depends on fat shattering (not sequential), enabling new learnability results for key classes\n- Interpolation guarantees with concrete mistake models (0-1 and eps-ball) and constructive algorithms (restarts + MW over interval counts)\n- Useful instantiations (Lipschitz, k-fold aggregations, bounded variation) with near-matching bounds that make the results tangible"}, "weaknesses": {"value": "- Practicality/constructiveness: the main transductive upper bound is largely minimax style; the explicit MWA-based learner is suboptimal, and computational aspects (cover construction, oracle assumptions, runtime/memory) are under-specified. It'd be useful to clarify its feasibility and provide a practical surrogate.\n- Empirics are thin: only a single synthetic study; no evaluation that varies predictor quality M or eps, or uses no real data streams, or compares to strong online baselines with imperfect forecasts. A more systematic experimental section would greatly strengthen the paper."}, "questions": {"value": "Please address the concerns raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pf8z8KnssU", "forum": "U1R7EEVtcs", "replyto": "U1R7EEVtcs", "signatures": ["ICLR.cc/2026/Conference/Submission14668/Reviewer_RXvk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14668/Reviewer_RXvk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762118164119, "cdate": 1762118164119, "tmdate": 1762925038067, "mdate": 1762925038067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies online regression beyond the standard adversarial setting. In particular, the authors provide results for two settings: \n- The transductive setting, where the sequence of examples (but not their corresponding labels) is revealed up front, \n- The prediction setting, where the learner has access to a predictor of the sequence of examples, and its performance depends on the accuracy of such predictors.\n\nFor the transductive setting, the authors prove that the combinatorial parameter underlying learnability is the fat-shattering dimension, which is strictly smaller than the sequential fat-shattering dimension, which characterizes online learnability in general. This result is existential, as it follows from a minimax argument. The authors also provide a simple learning algorithm (compute an eps-cover of the examples in the sequence and run MWU on them) with a suboptimal rate. \n\nThe authors also provide positive results for the learning augmented setting, where the learner receives predictions on the examples in advance. Here, two metrics of the predictor sub-routines are used: the $0/1$ loss, and the $\\varepsilon$-ball loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Online Regression is a classical topic with vast applicability.\n- The theoretical results are sound and complete. In particular, the general result for transductive learning is nearly tight. \n- The main body is generally well written."}, "weaknesses": {"value": "- The regret rate for the transductive setting is only existential. This fact should be stressed earlier in the paper, at least in the introduction. It is now only mentioned in Section 3. \n- I find it challenging to assess the novelty and technical contribution of the paper:\n- The transductive result seems to follow easily from the adversarial analysis, replacing the sequential Rademacher complexity with its non-sequential version, and using that the sequence of examples is known in advance. The practical algorithm for that setting is also fairly simple. \n- The algorithm for the 0/1 loss (which is then also used for the eps-ball loss) seems to heavily rely on Raman & Tewari.\n\nMinor Comments: \n- line 130 “online learning” -> “online learnable” \n- line 159: L_{loss} is defined a couple of pages later\n- The bibliography is disorganized: multiple styles are used. In particular, the NeurIPS 23 paper by Hanneke et al is cited in two different entries, the JMLR 15 by Rakhlin et al is also cited twice, while the citation in lines 684 and 685 is missing the name of the journal/conference."}, "questions": {"value": "Could you please elaborate on the novelty and the technical contributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DEAAZoi8dj", "forum": "U1R7EEVtcs", "replyto": "U1R7EEVtcs", "signatures": ["ICLR.cc/2026/Conference/Submission14668/Reviewer_3U75"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14668/Reviewer_3U75"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121112707, "cdate": 1762121112707, "tmdate": 1762925037689, "mdate": 1762925037689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}