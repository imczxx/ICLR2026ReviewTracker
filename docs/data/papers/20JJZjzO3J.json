{"id": "20JJZjzO3J", "number": 13479, "cdate": 1758218376409, "mdate": 1763062001575, "content": {"title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "abstract": "Text-to-image (T2I) models are increasingly used for synthetic dataset generation, but generating synthetic training data to improve fine-grained classification performance remains challenging. Fine-tuning the T2I model with a few real examples can help generate more appropriate synthetic training data; however, this fine-tuning may also introduce overfitting and reduce diversity in the generated samples. We propose a fine-tuning strategy BOB (Beyond OBjects) for mitigating these concerns. Given a small set of real examples, we first describe them using class-agnostic attributes such as scene background and object pose. We then explicitly condition on these attributes during fine-tuning of the T2I model and marginalize them out during generation. This design mitigates overfitting, thus preserving the T2I model’s generative prior and reducing estimation errors, and further minimizes unintended inter-class associations. Extensive experiments across multiple T2I models, backbones, and datasets demonstrate state-of-the-art performance in low-shot fine-grained classification when augmented with synthetic data. Concretely, BOB outperforms DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning a CLIP classifier with five real images augmented with 100 synthetic images). Additionally, in three of the four datasets, the fine-tuning downstream models with synthetic data generated from BOB and five real images achieves better performance than fine-tuning with 10 real images. Collectively, BOB outperforms prior art in 18 of 24 experimental settings, with 2+% accuracy improvements in 14 of these settings.", "tldr": "We propose a fine-tuning and generation strategy for text-to-image models that preserves diversity, achieving state-of-the-art results in few-shot fine-grained classification.", "keywords": ["Synthetic Data Generation", "Diffusion Model", "Data Augmentation", "Image Classification", "Fine-grained Classification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/29f798278b80eb7f4adc028a7ab0299490b52df1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors focus on generating effective synthetic training data, given few-shot examples of the target classes. They build upon existing methods which fine-tune generative models with the few-shot data, to also consider and marginalize out class-agnostic attributes. Specifically, they focus on background and pose. To do this, they identify sets of backgrounds and poses with Qwen2.5-VL, and expand the caption to explicitly condition on these attributes while fine-tuning the generative model. Later, they can randomly select from the sets to marginalize out their effects.\n\nThey present main results on 5 datasets, two few-shot settings, three backbones, and attached to seven existing data generation techniques, showing gains in most settings. They also use two long-tailed datasets. For additional analysis, they compare distributions, explore several captioning models, and include an ablation on the marginalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1) The results show consistent improvement.\n\nS2) A wide range of settings is considered, which provides good coverage of base model and several numbers of shots.\n\nS3) The methodology is straightforward and well-explained.\n\nS4) The paper is written clearly."}, "weaknesses": {"value": "W1) (related to Q1) There are specific downsides to the chosen evaluation setting, specifically with the datasets chosen when compared to what is done in other work (e.g. Kim 2024, also [A]). The five datasets in the chosen setting are only fine-grained datasets--this is much less comprehensive than the 10 datasets used in the other setting. In the other setting, there are also more general datasets (e.g. ImageNet, Caltech101), some out-of-distribution datasets (EuroSAT, DTD). \n\nW2) The work seems to under-emphasize the effect of the underlying generative model. Specifically, (a) the related work section misses discussion of the underlying text-to-image models and (b) the chosen generation models (SDv1.5 and SDv2.1) are far from state-of-the-art, given that many much stronger models are now available (e.g. SDXL, SDv3, FLUX, QwenImage). Given that synthetic data effectiveness is highly impacted by the underlying generative model, this would would be much more relevant to the current state if it explored or discussed more modern generative models.\n\nW3) (not a major flaw, but could be better) The dataset chosen in the long-tail analysis section seems misaligned with the purpose, which appears to be showing scaling. While scaling the number of images, this dataset also changes classes, which is much less easy to compare. It would be cleaner to use the same classes and scale the number of images. The dataset choice can also be a bit misleading for readers, who might expect a long-tail dataset to be providing insights into where the method is better / worse (which are missing, as it is rather uniform).\n\nW4) The takeaway in the section \"is it distillation\" seems a bit stronger than the provided results. It is conceivable that the model is distilling knowledge, but there are other factors bottlenecking the process further. The whether this is distillation is also a theoretical framework--if it is not distillation, it would be stronger to provide an alternative framework.\n\nSmall notes\nw1) In Table 2, Flower-LT, Many, the choice to pick one 100 for bolding and the other for underlining is potentially misleading. It would be more clear to use the same notation.\n\n\n[A] Diversified in-domain synthesis with efficient fine-tuning for few-shot classification, da Costa et al."}, "questions": {"value": "Q1) Why did you choose the setting from Wang & Chen 2025, as opposed to other settings like Kim et al. 2024? (related to W1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NlWCzw37TR", "forum": "20JJZjzO3J", "replyto": "20JJZjzO3J", "signatures": ["ICLR.cc/2026/Conference/Submission13479/Reviewer_DX7h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13479/Reviewer_DX7h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761161666463, "cdate": 1761161666463, "tmdate": 1762924097616, "mdate": 1762924097616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "mNJW7BtnKw", "forum": "20JJZjzO3J", "replyto": "20JJZjzO3J", "signatures": ["ICLR.cc/2026/Conference/Submission13479/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13479/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763062000789, "cdate": 1763062000789, "tmdate": 1763062000789, "mdate": 1763062000789, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BOB (Beyond Objects), a two-stage framework for few-shot text-to-image (T2I) data generation. In the first stage, context preservation, the T2I model is fine-tuned with enriched captions containing class names, backgrounds, and poses extracted by a vision-language model. In the second stage, context marginalization, backgrounds and poses are randomly recombined across the dataset to break class-context correlations. Experiments on multiple fine-grained and long-tail datasets show consistent improvements over seven baselines, demonstrating the method’s effectiveness in generating diverse and less biased synthetic data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper addresses an important and practical problem, reducing spurious correlations in few-shot text-to-image (T2I) data generation, and demonstrates clear performance gains on fine-grained and long-tail classification tasks.\n\n2.The proposed two-stage framework (context preservation + context marginalization) is conceptually simple, easy to implement, and yields consistent improvements across datasets and backbones."}, "weaknesses": {"value": "1.The method feels largely like an enhanced prompt optimization pipeline. Although the authors combine LoRA fine-tuning (context preservation) with dataset-level randomization (context marginalization), the overall novelty is limited.\n\n2.The causal explanation between foreground and background is not new — similar causal interpretations (e.g., back-door adjustment) have appeared in previous few-shot or domain generalization literature.\n\n3.It would be more convincing if the proposed method were demonstrated as a plug-and-play component applicable to existing baselines (e.g., Diff-II, DataDream), instead of only testing on SD v1.5/v2.1."}, "questions": {"value": "1.In Figure 1, the extracted “pose” appears to be semantic information about the scene (e.g., flying, landing, on water) rather than geometric pose?\n\n2.Could you provide an ablation on the number of synthetic images per class?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cchoBVhPbQ", "forum": "20JJZjzO3J", "replyto": "20JJZjzO3J", "signatures": ["ICLR.cc/2026/Conference/Submission13479/Reviewer_Rnaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13479/Reviewer_Rnaj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515640243, "cdate": 1761515640243, "tmdate": 1762924096741, "mdate": 1762924096741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BOB (Beyond OBjects), a fine-tuning and generation strategy to create synthetic training data for fine-grained classification from T2I models. The key ideas are: (1) Context preservation during T2I fine-tuning, which extracts class-agnostic attributes (background, pose) with a captioning model and injects them into per-image text prompts so the model learns to faithfully render context without overfitting to few exemplars; and (2) Context marginalization at generation: when synthesizing images, randomly sample background/pose pairs from a global caption bank (across classes) approximating an intervention that breaks spurious class–context links. Across Aircraft, CUB, Cars, Pets and three backbones (CLIP-B/16, ResNet-50, MAE-B/16), BOB improves few-shot accuracy and also helps in long-tail regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  It presents a clear causal framing for removing spurious context that integrates with existing SD backbones.\n\n2. The experiment is conducted on multiple datasets and backbones to demonstrate its good performance."}, "weaknesses": {"value": "1. BOB relies on captions extracted from only 5–10 real exemplars per class using a VLM (Qwen-VL-7B) to describe background and pose attributes. However, since modern large language models already possess rich world knowledge about object appearances and environments, this dependence on limited real images could be avoided. By prompting an LLM directly (e.g., “Describe possible backgrounds and poses for an aircraft image”), one could construct a large and diverse attribute bank without relying on the few available examples. Such a strategy would naturally produce richer contextual variations (e.g., aircraft on a lawn, snowfield, or desert) that text-to-image models can easily render. Even if some generated contexts are physically implausible, the resulting diversity could significantly enhance classifier robustness and help it focus on the object foreground rather than dataset-specific correlations.\n\n2. Although the paper presents a causal formulation based on the back-door adjustment via context marginalization, there is no empirical verification of the claimed causal effect. The authors assert that random sampling of background and pose achieves P(X∣do(Y)), but they do not quantify whether spurious correlations between class labels and contextual cues are actually reduced. Analyses such as mutual-information estimation, causal discovery, or counterfactual tests could provide such evidence. In addition, Modeling Z as just background+pose may be an oversimplification. Other nuisance variables (e.g., surrounding objects or complex scene elements) can still carry class-specific signals. Without sufficiency or sensitivity analyses on the definition of Z, the causal justification remains largely conceptual rather than empirically grounded.\n\n3.  Overall, despite solid experimental execution and clear motivation, the novelty and causal rigor are limited. The core idea, i.e., sampling cross-class contexts, resembles a heuristic augmentation strategy rather than a verified causal intervention."}, "questions": {"value": "1. Could the authors explain why the background and pose attributes must be extracted from the limited 5–10 real exemplars rather than generated directly via prompting a large language model? Would using an LLM to describe possible backgrounds and poses for each class (e.g., “Describe possible backgrounds and poses for an aircraft”) yield richer contextual diversity and potentially better generalization?\n\n2. Have the authors considered constructing a larger background/pose bank, including imaginative or less realistic scenes (e.g., aircraft on snow or desert), to improve synthetic diversity? Would such diversity benefit classifier robustness even if some contexts are implausible?\n\n3. How do the authors empirically verify that random sampling of background and pose actually implements the intended intervention P(X∣do(Y))? Are there any quantitative analyses (e.g., reduction in mutual information between class and background, causal discovery, or counterfactual tests) to support the causal claim?\n\n4. Why is Z defined solely as {background, pose}? Have the authors examined whether additional nuisance factors (e.g., nearby objects or composition) also correlate with class identity and thus need to be marginalized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QzXzwQP0w7", "forum": "20JJZjzO3J", "replyto": "20JJZjzO3J", "signatures": ["ICLR.cc/2026/Conference/Submission13479/Reviewer_4gDk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13479/Reviewer_4gDk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620655051, "cdate": 1761620655051, "tmdate": 1762924096377, "mdate": 1762924096377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To generate data for fine-grained classification, the paper proposes a method called BOB to guide text-to-image (T2I) generation. BOB first extracts class-agnostic attributes from real images and fine-tunes the T2I model. It then generates images with random attributes to augment data for fine-grained classification. Experiments demonstrate that this method is effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-motivated.\n- The paper is readable.\n- Numerous experiments demonstrate that this method is effective."}, "weaknesses": {"value": "1. The BOB method is too simple and lacks novelty. Obtaining more detailed captions and combining image elements to generate new images is rather trivial.\n\n2. If the T2I model directly generates synthetic data without fine-tuning—by simply adjusting prompts or random seeds—to augment real data, how would the downstream classification performance compare?\n\n3. In Table 1, are the training epochs for the “Read only” method and the “BOB” model the same? If so, it means that “Read only” was trained for fewer steps, which is unfair. A fair comparison should ensure both methods are trained for the same number of steps."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yGJNPvAMlt", "forum": "20JJZjzO3J", "replyto": "20JJZjzO3J", "signatures": ["ICLR.cc/2026/Conference/Submission13479/Reviewer_jwHB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13479/Reviewer_jwHB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738490788, "cdate": 1761738490788, "tmdate": 1762924096042, "mdate": 1762924096042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}