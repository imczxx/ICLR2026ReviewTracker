{"id": "9k7bvBVenZ", "number": 11004, "cdate": 1758186641226, "mdate": 1759897615081, "content": {"title": "FedMuon: Federated Learning with Bias-corrected LMO-based Optimization", "abstract": "Recently, a new optimization method based on the linear minimization oracle (LMO), called Muon, has been attracting increasing attention since it can train neural networks faster than the existing adaptive optimization methods, such as Adam.\nIn this paper, we study how Muon can be utilized in federated learning.\nWe first show that straightforwardly using Muon as the local optimizer of FedAvg does not work since the LMO is a biased operator.\nWe then propose FedMuon, which can mitigate this issue and can converge to the stationary point.\nWe also analyze how solving the LMO approximately affects the convergence rate and find that, surprisingly, FedMuon can converge for any number of Newton-Schulz iterations, while it can converge faster as we solve the LMO more accurately.\nThrough experiments, we demonstrated that FedMuon can outperform the state-of-the-art federated learning methods.", "tldr": "", "keywords": ["fderated learnig", "muon", "linear minimizatio oracle"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2ba3d76123c3d81d0254c87bb62e6b07894abca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies how to adapt the recently proposed Muon optimizer, an LMO-based method, to the federated learning setting. The authors show that directly applying Muon as a local optimizer fails to converge due to the inherent bias of the LMO operator. To address this, they propose FedMuon, which introduces bias correction via control variates similar to SCAFFOLD. The paper provides a detailed convergence analysis under both exact and approximate LMOs, showing that FedMuon converges for any number of Newton–Schulz iterations, and converges faster when the LMO is solved more accurately. Experiments on FashionMNIST and CIFAR-10 demonstrate superior performance in both homogeneous and heterogeneous settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides the first formal analysis of bias introduced by LMO in the federated setting and rigorously proves that straightforwardly applying Muon fails to converge.\n2. The author establish the convergence under both exact and inexact LMOs, and clarifying how the number of Newton–Schulz iterations affects the rate.\n3. The experiments align well with theoretical findings."}, "weaknesses": {"value": "1. The algorithm section (Section 4) only offers an intuitive, SCAFFOLD-like rationale for bias mitigation, but lacks formal theoretical support. It would be helpful to include a short proof sketch or lemma explicitly showing which terms or steps contribute to bias reduction.\n2. Appendix D includes an example to prove why LocalMuon does not converge, but it would be more convincing to show that FedMuon can indeed converge under the same problem setting.\n3. While illustrating the bias issue with a divergence example is interesting, similar bias phenomena commonly appear in other contexts such as bilevel optimization. A typical and simple approach in those settings is to communicate the momentum across clients in LocalMuon. However, the paper lacks discussion on why the authors chose not to adopt such straightforward momentum communication and instead opted for a control-variate-based correction mechanism.\n4. There is no discussion of heterogeneous settings in the theoretical section. Given that the experiments include heterogeneous data distributions, the theoretical analysis should also address how FedMuon behaves under such non-IID conditions.\n5. The empirical validation is confined to two small-scale vision datasets (FashionMNIST, CIFAR-10). The empirical validation should include large-scale or non-vision tasks to better demonstrate how FedMuon scales to realistic federated learning workloads or LLM fine-tuning scenarios.\n6. Figure 2 provides an example of how the number of Newton–Schulz iterations $T$ affects performance. However, prior works such as [1] and [2] commonly fix $T=5$, and [3] shows that increasing beyond 5 yields no performance gain but adds unnecessary wall-clock time. Thus, the absence of $T\\geq5$ in Figure 2 makes the empirical validation less convincing.\n\n[1] Jordan et al., Muon: An optimizer for hidden layers in neural networks, 2024.\n\n[2] Refael et al., Sumo: Subspace-aware moment-orthogonalization for accelerating memory-efficient llm training, 2025.\n\n[3] Semenov et al., Benchmarking Optimizers for Large Language Model Pretraining, 2025."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bb30YWjWmx", "forum": "9k7bvBVenZ", "replyto": "9k7bvBVenZ", "signatures": ["ICLR.cc/2026/Conference/Submission11004/Reviewer_YvBN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11004/Reviewer_YvBN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760980307024, "cdate": 1760980307024, "tmdate": 1762922187999, "mdate": 1762922187999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedMuon, a federated optimization algorithm that extends the recently proposed Muon optimizer—an LMO (Linear Minimization Oracle)-based method—into the federated learning setting. The authors identify that directly applying Muon in FedAvg (termed LocalMuon) fails to converge due to the bias of the LMO operator. To mitigate this, FedMuon introduces a bias-correction mechanism inspired by SCAFFOLD, along with a theoretical convergence analysis that also accounts for inexact LMO computations via the Newton–Schulz iteration. The authors provide both theoretical results and empirical validation on benchmark datasets (FashionMNIST, CIFAR-10) demonstrating FedMuon’s improved convergence and accuracy over existing methods such as FedAvg, FedAdam, and SCAFFOLD."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Novel and technically solid contribution.\nThe paper identifies a subtle but important issue (bias of the LMO in federated settings) and provides a theoretically justified correction method. The proofs (Theorem 1–3) are rigorous and clearly stated.\n\n2.Theoretical generality.\nThe analysis handles both exact and inexact LMOs, showing convergence under any number of Newton–Schulz iterations—a strong and non-trivial result rarely seen in this line of work.\n\n3.Experimental validation.\nThe experiments convincingly show FedMuon outperforming strong baselines under both homogeneous and heterogeneous data distributions. The empirical trends match the theoretical predictions (faster convergence as LMO accuracy increases).\n\n4.Bridging two active research areas.\nThe paper effectively connects recent LMO-based optimization (Muon) with federated learning theory, which is timely and relevant for distributed LLM optimization"}, "weaknesses": {"value": "1.Limited experimental diversity.\nThe experiments are conducted only on small-scale benchmarks (FashionMNIST and CIFAR-10). Demonstrating results on larger or more realistic FL setups (e.g., cross-device or NLP tasks) would strengthen the empirical claims. ViT model or roberta-base model，imagenet,CIFAR100 data.\n\n2.The proposed method simply combines the MUON and SCAFFOLD algorithms, and its communication cost is about twice that of the original approach.\n\n3.The experimental results do not show a significant advantage.\n\n4.The theoretical analysis of the paper is solid, but there is still room for improvement in the writing and experimental sections."}, "questions": {"value": "1. Can the communication cost be further reduced?\n\n2.Is the method still effective for training and fine-tuning large models?\n\n3.Can the code be open-sourced?\n\n4.The algorithm’s line 8 is written differently from the MUON optimizer — why is it designed this way?\n\n5.Is the number of local iterations too small（K=5） ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ArDoK4bPey", "forum": "9k7bvBVenZ", "replyto": "9k7bvBVenZ", "signatures": ["ICLR.cc/2026/Conference/Submission11004/Reviewer_paxH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11004/Reviewer_paxH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496055802, "cdate": 1761496055802, "tmdate": 1762922187672, "mdate": 1762922187672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on improving federated learning optimization by addressing the bias issues when applying the Muon optimizer, which uses Linear Minimization Oracle (LMO), in the FedAvg framework. The authors show that directly using Muon within FedAvg, referred to as LocalMuon, fails to converge due to the bias introduced by the LMO. To overcome this, they introduce FEDMUON, an optimization algorithm that incorporates a bias-correction mechanism inspired by SCAFFOLD, ensuring convergence. The theoretical analysis includes the convergence rate under inexact LMO computations, handled via Newton-Schulz iterations. Experiments on two benchmark datasets, FashionMNIST and CIFAR-10, demonstrate that FEDMUON outperforms several existing methods, including FedAvg, FedAvg (Adam), SCAFFOLD, and SCAFFOLD (Adam)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "$\\cdot$ The paper is well-motivated. Muon can outperform traditional adaptive optimizers like Adam in centralized settings, but its direct application to federated learning, specifically in FedAvg, fails to converge due to bias introduced by the LMO.\n\n$\\cdot$ The use of Newton-Schulz iterations to approximate the Linear Minimization Oracle (LMO) in FEDMUON is a key innovation, as it enables efficient bias correction while maintaining strong convergence guarantees even with inexact LMO solutions.\n\n$\\cdot$ The experimental results in the paper are highly consistent with the theoretical analysis, supporting the convergence guarantees of FEDMUON derived in the theory."}, "weaknesses": {"value": "$\\cdot$ It seems that the authors just incorporate the existing Muon to SCAFFOLD framework, which is just incremental. The authors should clarify their original contribution.\n\n$\\cdot$ The experiments are conducted on small datasets and simple models. Since optimizers like Adam perform better with larger models, comparisons on more complex models like Transformers would provide a better evaluation of FEDMUON's scalability and effectiveness.\n\n$\\cdot$ It would be better to report the time cost or communication cost of FEDMUON and baseline methods to empirically demonstrate that FEDMUON's advantages hold in terms of efficiency as well as performance."}, "questions": {"value": "$\\cdot$ Can FEDMUON be effective on larger models, such as transformers, and not just small-scale datasets and simple models?\n\n$\\cdot$ Besides SCAFFOLD from 2020, can the authors compare FEDMUON with more advanced federated learning algorithms？\n\n$\\cdot$ Could the authors report the communication or time costs of FEDMUON and baseline methods to demonstrate its efficiency advantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4R8VeefRJl", "forum": "9k7bvBVenZ", "replyto": "9k7bvBVenZ", "signatures": ["ICLR.cc/2026/Conference/Submission11004/Reviewer_B6xb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11004/Reviewer_B6xb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741544588, "cdate": 1761741544588, "tmdate": 1762922187207, "mdate": 1762922187207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedMuon, an algorithm improving usage of Muon in federated learning. They propose Scaffold-like modification in Muon's momentum term before LMO and observe improvement in stability. They also show some theoretical analysis in spectrum norm."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The writing is clear and easy to follow."}, "weaknesses": {"value": "I'm not convinced by the paper's solidity and novelty.\n1. The analysis in Section 3 regarding Muon’s failure to converge in FL seems rather trivial. Similar reasoning could apply to any momentum-based optimization algorithm. This claim should be supported by experimental evidence. In fact, based on the only experiments on LocalMuon shown in Figure 1, I don’t observe significant convergence issues. While there are some oscillations, I believe standard techniques such as learning rate or hyperparameter tuning could easily address them.\n\n2. The modification is just including Scaffold-like updates in Muon, which seems quite limited.\n\n3. The theoretical analysis is standard by combing Muon and Scaffold and classical SGD together. \n\n4. It’s already 2025, yet the experiments are limited to FashionMNIST and CIFAR-10 under very restricted settings, with only two baselines (FedAvg and SCAFFOLD). This level of evaluation would have been insufficient even several years ago. Moreover, the final accuracy shows almost no improvement over the baselines."}, "questions": {"value": "What weight decay term was used in the experiments? This factor has a significant impact on the results, and certain related techniques (e.g., [1]) can help stabilize training.\n\n[1] FedNAR: Federated Optimization with Normalized Annealing Regularization"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eZHUSiewy0", "forum": "9k7bvBVenZ", "replyto": "9k7bvBVenZ", "signatures": ["ICLR.cc/2026/Conference/Submission11004/Reviewer_E1uB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11004/Reviewer_E1uB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762898828401, "cdate": 1762898828401, "tmdate": 1762922186681, "mdate": 1762922186681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}