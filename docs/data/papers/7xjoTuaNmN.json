{"id": "7xjoTuaNmN", "number": 5295, "cdate": 1757894725882, "mdate": 1759897982627, "content": {"title": "OpenThoughts: Data Recipes for Reasoning Models", "abstract": "Reasoning models have made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best train-\ning recipes for reasoning since state-of-the-art models often rely on proprietary\ndatasets with little to no public information available. To address this, the goal of\nthe OpenThoughts project is to create open-source datasets for training reasoning\nmodels. Our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model\ntrained on public reasoning data to match DeepSeek-R1-Distill-32B on standard\nreasoning benchmarks such as AIME and LiveCodeBench. We then improve\nour dataset further by systematically investigating each step of our data genera-\ntion pipeline with 1,000+ controlled experiments, which led to OpenThoughts3.\nScaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields\nour OpenThinker3-7B model, which achieves state-of-the-art results: 53% on\nAIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Dia-\nmond – improvements of 15.3, 17.2, and 20.5 percentage points compared to the\nDeepSeek-R1-Distill-Qwen-7B. All of our datasets and models are available on\nANONYMIZED.", "tldr": "Data pipeline analysis for training reasoning models", "keywords": ["Reasoning", "Data", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ed2282122b603f957f6a85a58f7de7ea464a2c1.pdf", "supplementary_material": "/attachment/4a737df0515311d2b0ba4cd7249fcc0e891a5d88.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the challenge of creating publicly available data recipes for training reasoning models. It introduces a data generation pipeline for creating open-source reasoning data by empirically selecting the most effective approach at each stage. Using this pipeline, the authors construct the OpenThoughts2-1M and OpenThoughts3 datasets. A model trained on this data, named OpenThinker3-7B, achieves state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper conducts thorough experiments and constructs open-source datasets, which paves the way for future research on reasoning models.\n\nS2: The paper is well-structured and easy to follow.\n\nS3: The paper addresses a practical data problem for reasoning model training and research, which is an important contribution to the development of AI."}, "weaknesses": {"value": "W1: The paper's aim to create open-source datasets is undermined by the proposed pipeline's high dependency on closed-source LLM APIs, such as GPT-4o. This creates a contradiction and harms the reproducibility of the work, as results can vary significantly depending on the specific API version used.\n\nW2: Some experimental conclusions lack rigor. For instance, in Section 3.6, the paper dismisses answer filtering strategies as ineffective because they did not outperform the baseline. However, this conclusion is not well-supported because the comparison is unfair: the baseline was trained on 63,200 samples, while the answer filtering strategies used only 31,600. This confounding variable makes it impossible to rule out that the baseline's superior performance stems from having more training data, rather than the ineffectiveness of filtering.\n\nW3: The paper lacks an in-depth analysis of some interesting and counterintuitive experimental results, failing to provide deeper insights. For example, Section 3.7 shows that while Qwen-32B has a lower average score than DeepSeek-R1, it outperforms all other models when used as a teacher. This phenomenon is counterintuitive, and a more thorough analysis of why a lower-scoring model makes a better teacher could offer valuable insights for future work.\n\nW4: While it is understandable that some experimental results are placed in the appendix due to space limitations, the paper frequently has a significant separation between the presentation of results and their corresponding analysis. This disjointed structure makes the paper difficult to read."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vUCYB5tMWM", "forum": "7xjoTuaNmN", "replyto": "7xjoTuaNmN", "signatures": ["ICLR.cc/2026/Conference/Submission5295/Reviewer_1sri"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5295/Reviewer_1sri"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616987337, "cdate": 1761616987337, "tmdate": 1762917993715, "mdate": 1762917993715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a large-scale empirical study aimed at identifying effective data recipes for improving LLM reasoning performance. The authors systematically analyze the impact of various data curation strategies, including question sourcing, filtering, deduplication, teacher selection, and dataset scaling. Many controlled experiments are conducted to evaluate how different design choices affect model reasoning ability under supervised fine-tuning. The paper also introduces an open dataset and provides detailed ablations to ensure reproducibility and transparency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper constructs a large and well-curated dataset specifically designed to enhance LLM reasoning capabilities. This contribution is practically meaningful, as data quality and composition have become increasingly crucial for reasoning-oriented LLM development. The released dataset and accompanying analysis provide a useful foundation for future research in reasoning and instruction tuning.\n\n- The empirical study is extensive and carefully executed. The authors evaluate a wide range of data curation factors, such as filtering, question mixing, deduplication, teacher selection, and dataset scaling. The conclusions are supported with detailed ablation studies. The experiments are systematic and transparent, offering clear evidence for each design choice and contributing valuable insights for the broader community."}, "weaknesses": {"value": "- This work feels closer to a technical report than a research paper. The study is solid and well-executed, but it lacks significant technical novelty. The proposed pipeline is straightforward and largely follows existing practices in data curation and reasoning dataset construction, with limited conceptual innovation.\n\n- The proposed dataset provides only marginal improvements over existing baselines. As shown in Table 1, the average performance gain over previous datasets is relatively small (around 2.1), which raises questions about the practical significance of the proposed data recipes. While the results are consistent and reproducible, the improvement is quite modest and may not justify the large experimental effort."}, "questions": {"value": "I do not have specific questions for the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vd5rhMk09V", "forum": "7xjoTuaNmN", "replyto": "7xjoTuaNmN", "signatures": ["ICLR.cc/2026/Conference/Submission5295/Reviewer_KbX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5295/Reviewer_KbX7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645779004, "cdate": 1761645779004, "tmdate": 1762917993381, "mdate": 1762917993381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OpenThoughts3, a scalable and systematic data curation pipeline for building reasoning models through supervised fine-tuning. The authors carefully analyze each step of the data generation process—question sourcing, filtering, deduplication, answer sampling, and teacher selection—through more than 1,000 controlled experiments, resulting in a high-quality open dataset of 1.2M examples. The resulting model, OpenThinker3-7B, achieves state-of-the-art results among open-data models, surpassing comparable baselines like DeepSeek-R1-Distill-7B and Nemotron-Nano-8B across math, code, and science benchmarks. The paper emphasizes that careful dataset design can rival or exceed RL-based approaches, and provides a reproducible open-source recipe for training reasoning models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe work excels in its thorough dissection of the data curation process, covering multiple strategies at every pipeline stage. This level of experimental rigor goes beyond most existing work, making the conclusions highly credible and reproducible.\n2.\tBy scaling the dataset to 1.2M examples and carefully selecting teacher models, the authors deliver competitive results on multiple reasoning benchmarks. The scaling curves and ablation studies clearly demonstrate the effectiveness of their approach.\n3.\tUnlike many proprietary reasoning models, the authors commit to full open-source release of datasets and models. This can significantly accelerate community research and lower the entry barrier for reasoning model development."}, "weaknesses": {"value": "1.\tI appreciate the design of the SFT data pipeline, but it’s hard not to notice how the entire work leans almost exclusively on this single training paradigm. In recent reasoning models, RL or curriculum strategies often play a big role in pushing performance further. Even if the authors didn’t run those experiments, a more thoughtful discussion or positioning would have made the contribution feel less one-dimensional.\n2.\tI’m left unsure whether the gains translate to broader reasoning capabilities. A few results on less standard or more language-heavy tasks would help a lot here. Right now, the claims around generalization feel more suggestive than demonstrated.\n3.\tThe paper runs a huge number of experiments, but says very little about why certain design choices actually help. For instance, why does a relatively simple teacher mix outperform a seemingly stronger one? What kinds of examples drive the improvements? Without some interpretability or qualitative perspective, the work risks feeling like an “empirical recipe” rather than a deeper contribution."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WWYYx4NjW8", "forum": "7xjoTuaNmN", "replyto": "7xjoTuaNmN", "signatures": ["ICLR.cc/2026/Conference/Submission5295/Reviewer_1PHv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5295/Reviewer_1PHv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791840735, "cdate": 1761791840735, "tmdate": 1762917993108, "mdate": 1762917993108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OpenThoughts2-1M and OpenThinker2-32B, the first publicly available open source dataset and model for reasoning tasks that match the performance of DeepSeek-R1-Distill-32B on AIME and LiveCodeBench. They also perform a series of 1000+ experiments to systematically improve their data generation pipeline to develop OpenThoughts3, which, combined with scaling to 1.2M examples and using QwQ-3B, yields OpenThinker3-7B, which vastly outperforms DeepSeek-R1-Distill-Qwen-7B on AIME, LiveCodeBench, and GPQA. Key findings include that sampling multiple answers from a teacher model is an effective strategy to scale the size of the training data, models with better performance are not necessarily better teachers, verification and answer filtering methods don’t lead to significant performance improvements, data quality trumps data diversity, and filtering questions based on LLM labeled difficulty or response length yields better results than using embedding based filters typical to pre-training."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated and carefully designed experiments leading to clear takeaways about best practices for SFT training data design for reasoning models.\n2. OpenThinker3-7B achieves the best average performance across all the evaluation benchmarks.\n3. Efforts to decontaminate training data by removing samples with high similarity to the benchmark instances. \n4. Interesting results: Question sourcing (simple synthetic questions perform comparably or even better than complex or manually curated pipelines), Question filtering (difficulty filtering and response length filtering work well for code and math, compared to fasttext classifiers or embedding-based methods that work well for pre-training), Best teacher model is not necessarily the best performing model."}, "weaknesses": {"value": "1. Should further investigate the impact of deduplication and sampling multiple answers because the results are very inconclusive, and exhibit too much variance across code, math, and science. It is also weird that the authors choose varying deduplication strategies across math, code, and science, but choose to pick x16 answer sampling per question for all domains, even though it is not the best across the board. This choice seems arbitrary, and the reasoning that it is better for scaling seems weird since training on much more data for essentially the same or worse performance is potentially just a waste of compute.\n2. Again, the conclusions about answer filtering seem weird, since they bring up the point that training on all the data instead of filtering low-quality data makes no difference, so training on all the data is better for scaling. However, an alternative perspective would be that training on these low-quality instances doesn’t add to the performance, so why should one waste compute on training with these instances? In other words, answer filtering strategies could be used to find a smaller, more effective dataset that retains the same level of performance as the full dataset.\n3. The difficulty filters used for answer filtering do not use any verifiers, like code execution with test suites, etc., which, while hard to scale, have shown a lot of promise in rejection fine-tuning-based pipelines (like RAFT) [1, 2, 3]. \n4. There is seemingly a big gap in performance between the best-performing OpenThoughts3-7B model and the teacher models used (QwQ, DeepSeekReasoner). While this is reasonable given the parameter size difference, the 10-point difference is pretty significant.\n5. Some of the more intriguing results warrant further exploration, like why a less capable teacher model is a better teacher or why different question filtering strategies work for specific domains.\n\n[1] Zheng, Kunhao, et al. \"What Makes Large Language Models Reason in (Multi-Turn) Code Generation?.\" arXiv preprint arXiv:2410.08105 (2024).  \n[2] Xiong, Wei, et al. \"A minimalist approach to llm reasoning: from rejection sampling to reinforce.\" arXiv preprint arXiv:2504.11343 (2025).  \n[3] Dong, Hanze, et al. \"Raft: Reward ranked finetuning for generative foundation model alignment.\" arXiv preprint arXiv:2304.06767 (2023)."}, "questions": {"value": "Do you explore potential explanations of the QwQ being the best teacher model as an observation of the capacity gap phenomenon [1]?\n\n[1] Zhang, Chen, et al. \"Towards the law of capacity gap in distilling language models.\" arXiv preprint arXiv:2311.07052 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tNr9sFn1L0", "forum": "7xjoTuaNmN", "replyto": "7xjoTuaNmN", "signatures": ["ICLR.cc/2026/Conference/Submission5295/Reviewer_wnkN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5295/Reviewer_wnkN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973326906, "cdate": 1761973326906, "tmdate": 1762917992831, "mdate": 1762917992831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}