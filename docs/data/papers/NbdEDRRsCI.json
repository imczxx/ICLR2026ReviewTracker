{"id": "NbdEDRRsCI", "number": 8314, "cdate": 1758078364879, "mdate": 1763725590677, "content": {"title": "Many Eyes, One Mind: Temporal Multi-Perspective and Progressive Distillation for Spiking Neural Networks", "abstract": "Spiking Neural Networks (SNNs), inspired by biological neurons, are attractive for their event-driven energy efficiency but still fall short of Artificial Neural Networks (ANNs) in accuracy. Knowledge distillation (KD) has emerged as a promising approach to narrow this gap by transferring ANN knowledge into SNNs. Temporal-wise distillation (TWD) leverages the temporal dynamics of SNNs by providing supervision across timesteps, but it applies a constant teacher output to all timesteps, mismatching the inherently evolving temporal process of SNNs. Moreover, while TWD improves per-timestep accuracy, truncated inference still suffers from full-length temporal information loss due to the progressive accumulation process. We propose MEOM (Many Eyes, One Mind), a unified KD framework that enriches supervision with diverse temporal perspectives through mask-weighted teacher features and progressively aligns truncated predictions with the full-length prediction, thereby enabling more reliable inference across all timesteps. Extensive experiments and theoretical analyses demonstrate that MEOM achieves state-of-the-art performance on multiple benchmarks. Code will be released on GitHub.", "tldr": "We propose MEOM, a unified temporal distillation framework that improves SNN accuracy and time flexibility across timesteps via multi-perspective supervision and progressive alignment to full-length prediction.", "keywords": ["Spiking Neural Networks", "Knowledge Distillation", "Neuromorphic Computing"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/032716c11a748f5c5d35134d23cf351c9073bff6.pdf", "supplementary_material": "/attachment/7acd504cda3097c29668c288fb666b61e6fec5c9.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents MEOM (Many Eyes, One Mind), a Knowledge distillation (KD) framework for spiking neural networks (SNNs). It employs Temporal Multi-Perspective Distillation (TMPD) to introduce temporal variances to the ANN teacher output. It also utilizes Temporal Progressive Distillation (TPD) to align with the full-length prediction for truncated inference progressively."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed MEOM considers the temporal variances of SNNs.\n2. This paper provides theoretical analyses for the proposed submodules."}, "weaknesses": {"value": "1. The motivation of TMPD requires further clarification. Lines 51-69 claim that it is improbable for outputs across timesteps to remain identical. A more effective strategy would incorporate diverse temporal supervisory signals. However, as illustrated in Fig. 1, both final logits and membrane potential distribution do not exhibit significant differences across timesteps. Furthermore, the impact of such variations on SNN performance has not been verified.\n2. The proposed TMPD is more like a data augmentation method that introduces perturbations in the temporal dimension, rather than providing richer temporal supervision. Theorems 1 and 2 merely demonstrate that introducing perturbations results in higher temporal covariance and lower gradient variance, thereby proving the effectiveness of this data augmentation approach. They do not prove that TMPD provides richer temporal supervision. I believe this data augmentation method is not only applicable to KD but also equally effective for general SNN training.\n3. The proposed MEOM method does not show significant performance improvement over state-of-the-art methods. Compared to TWSNN, its accuracy gains across various datasets are less than 1%.\n4. The overall training objective introduces three hyperparameters $\\alpha$, $\\beta$, and $\\gamma$. The impact of hyperparameter settings on the effectiveness of the proposed method remains unclear. The robustness of hyperparameter settings across different tasks is also unclear."}, "questions": {"value": "1. Please analyze the temporal variance of SNNs and its effect on SNN performance.\n2. Please analyze the impact and robustness of the hyperparameter introduced in MEOM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jfqT5QnCzH", "forum": "NbdEDRRsCI", "replyto": "NbdEDRRsCI", "signatures": ["ICLR.cc/2026/Conference/Submission8314/Reviewer_uLvn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8314/Reviewer_uLvn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381092289, "cdate": 1761381092289, "tmdate": 1762920241664, "mdate": 1762920241664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEOM, a novel SNN distillation framework that tackles two key issues: static teacher supervision and poor truncated inference. It employs Temporal Multi-Perspective Distillation (TMPD) to generate diverse teacher signals and Temporal Progressive Distillation (TPD) to align predictions across timesteps. MEOM achieves state-of-the-art results, demonstrating significant improvements in both final accuracy and performance under truncated inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written, clearly structured, and logically sound. \n2. The critique of existing TWD methods is insightful. Identifying the \"static teacher vs. dynamic student\" mismatch and the \"information loss in truncated inference\" as two practical and important problems is a key contribution. \n3. The experimental results are convincing. The authors demonstrate state-of-the-art results across multiple datasets (CIFAR, ImageNet) and architectures (ResNet, Spiking Transformer), while a dedicated \"time flexibility\" experiment shows the method's advantage in truncated inference. Furthermore, thorough ablation studies clearly isolate the contributions of each proposed component (TMPD and TPD) confirming their complementary benefits."}, "weaknesses": {"value": "1. Masking strategy in TMPD. The use of random masks in TMPD is a simple and effective way to generate diverse teacher signals. However, this might not be the optimal strategy. The mask generation is based on a fixed random sampling and does not adapt to the student's learning state or the characteristics of different timesteps. Exploring the connection between masking strategies and temporal-wise features could potentially lead to further performance improvements.\n2. Interplay between TMPD and TPD. The paper presents TMPD and TPD as two complementary components. However, TMPD is designed to introduce \"diversity\" while TPD is designed to enforce \"consistency,\" two goals that could, at some level, be seen as being in tension. While the experiments show their combination is effective, the paper could benefit from a deeper discussion on how these components work synergistically rather than counteracting each other."}, "questions": {"value": "1. Regarding the mask design in TMPD, ehe random mask strategy is simple and effective. Have you explored other, more sophisticated masking approaches, such as learnable masks or masks that are dynamically adapted based on the timestep? Do you think such strategies could offer additional performance gains?\n2. Regarding the progressive alignment in TPD, the TPD achieves progressive consistency by aligning the \"cumulative average prediction\" of consecutive timesteps. How does this compare to a simpler strategy of directly aligning the cumulative prediction at each step with the final prediction (at time T)? Do you think the \"step-wise\" smoothing is crucial for training stability?\n3. Regarding the synergy between TMPD and TPD, as TMPD is designed to introduce \"diversity\" and TPD is designed to enforce \"consistency,\" two goals that could somehow be in tension. Could you elaborate on a deeper level how these two seemingly opposing goals work synergistically within the MEOM framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ldB0NoFLIQ", "forum": "NbdEDRRsCI", "replyto": "NbdEDRRsCI", "signatures": ["ICLR.cc/2026/Conference/Submission8314/Reviewer_dxXD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8314/Reviewer_dxXD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725434037, "cdate": 1761725434037, "tmdate": 1762920241295, "mdate": 1762920241295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a knowledge distillation method for SNN training, called MEOM (Many Eyes, One Mind). The method consists of two parts: Temporal Multi-Perspective Distillation (TMPD), which creates the teacher's feature for each time-step, and Temporal Progressive Distillation (TPD), which averages activation across different time-steps and calculates CE losses between the average results and targets.\nExperiment shows that the method has better accuracy than BKDSNN with Spikformer-8-384 on ImageNet."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper organization is clear.\n2. The proposed distillation methods are understandable.\n3. The results in CIFAR10/100 is good."}, "weaknesses": {"value": "1. Limited novelty:\nThe proposed work introduces two techniques to enhance the performance of knowledge distillation. However, the TMPD method only provides a marginal improvement (around 0.3%) on CIFAR-10/100, which is within the range of training variance and thus not convincing. Moreover, the TPD approach is rather straightforward and not inherently tied to the distillation framework. Therefore, it could also be applied to conventional BPTT-based training, which weakens its novelty.\n\n2. Insufficient experimental validation on ImageNet:\nThe experiments on ImageNet lack sufficient ablation studies, making it difficult to attribute the performance gain solely to the proposed techniques. In addition, the authors should consider evaluating on a larger model, such as the S-8-768 structure, where BKDSNN achieves 79.9% accuracy, to strengthen the evidence.\n\n3. Missing analysis of training overhead:\nThe proposed distillation-based methods are expected to introduce additional training overhead, especially due to the TMPD methods. The authors should include a comparison of the GPU memory footprint and total GPU hours between the proposed methods and standard distillation baselines."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Td6MB5ahNH", "forum": "NbdEDRRsCI", "replyto": "NbdEDRRsCI", "signatures": ["ICLR.cc/2026/Conference/Submission8314/Reviewer_tdth"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8314/Reviewer_tdth"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813575949, "cdate": 1761813575949, "tmdate": 1762920240350, "mdate": 1762920240350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEOM (Many Eyes, One Mind), a unified knowledge distillation framework for spiking neural networks (SNNs). It integrates two complementary modules: Temporal Multi-Perspective Distillation (TMPD), which enhances temporal diversity via masked teacher features, and Temporal Progressive Distillation (TPD), which gradually aligns truncated and full-length predictions to improve temporal consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper clearly identifies the weaknesses of prior temporal distillation approaches and motivates the need for temporal diversity and consistency.\n2.The unification of TMPD (“Many Eyes”) and TPD (“One Mind”) provides an intuitive and theoretically supported structure.\n3.Experiments cover multiple benchmarks, showing both performance improvement and robustness under truncated inference.\n4.The paper’s exposition and proofs (information gain, convergence robustness) provide reasonable theoretical support for the design choices."}, "weaknesses": {"value": "1. TMPD uses random, static masks. While effective, the design choice appears heuristic. Could adaptive or learnable masks yield further benefit? Consider comparing random vs. learned mask distributions or adding mask diversity analysis. \n2. TPD enforces progressive step-wise alignment but may neglect long-range temporal dependencies. A long-range consistency variant or additional analysis on global alignment would make the argument more complete.\n3. Assuming the final timestep is globally optimal may not hold universally. A calibration or stability analysis across timesteps could validate this assumption, or adaptive target-timestep selection could be explored.\n4. Evaluation focuses on ResNet-style SNN backbones, generalization to more complex or recent SNN backbones (e.g., Spiking Transformer, hybrid architectures) remains unexplored."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a0nZiaPhYB", "forum": "NbdEDRRsCI", "replyto": "NbdEDRRsCI", "signatures": ["ICLR.cc/2026/Conference/Submission8314/Reviewer_BP7W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8314/Reviewer_BP7W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102589656, "cdate": 1762102589656, "tmdate": 1762920239430, "mdate": 1762920239430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}