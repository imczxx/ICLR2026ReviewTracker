{"id": "a4fSF5pGJq", "number": 3202, "cdate": 1757364858260, "mdate": 1762975581067, "content": {"title": "AToken: A Unified Tokenizer for Vision", "abstract": "We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs in a shared 4D latent space, optimizing without separate model designs. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. \nIn downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving 1.44/2.23 gFID on ImageNet for continuous/discrete tokens, and 48.7% on MMMU and 64.5% on VideoMME. These results shed light on the next-generation multimodal AI systems built upon the unified visual tokenization.", "tldr": "", "keywords": ["Tokenizer", "Omni model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e81ea825bb761ab0c4af84ff05642873373db269.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AToken, a unified visual tokenizer arcross images, videos and 3D asset. By incorporating 4D positional embedding, space-time patch embedding, and a sparse transformer encoder, it enables different types of visual inputs to share a unified representation space. From a technical perspective, the paper introduces a Gram loss to replace the traditional GAN loss for more stable training, along with a well-designed training recipe that successfully scales to various input types. Extensive experiments demonstrate that AToken achieves excellent performance in both reconstruction and understanding tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper attempts to address a fundamental and important issue — how to unify different types of visual signals — and proposes a relatively simple yet effective solution (4D positional embedding, space-time patch embedding, and an image-pretrained sparse transformer encoder).\n- This paper introduces a Gram loss that directly optimizes based on second-order statistics to replace the GAN loss, which is both convincing and technically solid.\n- The paper conducts extensive experiments and ablation studies, clearly demonstrating the impact of different training methods and stages on performance, and performs evaluations across various downstream tasks."}, "weaknesses": {"value": "- Some implementation details are missing or difficult to locate."}, "questions": {"value": "- Why is it necessary to use an additional distillation loss for images instead of using only the sigmoid loss, as done for videos and 3D assets? After all, distillation loss requires an extra frozen image encoder. How large is the performance gap between the two methods?\n- The implementation details of the CLIP perceptual loss are unclear.\n- The arrangement of the formulas is somewhat confusing. For example, Equation (4) and Equation (6) are identical."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x3sKUIm1xm", "forum": "a4fSF5pGJq", "replyto": "a4fSF5pGJq", "signatures": ["ICLR.cc/2026/Conference/Submission3202/Reviewer_Kup7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3202/Reviewer_Kup7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751859204, "cdate": 1761751859204, "tmdate": 1762916599586, "mdate": 1762916599586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "hTkTq5jH1h", "forum": "a4fSF5pGJq", "replyto": "a4fSF5pGJq", "signatures": ["ICLR.cc/2026/Conference/Submission3202/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3202/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762975580411, "cdate": 1762975580411, "tmdate": 1762975580411, "mdate": 1762975580411, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ATOKEN, a unified visual tokenizer that achieves both reconstruction and semantic understanding across images, videos, and 3D assets. ATOKEN employs a progressive training curriculum, gradually expanding its capability from single images to videos and then to 3D data. Experimental results demonstrate that ATOKEN achieves promising performance across these modalities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written.\n\n - ATOKEN is a unified tokenizer capable of encoding images, videos, and 3D assets, demonstrating great potential in bridging multimodal representations."}, "weaknesses": {"value": "- The generation results of the tokenizer are an important evaluation of its effectiveness. However, it seems that ATOKEN does not achieve state-of-the-art (SOTA) performance in generation. Moreover, ATOKEN contains 192M parameters, which is much larger than typical tokenizers. I believe this would make the training of generative models based on it more difficult, and I therefore have concerns about the tokenizer’s generative capability.\n\n - The experimental results in Table 1 show that the model achieves SOTA performance in understanding tasks, but its reconstruction ability still lags behind. I hope the authors can clarify this discrepancy.\n\n - The authors should also explain the changes in gFID after Stage 2 and Stage 3, as shown in the Table 4."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vzov1v04AH", "forum": "a4fSF5pGJq", "replyto": "a4fSF5pGJq", "signatures": ["ICLR.cc/2026/Conference/Submission3202/Reviewer_ZxHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3202/Reviewer_ZxHw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812525007, "cdate": 1761812525007, "tmdate": 1762916599296, "mdate": 1762916599296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* This paper propose ATOKEN, a unified visual tokenizer, enables high-fidelity reconstruction and semantic understanding across images, videos, and 3D via a 4D latent space, transformer architecture, adversarial-free training, and progressive curriculum, achieving strong performance on benchmarks\n\n* It addresses three long-standing limitations in existing visual representation systems: the fragmentation between reconstruction-focused and understanding-focused models, the restriction to single modalities, and the instability of transformer-based tokenizers during training.\n\n* ATOKEN adopts several key technical designs: Unified 4D Latent Space,  A pure transformer with space-time patch embedding and 4D Rotary Position Embeddings (4D RoPE) ,Adversarial-Free Training Objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "After reading this paper, I quite like it. I believe this paper makes considerable technical contributions and has certain innovations. Specifically:\n\n1. It is the first visual tokenizer that unifies 2D & 3D reconstruction & generation, and achieves excellent performance on both understanding and generation-related benchmarks, with substantial workload and abundant experiments .\n\n2. The 4D space design is quite reasonable, and a corresponding 4D RoPE is also designed. Using a transformer to achieve state-of-the-art (SOTA) performance—from what I know, the current reconstruction and generation effects of transformer-based tokenizers are inferior to those of CNN-based tokenizers .\n\n3. It directly optimizes the texture/style errors in reconstruction through Gram matrix loss, replacing traditional GAN training. While ensuring reconstruction quality, it solves the training instability problem of large-scale visual transformers, and I think this optimization is very promising .\n\n4. Many engineering designs are worthy of praise, such as supporting native resolution and conducting experimental verification for both discrete and continuous tokens."}, "weaknesses": {"value": "Of course, I also think this paper has several shortcomings:\n\n1. The reason why the transformer achieves state-of-the-art (SOTA) reconstruction performance for the image & video tokenizer is not clearly explained, nor is there a detailed ablation study to clarify the causes of this performance .\n\n2. Since ATOKEN uses FSQ with a 48-dimensional latent space—where the 48-dimensional latents are partitioned into 8 groups of 6 dimensions, each quantized to 4 levels—and adopts the TokenBridge scheme, does this mean an additional autoregressive (AR) head is required? Additionally, will the FSQ design make it more difficult for the visual tokenizer to align with the language codebook, thereby hindering the formation of a unified codebook?"}, "questions": {"value": "The description of the Gram matrix loss is not clear enough. It is hoped that the authors will elaborate on its implementation and provide a detailed ablation study"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bRFuXs3HNr", "forum": "a4fSF5pGJq", "replyto": "a4fSF5pGJq", "signatures": ["ICLR.cc/2026/Conference/Submission3202/Reviewer_why4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3202/Reviewer_why4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961022595, "cdate": 1761961022595, "tmdate": 1762916598774, "mdate": 1762916598774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AToken, a unified tokenizer that bridges visual generation and visual understanding. Experiments demonstrate that AToken achieves superior performance on ImageNet image reconstruction, ImageNet zero-shot image classification, multimodal understanding and generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. AToken is the first work to explore a unified tokenizer that simultaneously supports single images, videos, and 3D data, representing a major step toward universal visual tokenization. This generality significantly broadens the applicability of discrete tokenizers beyond 2D imagery and establishes a strong foundation for multimodal generative models.\n2. The model achieves competitive or superior reconstruction performance without relying on GAN-based objectives, instead combining perceptual and Gram-matrix losses to preserve fine-grained texture and global consistency. This makes AToken more stable and easier to train while maintaining visual fidelity comparable to adversarially trained tokenizers.\n3. The paper presents comprehensive evaluations across multiple modalities and datasets, including image/video/3D reconstruction, multimodal understanding and generation."}, "weaknesses": {"value": "1. Several important results are missing: 1) text-to-image generation results 2) multimodal LLM benchmarks using AToken-so/D\n2. It would be better to show the ablation results using or not Gram loss for visual reconstruction\n3. It would be better to do ablation studies on the depth of Sparse Encoder/Decoder, progressive training"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ob3tUC7aQz", "forum": "a4fSF5pGJq", "replyto": "a4fSF5pGJq", "signatures": ["ICLR.cc/2026/Conference/Submission3202/Reviewer_hgrt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3202/Reviewer_hgrt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965182774, "cdate": 1761965182774, "tmdate": 1762916598370, "mdate": 1762916598370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}