{"id": "AYH7uBK1Gg", "number": 16340, "cdate": 1758263365721, "mdate": 1763641594557, "content": {"title": "DRIFT: Divergent Response in Filtered Transformations for Robust Adversarial Defense", "abstract": "Deep neural networks remain highly vulnerable to adversarial examples, and most defenses collapse once gradients can be reliably estimated. We identify \\emph{gradient consensus}—the tendency of randomized transformations to yield aligned gradients—as a key driver of adversarial transferability. Attackers exploit this consensus to construct perturbations that remain effective across transformations. We introduce \\textbf{DRIFT} (Divergent Response in Filtered Transformations), a stochastic ensemble of lightweight, learnable filters trained to actively disrupt gradient consensus. Unlike prior randomized defenses that rely on gradient masking, DRIFT enforces \\emph{gradient dissonance} by maximizing divergence in Jacobian- and logit-space responses while preserving natural predictions. Our contributions are threefold: (i) we formalize gradient consensus and provide a theoretical analysis linking consensus to transferability; (ii) we propose a consensus-divergence training strategy combining prediction consistency, Jacobian separation, logit-space separation, and adversarial robustness; and (iii) we show that DRIFT achieves substantial robustness gains on ImageNet across CNNs and Vision Transformers, outperforming state-of-the-art preprocessing, adversarial training, and diffusion-based defenses under adaptive white-box, transfer-based, and gradient-free attacks. DRIFT delivers these improvements with negligible runtime and memory cost, establishing gradient divergence as a practical and generalizable principle for adversarial defense.", "tldr": "", "keywords": ["Adversarial Robustness", "Transferability of Adversarial Attacks", "Randomized Defenses", "Gradient Consensus"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db747486df6297feafa016ed0bf2f8decb51359b.pdf", "supplementary_material": "/attachment/119bdfb2ddd9ae8aaeb922bd7f1b7b6e3f362520.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DRIFT (Divergent Response in Filtered Transformations), a new adversarial defense that breaks the tendency of randomized defenses to share aligned gradients exploitable by adaptive attacks. DRIFT inserts a small ensemble of learnable, differentiable filters before a frozen classifier and trains them to maximize gradient divergence in Jacobian and logit space while maintaining clean accuracy. The authors theoretically link gradient consensus to adversarial transferability and design a joint objective combining prediction consistency, Jacobian separation, logit-VJP separation, and adversarial robustness losses. Experiments on ImageNet with CNNs and Vision Transformers show that DRIFT achieves higher robustness under PGD, AutoAttack, BPDA, and EOT, outperforming baselines like JPEG, BaRT, DiffPure, and adversarial training. Sanity checks confirm no gradient masking, and runtime analysis shows orders-of-magnitude efficiency gains. Overall, DRIFT presents a lightweight, theoretically motivated, and empirically strong defense principle based on gradient divergence rather than masking or purification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces gradient consensus as a measurable cause of adversarial transferability and proposes gradient divergence as a defense principle. This framing is intuitive, theoretically motivated, and distinct from existing randomization or purification methods.\n\n2.DRIFT’s learnable residual filters provide a lightweight, differentiable, and architecture-agnostic front-end, requiring no retraining of the backbone. The method is computationally efficient (0.4 ms per image), a clear advantage over diffusion-based defenses.\n\n3.The experiments cover both CNNs and Vision Transformers on ImageNet, testing against diverse adaptive attacks. Results demonstrate consistently higher robustness while maintaining clean accuracy.\n\n4.The inclusion of gradient-norm sanity checks, finite-difference validation, and loss-surface visualization shows the authors are aware of gradient masking pitfalls and have rigorously verified true robustness."}, "weaknesses": {"value": "1.The link between consensus and transferability (Theorem 3.5) is intuitive but lacks formal rigor; constants are unspecified and empirical validation of consensus metrics (ρ) is missing.\n\n2.Ablation results show improvements, but the contribution of each loss component (LJS vs LLVJP) is not deeply analyzed; no visualization or metric demonstrates the intended gradient decorrelation effect.\n\n3.Comparisons to adversarial training and diffusion-based methods appear under-tuned, and evaluation is limited to ImageNet only, lacking smaller, more reproducible datasets."}, "questions": {"value": "The paper claims robustness under BPDA and EOT, but the attack strength (number of EOT samples, surrogate choice) is not detailed. Please include robustness curves under stronger settings and alternative BPDA surrogates."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper claims robustness under BPDA and EOT, but the attack strength (number of EOT samples, surrogate choice) is not detailed. Please include robustness curves under stronger settings and alternative BPDA surrogates."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SvoaQuRkW0", "forum": "AYH7uBK1Gg", "replyto": "AYH7uBK1Gg", "signatures": ["ICLR.cc/2026/Conference/Submission16340/Reviewer_Fyh6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16340/Reviewer_Fyh6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877102007, "cdate": 1761877102007, "tmdate": 1762926474236, "mdate": 1762926474236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new stochastic adversarial defense method called DRIFT. It identifies gradient consensus, i.e., the alignment of gradients across stochastic transformations, as the root cause of adversarial transferability. DRIFT combats this by learning a small ensemble of lightweight, differentiable preprocessing filters trained to maximize gradient divergence while maintaining clean accuracy. Theoretical analysis links low gradient consensus to reduced transferability. The training loss combines cross-entropy, Jacobian separation, logit-VJP separation, and adversarial robustness terms. Experiments on ImageNet with CNNs and Vision Transformers show substantial robustness gains over adversarial training, diffusion-based purification, and randomized transformations, under both adaptive (BPDA, EOT) and non-adaptive attacks, with negligible computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper reframes adversarial robustness around gradient divergence rather than obfuscation or purification. This is a fresh and intellectually elegant perspective.\n2. This paper provides a simple but formal link between gradient alignment and attack transferability, grounded in the geometry of loss gradients.\n3. This method applies across CNNs and Vision Transformers without modifying the backbone.\n4. This paper is evaluated under strong adaptive attacks (BPDA + EOT, AutoAttack) and validated with sanity checks (finite-difference tests, loss landscape visualization).\n5. This paper achieves 10⁴× faster inference than diffusion-based defenses, with minimal memory overhead.\n6. Presentation and structure of this paper are clear."}, "weaknesses": {"value": "1. The analytical part (Lemma 3.4–Theorem 3.5) is intuitive but lacks full proofs or empirical validation of constants; thus, it serves more as motivation than as a formal guarantee.\n2. No certified guarantees (unlike randomized smoothing), so the results, though strong, remain empirical.\n3. Filters are trained on a subset of ImageNet validation data—unclear if robustness generalizes across distribution shifts or unseen domains.\n4. The method assumes a high-quality pretrained base; how it performs on weaker or adversarially trained backbones is unclear.\n5. While the method ensures gradient divergence, it does not analyze how filters achieve this (e.g., frequency-domain or spatial patterns).\n6. Effects of ensemble size and loss weighting are reported briefly in appendices but could be expanded in the main text."}, "questions": {"value": "1. How does robustness scale with the number of filters? Is there diminishing return or optimal ensemble size beyond n = 4?\n2. Could DRIFT generalize to non-image domains (e.g., NLP, tabular, or time-series data)?\n3. How sensitive are results to the loss weights (β_JS, β_LVJP, λ)?\n4. If the base model is already adversarially trained, does DRIFT still provide gains, or is its effect redundant?\n5. Can the authors show qualitative examples of learned filters to interpret what kind of transformations disrupt gradient consensus?\n6. Could DRIFT be combined with randomized smoothing to yield both empirical and certified robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "okpXSJlkjD", "forum": "AYH7uBK1Gg", "replyto": "AYH7uBK1Gg", "signatures": ["ICLR.cc/2026/Conference/Submission16340/Reviewer_w2HJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16340/Reviewer_w2HJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895059590, "cdate": 1761895059590, "tmdate": 1762926473746, "mdate": 1762926473746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a randomized adversarial defense called DRIFT. The authors argue that the key to adversarial examples transferring across models is that the gradients tend to be consistent under different random transformations, which is called \"gradient consensus\". To this end, DRIFT breaks this consensus by learning a set of small and lightweight, differentiable preprocessing filters. These filters are optimized at training time so that they maximize the gradient difference as much as possible while keeping the accuracy of clean samples from decreasing. This paper also theoretically analyzes the relationship between gradient consensus and adversarial migration, and points out that reducing gradient consensus helps to reduce the success rate of migration attacks. The training objective of the method combines multiple losses such as cross-entropy, Jacobian separation, logit-VJP separation and adversarial robustness. Experiments on ImageNet covering both CNN and Vision Transformer architectures show that DRIFT significantly improves robustness under a variety of attack Settings, including adaptive attacks such as BPDA and EOT, as well as non-adaptive attacks. Compared with adversarial training, diffusion purification and randomized transformation methods, it hardly increases additional computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper reframes adversarial robustness around gradient divergence rather than obfuscation or purification. This is a fresh and intellectually elegant perspective.\n2. This paper provides a simple but formal link between gradient alignment and attack transferability, grounded in the geometry of loss gradients.\n3. This method applies across CNNs and Vision Transformers without modifying the backbone.\n4. This paper is evaluated under strong adaptive attacks (BPDA + EOT, AutoAttack) and validated with sanity checks (finite-difference tests, loss landscape visualization).\n5. This paper achieves 10⁴× faster inference than diffusion-based defenses, with minimal memory overhead.\n6. Presentation and structure of this paper are clear."}, "weaknesses": {"value": "1. Although the analysis given in the paper (from Lemma 3.4 to Theorem 3.5) is intuitive and easy to understand, it lacks a complete proof process and empirical verification of the constant terms, so it is more like providing intuitive support for the method than a strict theoretical guarantee.\n\n2. Unlike methods such as random smoothing, DRIFT does not provide formal authentication robustness, so although the experimental results are impressive, the overall robustness improvement is an empirical one.\n\n3. The training of the filter relies on a portion of the data in the ImageNet validation set, and it is unclear whether this robustness can be maintained in distributed shifts or unfamiliar domains.\n\n4. By default, our approach is built on top of a high-quality pre-trained backbone, but it remains to be seen whether the performance and benefits will change if we switch to a weaker model or one that has itself been adversarial trained.\n\n5. While the goal of DRIFT is to facilitate gradient dispersion, the paper does not dive into how these filters achieve this, e.g. by changing frequency domain components or spatial structure.\n\n6. The authors only briefly discuss the effect of ensemble size and the weight of each loss term in the appendix, but it would be helpful to include more text and analysis to understand the behavior and design tradeoffs of the method."}, "questions": {"value": "1. How does robustness scale with the number of filters? Is there diminishing return or optimal ensemble size beyond n = 4?\n2. Could DRIFT generalize to non-image domains (e.g., NLP, tabular, or time-series data)?\n3. How sensitive are results to the loss weights (β_JS, β_LVJP, λ)?\n4. If the base model is already adversarially trained, does DRIFT still provide gains, or is its effect redundant?\n5. Can the authors show qualitative examples of learned filters to interpret what kind of transformations disrupt gradient consensus?\n6. Could DRIFT be combined with randomized smoothing to yield both empirical and certified robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "okpXSJlkjD", "forum": "AYH7uBK1Gg", "replyto": "AYH7uBK1Gg", "signatures": ["ICLR.cc/2026/Conference/Submission16340/Reviewer_w2HJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16340/Reviewer_w2HJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895059590, "cdate": 1761895059590, "tmdate": 1763658757030, "mdate": 1763658757030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a defense method based on learnable filters. To mitigate the issue of gradient consistency, which makes models vulnerable to transfer attacks, the method reduces the transferability of adversarial examples across different filters while enhancing filter diversity for defense. Additionally, lightweight residual convolutional blocks are introduced within each filter to ensure that they can learn meaningful transformations. Experiments conducted on multiple CNN and ViT architectures using the ImageNet benchmark demonstrate that the proposed learnable filter ensemble method achieves strong defensive performance compared with baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors propose using a Jacobian separation loss to reduce transferability across filters, along with a logit-space separation loss to enhance filter diversity.\n\n2. To improve filter performance and enable meaningful feature transformations, the authors design residual modules for each filter.\n\n3. Experimental results on CNN and ViT models demonstrate that the proposed learnable random filter ensemble defense outperforms traditional defense methods."}, "weaknesses": {"value": "1. Theoretical details are insufficient, as the derivations of the inequalities in Lemma 3.4 and Theorem 3.5 are not provided.\n\n2. The total loss comprises four components. However, it remains unclear how the contributions of each loss are balanced during filter training and how the optimal values of the associated hyperparameters are determined.\n\n3. The overall training procedure of the DRIFT filters is not clearly explained. For instance, Algorithm 1 includes a warm-up stage, separates the optimization of individual losses, and maximizes $L_{adv}$ over the index $i$. However, the paper lacks sufficient details regarding the training process and implementation.\n\n4. The experimental comparison is not comprehensive. The paper lacks evaluations against more recent defense methods and does not consider evaluations against the latest attack techniques. Furthermore, some mentioned methods (e.g., ANF, FFR) are not compared in Table 1, which limits the completeness of the comparative analysis.\n\n5. In Section 6.2, the choice of perturbation size (4/255) is not explained, and no experiments are provided to compare different perturbation magnitudes. \n\n6. The paper lacks ablation studies on the hyperparameters of the proposed loss function to analyze their impact on the experimental results.\n\n7. In Section 6.7, Table 6 presents efficiency comparisons only with the DiffPure method and lacks comparisons with other baseline methods."}, "questions": {"value": "1. Could the authors provide more details on how the inequalities presented in Lemma 3.4 and Theorem 3.5 are derived? \n\n2. Could the authors offer additional theoretical justification to explain why the proposed Jacobian loss and logit separation loss outperform other adversarial training methods?\n\n3. Could the authors provide more details on how each loss component contributes to the overall filter training process and how the optimal values for the associated hyperparameters are determined?\n\n4. The authors claim that the proposed method can function as a plug-and-play module. Could the authors provide comparative experiments demonstrating how this defense method integrates with and performs across different model architectures?\n\n5. Could the authors provide more details on how different perturbation sizes affect the performance of the proposed method on CNN and ViT models?\n\n6. Could the authors provide additional experimental results on runtime efficiency, including comparisons with more baseline methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SrNZfYeyai", "forum": "AYH7uBK1Gg", "replyto": "AYH7uBK1Gg", "signatures": ["ICLR.cc/2026/Conference/Submission16340/Reviewer_BXyw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16340/Reviewer_BXyw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992172902, "cdate": 1761992172902, "tmdate": 1762926473193, "mdate": 1762926473193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}