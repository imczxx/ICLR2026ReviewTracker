{"id": "NYphgYTloq", "number": 10492, "cdate": 1758173673668, "mdate": 1763534892176, "content": {"title": "Fair in Mind, Fair in Action? A Synchronous Benchmark for Understanding and Generation in UMLLMs", "abstract": "As artificial intelligence (AI) permeates society, ensuring fairness has become a foundational challenge. However, the field faces a “Babel Tower” dilemma: fairness metrics abound, yet their underlying philosophical assumptions often conflict, hindering unified paradigms—particularly in unified multimodal large language models (UMLLMs), where biases propagate systemically across tasks. To address this, we introduce the IRIS Benchmark, to our knowledge the first benchmark designed to synchronously evaluate the fairness of both the understanding and generation in UMLLMs. Enabled by our high-fidelity demographic classifier, ARES, and four supporting large-scale datasets, the benchmark is designed to normalize and aggregate arbitrary metrics into a high-dimensional “fairness space”, integrating 60 granular metrics across three dimensions—Ideal Fairness, Real-world Fidelity, and Bias Inertia & Steerability (IRIS). Through this benchmark, our evaluation of leading UMLLMs uncovers systemic phenomena such as the “generation gap”, individual inconsistencies like “personality splits”, and the “counter-stereotype reward”, while offering diagnostics to guide the optimization of their fairness capabilities. With its novel and extensible framework, the IRIS benchmark is capable of integrating ever-evolving fairness metrics, ultimately helping to resolve the “Babel Tower” impasse.", "tldr": "", "keywords": ["AI Fairness", "Unified Multimodal Large Language Models (UMLLMs)", "Fairness Benchmark", "Cross-Task Evaluation", "Bias Measurement"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ddab14e156fcffc2864ce1a1c347acbb8552ec5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a unified fairness benchmark for evaluating UMLLMs on both understanding and generation across three dimensions of ideal fairness, real-world fidelity, and bias inertia & steerability, while aggregating 60 metrics into a single, interpretable score. They also develop ARES, an adaptive-routing demographic classifier, and curate four purpose-built datasets to enable large-scale labeling and IRIS evaluation of UMLLMs. After through validation of the proposed framework, they present empirical analyses that reveal trade-offs and model-specific patterns of bias."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper provides:\n- A novel, uniform pipeline that evaluates both understanding and generation, organizing 60 metrics along three axes with transparent normalization. They also provide scalable demographic labeling (ARES) and four purpose-built datasets for large-scale measurement.\n\n- A comprehensive validation of proposed IRIS. Then they applies IRIS to a broad model set with interpretable outputs (persona profiles, sector breakdowns) that expose system-level trade-offs.\n\n- A set of novel findings, including: generation gap, clear cross-dimension correlations, separation of willingness vs ability under counter-stereotype prompts, localization of where bias emerges, and model-specific persona splits."}, "weaknesses": {"value": "- Limited geographic/generalization coverage: Fairness conclusions are calibrated to U.S./E.U. demographic statistics and taxonomies, so results may not transfer to other regions, cultures, or alternative demographic schemes.\n\n- Proxy-based quality/semantic scoring without human validation: Reported “quality” and “semantic consistency” rely only on automated proxies rather than task-specific human judgments, leaving validity uncertain.\n\n- Potential sparsity for fine-grained disparities: After stratifying by occupation, age, gender, and skin tone (and their intersections), some categories likely have small sample counts, making disparity estimates unstable or untrustworthy. \n\n- Generation fairness (IFS-Gen/RFS-Gen) is driven by a single template  (“Please generate a photo of a/an {occupation}”) across 52 jobs. Even small wording choices can shift demographic priors, yet the paper doesn’t probe prompt phrasing sensitivity. This risks injecting template bias into both the “ideal” and “real-world fidelity” results."}, "questions": {"value": "- How sensitive are the fairness results to the exact “neutral” prompt template? Did alternative phrasings change scores?\n\n- Since ARES both defines demographic labels and scores counter-stereotype success, did you try any alternative labelers to quantify labeler-induced bias on results?\n\n- In counterfactual pairs, what controls ensure only the target attribute changes?\n\n- It can by valuable to provide a finer-grained perfromance of ARES by different categories such as generator family and attributes?\n\n- In multilingual and alternative cultural contexts, how would prompts and occupation terms impact the IFS/RFS/BIS dimensions? Can IRIS be adapted to this context, and if so, what concrete changes (e.g., demographic taxonomies, target distributions, prompt templates) would be required?\n\n- When IFS, RFS, and BIS conflict, how should practitioners prioritize them? Can you offer policy guidance or concrete decision rules for trade-off selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jhl8EBgQNv", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Reviewer_RvCU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Reviewer_RvCU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514169100, "cdate": 1761514169100, "tmdate": 1762921780847, "mdate": 1762921780847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors create a multidimensional fairness benchmark based on Ideal Fairness, Real-world Fidelity, and Bias Inertia & Steerability (IRIS) with customized datasets and multiple sensitive attributes. As part of this, they develop Adaptive Routing Expert System (ARES), a classifier for demographics in images. They posit various phenomenon foregrounded by IRIS, including a \"generation gap\" and \"personality splits.\""}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors address an important problem (the diversity of fairness metrics and their practical applicability) and target a viable strategy (a multidimensional benchmark that integrates these metrics in practical contexts).\n2. The toolkit appears well-documented and transparent.\n3. The authors bring up some interesting concepts, such as personality profiles and the generation gap, that could enrich the fairness literature if developed fully."}, "weaknesses": {"value": "1. The authors are trying to do way too much in a single paper. The core contribution is meant to be a unification of fairness measures, but there is very little on this (e.g., why this set of measures is better than alternatives). The contributions, such as unifying different notions of fairness in a common framework, are not yet developed or defended enough to constitute a major contribution to the fairness literature.\n2. The paper is too verbose with many paragraphs and sentences that are just repeatedly stating what the paper contributes instead of actually showing how it advances our understanding. Lines 041-053 is an example, and honestly I think basically the first 4 pages could be greatly shortened (though I like Figure 1 and Table 1). This would free up room to develop scholarly contributions.\n3. I don't see why there is an algorithm (Lines 162-129) in this paper. It seems like this is just a way of calculating score across dimensions. An algorithm (and the algorithm box) is not the best way to describe a scoring procedure because there's nothing particularly algorithmic. Presenting it differently would be much clearer, such as by not having undefined variables and unclear notation.\n4. The core intellectual content of the paper is in the selection of items in the second and third columns of Table 1, but I'm not convinced of the authors' choices. This is the main part of the paper that I need to see developed before being more positive on the submission.\n5. The paper remains focused on particular sensitive attributes (e.g., age) and modalities without clear generalization. Even if my other points are addressed, I'm not sure if this is really a general fairness evaluation strategy rather than just one of potentially many combinations of metrics. This would be fine if the sensitive attributes were just examples of a broader machine learning theory, but that is currently lacking.\n6. I find the current ordering of the paper very unclear. I suggest not going by \"enabling technologies\" but instead chronological through the application of the tool. Start with the datasets and the metrics and how they relate to the fairness literature."}, "questions": {"value": "1. How were the items in the second and third columns of Table 1 determined? For example, why is statistical parity used, when it is widely considered an oversimplified fairness metric? I understand it is mathematically convenient (hence it being used so often), but is it sufficient for real-world application?\n2. Why are IFS and RFS separated in that way? What exactly do they mean? I encourage the authors to speak in terms of the fairness literature (e.g., \"statistical parity\" is a well-defined concept that is widely understood in the literature) rather than trying to coin new terminology. For example, \"fairness through awareness\" is very broad and plausibly includes everything else in the second column except \"fairness through unawareness,\" which is a largely outdated notion that doesn't actually seem to show up in the empirics of the paper.\n\nAlso see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5MpBIH9hE", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Reviewer_Z61h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Reviewer_Z61h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607514925, "cdate": 1761607514925, "tmdate": 1762921779414, "mdate": 1762921779414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IRIS, a benchmark for measuring fairness in unified multimodal large language models (UMLLMs) across both understanding and generation. IRIS evaluates their fairness across three dimensions: Ideal Fairness, Real-World Fidelity, and Bias Inertia & Steerability. The framework uses a custom demographic classifier (ARES) and four curated datasets to compute 60 fairness metrics projected into a “fairness space.” It also includes a qualitative profiling tool (IRIS-MBTI) that summarizes a model’s fairness personality.\nThrough this benchmark, the authors uncover systemic phenomena—like the generation gap (UMLLMs being less fair in generation), personality splits (task-dependent fairness behavior), and the counter-stereotype reward (improved output quality when breaking stereotypes)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "• Novel and meaningful contribution: A first benchmark to jointly assess fairness in UMLLMs. The unified fairness-space idea and the three-dimensional structure are elegant and address the “Babel Tower” problem of conflicting fairness metrics. The ARES classifier and supporting datasets make multi-dimensional fairness evaluation feasible in an automated fashion. \n\n• Clarity: The paper is easy to follow despite its scope. Figures clearly walk the reader through the pipeline, and the IRIS-MBTI profiles make results intuitive and interpretable. I appreciate the figures! \n\n• Interesting findings. The “generation gap” and “personality split” results are particularly interesting and likely to influence future multimodal fairness research."}, "weaknesses": {"value": "* Conceptual trade-off: Fairness is inherently multi-dimensional, and each metric captures a distinct philosophical or statistical notion. Collapsing these / projecting them into a high-dimensional “fairness space”  might blur important nuances and make interpretability harder. How the authors balance this abstraction with human-understandable fairness judgments would be worth discussing.\n\n* Metric sensitivity. The benchmark combines 60 metrics with tuned weights. It’s unclear how sensitive the global IRIS score is to metric choice or weighting; an ablation would help establish robustness."}, "questions": {"value": "1. How is the IRIS-MBTI profiling validated? Are these categories empirically meaningful or mainly heuristic?\n\n2. How robust are the IRIS scores to metric weighting or selection changes? \n\n3. How do the authors think about the potential loss of nuance or interpretability when collapsing many distinct fairness notions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BRxEn5gDUa", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Reviewer_LWHZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Reviewer_LWHZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792526130, "cdate": 1761792526130, "tmdate": 1762921778951, "mdate": 1762921778951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles the ever-present and frustrating problem in fairness that there are many, sometimes conflicting, fairness metrics that can be used; however, ML practitioners often pick only one metric to judge a model’s fairness.  The authors present a comprehensive fairness benchmark for unified multimodal large language models (UMLLMs) called IRIS.  They simultaneously analyse the fairness of the understanding and generative capabilities of a model by incorporating 11 fairness metrics into a novel embedding space across 3 dimensions to measure what they call Ideal Fairness, Real-world Fidelity, and Bias Inertia and Steerability.  They analyse a number of modern UMLLMs to demonstrate the utility of the benchmark and highlight a number of phenomena such as a gap in fairness across generation and understanding of models.  They conclude by showing how the benchmark can be used to understand where bias is generated within a model to allow for effective mitigation techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark is a valuable contribution to the community and demonstrates that you can combine numerous, sometimes conflicting, fairness metrics into a single benchmark for a holistic view of the fairness of UMLLMs.\n2. The benchmark analyses both understanding and generative capabilities of the models.\n3. The authors demonstrate effectively the utility of the benchmark for determining characteristic of models and phenomena that might be present.\n4. The authors demonstrate how you might use the benchmark to understand the source of bias within models and use that the mitigate biases going forward.\n5. The contribution of annotated datasets for the ARES classifier.\n6. The tests to determine the foundational integrity of the benchmark are a great addition to this paper."}, "weaknesses": {"value": "1. Too much of the paper is in the Appendix.   I don’t know how this can be fixed as the paper has a lot of content, but a huge proportion of the paper is in the Appendix, which was too long for me to evaluate completely.  The authors should consider ways in which you can bring some important details into the main paper concisely (like the models analysed and the personality profiles).  Many important assumptions are also only reported in the appendix (the simplification of gender, age, and skin tone attribute sub-groups for example).  Any important considerations, limitations, or assumptions should be expressed in the main body of the paper, even if they are expanded on in the Appendix.\n2. Biases in the ARES classifier will propagate into the metrics which is impossible to disentangle from the biases in the models being measured.  \n3. I like the idea of personality profiles for models, but it would be good to have names that are more easily consumable and intuitive than UAF, HDF, etc.  explained in the main paper in a table.\n4. This was mentioned in the limitations and is fairly significant “The scoring pipeline depends on calibrated hyper-parameters whose robustness across other model families and domains requires further validation”."}, "questions": {"value": "1. Line 069: Figure 1 link to models being tested is incorrect.  Should be A.1.1.\n2. How can you guarantee that biases from the ARES classifier don’t propagate into the benchmark measurements?  Have you measured its bias?  How might we reduce this risk going forward?  It has an overall accuracy of 88%.  Is this sufficient?  Where was it failing?  Do you propagate the uncertainty generated by an imperfect classifier into the benchmark results so that consumers know which results are reliable?\n3. Can you please state in the main paper what demographic markers the ARES classifier classifies (line 305)?\n4. Line 046 and 126: the literature on the bias transfer hypothesis [1] is somewhat divided on the conclusion that intrinsic biases “carry over” to downstream tasks.  Studies in the fine-tuning space are conflicted with some works saying that biases do transfer [1,2,3], while others claim they do not [4].  Recent work fixes some issues with previous studies, uses unified metrics to measure intrinsic and extrinsic biases, and demonstrates that they do conclusively transfer across prompting adaptation [5].  It’s a minor thing but the authors should update the text to reflect the state of the community on this.\n5. Line 086: In reference to a fairness assessment goal seeking a single optimal solution - isn’t it the case though that sometimes there is one fairness metric that represents the ideals that the ML practitioners are trying to adhere to?  So this isn’t always a negative and maybe as part of this benchmark you would also want to report the individual fairness metrics as well.\n6. How do you obtain 60 “granular” metrics from the 11 metrics in table 4?  Line 159 say they are derived from the core 11 metrics but I don’t see in App A.2.1 how they are expanded to 60 metrics.\n7. Line 174: how is m_i normalised to form u_i?\n8. Line 230: I don’t understand the Core Question & Metrics for Understanding in Real-world Fidelity.  Can you please rephrase this to make it clear what the metric is for this?  This sounds more like a research question to answer rather than a metric.\n9. Line 373: can you comment on the practical implications of the trade-off between Real-world Fidelity and Steerability?\n10. Line 375: How are Real-world Fidelity (gen) and Ideal Fairness (understanding) synergistic?  They are correlated which means their combined effect might not be greater than the sum of their parts.\n11. Line 383: Could you please expand on how you determined that the two traits HAF and UDF across generation and understanding means that “This finding suggests that a shared representation space does not guarantee consistent fairness characteristics across tasks”?\n12. Line 429: There is a typo in the heading “Gudied” -> “Guided”.\n13. Line 457: Interesting finding.  But I wonder if the internal representations are just a result of slightly OOD data and this is a form of “over-fitting” phenomena where large but opposing weights are used to model noisy data.\n14. What effect does the suite of metrics chosen have on the framework?  Have you done a sensitivity analysis to determine how sensitive the method is the choice of core fairness metrics?\n15. In the Ethics statement on line 492 you should also mentioned that the use of the ARES annotator can introduce bias as well as measurement noise.\n16. More discussion on why the three core dimensions of Ideal Fairness, Real-world Fidelity, and Bias Inertia and Steerability, were picked would have been appreciated.  \n\n\n[1] Ryan Steed, Swetasudha Panda, Ari Kobren, and Michael Wick. 2022. Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, Dublin, Ireland.\n[2] Sarah Schröder, Alexander Schulz, Philip Kenneweg, and Barbara Hammer. 2023. So can we use intrinsic bias measures or not? In Proceedings of the 12th International Conference on Pattern Recognition Applications and Methods.\n[3] Masahiro Kaneko, Danushka Bollegala, and Naoaki Okazaki. 2022. Debiasing Isn’t Enough! – on the Effectiveness of Debiasing MLMs and Their Social Biases in Downstream Tasks. In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea.\n[4] Xisen Jin, Francesco Barbieri, Brendan Kennedy, Aida Mostafazadeh Davani, Leonardo Neves, and Xiang Ren. 2021. On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.\n[5] Nivedha Sivakumar, Natalie Mackraz, Samira Khorshidi, Krishna Patel, Barry-John Theobald, Luca Zappella, and Nicholas Apostoloff, Bias after Prompting: Persistent Discrimination in Large Language Modelsm, EMNLP, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TdoUkjAvN5", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Reviewer_ci6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Reviewer_ci6e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904067285, "cdate": 1761904067285, "tmdate": 1762921778373, "mdate": 1762921778373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (Part 5 of 5)"}, "comment": {"value": "## **3. Benchmark Robustness and Sensitivity Analysis**\n\nReviewers raised important concerns about the benchmark's robustness to the selection of **hyperparameters** (specifically scaling factors $S$ and $K$) and **granular metrics** ($m_i$).\n\n### **1) Hyperparameter Sensitivity**\n\n**(Responding to Reviewer LWHZ and Reviewer ci6e)**\n\nOur benchmark's rankings are highly robust because our core methodology deliberately avoids data-driven hyperparameters.\n\n- **Normalization:** We explicitly avoided model-sensitive min-max or z-score normalization. Instead, we use normalization by **theoretical maximums** or **log-transforms** (see Appx. A.2.2). This ensures our benchmark is \"open\" and \"additive,\" allowing new models and metrics to be added without changing the baseline.\n- **Scaling (S and K):** The *only* hyperparameters used (S and K) are for the final step: scaling the Euclidean distance (the objective fairness measure) for **readability** into an approximately 0-100 range, which is easily understood by humans. These parameters **do not, under any circumstances, alter the relative ranking of models.**\n\nWhile rankings are robust, we acknowledge (A.3.4) that the \"perceptible range\" of scores may shift as models improve, and we commit to updating these scaling guidelines to maintain readability.\n\n### **2) Metric Sensitivity**\n\n**(Responding to Reviewer LWHZ and Reviewer ci6e)**\n\nOur original paper (Fig. 4(b)) already conducted the Metric Sensitivity experiment by: 1) adjusted the weights of the sub-metric sets for the three dimensions (IFS, RFS, and BIS) by 10% when calculating the final total score; and 2) adjusted the weights of select sub-metrics within each of the three dimensions by 10%. Parts of the results, as shown in Figure 4(b), demonstrated that after these adjustments, the Spearman’s $\\rho$ of the new model rankings against the original was > 0.96.\n\nTo address this concern more thoroughly, we conducted a new, stricter, fine-grained **Leave-One-Out (LOO) sensitivity analysis**. We iteratively removed *each* of the 60 granular sub-metrics, recalculated the total IRIS score, and re-ranked the models.\n\nThe results were definitive: the Spearman rank correlation remained exceptionally high, with **$\\rho > 0.9286$ in all 60 cases**. Even the lowest correlation (0.9286, from removing Penalty_$\\Delta$GSR) confirms the final rank is not dependent on any single metric. The full LOO analysis is now in the appendix.\n\nThese experiments powerfully demonstrate that our aggregation method is robust and the final rankings represent a holistic consensus, not a dependency on any single metric.\n\n### **3) Manuscript Revisions**\n\n- **Added New Appendix Section (A.2.2):** We added a new section detailing this rigorous Leave-One-Out metric sensitivity analysis and its results. \n\n  > (Lines 1026-1109)\n\n- **Updated Main Text (§4.1):** We updated the text in §4.1 to reference this new, more comprehensive sensitivity analysis. \n\n  > (Lines 356, 357)"}}, "id": "80tOnwYo9B", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763533897878, "cdate": 1763533897878, "tmdate": 1763535262011, "mdate": 1763535262011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (Part 4 of 5)"}, "comment": {"value": "## **2. Quantifying ARES Classifier Error and Uncertainty**\n\nWe are grateful for the critical concerns regarding the ARES classifier. We agree that a biased or inaccurate ruler would invalidate the benchmark, and this component demands the strictest scrutiny.\n\nIn response, we conducted a fine-grained performance decomposition of ARES and an analysis of its internal biases.\n\n### **1) On \"Is 88% accuracy sufficient?\": Fine-grained Performance Decomposition**\n\n**(Responding to Reviewer ci6e)**\n\nThe reviewer's concern about the 88% overall accuracy is reasonable.\n\nFirst, we emphasize that 88% is, in itself, a strong result. Classifying *generated* occupational portraits poses unique challenges not found in standard real-world datasets (e.g., FairFace, UTK-Face), as the classifier must handle numerous interferences (see **new Fig. 9**), such as:\n\n- Common generative artifacts (e.g., distorted, occluded, or incomplete facial features).\n- Artistic processing (e.g., unique art styles, strong confounding lighting).\n\nAs detailed in Appendix C.4, we explicitly trained ARES to handle these interferences. The 88% accuracy was measured on our most challenging *Random-200* dataset, which is replete with such artifacts. Given these challenges, we believe this accuracy demonstrates the robustness of our system compared to standard classifiers which often fail on generated content.\n\nFurthermore, to provide a definitive breakdown, we conducted new experiments on a **new, larger, human-annotated, and balanced V-720 dataset** (N=720, with 40 samples for each of the 18 intersectional attribute combinations).\n\nOn this new validation set, ARES demonstrates high accuracy,  consistent with our existing ablation studies :\n\n- **Gender:** 98.9% accuracy (98.5% on Random-200, 99.7% on Selected-300)\n- **Age:** 93.2% accuracy (93.0% on Random-200, 97.7% on Selected-300)\n- **Skin Tone:** 90.4% accuracy (91.5% on Random-200, 93.1% on Selected-300)\n\n(Full P/R/F1-scores and confusion matrices are in **new Appendix B.4**).\n\n### **2) On \"ARES's own bias\": Classifier Fairness Analysis**\n\n**(Responding to Reviewer ci6e and Reviewer RvCU)**\n\nA good 'ruler' must also be fair. To investigate if ARES injects bias, we used the balanced V-720 dataset to calculate the classifier's own **Accuracy Disparity (AD)** (Max Recall - Min Recall). This measures whether predictive power is biased toward any group.\n\nThe results show ARES's internal AD is extremely low:\n\n- **Gender (0.56%):** Negligible, confirming high fairness.\n- **Age (4.17%) & Skin Tone (5.83%):** Very low.\n\nOur analysis reveals this minor disparity is driven by the **\"middle\" categories** (middle-age, middle-skintone). This is not systemic bias, but an expected statistical property:\n\n- \"Extreme\" categories (e.g., *young*, *dark*) have *unique* features, forming separable clusters.\n- \"Middle\" categories are an *ambiguous spectrum* with *feature overlap* (e.g., some middle-aged samples look young, some look old), placing them *near decision boundaries* and making them inherently harder to classify.\n\nIn summary, this is an expected, non-systemic error that is uniformly distributed across all models, and does not affect the relative ranking between models.\n\n### **3) Manuscript Revisions**\n\nBased on this, we have made the following revisions:\n\n- **Added New Appendix Section (B.4):** We added a new appendix section dedicated to this deep validation of ARES. It contains the full, fine-grained P/R/F1 reports, confusion matrices, the classifier's own fairness (AD) analysis, and a discussion of these limitations with case studies. \n\n  > (Lines 1566-1711)\n\n- **Clarified Main Text (§4.1):** We clarified in §4.1 that the 88% accuracy is the \"Overall\" score on \"challenging data\" and added a pointer to the comprehensive analysis in the new Appendix B.4. \n\n  > (Lines 345, 346)"}}, "id": "vSfqjkLI5u", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763535303788, "cdate": 1763535303788, "tmdate": 1763566319644, "mdate": 1763566319644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (Part 3 of 5)"}, "comment": {"value": "### **3) On Framework Limitations and Extensible Design**\n\n**(Responding to Reviewer LWHZ and Reviewer Z61h)**\n\nFinally, we address **concerns (R-LWHZ, R-Z61h)** about framework completeness. We agree that AI fairness is a vast, evolving field, and our \"default-cognition-execution\" chain does not capture all perspectives.\n\nHowever, IRIS was designed from the ground up with **extensibility** as a core principle to meet this challenge. We ensure this through:\n\n- **Open High-Dimensional Space:** Our core methodology (§ 3.1, Alg 1) projects metrics into an open space. New, computable metrics can be seamlessly incorporated as new dimensions without invalidating the framework.\n- **Robust Normalization Strategy:** We deliberately avoid data-driven *min-max* or *z-score* normalization. We use normalization by theoretical maximums or log transforms (§ A.2.2), ensuring score stability and **additivity** for new metrics.\n- **Modular ARES Toolchain:** ARES is a modular adaptive expert system. We currently use age, gender, and skin tone as illustrative, widely-used attributes, consistent with broad practice, but new attributes (e.g., \"emotion\") can be added by simply training a new expert and adding it to the ARES routing network.\n- **Swappable Data:** For RFS, we use BLS and Eurostat data as examples. Researchers can substitute this with any real-world distribution matching their target region or context.\n\nIn summary, the current limitations of IRIS were the very motivation for its open and extensible design in the future.\n\n### **4) Manuscript Revisions**\n\nBased on this discussion, we have made the following revisions:\n\n- Expanded **§3.2** to detail the \"Should-be (IFS) $\\rightarrow$ As-is (RFS) $\\rightarrow$ Can-be (BIS)\" evaluation chain  (i.e., \"default $\\rightarrow$ cognition $\\rightarrow$ execution\"). \n\n  > (Lines 157-161, 211-213).\n\n- Revised **Table 1** to clarify the \"Core Question\" for RFS-Understanding, making the objective clearer in response to **R-ci6e**.\n\n  > (Lines 196-199).\n\n- Moved essential content (evaluated models, attribute setup, MBTI archetypes) from the appendix into the main text (**§3.3**) to improve readability.\n\n  > (Lines 244-264, 270-282)."}}, "id": "QF13llBMth", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763535334328, "cdate": 1763535334328, "tmdate": 1763535334328, "mdate": 1763535334328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (Part 2 of 5)"}, "comment": {"value": "### **2) The Design Philosophy of IRIS: A Three-Dimensional Evaluation Chain**\n\n**(Responding to Reviewer ci6e and Reviewer Z61h)**\n\nReviewers critically questioned why we chose these three core dimensions (IFS, RFS, BIS). We defend this choice explicitly in the revised manuscript (**see Table 1 and §3.2**) and summarize the rationale here.\n\nBuilding on our trade-off philosophy, we selected dimensions that effectively construct this tension-filled fairness space. We argue that fairness evaluation for UMLLMs should be a **complete logical chain**: from its \"default instincts\" to its \"real-world cognition,\" and finally to its \"controllability.\" The three distinct, non-overlapping axes of IRIS cover this entire chain:\n\n#### Ideal Fairness (IFS): Measuring the Model's \"Should-be\"\n\n- **Core Question:** In the absence of specific context, what are the model's intrinsic, unconditional \"Default Values\"? How far do its priors deviate from a Utopian, perfectly egalitarian (\"should-be\") world?\n- **Metric Choice:** This is why we select metrics like Representation Disparity (RD) and Statistical Parity Difference (SPD). **(To R-Z61h):** While SPD may be \"oversimplified\" as a *normative* goal, we argue it is the *exact* tool for this *diagnostic* purpose: to precisely measure deviation from an \"ideal uniform distribution\".\n- **Practical Implication:** IFS indicates the model's \"factory safety settings\". High IFS implies a lower risk of \"out-of-the-box\" harm. **Examples:** 1. Public APIs (where prompts are uncontrolled). 2. Creative/Educational Contexts (e.g., children's books) that desire an idealized world.\n\n#### Real-world Fidelity (RFS): Measuring the Model's \"As-is\"\n\n- **Core Question:** After assessing the \"ideal,\" we must assess its \"reality\". Does the model accurately comprehend the world \"as-is\"?\n- **Metric Choice:** This dimension evaluates if model outputs faithfully reflect observable, real-world distributions. It embodies \"descriptive awareness\" (echoing \"Fairness through Awareness\" and \"descriptive fairness\"), assessing the model's knowledge foundation: *does the model know the real-world ratios?*\n- **Practical Implication:** RFS indicates \"cognitive accuracy\". **Examples:** 1. Societal Simulation: Generating synthetic data that accurately reflect demographic distributions for social science research. 2. Decision Support: Applications requiring priors grounded in statistical reality rather than idealized equality.\n\n#### Bias Inertia & Steerability (BIS): Measuring the Model's \"Can-be\"\n\n- **Core Question:** Given the model's \"defaults\" (IFS) and \"cognition\" (RFS), how much *Inertia* does it exhibit? How steerable is it toward a desired, counter-stereotypical (\"can-be\") state?\n- **Metric Choice:** This final \"action\" step draws from Counterfactual Fairness. We measure *if* the model can be steered (e.g., $\\Delta$GSR) and the *cost* of doing so (e.g., performance *Penalty* via QPS, Hallucination Rate).\n- **Practical Implication:** This is the model's \"teachability\" or \"alignment cost.\" A high BIS (low inertia) model may be highly valuable even if its IFS/RFS is poor. **Examples:** 1. AI Alignment: Assessing the difficulty of debiasing a base model during fine-tuning (high inertia = hard to align). 2. High-Fidelity Control: Applications requiring the generation of specific, counter-stereotypical samples (e.g., \"an older dark-skin male nurse\") while maintaining high image quality."}}, "id": "wwCo84L0U0", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763535397548, "cdate": 1763535397548, "tmdate": 1763566234434, "mdate": 1763566234434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (Part 1 of 5)"}, "comment": {"value": "# **General Response to All Reviewers**\n\nWe are encouraged that **Reviewer ci6e** and **Reviewer LWHZ** rated our work highly (8), recognizing the IRIS framework as a \"valuable contribution\" and an \"elegant\" design. We are equally grateful for the constructive feedback from **Reviewer Z61h** and **Reviewer RvCU**. While they raised valid concerns, we appreciate that they also highlighted the importance of our problem setting, the novelty of our unified pipeline, and the value of our empirical findings (e.g., the \"generation gap\"). We have addressed the core concerns from all reviewers with new experiments and revisions below.\n\n\n\n## **1. Core Contribution and Philosophy (IFS/RFS/BIS) :**\n\n### **1) Clarifying the Core Contribution: A \"Dashboard,\" not a \"Single Metric\"**\n\n**(Responding to Reviewer Z61h)**\n\nWe must first clarify our response to **R-Z61h**'s critique that the \"core contribution... [is not yet] developed or defended enough\".\n\nOur core contribution is **not** to propose a new, single \"unification of fairness measures\". This is often mathematically infeasible (e.g., the inability to simultaneously satisfy demographic parity, equalized odds, and predictive rate parity, except in trivial cases).\n\nInstead, our core contribution is to provide the first **unified fairness benchmark framework** for various architectures of UMLLMs. The value of IRIS lies precisely in this:\n\n- **Philosophical Innovation: A \"Dashboard,\" not a \"Single Metric.\"** Our work does not attempt to \"unify\" (i.e., collapse) the multitude of philosophically and mathematically conflicting fairness metrics. Instead, IRIS serves as a **\"dashboard\"**. Beyond the single overall IRIS-score, we also provide the **six-dimensional score vector** (IFS-Und, RFS-Gen, etc.), as well as the Personality diagnostic. Its value lies in simultaneously displaying these conflicting values (e.g., IFS vs. RFS) to enable **Trade-off Analysis**. Our solution to the \"Babel Tower\" dilemma is not to find the lost \"universal language\" but to provide a high-quality \"simultaneous interpretation system\" to manage these conflicts.\n- **Methodological Instantiation: IFS, RFS, and BIS as the Axes of Trade-off.** Our proposed dimensions—Ideal Fairness, Real-world Fidelity, and Bias Inertia & Steerability—are the concrete instantiation of this trade-off analysis.\n- **Disciplinary Necessity: Filling the UMLLM Evaluation Gap.** IRIS fills a critical gap in the emerging UMLLM paradigm. Unlike existing unified benchmarks that mainly focus on capabilities (e.g., reasoning), IRIS targets the equally critical domain of **fairness**. Crucially, applying this multi-dimensional analysis has allowed us to uncover systemic issues in leading UMLLMs—such as the **\"Generation Gap\"** and **\"Personality Splits\"**—which single-metric evaluations may fail to capture."}}, "id": "9pxpRTS68Z", "forum": "NYphgYTloq", "replyto": "NYphgYTloq", "signatures": ["ICLR.cc/2026/Conference/Submission10492/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10492/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission10492/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763535431216, "cdate": 1763535431216, "tmdate": 1763535431216, "mdate": 1763535431216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}