{"id": "qQ5MZ5Mx7p", "number": 15726, "cdate": 1758254409885, "mdate": 1763658524602, "content": {"title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction", "abstract": "Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.", "tldr": "", "keywords": ["Long-Horizon Agents", "Deep Research"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2883dee8de535e4f2ee0805b82dda32a6e76e639.pdf", "supplementary_material": "/attachment/da28fa1e52f645359c78455e5f242a7ac064e170.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes IterResearch, a new method for deep research. Current approaches keep expanding the context in the think->act phases which can limit the space available for a response. IterResearch is inspired from MDPs and compresses the current context by summarizing it into a report. Thus, the new context consists of the original question, the report, and any actions to take.\n\nThe authors also introduce EAPO - Efficiency Aware Policy Optimization by using discounted rewards for binary, sparse deep research settings where the reward is only given on the final step.\n\nFinally, IterResearch can also be used as a prompting paradigm directly so that no training etc is needed and can work with current models.\n\nThe authors then conduct an empirical evaluation with different baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clear and well-written\n\n2. The overall idea is intuitive\n\n3. Results demonstrate significant improvement (although there are some open questions here)"}, "weaknesses": {"value": "Overall, the results clearly seem to advance the SOTA so Im hoping for more clarifications to my identified weaknesses. Im happy to engage in discussion and revise my score if my concerns are adequately addressed.\n\n1. I think one of the major claims of the paper is the Markovian State Reconstruction. There is not really any guarantee that this report is Markovian in any sense. The Markovian State suggests that the future state is independent of the history given the current state. \n\nIf that were the case, then definitely the transition function with the input history or with the report is not going to change the outcome. So the paper title feels a bit bold. It is more apt to call it summarization but calling it Markovian State Reconstruction is a bit too much. I think experiments would be needed to back your claims.\n\nThe idea is intuitive to improve the performance due to limitations on attention windows/distributions but to claim that summarization of the past context and making it a report is making it Markovian seems a bit overblown IMO.\n\n2. I tihnk one problem with the results is that the prompts are not provided for the baselines in the prompt-based approach. The prompt used in appendix F.2 is quite exhaustive and Im not sure if some instructions that are agnostic to the approach were used in the baselines.  For example, the entire section of ## Working Principles. This might be an issue with clarity in the paper.\n\n3. I think the claim of unbounded interactions is overstated. Even summarization has limits. Are the summaries never going to grow? How does the LLM know what information is important. There could be some tool response that is only going to be needed several steps later. This is often the case in partial-order planning. One issue here is that once that information is not included in the summary, it is lost forever. At least in the mono-context case it is possible to be \"re-\"discovered by a later step based on the generated output and attended to. I am a bit confused on how the LLM knows to generate a good report so that such issues do not occur.\n\n3. I dont think EAPO is really novel. RL algorithms use discounting by definition and this does not seem like a new contribution. Please justify why you think EAPO is novel?"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Qkvn6CH6t", "forum": "qQ5MZ5Mx7p", "replyto": "qQ5MZ5Mx7p", "signatures": ["ICLR.cc/2026/Conference/Submission15726/Reviewer_XKsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15726/Reviewer_XKsG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809989310, "cdate": 1761809989310, "tmdate": 1762925968707, "mdate": 1762925968707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IterResearch, a long-horizon reasoning framework where an agent maintains a Markovian workspace instead of accumulating the full dialogue history. The evolving report acts as compressed memory, keeping context size nominally constant. Training uses Efficiency-Aware Policy Optimization (EAPO) with geometrically discounted rewards to promote concise, successful trajectories.\n\nIterResearch achieves notable gains (+14.5 pp on six benchmarks) and reportedly scales from 32 to 2048 reasoning steps, outperforming prior open-source systems and even rivaling proprietary ones. However, the \"constant-context\" claim is partly theoretical - since the report must still grow or overwrite information, true unbounded reasoning is unproven. The paper also lacks experiments confirming recovery from early false summaries or testing smaller training horizons. Comparisons rely on older baselines (GPT-4.1, o4-mini), and it remains unclear whether gains stem from the paradigm itself or its prompting format.\n\nOverall, IterResearch is an elegant compression-based reformulation of long-horizon reasoning, but its claimed scalability and memory efficiency need stronger empirical validation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, principled formulation. The Markovian workspace reconstruction is simple and well-motivated, giving constant context size and avoiding \"context suffocation\"/\"noise contamination\". The formal transition s_{t+1}=T(s_t,d_t,E(a_t)) is explicit.  \n\n2. Strong empirical coverage. Six benchmarks with diverse characteristics; competitive vs. proprietary systems on several, e.g., surpassing OpenAI DeepResearch on HLE and BrowseComp-zh. \n\n3. General usefulness as a prompting recipe. IterResearch prompting outperforms ReAct with frontier models (o3, DeepSeek-V3.1), suggesting paradigm-level value beyond one implementation."}, "weaknesses": {"value": "1. Markovian constant context is a conceptual simplification rather than a true breakthrough.\nThe claimed $O(1)$ workspace complexity is only formal when the evolving report $|M|$ is treated as fixed. In practice, $|M|$ grows with the number of synthesized summaries, and each tool response $|TR|$ can vary widely across steps. Therefore, the overall process still depends on in memory and computational complexity, merely folded into a different structure. This makes the \"constant\" context claim more of a heuristic compression scheme than a provably unbounded process. Once the report becomes saturated, the system risks discarding fundamental early-stage information, effectively re-introducing the limitations it sought to remove.\n\n2. Potential inefficiency from report initialization.\nIf the evolving report $|M|$ is kept constant, the first few interactions still include a large workspace prompt even when the agent has accumulated little information. This may yield unnecessary compute overhead at the start of every episode, contradicting the claimed \"efficiency-aware\" principle.\n\n3. Questionable extrapolation to long horizons.\nThe claim of scaling from $T_{max}=32$ (training) to $2048$ interactions (inference)rests on the assumption that the agent can reconstruct forgotten information through new searches. Yet, no evidence is given that this process retrieves previously lost or overwritten insights, as opposed to repeatedly circling around similar queries or rediscovering the same partial results. Without qualitative trajectory inspection, it is unclear whether high-turn runs reflect genuine continued reasoning or redundant loops.\n\n4. No measurements for smaller training horizons.\nWhile Section 4.4 evaluates scaling at inference, there are no experiments showing performance under smaller $T_{max}$ during training. Thus, it is impossible to assess whether longer training horizons produce better generalization or if the model’s extrapolation is accidental.\n\n5. Outdated or inconsistent baseline comparisons.\nThe benchmark comparison includes GPT-4.1 and o4-mini but omits GPT-5-series or contemporary baselines. This limits the credibility of claims about competitiveness \"with frontier proprietary systems\".\n\n6. Lack of ablation on report-based prompting alone.\nThe paper does not include a variant where the same prompt structure is given to a baseline model without the workspace-reconstruction mechanism (i.e., the same input text but no internal state machine). It remains unclear how much of the improvement arises from the report-prompt structure rather than the Markovian update itself.\n\n7. Novelty of EAPO may feel incremental. Geometric discounting for terminal rewards is standard MDP practice; while appropriate here, the RL contribution beyond applying discounting + GSPO + downsampling reads as engineering rather than fundamentally new learning theory. Ablations isolate length reduction but not a deeper analysis of policy changes."}, "questions": {"value": "1. What happens when $T_{max}$ during training is reduced to, say, $8$ or $16$? Does extrapolation still hold, or does generalization degrade proportionally?\n\n2. How can we confirm that high-turn (>100) trajectories generate genuinely new insights rather than re-searching similar content? Have the authors analyzed overlap of retrieved URLs or paraphrased content between early and late rounds?\n\n3. How do accuracy and average step count vary with $\\gamma$ (e.g., $0.99$/$0.997$/$0.999$)? Is the $5.7%$ length reduction robust, and where does performance begin to trade off?\n\n4. If the evolving report contains an early false hypothesis, can the agent recover? Have the authors measured recovery rate with stress tests where earlier synthesized content is wrong/misleading?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YYbSsK0Ev8", "forum": "qQ5MZ5Mx7p", "replyto": "qQ5MZ5Mx7p", "signatures": ["ICLR.cc/2026/Conference/Submission15726/Reviewer_wSFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15726/Reviewer_wSFE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994700232, "cdate": 1761994700232, "tmdate": 1762925968346, "mdate": 1762925968346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IterResearch, an iterative deep-research paradigm that overcomes the limitations of mono-contextual approaches in long-horizon reasoning. It maintains an evolving report as memory and periodically synthesizes insights. The system was trained on an aggregated QA dataset, using both supervised fine-tuning and a reinforcement learning phase. Experiments demonstrate substantial improvements over existing open-source agents and notable gains when used as a prompting strategy for frontier models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is well-motivated: mono-contextual approaches accumulate information in a single context window, causing noise and limiting effectiveness on long-horizon tasks.\n- The presented approach significantly outperforms current state-of-the-art models in solution quality on selected benchmarks."}, "weaknesses": {"value": "W1: The novelty of the approach is somewhat limited, as many of its core ideas, such as using discounted rewards and iterative refinement of trajectories, are already widely used in classical reinforcement learning.\n\nW2: The MDP formulation appears incomplete. It omits a reward function (e.g., a verification signal), and the transition function is likely not deterministic, since tool outputs can vary over time (e.g., search results). The decision space is unconventional: including Think and Report as part of the action reflects internal computation rather than true environmental actions. Additionally, the “reconstructed workspace” manages context by discarding some history, but this alters the state definition across timesteps and may violate the Markov property.\n\nW3: While I understand the intuition behind why discounted reward shaping works, the analogy with RL and the discount factor appears somewhat misleading. First, if we treat this reward as in a typical RL environment, it becomes biased: two similar trajectory prefixes may receive different rewards depending on the trajectory length (the length of the suffixes). Consequently, this mechanism not only amplifies the signal at the end of successful trajectories but also implicitly encourages shorter trajectories. I think this modification is more akin to how value is computed in RL, where there exist well-established alternatives, such as GAE. \n\nW4: The approach was trained exclusively with Qwen-3-30B-A3B, which may limit its generalization to other models. Experiments with models used without fine-tuning only partially address this concern.\n\nW5: Provide a stronger comparison with AlphaEvolve [1] (and similar systems), as the three components used, main objective question (task), evolving report (previous high-level ideas), and immediate context (code), closely mirror its design (in the case of AlphaEvolve, for coding tasks). The use of a reward function applied at the end of generation similarly provides the necessary learning signal for optimization. Although Alpha-Evolve itself is closed-source, several open-source implementations exist (e.g., OpenEvolve). I recommend that the authors at least position their work in relation to these approaches, and ideally include a direct empirical comparison.\n\nW6: The paper assumes that the LLM can reliably summarize, filter noise, and preserve crucial information in each round. In practice, this compression step could lead to the loss of essential context, especially under noisy tool outputs, and may worsen compounding summarization errors.\n\n[1] Novikov A, Vũ N, Eisenberger M, Dupont E, Huang PS, Wagner AZ, Shirobokov S, Kozlovskii B, Ruiz FJ, Mehrabian A, Kumar MP. AlphaEvolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131. 2025 Jun 16.\n\n**Minor suggestions**:\n- I recommend that the authors provide confidence intervals in the table, and include the token budget used for each model.\n- It is recommended to include in the description of Table 1 an explanation of why proprietary models were not tested on some benchmarks, to make the table self-contained."}, "questions": {"value": "Q1: Could the authors clarify how the baselines were adopted for the studied benchmarks? Were the open-source agents also fine-tuned on the same data as IterResearch?\n\nQ2: Which datasets were used during the RL phase? Were there tasks in the training set similar to those on which the approach was evaluated, or was it still only the QA Collection? How does the test set intersect with the QA Collection?\n\nQ3: What is the token budget and inference cost for the baseline LLMs? Which budget is needed to reproduce the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cBQcawjAdm", "forum": "qQ5MZ5Mx7p", "replyto": "qQ5MZ5Mx7p", "signatures": ["ICLR.cc/2026/Conference/Submission15726/Reviewer_reAn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15726/Reviewer_reAn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998274712, "cdate": 1761998274712, "tmdate": 1762925967938, "mdate": 1762925967938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response 3"}, "comment": {"value": "#### **Why This Satisfies the Markov Property**\n\n1. **Policy Markovness**:  \n   The policy $\\pi(a_t \\mid s_t)$ depends exclusively on the current explicit workspace $s_t$, not on prior states or actions — consistent with the definition of a Markov policy.\n\n2. **Transition Markovness**:  \n   Although $TR_t$ is stochastic, the next state $s_{t+1}$ is fully determined by the current state $s_t$ (which generates the decision $d_t$) and the environment response $TR_t$. Since $s_t$ encapsulates all relevant history through $\\mathcal{M}_{t}$ and $[a_{t-1}, TR_{t-1}]$, and $\\mathcal{M}_t$ is a sufficient statistic of the history, we have:  \n   $$\n   P(s_{t+1} \\mid s_t, d_t) = P(s_{t+1} \\mid h_t, d_t)\n   $$\n   Thus, the transition probability is memoryless: future states depend only on the present state and decision (internal thought and action), not on the full trajectory.\n\n3. **$\\mathcal{M}_t$ as a Sufficient Statistic**:  \n   The evolving reasoning summary $\\mathcal{M}_t$ functions analogously to the hidden state of an RNN or the belief state in a POMDP — but with the critical advantage of being *explicit, interpretable, and learnable*.  \n   While compression may risk information loss, reinforcement learning ensures that only *task-relevant* information is retained: if $\\mathcal{M}_t$ omits critical context, the agent receives zero reward and learns to avoid such pruning.  \n   Therefore, $\\mathcal{M}_t$ is not a heuristic summary — it is an *end-to-end learned sufficient statistic*, optimized by RL to maximize long-term reward.\n\n---\n\n#### **Addressing Reviewer Concerns**\n\n> *“What if the summary misses key information?”*  \n> We acknowledge this possibility — but treat it not as a flaw, but as an *optimization objective*.  \n> The MDP formulation does not assume perfect compression; rather, it enables the agent to *learn the optimal trade-off between compression and sufficiency*.  \n> RL implicitly discovers the minimal representation $\\mathcal{M}_t$ required for success, discarding noise and redundancy that would otherwise cause “context suffocation” in monolithic LLM inference.  \n> This is not ad hoc summarization — it is *task-driven information distillation*, learned end-to-end under reward pressure.\n\n> *“Is this truly Markovian?”*  \n> Yes — in precisely the same sense that an RNN-based policy is treated as Markovian: the hidden state $h_t$ encodes the sufficient history, and transitions depend only on $h_t$ and $a_t$.  \n> Our model makes this latent structure *explicit and verifiable*: $\\mathcal{M}_t$ is not a black-box representation, but a human-interpretable, dynamically updated report.  \n> The Markov property holds *by construction*, because the state $s_t$ contains all and only the information needed to determine the next state and reward.\n\n---\n\n#### **Conclusion**\n\nOur framework is a *practical, well-defined MDP* in which state compression is not a limitation — it is the *core mechanism enabling scalable, sequential reasoning*.  \nThe Markov property is formally satisfied through the design of $s_t$ as a sufficient statistic, and the reward structure ensures that the agent learns to maintain only task-relevant information.  \nThis formulation not only justifies our approach theoretically, but also provides a principled basis for analyzing reasoning length, information retention, and policy convergence.\n\n---\n\nWe thank the reviewers for their insightful critique — their feedback has been instrumental in sharpening our formal grounding and clarifying the theoretical foundations of our method."}}, "id": "Hip4Em3am3", "forum": "qQ5MZ5Mx7p", "replyto": "qQ5MZ5Mx7p", "signatures": ["ICLR.cc/2026/Conference/Submission15726/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15726/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15726/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656832211, "cdate": 1763656832211, "tmdate": 1763657788439, "mdate": 1763657788439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response 2"}, "comment": {"value": "We thank the reviewers for their rigorous examination of our formalization. We appreciate the opportunity to clarify how IterResearch reformulates long-horizon reasoning. Instead of a traditional history-dependent view, we model it as an MDP with **a structured decision space that explicitly separates internal cognitive updates from actions**.\nTo address concerns regarding the Markov property, we provide a step-by-step breakdown of our MDP state transitions and decision process.\n\n---\n\n#### **Formal MDP Definition**\n\nTo address Reviewer reAn's concern regarding the \"unconventional\" decision space and \"incomplete\" definitions, we rigorously specify the MDP tuple $(\\mathcal{S}, \\\\mathcal{D}, \\\\mathcal{T}, R)$:\n\n- **State** $s_t = \\{q, \\\\mathcal{M}_{t}, [a_{t-1}, TR_{t-1}]\\}$  \n\n  represents the agent’s explicit workspace at decision step $t$, composed of:  \n  - the original query $q$ (constant across steps),  \n  - the current compressed reasoning summary $\\mathcal{M}_{t}$, encoding the history of prior reasoning up to step $t-1$,  \n  - the last executed action-response pair $[a_{t-1}, TR_{t-1}]$, where $a_{t-1} \\in \\mathcal{A}$ is the prior action and $TR_{t-1}$ is its environmental response.  \n  - This state design enables the Markov property by construction: the policy $\\pi(d_t|s_t)$ depends solely on this workspace, blocking access to the discarded raw history.\n\n  The summary $\\mathcal{M}_t$ is updated from $s_t$ which serves as the information compression.\n\n- **Structured Decision Space $d_t \\in \\mathcal{D}$ (Internal Thought & External Action)**: \n  We explicitly formulate the decision $d_t$ as a composite output generated by the policy $\\pi$. Crucially, this formulation enforces the Markov property: the entire decision depends only on the current state $s_t$. It consists of two distinct phases:\n\n$$\\underbrace{d_t}_{\\text{Decision}} = [\\underbrace{\\text{Think}_t, \\mathcal{M}_{t+1}}_{\\text{Internal Thought}}, \\underbrace{a_t}_{\\text{External Action}}] \\sim \\pi(\\cdot \\mid s_t)$$\n\n  * **Internal Thought**: The agent performs reasoning ($\\text{Think}_t$) and actively constructs the memory for the next state ($\\mathcal{M}_{t+1}$). We agree with Reviewer reAn that this represents internal computation rather than environmental interaction. However, crucial to our framework, **this computation is an explicit output of the LLM policy**. By formalizing it as part of the decision space, we treat \"thinking\" and \"memorizing\" as **learnable cognitive actions** that can be optimized via RL, **just like external actions**.\n\n  * **External Action** $a_t \\in \\mathcal{A}$:\n  This represents the **actual interaction with the environment** (e.g., search queries, code execution) or the final answer. Actions are generated by a policy that depends *only* on the current state $s_t$. Unlike the internal thought which deterministically updates the agent's state, the external action triggers the stochastic result via the environment's response ($TR_t$). \n\n\n- **Transition** $\\mathcal{T}(s_{t+1} \\mid s_t, d_t, TR_t)$  \n  where $d_t=[\\text{Think}_t, \\mathcal{M}_{t+1}, a_t]$. The state transition logic is **deterministic given the environment's output**. Specifically, while the tool response $TR_t$ is drawn from the stochastic environment (which may be noisy due to stochastic web search) $TR_t \\sim \\mathcal{E}(\\cdot \\mid a_t)$, given the current state $s_t$, decision $d_t$ (incorporating internal thought and external action), and response $TR_t$, the next state is deterministically constructed as:\n\n  $$\n  s_{t+1} = \\{q, \\mathcal{M}_{t+1}, [a_t, TR_t]\\}\n  $$\n\n  Crucially, the stochasticity resides solely in the environment rather than the state construction logic. Thus, the full transition kernel is:  \n  \n$$\n  \\mathcal{T}(s_{t+1} \\mid s_t, d_t) = \\mathbb{E}_{TR_t \\sim \\mathcal{E}(\\cdot \\mid a_t)} \\left[ \\mathbb{I}\\left(s_{t+1} = \\{q, \\mathcal{M}_{t+1}, [a_t, TR_t]\\}\\right) \\right]\n  $$\n\n  This formulation ensures that the transition depends *only* on the current state, the policy's decision, and the environment — strictly satisfying the Markov property.\n\n- **Reward** $R(s_t, a_t)$  \n  is defined as a sparse, terminal reward:  \n\n  $$\n  R(s_t, a_t) = \\begin{cases}\n  1 & \\text{if } a_t \\text{ is a terminal action and yields a correct final answer}, \\\\\n  0 & \\text{otherwise}.\n  \\end{cases}\n  $$\n\n  Termination is triggered by a designated terminal action $a_{\\text{term}} \\in \\mathcal{A}$, which signals the agent’s intent to submit a final answer. No intermediate rewards are used."}}, "id": "azsys2vGyK", "forum": "qQ5MZ5Mx7p", "replyto": "qQ5MZ5Mx7p", "signatures": ["ICLR.cc/2026/Conference/Submission15726/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15726/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15726/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763657311210, "cdate": 1763657311210, "tmdate": 1763659228311, "mdate": 1763659228311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response 1"}, "comment": {"value": "We sincerely thank all reviewers (reAn, wSFE, XKsG) for their valuable feedback and insightful comments on the theoretical novelty of our methods. We greatly appreciate this opportunity to clarify a core misunderstanding and frame our contribution more precisely.\n\nThe fundamental limitation of existing agent frameworks is not their reasoning ability, but their inability to scale interactions beyond a few dozen steps due to structural context saturation. Our work addresses this critical bottleneck: **Interaction Scaling (the core objective of our work)**. Existing \"mono-context\" paradigms (e.g., ReAct) are structurally limited. They inevitably suffer from \"context suffocation\" and \"noise contamination,\" which prevents them from scaling to long-horizon tasks (e.g., typically failing beyond ~100 interactions).\n\nTo solve this, our contribution is twofold: (1) a new agent paradigm (IterResearch) designed to solve interaction scaling, and (2) a specific engineering optimization (EAPO) designed to make this new paradigm trainable.\n\n**1. The Core Innovation: IterResearch, a New Paradigm for Interaction Scaling**\n\nOur core innovation is the IterResearch paradigm itself. Its value is demonstrated by three key pieces of evidence:\n\n* **Evidence 1: Effectiveness Across Trained & Training-Free**\nWe have demonstrated that our IterResearch is highly effective across different training settings. *As a Trained agent*, it achieves state-of-the-art results on our smaller 30B-A3B model (+14.5% across 6 challenging benchmarks on Table 1). *As a Training-Free strategy*, its core logic is general enough to be applied to large frontier models (Fig. 4), where it significantly outperforms ReAct (+19.22% on BrowseComp).\n\n* **Evidence 2: Unprecedented Interaction Scaling (Fig. 3)**\nTo our knowledge, we are the **first** to successfully scale an agent to 2048 interactions within only 40k context. Performance dramatically improves with this scale (3.5% $\\rightarrow$ 42.5%), proving our paradigm can effectively utilize the long horizon that mono-context methods cannot even reach. *Interaction Scaling: the ability of an agent to maintain effective reasoning performance as the number of sequential interactions (e.g., think-action-observe cycles) increases beyond 100+ steps.*\n\n* **Evidence 3: The Critical Ablation (Table 2)**\nThis is our *most direct proof*. In a strict, controlled ablation with *identical data and tools*, IterResearch outperforms the mono-context baseline by +12.6% on average. This proves the performance gain comes from the fundamental design of our paradigm.\n\n**2. The Enabling Optimization: EAPO, an Engineering Solution for Trainability**\n\nWe must clarify that EAPO (reAn-W3, wSFE-W7, XKsG-W4) is not proposed as a new RL theory. It is *a key engineering optimization designed to adapt standard RL (GSPO) to solve the unique challenges introduced by our new IterResearch paradigm.*\n\n* Challenge 1 (Efficiency Preference): In our paradigm, each round is an independent sample. **Standard RL might lose \"interaction-level awareness,\"** treating a step from a 5-turn trajectory and a 50-turn trajectory equally, ignoring that a correct 5-step solution is far more valuable than a 50-step one riddled with noise. We introduce geometric discounting (the \"interaction-level decay\") to make the model \"turn-aware\" and explicitly prefer shorter, correct trajectories, which is essential for efficiency in long-horizon tasks.\n\n* Challenge 2 (Training Stability): This \"per-round\" sample structure creates a highly variable and unpredictable number of samples per batch, which **breaks standard data-parallel distributed training**. EAPO's adaptive downsampling is *the engineering solution* that stabilizes the batch size, preventing training crashes while retaining the vast majority of data.\n\nIn summary, IterResearch is a new paradigm that solves the interaction scaling problem in deep research, and EAPO is the necessary engineering that makes this new paradigm trainable and efficient. Together, IterResearch and EAPO establish a new design principle for long-horizon reasoning agents: decouple reasoning structure from training mechanics, enabling scalability without sacrificing efficiency."}}, "id": "J7drqqQzxS", "forum": "qQ5MZ5Mx7p", "replyto": "qQ5MZ5Mx7p", "signatures": ["ICLR.cc/2026/Conference/Submission15726/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15726/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission15726/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763657341304, "cdate": 1763657341304, "tmdate": 1763657341304, "mdate": 1763657341304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their diligent work and constructive feedback.\n\nWe identified a few key misunderstandings in the initial reviews. Because our goal is to address these points thoroughly and to answer every valuable question raised, our rebuttal is necessarily detailed. We have provided specific, multi-part responses for each reviewer to ensure all concerns are met.\n\nGiven the detail, we respectfully ask for your patience in reading the full set of responses provided for their review, as they contain critical clarifications and experimental context. We **deeply appreciate the additional time and diligent effort this requires** and look forward to a constructive exchange.\n\nFurthermore, in direct response to your feedback, we **have submitted a revised manuscript and supplementary material**. All modifications are marked in **red text** to help you quickly locate the changes.\n\nWe believe these clarifications and revisions fully address the initial concerns. We thank you again for your time and guidance, and we respectfully hope you will re-evaluate our work in light of this new information."}}, "id": "UFqC4f4qLa", "forum": "qQ5MZ5Mx7p", "replyto": "qQ5MZ5Mx7p", "signatures": ["ICLR.cc/2026/Conference/Submission15726/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15726/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission15726/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763657413193, "cdate": 1763657413193, "tmdate": 1763657413193, "mdate": 1763657413193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}