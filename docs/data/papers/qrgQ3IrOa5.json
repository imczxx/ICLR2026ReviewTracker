{"id": "qrgQ3IrOa5", "number": 8276, "cdate": 1758077134643, "mdate": 1759897794804, "content": {"title": "Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding", "abstract": "Large Vision-Language Models (LVLMs) have recently achieved impressive results in multimodal tasks such as image captioning and visual question answering. However, they remain prone to **object hallucination**—generating descriptions of nonexistent or misidentified objects. Prior work has partially mitigated this via auxiliary training objectives or external modules, but challenges remain in terms of scalability, adaptability, and model independence. To address these limitations, we propose **A**daptive **T**oken **E**nsemble **D**ecoding (**ATED**), a training-free, token-level ensemble framework that mitigates hallucination by aggregating predictions from multiple LVLMs during inference. ATED dynamically computes uncertainty-based weights for each model, reflecting their reliability at each decoding step. It also integrates diverse decoding paths to improve contextual grounding and semantic consistency. Experiments on standard hallucination detection benchmarks demonstrate that ATED significantly outperforms state-of-the-art methods, reducing hallucination without compromising fluency or relevance. Our findings highlight the benefits of adaptive ensembling and point to a promising direction for improving LVLM robustness in high-stakes applications.", "tldr": "", "keywords": ["ensemble decoding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1dd8e865b4976fb55ee04afd172333ca6d67a65.pdf", "supplementary_material": "/attachment/d414df710225179d7805ea521e8d8612be9ad52f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Adaptive Token Ensemble Decoding (ATED), a training-free framework for mitigating object hallucination in Large Vision-Language Models (LVLMs).\nATED aggregates token-level logits from multiple LVLMs via an uncertainty-guided weighting mechanism that dynamically adjusts each model’s contribution during decoding.\nA greedy uncertainty-minimization algorithm (UGO) further refines ensemble weights to balance reliability and computational cost.\nAcross POPE, CHAIR, and MME benchmarks, ATED consistently outperforms strong baselines, reducing hallucinations without harming fluency.\nAblation studies confirm that adaptive weighting and visual perturbations are key to robustness, while latency experiments show favorable trade-offs between inference speed and caption quality.\nOverall, ATED demonstrates a flexible and effective approach to improving LVLM robustness in multimodal reasoning and captioning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and persistent weakness of LVLMs (object hallucination) with a conceptually simple yet broadly applicable ensemble framework.\n- ATED can be integrated with various off-the-shelf LVLMs (InstructBLIP, MiniGPT-4, LLaVA-1.5/Next) without retraining, showing practical scalability.\n- The experiments span multiple benchmarks (POPE, CHAIR, MME) and include both quantitative and qualitative analyses, providing convincing empirical support."}, "weaknesses": {"value": "- The proposed ATED appears conceptually close to prior methods: ensemble decoding (ED) and visual-contrastive decoding (VCD).\nThe only substantial novelty lies in the Uncertainty-Greedy Optimization (UGO), whose ablation (Table 3) suggests only marginal contribution, making the methodological innovation relatively weak.\n\n- The paper lacks statistical significance analysis or variance reporting, which limits confidence in the reported percentage improvements.\n\n- While ATED claims to be “training-free,” the computation overhead of multi-model forward passes is non-trivial; the paper does not provide detailed latency or cost comparisons under identical hardware constraints."}, "questions": {"value": "- How many LVLMs were actually ensembled in each experiment, and how does performance scale with the number of models (N = 2 → 3 → 4)?\n\n- How sensitive is ATED to the hyperparameters?\n\n- Can the uncertainty-guided weighting be estimated within a single model using internal heads or adapters, rather than across multiple LVLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4VhJkmHJgO", "forum": "qrgQ3IrOa5", "replyto": "qrgQ3IrOa5", "signatures": ["ICLR.cc/2026/Conference/Submission8276/Reviewer_oHTM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8276/Reviewer_oHTM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658339072, "cdate": 1761658339072, "tmdate": 1762920210987, "mdate": 1762920210987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Token Ensemble Decoding (ATED), a training-free, token-level ensemble method to curb object hallucination in LVLMs. ATED aggregates per-step predictions from multiple LVLMs using uncertainty-adaptive weights and fuses diverse decoding paths to strengthen grounding and semantic consistency—without extra training or model-specific modules. On standard hallucination benchmarks, ATED lowers hallucination rates while preserving fluency and task relevance, outperforming prior objectives and post-hoc detectors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is interesting and tries to address an important problem.\n\n2. The paper writing is easy to follow."}, "weaknesses": {"value": "1. Beyond entropy, can uncertainty be measured with alternative metrics?\n\n2. I’m unclear on the exact decoding procedure. After computing model-specific uncertainty weights, which model (or aggregation) actually drives decoding? Does this operate token-by-token only, or can it decode full sentences? If full sentences, must outputs from all models be forced to match exactly?\n\n3. Please provide efficiency measurements for the entire procedure.\n\n4. I would like deeper analysis—for example, showing whether higher-performing models systematically receive larger weights.\n\n5. Can the proposed method be applied to the Qwen-2.5-VL family?\n\n6. For the benchmark discussion, note that several recent studies [1, 2, 3] address both hallucination and maintain performance (even some improvement) on general scenario. I recommend the authors add some benchmarks like OCRBench, MMMU, MME etc.\n\n[1] Mitigating Object Hallucinations via Sentence-Level Early Intervention.\n\n[2] A topic-level self-correctional approach to mitigate hallucinations in mllms.\n\n[3] Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m4AWOQirnn", "forum": "qrgQ3IrOa5", "replyto": "qrgQ3IrOa5", "signatures": ["ICLR.cc/2026/Conference/Submission8276/Reviewer_5jsQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8276/Reviewer_5jsQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756738044, "cdate": 1761756738044, "tmdate": 1762920210677, "mdate": 1762920210677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Token Ensemble Decoding (ATED), a training‑free framework that mitigates hallucinations in large vision‑language models by fusing next‑token logits from multiple LVLMs with uncertainty‑guided weights. ATED generates perturbed image variants, applies contrastive decoding, and greedily minimizes entropy to assign importance weights to each model, allowing a trade‑off between inference latency and accuracy. Experiments on POPE, CHAIR and MME demonstrate consistent gains over individual backbones and several plug‑and‑play decoding baselines, with ablations analysing weighting strategies and latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Training-free, plug-and-play method that leverages existing LVLMs without retraining; works across several backbones. \n(2) Consistent empirical gains on POPE / CHAIR / MME over strong decoding baselines (VCD, ICD, SID). \n(3) Ablations + latency knob make the method well-diagnosed and practically tunable."}, "weaknesses": {"value": "(1) The paper compares a multi-model ensemble to single-model baselines; real-world feasibility of running 2–3 LVLMs + perturbations per token is unclear. \n\n(2) The paper also lacks comparison to 2025 ED / FastED / iTaD / IFCD-style plug-and-play hallucination mitigators, weakening the “significantly outperforms SOTA” claim."}, "questions": {"value": "(1) Can you provide a fair multi-model baseline (e.g., 3 LVLMs with uniform logit averaging, same perturbations) to isolate the gain from your uncertainty-greedy weighting?\n\n(2) Can you add or report results vs 2025 ED/FastED-type methods on at least one of POPE/CHAIR to strengthen the SOTA claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9FEmpeP1m7", "forum": "qrgQ3IrOa5", "replyto": "qrgQ3IrOa5", "signatures": ["ICLR.cc/2026/Conference/Submission8276/Reviewer_akFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8276/Reviewer_akFE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990244217, "cdate": 1761990244217, "tmdate": 1762920210293, "mdate": 1762920210293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Token Ensemble Decoding (ATED), a training-free ensemble framework designed to mitigate multimodal hallucinations in large vision-language models (LVLMs). ATED dynamically fuses token-level logits from multiple LVLMs based on uncertainty-guided weighting, allowing it to leverage the complementary strengths of each model during inference. The method is evaluated on several benchmarks including POPE, CHAIR, and MME, and achieves significant improvements in hallucination reduction without requiring retraining or fine-tuning. The authors further discuss the trade-off between inference latency and accuracy and analyze various ensemble strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a clear and well-motivated idea: ensemble decoding at the token level across multiple LVLMs guided by adaptive uncertainty. This fine-grained approach extends ensemble learning into multimodal generation, which is both innovative and practically relevant.\n2. ATED does not require additional training, making it broadly applicable across existing LVLMs and compatible with open-source backbones like LLaVA, InstructBLIP, and MiniGPT-4.\n3. The paper includes comparisons on multiple benchmarks (POPE, CHAIR, MME) and provides ablation studies showing the contribution of each component, such as uncertainty-guided weighting and greedy optimization."}, "weaknesses": {"value": "1. The proposed ATED framework requires simultaneous inference across multiple LVLMs, which substantially increases GPU memory usage and deployment cost. Figure 4 also shows that inference latency can increase up to six times compared to standard decoding. In contrast, other training-free approaches such as VCD typically introduce at most a twofold increase in latency. This raises concerns about ATED’s scalability and practicality in real-world applications where efficiency is critical.\n\n2. The experiments mainly compare ATED with training-free decoding strategies (e.g., VCD, ICD, SID, OPERA), but do not include training-based hallucination mitigation methods, such as instruction-tuning or preference optimization. Including such comparisons would better demonstrate ATED’s effectiveness and contributions.\n\n3. While the paper reports improvements on hallucination-related metrics such as POPE, CHAIR, and MME (hallucination subset), it lacks qualitative and quantitative evaluation of overall generation quality (e.g., fluency, coherence, descriptive richness). For instance, benchmarks like RefoMB, LLaVA-Bench, MMStar, and MM-Vet could assess how ATED affects long-form captioning and reasoning under complex visual conditions. Without these results, it remains unclear whether ATED preserves or degrades naturalness in extended outputs.\n\n4. The paper focuses mainly on hallucination benchmarks but does not discuss whether ATED affects general-purpose multimodal understanding. Evaluations on textVQA, DocVQA, InfoVQA, and VQAv2 would help determine whether the ensemble decoding alters the model’s broader reasoning or comprehension abilities. It is important to verify that the hallucination reduction does not come at the cost of decreased general accuracy or robustness on standard multimodal benchmarks."}, "questions": {"value": "1. Include an analysis of deployment efficiency, especially GPU memory consumption and throughput, to clarify ATED’s practical applicability.\n2. Add comparisons with training-based hallucination mitigation methods, such as fine-tuned models using preference alignment or reinforcement learning.\n3. Extend the evaluation to long-form generation benchmarks and provide both evaluations of fluency and relevance.\n4. Evaluate ATED’s impact on general-purpose performance using standard multimodal benchmarks (e.g., textVQA, DocVQA, VQAv2)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1XwUINcmOn", "forum": "qrgQ3IrOa5", "replyto": "qrgQ3IrOa5", "signatures": ["ICLR.cc/2026/Conference/Submission8276/Reviewer_caNz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8276/Reviewer_caNz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990639613, "cdate": 1761990639613, "tmdate": 1762920209869, "mdate": 1762920209869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}