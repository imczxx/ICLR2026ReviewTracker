{"id": "zmZsWCGzUV", "number": 12727, "cdate": 1758209816690, "mdate": 1759897491180, "content": {"title": "Translate Policy to Language: Flow Matching Generated Rewards for LLM Explanations", "abstract": "As humans increasingly share environments with diverse agents powered by RL, LLMs, and beyond, the ability to explain agent policies in natural language is vital for reliable coexistence. We introduce a general-purpose framework that trains explanation-generating LLMs via reinforcement learning from AI feedback, with distributional rewards generated by generative continuous normalizing flows (CNFs). CNFs capture the pluralistic and probabilistic nature of human judgments about explanations. Moreover, under mild assumptions, CNFs provably bound deviations from true human reward distributions when trained on noisy proxy rewards from LLMs. We design a specialized CNF architecture that selectively attends to linguistic cues in decision context and explanations when generating rewards. Human and LLM evaluators find that our method delivers explanations that enable more accurate predictions of true agent decisions, exhibit greater logical soundness and actionability, and impose lower cognitive load than explanations trained with proxy LLM rewards or state-of-the-art RLHF and RLAIF baselines.", "tldr": "", "keywords": ["LLM", "Continuous Normalizing Flow", "Diffusion Model", "RLAIF", "Explainable AI"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/538894b36e19b8ced75b1ac8d11ac50f4f18d860.pdf", "supplementary_material": "/attachment/809db63ff9632d17fda6e631d76158ad4b76d672.zip"}, "replies": [{"content": {"summary": {"value": "LLMs have been used to explain other AI agents’ behaviors, but the authors argue that these ‘Explanation LLMs’ can be biased by AI agents’ decisions. To address this issue, they propose using continuous normalizing flows (CNF) models to approximate/generate reward distributions and use them to train Explanation LLMs. As explanation LLMs are trained with rewards predicted by CNF models, training is independent from AI agents’ decisions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "In this study, Proxy LLMs are used to estimate the likelihood of humans inferring AI agent’s actions from Explanation LLMs’ outputs (i.e., explanations). Proxy LLMs accept contexts and explanations as input prompts and predict AI agent’s actions. To incorporate language cues into the flow model, the authors build a new rectified flow model by using Proxy LLMs’ modules. After training, this rectified flow model can model human evaluation scores. Such proposals are novel, and their empirical evaluation suggests that CNF allows Explanation LLMs to generate high quality explanations on agents’ decisions on complex tasks. \n\nGiven the novelty and strong empirical evidence, I believe this study would bring about strong interest from the readers."}, "weaknesses": {"value": "I would like to point out that the main text was a bit difficult to follow at first, as some of essential information (e.g., Figure 2) were found in Appendix. I recommend including the information (relevant details of the rectified flow model and Figure 2) in the main text.  \n\nIn lines 062-064, the authors state: “Intuitively, the PROXY LLM acts as a forward process that injects noise into human rewards, and the CNF learns its inverse, partially denoising and recovering the underlying human rewards.” This sentence would make sense only in the context of diffusion process, and therefore I recommend providing more context (a figure or diagram) for the readers."}, "questions": {"value": "In Line 203, they say, “The context c_{j} and explanations y^{e}_{j} are encoded by an LLM.”  What does LLM refer to? It looks like they are referring to Explanation LLM, but this needs to be clarified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HNg7SYK2NM", "forum": "zmZsWCGzUV", "replyto": "zmZsWCGzUV", "signatures": ["ICLR.cc/2026/Conference/Submission12727/Reviewer_e3dx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12727/Reviewer_e3dx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879199835, "cdate": 1761879199835, "tmdate": 1762923549527, "mdate": 1762923549527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that translates reinforcement learning policies into natural language descriptions and then uses these textual representations as intermediate supervision for imitation learning. By bridging policy representations with LLM reasoning, the approach aims to improve interpretability, sample efficiency, and generalization. Experiments on MiniGrid, BabyAI, and Crafter show consistent improvements over imitation and language-conditioned baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) Conceptually interesting: The idea of using language as an intermediate representation is intuitive and well-motivated, offering a new perspective for explainable and interpretable RL.\n\n2) Clear methodology: The framework is well structured, with each component (policy translator, language learner, and imitation stage) clearly explained.\n\n3) Empirical consistency: Results are stable across multiple benchmarks, with useful ablation studies supporting the core claims.\n\n4) Readable and coherent writing: The paper is well organized and easy to follow."}, "weaknesses": {"value": "1) Limited experimental scope: All tasks are symbolic or low-dimensional toy environments. It is unclear whether the method scales to more complex RL domains (e.g., continuous control, high-dimensional observations). This substantially limits the claim of generality.\n\n2) Dependence on pretrained LLMs: The policy-to-language translation relies on large models like GPT-4; the paper does not analyze whether smaller models could achieve similar results.\n\n3) Lack of deeper analysis: The paper does not provide theoretical or qualitative explanations for why language intermediates improve policy learning beyond empirical gains. It could be an add if provided."}, "questions": {"value": "1) How sensitive is the approach to translation quality—does noisy or partial text reduce imitation performance?\n\n2) Could the same idea extend to multi-step reasoning or continuous control tasks?\n\n3) Have the authors considered analyzing the semantic consistency between translated language and policy behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DJdeBro1Eb", "forum": "zmZsWCGzUV", "replyto": "zmZsWCGzUV", "signatures": ["ICLR.cc/2026/Conference/Submission12727/Reviewer_T4VY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12727/Reviewer_T4VY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984310945, "cdate": 1761984310945, "tmdate": 1762923549249, "mdate": 1762923549249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a general-purpose framework for training an LLM-based explainer using reinforcement learning. The core idea is to map random noise to a complex reward distribution by training a continuous normalizing flow (CNF) model to approximate reward samples generated by a separate proxy LLM. The authors further support their method with both theoretical guarantees and empirical validation on real-world datasets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strength:\n1 The topic is interesting, and to my knowledge, this is the first work that applies diffusion model to fit the reward for LLM explanations.\n2 The experiments are abundant.\n3 The paper provides a thorough analysis of the proposed algorithm, including an error bound for recovering the true human reward distribution (Theorem 1), which reinforces the soundness of the approach."}, "weaknesses": {"value": "Weakness:\n1 In Section 3.1, the authors approximate the likelihood of producing multiple actions by averaging token logits. This approximation may introduce significant estimation error, especially when a single abnormal token contributes disproportionately.\n2 The framework does not address the issue of long-term reward accumulation, which is a key consideration in RL-based settings."}, "questions": {"value": "Suggestions and Questions:\n1 I suggest that the authors analyze the robustness of the proposed method under noisy or imperfect reward signals.\n2 Since the framework is claimed to be general-purpose, could the authors demonstrate its applicability in a standard RL environment (e.g., Mujoco)? This would further support the generality of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lnsgqc4SwA", "forum": "zmZsWCGzUV", "replyto": "zmZsWCGzUV", "signatures": ["ICLR.cc/2026/Conference/Submission12727/Reviewer_ZwDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12727/Reviewer_ZwDH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251659768, "cdate": 1762251659768, "tmdate": 1762923548892, "mdate": 1762923548892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how to train LLMs to produce natural language explanations for the underlying decision. The authors use a distributional reward model based on a rectified flow that is conditioned on the context and on the explanation. This flow model is trained to model noisy proxy rewards  from proxy LLMs and the authors argue that if the base noise matches the proxy noise, the learned reward distribution stays close to the unseen human reward distribution. The explanation model is then optimized with PPO using sentence level rewards. Experiments on SMAC and on multiple LLM benchmarks show improvements over RLHF and RLAIF baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The use of a language conditioned rectified flow as a reward model is novel.\n- The empirical section is broad, with many benchmarks and baselines.\n- Human evaluation is convincing."}, "weaknesses": {"value": "1. The positioning of the paper is hard to grasp. Stripped of its technical machinery, the method is essentially training a model that, given only the current context or prompt, produces a reasoning trace without ever emitting the final decision/action/answer. It is therefore unclear what the concrete utility of such an “Explanation LLM” is. The title “translate policy to language” confuse the reviewer that the goal is to explain an existing policy’s decision process, but the actual pipeline trains a model from offline data to generate reasoning. This makes it difficult to see what problem for the community the work is solving. Existing reasoning LLMs already produce step-by-step rationales and, importantly, also output the final answer; the proposed model looks like a strict subset of that capability. Without a clear downstream role, the contribution feels under-motivated.\n\n2. This ambiguity in purpose is reinforced by the evaluation. The paper trains a model whose defining feature is that it only generates explanations, yet the core metric is whether another LLM (GPT-4o) can recover the correct action from that explanation. This shifts the assessment away from explanation quality and back to task accuracy. There are no direct measures tailored to the produced explanations; instead, the paper relies on inference accuracy as a proxy. If the work is to be read as an interpretability paper, this is a substantial gap: explanation-centric models should be judged on properties of the explanations themselves. As a result, the current evaluation protocol does not convincingly demonstrate that the method has advanced explanation quality.\n\n3. A further limitation is that the method is demonstrated only in settings with small, discrete action spaces such as multiple-choice QA and SMAC-style actions. This leaves out a very large portion of LLM use cases, where the output space is open-ended. In such cases, training a standard reasoning LLM that emits both rationale and answer is simpler and more broadly applicable. Because the paper does not show how to extend the method to richer action spaces, the overall practicality and generality of the proposed architecture is limited."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L3Ba7XIfTc", "forum": "zmZsWCGzUV", "replyto": "zmZsWCGzUV", "signatures": ["ICLR.cc/2026/Conference/Submission12727/Reviewer_BsL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12727/Reviewer_BsL4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762778746802, "cdate": 1762778746802, "tmdate": 1762923548545, "mdate": 1762923548545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for producing natural language explanations for an agent policy. This framework trains a dedicated LLM to generate explanations by using a distributional reward model based on continuous normalizing flows and an RLAIF training pipeline. The method is evaluated over three benchmarks, showing improved action prediction performance compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* I like the formulation of the rectified flow model for modeling rewards. In general, the setup does make sense to me for training a model to generate explanations of other policies, and the pluralistic/probabilistic view of explanations is reasonable.\n* Outside of the choice of the benchmarks (see weakness below), I appreciate the thoroughness of the empirical results. The ablations over different backbones and rewards (Table 7-10) help better understand the method’s performance and the generalization figure (Fig. 1) helps support the idea that the reward model is able to separate true/false samples."}, "weaknesses": {"value": "* I think the framing is a bit confusing. The stated goal is to explain a policy’s decision, where the policy is that of an RL agent. But the only actual “policy” that is explained is for the SMAC benchmark, with MMLU and MathQA being language-based multiple choice questions where the agent’s “policy” is simply selecting a single action out of a set. Thus, for these benchmarks there is no sequential decision making aspect (unless we are referring to sequential prediction of tokens, which doesn’t seem to be the case), and the “context” is simply the question which is independent of the policy (this could be considered the observation to an agent policy). Thus, I find the whole framing around recovering the policy and explanation for some RL agent to be needlessly confusing, as 2/3 of the benchmarks seem more like a commonsense reasoning task to me as they are agnostic to any policy.\n* The value of the explanation is measured purely in the action prediction performance. Thus the validity or faithfulness of the explanation is not measured at all. Which is also related to the above weakness: what does it mean to have a “faithful” explanation for a multiple choice question with no information about the model which is generating the answer itself (only access to the question)? This seems like an ill-posed problem to me."}, "questions": {"value": "1. It is not clear to me how the baselines are organized. Is SFT training the ExplanationLLM still? What about RLHF? Does DeepSeek and o3-mini mean they are used in place of the ExplanationLLM?\n2. In the case of MMLU example in Table 5, this is a very technical explanation. How would either the LLM or human annotators actually know whether the explanation is correct or not without having domain expertise? How do you know the explanation in Table 5 is correct at all, for example?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZaBHoQt5DT", "forum": "zmZsWCGzUV", "replyto": "zmZsWCGzUV", "signatures": ["ICLR.cc/2026/Conference/Submission12727/Reviewer_1rW4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12727/Reviewer_1rW4"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762840701160, "cdate": 1762840701160, "tmdate": 1762923548123, "mdate": 1762923548123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}