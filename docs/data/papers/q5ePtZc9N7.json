{"id": "q5ePtZc9N7", "number": 7698, "cdate": 1758032611049, "mdate": 1759897838640, "content": {"title": "Reliable Poisoned Sample Detection against Backdoor Attacks Enhanced by Sharpness Aware Minimization", "abstract": "This work investigates Poisoned Sample Detection (PSD), a promising defense approach against backdoor attacks. However, we observe that the effectiveness of many advanced PSD methods degrades significantly under weak backdoor attacks (\\eg, low poisoning ratios or weak trigger patterns). To substantiate this observation, we conduct a statistical analysis across various attacks and PSD methods, revealing a strong correlation between the strength of the backdoor effect and the detection performance. Inspired by this, we propose amplifying the backdoor effect through training with Sharpness-Aware Minimization (SAM). Both theoretical insights and empirical evidence validate that SAM enhances the activations of top Trigger Activation Change (TAC) neurons while suppressing others. Based on this, we introduce SAM-enhanced PSD, a simple yet effective framework that seamlessly improves existing PSD methods by extracting detection features from the SAM-trained model rather than the conventionally trained model. Extensive experiments across multiple benchmarks demonstrate that our approach significantly improves detection performance under both strong and weak backdoor attacks, achieving an average True Positive Rate (TPR) gain of +34.3% over conventional PSD methods. Overall, we believe that the revealed correlation between the backdoor effect and detection performance could inspire future research advancements.", "tldr": "", "keywords": ["Backdoor Defense", "Poisoned Sample Detection", "AI security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c9e02d96a91a0a0673897eed82ee035737ca54b.pdf", "supplementary_material": "/attachment/772fca13707bdf754173f0c64147600ba01a7a0a.zip"}, "replies": [{"content": {"summary": {"value": "This work investigates Poisoned Sample Detection (PSD), a promising defense approach against backdoor attacks. The investigation results show that PSD perform poorly against weak backdoor attacks. Some experiments are conducted to validate the observation. To resolve this problem, the authors propose SAM, a method to enhance the backdoor attack effect to make it easier to be detected by PSD defenses.Combined with PSD and SAM, the newly proposed defense is claimed to be more effective in a wider range of attack scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Investigate the weakness of current PSD-based backdoor defense via theoretical analysis and some experiments;\n2. Propose a method, called Sharpness-Aware Minimization (SAM), to enhances the activations of top trigger activation change neurons during backdoor attacks.\n3. By combining SAM and PSD, propose a new defense that outperforms existing PSD methods."}, "weaknesses": {"value": "1. Directly applying SAM to PSD limits the contribution of this work.\n2. Some experimental results are repeatedly shown, e.g., Fig. 2 (right) and Fig 5. Moving the repeated part to the appendix and add more \n3. Some experimental results looks like a little strange.\n4. Maybe suffer a low performance over hard-to-learn datasets."}, "questions": {"value": "1. From this paper, SAM looks to be a 'perfect' amplification for sparse neuron activations and can be combined with all kinds of previous PSD methods. While, I know SAM is good, but presenting its limitation and explaining why it does not matter to the defense can make us understand more about the newly proposed defense.\n2. As shown in Fig.5, from the visualization results in the left, the two clusters are almost separated. However, the right part shows that the poisoned samples are totally mixed together with the clean data. This is very strange, and impossible for most cases. I'm wondering why. \n3. Another question is about the FPR. A very lot of experiments are conducted by the authors. In almost every experiment, the reported FPR is affected very slightly by the added SAM mechanism. This phenomenon violates the assumption of SAM. That is, if SAM truely and only amplify the difference between poisoned and clean data like what has been shown in this paper, the indicator of FPR should also be improved but not only the TPR. Maybe, the reason is that the SAM also amplified the difference between some 'rarely-occurred' clean data with normal ones. Therefore, the proposed defense may suffer a low performance on some hard-to-learn dataset. I suggest the author to clarify this phenomenon."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Uo9hPV6vB8", "forum": "q5ePtZc9N7", "replyto": "q5ePtZc9N7", "signatures": ["ICLR.cc/2026/Conference/Submission7698/Reviewer_GmvL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7698/Reviewer_GmvL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396087762, "cdate": 1761396087762, "tmdate": 1762919758598, "mdate": 1762919758598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method for detecting poisoned samples in weak-trigger backdoor attacks. The authors propose to use Sharpness-Aware Minimization (SAM) to amplify the representations of backdoor-related neurons, improving the separability between poisoned and clean samples. The approach is theoretically grounded and shows consistent gains across multiple detection methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The studied setting is both challenging and practical, focusing on data poisoning with weak and inconspicuous triggers.\n\nThe proposed method is principled, following an optimization-based strategy with clear theoretical motivation.\n\nExperimental results demonstrate that it significantly improves the detection accuracy of existing poisoned-sample detection approaches."}, "weaknesses": {"value": "The method still requires access to a small number of clean samples for feature extraction, which may limit applicability in fully unsupervised settings.\n\nThe approach introduces noticeable computational overhead due to the use of SAM. Although the authors mention efficient variants such as MSAM, the extra cost during training remains substantial."}, "questions": {"value": "How does the performance scale with the number of available clean samples?\n\nIt would be helpful if the authors could provide more details on how backdoor-related features are extracted in Section 3.4, since this step is central to the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5yWWs2f9CG", "forum": "q5ePtZc9N7", "replyto": "q5ePtZc9N7", "signatures": ["ICLR.cc/2026/Conference/Submission7698/Reviewer_bYYD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7698/Reviewer_bYYD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620214178, "cdate": 1761620214178, "tmdate": 1762919757733, "mdate": 1762919757733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of Poisoned Sample Detection (PSD) in defending deep neural networks against backdoor attacks. It observes that existing PSD methods degrade significantly when the backdoor effect is weak (e.g., low poisoning ratio or weak trigger patterns). Through extensive statistical analysis, the authors reveal a strong positive correlation between the backdoor effect (quantified via Trigger Activation Change, TAC) and detection performance. To enhance detection, the paper proposes SAM-enhanced PSD, a framework that applies Sharpness-Aware Minimization (SAM) during model training to amplify the backdoor effect without modifying the dataset. This increases feature-space separability between poisoned and clean samples, boosting the performance of existing PSD detectors. Experiments on CIFAR-10, Tiny ImageNet, and GTSRB across multiple architectures and attack types show an average True Positive Rate (TPR) gain of +34.3% over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear empirical motivation and clarity: The paper is well organized and easy to follow, empirically analyzing the correlation between backdoor strength and detection success.\n2. Method simplicity and compatibility: SAM-enhanced training can be applied universally to many PSD methods with no structural modification.\n3. Comprehensive Experimental Validation: Across 13 attack types and 5 detection methods, the SAM-enhanced pipeline shows robust gains, particularly under weak backdoor attacks. Extensive evaluation and ablation include adaptive attacks, robustness to auxiliary data, runtime efficiency, and stability analysis."}, "weaknesses": {"value": "1. Limited Novelty: Although the proposed idea is elegant, it essentially amounts to incorporating Sharpness-Aware Minimization (SAM), Trigger Activation Change(TAC) and Spectre into the standard training process to enhance feature separability between poisoned samples and benign samples. The work does not introduce a fundamentally new algorithmic component or theoretical framework. Moreover, the theoretical analysis, while insightful, is derived under a simplified two-layer ReLU assumption and may not fully generalize to deeper or more complex network architectures. \n\n2. Restricted dataset and model diversity: Despite the extensive number of experiments, the evaluations are limited to relatively simple datasets (e.g., CIFAR-10, Tiny ImageNet) and standard architectures (e.g., ResNet-18). To demonstrate broader applicability, the study should include results on more complex or large-scale architectures such as ViT or CLIP, and potentially extend to other modalities like text or multimodal backdoor scenarios."}, "questions": {"value": "1. The authors propose the key statement \"These results suggest that SAM selectively amplifies the most discriminative backdoor features by encouraging sharper activation patterns, thereby enhancing the backdoor effect.\" \nFor me, SAM method compels attackers to more carefully select adversarial perturbations to achieve their goal, thereby increasing the divergence between poisoned and normal samples on the TAC. This stems from SAM being a more robust learning approach; attacking SAM models introduces greater variations at the activation, making PSD inherently easier. This conclusion follows naturally and is not a key contribution from the authors of this paper.\n\n2. Increased FPR: Several experimental results indicate a rise in FPR when applying SAM-enhanced PSD. The paper would benefit from a deeper analysis of this phenomenon, including potential causes (e.g., over-amplification of benign neuron activations) and mitigation strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GfqkfjPEv5", "forum": "q5ePtZc9N7", "replyto": "q5ePtZc9N7", "signatures": ["ICLR.cc/2026/Conference/Submission7698/Reviewer_CppV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7698/Reviewer_CppV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795883519, "cdate": 1761795883519, "tmdate": 1762919757124, "mdate": 1762919757124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why existing pre-training poisoned sample detection methods fail under weak or low-poisoning-rate backdoor attacks, and proposes a principled enhancement mechanism using Sharpness-Aware Minimization. The authors begin by establishing a strong positive correlation between the \"backdoor effect\" strength and PSD performance. A weak backdoor effect leads to poor separability between poisoned and clean samples in the feature space, thereby crippling detection. The authors find that higher Top-k TAC values correspond to higher separability between clean and poisoned samples. They then theoretically and empirically show that training the model with SAM amplifies TAC on “backdoor neurons,” thereby enhancing the discriminability of poisoned samples.\nBased on this, the paper introduces SAM-enhanced PSD. The three-stage pipeline involves: (1) training a model on the suspicious dataset using SAM; (2) extracting features from this SAM-trained model (using PCA as a surrogate for high-TAC neuron features); and (3) feeding these \"enhanced\" features to existing feature-based PSD detector. Experiments show that this approach significantly boosts the performance of various PSD methods against both strong and weak attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies an interesting empirical relationship between backdoor effect strength (Top-k TAC) and PSD performance, which offers some  interpretation of why some PSDs fail under weak attacks.\n2. SAM-enhanced training is plug-and-play and can be combined with many existing PSDs\n3. An analytical study (in a 2-layer ReLU model) shows why SAM’s second-order correction term selectively increases activation differences on “backdoor neurons” while regularizing irrelevant ones.\n4. Results cover multiple datasets, network architectures, and attack types, showing consistent TPR and F1 improvement. This method maintains clean accuracy while increases computational cost (≈2× over SGD)."}, "weaknesses": {"value": "1. Stage-2 (PCA + covariance whitening) still assumes access to some clean or near-clean reference data, which may not always be available in realistic settings.\n\n2. SAM roughly doubles training cost. Although mitigated by MSAM/ASAM variants, this cost may limit scalability to large-scale or foundation models.\n\n3. Limited analysis of adaptive attackers. The paper briefly mentions adaptive attacks but does not rigorously test scenarios where attackers design triggers to minimize TAC or exploit SAM’s bias.\n\n4. Experiments focus on standard small-scale datasets. Demonstrations on larger or self-supervised models would better validate the scalability and generality of the approach."}, "questions": {"value": "1. Is there a systematic way to balance enhanced separability (TAC increase) with preserved clean accuracy?\n2. Can the SAM-induced amplification of backdoor neurons inadvertently amplify benign, class-specific rare patterns, potentially increasing false positives?\n    It would be interesting to see whether SAM also enlarges intra-class variance or amplifies rare but legitimate sub-patterns, which might explain potential FPR changes.\n3. SAM is known to improve generalization, but how does it affect the clean accuracy (ACC) and robust accuracy (RA) of models trained on poisoned data before detection? The retraining results in Sec. C.6 are promising, but could you provide metrics on the intermediate SAM-trained model's performance prior to PSD application?\n4. It is good that Sec. C.4 shows limited gains for perturbation- and topology-based detectors, but I am still uncertain why SAM suppresses rather than enhances these methods.  \n5. Since SAM is originally designed to improve model generalization by seeking flatter minima, could its effect of amplifying backdoor-related features be interpreted as a byproduct of enhancing the model’s sensitivity to stable patterns in the poisoned training data?\n6. If the attacker uses multiple, distinct trigger patterns across the poisoned dataset, can SAM still effectively identify and amplify a coherent signal for detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W5cuIEkbmc", "forum": "q5ePtZc9N7", "replyto": "q5ePtZc9N7", "signatures": ["ICLR.cc/2026/Conference/Submission7698/Reviewer_ELXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7698/Reviewer_ELXT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762273253157, "cdate": 1762273253157, "tmdate": 1762919756600, "mdate": 1762919756600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}