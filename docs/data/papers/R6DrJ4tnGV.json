{"id": "R6DrJ4tnGV", "number": 8934, "cdate": 1758103063680, "mdate": 1759897752943, "content": {"title": "Scaling Linear Attention with Sparse State Expansion", "abstract": "The Transformer architecture, despite its widespread success, struggles with long-context scenarios due to quadratic computation and linear memory growth. While various linear attention variants mitigate these efficiency constraints by compressing context into fixed-size states, they often degrade performance in tasks such as in-context retrieval and reasoning. To address this limitation and achieve more effective context compression, we propose two key innovations. First, we introduce a row-sparse update formulation for linear attention by conceptualizing state updating as information classification. This enables sparse state updates via softmax-based top-$k$ hard classification, thereby extending receptive fields and reducing inter-class interference. Second, we present Sparse State Expansion (SSE) within the sparse framework, which expands the contextual state into multiple partitions, effectively decoupling parameter size from state capacity while maintaining the sparse classification paradigm. Supported by efficient parallelized implementations, our design achieves effective classification and highly discriminative state representations. We extensively validate SSE in both pure linear and hybrid (SSE-H) architectures across language modeling, in-context retrieval, and mathematical reasoning benchmarks. SSE demonstrates strong retrieval performance and scales favorably with state size. Moreover, after reinforcement learning (RL) training, our 2B SSE-H model achieves state-of-the-art mathematical reasoning performance among small reasoning models, scoring 64.5 on AIME24 and 50.2 on AIME25, significantly outperforming similarly sized open-source Transformers. These results highlight SSE as a promising and efficient architecture for long-context modeling.", "tldr": "SSE is a partitioned state expansion method under a row-sparse update framework for linear attention, improving retrieval and reasoning.", "keywords": ["Linear Attention", "Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1263f763365c78678dd64a677e47a53d7aed5497.pdf", "supplementary_material": "/attachment/6e3475d7248015327464c0fe2192bc1fb8d0142d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework to enhance the efficiency and scalability of linear attention models for long-context processing. It introduces two innovations: (1) row-sparse state updates, which treat information storage as a classification task using top-k softmax selection to reduce interference and extend context range, and (2) Sparse State Expansion (SSE), which partitions the state into multiple sparsely updated segments to increase memory capacity without adding parameters. Experiments show that SSE and its hybrid variant (SSE-H) outperform earlier linear attentions across language modeling, retrieval, and reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear motivation, sound implementation, and good performance with model at scale."}, "weaknesses": {"value": "1. From the Fig.4 we can see that the wall time is not linear with respect to the sequence length. Why? Hope the authors could provide some more experiment results comparing the wall time of their architecture against earlier linear attentions, GLA, GDN, Mamba2, and full attention.\n2. Hope the authors could also provide scaling experiments compare their architecture against earlier linear attentions, GLA, GDN, Mamba2, and full attention, considering training wall time vs perplexity.\n3. Hope the authors could also discuss MoE for quadratic attention in their related work."}, "questions": {"value": "On which datasets was the model trained? Will these data be open-sourced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTSAYPRgRM", "forum": "R6DrJ4tnGV", "replyto": "R6DrJ4tnGV", "signatures": ["ICLR.cc/2026/Conference/Submission8934/Reviewer_KhC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8934/Reviewer_KhC1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761019914338, "cdate": 1761019914338, "tmdate": 1762920680205, "mdate": 1762920680205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework to enhance the efficiency and scalability of linear attention models for long-context processing. It introduces two innovations: (1) row-sparse state updates, which treat information storage as a classification task using top-k softmax selection to reduce interference and extend context range, and (2) Sparse State Expansion (SSE), which partitions the state into multiple sparsely updated segments to increase memory capacity without adding parameters. Experiments show that SSE and its hybrid variant (SSE-H) outperform earlier linear attentions across language modeling, retrieval, and reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear motivation, sound implementation, and good performance with model at scale."}, "weaknesses": {"value": "1. From the Fig.4 we can see that the wall time is not linear with respect to the sequence length. Why? Hope the authors could provide some more experiment results comparing the wall time of their architecture against earlier linear attentions, GLA, GDN, Mamba2, and full attention.\n2. Hope the authors could also provide scaling experiments compare their architecture against earlier linear attentions, GLA, GDN, Mamba2, and full attention, considering training wall time vs perplexity.\n3. Hope the authors could also discuss MoE for quadratic attention in their related work."}, "questions": {"value": "On which datasets was the model trained? Will these data be open-sourced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTSAYPRgRM", "forum": "R6DrJ4tnGV", "replyto": "R6DrJ4tnGV", "signatures": ["ICLR.cc/2026/Conference/Submission8934/Reviewer_KhC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8934/Reviewer_KhC1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761019914338, "cdate": 1761019914338, "tmdate": 1763773190385, "mdate": 1763773190385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper improves context compression in linear attention models with row-sparse update and sparse state expansion (SSE). The row-sparse update learns a sparse mask with top-k and softmax to update only a few rows in the contextual states, which utilizes the state space more effectively than dense update. Since different tokens are associated with different rows, the row-sparse update eliminates the need of gating at each step, thereby avoiding the limited receptive field caused by gating. The authors then propose SSE to extend row-sparse update to larger state space. SSE first divides a large state space into N partitions, and then perform dense updates on k partitions. By sharing the attention parameters across partitions, SSE decouples the parameters size and the memory capacity, thereby solving the capacity bottleneck of linear attention models. Experiments on language modeling, needle-in-a-haystack and reasoning benchmarks show that SSE and the hybrid SSE-H achieves state-of-the-art performance among linear attention models."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper tackles the memory capacity problem, a key issue in linear attention models with clear motivations. The row-wise sparse update aims to improve the utilization of a fixed size state space, while the SSE extends the memory capacity without increasing the number of parameters.\n2. Experiments are thorough and solid. The authors visualizes the cosine similarity of the state space and show that row-sparse update significantly improves state space utilization compared to existing designs. The final SSE model is evaluated against Transformer and linear attention baselines on a wide range of benchmarks, with a rigorous setup of fixed number of parameters.\n3. The efficiency of SSE is not confined to theory. The authors implemented SSE by grouping tokens into subsequences according to their partitions and executing them with a linear attention kernel call. This makes SSE practical for real-world long context use."}, "weaknesses": {"value": "1. The writing of this paper may be largely improved. The authors used quite a few terminologies or preliminaries without enough explanations, which make this paper hard to follow. For example, the authors didn’t explain how they computed the cosine similarity in Figure 1. Line 176 compares SSE against gated linear attention, but the form of gated linear attention is never mentioned in the paper. There aren’t ground truth classes nor a classification task, but the authors keep using the term classification to refer to state space utilization. In Line 245, the term segmented clustering is used, but this isn’t a commonsense for audience. Captions of figure 1 and 3 need to be extended with their implications. See more in questions.\n2. The title doesn’t exactly reflect the contribution of this paper. “Scaling linear attention\" sounds like this paper studies the model performance under different linear attention sizes. I would suggest modifying it to be “Extending linear attention capacity with sparse state expansion”.\n3. While SSE focuses on improving linear attention models, there isn’t any comparison of wall time for SSE and baselines. Could you please report the performance-time trade-off curve for SSE, Transformer and other linear attention models? That will help audience know which model to use given a specific context length."}, "questions": {"value": "1. Is the cosine similarity computed for a single step then averaged over a sequence or the whole dataset?\n2. The logic in Line 174-178 is hard to understand. It’s hard to understand what Propositions 2-4 are without looking into the Appendix. Propositions 2-4 use the row-sparse update, which is introduced only in later sections. Besides, it’s not very clear how the conclusion of decay in gated variants is derived before looking into Proposition 4. As row-sparse update is complementary to gating and they may co-exist, it’s also hard to understand why SSE solves the issue caused by gating. You may discuss eliminating gating as a benefit of SSE after introducing the method.\n3. Line 184-186: By theoretical analysis, do you mean Proposition 2? It’s a little bit hard to understand this without looking into the appendix. You need to add more details in the main paper.\n4. The functions softmax and top-k produce a $k$ dimension vector by their definition, which is not correct. Do you mean softmax on the $k$ non-zero elements and keep the rest as 0? Then you need to re-define your softmax function.\n5. Line 263-265: This key insight should be brought to early paragraphs of Sec 4.1. Otherwise, it’s hard to understand how SSE is connected with row-sparse update. You’re essentially factorizing a row-sparse update for N*c rows into two small parameter matrices.\n6. Font size in Figure 3 & 4 should be increased.\n7. Line 284 & 286: Why can the always-selected partition capture local interactions? The inputs are accumulated by addition, which doesn’t model interactions.\n8. Line 294-296: Please explain why singular value entropy reflects the difficulty of compression. It’s not a commonsense to audience.\n9. Line 301: Sequential computation is never explained in the main paper.\n10. Line 308-310: I would recommend to draw a figure for grouping and varlen technique.\n11. In Table 1, linear attention models are worse than Transformer. Is it because language modeling requires more pairwise interactions? Then why SSE becomes on par with Transformer in Table 3 & 4?\n12. Figure 5: Which task is it?\n13. Figure 6: Does SSE-Shared refer to the always-selected partition? Please be consistent in the terms.\n14. Line 457: Is the sparsity measured within the selected partitions? For n4k1, I understand the upper bound of sparsity for all partitions should be 25%, right?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2EE5YHbpQy", "forum": "R6DrJ4tnGV", "replyto": "R6DrJ4tnGV", "signatures": ["ICLR.cc/2026/Conference/Submission8934/Reviewer_heMc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8934/Reviewer_heMc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451662833, "cdate": 1761451662833, "tmdate": 1762920679751, "mdate": 1762920679751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Sparse State Expansion (SSE) for linear attention. Two ideas drive the method: (i) row‑sparse updates that treat state updates as a classification problem and write only to top‑k rows via a softmax head, and (ii) state expansion into N shared‑parameter partitions chosen by a write–read gate, so capacity (state size) scales without growing parameter count. Efficient masked/varlen implementations are provided. Empirically, SSE and its hybrid variant are tested on language modeling, retrieval, and math reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Modeling state updates as information classification is well argued and operationalized \n2. Decoupling the model’s parameters and state's parameters is important direction for improving linear transformers."}, "weaknesses": {"value": "1. There is no convincing explanation on why SSE is only effective for linear attention but not deltanet.\n2. Baselines seem to be cherry picked since they are neither the most powerful nor fundamentally relevant sequence models. \n3. The paper uses hard partition selection (top‑k) and softmax row selection, but the gradient treatment for the discrete top‑k isn’t described. Can you please clarify how gradients flow through Eqs. 7–9 and whether any implementation tricks are needed for stability."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YFw52mwlxX", "forum": "R6DrJ4tnGV", "replyto": "R6DrJ4tnGV", "signatures": ["ICLR.cc/2026/Conference/Submission8934/Reviewer_Jg4D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8934/Reviewer_Jg4D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893411107, "cdate": 1761893411107, "tmdate": 1762920679334, "mdate": 1762920679334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes row-selector to bottleneck the update of state and use multiple partitions to expand state size for improved expressiveness. The results show their methods can get comparable performance with Transformers."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. results are strong: SOTA in 2B reasoning model.\n2. preliminaries are well-organized, proofs are completed.\n3. enable multiple efficient parallelized implementations."}, "weaknesses": {"value": "1. phrasing can be simplified and changed for better delivery: information classification, row-sparse -> row-selector. (TBH information classification is very confusing).\n2. miss an important baseline Mamba/Mamba2"}, "questions": {"value": "1. can you provide some efficiency analysis, especially empirical evidence, when compared to baseline linear attention models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZyCXTzc2XM", "forum": "R6DrJ4tnGV", "replyto": "R6DrJ4tnGV", "signatures": ["ICLR.cc/2026/Conference/Submission8934/Reviewer_dHEC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8934/Reviewer_dHEC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975107438, "cdate": 1761975107438, "tmdate": 1762920678878, "mdate": 1762920678878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}