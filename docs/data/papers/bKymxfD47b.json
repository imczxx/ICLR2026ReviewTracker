{"id": "bKymxfD47b", "number": 4512, "cdate": 1757693442364, "mdate": 1759898028876, "content": {"title": "Observational Scaling Laws in LLM-based Embodied Decision Making", "abstract": "We introduce an observational method to derive scaling laws for LLM performance in embodied decision-making tasks, allowing us to predict embodied skills, quantify simulation gaps, and algorithm intervention. In contrast to conventional scaling research that trains multiple models from scratch at different scales, our approach bypasses new model training and instead \nUses publicly available pretrained LLMs to model performance trends across different model families and sizes. Constructing such unified scaling law across diverse model families is challenging, as these models differ in both training compute efficiency and resulting capabilities. We address this by employing a generalized scaling framework that expresses model performance as a function of a low-dimensional capability space. We first validate such scaling law on the Embodied Agent Interface (EAI) benchmark across 125 LLMs, confirming a predictive accuracy that represents at least a 50\\% improvement over traditional compute scaling laws. We then find that an LLM's decision-making ability is highly predictable—accurately forecasting the performance of larger models using data from those as small as 40B parameters—which allows us to quantify both the performance gap between simulation environments and the impact of structured decoding.", "tldr": "", "keywords": ["Scaling Laws", "Embodied AI"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77cad96c86ed0d754d3d536c89c9b49f3bef3600.pdf", "supplementary_material": "/attachment/71c9d21c6896b16a9a1342aef9ec1f651bca91cf.zip"}, "replies": [{"content": {"summary": {"value": "This paper applies the observational scaling law framework to embodied decision-making tasks via the Embodied Agent Interface. Instead of training models at multiple scales, the authors estimate scaling trends across 125 publicly available LLMs and attempt to predict performance on downstream embodied skills, quantify simulation-to-simulation gaps, and measure the effect of structured decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The study on embodied benchmarks is thorough, covering a diverse family of LLMs across scales on different embodied benchmarks, and observes idiosyncratic performance change when structured decoding is used to ensure valid format in generating plans and actions.\n- The paper is clearly written, the visualizations are clear and the experiments appear to be carefully executed."}, "weaknesses": {"value": "- The novelty is very limited. The core method originates from observational scaling law. The contribution here is to apply and validate it to a different benchmark suite. I don't see any major contributions other than empirically fitting an existing framework to embodied benchmarks with the conclusion that this existing methodology works for certain benchmarks that the original authors did not evaluate. Therefore, I am only convinced on the second contribution that the authors mentioned -- analyzing gaps between different simulation environments and structured outputs that are domain-specific. But I don't believe this contribution alone is suitable for a conference paper.\n- Even only considering validating observational scaling law on decision-making tasks as a novelty, it is still very incremental because observational scaling law already discussed scaling on agentic tasks, and the evaluated benchmarks in this work only have a different domain (planning and execution in physical world tasks vs digital agents e.g., coding)."}, "questions": {"value": "- Can the authors clearly articulate what new conceptual or methodological insight is introduced beyond applying observational scaling to EAI?\n- How does EAI differ from AgentBench/AgentBoard in terms of scaling behavior? Are there qualitative failure modes unique to embodied settings?\n- Could the authors analyze where the scaling law breaks down in embodied tasks? That could provide deeper value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vrKdTDJmpz", "forum": "bKymxfD47b", "replyto": "bKymxfD47b", "signatures": ["ICLR.cc/2026/Conference/Submission4512/Reviewer_RaeT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4512/Reviewer_RaeT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873331905, "cdate": 1761873331905, "tmdate": 1762917414509, "mdate": 1762917414509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel observational method for deriving scaling laws for Large Language Models (LLMs) in embodied decision-making tasks. Instead of the conventional, resource-intensive approach of training models from scratch at various scales, the authors leverage a large set of 125 publicly available pre-trained LLMs. The core idea is to model performance not as a direct function of compute, but as a function of a low-dimensional \"capability space\" derived from standard NLP benchmarks via Principal Component Analysis (PCA). This generalized framework allows for unifying scaling trends across diverse model families (e.g., Llama, Qwen, Gemma).\nThe authors validate their approach on the Embodied Agent Interface (EAI) benchmark, demonstrating that their observational scaling law achieves significantly higher predictive accuracy (over 50% improvement in MSE) for larger models (>40B parameters) compared to traditional laws based on model size or training FLOPs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The claims are well-supported by extensive experiments on 125 open LLMs from 28 different families. The results clearly show that the proposed observational scaling law significantly outperforms traditional compute-based baselines in extrapolating performance to larger models, as evidenced by the substantially lower test MSE.\n2. The paper is well-written, and the methodology is explained clearly. The figures, particularly the comparisons of scaling curves in Figure 4 and the analysis of regression coefficients in Figure 5, are effective at conveying the key results and their implications."}, "weaknesses": {"value": "- Limited Scope of Models: The study is confined to open-source models. While this is a practical necessity for the methodology, it overlooks the performance of state-of-the-art, closed-source models (e.g., from the GPT-4 or Claude families), which often define the upper frontier of capabilities. It remains an open question whether these models would conform to the same scaling laws, and their exclusion might limit the generality of the conclusions.\n- Generalizability to New Tasks and Paradigms: The scaling laws are derived and validated on the specific tasks within the EAI benchmark. The paper does not provide evidence on how well these laws might generalize to entirely new embodied tasks or different simulation environments. Furthermore, the current landscape of LLMs is largely based on the transformer architecture. If a fundamentally new training paradigm or model architecture emerges, it is uncertain whether this observational framework would remain effective without significant recalibration."}, "questions": {"value": "- The paper sets the number of principal components to K=3, noting it captures ~97% of the variance. How was this value of K chosen? Was there a more principled method used, or was it based on this explained variance threshold? How sensitive are the final predictions and the interpretation of the capability space to different choices of K (e.g., K=2 or K=5)?\n\n- As noted in the weaknesses, the framework's reliance on current benchmarks and model architectures raises questions about its long-term robustness. Can the authors comment on how they envision this method adapting to (a) novel embodied tasks with different challenges, and (b) potential future shifts in LLM training paradigms that might alter the relationship between standard benchmarks and embodied skills?\n\n- The predictive accuracy of the scaling laws is evaluated using Mean Squared Error (MSE). While MSE is a standard metric, it can be sensitive to outliers. Is one MSE metric enough? Have the authors considered alternative metrics, such as Mean Absolute Error (MAE) or a ranking correlation metric (like Spearman's ρ), to provide a more robust assessment of the model's predictive power?\n\n\n- The paper reports a strong correlation (R² > 0.89) for the log-linear fit between capability (PC-1) and compute within model families. A high R² on observed data indicates a good fit but does not guarantee strong extrapolative performance. Could the authors elaborate on the risk of this relationship being a good interpolation that doesn't hold for models significantly larger than those in the training set?\n\n- The methodology filters out models with \"zero task performance\" or \"missing benchmark scores.\" This could introduce a selection bias, as these excluded models (especially the zero-performance ones) might represent a critical low-capability regime. Could the exclusion of these data points potentially skew the fitted scaling curve and affect the model's ability to accurately predict the initial \"emergence\" of a capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4aphibGdlH", "forum": "bKymxfD47b", "replyto": "bKymxfD47b", "signatures": ["ICLR.cc/2026/Conference/Submission4512/Reviewer_sDo5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4512/Reviewer_sDo5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917251638, "cdate": 1761917251638, "tmdate": 1762917414010, "mdate": 1762917414010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an observational scaling framework to derive scaling laws for LLM performance in embodied decision-making tasks without training new models, using a low-dimensional capability space to unify trends across 28 model families (125 LLMs total). Validated on the Embodied Agent Interface (EAI) benchmark, it predicts emergent skills in models >40B from smaller ones, quantifies performance gaps between simulators, and measures degradation from structured decoding, achieving 50% better predictive accuracy than compute-based laws."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Novel observational approach bypasses costly retraining by leveraging existing LLMs and upstream benchmarks, enabling unified scaling across heterogeneous families like LLaMA, Qwen, and Gemma.\n2.Strong empirical validation on EAI with 125 models shows high predictive power, e.g., forecasting large-model performance from <40B data and quantifying sim-to-real gaps.\n3.Practical insights into interventions: reveals structured outputs degrade performance and simulator differences impact scaling, with clear visualizations and ablations.\n4.Theoretically grounded in generalized scaling (log-linear trends in capability space), bridging language and embodied domains."}, "weaknesses": {"value": "1.Relies on upstream benchmarks (e.g., reasoning, coding) as proxies for capabilities, which may not fully capture embodied-specific skills like spatial reasoning or dynamics modeling.\n2.Limited to open LLMs up to 40B-70B scale; excludes proprietary giants like GPT-4, potentially biasing cross-family generalizations.\n3.EAI benchmark is simulation-only; no real-world validation, so sim gap quantification remains hypothetical without physical robot tests.\n4.Predictive accuracy claims (50% over baselines) lack error bars or sensitivity analyses to noisy upstream data or family-specific efficiencies.\n5.Focuses on zero-shot LLM prompting; ignores fine-tuning or VLA integrations that could alter scaling behaviors."}, "questions": {"value": "1.How do you select the low-dimensional capability space? Any ablation on including/excluding specific upstream benchmarks?\n2.Can the framework predict scaling for proprietary models (e.g., GPT-5) if upstream scores are available?\n3.What are the compute costs for evaluating 125 LLMs on EAI—e.g., total inference tokens or time?\n4.How robust is the sim gap quantification to different EAI variants or non-household tasks?\n5.Does the degradation from structured decoding hold for advanced techniques like JSON mode or tool-calling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VGhpG1AIXM", "forum": "bKymxfD47b", "replyto": "bKymxfD47b", "signatures": ["ICLR.cc/2026/Conference/Submission4512/Reviewer_hjGC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4512/Reviewer_hjGC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996417301, "cdate": 1761996417301, "tmdate": 1762917413436, "mdate": 1762917413436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}