{"id": "2x9Dat4PBO", "number": 8382, "cdate": 1758080909676, "mdate": 1759897788542, "content": {"title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning", "abstract": "Open-ended evaluation is essential for deploying large language models in real-world settings. In studying HealthBench, we observe that using the model itself as a grader and generating rubric-based reward signals substantially improves reasoning performance. Remarkably, the trained model also becomes a stronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning, a lightweight framework that enables faster and more resource-efficient training while surpassing baselines. Remarkably, on Qwen3-32B, training with just the 4000-sample *HealthBench Easy* subset is sufficient to obtain a model that exceeds GPT-5 on *HealthBench Hard*. Incorporating a small amount of teacher-graded data further enhances performance for less capable models.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Language Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af4d3556c10285bdc8621d28a134bd140c27ccb5.pdf", "supplementary_material": "/attachment/134e7364de78b5c0b3ae5192f8db75eae68fdc03.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces self-rewarding rubric-based RL for open-ended reasoning task, i.e., HealthBench. Firstly, the paper conducts a meta-evaluation on open-source models, and qualitatively evaluates the ability of Qwen3-32B and Qwen3-8B on meta-evaluation. Then, it introduces a self-rewarding RL framework for improving training efficiency and open-ended downstream tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-written and structured.\n- Introduce a self-rewarding RL framework for open-ended downstream tasks."}, "weaknesses": {"value": "I am concerned that the novelty and soundness of this method are limited. On the one hand, previous work already introduced similar ideas, such as [1], [2]. Only the training objective introduces variance. On the other hand, this paper only verifies the effectiveness on HealthBench. I believe that introducing more benchmarks, such as creating writing, is more convincing.\n\n[1]Constitutional AI: Harmlessness from AI Feedback;\n[2]Self-Rewarding Language Models;"}, "questions": {"value": "Same to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dRTVBVAQWB", "forum": "2x9Dat4PBO", "replyto": "2x9Dat4PBO", "signatures": ["ICLR.cc/2026/Conference/Submission8382/Reviewer_LEaX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8382/Reviewer_LEaX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921923175, "cdate": 1761921923175, "tmdate": 1762920288115, "mdate": 1762920288115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the research question of \"can the model grade itself using those rubrics and get better at both reasoning and grading?\" They adapt the GRPO/DAPO-style RL and replace the external reward model with the policy itself, scoring each rubric item generatively and summing normalized positive rubric points as the reward (without a KL penalty). They train Qwen3‑32B on 4 K HealthBench-Easy prompts, achieving self-rewarding yields of HealthBench-Hard (graded by GPT-4.1), outperforming their RL baseline and a reported GPT-5 score."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The self-rewarding method sounds novel. The policy model doubles as a generative rubric scorer, yielding fine‑grained, verifiable eward signals without a separate reward model.  It is a crisp, impactful tweak that measurably improves performance and efficiency on a hard, open‑ended benchmark.\n2. This method also sounds promising but would benefit from frozen‑grader/KL/length‑control ablations and multi‑grader/human eval to preempt known pitfalls (grader drift, reward hacking, verbosity bias).\n3. The baseline comparison is complete. This paper first runs a “standard” rubric‑graded RL baseline, then switches the grader to the policy and compares.  \n4. This paper explains why self‑rewarding is faster in their infra and quantifies the effect"}, "weaknesses": {"value": "1. **This is the major concern of this paper**. All results are in the medical domain and on a single benchmark (HealthBench). Generalization to other open‑ended tasks is untested and explicitly left for future work.  \n2. The paper cites a GPT‑5 score (Table 4). Because this is a single benchmark + single grader (GPT‑4.1), the comparison should be framed cautiously and replicated under multiple graders/humans.  \n3. The paper omits KL divergence but does not give a reason. \n4. The experimental study is not comprehensive. I do acknowledge that this paper tries many open-source models, but most of them are from the Qwen & Deepseek series. Including more diverse models, such as llama, gemma, will make the evaluation more convincing."}, "questions": {"value": "Questions:\n1. Why omit KL divergence? \n2. How robust is self‑grading? If freezing the grader to the initial policy (vs moving with training), how do scores and stability change? \n3. Any evidence that the policy learns to parrot the rubric triggers? That's to say, any reward hacking checks?\n4. How does the approach perform on non‑medical, open‑ended datasets using different rubric styles? \n\nSuggestions: \n1. Add at least one non‑medical, open‑ended task with rubrics (e.g., instruction‑following or advice) to test portability of “policy‑as‑grader.” Even a small‑scale run would reduce the domain‑restriction limitation.  \n2. This method removes KL and uses the moving policy as a grader. Providing a 2×2 ablation: {frozen GRM, moving GRM} × {no‑KL, small‑KL} will be better.\n3. The citation seems to have a few issues. Specifically, in line 644, there is something called \"5 Team\", but I have no idea where it comes from. Also, I am not sure why some citations have TLDR, but some do not. (This is a minor question raised during the review of the whole paper, which does not impact my decision of ratings)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PkQm3nylZE", "forum": "2x9Dat4PBO", "replyto": "2x9Dat4PBO", "signatures": ["ICLR.cc/2026/Conference/Submission8382/Reviewer_GKXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8382/Reviewer_GKXq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935865880, "cdate": 1761935865880, "tmdate": 1762920287752, "mdate": 1762920287752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-reward reinforcement learning method based on rubrics for handling open-ended reasoning tasks. The model evaluates its own outputs using rubrics (primarily derived from the HealthBench dataset) and employs these self-generated scores as rewards during GRPO training, while removing the KL divergence term and relying on scoring criteria for normalization. The system trained the Qwen3-32B model on the HealthBench-Easy dataset. When evaluated against GPT-4.1, it outperformed both GPT-5 and o3 models on the HealthBench-Hard dataset. The paper also examines training efficiency, dataset effects, and the SFT collapse phenomenon."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This paper highlights the challenges of open-ended evaluation in real-world reasoning, arguing that task-specific scoring criteria can provide models with stable and interpretable reward signals.\n\nOn the HealthBench Hard test, the self-reward-based GPT-4.1 model achieved a score of 0.500, outperforming GPT-5 (0.462) and o3 (0.32). This result holds consistently across two scoring temperatures (0.6 and 1.0).\n\nCompared to baseline reinforcement learning approaches, this method reduces reward computation time by half, decreases step time by approximately 25-35%, and requires fewer GPUs.\n\nBeyond reporting final scores, this paper provides detailed analysis of post-training changes across evaluation dimensions: for instance, improvements in completeness and context awareness, though communication quality occasionally declines with longer responses. The study also quantifies the frequency of positive and negative shifts across dimensions within the sample, exploring trade-offs rather than merely presenting isolated benchmark values."}, "weaknesses": {"value": "All training and evaluation are confined to the HealthBench medical benchmark dataset. This limits the model's extrapolation capabilities to open domains such as law, finance, or writing.\n\nThe reward evaluator belongs to the same model family as the policy model, utilizing only the previous checkpoint and removing the KL divergence penalty term. This may introduce risks of reward drift or self-confirmation bias. The paper primarily employs score normalization and truncation based on scoring criteria but does not propose stronger safety controls or human verification mechanisms.\n\nCore experimental results use GPT-4.1 as the final evaluator instead of human experts. The study also indicates evaluator disagreement, with some weaker evaluators tending toward leniency, raising concerns about score stability among evaluators.\n\nUnder hybrid objective settings incorporating scoring data, the Qwen3-8B model exhibited instability. The paper notes that Qwen3-8B “crashed after approximately 600 training steps due to repetitive outputs,” while Qwen3-32B remained stable, without proposing any mitigation strategies."}, "questions": {"value": "When evaluators share the same strategy and no KL constraint is applied, how is the stability of reward behavior ensured? Can quantitative evidence of reward drift or bias be provided?\n\nHas GPT-4.1's scoring of 0.500 results been validated through human assessment? If not validated, what is your confidence level in GPT-4.1's consistency with expert judgment?\n\nCan the proposed method be extended to open benchmarks outside the medical domain?\n\nCan you explain how setting the scoring temperature to the expansion temperature improves the final score? Is it because it reduces the mismatch between how answers are generated and how they are scored?\n\nRegarding the collapse phenomenon in 8B models, what mitigation strategies (e.g., entropy regularization, output diversity penalties) were tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "392cigyNCR", "forum": "2x9Dat4PBO", "replyto": "2x9Dat4PBO", "signatures": ["ICLR.cc/2026/Conference/Submission8382/Reviewer_kgBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8382/Reviewer_kgBb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964947472, "cdate": 1761964947472, "tmdate": 1762920287450, "mdate": 1762920287450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Self-Rewarding Rubric-Based Reinforcement Learning (SR-RBRL) for open-ended reasoning. Instead of relying on a separate reward model, the policy model grades its own outputs using task-specific rubrics, and those composite rubric scores drive RL updates. \nThe method omits the KL penalty used in standard GRPO (similar to DAPO), aiming to keep rewards verifiable (via explicit rubrics) while cutting scoring/compute overhead. \n\nOn HealthBench (a dialogue-style, rubric-graded medical benchmark), SR-RBRL improves performance and training efficiency: e.g., Qwen3-32B trained only on the Easy subset surpasses GPT-5 (and o3) on HealthBench-Hard, and step time/reward time drop by 25-52% under the authors’ setup. Also, The authors show that grading ability is preserved (slightly improved) after RL, and that adding a small amount of teacher-graded data helps weaker models (e.g., 8B) more than stronger ones (32B)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I think overall the task-specific rubric criteria and the model itself as grader is a neat design, avoiding a separate reward model and aligning rewards with what is actually evaluated.\n2. With only the Easy split for training, Qwen3-32B exceeds GPT-5/o3 on HealthBench-Hard, supporting the central claim.\n3. Beyond performance, Post-RL models show slight Meta-score gains as grading ability is maintained."}, "weaknesses": {"value": "1. The paper shows response length rising with training alongside reward/score improvements, but the readability/conciseness impact isn’t systematically measured. It will be interesting to see if the model behavior or other generation pattern switches during the training."}, "questions": {"value": "1. Results are concentrated on HealthBench (medical); it remains unclear how robust the approach is across other open-ended domains (education, legal, multi-domain assistants). Is there any testing scenario already? If so, do you think SR-RBRL will also fit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U0bcq5r32o", "forum": "2x9Dat4PBO", "replyto": "2x9Dat4PBO", "signatures": ["ICLR.cc/2026/Conference/Submission8382/Reviewer_otVK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8382/Reviewer_otVK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984894610, "cdate": 1761984894610, "tmdate": 1762920287040, "mdate": 1762920287040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}