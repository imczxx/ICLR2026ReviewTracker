{"id": "OiwMgMjeRz", "number": 24875, "cdate": 1758361422110, "mdate": 1759896744448, "content": {"title": "Shared Recurrent Memory Transformer for Multi-agent Lifelong Pathfinding", "abstract": "Coordination in decentralized multi-agent reinforcement learning (MARL) necessitates that agents share information about their behavior and intentions. Existing approaches rely on communication protocols with domain or resource constraints or centralized training that poorly scales to large agent populations. We introduce the Shared Recurrent Memory Transformer (SRMT), which enables coordination through unconstrained communication. SRMT provides a global memory workspace where agents broadcast their learned working memory states and query others' memory representations to exchange information and coordinate while maintaining decentralized training and execution. We evaluate SRMT on the Partially Observable Multi-Agent Pathfinding (PO-MAPF) problem, where coordination is vital for optimal path planning and deadlock avoidance. We demonstrate that shared memory enables emergent coordination even when the reward function provides minimal or no guidance. On the specifically constructed Bottleneck task that requires negotiation, SRMT consistently outperforms communicative and memory-augmented baselines, particularly under sparse reward signals, and successfully generalizes to longer corridors unseen during training. On POGEMA maps, SRMT scales with the increasing agents' population and map size, achieving competitive performance with recent MARL, hybrid, and planning-based methods while requiring no domain-specific heuristics. These results demonstrate that a transformer with shared recurrent memory enhances coordination in decentralized multi-agent systems.", "tldr": "", "keywords": ["Multi-agent System", "Shared Memory", "Transformer"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/494d40f4d440c457023c115607e2d996791b2a89.pdf", "supplementary_material": "/attachment/bbc9e551313c2bd3e8cfdbf14c8b9f549a5416e0.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Shared Recurrent Memory Transformer (SRMT) for Partially Observable Multi-Agent Pathfinding (PO-MAPF) problem by enabling coordination of agents through unconstrained communication. Specifically, SRMT proposes a shared recurrent memory transformer to broadcast their recurrent memories to a global workspace with self-attention on individual memory and observation and cross-attention on shared agent memories. Results on six maze environments from the POGEMA benchmark show that SRMT outperforms various baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation for using a global shared memory to help coordination in the PO-MAPF problem is clear.\n2.\tThis paper is well-written and well-organized. The related works are well discussed.\n3.\tPerformance of SRMT is very strong compared with various cooperative and memory baselines."}, "weaknesses": {"value": "1.\tThe idea of a global shared memory with self-attention and cross-attention has limited novelty.\n2.\tWhen compared with advanced path-planning baselines such as RHCR, the performance of SRMT is not significant. It decreases the contribution of this work as centralized planning methods are already good especially SRMT also needs a global shared memory mechanism.\n3.\tSRMT is only tested with an algorithm PPO on POGEMA. Whether SRMT could be applied to other MARL algorithms is not clear.\n4.\tOther MARL algorithms with specially designed communication mechanisms are not compared."}, "questions": {"value": "1.\tCould SRMT be integrated into QPLEX or some other advanced MARL algorithms?\n2.\tHow does SRMT compare with MARL algorithms with communication mechanism?\n3.\tWhy there is still a large performance gap of SRMT when compared with RHCR in some POGEMA maps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OLZVepEFPb", "forum": "OiwMgMjeRz", "replyto": "OiwMgMjeRz", "signatures": ["ICLR.cc/2026/Conference/Submission24875/Reviewer_hozZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24875/Reviewer_hozZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761031758743, "cdate": 1761031758743, "tmdate": 1762943229004, "mdate": 1762943229004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Shared Recurrent Memory Transformer, SRMT, an architecture for decentralized multi agent reinforcement learning in partially observable multi agent pathfinding. Instead of relying on a fixed, hand designed communication protocol or on centralized training that does not scale, SRMT gives each agent a recurrent working memory state, broadcasts those memory states into a shared global memory workspace, and then lets every agent query that shared memory via cross attention at every timestep. The idea is that agents can implicitly coordinate by aligning their internal memory representations, rather than by passing explicit structured messages. The model is trained end to end with PPO in a fully decentralized setting, the same policy weights are shared across agents, and no centralized controller is used at execution time. The paper evaluates on two settings, a minimal two agent Bottleneck task that requires negotiation to get through a one cell corridor without deadlock, and large scale partially observable multi agent pathfinding (both classical and lifelong) using the POGEMA benchmark, including environments with up to hundreds of agents. On Bottleneck, SRMT solves the task even with sparse rewards, and generalizes to corridor lengths up to 1000 cells despite only training on corridors of length 3 to 30. On POGEMA, SRMT is competitive with or ahead of centralized training methods and memory based baselines, and scales to 128 plus agents with reasonable throughput, matching or beating communication heavy baselines in coordination metrics and in congestion handling. The paper also presents ablations which suggest that shared memory is essential in the hardest settings, where agents must negotiate under weak or delayed feedback, and that the learned shared memory actually encodes inter agent coordination state, because the cosine distance between two agents’ memories shrinks before they meet in the corridor and stays aligned while they resolve who yields."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "SRMT is a clean, well motivated architectural idea. Instead of engineering what messages agents should send, or imposing a fixed-bandwidth channel, the paper proposes a shared workspace that all agents write to and read from through attention. This is conceptually similar to a blackboard system in classical distributed AI, but implemented as differentiable recurrent memory, and updated every timestep inside a transformer block with self attention on each agent’s local history and cross attention to the shared workspace. This lets the method keep decentralized execution, which is crucial in large swarms, while still allowing high bandwidth coordination, which typical CTDE or bandwidth limited communication schemes struggle with. The Bottleneck task is a nice diagnostic experiment. It isolates negotiation, forces agents to decide who yields in a single lane corridor, and stresses partial observability when the corridor is longer than the view range. SRMT achieves perfect cooperative success rate in both directional reward and sparse reward settings, where baselines like MAMBA, QPLEX, ATM, and RRNN either fail completely or only solve easier reward structures, and SRMT keeps working even when the reward gives no dense guidance along the way. The generalization test is also impressive, SRMT trained only on corridors of length 3 to 30 successfully coordinates in corridors up to length 1000 at test time, maintaining near perfect success at lengths up to 400 in the sparse reward case, and remaining the top performer in moving negative reward all the way out to 1000, which is strong evidence that it learned a transferable negotiation routine, not memorized scripts for a particular map size. The memory analysis is compelling. The paper shows that cosine distance between two agents’ memory vectors shrinks as they approach each other, stays low while they pass in the corridor, and then increases again once they are free to pursue their own goals, so the shared memory is not just a dump of observations, it is aligned with “we are currently coordinating” versus “we are independent” phases. On POGEMA, which is widely used for partially observable multi agent pathfinding, SRMT remains competitive or better than CTDE based learners like QPLEX, memory baselines like RATE and RRNN, and even planning based methods on some maps, while requiring no domain specific heuristics. The paper also pushes into lifelong multi agent pathfinding, where agents keep getting new goals instead of terminating, and shows that SRMT can be scaled to dozens of agents, then mixed population training improves further, and that adding a heuristic path decider improves congestion handling. Finally, the authors include system level measurements, steps per second and GPU memory footprint of SRMT with and without shared memory, and show roughly linear scaling in agent count in terms of runtime per step, which is important for claims about practical scalability."}, "weaknesses": {"value": "The approach still assumes an effectively unconstrained high bandwidth shared memory bus, which is conceptually decentralized, but physically looks like broadcast of latent state to everyone at every timestep. In other words, the method offloads coordination into a global attention accessible structure, which in many practical robot or wireless settings would itself be a bottleneck, so while the paper argues that SRMT maintains decentralized execution, it is really assuming reliable, synchronous, instantaneous global memory sharing, which is a strong assumption in large fleets. The paper acknowledges this only briefly in the limitations, and it would be useful to quantify how performance degrades if access to the shared memory is lossy, delayed, or spatially restricted. Almost all experiments assume homogeneous agents with identical policies and identical capabilities. This is a hard setting in terms of emergent symmetry breaking, and the paper is right to point that out, but it is also the easiest setting for weight sharing and for a shared latent memory workspace, because everyone writes embeddings in the same format. It is unclear whether SRMT would remain stable and interpretable if different agents had different roles or actuation constraints, or if some had different observation ranges. Ablations are strong for Bottleneck, but thinner for the large scale POGEMA case. For Bottleneck, the paper shows that removing the memory head, history, or shared memory hurts badly, which supports the claim that shared recurrent memory is really doing the work, not just generic recurrence. For lifelong POGEMA, the ablations are mostly variants of SRMT with or without heuristic path decider and with different training population sizes, but do not fully isolate how much of the gain comes from shared memory in the many agent regime relative to, say, just a large transformer with private memory and a good heuristic injector. Evaluation is broad in terms of environments, but still somewhat narrow in terms of baselines. The paper compares with CTDE style QPLEX and cooperative MARL methods, and with path planners like RHCR and Follower, but I did not see explicit comparisons to more recent transformer based communication architectures like SCRIMP in the same partially observable settings, or to implicit communication via learned messaging under bandwidth constraints. This leaves a small gap in the claim that SRMT is strictly better than communication learning, because the competitors either assume centralized training or use memory without sharing. Some of the large scale throughput plots are difficult to interpret without additional statistics. For example, SRMT plus heuristic planning beats others on congestion and throughput in warehouse style maps, but MAMBA still produces a higher raw throughput in that single warehouse configuration. The paper attributes this to reduced diversity of layouts in that evaluation, but a more explicit discussion of failure modes would help, for example, where SRMT’s coordination logic breaks down compared to a heuristic planner, or situations where shared memory entrains everyone into the same lane and causes traffic jams. Finally, the shared memory workspace looks a lot like a differentiable blackboard, but there is not yet a learned discipline about what to write there. It would be interesting to know if the model ever writes misleading or stale information that hurts others, or if it ever “lies” to negotiate, for example stalls another agent on purpose to grab priority. These behaviors are important for safe deployment in real multi robot systems, and are not analyzed."}, "questions": {"value": "How realistic is the shared memory assumption in real swarms. Concretely, do you assume every agent can broadcast its memory vector to every other agent at every step with no delay and no packet loss. Have you tried injecting communication delay or partial observability into the shared memory itself, for example only nearby agents’ memories are visible, and if so, how badly does performance drop on Bottleneck and POGEMA. Can you report a fairness controlled comparison on POGEMA against a bandwidth limited communication baseline, for example SCRIMP style transformer communication with a fixed message budget, so we can see if SRMT still wins when the other method is not artificially bottlenecked. In lifelong multi agent pathfinding on large maps, what is the failure mode when SRMT fails. Is it deadlock, local congestion, oscillation, or just extremely long detours. A short qualitative analysis would help clarify what the shared memory is and is not learning. In Figure 3, you show that cosine distance between agent memories decreases as agents approach each other and stays low during corridor negotiation. Can you expand this analysis to more than two agents, for example, in POGEMA warehouse style congestion, do clusters of agents locally align their memory states in the same way, does the shared memory reflect global traffic patterns like “I am blocking aisle 5, you reroute”. How sensitive is SRMT to the observation patch size and the history window h. You ablate history length, and performance collapses without history on Bottleneck under hard rewards, which is informative. Could you also report sensitivity curves for observation range in POGEMA or for the memory dimensionality in SRMT. Finally, you report steps per second and memory usage up to 1024 agents on an A100. Can you comment on whether the cross attention to the shared memory is the dominant runtime cost, and whether you explored sparse attention or locality restricted attention, which may be important for scaling beyond 1k agents or for deployment on cheaper hardware."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SHSTsVFgdu", "forum": "OiwMgMjeRz", "replyto": "OiwMgMjeRz", "signatures": ["ICLR.cc/2026/Conference/Submission24875/Reviewer_31Ta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24875/Reviewer_31Ta"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595634357, "cdate": 1761595634357, "tmdate": 1762943228708, "mdate": 1762943228708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the Partially Observable Multi-Agent Path Finding (PO-MAPF) problem by introducing a new architecture called Shared Recurrent Memory Transformer (SRMT). Instead of explicit message passing or centralized training, SRMT allows all agents to read and write to a shared recurrent memory via attention, effectively forming a global workspace for implicit communication. Each agent encodes its own history and observation through self-attention, interacts with the shared memory through cross-attention, and updates its own state recurrently. The method is evaluated on the Bottleneck coordination task and POGEMA environments, demonstrating improved coordination and scalability under sparse reward and partially observable settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem setting. The paper clearly articulates the challenges of decentralized coordination under partial observability and sparse rewards, which are highly relevant to real-world multi-robot and swarm scenarios.\n\n2. Conceptually simple, implementation-friendly. The shared-memory attention mechanism is intuitive, modular, and can be easily plugged into existing transformer-based MARL pipelines.\n\n3. Strong empirical validation. Experiments on the Bottleneck and POGEMA benchmarks are systematic, covering different reward densities, map scales, and agent counts. \n\n4. Good interpretability. The visualization of memory alignment between agents (e.g., cosine similarity peaking near encounters) provides evidence that the shared workspace learns meaningful coordination semantics."}, "weaknesses": {"value": "1. Limited novelty. SRMT mainly reconfigures known ideas (shared latent space + recurrent transformer) rather than introducing a fundamentally new learning principle.\n\n2. Restricted task diversity. All experiments are in grid-world pathfinding domains (POGEMA and its variants). There are no tests on heterogeneous agents, dynamic obstacles, or more complex continuous-control tasks, which weakens claims of generality.\n\n3. Lack of theoretical or conceptual depth. The paper positions SRMT as a general coordination mechanism, but provides no formal analysis or ablation beyond empirical metrics to support the claimed emergence of “implicit communication.”"}, "questions": {"value": "1. Since every agent attends to the shared memory of all agents, how does the method scale computationally as the number of agents increases? Have you tested SRMT in settings with hundreds of agents?\n\n2. How generalizable is SRMT to heterogeneous or continuous-control environments?\n\n3. Are the improvements over baselines primarily due to the shared-memory mechanism or to larger model capacity and richer representations?\n\n4. The “unconstrained communication” assumption may not hold in realistic multi-robot systems. How would SRMT perform under communication delay or bandwidth limits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1lgVPYJCqi", "forum": "OiwMgMjeRz", "replyto": "OiwMgMjeRz", "signatures": ["ICLR.cc/2026/Conference/Submission24875/Reviewer_MLNw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24875/Reviewer_MLNw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918040270, "cdate": 1761918040270, "tmdate": 1762943228512, "mdate": 1762943228512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a shared recurrent memory transformer (SRMT) architecture for coordination in multi-agent systems. The paper particularly focuses on the problem of cooperative path planning, in which each agent aims to find a path to its goal location, without colliding or interfering with other agents. The intuition of the SRMT is that its cross attention weights, when applied to agents’ memories, allow for implicit coordination between agents. Unlike centralized training and decentralized execution (CTDE) methods, however, SRMT allows each agent to make its action decisions individually, even in the training phase, leading to greater scalability to many agents. Experiments on a variety of pathfinding environments show that SRMT outperforms other CTDE baselines, as well as decentralized baselines that utilize memory."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Cooperative pathfinding is an important application scenario, e.g., for robotics applications, and it exemplifies many of the challenges in coordinating multi-agent systems.\n\n+ The proposed method scales well to a large number of agents, supporting up to 128 agents in simulations as shown in Table 4’s results on inference efficiency. Intuitively, the cross-attention mechanism in the SRMT should be easier to train than CTDE methods that require training over combinations of agent actions."}, "weaknesses": {"value": "--The intuition and the technical challenges behind using the SRMT for agent cooperation are not explained well in the paper, and thus it’s difficult to fully appreciate SRMT’s novelty or technical merit. The SRMT architecture seems to be a standard transformer.\n\n--It’s also not clear how the SRMT would have access to memories from other agents. Wouldn’t agents need to share these memory states in order to pass them into the SRMT? Does each agent maintain its own trained SRMT, or is a single SRMT model maintained at a central server? How is the SRMT combined with the PPO framework? Without these details, it is difficult to appreciate the proposed method.\n\n--In the experiments, the paper utilizes three different reward functions in order to showcase different aspects of cooperation. However, the choice of reward seems rather artificial as these functions are all applied to the same environments (and in general, one can choose any desired reward function in reinforcement learning, so long as it is observable). At a minimum, example settings in which each reward function would be appropriate should be given so that the rewards appear to be realistic instead of artificially constraining the evaluated methods."}, "questions": {"value": "Please see also the weaknesses above.\n\n1) What was the overall (not just per-step) training time for SRMT compared to MAMBA, and how does it scale with the number of agents?\n\n2) It’s not clear how many agents were used to obtain the experiment results in Figure 2 and Table 1. Why is comparing SRMT with 64 agents a fair comparison to SRMT with a mix of 64 and 128 agents—wouldn’t a different number of agents change the environment entirely? Or can the number of agents present in the environment change over time? How many agents were used in the baselines (MAMBA, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ghfNX90irY", "forum": "OiwMgMjeRz", "replyto": "OiwMgMjeRz", "signatures": ["ICLR.cc/2026/Conference/Submission24875/Reviewer_Vgu5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24875/Reviewer_Vgu5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146897725, "cdate": 1762146897725, "tmdate": 1762943228284, "mdate": 1762943228284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}