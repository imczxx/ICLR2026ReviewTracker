{"id": "1m4cKCr0vx", "number": 16374, "cdate": 1758263858665, "mdate": 1759897244967, "content": {"title": "Scaling Laws for Parameter Pruning in LLMs", "abstract": "Scaling up model parameters and training data consistently improves the performance of large language models (LLMs), but at the cost of rapidly growing memory and compute requirements, which makes deployment on resource-limited hardware infeasible. *Model pruning*, a widely used compression technique, reduces inference costs by removing redundant parameters. However, its impact on downstream performance remains unpredictable and is typically assessed only through costly empirical sweeps. To address this gap, we introduce *pruning laws* -- simple and interpretable scaling relations that connect a pruned LLM's post-pruning performance to its unpruned performance and pruning ratio. Across five LLMs (2.7B–13B parameters), three pruning strategies (unstructured, width, and depth), and eight diverse tasks, we show that pruning laws achieve strong predictive accuracy (average extrapolation error $<7$%), reliably quantify performance degradation, and identify critical pruning thresholds beyond which recovery is infeasible. Moreover, we demonstrate that the same laws transfer universally across architectures, pruning methods, and even unseen models in zero-shot and one-shot setups. These results provide both researchers and practitioners with a principled framework to select pruning strategies, estimate safe pruning ratios without exhaustive tuning, and deploy LLMs efficiently under real-world compute and latency constraints.", "tldr": "", "keywords": ["Scaling laws", "Model Compression", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e9f9fa8945c3770c4609d04f501858ee8c61861.pdf", "supplementary_material": "/attachment/b1eda28118c0638c8606fdba8e19988fb3d5af95.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces pruning laws, which are analytical scaling relationships that model how a large language model’s (LLM’s) performance degrades as parameters are pruned. The authors propose a simple power-law formulation $L = L_0 P_0 (1-r)^{\\alpha}$ linking post-pruning performance $L$ to base performance $L_0$ and pruning ratio $r$, with fitted parameters $\\alpha$ and $P$."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments are extensive, covering multiple architectures (OPT, LLaMA, Phi-3), pruning strategies, and task types. The universality and cross-method transfer analyses (zero-shot and one-shot setups) convincingly demonstrate robustness.\n2. The proposed formulation enables practitioners to estimate safe pruning ratios without retraining or fine-tuning.\n3. The derivation of the pruning law, its logarithmic linearization, and the OLS-based fitting procedure are clearly articulated."}, "weaknesses": {"value": "1. The main concern with developing such a scaling law is that post-pruning evaluation is relatively inexpensive compared to pre-training. It is therefore unclear why a separate scaling law is necessary to model post-pruning performance, given that pruning results can typically be obtained within minutes. In addition, AutoML techniques can be employed to efficiently narrow the search space and identify optimal pruning strategies.\n2. The study explicitly avoids recovery fine-tuning. While this isolates pruning effects, it limits applicability, since fine-tuning is standard practice for high-quality pruned models. A section quantifying how fine-tuning interacts with the law would be valuable.\n3. The proposed pruning method demonstrates limited effectiveness when applied to LLaMA-3.2 and Qwen-3 architectures. In recent work, pruning techniques are often integrated directly into the training process to achieve better efficiency and stability.\n4. There are several existing papers on scaling laws for inference-efficient models; it would be helpful if the authors discussed how their approach differs from these works: (1) https://arxiv.org/abs/2401.00448 (2) https://arxiv.org/abs/2501.18107"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jdA6pzse1q", "forum": "1m4cKCr0vx", "replyto": "1m4cKCr0vx", "signatures": ["ICLR.cc/2026/Conference/Submission16374/Reviewer_xFt1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16374/Reviewer_xFt1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760920528348, "cdate": 1760920528348, "tmdate": 1762926496793, "mdate": 1762926496793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces pruning laws, a framework to predict the performance of a pruned Large Language Model (LLM) based on its original performance and the pruning ratio. The authors propose a simple power-law that connects the post-pruning performance to the base model's performance and the pruning ratio. \n\nThe author empirically demonstrates that the law can be applied to newer model architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written, and the experiment results are comprehensive.\n2. The authors are commendable in that they reported negative results along with positive ones."}, "weaknesses": {"value": "1. It's not clear whether the power law is the best curve to fit for this problem. Figure 3 shows many tasks that don't seem to fit well at all. The authors did not discuss other curve-fitting options.\n\n2. Even if we ignore the point above and assume that the power law is a good fit for OPT and LLaMA, there is not enough evidence that the curve holds for modern massively overtrained LLMs. It's quite difficult to believe that these laws would hold for massively overtrained LLMs (e.g., the latest LLaMA/Qwen/Gemma, etc) without incorporating training data as a factor. The intuition is that the more overtrained the models, the less sparsity there is in the model, and the harder it is to prune the models without degrading the downstream tasks' performance significantly. They could behave completely differently compared to older model generations. One experiment on OOD model extrapolation is not nearly sufficient to show why these laws would work on modern models. \n\n3. Width-pruning and unstructured-pruning provide much less real-world benefit than depth-pruning (due to difficulty in converting to wall-clock latency gain). The utility of these scaling laws is diminished if they don't hold for depth-pruning.\n\n4. The paper's interpretation of P_0 is problematic when it's greater than 1. Does that mean pruning a small /epsilon would improve performance over the baseline?\n\n5. It's commendable that the authors show many negative results, but it would be of real scientific value if the authors could explain the failure modes and what the failures reveal about the limitations of the scaling laws."}, "questions": {"value": "1. Pruned models often go through a recovery fine-tuning stage to recover performance on downstream tasks. How would that affect the scaling law? \n\n2. How is the latency measured? The paper lacks critical details in the measurement setup.\n\n3. Why does the law fail on knowledge-intensive tasks such as MMLU?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RxWNpJCnFM", "forum": "1m4cKCr0vx", "replyto": "1m4cKCr0vx", "signatures": ["ICLR.cc/2026/Conference/Submission16374/Reviewer_rFqp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16374/Reviewer_rFqp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761015743093, "cdate": 1761015743093, "tmdate": 1762926496527, "mdate": 1762926496527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simple, empirical power-law relations to predict a pruned LLM's performance (L) based on its original score (L0)\nand the pruning ratio (r). The model is fitted using five LLM families (2.7B–13B), three pruning strategies, and eight tasks.\n\nWhile the evaluation is empirically broad and the authors claim strong universality with average extrapolation error below 7%, the contribution is fundamentally a descriptive curve-fitting exercise. While practically useful, the analytical justification is minimal."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses the important and under-studied problem of predicting pruning behavior. The evaluation is reasonably extensive, including multiple architectures, pruning methods, tasks, and metrics. The resultant model is simple with large application potential."}, "weaknesses": {"value": "The law is essentially obtained by empirical curve-fitting exercise using power-law regression on known monotonic degradation trends, with little theoretical justification. Despite claims of strong universality, actual errors could be high, especially given the reasonable but still small size of the chosen downstream tasks tested. The robustness of the transferability test is limited, raising concerns of overfitting.\n\nExcluding recovery fine-tuning may bias the performance results downward and distort real-world pruning behavior."}, "questions": {"value": "+ How sensitive are the two fitted parameters to dataset choice, metric noise, and random seeds? \n\n+ Why does the \"one-shot calibration\" process sometimes worsen performance? Does this indicate potential overfitting? \n\n+ Would the same α coefficient remain applicable to models that have undergone recovery fine-tuning? \n\n+ How does the pruning law perform when extrapolated beyond 90% pruning—does it accurately predict model collapse or divergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0hklYJuUKF", "forum": "1m4cKCr0vx", "replyto": "1m4cKCr0vx", "signatures": ["ICLR.cc/2026/Conference/Submission16374/Reviewer_Rm4J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16374/Reviewer_Rm4J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381248600, "cdate": 1761381248600, "tmdate": 1762926496166, "mdate": 1762926496166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces “Pruning Laws”: a concise, interpretable power-law that directly relates the downstream performance of a pruned LLM to its un-pruned performance and the pruning ratio r. While scaling parameters and data jointly boosts accuracy, the accompanying memory/compute explosion makes deployment on resource-constrainedhardware prohibitive. Model pruning is a popular remedy, yet its impact on downstream tasks has remained unpredictable and costly to assess. Over 5 models, 3 pruning granularities  and 8 tasks (reasoning, QA, language modeling) show that Pruning Laws\n•\tpredict post-prune accuracy with <7 % mean extrapolation error and ≤8 % error when zero-shot transferred to unseen models (LLaMA-3.1, Phi-3) or algorithms (SlimGPT, SVD-LLM);\n•\tforetell the critical pruning threshold beyond which performance collapses, eliminating expensive grid-search;\n•\tquantify task sensitivity: reasoning is most robust, QA most fragile; depth pruning yields 5× speed-up but high variance, unstructured keeps accuracy yet almost no speed-up, width sits in between;\n•\tdeliver actionable guidelines for choosing method and ratio under any task or budget, enabling zero-shot or single-point-calibrated deployment.\nPruning Laws thus provide a principled, universally applicable framework for compressing and deploying LLMs without full re-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe experiments are large-scale, detailed, and sufficient; all reproduction materials are fully open-sourced.\n2.\tThe paper attempts to uncover a law governing the trade-off between model pruning and performance, offering both theoretical and practical value.\n3.\tThe proposed formula is concise, intuitive, and easy to use."}, "weaknesses": {"value": "1.\tThe proposed formula is largely empirical and lacks solid theoretical justification.\n2.\tThe experiments show that the parameters P₀ and α depend on the specific model, pruning method, and task; their determination in practice remains highly empirical.\n3.\tThe law fails to fit knowledge-extraction tasks such as MMLU, and it is still unclear whether it generalizes to more complex scenarios like mathematics, code generation, or multimodal applications."}, "questions": {"value": "1.\tThe explanation of the proposed formula is largely empirical. Could you further investigate, from a theoretical perspective and based on the architecture of large models, why the pruning law can be modeled as such a power-law relationship? Have you attempted to fit the data with other functional forms?\n2.\tP₀ and α are conditioned on the specific model, pruning method, and task. Could you elaborate on the exact procedure used to derive their values for a given task in the experiments?\n3.\tThe pruning scaling law performs well on models ranging from 2.7B to 13B parameters. Does it still hold for much smaller models (e.g., below 1B) or much larger ones (e.g., above 100B)?\n4.\tIn the paper, recovery fine-tuning is deliberately excluded to isolate the effect of pruning itself. However, in practical applications, recovery fine-tuning is a common step. If recovery fine-tuning is introduced, will the pruning scaling law still hold? Would it be necessary to modify or extend the law?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vJK9EBzvOB", "forum": "1m4cKCr0vx", "replyto": "1m4cKCr0vx", "signatures": ["ICLR.cc/2026/Conference/Submission16374/Reviewer_39HH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16374/Reviewer_39HH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708715640, "cdate": 1761708715640, "tmdate": 1762926495708, "mdate": 1762926495708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Meta Rebuttal by Authors"}, "comment": {"value": "We thank the reviewers for meticulously evaluating our paper and providing additional feedbacks. Below we summarize the core clarifications and new evidence added in response to reviewer concerns. We hope that the responses improve the quality of our paper. We also hope to see a fairer evaluation that our paper deserves. \n\n**(1) Theory:**  We clarified that the pruning law follows directly from:\n\n- **Iterative compositionality of pruning**, which yields the functional equation $f(x_1x_2)=f(x_1)f(x_2)$; its only continuous solution is a **power law** in the retention factor $(1-r)$.  \n- **Capacity scaling in Transformers**, where pruning reduces effective capacity multiplicatively. Classical pretraining scaling laws (Kaplan; Hoffmann) then imply $L = L_0 P_0 (1-r)^\\alpha$.\n\n**(2) Parameter Stability:**  Across all tables, both coefficients $\\alpha$ and $P_0$ remain **highly stable**, with typical standard deviations:\n- $\\alpha$: **±0.02–0.13** at task-level and **±0.01–0.09** at model–task level,  \n- $P_0$: **±0.02–0.14**.  \nThis demonstrates robustness to dataset choice, metric noise, and pruning randomness.\n\n**(3) Functional Form Validation:**  We compared 8 alternative functional forms; our proposed form achieves the **lowest train (0.02)** and **lowest test error (0.05)**, confirming it is the best predictor among widely used decay families.\n\n**(4) Generality Across Models and RFT:**  With post-pruning recovery fine-tuning (LoRA), the law continues to fit strongly (adj. $R^2$ = 0.84–0.96; test error 0.01–0.05).  On new and extreme-scale models (OPT-125M, OPT-30B, LLaMA-3.2-3B/8B, Phi-3-mini), test errors remain **0.04–0.10**, reinforcing cross-architecture robustness.\n\n**(5) Task Boundaries:**  The law holds reliably for **reasoning, language, and QA-like tasks**. Knowledge-extraction tasks (e.g., MMLU) violate smooth capacity assumptions, consistent with prior pruning literature and we clearly state this boundary.\n\n**(6) Practical Utility:**  Although single pruning evaluations are cheap, the **full pruning search space is large** (model × task × method × ratio).  \n\nOur law enables:\n- **Zero-shot and one-shot prediction**,  \n- Accurate extrapolation from **3–5 data points**,  \n- Immediate estimation of model/task elasticity.  \n\nThe scaling law significantly reduces the need for brute-force or AutoML-style exploration.\n\n**(7) Interpretation of Parameters:**  We clarified that:\n- $\\alpha$ captures task/model **elasticity**,  \n- $P_0$ is a **normalization factor**, not performance at zero pruning (so $P_0>1$ is benign).\n\n**In summary**, we provide:\n- A **theoretically justified** functional form,  \n- **Highly stable parameters** across models/methods,  \n- **Reliable predictive accuracy**,  \n- **Robustness with and without recovery fine-tuning**,  \n- **Generalization to new architectures and scales**,  \nmaking this the first comprehensive **post-training pruning scaling law** for LLMs."}}, "id": "EGJICZhA68", "forum": "1m4cKCr0vx", "replyto": "1m4cKCr0vx", "signatures": ["ICLR.cc/2026/Conference/Submission16374/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16374/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission16374/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763611734723, "cdate": 1763611734723, "tmdate": 1763611734723, "mdate": 1763611734723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}