{"id": "xgdPfzy9Ww", "number": 2526, "cdate": 1757134989927, "mdate": 1759898142933, "content": {"title": "SurgRe4DGS: Open-Vocabulary Vision-Language Gaussian Splatting for Retrievable 4D Surgical Scene Reconstruction", "abstract": "Reconstructing dynamic 4D models from surgical workflows is crucial for robotic-assisted surgery, real-time navigation, and postoperative analysis. However, real-world challenges encountered during surgery, such as tissue deformations and occlusions, significantly increase the difficulty of reconstruction and semantic perception. In this work, we introduce SurgRe4DGS, a novel framework that jointly realizes 4D surgical scene reconstruction and open-vocabulary instrument retrieval through synergistic vision–language Gaussian representations. Particularly, we leverage visual clues to enhance color representations through spatial-color blending with temporal modulation, while language features are instilled with hybrid injection strategies via semantic map rendering and weighted top-K aggregation with contrastive learning. With a dual-branch mechanism for vision-language context integration, SurgRe4DGS ensures precise cross-modal alignment and semantic consistency in deformable scenarios. To fill the gap in existing benchmarks for language-driven multimodal 4D reconstruction and open-vocabulary instrument retrieval, we introduce StereoLung15K, a pioneering surgical dataset with 26 binocular thoracoscopic sequences, over 15,000 annotated frames, and multimodal (RGB/depth/text) support at 30 FPS, enabling joint reconstruction and text-guided retrieval tasks. Extensive experiments on two public datasets, StereoMIS and EndoNerf, as well as our StereoLung15K, demonstrate that SurgRe4DGS achieves state-of-the-art performance in terms of reconstruction fidelity and retrieval precision. \nOur code and data will be publicly available.", "tldr": "", "keywords": ["4D Surgical Scene Reconstruction", "Surgical Instrument Retrieval", "4D Gaussian Splatting", "Vision-language"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33ccefd8692c0f795cf40c03090ae7bb228877d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscripts presents a method for reconstructing dynamic surgical scenes for open-vocabulary retrieval tasks using a synergistic vision-language Gaussian representation, called SurgRe4DGS. This is potentially important for robotic-assisted surgery, where understanding the tool-to-tissue interactions is crucial for potental applications such as automation, training, and guidance. The method uses spatial color blending with temporal modulation to enhance color represenations, while language features depend on instrument-wise segmentation and interpretation by a multi-modal large language model (MLLM), with text encoding of the resultant description being used as the language features. This enables query-time retrieval of specific surgical tools in the 4D scene as well as novel view synthesis. The method is evaluated on StereoMIS, EndoNerf, and a novel bronchoscopy dataset, StereoLung15K, with results showing improved performance over prior methods in terms of novel view quality (PSNR, SSIM, LPIPS, etc) and open vocabulary instrument retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "\\- The paper is well-written and detailed, with a detailed explanation of Gaussian splatting prerequisites, the proposed method, and the experimental setup.  \n\\- The novel dataset, StereoLung15K, is a valuable contribution to the field, providing a new benchmark for evaluating 4D reconstruction and retrieval methods in bronchoscopy scenarios.  \n\\- The method outperforms prior methods for novel view synthesis on multiple datasets, including a new bronchoscopy dataset introduced in the paper, achieving a PSNR gain of 2 \\- 3 % over the strongest baseline, for example.  \n\\- For instrument retrieval, the method performs significantly better than related methods in terms of mIoU, mDice, and mRecall, especially on StereoLung15kK and StereoMIS. On StereoLung15K, for example, the mIoU is improved from 49.9 to 57.9."}, "weaknesses": {"value": "- The \"open-vocabulary\" retrieval is limited to surgical tools only, considerably narrowing the scope and potential applications of the method. Any surgical robot can automatically determine the tools being used in the procedure based the electronic tags associated with the instruments, making it a simple matter to retrieve tool segmentations in 2D images using the frozen tool models mentioned in the paper and associate them with 3D locations using available 4D reconstruction methods. The real value of open-vocabulary retrieval would be the ability to retrieve instruments and tissue types both.\n- Relatedly, the use of 4DGS in the context of RAS is poorly motivated. The abstract claims that 4D reconstruction of surgical workflows is **crucial** for robot-assisted surgery, but this is clearly not the case, because current surgical robots operate without any 4D reconstruction capabilities. Intra-operative augmented reality applications, as mentioned in the introduction, may benefit from the open-vocabulary retrieval and novel view synthesis capabilities, but the connection between \"capturing tissue deformation, instrument motion,...\" and any practical application is not made clear, especially given the limits of the method to surgical tools. What specific augmented reality application would benefit from the ability to retrieve surgical tools in 4D space? The surgeon can clearly see where the tools are without any AR assistance; rather, the location of anatomical features would be more useful.\n- The approach has limited novelty compared to prior work, particularly 4D LangSplat [1], which also uses MLLMs to generate language features for Gaussian splatting-based 4D reconstruction.\n\n1. Li, Wanhua, et al. \"4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models.\" arXiv, 13 Mar. 2025, doi:10.48550/arXiv.2503.10437."}, "questions": {"value": "1\\. How well does the method generalize to surgical tools not seen during training? Can the authors provide quantitative results on this aspect?  \n2\\. Given the limited number of surgical tools typically used in RAS procedures, what is the practical advantage of open-vocabulary retrieval of surgical tools compared to using known tool models to identify and retrieve them in 4D space?  \n3\\. Can the authors clarify the specific clinical applications that would benefit from the proposed method, especially considering its limitation to surgical tools only?  \n4\\. Can the authors ensure that the dataset (StereoLung15K) will be made public?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KtaHAgGOOH", "forum": "xgdPfzy9Ww", "replyto": "xgdPfzy9Ww", "signatures": ["ICLR.cc/2026/Conference/Submission2526/Reviewer_ezT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2526/Reviewer_ezT5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761230283544, "cdate": 1761230283544, "tmdate": 1762916267978, "mdate": 1762916267978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SurgRe4DGS, a vision-language framework for retrievable 4D surgical scene reconstruction using Gaussian Splatting. It introduces a dual-branch architecture combining visual priors and language semantics via MLLM-guided embeddings to enable open-vocabulary instrument retrieval alongside dynamic reconstruction. The authors also present StereoLung15K, a new high-frame-rate multimodal dataset with RGB-D-text annotations. Experiments show strong gains over recent 4DGS and retrieval baselines. I firmly urge the authors to release the dataset publicly to ensure fair evaluation and to benefit the community. I think this paper presents a strong idea, a solid framework, but incomplete execution. I still maintain a positive attitude toward this paper, provided the authors can demonstrate strong reproducibility, as this work has the potential to contribute significantly to this field"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces unified vision-language Gaussian splatting for 4D surgery. and it outperforms baselines across 4D reconstruction and retrieval tasks. Another contribution is presenting StereoLung15K: a multimodal, densely annotated surgical dataset. This paper also provides thorough ablation on architectural components and semantic injection, which is appreciated."}, "weaknesses": {"value": "Though the authors present substantial experiments, several issues raise concern:\n1. No video results or temporal visualizations undermine the 4D claims, and no code or dataset is released, contradicting reproducibility expectations. I strongly encourage any paper in this field to provide reproducible code and substantial video comparisons to ensure a fair evaluation.\n2. Experiments use only a few sequences per dataset; robustness across diverse surgical conditions, unseen instruments, or more complex anatomy remains unproven.\n3. No runtime or inference latency is reported, and the pipeline involves high computational overhead: it depends on a large vision model (SAM), a multi-modal language model (MLLM), Gaussian rendering(this is fast though), and contrastive training. As presented, this appears to be an offline process requiring significant compute. It is unclear whether the system is usable for real-time or even near-time surgical applications.\n4.The method relies on pre-calibrated camera poses, yet makes no reference to surgical SLAM systems (e.g., EndoSLAM, EndoGSLAM), which directly address pose estimation in deformable surgical scenes. These are foundational to the assumptions of this paper and must be cited.\n5.Although the method claims open-vocabulary capability, the model is trained only on structured, attribute-based annotations from the StereoLung15K dataset (instrument type, action, position). It remains unclear how well the system generalizes to unseen or natural-language queries, limiting the demonstrated scope of retrieval.\n6. The evaluation focuses on reconstruction quality and retrieval accuracy but omits practical metrics like retrieval latency or long-term temporal stability. These are critical for assessing real-world surgical utility."}, "questions": {"value": "1. Why are there no videos or visualizations of reconstructed sequences?\n1. Will the authors release code, StereoLung15K, and trained models?\n1. How does the system generalize to unseen surgical instruments or phrasing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b8KJf54PDv", "forum": "xgdPfzy9Ww", "replyto": "xgdPfzy9Ww", "signatures": ["ICLR.cc/2026/Conference/Submission2526/Reviewer_ADhH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2526/Reviewer_ADhH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761480831272, "cdate": 1761480831272, "tmdate": 1762916267815, "mdate": 1762916267815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SurgRe4DGS, a new method for creating 4D models of surgical scenes from videos. The main idea is to combine 4D scene modeling using 3D Gaussian Splatting with language features. This allows the system to not only reconstruct the 3D scene as it changes over time, but also allows users to find specific surgical instruments using text-based search queries (e.g., \"find the grasper on the right bottom\"). A key contribution is the introduction of a new, large-scale dataset called StereoLung15K, which was created specifically for this combined task of 4D reconstruction and text-based retrieval. The authors show that their method performs better than existing methods on this new dataset and two other public datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive Experimental Validation:** The paper's primary strength is its thorough experimental evaluation. The authors validate their method on three different datasets, StereoLung15K, StereoMIS, and EndoNerf. They compare against a large number of existing methods, including 11 baselines for reconstruction in Table 2, and 3 baselines for retrieval in Table 3, and demonstrate state-of-the-art results across all of them.\n2. **Extensive Ablation Studies:** The authors provide a strong set of ablation studies in Sec 4.3 and Appendix E that clearly justify their design choices. They systematically test the impact of their spatial structural prior, the hybrid semantics injection, and even other parameters like top-K and the choice of language model. This makes the experimental results very convincing.\n3. **Dataset Contribution:** The introduction of the StereoLung15K dataset is a major contribution to the field. As shown in Table 1, this appears to be the first large-scale dataset that provides annotations for both 4D reconstruction and text-driven tool retrieval in a surgical setting. This new resource will be valuable for future research."}, "weaknesses": {"value": "1. **High Methodological Complexity:** The proposed method in Section 3 is very complex and combines many advanced techniques. It uses an MLLM to enrich text descriptions, a dual-branch mechanism, a hybrid semantic injection scheme, and contrastive learning. From the perspective of a non-expert (for me), it is difficult to follow the precise interactions between all these parts and pinpoint the most critical component responsible for the performance gain.\n2. **Generalizability:** The new dataset focuses specifically on lung surgery (thoracoscopic procedures). While the method is also tested on other endoscopic datasets, it is unclear how well this approach would generalize to other, visually different types of surgery (e.g., open-field surgery) or to non-medical dynamic scenes."}, "questions": {"value": "1. Could the authors provide a simplified, high-level diagram or explanation of how the visual features from SAM and the language features from the MLLM are combined in the \"unified latent vector\" and how this vector is used during retrieval?\n2. The performance on the new StereoLung15K dataset is excellent. Do the authors believe this method is ready for clinical application, or what are the main practical barriers (e.g., computational speed, robustness) that still need to be addressed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper includes a clear Ethical Statement and properly addresses data anonymization. They also include a statement on LLM usage at Appendix H, which is commendable."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YmUExcNNgb", "forum": "xgdPfzy9Ww", "replyto": "xgdPfzy9Ww", "signatures": ["ICLR.cc/2026/Conference/Submission2526/Reviewer_2V1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2526/Reviewer_2V1t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813642692, "cdate": 1761813642692, "tmdate": 1762916267575, "mdate": 1762916267575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce SurgRe4DGS, a novel framework that jointly realizes 4D surgical scene reconstruction and open-vocabulary instrument retrieval through synergistic vision–language Gaussian representations. Particularly, they leverage visual clues to enhance color representations through spatial-color blending with temporal modulation, while language features are instilled with hybrid injection strategies via semantic map rendering and weighted top-K aggregation with contrastive learning. Extensive experiments on two public datasets, StereoMIS and EndoNerf, as well as the custom dataset StereoLung15K, demonstrate thatSurgRe4DGS achieves state-of-the-art performance in terms of reconstruction fidelity and retrieval precision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper conduct extensive experiments, and the results are promising and show some improvement.\n2. The supplementary section is helpful.\n3. The research gap is addressed well."}, "weaknesses": {"value": "1. I don't see the significant testing of this paper, and I won't be able to tell how significant these results are compare with current SOTA methods without the rigorous testing. I would recommend to perform such testings for improving the soundness of this paper.\n2. Figure 3 is unnecessary.\n3. Figure 4 is really cluttered and complicated with a lot of terms and parameters. I would expect the authors to do more through explanation for this paper instead of just summarizing it in couple of sentences.\n5. The figure wrapped inside of the text is really hard to read and I believe it should be separated from the text.\n6. Is there any failure cases (figures)? Why they are not being included in the paper for comparison and discussion?\n7. Could the author make a separate table for the detailed meaning of the parameters used in the equation? It is hard to asset the accuracy/soundness of the provided equations without referring to a table for clarity since there are a lot of them needed to be tracked."}, "questions": {"value": "The equations given are primarily adopted from prior arts, and can the authors specify what is the innovation part (either preprocessing or just a proof of the concept) in terms of the mathematical expression?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PDbl28zLFd", "forum": "xgdPfzy9Ww", "replyto": "xgdPfzy9Ww", "signatures": ["ICLR.cc/2026/Conference/Submission2526/Reviewer_W7vJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2526/Reviewer_W7vJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106444122, "cdate": 1762106444122, "tmdate": 1762916267269, "mdate": 1762916267269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}