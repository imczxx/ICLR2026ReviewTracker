{"id": "qUeA0BYyAk", "number": 4667, "cdate": 1757740177657, "mdate": 1759898020896, "content": {"title": "Know but can't Say: Exploring the Hidden Knowledge of Large Vision-Language Models for Fine-grained Perception", "abstract": "Fine-grained perception is everywhere in the real world, but it is a challenging task for Large Vision-Language Models (LVLMs), although they have shown remarkable generalization capability.\nHow to enhance the fine-grained perception of LVLMs, achieving generalizable fine-grained perception, has become a critical research problem. \nIn this paper, we focus on Fine-Grained Visual Classification (FGVC), a representative task of fine-grained perception. \nMainstream views attribute the poor performances to the absence of relevant knowledge, such as the appearance of a specific fine-grained category, and fine-tune LVLMs with fine-grained annotated datasets. \nHowever, due to the limited scale of datasets, these approaches face the risk of overfitting, degrading the generalization capability of LVLMs. \nWe find out that LVLMs have already been equipped with the capabilities of FGVC, which is not reflected in the generated responses.\nWe refer to this phenomenon as hidden knowledge, i.e., the model knows the answer, but cannot say it.\nThe existence of hidden knowledge is verified by probing techniques on LVLMs' hidden states, which reveals a gap between the internal knowledge in parameters and the external knowledge in responses. \nFurthermore, our probing technique discovers a generalizable, domain-invariant pattern. By leveraging this pattern, we improve the accuracy on FGVC without using annotated data of the target domain. \nThis improvement indicates that unleashing the hidden knowledge of LVLMs can help achieve generalizable fine-grained perception.", "tldr": "", "keywords": ["Fine-grained Perception", "Hidden Knowledge", "Large Vision-Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bbfafcd14ec28d7625f22d7860231d80977f1e34.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the task of Fine-Grained Visual Classification (FGVC) for Large Vision-Language Models (LVLMs).\n\nThe authors empirically found that LVLMs have already learned the capabilities of FGVC, yet it cannot fully leverage such knowledge to give the correct categorical responses. This exploration is interesting and can provide a good guide for future research.\n\nLeveraging the generalizibale, domain-invariant pattern discovered from the above observation, the authors proposed techniques to improve FGVC performance of LVLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This manuscript is generally well-written.\n- The experiments on verifying the existence hidden knowledge about fine-grained concepts of LVLMs are interesting and novel in the field of FGVC.\n- The experiments with different probing techniques are extensive.\n- The proposed technique is validated across 5 datasets and 4 LVLMs, which is sufficient.\n- The proposed technique demonstrates domain generalization without using fine-grained annotations."}, "weaknesses": {"value": "Primary weakness:\n- **Lack of Comparison with highly related methods:** The most related work / method to this work are FineR [1] and Finedefics [2]. First of all, FineR [1] is training-free, zero-shot and compatible with all LVLMs, using attributes obtained from LLMs to improve FGVC of LVLMs; Thus, a comparison with FineR is necessary to fairly benchmark the performance and effectiveness of the technique proposed in this work. Second, although Finedefics [2] fine-tunes LVLMs on pseudo-labelled fine-grained data using FineR, a comparison with Finedefics is necessary under a controlled and adapted experimental setting.\n\n- **Data Leakage Issue:** The whole study and experiments conducted in this work are on well-established, long-stand FGVC datasets. These datasets, of course, are used during the training/pre-training/fine-tuning of LVLMs and also different components inside these models (e.g., CLIP encoders). Therefore, the test data, probing data, are actually not unseen, novel data for LVLMs. This makes the observation and claim of \"domain generalization without using fine-grained annotations\" unclear and unsubstantiated, because the fine-grained annotations were actually used in the original LVLMs training already. The reviewer wonders, will the claims and observations of this work still hold on truly unseen/novel images contain fine-grained concepts? It is important to verify this. It does not have to be a large-scale experiments. Probably evaluation on a few hundred or a thousand pictures will suffice.\n\n- **Vague Presentation:** To understand Figure 1, one should read the entire manuscript to get all meanings of the terms used in Figure 1 within the context of this work. This is not a a good practice. A figure appears in Intro should be intuitive and clear by itself.\n\n[1] Liu, M., Roy, S., Li, W., Zhong, Z., Sebe, N., & Ricci, E. (2024). Democratizing fine-grained visual recognition with large language models. In ICLR, 2024.\n\n[2] He, H., Li, G., Geng, Z., Xu, J., & Peng, Y. (2025). Analyzing and boosting the power of fine-grained visual recognition for multi-modal large language models. In ICLR 2025."}, "questions": {"value": "Minor questions:\n- The reviewer wonders, for the multiple-choice FGVC Q&As, what will happen if the condidate choices contain not only false answers under the same fine granularity, instead, containing actually \"correct\" answers but in coarser granularity? **For example: A. Bird / B. Black Footed Albatross / C. Albatross / D. Laysan Albatross** Will this completely change the observation?\n- Please try to organize the section structure better. The current organization is hard for the readers to understand the primary contributions of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ToCgVsgwsI", "forum": "qUeA0BYyAk", "replyto": "qUeA0BYyAk", "signatures": ["ICLR.cc/2026/Conference/Submission4667/Reviewer_bFuN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4667/Reviewer_bFuN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760691390490, "cdate": 1760691390490, "tmdate": 1762917503124, "mdate": 1762917503124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the poor performance of Large Vision-Language Models on fine-grained visual classification (FGVC) tasks. The authors propose a \"hidden knowledge\" hypothesis, arguing that LVLMs do possess the necessary knowledge in their internal hidden states but fail to express it in their final output. To demonstrate this, they introduce a \"verification probe\" trained to extract this internal knowledge from hidden states. They show that this probe's accuracy on a True/False verification task consistently surpasses the model's direct \"external\" answer. Crucially, they find this internal representation of \"correctness\" generalizes across domains. Leveraging this discovery, the authors apply the probe to evaluate options in multiple-choice question tasks, showing that this method improves performance over the baseline model's direct answers across four LVLMs and five FGVC datasets. The paper concludes that the bottleneck for LVLMs in FGVC is a failure of knowledge extraction or alignment, not a fundamental lack of knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's starting point (encoder 70% vs. full model 10%) is highly persuasive and clearly identifies an important problem worthy of study.\n2. The paper's strongest contribution is revealing that a generalizable, cross-domain \"correctness\" representation exists internally in LVLMs. The cross-domain experiments in Figure 3 and the t-SNE visualizations strongly support this finding. This suggests the model's deeper layers are indeed distinguishing the abstract concept of \"correct\" vs. \"incorrect,\" not just different domains.\n3. The paper validates its findings across 4 different LVLMs and 5 FGVC datasets, increasing the credibility of the results."}, "weaknesses": {"value": "1. The paper's core argument for a knowledge \"gap\" (Figure 3) is based on an unfair comparison. It compares the accuracy of a supervised probe (trained on \"True/False\" labels) with the zero-shot accuracy of the LVLM (which was not trained on this task). A supervised classifier will almost always outperform a zero-shot one. This \"gap\" may therefore just be an artifact of the (supervised vs. zero-shot) evaluation setup, not a true measure of \"hidden\" vs. \"external\" knowledge.\n2. The paper does not explicitly state whether the probes used in Table 1 were trained in-domain or cross-domain. This is essential information for evaluating the paper's central claim. If the probe was trained in-domain (e.g., trained on \"Aircraft\" data to test on \"Aircraft\"), the entire premise of leveraging generalizable knowledge collapses, and it becomes a simple supervised method.\n3. The proposed solution for MCQs requires K separate forward passes for a K-option question (one for each Question + Option_i). This results in at least a K-fold increase in inference cost compared to the baseline model. This is a very significant practical limitation that the paper completely fails to mention, making the performance gains appear computationally \"free\" when they are not."}, "questions": {"value": "Please respond to the weaknesses I mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EzRWDVLFuJ", "forum": "qUeA0BYyAk", "replyto": "qUeA0BYyAk", "signatures": ["ICLR.cc/2026/Conference/Submission4667/Reviewer_ZPRL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4667/Reviewer_ZPRL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917429616, "cdate": 1761917429616, "tmdate": 1762917502315, "mdate": 1762917502315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to enhance fine-grained perception in Large Vision-Language Models (LVLMs), focusing on Fine-Grained Visual Classification (FGVC) as a representative task. The authors argue that LVLMs already possess fine-grained visual knowledge internally, but this knowledge is not expressed in outputs—termed “hidden knowledge” (“the model knows the answer but cannot say it”). Using probing techniques to reveal and leverage this hidden knowledge, they discover domain-invariant patterns that improve FGVC accuracy without using annotated data. The work suggests that unleashing hidden knowledge can improve the generalization ability of LVLMs on fine-grained perception tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This paper introduces a probing method to verify the existence of hidden knowledge in LVLMs related to fine-grained perception.\n\n- Through experiments, the authors show that the intermediate hidden states contain fine-grained perceptual knowledge, but this capability is lost during response generation, as revealed by their probing method."}, "weaknesses": {"value": "- While I agree that understanding the visual perception capability of LVLMs is highly important, the proposed probing method itself is not particularly novel or interesting. Although from a different domain, similar probing techniques have already been widely explored in the LLM community [1], along with approaches such as LogitLens [2] and PatchScope [3], which aim to interpret the hidden states of LLMs. The proposed probing method focuses on training a binary classifier using the hidden states of the LLM itself, rather than probing the vision encoder, cross-modal hidden states, or attention maps, making it conceptually similar to or derivative of these prior works.\n\n- The terminology of “external knowledge” in the paper was somewhat confusing. Since both “internal” and “external” knowledge rely on the same intermediate representations within the LVLM, the term “external” feels somewhat inappropriate or ambiguous.\n\n- To convincingly claim that “the model possesses internal knowledge but lacks external knowledge,” a simple classification accuracy comparison may not be sufficient. Without a more mechanistic analysis of why the model fails to generate correct answers at the final stage (e.g., from the LLM’s last layer to the LM head), it is difficult to conclude that the observed phenomenon has been thoroughly explained.\n\n- I also find it difficult to assess whether the proposed probing method is truly reliable. In the current setup, the model is given a question and an answer and is then asked to predict “Yes” or “No.” This raises doubts about whether the trained classifier genuinely detects the presence of hidden knowledge, or whether it is simply overfitting to the Yes/No classification task.\n\n---\n\nReferences:\n\n[1] Zhang, Anqi, et al. \"Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification.\" arXiv preprint arXiv:2504.05419 (2025).\n\n[2] Belrose, Nora, et al. \"Eliciting latent predictions from transformers with the tuned lens.\" arXiv preprint arXiv:2303.08112 (2023).\n\n[3] Ghandeharioun, Asma, et al. \"Patchscopes: A unifying framework for inspecting hidden representations of language models.\" arXiv preprint arXiv:2401.06102 (2024)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UX4F8MGjPj", "forum": "qUeA0BYyAk", "replyto": "qUeA0BYyAk", "signatures": ["ICLR.cc/2026/Conference/Submission4667/Reviewer_X9pg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4667/Reviewer_X9pg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972949710, "cdate": 1761972949710, "tmdate": 1762917501944, "mdate": 1762917501944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates Fine-Grained Visual Classification (FGVC) capabilities in Large Vision-Language Models (LVLMs). The authors demonstrate that LVLMs internally possess the knowledge necessary for FGVC, a phenomenon they term hidden knowledge, but fail to express it in their external responses. Through designed experiments, they verify the existence of this hidden knowledge. Furthermore, the study introduces a probing technique that reveals a hidden, generalizable pattern. By leveraging this pattern, the LVLMs' FGVC accuracy is improved without using any annotated data from the target domain."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well wirtten and easy to follow, as well as well-motivated.\n* The finding is interesting, as the authors reveal that the VLMs \"Know\" but can not \"Say\". The verification of the existence of hidden knowledge bring new insghts for the improvements of LVLMs not only for FGCV.\n* The experiments are extensive and solid, across different model architecture, from LLaVA to QwenVL, which validates the generalization of the proposed method."}, "weaknesses": {"value": "* Do different tasks require using hidden states from different layers to achieve the best results? Is there a pattern that can be sought?\n* Could you discuss more on the potential application of the hidden knowledge beyound of FGVC?"}, "questions": {"value": "* Do different tasks require using hidden states from different layers to achieve the best results? Is there a pattern that can be sought?\n* Could you discuss more on the potential application of the hidden knowledge beyound of FGVC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4BbIIxUj6j", "forum": "qUeA0BYyAk", "replyto": "qUeA0BYyAk", "signatures": ["ICLR.cc/2026/Conference/Submission4667/Reviewer_Kpsc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4667/Reviewer_Kpsc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996732085, "cdate": 1761996732085, "tmdate": 1762917501583, "mdate": 1762917501583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}