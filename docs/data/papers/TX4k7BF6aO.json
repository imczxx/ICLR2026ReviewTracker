{"id": "TX4k7BF6aO", "number": 4089, "cdate": 1757598457468, "mdate": 1759898053439, "content": {"title": "Agentic Reinforced Policy Optimization", "abstract": "Large-scale reinforcement learning with verifiable rewards (RLVR) has proven effective in harnessing the potential of large language models~(LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs often rely on external tools to assist in task-solving processes. However, current RL algorithms typically employ trajectory-level rollout sampling, consistently neglecting the fine-grained exploration of multi-turn tool-call steps. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Our preliminary experiments reveal that LLMs frequently exhibit increased uncertainty after tool-call steps, as evidenced by higher entropy in the distribution of generated tokens. Motivated by this, ARPO incorporates an entropy-based adaptive rollout mechanism, encouraging the policy model to adaptively branch sampling during high-entropy tool-call rounds, thereby promoting step-level exploration of latent tool-use behaviors. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Experiments across 13 challenging benchmarks  demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments.", "tldr": "ARPO proposes an entropy-based adaptive rollout mechanism to adaptively manage branch sampling during high-entropy tool-use steps and employs advantage attribution estimation to assist the LLM in understanding step-level tool-use behaviors.", "keywords": ["Agentic Reinforcement Learning", "Large Language Model", "Agentic Reasoning", "Tool-use Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d982be520861ea9c08f4bbe475a8936a9ad53df0.pdf", "supplementary_material": "/attachment/52e936cca8897f44ffcf3af4f9e18d715f3984be.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a sampling strategy, which generates additional trajectories by forking the trajectory at timesteps where the tool call probabilities indicate high entropy, and use this strategy as part of the rollout stage for multi-turn agentic RL training for LLMs. The presented method (ARPO) provides consistent improvement in a variety of tasks (math, coding, deep search) and highlights the importance of further research in exploration within the area of RLVR for LLMs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is well motivated and the observation of high entropy tool calling steps is clearly presented. \n- Consistent improvements in evaluated domains (math, coding, deep search).\n- Mathematical formulation is grounded and highlights the efficacy of their sampling approach."}, "weaknesses": {"value": "- Limited Novelty: The main contribution seems to be the additional sampling introduced by branching from high entropy actions. The learning objective is simply multi-turn GRPO given rollouts generated by their sampling mechanism. \n- Effects of branching sampling ($P_{t}$) is unclear. There are no ablations documenting the effects of relying on the base probability ($\\alpha$) versus relying on the entropy differential ($\\beta$) or the branching cutoff ($\\tau$).\n- Unclear if the effectiveness of high-entropy branching is unique to tool-calling steps or broadly applicable (e.g. to text-based reasoning steps)"}, "questions": {"value": "1. Could the authors quantify the effects of varying $\\alpha$, $\\beta$, and $\\tau$ during sampling? The entropy figure in the Appendix indicates that if entropy is too high performance degrades. Thus, understanding the relationship between base model probs and entropy during sampling seems crucial.\n2. Are there other actions, besides high entropy tool calls, where branching is effective? The authors seem to only branch at high-entropy tool calling steps, but I wonder if this method is generally applicable to reasoning steps too."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "38NlsL7w9B", "forum": "TX4k7BF6aO", "replyto": "TX4k7BF6aO", "signatures": ["ICLR.cc/2026/Conference/Submission4089/Reviewer_BZ6R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4089/Reviewer_BZ6R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543285993, "cdate": 1761543285993, "tmdate": 1762917173239, "mdate": 1762917173239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ARPO, a RL algorithm designed to improve LLM agents that use external tools in multi-turn interactions. The key novelty is an entropy-based adaptive rollout mechanism that dynamically performs additional sampling at high-entropy tool-call steps and an advantage attribution estimation method that distinguishes shared vs. branched trajectories. Experiments across 13 benchmarks (math, knowledge reasoning, and deep search tasks) show that ARPO improves accuracy and efficiency, reportedly achieving similar or better performance with half the tool-use budget compared to GRPO and other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The evaluation covers a wide range of tasks (reasoning, knowledge retrieval, deep search), showing consistent improvements over multiple baselines (GRPO, DAPO, Reinforce++).\n- ARPO achieves better or comparable results with roughly half the tool-call budget, which is practically valuable given the high cost of tool-based RL.\n- The GPG theorem provides a solid conceptual framework connecting macro-step rollouts to standard policy gradients, offering a theoretical foundation.\n- The paper is well-organized, with clear modular explanations of the rollout mechanism and advantage estimation, supported by informative figures."}, "weaknesses": {"value": "- The paper’s central motivation that token entropy reliably increases after tool calls is not convincingly demonstrated. Figures 1 and 2 appear anecdotal, with unclear statistical support, and may rely on a single sample.\n- While ARPO’s adaptive rollout is designed to mitigate high-entropy uncertainty, the paper does not show post-training token entropy patterns to demonstrate that the method actually reduces or stabilizes entropy. Without before-and-after comparisons, it’s unclear whether ARPO genuinely controls model uncertainty or simply improves performance indirectly.\n- The experiments only report numerical performance gains, without showing any entropy-related observations or analyses during or after training. This creates a disconnect between the paper’s motivation entropy-guided exploration and its empirical validation, weakening the causal link between the proposed mechanism and the observed improvements."}, "questions": {"value": "1. The paper’s motivation relies on the observation that token entropy tends to increase after tool-call steps, which is claimed to indicate model uncertainty. Could the authors provide quantitative statistics (e.g., average entropy changes, variance, and significance tests) across multiple datasets and trajectories to confirm that this pattern consistently holds?\n2. Since ARPO is designed to mitigate high-entropy uncertainty, it would be important to show entropy evolution before and after training. Do the authors have measurements or visualizations demonstrating that entropy indeed decreases or stabilizes after ARPO training? \n3. Did the authors observe any instability or divergence issues during training due to dynamic branching?\n4. It would be helpful to include a PPO baseline or comparison, given that PPO is a standard RL method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oeOSeDVIrv", "forum": "TX4k7BF6aO", "replyto": "TX4k7BF6aO", "signatures": ["ICLR.cc/2026/Conference/Submission4089/Reviewer_tZJ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4089/Reviewer_tZJ2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685720737, "cdate": 1761685720737, "tmdate": 1762917172806, "mdate": 1762917172806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Agentic Reinforced Policy Optimization (ARPO), a reinforcement learning algorithm designed for multi-turn, tool-using large language model (LLM) agents. Unlike existing trajectory-level RL methods that sample entire reasoning paths, ARPO focuses on step-level exploration by leveraging the empirical observation that token entropy rises significantly after tool calls, reflecting uncertainty during reasoning. ARPO introduces two core components:\n\n(1) an entropy-based adaptive rollout mechanism that dynamically branches exploration when tool-call steps exhibit high entropy, enabling efficient step-level sampling;\n\n(2) Advantage Attribution Estimation, which assigns shared or distinct advantages to tokens depending on whether they belong to common or branched reasoning paths, allowing finer-grained credit assignment.\n\nA theoretical contribution, the Generalized Policy Gradient (GPG) Theorem, generalizes the policy gradient to macro-action rollouts, providing a solid foundation for partial sampling optimization.\nAcross 13 benchmarks covering mathematical reasoning, knowledge-intensive QA, and deep-search tasks, ARPO achieves 4–6% higher accuracy than baselines such as GRPO, DAPO, and Reinforce++, while requiring only half the tool-call budget, demonstrating both superior performance and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tStrong Motivation and Insight – The identification of entropy spikes following tool use provides a concrete empirical basis for adaptive exploration and directly informs ARPO’s design.\n2.\tInnovative Algorithmic Mechanism – The entropy-based adaptive rollout efficiently balances global and partial sampling, encouraging diverse tool-use behaviors without excessive sampling.\n3.\tRefined Credit Assignment – The Advantage Attribution Estimation effectively separates shared versus individual token updates, stabilizing training and improving interpretability.\n4.\tComprehensive Empirical Results – ARPO consistently surpasses trajectory-level baselines across multiple reasoning benchmarks and model backbones (Qwen, Llama), showing broad applicability.\n5.\tEfficiency and Scalability – ARPO achieves comparable or better results using half as many tool calls as GRPO, confirming both cost-effectiveness and computational scalability.\n6.\tSolid Theoretical Foundation – The GPG theorem formalizes ARPO’s macro-action rollouts, grounding its design in a generalizable reinforcement learning framework."}, "weaknesses": {"value": "1.\tLimited Domain Generalization – All experiments are text-based; no multimodal or embodied environments are tested, restricting claims of general agentic applicability.\n2.\tHyperparameter Sensitivity – The algorithm depends on key parameters (entropy threshold τ, stability factor β), but no sensitivity or ablation study is provided.\n3.\tLack of Runtime Validation – While ARPO claims reduced rollout complexity (O(n log n)), there is no empirical runtime or resource cost analysis to verify this.\n4.\tReward Design Ambiguity – The hierarchical reward function (Eq. 5) combines multiple factors (accuracy, format, multi-tool usage) but lacks a detailed analysis of their relative influence.\n5.\tPotential Evaluation Bias – Reliance on LLM-as-a-judge scoring could introduce evaluation bias due to stylistic similarity between training and evaluation models."}, "questions": {"value": "1.\tDoes adaptive branching ever lead to redundant exploration or inefficiency on simpler reasoning tasks?\n2.\tHow does Advantage Attribution Estimation perform under noisy or sparse rewards?\n3.\tWhat is the computational overhead of real-time entropy computation and partial rollouts compared to GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lsk0Pk24G0", "forum": "TX4k7BF6aO", "replyto": "TX4k7BF6aO", "signatures": ["ICLR.cc/2026/Conference/Submission4089/Reviewer_tzPk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4089/Reviewer_tzPk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762039776842, "cdate": 1762039776842, "tmdate": 1762917172536, "mdate": 1762917172536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}