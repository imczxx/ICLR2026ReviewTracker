{"id": "IjMfTINVcq", "number": 12081, "cdate": 1758205571574, "mdate": 1759897535267, "content": {"title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning", "abstract": "Large language models are increasingly used for complex reasoning tasks where high-quality offline data such as expert-annotated solutions and distilled reasoning traces are often available. However, in environments with sparse rewards, reinforcement learning struggles to sample successful trajectories, leading to inefficient learning. At the same time, these offline trajectories that represent correct reasoning paths are not utilized by standard on-policy reinforcement learning methods. We introduce SuperRL, a unified training framework that adaptively alternates between RL and SFT. Whenever every rollout for a given instance receives zero reward, indicating the absence of a learning signal, SuperRL falls back to SFT on the curated offline data. Extensive experiments across diverse reasoning benchmarks show that SuperRL surpasses vanilla RL by delivering higher sample efficiency, stronger generalization, and improved robustness under sparse rewards.", "tldr": "We propose a hybrid SFT-RL training framework that dynamically fuses supervised and reinforcement signals to improve stability, sample efficiency, and generalization in large language model fine-tuning.", "keywords": ["Large Language Model", "Reasoning", "Question Answering"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b12d5ca55a43c28b0f7be708cf2a9fb4ec45de53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SuperRL, a framework blending RL and SFT adaptively. It uses RL for non-zero rewards, SFT for zero rewards, outperforming baselines in sample efficiency, generalization, and stability across reasoning benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple yet practical, requiring minimal architectural change. \n\n2. The paper is clearly written and well-organized, providing an intuitive explanation of both the theoretical motivation and algorithmic design of SuperRL."}, "weaknesses": {"value": "1. The evaluation scope is somewhat narrow, focusing mainly on mathematical and reasoning benchmarks without validating applicability to broader or interactive tasks such as tool use or dialogue-based reasoning.\n\n2. The novelty of the paper is limited. Although the adaptive fallback is smart, the paper should explain more clearly how it’s different from existing hybrid SFT-RL methods.\n\n3. The theoretical justification should be explained more clearly, with stronger intuition, and clearer connections to the practical SuperRL framework."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XHACgLG4vp", "forum": "IjMfTINVcq", "replyto": "IjMfTINVcq", "signatures": ["ICLR.cc/2026/Conference/Submission12081/Reviewer_k1zf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12081/Reviewer_k1zf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761397912054, "cdate": 1761397912054, "tmdate": 1762923053796, "mdate": 1762923053796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SuperRL, a novel training framework for LLM reasoning that intelligently combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To this end, it employs a dynamic, instance-level gating mechanism that routes each training example to SFT or RL based on its estimated reward or advantage. The framework aims to overcome two key issues: standard RL's failure with sparse rewards and SFT's inability to fully exploit available expert data. This is grounded in the theory that the SFT objective provides the only viable learning signal when RL explorations yield no reward. Leveraging this simple reward-aware control flow, the method mitigates sparse-reward exploration failures in RL and overfitting/rigidity from SFT, without adding architectural complexity. Through experimental results on multiple datasets, SuperRL outperforms SFT-only, RL-only, and SFT-then-RL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper effectively combines rigorous theoretical analysis (Sec. 3.1) with empirical validation (e.g., Table 1), providing clear motivations for the adaptive switching mechanism in SuperRL.\n\n2.The manuscript is well-structured and straightforward to follow, with intuitive explanations (e.g., Fig. 1 overview), precise variant descriptions (SuperRL-R vs. SuperRL-A in Sec. 3.2), and insightful discussions on data quality dependencies (Sec. 4.2.2), making it accessible despite the technical content.\n\n3.Experiments demonstrate the comparable performance over baselines in math reasoning tasks, particularly in generalization from single-dataset training to OOD benchmarks"}, "weaknesses": {"value": "1. Limited Evaluation on Larger-scale Models and Multi-domain Tasks to Validate Scalability and Generalization. The authors do commendable work in demonstrating SuperRL's effectiveness on the Qwen2.5-1.5B model across single datasets like GSM8K, PRM12K, and OpenR1, with encouraging gains in ID/OOD settings as shown in Table 1. It would be beneficial to explore its performance on larger models (e.g., 7B+), multi-task training setups, or diverse domains such as code generation and commonsense reasoning to fully substantiate the claims of \"scalability\" and \"comprehensive empirical validation\".\n\n2. More experiments are suggested. The paper provides a clear and thoughtful comparison of SuperRL with basic baselines (RL-only, SFT-only, and SFT-then-RL) in Table 1, which helps contextualize its performance. However, including comparisons with other relevant methods or RL algorithms would further highlight SuperRL’s unique contributions. \n\n3. Suggestions for Ablation Studies. The claim that the adaptive mechanism is key to the performance needs stronger validation. How does the proposed method compare to a baseline that uses a fixed rule for combining SFT and RL, rather than the advantage-gated switch? Such an ablation would directly isolate the improvement stemming from the adaptivity itself.\n\n4. Theoretical Analysis of Noisy/Partial Signals Remains Unexplored. Section 3.1 proves SFT's utility in extreme zero-reward/zero-advantage cases but ignores realistic sparse-reward scenarios with noisy or partial signals (which is common in LLM reasoning). No convergence guarantees, regret bounds, or analysis of switching overhead (e.g., how frequent fallbacks affect RL exploration) are provided, weakening the \"rigorous theoretical justification\" claim.\n\n5. Typos and Missing references. The manuscript includes typographical errors (e.g., “CONCLUSTION” on page 9) , which the authors should carefully check and correct throughout the paper. And some key references are missing — for instance, DeepScaleR on page 9 should be properly cited."}, "questions": {"value": "1.How is the number of rollouts per instance determined (e.g., fixed at 4 as implied in Fig. 1)?\n\n2.In Appendix I (Table 8), what specific reductions are made to the datasets for SuperRL and how do they ensure fairness against cited ReLIFT/LUFFY results? Why is SRFT excluded despite its relevance in Sec. 2.2?\n\n3.Table 1 shows SuperRL-A outperforming SuperRL-R on PRM12K but underperforming on OpenR1—why does SuperRL-A degrade on lower-quality data (Sec. 4.2.2)? Are there any guidelines for choosing between variants based on dataset characteristics, and an ablation study isolating the effect of the hybrid triggering mechanism is necessary to validate its necessity.\n\n4.While the authors mention three families of open-source language models in Appendix D, it seems that experiments are all conducted on Qwen2.5. \n\n5.How does the adaptive switching affect training efficiency (e.g., wall-clock time or compute overhead) compared to baselines? What is the typical frequency of SFT fallbacks during training (e.g., percentage of instances per epoch), and have any optimizations been implemented to mitigate potential slowdowns?\n\n6.When the trajectories yield zero reward or advantage, SuperRL falls back to SFT using offline data. How do the authors ensure that these fallback data are of high quality and truly beneficial for learning rather than introducing noise or bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2hH6f8At1p", "forum": "IjMfTINVcq", "replyto": "IjMfTINVcq", "signatures": ["ICLR.cc/2026/Conference/Submission12081/Reviewer_dnSa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12081/Reviewer_dnSa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837719164, "cdate": 1761837719164, "tmdate": 1762923053294, "mdate": 1762923053294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new approach to switch between reinforcement learning and supervised fine-tuning (SFT) for LLMs. The underpinning hypothesis of the paper is that the reward should be considered as a \"switch\" or weighting between the two paradigms. In scenarios of zero rewards, the \"absence of a learning signal\" requires expert guidance through SFT. In contrast, non-zero rewards may indicate benefit of exploration for \"policy refinement\"."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This is a well-written and that is accessible to experts in adjacent fields. The hypothesis of the paper is clear and well motivated. The performance of the proposed approach is evaluated and compared against a selection of alternative methods. The results indicate some improvements over alternative methods for different benchmarks. The evaluation shows that the results are comparable to state-of-the-art models."}, "weaknesses": {"value": "The paper claims that a “unified training framework” is proposed for switching between RL and SFT. However, the methodology in Section 3 is limited to the extrema of zero-reward vs non-reward reward. This results in a heuristic switching mechanism between the two paradigms. While this switching mechanism is reasonably well justified, it is unclear to what extent it enables a “unified framework that adaptively combines” RL and SFT.\n\nWhile the paper is very easy to read and the results are intuitive, the evaluation study could be strengthened by providing deeper scientific insight. For example, as acknowledged in Appendix B, dynamic switching between SFT and RL may impact on smooth gradient integration.  I share this concern and would have liked to see a more thorough analysis of the potential impact of the switching mechanism."}, "questions": {"value": "-\tCan you provide a more thorough analysis of the impact of a hard switching mechanism between RL and SFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "WrekUNXE5h", "forum": "IjMfTINVcq", "replyto": "IjMfTINVcq", "signatures": ["ICLR.cc/2026/Conference/Submission12081/Reviewer_DwCV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12081/Reviewer_DwCV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950012301, "cdate": 1761950012301, "tmdate": 1762923052747, "mdate": 1762923052747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the limitations of existing approaches to integrating SFT and RL. The sequential “SFT-then-RL” paradigm disrupts preexisting reasoning patterns, requiring readaptation during RL training, while overfitting to expert data restricts exploration. To overcome these issues, the authors propose a unified formulation, SuperRL, which integrates SFT and RL into a single objective and achieves an optimal balance between them — not based on predefined rules, but on the availability of learning signals during training.\n\nThey provide a rigorous theoretical analysis of the unified SFT–RL framework and conduct extensive experiments to validate their approach, including evaluations on both in-distribution and out-of-distribution data, as well as analyses on benchmarks of varying difficulty and training stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the paper are outlined as below:\n1. The paper proposed a simple approach to unify SFT and RL by using zero rewards or zero advantage as a switching signal. \n2.  The paper proposed two fallback mechanisms for triggering SFT during RL training based on advantage and reward: SuperRL-A and SuperRL-R. They validated both their mechanism and provided empirical guidelines for choosing one method over another.\n3. SuperRL experimental results demonstrate strong performance both for in-domain and out-of-domain scenarios.\n4. Experiments also suggest that SuperRL is more stable than Vanilla RL."}, "weaknesses": {"value": "The weaknesses of the paper can be summarized as follows:\n\n1. It is unclear how the trajectories for SFT are selected when the advantage or reward approaches zero. Are the samples with zero advantage or reward directly used for SFT?\n\n2. SFT typically requires high-quality samples. If trajectories with zero reward are used for SFT, how can they be considered high-quality?\n\n3. In terms of stability, SuperRL’s improvement is modest. Although there are some reductions in variance, the change in range is negligible, and for OpenR1, the range even increases."}, "questions": {"value": "1. Please clarify how the trajectories for SFT are selected during fallback in SuperRL.\n2. Since the gains in performance for AIME24 and AIME25 are very small, it is very hard to interpret. Can you please give some sample examples to understand how SuperRL has benefited over SFT-then-RL?\n3. Please clarify the gain in stability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gTJniQF7oO", "forum": "IjMfTINVcq", "replyto": "IjMfTINVcq", "signatures": ["ICLR.cc/2026/Conference/Submission12081/Reviewer_SXYU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12081/Reviewer_SXYU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980355145, "cdate": 1761980355145, "tmdate": 1762923052270, "mdate": 1762923052270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SuperRL, a simple yet effective unified training framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by addressing the inherent conflict between supervised fine-tuning (SFT) and Reinforcement Learning (RL) in sparse-reward settings. SuperRL proposes an adaptive control flow: for any batch of data instances where all sampled trajectories yield a zero reward (indicating a lack of meaningful gradient signal for the RL policy update), the framework falls back to a standard SFT update using the high-quality offline data. This mechanism ensures that the model always receives a positive, meaningful gradient signal. The authors demonstrate that this simple, reward-aware fallback mechanism is highly effective, achieving competitive or superior performance across diverse mathematical and reasoning benchmarks (AIME24/25, MATH-500) while maintaining a lightweight training structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Simplicity and Effectiveness:** SuperRL's core concept—a conditional switch between RL and SFT based on the observed reward signal—is elegantly simple. It requires minimal hyperparameter tuning and avoids the complexity of multi-stage pipelines or manually interpolated loss functions, which significantly enhances reproducibility and scalability.\n\n2. **Targeted Solution to the Sparsity Problem:** The framework directly addresses the primary challenge of applying RL in reasoning tasks: sparse rewards. By ensuring a meaningful SFT gradient is always applied when the RL gradient is non-existent (zero reward), SuperRL effectively improves sample efficiency and robustness, especially on tasks with small data and high complexity (e.g., AIME benchmarks).\n\n3. **Strong Performance and Generalization:** Despite its simplicity, SuperRL achieves the strongest overall performance on the challenging AIME24/25 benchmarks, outperforming more complex, hybrid methods like LUFFY. This suggests a strong generalization advantage to small-sample reasoning tasks where maintaining a stable, supervised training signal is critical."}, "weaknesses": {"value": "1. **Missing Crucial Baseline Comparison:** The paper lacks a comparison to a conceptually simple, yet potentially competitive, baseline method. Specifically, an ablation where, after SFT and during the RL phase, the model simply drops or masks any trajectories/prompts that yield zero reward/advantage (i.e., treating them as noise and performing the RL update only on rewarded trajectories) should be included. This comparison is necessary to demonstrate that the benefit of SuperRL comes specifically from falling back to SFT rather than just filtering out unrewarded samples.\n\n2. **Algorithm Presentation:** The visualization of Algorithm 1 in Section 3.2 is presented as a screenshot or image, which hinders readability and formal presentation. For an academic paper, this algorithm should be typeset using a standard $\\LaTeX$ algorithm block to ensure clarity, proper formatting, and maintain a consistent style."}, "questions": {"value": "1. Threshold Sensitivity: SuperRL uses a binary condition (all sampled trajectories yield zero reward) to trigger the SFT fallback. Have the authors explored a soft threshold (e.g., falling back to SFT if the average or maximum advantage/reward across the batch falls below a certain $\\epsilon > 0$)? If explored, did this compromise performance or complexity?\n\n2. Efficiency and Training Time: While the paper mentions the lightweight nature of SuperRL, could the authors provide a comparison of the total wall-clock training time or resource consumption relative to other baselines? This would further quantify the practical efficiency advantage.\n\nI am willing to raise the final rating if all concerns are well-addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vJhRFjTrmQ", "forum": "IjMfTINVcq", "replyto": "IjMfTINVcq", "signatures": ["ICLR.cc/2026/Conference/Submission12081/Reviewer_iapZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12081/Reviewer_iapZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762166769435, "cdate": 1762166769435, "tmdate": 1762923051665, "mdate": 1762923051665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}