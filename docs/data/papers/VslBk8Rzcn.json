{"id": "VslBk8Rzcn", "number": 16281, "cdate": 1758262644453, "mdate": 1763655221594, "content": {"title": "Bootstrap Your Own Noise: Denoising Adaptive Noise in Diffusion Models for SSL", "abstract": "We introduce Bootstrap Your Own Noise (BYON), a self-supervised pretraining framework that unifies denoising diffusion with uncertainty-guided contrastive learning to enhance both local and global feature representations. BYON forms a self-reinforcing loop: contrastive learning improves reconstruction quality, and in turn, improved reconstructions refine feature alignment. A Semantic Uncertainty Estimation (SUE) module adaptively reweights contrastive updates based on reconstruction quality, while an Image-specific Adaptive Noise (IAN) adaptively modulates the noise intensity at the image level based on token saliency, perturbing more informative images more strongly.\nBYON consistently boosts performance on image classification, semantic segmentation, object detection, instance segmentation, and fine-grained visual classification (FGVC) tasks. To ensure reproducibility, the code is available in the Supplementary material.", "tldr": "BYON: bootstrapping from noise to couple diffusion and contrastive alignment for transferable representations.", "keywords": ["Self-Supervised Learning", "local-global representation learning", "bootstrapping representation", "Denoising Diffusion Model", "Contrastive Learning", "Masked Image Modeling"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/756b2dcf90ef4c78eda35f497e48a078cea25ad6.pdf", "supplementary_material": "/attachment/5731d6813b6bdc404d8d33cd7fc04e1fd7c67ba1.zip"}, "replies": [{"content": {"summary": {"value": "This paper, \"Bootstrap Your Own Noise\" (BYON), introduces a new self-supervised learning (SSL) framework. The core concept is what the authors call a \"self-reinforcing loop\" that unifies three major paradigms: contrastive learning (like SimCLR), diffusion-based image reconstruction (denoising) and MIM-based image reconstruction (demasking).\nTo make this work, they introduce two novel modules: a \"Semantic Uncertainty Estimation\" (SUE) module that adaptively re-weights the contrastive loss (paying more attention to good reconstructions) and an \"Image-specific Adaptive Noise\" (IAN) module, which cleverly adds more noise to more salient or \"informative\" parts of the image. They show that the resulting pre-trained model produces very strong features that transfer well to a whole range of downstream tasks like classification, detection, and segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe authors have managed to creatively combine three powerful ideas (denoising/demasking, contrastive, and uncertainty) into a single, cohesive framework. The IAN module, in particular, which adapts the noise schedule based on image content, strikes me as a very smart idea that moves beyond the typical \"one-size-fits-all\" noise of standard diffusion models.\n\n2.\tThe evaluation looks impressive."}, "weaknesses": {"value": "1.\tThis framework has a lot of moving parts: a diffusion model, a contrastive learning branch, an uncertainty module (SUE), and an adaptive noise module (IAN). This has to be an absolute monster to train and tune. The paper indicates that ‘BYON incurs higher cost than DiffMAE/MaskDiT due to the added contrastive/uncertainty pathways’, which I suspect is a major weakness compared to simpler, more scalable methods.\n\n2.\tSecond, because there are so many new components, it's hard to tell what's really driving the performance. From Table 1, we can see that De-masking + SUE already achieves acc of 83.02%, the introduction of De-noising even make acc a little bit worse. Even with all components, the performance gain seems to be marginal, compared to simply using De-masking.\n\n3.\tAll the major components including denoising, demasking, contrastive learning and uncertainty estimation are existing and well-explored techniques. The simple combination of these limits the originality of this work."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FQHNYFAKQx", "forum": "VslBk8Rzcn", "replyto": "VslBk8Rzcn", "signatures": ["ICLR.cc/2026/Conference/Submission16281/Reviewer_yQ7K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16281/Reviewer_yQ7K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497084056, "cdate": 1761497084056, "tmdate": 1762926429031, "mdate": 1762926429031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed self-supervised, uncertainty-guided contrastive learning method with diffusion model to improve feature representations. They suggested 3 main platforms SUE, IAN, and bootstrapping representations from the diffusion model. The suggested model developed a self-reinforcing loop: better contrastive alignment enhances reconstructions, which in turn refines global feature learning. This method outperforms existing masked image modeling and diffusion-based approaches across classification, segmentation, detection, and fine-grained recognition tasks(ImageNet top-1 accuracy by up to 0.6% and segmentation by 1.8%)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The overall organization and style of the manuscript is high quality. It's easy to follow and try to explain the suggested concept clearly.\n\n-The idea to use image-specific adaptive noise (IAN) with self supervised learning is promising in contrastive learning; IAN makes diffusion model be hard to discriminate source encoder's feature and reference feature."}, "weaknesses": {"value": "- Application set is limited on vision benchmarks such as image classification, segmentation, and detection. can it be further utilized for multimodal learning or cross-modal  cases? This method relies on saliency uncertainty and it is specialized for vision tasks so it may limit  the broadness of applications.\n\n- algorithmic advancement is limited; such contrastive learning may be biased to training set and believed to be unstable under limited configurations (i.e. limited datasets, small model.). The idea brings contrastive learning with bootstrapping generation from diffusion models and it looks working. However, the computational cost of retraining the representers (source and reference encoders) appears to outweigh the resulting reduction in error.\n\n- lacks of details on self-reinforcing feedback loop. Self feedback loop is a main workhorse about this paper, but I didn't catch detailed mathematical description for that. For example, the type of used semantic guidance and category of uncertainty measure were not explained."}, "questions": {"value": "-The section of self-reinforcing feedback loop limits the details. I would like to know the learning objective for self-reinforcing feedback loop and the network type to be used.  Does it use eq.(3) for their training objective?\n\n-IAN adapts diffusion noise per image using token saliency scores. But the interaction between IAN’s noise schedule and the diffusion timesteps is not fully specified in terms of \\lambda, \\delta, or \\eta. Is it chose in a manner of greedy search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "V5RNcWZVs9", "forum": "VslBk8Rzcn", "replyto": "VslBk8Rzcn", "signatures": ["ICLR.cc/2026/Conference/Submission16281/Reviewer_9LjC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16281/Reviewer_9LjC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918677001, "cdate": 1761918677001, "tmdate": 1762926428729, "mdate": 1762926428729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper is a self-supervised learning method that aims to improve previous MIM + diffusion pretraining approaches.\n- The authors’ motivation is that earlier MIM + diffusion methods understand local features well but lack global feature understanding.\n- To address this, they introduce a contrastive loss along with additional components called the SUE module and IAN.\n- Using the proposed SSL framework, which enhances both local and global feature understanding, they achieve improved top-1 accuracy on ImageNet classification and significant gains on downstream tasks such as semantic segmentation (ADE20K), object detection (COCO), and instance segmentation (COCO)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed SSL method shows performance improvements in image classification, segmentation, and detection, which demonstrates its effectiveness. (There is a slight improvement in image classification, but a large improvement in detection and segmentation.)"}, "weaknesses": {"value": "- The authors could have provided more detail in both the architecture figures and the loss formulations.\n\n1. Figure detail: For example, Figure 2 could have been illustrated more clearly, with notations and loss details included. As currently presented, it is difficult to interpret intuitively.\n2. Loss detail: The definitions of L_demask, L_denoise are missing.\n\n- It would be helpful to include qualitative comparisons, such as attention maps or other visual analyses, across MIM, diffusion, and MIM + diffusion. These visualizations could provide intuitive insights into whether different pretraining methods capture complementary aspects of the representation."}, "questions": {"value": "- If both MIM and diffusion pretraining aim to learn local features, what is the advantage of combining them? Do they learn local features even better when used together compared to using only one pretraining method?\n- In the proposed BYON framework, global feature learning comes from applying contrastive learning on the ViT CLS tokens. However, this same strategy can also be applied to pure MIM pretraining, since MIM methods also use transformer architectures with CLS tokens. The authors should provide quantitative results showing that applying contrastive learning on CLS tokens only with MIM pretraining is inferior to using MIM + diffusion to justify their claim.\n- Looking at Table 1, which ablates the proposed components, de-masking (MIM) alone already achieves strong accuracy (82.89), better than de-noising (diffusion) at 80.14. Using both de-noising and de-masking yields 82.86, which suggests that the performance gain mainly comes from de-masking rather than de-noising. This raises the question of whether combining MIM and diffusion is truly necessary for an effective pretraining objective.\n- Since the ablation in Table 1 seems to focus solely on image classification, it’s unclear whether the same trend holds for downstream tasks like detection or segmentation. Could you include quantitative downstream results for the ablated components to support the claim that combining MIM + diffusion yields broader benefits?\n\nI would be willing to increase my rating if the authors provide further results or clarifications that address these points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7q9XjVPveN", "forum": "VslBk8Rzcn", "replyto": "VslBk8Rzcn", "signatures": ["ICLR.cc/2026/Conference/Submission16281/Reviewer_468K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16281/Reviewer_468K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986581511, "cdate": 1761986581511, "tmdate": 1762926428292, "mdate": 1762926428292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **Bootstrap Your Own Noise (BYON)**, a self-supervised learning framework that combines diffusion-based denoising with contrastive representation learning.\nTo address the limitations of uniform noise and instability in previous diffusion SSL methods, the authors introduce two components:\n\n1. **Semantic Uncertainty Estimation (SUE)**, which adaptively weights contrastive objectives based on reconstruction reliability.\n2. **Image-specific Adaptive Noise (IAN)**, which adjusts the corruption strength according to token saliency, encouraging more informative learning from complex regions.\n\nBy coupling local denoising and global alignment objectives, BYON enables mutually reinforcing feature learning.\nExtensive experiments on ImageNet, ADE20K, and COCO demonstrate consistent improvements over prior diffusion- and masking-based pretraining approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Novel integration:** The paper successfully bridges diffusion-based denoising and contrastive learning within a unified self-supervised framework, which represents an important research direction in representation learning. While it remains an open question whether diffusion models can learn representations comparable to or distinct from those obtained through contrastive learning, this work demonstrates that integrating the two paradigms—reconstruction-based methods (e.g., MAE, diffusion) and contrastive learning—can yield complementary benefits and improved performance. This integration is a meaningful and timely contribution.\n* **Balanced local–global learning:** The method couples local reconstruction (denoising) with global alignment (contrastive learning), addressing a key gap between generative and discriminative SSL approaches.\n* **Comprehensive experiments:** BYON is thoroughly evaluated across classification, segmentation, and detection benchmarks, showing consistent and notable improvements over strong baselines such as MAE, DiffMAE, and MaskDiT.\n* **Clarity and reproducibility:** The paper is well-organized, provides detailed ablation studies, and reimplements baselines under consistent training settings, supporting the credibility of the reported gains."}, "weaknesses": {"value": "* **Analysis on self-supervision components:**\n  If I understand correctly, the core contribution of this paper lies in integrating *diffusion- and reconstruction-based learning (MAE, denoising)* with *contrastive learning* to jointly learn global and local representations (as shown in Eq. 12).\n  However, the ablation study mainly focuses on *de-noising* and *de-masking*, without isolating the effect of *contrastive learning*.\n  It would be valuable to analyze how much the contrastive objective itself contributes to the final representation quality.\n\n* **Positioning vs. prior diffusion SSL:**\n  The paper could better clarify how BYON conceptually differs from prior diffusion-based SSL methods (e.g., DiffMAE, MaskDiT) beyond empirical performance gains, particularly regarding the training objectives and the resulting representation properties."}, "questions": {"value": "* **Component-wise analysis:**\n  Could the authors provide quantitative and/or qualitative comparisons between (1) contrastive-only, and (2) denoising / demasking-only settings?\n  Such analysis would highlight how each supervision signal contributes to the learned representation, offering deeper insight into the proposed integration of reconstruction and contrastive paradigms.\n\n* **Qualitative feature visualization:**\n  While downstream metrics (e.g., classification, segmentation) demonstrate overall representation quality, it would also be interesting to visualize the feature space directly.\n  The self-attention maps in Figure 5 are promising, showing meaningful inter-object similarity.\n  Could the authors further visualize feature embeddings, for instance, via PCA or similarity maps as done in DINO[1,2], to illustrate the qualitative differences among features learned with (a) denoising/demasking only, (b) contrastive loss only, and (c) the combined BYON setup?\n  Such analysis would strengthen the claim that BYON effectively unifies local and global representation learning.\n\n\n[1] Emerging Properties in Self-Supervised Vision Transformers\n\n[2] DINOv2: Learning Robust Visual Features without Supervision"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPvtLtCpCa", "forum": "VslBk8Rzcn", "replyto": "VslBk8Rzcn", "signatures": ["ICLR.cc/2026/Conference/Submission16281/Reviewer_MG4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16281/Reviewer_MG4B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991293978, "cdate": 1761991293978, "tmdate": 1762926427696, "mdate": 1762926427696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Paper Revision Summary"}, "comment": {"value": "We sincerely thank the Area Chair and all reviewers for their time, insightful feedback, and constructive suggestions. We are encouraged by the recognition of our work’s novelty and timeliness (MG4B), the positive assessment of our core ideas (yQ7K, 9LjC), the comments on clarity of presentation (MG4B, 9LjC), and the acknowledgment of thorough and strong empirical results across benchmarks (MG4B, 468K, yQ7K). We also appreciate the confirmation that independent re-implementation supports the credibility of our reported gains (MG4B).\n\nWe would like to emphasize that **we have conducted all requested experiments and clarifications**, and have added these results in the Appendix of the revised manuscript. We note, however, that all of our new experiments are **in-line** with our findings, rooted from our initial detailed analysis in the original submission. We believe the new results strengthen our case, but the main substance of the paper, even revised, is the same as before, centered on the key insights we originally presented.\n\nKey revisions (mainly reflected in the Appendix) include:\n\n- **Table 1, Appendix M**: Comprehensive ablation studies and component analysis; Inclusion of the \"Contrastive loss only\" variant for complete decomposition of learning signals (Table 1), and extended ablations on downstream tasks (ADE20K and COCO) to clarify the contributions of IAN and SUE (Appendix M).\n\n- **Appendix N**: Demonstration of generality and applicability; Extension to Multimodal Pretraining (implemented on the CLIP architecture) to validate the architecture-agnostic nature of the core principles (Appendix N).\n\n- **Appendix K**: Expanded qualitative comparisons and visualizations; Attention map comparisons against BYOL, MAE, and DiffMAE, illustrating the globally coherent yet locally discriminative representations learned by BYON (Appendix K).\n\n- **Figure 2, Appendix L**: Further clarifications and methodological details; Updated architecture diagram with consistent notation (Figure 2) and detailed formulations for the loss functions (Appendix L).\n\nWe believe these additions thoroughly address all reviewer concerns and significantly strengthen our manuscript. We thank the reviewers again for their insightful engagement in improving our work."}}, "id": "U3ESkVSqLU", "forum": "VslBk8Rzcn", "replyto": "VslBk8Rzcn", "signatures": ["ICLR.cc/2026/Conference/Submission16281/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16281/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16281/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763660534763, "cdate": 1763660534763, "tmdate": 1763660534763, "mdate": 1763660534763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}