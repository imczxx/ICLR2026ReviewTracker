{"id": "SmYN0Ww6mx", "number": 313, "cdate": 1756735035821, "mdate": 1759898267989, "content": {"title": "Multi-modality Image Fusion under Adverse Weather: Mask-Guided Feature Restoration and Interaction", "abstract": "Multi-modality image fusion (MMIF) enhances scene representation by exploiting complementary cues from different modalities. Adverse weather, however, causes significant image degradation, disrupting feature representation and requiring simultaneous feature restoration and cross-modal complementarity. Existing methods often struggle with effective representation learning under such conditions, limiting their practical performance. To address these challenges, we propose a mask-guided MMIF method that integrates feature restoration and interaction. We first introduce \"Pseudo Ground Truth\" to simplify training, promoting faster and more effective feature learning. Then, we design a mask generation mechanism based on the mapping relationship between the fused result and the source images, quantifying the relative contribution of each modality during the fusion process. By incorporating the proposed mask-guided cross-modal cross-attention mechanism, the network is encouraged to selectively attend to informative features during modality interaction, mitigating the risk of overfitting to the static distribution of the \"Pseudo Ground Truth\". Additionally, we propose a mask-guided and a task-coupled degradation-aware strategy to balance feature restoration and interaction. Extensive experiments on synthetic and real-world datasets (rain, haze, and snow) demonstrate that our method surpasses state-of-the-art approaches in visual quality, quantitative metrics, and downstream tasks.", "tldr": "", "keywords": ["Multi-modality Image Fusion", "Complex Scenes", "Feature Interaction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b38e571889585c7b62502a96c65cbf52805c4c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel multi-modality image fusion method designed to address image fusion challenges under adverse weather conditions. The proposed method integrates feature restoration with cross-modal interaction through a mask-guided mechanism, utilizing \"Pseudo Ground Truth\" to simplify the training process. This helps in recovering degraded features and enhancing the interaction between multi-modal features. Key innovations include the construction of a mask that quantifies each modality's contribution during the fusion process, enabling effective separation and interaction of features. Additionally, the method introduces a Mask-Guided Feature Extraction Module (MFEM), Mask-Guided Learning Strategy (MGLS), and Task-Coupled Degradation-Aware Learning Strategy (TDAS) to improve feature restoration and cross-modal interaction. Extensive experiments on both synthetic and real-world datasets under adverse weather conditions demonstrate that AMG-Fuse outperforms state-of-the-art methods in visual quality, quantitative metrics, and downstream tasks, while also showing strong adaptability in ideal conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The AMG-Fuse integrates feature restoration and cross-modal interaction into a unified framework to tackle the challenges of image fusion under adverse weather conditions. The mask-guided mechanism combined with \"Pseudo Ground Truth\" allows the network to learn dynamic cross-modal information allocation, overcoming the limitations of traditional methods relying on static modality distribution.\n\n2.The proposed method is thoroughly tested on both synthetic and real-world datasets under various weather conditions. The results demonstrate AMG-Fuse’s superiority in visual quality, quantitative metrics, and downstream tasks, proving its robustness and applicability across different environments.\n\n3.While AMG-Fuse excels in handling adverse weather, it also performs well in ideal environments, showcasing its generalization ability and versatility in a variety of conditions."}, "weaknesses": {"value": "1.The inclusion of the Histogram Transformer module and depthwise separable convolutions leads to high computational demands and a large number of parameters, which may limit its applicability in real-time scenarios with resource constraints.\n\n2.The overall architecture of this article is similar to a simple stacking of modules, which also includes a frozen restoration model in the middle. The understanding of the essential relationship and differences between restoration and fusion tasks is not yet thorough enough.\n\n3.While the method performs well in adverse weather conditions, it struggles with noise and other degradation types, as the mask fails to accurately capture the degradation distribution in such cases, leading to suboptimal performance.\n\n4.The degradation targeted in this article mainly includes rain, snow, and haze, and the rain and snow part mainly uses artificial virtual data, which is not realistic enough in terms of visual effects. Therefore, it is difficult to reflect the value in practical applications. The method may require further optimization for handling other types of degradation, such as motion blur or strong lighting variations.\n\n5.The method involves multiple strategies that require careful tuning of hyperparameters, which increases the complexity of the training process and experimental design."}, "questions": {"value": "1.As shown in the Haze section of Fig.6, the restoration results of AdaIR are obviously not excellent enough. Can the author choose other restoration models for comparative experiments and prove that the proposed method is indeed more effective than the method of two models.\n\n2.Some of the results in the ablation experiment in this article do not reflect the significance of this module, especially in Mask Guided Learning Strategy, where the contours of the characters are very blurry. The author needs to submit other figures to prove it.\n\n3.In the author's comparison method, only Text-if and AWFusion are integrated restoration fusion methods. There have been many works in this area in the past year, and the author should provide more reference methods.\n\n4.The author needs to provide performance on real-world datasets to validate the practical value of the method.\n\n5.The author did not provide visual results for object detection, and relevant content needs to be supplemented.\n\n6.Is it possible to use model compression or quantization techniques to reduce the computational overhead during inference and improve the feasibility of this method in real-time applications for high computational complexity problems?\n\n7.If the degree of degradation changes, will the model's processing capability weaken and will the area of focus of Mask change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PSSEjexzFz", "forum": "SmYN0Ww6mx", "replyto": "SmYN0Ww6mx", "signatures": ["ICLR.cc/2026/Conference/Submission313/Reviewer_AjCT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission313/Reviewer_AjCT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847212799, "cdate": 1761847212799, "tmdate": 1762915492034, "mdate": 1762915492034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors proposed a mask-guided multi-modal image fusion method designed to handle degradations under adverse weather. The main contributions are the proposed mask-guided feature extraction module and task-coupled degradation-aware learning strategy, which alleviate the optimization difficulty of jointly learning cross-modal interactions and feature restoration in complex scenes. Experiments show that the proposed method consistently outperforms the state-of-the-art methods on both image fusion and downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(1) By explicitly separating modality-specific components within the fused image, the network dynamically learns how to allocate multimodal information. This differs from existing mask-based fusion methods and is both novel and effective.\n\n(2) The proposed mask-guided feature extraction module leverages masks to recover degraded features while enhancing cross-modal interactions, offering a valuable new perspective for image fusion task in challenging scenarios. The proposed task-coupled degradation-aware learning strategy exploits restoration priors to encourage the fusion network to learn cleaner representations, which is a well-motivated supervision scheme."}, "weaknesses": {"value": "**Major**\n\n(1) In Fig. 2, constraining the network directly with clean multimodal source images seems to improve generalization. However, intuitively, models trained on harder examples often generalize better to easier ones. Authors should clarify why the opposite appears beneficial here, and provide theoretical insights or empirical evidence.\n\n(2) The mask is constructed based on Eq. (3). Please discuss whether this formulation captures the composition of more general fusion tasks, and detail its assumptions and potential failure modes.\n\n(3)The evaluation on downstream tasks is not sufficiently comprehensive. Currently, only detection on M3FD is reported. Please add semantic segmentation comparisons on the MSRS dataset.\n\n**Minor**\n\n(1) Please standardize notation (case, subscripts/superscripts) throughout, including symbols in equations. Also, keep the term “Pseudo Ground Truth” consistent.\n\n(2) The qualitative results in Fig. 6 are relatively small. please add higher resolution visual comparisons for adverse-weather cases in the appendix."}, "questions": {"value": "(1) How is the Pseudo Ground Truth generator trained? Does it use off-the-shelf weights, or is it re-trained/fine-tuned on AWMM-100k? Please provide more training details and data splits.\n\n(2) Can the proposed method generalize to other image fusion tasks, such as CT–MRI fusion in medical imaging?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vhJfwT7AJj", "forum": "SmYN0Ww6mx", "replyto": "SmYN0Ww6mx", "signatures": ["ICLR.cc/2026/Conference/Submission313/Reviewer_Gdro"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission313/Reviewer_Gdro"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877986666, "cdate": 1761877986666, "tmdate": 1762915491814, "mdate": 1762915491814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a mask-guided multi-modality image fusion (MMIF) framework that jointly performs feature restoration and cross-modal interaction under adverse weather. \nIt introduces pseudo ground-truth to simplify training and a mask generation mechanism to measure each modality’s contribution during fusion. \nA mask-guided cross-modal attention enables the network to focus on informative features, while degradation-aware learning strategies balance restoration and interaction. \nExperiments on rain, haze, and snow datasets show that the proposed method achieves superior visual quality, quantitative performance, and downstream task results compared to state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work introduces a mask-guided framework (AMG-Fuse) that unifies feature restoration and modality interaction, addressing the limitations of two-stage “restoration + fusion” pipelines.\n\n2. The Mask-Guided Feature Extraction Module (MFEM), Mask-Guided Learning Strategy (MGLS), and Task-Coupled Degradation-Aware Strategy (TDAS) are well-motivated and technically sound contributions that enhance cross-modal complementarity.\n\n3. Extensive experiments on multiple adverse weather datasets (rain, haze, snow) and clean datasets demonstrate consistent improvements across quantitative and qualitative metrics.\n\n4. The proposed method outperforms state-of-the-art methods in both degraded and ideal conditions, showing robustness and generalization ability.\n\n5. The paper is well-organized, with detailed ablation studies, visualization analyses, and fair comparisons that support the claims effectively."}, "weaknesses": {"value": "1. Although conceptually justified, the reliance on pseudo targets may still raise questions about the upper-bound constraint on model generalization and possible supervision bias not fully explored in the experiments. Though softened by decaying loss weights, the reliance on pseudo targets may introduce bias and constrain the model’s learning dynamics. Would this be addressed by theoretical or empirical analysis?\n\n2. Could the authors provide a more extensive comparisons on FLOPs or runtime for efficiency analysis?\n\n3. The method introduces Histogram Transformer and multi-head attention structures, yet the discussion of computational trade-offs versus the versions of lightweight fusion models remains underdeveloped.\n\n4. The manuscript has neglected several related works, such as pseudo ground-truth in detection under adverse weather [1], image fusion under hazy condition [2], etc. More extensive literature review is recommended.\n\n[1] Rethinking image restoration for object detection. 2022.\n[2] Image dehazing by artificial multiple-exposure image fusion, 2018.\n\n5. Some mathematical derivations (e.g., Equation 5 reformulation) and visualizations could be better contextualized for readers unfamiliar with modality-decoupled learning. The citation format of the paper should be revised.\n\nThe concerns are not vital and could be addressed before potentially accepted."}, "questions": {"value": "Would the authors consider providing theoretical or empirical analysis to clarify whether the reliance on pseudo targets introduces supervision bias or constrains the model’s generalization capacity and learning dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fMNntSxnYI", "forum": "SmYN0Ww6mx", "replyto": "SmYN0Ww6mx", "signatures": ["ICLR.cc/2026/Conference/Submission313/Reviewer_cVus"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission313/Reviewer_cVus"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930812723, "cdate": 1761930812723, "tmdate": 1762915491622, "mdate": 1762915491622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}