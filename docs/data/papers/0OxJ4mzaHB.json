{"id": "0OxJ4mzaHB", "number": 7837, "cdate": 1758038229261, "mdate": 1763684009946, "content": {"title": "Can Interpretation Predict Behavior on Unseen Data?", "abstract": "Interpretability research often aims to predict how a model will respond to targeted interventions on specific mechanisms. However, it rarely predicts how a model will respond to unseen input data. This paper explores the promises and challenges of interpretability as a tool for predicting out-of-distribution (OOD) model behavior.  Specifically, we investigate the correspondence between attention patterns and OOD generalization in hundreds of Transformer models independently trained on a synthetic classification task. These models exhibit several distinct systematic generalization rules OOD, forming a diverse population for correlational analysis. In this setting, we find that simple observational tools from interpretability can predict OOD performance. In particular,  when in-distribution attention exhibits hierarchical patterns, the model is likely to generalize hierarchically on OOD data---even when the rule's implementation does not rely on these hierarchical patterns, according to ablation tests. Our findings offer a proof-of-concept to motivate further interpretability work on predicting unseen model behavior.", "tldr": "Proof-of-concept that interpretability can be used to predict future model behavior on unseen inputs", "keywords": ["interpretabilty", "generalization", "out-of-distribution"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/690fd66af9cfac523e1934867ecb412d5797c6fc.pdf", "supplementary_material": "/attachment/433dd75f56861bcd852d97733781655793f2f8dc.zip"}, "replies": [{"content": {"summary": {"value": "The paper argues that by examining how Transformers allocate attention on standard (in-distribution) inputs, we can predict which rule they’ll apply to unseen (out-of-distribution) cases. Using a toy parentheses-balancing task, the authors train hundreds of small Transformers varying depth, width, regularization, and random seeds, then evaluate which rule each model actually follows OOD. They find that models cluster by their OOD rule, attention patterns can forecast that rule, and correlation does not guarantee causation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tGood and ambitious motivation.\n2.\tAlthough it uses a toy setup, it includes many models and experiments, and some findings are interesting.\n3.\tThe paper is well written, with accurate, clear descriptions and excellent details."}, "weaknesses": {"value": "1. The experimental method relies solely on a simplified parentheses-balancing task. This narrow setup may limit the generality of the conclusions.\n2. While the findings (e.g., “independently trained models cluster around systematic generalization rules”) are interesting, the paper would benefit from demonstrating at least one concrete example or use case that shows how such findings could be useful in practical applications or improvements for model designs.\n3. The dataset used in evaluation is large, but I'm not entirely sure whether the empirical evidence is sufficient to support the general claims."}, "questions": {"value": "1. Could the authors elaborate more on the design choice of using the parentheses-balancing task? Why is it an appropriate setting for testing interpretability and generalization?\n2. Would the observed findings—such as the clustering of independently trained models around systematic generalization rules—also hold for other tasks or architectures beyond this synthetic setup?\n3. How do the identified patterns or correlations facilitate specific downstream applications or improvements in model designs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9yDB5JEhML", "forum": "0OxJ4mzaHB", "replyto": "0OxJ4mzaHB", "signatures": ["ICLR.cc/2026/Conference/Submission7837/Reviewer_ZSQM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7837/Reviewer_ZSQM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445850223, "cdate": 1761445850223, "tmdate": 1762919882248, "mdate": 1762919882248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their thorough efforts. There are two common threads in the weaknesses, which are related. First, some reviewers were surprised by our unconventional decision to start the paper with a philosophical discussion. Second, several reviewers questioned whether our methods or findings will generalize to realistic settings or other tasks. \n\nThe core problem is that our goal is outside the norm in interpretability, requiring explicit discussion about how our goal differs and why we consider it worthwhile. In the interpretability field, where questions of what counts as faithful explanation are paramount, we believe that researchers should not shrink away from the philosophy of science. \n\nOur primary objectives are:\n- To convey a novel goal (predicting future, unseen model behavior from our interpretations of activations).\n- To show that it is possible to achieve this goal in a restricted setting.\n- To demonstrate that this goal is distinct from a more popular causal mechanistic objective—and that mechanistic faithfulness is neither necessary nor sufficient to achieve our goal.\n\nImportantly, none of these points rely on any specific method or result generalizing from our toy setting to a real task. Our objective is to illustrate the goal and show that it is not necessarily guaranteed by causal faithfulness. There are several other findings that we discuss, because our setting provides several other intriguing results, but these are the primary objectives. We hope that the reviewers understand that generalization is not a concern for them, despite our discussion of other, more context-specific findings. We would appreciate some feedback on what results we should move to the appendix to avoid confusing distractions. We are also currently running similar experiments in a synthetic language modeling setting which is previously documented to exhibit clustering OOD generalization behavior across random seeds. We hope to show that we can find intuitive links between ID representations and OOD behavior in that setting as well.\n\nAs several reviewers noted, it’s unusual to have a philosophical discussion section in an ICLR paper, but our objective is an empirical “position” with experimental support, so we think it’s important to have our position clearly expressed. However, our revised version does include additional details about the case studies we draw on in the natural sciences. Recognizing that several reviewers found it difficult to adapt to our paper structure, we have also moved the philosophical motivation to the end of the paper where a reflective discussion section would normally sit, rather than early in the paper as a motivation section.\n\nAlthough o6W7 complimented our presentation and explanation, several other reviewers clearly still struggled to follow the central thesis and so focused on our setting-specific method or on minor results. We have revised the written presentation for clarity. Specifically, we have moved some experimental details into the appendix, restated the nature of the toy task in the introduction, clarified the concept of an ambiguous rule experiment, moved some extended philosophical discussion later in the paper, highlighted our terminology reference in the appendix, and changed some terminology to be more comfortable for readers who have not encountered hierarchical generalization research before (moving from “negative vs nonnegative depth” phrasing to clarify that negative depth positions are violations of the condition in equation 2)."}}, "id": "JX1NHK9RIm", "forum": "0OxJ4mzaHB", "replyto": "0OxJ4mzaHB", "signatures": ["ICLR.cc/2026/Conference/Submission7837/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7837/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7837/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684001112, "cdate": 1763684001112, "tmdate": 1763684001112, "mdate": 1763684001112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to utilize interpretability as a tool to predict model outputs on OOD, unseen input data. The problem setting explored here is a synthetic classification task on parentheses strings, where the ID training data is designed to be learnable from either an Equal-Count or Nested rule. However, the OOD test data of most interest for this problem conforms to the Equal-Count rule only; as a result, models that have internalized the Equal-Count (OOD-relevant) rule are expected to perform better on OOD inputs than those which have not. The authors demonstrate that the models indeed show evidence of learning these rules in their attention patterns (identifying several relevant design decisions), as well as other heuristics, and that there is resulting explanatory as well as predictive power from what a model has learned, specifically in predicting its performance on the (carefully defined, for this problem) OOD inputs. They also show that the common practice of ablating proposed explanatory mechanisms is data-dependent, and does not work as conventionally applied, on their OOD inputs. They claim this framework can be useful beyond the specific problem they explore."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "I believe the experiments are interesting and may be sound (difficult to evaluate given the poor presentation).\n\nSection 4, with the experiments and results around vestigial circuits and factors in rule-selection, is interesting and readable. Section 5 (also well-named) also reads better than Sections 1-3, but it was a struggle to understand the problem framing and experimental setting, so it is difficult for me to comment on soundness. The observation that the common setting of ablating proposed explanatory mechanisms is data-dependent, i.e. robust to ID data but not OOD data, would definitely be of interest to the community if the rest of the paper were more understandable. In my opinion, this last finding is the most interesting and, currently, the best-substantiated.\n\nThe figures are generally helpful in providing the appropriate intuition to understand the paper, but I would urge the authors to still be much clearer in the text."}, "weaknesses": {"value": "MAJOR: The paper is needlessly hard to follow and feels vague in many places. It makes for a frustrated reader. Many of the questions I have about this paper are probably due to the poor exposition of the motivation, concepts, and experimental setting. The paper flip-flops awkwardly between tedious details and broad, vague conceptual or epistemic statements, making it difficult to follow, evaluate, or build on the work presented.\n\nMAJOR: The paper is missing precise statements to guide the reader through the authors’ motivation and justification for experiments/analyses presented. For example, they should tell the reader upfront that they are first interpreting how Transformers internalize the classification rules needed to separate the training (ID) data, and then using this understanding to model behavior on OOD inputs. The current intro is disjointed and hard to follow in a first read. Without a clear outline, there’s no point in reading the subsequent sections.\n\nMAJOR: The paper uses only a synthetic classification task on parentheses strings for validation and illustration. It is unclear how this extends to data settings where we cannot simply look at the data/task and come up with a good rule that DNNs may or may not internalize when trained. While the authors admit it is a proof-of-concept work, they do not provide any recommendation for how this might be applied in more realistic settings, where we would need to understand what heuristics/rules may apply to the underlying problem. They don’t even state what would hypothetically be necessary to generalize the methods presented. 469-470 “If we identify similar cases in real-world settings…” – how would one begin to do this?\nRelated to this point: 363-364 The authors state “If we claim to understand a model, we should know its behavior under many unseen conditions” but according to Table 2 on Page 15, they do not even test the very simple opposite setting of OOD data with Equal-Count false and Nested true.\n\nThe sections are also disjointed and confusingly repetitive. The two sections (3.1.1. and 3.2.1) both called Experimental Details are confusing (one is under Data and the other under Models, I guess). But the so-called “Details” sections are not actually helpful. Ideally, you have a conceptually clear description with salient details in the main text, and full details in an Appendix or later Methods section. In your case, there is actually not much detail provided, and you could collapse 3.1.1 into 3.1 and 3.2.1 into 3.2 – you’d probably save space. The paper consistently uses vague wording. Examples from the sections mentioned: “models” without specifying DNNs, then “Transformers”, then “a population of classifier models based on the miniGPT architecture with hidden dimension 64” which is still not a full description of the model (“based on”?). \n\n(not affecting score) Why is Vaswani et al. cited with the year 2023? Everyone knows it as a 2017 paper, and the authors don’t seem to be specifically referring to any concepts from a newer version."}, "questions": {"value": "What is the data exactly? Four paragraphs in, the authors are describing Equal-Count and Nested but have not properly explained the task setting or what the input data looks like (I see red and green parentheses in Fig 1, but no description anywhere). Even something broad like “classifying on strings” would help guide the reader–the authors can probably do better than that.\n\nWhy is the synthetic classification task not trivial? I appreciate the citations, but perhaps add a sentence about the standard settings before lines 148-149. Why Transformers needed to model this? (For example, if Transformers are being used simply for the purpose of attention-based interpretability to see whether a model has learned the logical rules of Equal-Count and/or Nested, this choice should be properly explained and justified).\n\nIn the Philosophical Motivation section, how did the authors choose which statements needed citations? No citations are provided for what they say about “other sciences” and “genetics”. In general, this section does not convincingly add value to the paper; though it attempts to situate the authors’ perspective, it is not precise or well-substantiated (in part due to the lack of citations for many of the statements given).\n\nWhat does it mean for the training dataset to be “compatible” with either of the two rules? the authors repeat this several times, but the meaning doesn’t become clear because they first say:\n148-150 “Unlike standard parentheses-balancing settings, our training dataset is compatible with either EQUAL-COUNT, an unordered counting rule, or NESTED, a hierarchical parentheses-balancing rule.”\nand then\n162-163 “Our training set is compatible with both EQUAL-COUNT and NESTED: every input sequence satisfies either both or neither of Equations 1 and 2.”\nwhich at first seems like a contradiction: XOR vs (both or neither).\nHowever, after re-reading the paper a few times, I think they mean to say that a model which has learned either rule (or both?) should be able to correctly classify the ID data, since all ID data points satisfy either both or neither of the constraints. In contrast, the OOD data is not classifiable from the Nested rule, only the Equal-Count rule (as in Fig 1). This is not evident from how they phrase “compatible” or \"ambiguous rule” and should be properly clarified.\n\nWhere are the “OOD output probabilities” coming from? Can this be defined better since it’s so important for the clustering in Section 4?\n\nI believe you should be clearer about stating that you train only on ID but most interested in investigating and testing on OOD.\n\nThe authors say the method is meant to be correlational and holistic. They also talk a lot about causality, and a broad conclusion of this work seems to be that even what we sometimes consider causal in XAI (interpretability ablations) is actually not strictly causal under OOD data. For a stronger paper, the authors should make more precise claims, and perhaps better separated claims, about 1) what they can interpret from an attention-based model trained in this synthetic setting where it learns hard rules or heuristics, and 2) what the implications are for current interpretability practices, from the finding that ablation analyses intended to demonstrate mechanistic causality are in fact data-dependent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RIKxR12J65", "forum": "0OxJ4mzaHB", "replyto": "0OxJ4mzaHB", "signatures": ["ICLR.cc/2026/Conference/Submission7837/Reviewer_WV3D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7837/Reviewer_WV3D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950712827, "cdate": 1761950712827, "tmdate": 1762919881793, "mdate": 1762919881793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to explore whether interpretability can be used to predict OOD generalization for unseen data. They introduce EQUAL-COUNT and NESTED rules into a synthetic parentheses dataset to investigate if interpretations of in-distribution data could predict the OOD behavior on the testing set. I think the topic is quite relevant and important for the community, looking at how interpretability can be used to analyze the model behavior. However, the authors only tested the idea on Transformer models trained on a synthetic dataset, which may not be sufficient to validate the idea reliably."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using interpretability to analyze the model behavior could be an interesting topic.\n2. The authors provide the dataset and code with detailed experimental settings for good reproducibility."}, "weaknesses": {"value": "1. The title can be misleading. The authors mainly look at the OOD generalization, and only test the miniGPT type transformer model on a specific synthetic dataset. To me, it is not appropriate to use a general phrase “model behavior”.\n2. Also, with a transformer-based architecture with different hyperparameters that can influence the OOD generalization, I think it shouldn’t have been stated “hundreds of models”.\n3. All the results presented in the paper rely heavily on specific synthetic dataset configurations and the transformer model architecture with limited hyperparameters. I would assume that this method may not be easily applicable to real-world problems. It is unclear whether the key conclusions of the paper will still remain valid when applied to real-world scenarios across different model architectures.\n4. The authors mention that a head is a hierarchical head if it tracks depth on at least 80% of mixed-depth inputs. How sensitive is the method to this threshold? The paper did not justify this threshold or investigate the sensitivity.\n5. The authors take space to discuss the philosophical motivation. However, I did not see a tight link based on their empirical results."}, "questions": {"value": "1. Did the authors try to adapt the proposed method to real-world problems?\n2. Can the authors discuss whether the main conclusions will remain valid in more complex cases?\n3. Did the authors consider other quantitative evaluations regarding the OOD generalizability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OqzgcCeAhA", "forum": "0OxJ4mzaHB", "replyto": "0OxJ4mzaHB", "signatures": ["ICLR.cc/2026/Conference/Submission7837/Reviewer_bny3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7837/Reviewer_bny3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991263043, "cdate": 1761991263043, "tmdate": 1762919881444, "mdate": 1762919881444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether attention patterns in Transformers can be used to predict how the model will act on Out-of-Distribution (OOD) data. While the experiments show that attention patterns do **correlate** with OOD behavior, they are not necessarily the **cause** of the behavior (attention ablation does not always inhibit the behavior). Finally, the paper cautions that ablation (interventions) on Neural Network architectures should be done on both In-Distribution and OOD data since the results of the ablation can differ considerably."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper has a great presentation. The Figures are of high quality, the text is easy to read, and the experiments are well explained.\n\nThe paper tackles an important challenge: whether we can use attention heads to predict how a Transformer will act \"in the wild\". Notably, the fact that attention ablation has no effect In-Distribution, but can have unpredictable behaviors Out-of-Distribution is an important result. It warns explainability researchers that only evaluating explainability methods on In-Distribution data does not give the full picture of the model. This observation might influence research in other areas e.g. explainability in computer vision.\n\nWhile the paper focuses on a simplified setting (simple data and models), it pushes this experimental setup to the limit. The experiments investigate various model depths, weight decays, number of attention heads (Appx C.2). Transformers are also compared with LSTMs in Appx C.1, showing that LSTMs are unable to learn the \"Equal-Count\" rule."}, "weaknesses": {"value": "## Incomplete Related Work\n\nWhile Section 2 is a great read, I think it is too high-level for ICLR. I would rather motivate the current work by having a Related Work section that discusses in more depth the papers from the introduction. It would be interesting to describe what is activation steering, activation patching, and Sparse Autoencoder (SAE), and their limitations when it comes to OOD data. For instance, the work of (Kisanne et al. 2024, Smith et al. 2025) (line 48 of the manuscript) focus on limitations of SAEs. Is there existing work that shows limitations of other techniques (activation steering and patching) when it comes to OOD data?\n\n## Citations\n\nSome citations are not correct. For example, the paper \"Attention is all you Need\" is cited with year 2023 while the paper was published in 2017. Other papers are cited using their Arxiv version while they were published e.g. \"Extracting Latent Steering Vectors from Pretrained Language Models\" was published at ACL 2022. The citations should be corrected in the final manuscript.\n\n## Confusing Terminology\n\nThe paper employs a lot of new terminology which can be hard to follow. Appendix A helped me a lot but it is not referenced in the manuscript. I stumbled upon it by chance. Adding a reference to Appx A would help greatly."}, "questions": {"value": "How sensitive are some of the conclusions to the length of the sequences? Are there some heads that track depth on short sequences but not on large ones?\n\nAre there other ways to perform ablation of attention head? Perhaps forcing a token to only attend to its immediate neighbor is an interesting alternative that inhibits the network from using long-term dependencies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "98zICDjtD3", "forum": "0OxJ4mzaHB", "replyto": "0OxJ4mzaHB", "signatures": ["ICLR.cc/2026/Conference/Submission7837/Reviewer_o6W7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7837/Reviewer_o6W7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998758174, "cdate": 1761998758174, "tmdate": 1762919880991, "mdate": 1762919880991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}