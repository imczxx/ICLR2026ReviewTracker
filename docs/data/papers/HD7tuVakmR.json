{"id": "HD7tuVakmR", "number": 2553, "cdate": 1757143974641, "mdate": 1759898141214, "content": {"title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models", "abstract": "Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. We will release the code and models soon.", "tldr": "This paper proposes Quant-dLLM, a 2-bit weight-only post-training quantization framework for diffusion large language models that achieves state-of-the-art accuracy.", "keywords": ["Difussion-LLM", "Quantization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbc76d28442c8daf0351cbc31f928c92799e5611.pdf", "supplementary_material": "/attachment/f67ae67f5b77038983dc4fb4091305fd02b765c5.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Quant-dLLM, a post-training quantization (PTQ) framework tailored for diffusion-based large language models (dLLMs) under an extreme 2-bit weight-only budget. The authors propose three key components: 1) Masked Calibration Simulation (MCS) to align calibration with timestep-dependent masking; 2)Data-aware Any-order Quantizer (DAQ) to reconstruct weights using multi-binary matrices with row-column scaling; 3)Adaptive Blockwise Mixed Precision (ABMP) to allocate varying precision across blocks under a strict 2-bit average.\nExtensive experiments on recent dLLMs (LLaDA, Dream) show that Quant-dLLM significantly outperforms strong baselines (GPTQ, GPTAQ, Slim-LLM) across general QA, math, and code tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, with detailed algorithms and clear exposition. The authors commit to releasing code and models.\n2. Quant-dLLM shows improvements of challenging tasks like math and code. Ablation studies verify the effectiveness of each module.\n3. MCS directly tackles the distributional mismatch caused by masked denoising in dLLMs, a unique challenge not present in autoregressive LLMs."}, "weaknesses": {"value": "1. Missing Baselines: The paper focuses on 2-bit quantization and lacks classical comparison methods [1] [2]. \n2. Incremental Novelty: The core ideas, multi-binary approximation and mixed-precision allocation, are extensions of prior work (e.g., Slim-LLM). The authors should clearly articulate the technical distinctions and why these ideas are nontrivial to adapt to dLLMs.\n\n[1] QuIP: 2-Bit Quantization of Large Language Models With Guarantees\n\n[2] DB-LLM: Accurate Dual-Binarization for Efficient LLMs"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sxEcjKxTG5", "forum": "HD7tuVakmR", "replyto": "HD7tuVakmR", "signatures": ["ICLR.cc/2026/Conference/Submission2553/Reviewer_hDfx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2553/Reviewer_hDfx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750759585, "cdate": 1761750759585, "tmdate": 1762916280204, "mdate": 1762916280204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an ultra low bit PTQ method Quant-dLLM, which aims at addressing the specific characteristics in DLLM quantization including calibration, sensitivity distribution and low-bit quantization optimization. Extensive comparison and ablation results demonstrate its effectiveness."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "By considering the unique characteristics of DLLMs, the authors identify the shortcomings of existing LLM quantization methods in this context and conduct targeted explorations to overcome them. As one of the pioneering studies in this direction, this work offers meaningful insights and guidance for subsequent research."}, "weaknesses": {"value": "1. The paper requires improvements in writing, as it lacks a logical flow and several parts are inadequately explained, causing confusion for the readers.\n2. The paper introduces block-wise bitwidth allocation but lacks a clear definition of what constitutes a *block*. As far as I understand, it should refer to the same concept as *group* in group-wise quantization, rather than a transformer block. This should be made consistent throughout the paper.\n3. The importance matrix is unstructured and is related to the scaling factors and bitwidth allocation. If this information is required during inference, such as identifying the corresponding weights for the scaling factor when GEMM, the additional memory overhead introduced by the importance matrix will be non-negligible. For instance, an importance matrix could increase the average bitwidth of the weights by 1bit. The authors should provide a more detailed explanation of this aspect."}, "questions": {"value": "1. The paper focuses on the 2-bit quantization but lacks specific 2-bit quantization baselines, such as QuIP[1] and PBLLM[2], which would provide a more comprehensive comparison.\n2. In addition to accuracy, the authors should also provide memory usage and runtime statistics for the quantization process. Efficiency is an important factor that determines the broader impact of the proposed method\n\n---\n\n> [1] Chee J, Cai Y, Kuleshov V, et al. Quip: 2-bit quantization of large language models with guarantees[J]. Advances in Neural Information Processing Systems, 2023, 36: 4396-4429.\n\n>[2] Shang Y, Yuan Z, Wu Q, et al. Pb-llm: Partially binarized large language models[J]. arXiv preprint arXiv:2310.00034, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xTI1wgCk03", "forum": "HD7tuVakmR", "replyto": "HD7tuVakmR", "signatures": ["ICLR.cc/2026/Conference/Submission2553/Reviewer_ayMd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2553/Reviewer_ayMd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880897864, "cdate": 1761880897864, "tmdate": 1762916279976, "mdate": 1762916279976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Quant-dLLM, a 2-bit, weight-only PTQ pipeline designed explicitly for diffusion LLMs (dLLMs). It has three parts: first,  a masked calibration simulation (MCS)  that creates a synthetic dataset for the calibration of the quantizer appropriate for dLLM, seconds, a data-aware any-order quantizer (DAQ) inspired by DB-LLM that represents weights as a sum of row–column–scaled binary matrices, and finally, a mixed-precision approach inspired by Slim-LLM that allocates orders $K \\in \\{1,2,3\\}$ per block under a strict 2-bit average budget. Across LLaDA and Dream series models, Quant-dLLM outperforms strong 2-bit weight-only PTQ baselines (GPTQ, GPTAQ, Slim-LLM) on general, math/science, and code tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a well-motivated PTQ paper that clearly identifies the idiosyncrasies of diffusion LLMs (dLLMs) and proposes a data-aware combination of PTQ and mixed-precision quantization to aggressively compress models under a strict 2-bit average precision budget. The approach is conceptually elegant and technically well-executed, addressing a real gap between autoregressive and diffusion-style inference.\n\nThe results show consistent improvements across all benchmarks compared to existing low-bit PTQ baselines (GPTQ, GPTAQ, Slim-LLM), including on challenging math and code tasks. The paper is carefully presented, with clear motivation, ablation studies, and sound empirical analysis. Overall, this is a strong, mature piece of work that meaningfully advances post-training quantization for a new class of models."}, "weaknesses": {"value": "* Limited analysis of runtime/latency and kernel costs. Quant-dLLM reports a favourable model size (Table 3) but lacks end-to-end latency/throughput on full models and a cost breakdown for DAQ/ABMP kernels. Reporting wall-clock metrics on common hardware and the proportion of time in quant ops would help assess deployability\n\n* Sensitivity to hyperparameters is only partially explored:  λ (importance mask weight), 3σ thresholding, group/block sizes, or order K choices beyond the \\{1,2,3\\} scheme."}, "questions": {"value": "* Section 4.4: any discussion or latency/throughput? Are there any costs associated with the quantisation method that would affect deployment, e.g. additional kernels, etc?`\n\n* Ablation studies: I would like to see some ablation studies (even in the appendix) on the effect of the quantization block size or the effect of $\\lambda$\n\n* Line 320: regarding the bit relocation strategy. Is this inspired by another mixed-precision work (in which case, please cite). Have you tried less-heuristic, more systematic alternatives for reallocation?\n\n* Line 184:  “Leading to inaccurate quantization statistics …” Do you have measurements (e.g., per-timestep activation histograms, KL/TV distances) that quantify the mismatch between AR-style calibration and masked-denoising activations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eTrDUvQTMK", "forum": "HD7tuVakmR", "replyto": "HD7tuVakmR", "signatures": ["ICLR.cc/2026/Conference/Submission2553/Reviewer_xcW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2553/Reviewer_xcW1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994770273, "cdate": 1761994770273, "tmdate": 1762916279336, "mdate": 1762916279336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes 2-bit weight-only quantization methods tailored for masked diffusion language models (MDLMs). To address the limitations of GPTQ on MDLMs, it introduces three techniques: MCS, which corrects statistics during masked denoising; DAQ, which matches (WX) instead of (W) by using data/activations (X) during quantization; and ABMP, which assigns 3-bit to top-K sensitive blocks and 1-bit to bottom-K (best around 5–10%). The improvement on standard benchmarks is significant."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The method effectively prevents GPTQ from breaking down at 2-bit extreme quantization. Accuracy remains high enough to maintain, to some extent, performance on complex tasks such as math and coding, where 2-bit GPTQ mostly fails. Empirical memory saving exceeds 4× for an 8B MDLM."}, "weaknesses": {"value": "* Comparisons are limited: no low-rank adapters, QAT, hybrid weight+activation quantization.\n* The writing needs overall improvement and simplification.\n* The naming of sub-methods should be improved: there are too many acronyms, and several are grammatically awkward (missing nouns, missing hyphens, missing capitalization after hyphens, “order-**K**”, Quantiz**ation**, etc.). The lowercase “d” in dLLM is also confusing. It is recommended to consult an LLM.\n* The extra “=0” at the end of each function should be removed when compiling the algorithm environment.\n* The full-precision model is missing in the math and coding bar plots.\n* Although significantly better than GPTQ, the performance drop is still notable, leaving room for improvement.\n* Empirical latency/throughput is not reported.\n* K is hand-tuned but not heuristic.\n* Activation statistics are not reported, so the effect of the correction is invisible."}, "questions": {"value": "* I wonder if SVDQuant [1] (with native low-bit CUDA kernels, activation quantization) could be combined with the proposed method?\n* Does the distribution mismatch in MDLMs arise from the increasing number of “MASK” tokens over timesteps? If so, could this be partially resolved by [2]?\n\n[1] Li, et al. “SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models.” ICLR 2025. [arXiv:2411.05007](https://arxiv.org/pdf/2411.05007).\n\n[2] Deschenaux, et al. “Partition Generative Modeling: Masked Modeling Without Masks.” [arXiv:2505.18883](https://arxiv.org/pdf/2505.18883)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7ANpWWfYGh", "forum": "HD7tuVakmR", "replyto": "HD7tuVakmR", "signatures": ["ICLR.cc/2026/Conference/Submission2553/Reviewer_skef"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2553/Reviewer_skef"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185966586, "cdate": 1762185966586, "tmdate": 1762916279215, "mdate": 1762916279215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper correctly highlights the timestep-dependent masking in dLLMs and the accumulated denoising-step error, both of which invalidate classic AR-LLM PTQ assumptions (page 1–2). This motivation is strong and well supported by experimental trends."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The DAQ method builds on RC-scaled binarization and extends it to multi-order binary compositions (Algorithm 2, page 5).\nStrong points:\nData-aware objective reformulation using second-moment statistics (Eq. 4).\nImportance mask using 3\\sigma outlier detection (page 5) is computationally cheap.\nClosed-form alternating updates (Eqs. 8–9) are well derived.\nMulti-order fitting of residuals is natural and preserves binary-friendly execution.\nThe ablations (Table 2c) show DAQ contributes the largest accuracy jump.\n\nAcross 5 dLLMs and 7 tasks, the improvement over GPTQ/GPTAQ/Slim-LLM is consistent and large (Table 1, page 7).\n\nPerformance remains robust even on math and code tasks, where low-bit AR PTQ collapses (Figure 3, page 8)."}, "weaknesses": {"value": "The 3σ rule (page 5) is heuristic. Can the authors give a intuition or analysis on:\n-stability of \\DELTA across calibration seeds,\n- sensitivity of performance to \\sigma threshold or \\lambda,\n- whether importance correlates with diffusion-step Jacobians or outlier distributions\n\nGroup size = 128 is used universally. No analysis on: effect of block size, potential misalignment between block boundaries and meaningful weight structure."}, "questions": {"value": "Can the authors comment on sacalability? i.e. scaling to larger models (≥30B) and comment on the generality claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W1D11ZUJUx", "forum": "HD7tuVakmR", "replyto": "HD7tuVakmR", "signatures": ["ICLR.cc/2026/Conference/Submission2553/Reviewer_dsVf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2553/Reviewer_dsVf"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2553/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744972105, "cdate": 1763744972105, "tmdate": 1763744972105, "mdate": 1763744972105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}