{"id": "uHEaVkj8I3", "number": 8441, "cdate": 1758083645017, "mdate": 1763708501368, "content": {"title": "LAP: Fast $\\textbf{LA}$tent Diffusion $\\textbf{P}$lanner with Fine-Grained Feature Distillation for Autonomous Driving", "abstract": "Diffusion models have demonstrated strong capabilities for modeling human-like driving behaviors in autonomous driving, but their iterative sampling process induces substantial latency, and operating directly on raw trajectory points forces the model to spend capacity on low‑level kinematics, rather than high‑level multi-modal semantics. To address these limitations, we propose $\\textbf{LA}$tent $\\textbf{P}$lanner (LAP), a framework that plans in a VAE-learned latent space that disentangles high-level intents from low-level kinematics, enabling our planner to capture rich, multi-modal driving strategies. We further introduce a fine-grained feature distillation mechanism to guide a better interaction and fusion between the high-level semantic planning space and the vectorized scene context. Notably, LAP can produce high-quality plans in $\\textbf{one single denoising step}$, substantially reducing computational overhead. Through extensive evaluations on the large-scale nuPlan benchmark, LAP achieves $\\textbf{state-of-the-art}$ closed-loop performance among learning-based planning methods, while demonstrating an inference speed-up of at most $\\mathbf{10\\times}$ over previous SOTA approaches. Project website: https://anonymous.4open.science/w/Latent-Planner/.", "tldr": "We propose a latent diffusion planning framework for autonomous driving.", "keywords": ["diffusion planning", "autonomous driving"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5fbfed46044d0318ed3211bdd5e63c587510791.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents LAP (LAtent Planner), a novel planning framework for autonomous driving. The core idea is to first use a Variational Autoencoder (VAE) to learn a disentangled latent space that separates high-level strategic intents from low-level kinematic details. A latent diffusion model is then trained in this compact space to generate plans. The authors also introduce a \"fine-grained feature distillation\" mechanism to enhance the interaction between the planner's semantic representation and the vectorized scene context. The method is shown to achieve state-of-the-art (SOTA) closed-loop performance on the nuPlan benchmark while being highly efficient, capable of generating plans in a single denoising step."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel and elegant architecture. Applying diffusion models to a learned latent space for planning is a promising direction. The conceptual decoupling of high-level strategy and low-level control is well-motivated and addresses a key challenge in end-to-end planning.\n\n2. The claimed SOTA performance on a large-scale, closed-loop benchmark like nuPlan is impressive. This provides strong validation for the proposed framework and its practical effectiveness.\n\n3. A major contribution is the model's ability to operate in a single denoising step. This directly tackles the primary drawback of diffusion models (high latency) and makes the approach far more viable for real-time applications in autonomous driving."}, "weaknesses": {"value": "1. The \"fine-grained feature distillation\" is a key component, but its exact architectural details are slightly underdeveloped in the main text. A more detailed diagram or explanation in the appendix would be beneficial.\n\n2. The paper's premise relies on the VAE's ability to achieve disentanglement. While the strong downstream results imply this is successful, the paper would be more compelling with a brief qualitative analysis (e.g., latent space interpolation) to visually demonstrate this property.\n\n3. The relationship between the \"single-step\" inference and the \"multi-modal\" nature of the planner could be made more explicit. A brief discussion on how plan diversity is (or is not) preserved in this fast-inference mode would be helpful."}, "questions": {"value": "1. Could you please elaborate on the single-step inference mode? How does it maintain the ability to generate diverse, multi-modal plans, which is a key benefit of diffusion models? Or does it produce a single, high-quality \"mean\" trajectory?\n\n2. Can you provide more implementation details on the feature distillation module? What design choices were most critical to its success?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z3UbLk7BGE", "forum": "uHEaVkj8I3", "replyto": "uHEaVkj8I3", "signatures": ["ICLR.cc/2026/Conference/Submission8441/Reviewer_MsQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8441/Reviewer_MsQH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812457954, "cdate": 1761812457954, "tmdate": 1762920331059, "mdate": 1762920331059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LAtent Planner (LAP), a latent diffusion framework for motion planning in autonomous driving. It first trains a Trajectory VAE to learn a compact latent space disentangling high-level semantics from low-level kinematics, and then performs conditional diffusion in this latent space. To bridge the gap between the latent semantic space and the vectorized perception features, the authors introduce a fine-grained feature distillation module, using features from a teacher diffusion planner as guidance. LAP claims to significantly improve both planning quality and inference efficiency, achieving state-of-the-art closed-loop scores on the nuPlan benchmark with up to 10× faster inference than existing diffusion-based planners."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Motivation clarity: The paper articulates a clear motivation — diffusion planners suffer from high latency and focus too much on low-level trajectory details. Planning in latent space is a logical response.\n- Efficiency gains: The reported inference speedup (10× faster) is notable and relevant for real-time deployment in driving systems."}, "weaknesses": {"value": "- The paper claims that operating in latent space improves semantic modeling, but does not provide meaningful analysis of the learned latent representations (e.g., clustering by intent, diversity metrics). The VAE–diffusion interface is treated as a black box.\n- Closed-loop performance improvements over Diffusion Planner are marginal (≤1–2%), which may fall within the noise margin of nuPlan’s stochastic simulator."}, "questions": {"value": "- How stable is the VAE–diffusion training pipeline? Does the VAE reconstruction error correlate with downstream planning performance?\n- How does the model handle OOD (out-of-distribution) scenarios where latent-space priors may fail to represent unseen semantics?\n- Can the authors provide evidence that the latent space truly captures strategic semantics (e.g., intent categories) rather than just being a compressed waypoint representation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iKJYsDbBLV", "forum": "uHEaVkj8I3", "replyto": "uHEaVkj8I3", "signatures": ["ICLR.cc/2026/Conference/Submission8441/Reviewer_iCPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8441/Reviewer_iCPG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813925407, "cdate": 1761813925407, "tmdate": 1762920330698, "mdate": 1762920330698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose to learn planning task in latent space, disentangles  high-level intents from low-level kinematics, to capture rich, multi-modal driving strategies. By designing a two stage diffusion-based planner, this paper achieves good performance and inference speed-up."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* A trajectory VAE is proposed to learn a semantic latent space which disentangles high-level strategic semantics from low-level kinematic execution.\n* A latent diffusion model is designed to learn planning task in high-level semantic latent space, and a feature distillation method is proposed to bridge the gap between semantic space and vectorized scene perception.\n* The whole framework, thanks to the two-stage design, achieves 10x speed-up than baseline."}, "weaknesses": {"value": "* Though the paper claims the planning should be learned in a high-level semantic, the reason is not well described in the paper(only the visualization result in Fig.3 seems not enough)， making the motivation not clear enough.\n* The inference speed is fast, but more details are needed in the paper, e.g. model size, FLOPS.\n* First replace low-level planning space with high-level semantic space, but then introduce another feature distillation module to align these two spaces, makes the framework complicated and seems unnecessary."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wQeZ19Yqfi", "forum": "uHEaVkj8I3", "replyto": "uHEaVkj8I3", "signatures": ["ICLR.cc/2026/Conference/Submission8441/Reviewer_dMwV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8441/Reviewer_dMwV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898494854, "cdate": 1761898494854, "tmdate": 1762920330255, "mdate": 1762920330255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear AC and reviewers,\n\nWe thank all reviewers for their valuable time and insightful feedback. Following your suggestions, we have added new experiments, analyses, and explanations to better demonstrate the advantages of LAP from multiple perspectives. Please refer to our [updated paper](https://openreview.net/pdf?id=uHEaVkj8I3), where all changes are highlighted in blue. We will also open-source our code so that the community can further explore trajectory planning in the latent space.\n\nWe hope our responses address your concerns. If you have any further questions or would like additional clarifications, we are more than happy to provide them and continue improving the paper.\n\nBest regards,  \nAuthors of Submission 8441"}}, "id": "yjHNntK2vy", "forum": "uHEaVkj8I3", "replyto": "uHEaVkj8I3", "signatures": ["ICLR.cc/2026/Conference/Submission8441/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8441/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission8441/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763663751544, "cdate": 1763663751544, "tmdate": 1763663751544, "mdate": 1763663751544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}