{"id": "a9dngZLqGS", "number": 8011, "cdate": 1758051536355, "mdate": 1759897814772, "content": {"title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning", "abstract": "We propose **FROST**, an attention-aware method for efficient reasoning. \nUnlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. \nMethodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. \nTheoretically, FROST preserves and enhances the model’s reasoning capacity while eliminating outliers at the sentence level. \nEmpirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-oss-20B), outperforming state-of-the-art methods such as TALE and ThinkLess.\nNotably, FROST achieves an average **58.72%** reduction in token usage and a **10.64%** improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm $\\lVert \\mathbf{x} \\rVert_{\\infty}$\nby **15.97%** and the average kurtosis by **91.09%** compared to the base model.", "tldr": "", "keywords": ["Efficient Reasoning", "Attention Outlier", "Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f65cf0d68ff7294bc00d8d07f2b76a5409980833.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel training method that introduces a softmax-based activation function to suppress the attention weights of outlier tokens while preserving important reasoning paths.\nExperimental results demonstrate that the proposed approach outperforms existing baselines and can be effectively applied to strong reasoning models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is simple yet effective, significantly reducing token usage compared to prior approaches.\n- The theoretical analysis seems solid and support for the design and interpretation of the method.\n- Reproducibility is high — the authors provide full code and detailed implementation information."}, "weaknesses": {"value": "**[W1] Restricted experimental scope.** Despite the interesting ideas, both the analyses and experiments are conducted on a limited number of models. In particular, the observations are primarily focused on the Phi-reasoning model, making the study somewhat specialized and reducing the general applicability of the findings.\n\n**[W2] Limited discussion of related work.** There are already recent studies [1, 2] analyzing internal attention patterns in reasoning models. A deeper discussion comparing this work with those, and clarifying its novelty relative to them, would strengthen the contribution. Notably, the analysis of the attention distribution around the end of thinking token (Figure 3) appears similar to [1].\n\n[1] Choi et al., Think Clearly: Improving Reasoning via Redundant Token Pruning, EMNLP 2025 (Findings) \n\n[2] Cai et al., R-KV: Redundancy-aware KV Cache Compression for Reasoning Models, NeurIPS 2025"}, "questions": {"value": "**[Q1]** Please verify whether the proposed observations and improvements generalize to other reasoning models (e.g., R1-distill models).\n\n**[Q2]** In Figure 2, why were layers 0, 30, and 39 and specifically the first and last heads selected for visualization? A quantitative analysis across all layers and heads would make the claim more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cgrm5b839b", "forum": "a9dngZLqGS", "replyto": "a9dngZLqGS", "signatures": ["ICLR.cc/2026/Conference/Submission8011/Reviewer_oTRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8011/Reviewer_oTRF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657522928, "cdate": 1761657522928, "tmdate": 1762920015078, "mdate": 1762920015078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FROST, a novel method designed to improve the efficiency and accuracy of Large Reasoning Models (LRMs). The authors identify a key problem in current LRMs: the generation of \"reasoning outliers,\" which are defined as uncritical, irrelevant, or redundant steps in a reasoning chain. These outliers increase computational cost (token usage) and can negatively impact the final answer's accuracy.\n\nThe core of FROST is a simple yet effective architectural modification: replacing the standard Softmax function in the self-attention mechanism with Softmax_1. This function has the property of aggressively suppressing small input values (driving low attention weights toward zero) while preserving large ones. By doing so, FROST prunes the influence of outlier sentences during generation. The authors combine this modification with a lightweight Supervised Fine-Tuning (SFT) process to adapt the model to the new attention dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary strength lies in its empirical results. The method achieves a compelling combination of significantly reduced token usage while simultaneously improving accuracy across multiple benchmarks and models. This is a strong and desirable outcome for any efficiency-focused technique.\n- The approach is methodologically simple and elegant: swapping an activation function and performing a short SFT run. This makes the method practical and easily reproducible.\n- The supplementary experiments are valuable. The ablation study comparing Softmax1 with other activation functions (Table 2) effectively supports the authors' design choices, and the analysis of outlier metrics like kurtosis and infinity norm (Table 3) provides concrete evidence that the method is working as intended."}, "weaknesses": {"value": "- My main concern is the limited novelty of the core mechanism. The paper frames the use of the Softmax1 function as a key contribution for removing \"reasoning outliers.\" However, this exact function and its properties for suppressing outlier/low-value attention scores were previously discussed in other contexts, notably in Evan Miller's 2021 blog post \"Attention is Off by One.\" While applying this function to reasoning chains is new, the underlying technique for attention modification is not, which should be more clearly acknowledged.\n- The paper introduces \"reasoning outliers\" as sentences with low attention and negligible contribution. While the concept is intuitive, the method for identifying them seems to be implicitly handled by Softmax1 rather than through an explicit detection step. This makes the framing feel a bit like a post-hoc justification for using a pre-existing technique."}, "questions": {"value": "- The paper uses the maximum infinity norm and average kurtosis as metrics to quantify the presence of attention outliers. Have the authors considered or compared other statistical measures for identifying outlier activations? Why were these specific metrics chosen, and are they considered optimal for this task?\n- The results show a significant reduction in token count. Could you provide a qualitative example or analysis of what kind of reasoning steps are typically pruned? Are they mostly redundant self-corrections, or does the model sometimes remove steps that could be considered part of a valid, albeit longer, reasoning path?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rfy1FCqswr", "forum": "a9dngZLqGS", "replyto": "a9dngZLqGS", "signatures": ["ICLR.cc/2026/Conference/Submission8011/Reviewer_ej9v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8011/Reviewer_ej9v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720579859, "cdate": 1761720579859, "tmdate": 1762920013896, "mdate": 1762920013896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to improve LLM’s mathematical reasoning capabilities by suppressing less important tokens. Specifically, it adopts the softmax1 function, which is less prone to long-tailed distribution. Theoretical proof is provided to validate the usefulness of the approach on sentence-level pruning. Experimental results on several datasets demonstrate the advantages of the method in efficient token usage and enhanced performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) It is an important topic to develop efficient LLMs that adaptively reduce computational overheads based on inputs.\n\n(2) The paper draws several observations on the attention characteristics of reasoning models, which facilitates understanding on their underlying decision-making process.\n\n(3) The proposed method shows generalizability across different models and datasets."}, "weaknesses": {"value": "(1) Prior studies (e.g., Xiao et al, 2024, denoted as [ref1] in the remaining review) have already studied the use of softmax1 activation in developing efficient LLMs. Adopting the same methods on a specific reasoning scenario introduces rather limited technical contributions. In addition, the observations in the paper are also similar to previous ones, for instance, Figure 2 is similar to Figure 7 in [ref1], and [ref1] also pointed out the focus on specific tokens in deeper layers (initial tokens vs think token in this paper).  \n\n(2) The paper highlights different characteristics of reasoning in Section 3 (e.g., progressive refinement and reasoning traces), nevertheless, I found the proposed method disconnected from these observations. For instance, how does softmax1 affect the attention distribution in different layers? Does softmax1 enhance or reduce the progressive refinement? Figure 4 only shows the theoretical analysis of softmax1 on deep layers, does it hold true in practical experiments? What kinds of tokens does softmax1 activation prune out?\n\n(3) Softmax1 only suppresses the contribution of tokens with lower weights, instead of pruning it from the computational graph, thus there may not be actual reduction of computational overhead.\n\n(4) The method requires supervised finetuning to replace softmax with softmax1 in pretrained models. This could be relatively impractical for large-scale experiments, and may also affect the generalizability of pretrained models (due to finetuning on specific datasets)."}, "questions": {"value": "(1) Please justify the technical contribution of the proposed studies over prior ones using softmax1 activation.\n\n(2) How does the observation in Section 3 motivate the use of softmax1? \n\n(3) How does incorporating softmax1 affect the attention behaviors across layers for both inference and during training?\n\n(4) How would the proposed method reduce the computational overhead in practice?\n\n(5) While I am aware of the focus on mathematical reasoning in this paper, the requirements of supervised fine-tuning makes me concerned about the potential issues on model generalization. For instance, does fine-tuning on mathematical dataset hurt the model performance on other reasoning tasks? How would training models with softmax1 from scratch work, compared to post-hoc fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yTZujXCpKo", "forum": "a9dngZLqGS", "replyto": "a9dngZLqGS", "signatures": ["ICLR.cc/2026/Conference/Submission8011/Reviewer_ncQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8011/Reviewer_ncQk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773228057, "cdate": 1761773228057, "tmdate": 1762920012901, "mdate": 1762920012901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LRMs typically generate a large number of noncritical steps, inevitably introducing inefficiencies and potential inaccuracies. The authors propose FROST, a method for accelerating and strengthening inference. They first introduce the concept of inference outliers: noncritical steps with low attention weights and low entropy. Technically, the authors use softmax1 to detect and eliminate inference outliers. Preliminary experiments effectively reveal the problem FROST aims to address, and comprehensive experiments demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The authors clearly articulate and demonstrate the problem of outliers in reasoning through premise experiments, while also providing a simple yet highly effective solution. This is easy to understand and well-intentioned.\n2.The experimental results on multiple datasets are significant and compared with the state-of-the-art algorithms, convincingly demonstrating the effectiveness of FROST.\n3.In addition to basic performance experiments, the paper also includes a large number of validation experiments, such as using some metrics to try to quantify the performance of outlier removal."}, "weaknesses": {"value": "1.In Figure 4, the attention weight of S2 is also significantly compressed, while S3 is actually improved. I understand that S24 is the key reasoning path and has been improved. However, further analysis of S2 and S3 is necessary.\n2.The authors demonstrate in Table 3 that removing attention outliers increases the probability assigned to critical sentences. Token entropy is used as an indicator of criticality. While most experiments meet expectations, Entmax15 and Sparsemax exhibit unexpected performance. The authors offer no explanation. This is necessary to clarify whether removing inference outliers actually improves reasoning ability. Please try to provide possible explanations as to whether there is token entropy or some other fluctuation."}, "questions": {"value": "1.The x and y axes of all the graphs are unclear, especially in graph 2, where the y-axis of the two upper subgraphs is even all zero due to missing parts. Could you please make the graphs clearer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WYe7dhYhvv", "forum": "a9dngZLqGS", "replyto": "a9dngZLqGS", "signatures": ["ICLR.cc/2026/Conference/Submission8011/Reviewer_vRQV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8011/Reviewer_vRQV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973526640, "cdate": 1761973526640, "tmdate": 1762920011402, "mdate": 1762920011402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}