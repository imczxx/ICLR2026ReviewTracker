{"id": "fs2uDib85s", "number": 10572, "cdate": 1758176021045, "mdate": 1759897642710, "content": {"title": "Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning", "abstract": "A fine-grained data recipe is crucial for pre-training large language models (LLMs), as it can significantly enhance training efficiency and model performance. One important ingredient in the recipe is to select samples based on scores produced by defined rules, LLM judgment, or statistical information in embeddings, which can be roughly categorized into quality and diversity metrics. Due to the high computational cost when applied to trillion-scale token pre-training datasets such as FineWeb and DCLM, these two or more types of metrics are rarely considered jointly in a single selection process. However, in our empirical study, selecting samples based on quality metrics exhibit severe diminishing returns during long-term pre-training, while selecting on diversity metrics removes too many valuable high-quality samples, both of which limit pre-trained LLMs' capabilities. Therefore, we introduce DATAMASK, a novel and efficient joint learning framework designed for large-scale pre-training data selection that can simultaneously optimize multiple types of metrics in a unified process, with this study focusing specifically on quality and diversity metrics. DATAMASK approaches the selection process as a mask learning problem, involving iterative sampling of data masks, computation of policy gradients based on predefined objectives with sampled masks, and updating of mask sampling logits. Through policy gradient-based optimization and various acceleration enhancements, it significantly reduces selection time by 98.9% compared to greedy algorithm, enabling our study to explore joint learning within trillion-scale tokens. With DATAMASK, we select a subset of about 10% from the 15 trillion-token FineWeb dataset, termed FineWeb-Mask. Evaluated across 12 diverse tasks, this high-quality and diverse subset achieves significant improvements of 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model after pre-training with hundreds of billions of tokens, demonstrating its effectiveness.", "tldr": "", "keywords": ["LLM pre-training", "data selection", "mask learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fbec5c8ddb45d4d56ca9fbd50ba7e1c46f854358.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DATAMASK, an efficient framework for large-scale pre-training data selection by jointly optimizing for both quality and diversity. DATAMASK reframes data selection as a mask learning problem, using a policy gradient-based approach to learn an optimal mask distribution. Instead of exhaustive comparisons, it iteratively samples small data batches, computes joint metrics, and updates a global sampling probability for each data point. By applying DATAMASK to the 15 trillion-token FineWeb dataset, the authors created FineWeb-Mask, a high-quality and diverse 10% subset, which achieves significant improvements over baselines trained on the random subset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed DATAMASK algorithm is highly efficient and capable of handling data selection on large-scale datasets.\n2. Based on pre-training results, the resulting data subset demonstrates higher quality than other existing subsets."}, "weaknesses": {"value": "1. The necessity of the policy gradient algorithm is questionable. A simpler approach, such as directly assigning logit values based on each sample's quality and diversity score (evaluated on a small group), might achieve a similar effect.\n2. The assumption that sample selection probabilities are independent is a potential limitation. This modeling choice seems inconsistent with the goal of optimizing for diversity, which is inherently a set-level property that depends on the relationships between samples.\n3. The algorithm's performance is likely sensitive to the learning rate, yet a corresponding analysis is absent. A low learning rate risks insufficient differentiation among logits, approximating uniform sampling, whereas a high learning rate may lead to deterministic, greedy-like behavior. The effect of this hyperparameter on model performance remains unclear.\n4. The writing in some sections is not clear enough, for example, the explanation of the content in Figure 3."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2WFmwKVUgt", "forum": "fs2uDib85s", "replyto": "fs2uDib85s", "signatures": ["ICLR.cc/2026/Conference/Submission10572/Reviewer_NCCX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10572/Reviewer_NCCX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760538511735, "cdate": 1760538511735, "tmdate": 1762921842648, "mdate": 1762921842648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on the large-scale pre-training data selection for LLMs, which aims at optimizing the metrics for like data quality and data diversity. In this paper, the author point out that current open sourced dataset like FineWeb and DCLM rarely consider the data quality and diversity or the more metrics jointly in a single selection process, which leads to these problems based on their research: Selection based on quality metrics will leads to severe diminishing returns during long-turn pre-training, while the selection based on quality will remove to many high-quality samples, which limits the effect of pre-training LLMs. To solve this problem, this paper introduce DATAMASK: a new joint learning framework designed for large-scale pre-training data selection to simultaneously optimize multiple types of metrics (mainly focusing on quality and diversity). By interpreting the selection task as a mask learning problem and using multiple enhance technique, DATAMASK significantly reduces selection time by 98.9% compared with greedy algorithm. And in FineWeb dataset, they select a FineWeb-Mask subset, and achieves significant improvements of 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model after pre-training with hundreds of billions of tokens, demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novelty for the problem definition: The paper conceptualizes the large-scale data selection problem into a learnable mask optimization task and use policy gradient-based optimization and various acceleration enhancements to optimize the selection speed.\n\nStrong motivation and empirical analysis: The paper demonstrates the fundamental limitations of single-metric selection on large scale pre-training dataset, and use the visualizations to express the conflict of data quality and diversity that supports the central motivation.\n\nHigh engineering availability and scalability: Implementation on 15T FineWeb corpus shows the method’s availability in engineering. Decrease 98.5% time cost compare with greedy algorithm, enabling the studies in large-scale datasets shows the scalability more research."}, "weaknesses": {"value": "Limited methodological originality: The novelty is incremental, not a fundamentally new method. The framework of combination of Mask Learning and Policy Gradient is a direct application of Reinforce-style policy gradient to a combinatorial subset selection problem. Similar implementation have been used in: RL-based data pruning or sample selection (e.g., RLDataSampler, ICML 2022);Differentiable subset selection in vision and NLP (e.g., DPPNet, CVPR 2021; SubsetFormer, NeurIPS 2023).\n\nLack of fair on challenging baselines:All baselines are existing data recipes, but without comparison with recent learning-based selection frameworks like: DataComp-LM (Li et al., NeurIPS 2024) etc. Also missing the comparison with other learnable data selection methods, which weakens the claims of the effectiveness.\n\nLack of the explainability:Even the paper gives the heat map, the paper didn’t make qualitative analysis and visualization for the selected datas’ types, realms etc. \n\nWeak theoretical justification:The paper only gives the upgrade formula for optimization in the end of method, and then comes into the experiment part, without explanation for convergence, bias introduced by probabilistic relaxation or gradient variance."}, "questions": {"value": "1.What is the difference between DATAMASK and the classical reinforce-based subset selection? Please clarify the difference between traditional reinforce mechanism or active learning.\n2.Can the union optimization frame explains to more than two metrics? Like adding text toxic or the other metrics.\n3.Can you release more visualizations of selected samples? For example: distribution of domains, text lengths etc.\n4.How to confirm the stability for optimization? In the sampling upgrade with larger variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TVXZm6soXz", "forum": "fs2uDib85s", "replyto": "fs2uDib85s", "signatures": ["ICLR.cc/2026/Conference/Submission10572/Reviewer_nve6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10572/Reviewer_nve6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535742968, "cdate": 1761535742968, "tmdate": 1762921842309, "mdate": 1762921842309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies that existing pre-training data selection methods optimize quality or diversity separately, leading to either semantic redundancy or loss of valuable samples, which limits LLM performance. To address this, the authors introduce DATAMASK, a novel framework that jointly optimizes multiple metrics. Applied to FineWeb, DATAMASK produces FineWeb-Mask (1.5 trillion tokens), which demonstrates 3.2% and 1.9% performance improvements on 1.5B and 7B (MoE) models respectively. The work proves that balanced quality-diversity optimization is both computationally feasible and empirically superior for large-scale pre-training data selection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- There is an inherent trade-off between generality and specificity that has not been considered in existing related work. \n- I appreciate the fomalized approach that provides users with a more principled way of data curation. I believe such techniques are particularly valuable in increasing the sample efficency during pre-training and ultimately driving down cost. \n- The transparent cost breakdown helps others estimate whether datamask is a useful (and affordable) technique for their individual use cases."}, "weaknesses": {"value": "- When arguing about pre-training the proposed dataset the FineWeb-Mask rather small for fully training 7/8B parameter (dense) or even larger models. SOTA 8b dense models are typically trained on 10T+ tokens. I could see the dataset to be applicable for what sometimes is refered to stage-two pre-training, i.e., showing documents to a model that contain desirable information for later post-training steps that require versatility. Exploring how well the specificity-/generality-balance introduced through datamask would make a useful addition to the paper. \n- The paper misses a recent work on a pre-trainign dataset that is also derived from fineweb. Even though, it's based on heuristics, the data processing pipeline makes an effort to carefully balance specificity and generality [1]. \n\n**Sources:**\n\n[1] GneissWeb: Preparing High Quality Data for LLMs at Scale, Gohari et al., 2025"}, "questions": {"value": "- What does \"optimized score\" refer to in the G ablation study? I understand that G is used to keep the computational costs in check but doesn't it also implicitly change the optimization objective later on because of the dependency among samples in a group? \n- Out of curiosity, is there any notable performance benefits for post-training when using datamask over vanilla fineweb? \n- Will you open-source the data curation recipe (code) if the paper gets accepted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dax7xxEBHS", "forum": "fs2uDib85s", "replyto": "fs2uDib85s", "signatures": ["ICLR.cc/2026/Conference/Submission10572/Reviewer_iTj3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10572/Reviewer_iTj3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903855605, "cdate": 1761903855605, "tmdate": 1762921841809, "mdate": 1762921841809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper purpose DATAMASK, a new framework for large-scale data selection method based on policy-gradient mask learning framework. It targets jointly optimizes the quality and diversity in trillion-token constraints. The authors use differentiable optimization instead of probabilistic masks, and run experiments on FineWeb datasets, achieving a 98.9% reduction in selection cost while not sacrificing the performance. Evaluated across 12 diverse downstream tasks, this subset achieves significant performance gains of 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model, demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper addresses a critical problem, namely the trade-off between quality and diversity in LLM pre-training data selection. The idea of treating data selection as a mask learning problem and using policy gradients for optimization is novel. It moves beyond traditional greedy selection strategies, offering a unified learning-based approach.\nQuality: The paper is supported by rigorous empirical validation, including large-scale experiments on trillion-token datasets. Ablation studies are provided to analyze the impact of different diversity metrics and hyperparameters (e.g., λ, G). The method is evaluated on 12 diverse downstream tasks, demonstrating consistent and significant improvements.\nClarity: The paper is well-structured and clearly written. The core mechanism is illustrated with formulas and algorithmic descriptions. The use of visualizations (e.g., t-SNE plots, heatmaps) helps show key insights effectively.\nSignificance: The proposed method has practical value for improving LLM pre-training efficiency and performance. It opens up a new direction for joint optimization of multiple data metrics, which could influence future data curation paradigms. The released FineWeb-Mask dataset is a valuable resource for the community."}, "weaknesses": {"value": "1.\tPartial Ablation of Core Parameters\nWhile the paper ablates diversity metrics, the balancing hyperparameter (λ), and the group size (G) in policy gradient estimation, it lacks systematic exploration of other key hyperparameters, such as the learning rate, the number of update epochs, and the initialization strategies for logits. This omission limits the understanding of the method's robustness and sensitivity to its full configuration.\n2.\tInsufficient Accessibility and Clarity in Figures\nFigure 2 (t-SNE) uses colors that are not colorblind-friendly, and the legend is small with ambiguous labels. Figure 3 (score evolution) lacks units on the x-axis and error bars/confidence intervals, making it difficult to interpret convergence behavior and stability.\n3.\tMissing Comparison with Scalable Learned Selection Paradigms\nComparisons are made primarily against heuristic baselines. A comparison with other learned selection methods that are scalable to trillion-token datasets (if available) is needed to better situate DATAMASK's advantages. For methods limited to small-scale data, this could be framed as future work instead of a current limitation."}, "questions": {"value": "1.Beyond computational constraints, the key hyperparameters like the learning rate and number of update epochs are excluded from ablation. Was wondering if it is due to observed insensitivity in preliminary experiments. It will be great to analyze their expected impact on DATAMASK’s performance and convergence?\n2. It seems valuable to compare DATAMASK with learned data selection strategies that are scalable to trillion-token datasets. For methods limited to small-scale data, could you elaborate on why they are not suitable for direct comparison, or outline how DATAMASK might outperform them at scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3jCWqE86Cj", "forum": "fs2uDib85s", "replyto": "fs2uDib85s", "signatures": ["ICLR.cc/2026/Conference/Submission10572/Reviewer_5FG7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10572/Reviewer_5FG7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990676082, "cdate": 1761990676082, "tmdate": 1762921841255, "mdate": 1762921841255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}