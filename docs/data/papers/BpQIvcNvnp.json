{"id": "BpQIvcNvnp", "number": 24243, "cdate": 1758354538519, "mdate": 1759896774695, "content": {"title": "On Why Form Shapes Reasoning: Structuring Latent Program Networks with Category-Theoretic Constraints", "abstract": "Human reasoning is inherently structured: we perceive, compose, and abstract patterns to make sense of the world. Following Kant’s view that cognition imposes structure on experience, we ask how neural networks can acquire structured, compositional reasoning. We present a category-theoretic formulation of Latent Program Networks (LPNs), neural architectures that represent programs as continuous latent vectors optimized at test time. We treat latent transformations as categorical morphisms and introduce differentiable constraints enforcing associativity, identity, and closure, thereby shaping the latent space into a compositional system without explicit symbolic rules. On structured grid-transformation tasks, these constraints significantly improve compositional generalization, latent alignment, and interpretability. Our results demonstrate that category-theoretic structure can be imposed on latent representations to induce compositional reasoning in neural networks.", "tldr": "When form is imposed categorically on latent programs, reasoning emerges compositionally — enabling neural networks to generalize more systematically.", "keywords": ["Compositional Reasoning", "Category Theory", "Latent Program Networks"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa918e233fcab0871f673e654de8efd957e9083f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes adding several inductive biases from cathegory theory in form of additional losses in the training phase of LPNs to improve their overall compositional abilities. The authors perform evaluations on grid transformation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I very much like the idea of this work and find it valuable for the community. The paper is refreshingly well structured and written making it easy to follow what is being introduced and why. The experimental evaluations also show strong evidence for the author's claims."}, "weaknesses": {"value": "Overall, there are mainly a few clarifications. The main weakness, and I hate to be that reviewer, is that the experimental evidence though strong is only from one specific task/domain. It would be great if the authors could provide more experimental evidence of the overall findings. E.g. scale up the data in terms of grid size, look at more x-step compositions, or different kind of transformations. Or indeed use the the ARC challenge as in the original LPN paper.\n\nIt would be important to specify what the model is in ll 194.\n\nI think an important baseline would be, instead of training via the losses, create a training set that explicitly represents the targeted categorical constraints.\n\nll 290: \"Structured latent composition encourages semantic regularity: traversals in latent space yield coherent transformations\" --> this is really interesting, but is there a way to specifically see this in terms of results? Right now I am missing evidence for this claim.\n\nMinor: Providing some name or pseudonym for the proposed approach might make it more intuitive rather than \"Full\" in Tab. 2 or \"Method/Model\" as section header for section 3. \n\nAlso an overview figure to visuallize the intuition behind the training setup would be good.\n\nAlso please fix the table overflow of Tab. 2.\n\n\nOverall, if the authors can provide justifications or additional material regarding these issues I would definitely consider raising my score as I find the paper valuable enough."}, "questions": {"value": "Tab. 1: why do the authors provide this table if only three fo these metrics are actually used in the evaluations?\n\nTab 2.: Why were these particular metrics chosen from the set in Tab. 1? What does Closure tell us exactly? Maybe a formula would be good for this.\n\nll 260-264: Interesting that identity is so important. What is the intuition behnd this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k7TSz0VtAX", "forum": "BpQIvcNvnp", "replyto": "BpQIvcNvnp", "signatures": ["ICLR.cc/2026/Conference/Submission24243/Reviewer_hvgG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24243/Reviewer_hvgG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658283534, "cdate": 1761658283534, "tmdate": 1762943014630, "mdate": 1762943014630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors attempt to impose structure on the latent vectors learned in an autoencoder. They do this as a means of improving compositional reasoning. They use category theory to implement the structure."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- the authors are addressing an interesting issue of symbolic structure within neural latent representations\n- the authors took inspiration from human reasoning\n- what was presented in the paper was well written, clear, easy to follow"}, "weaknesses": {"value": "My main concerns for this paper are that it acts as though modern LLMs and video models don't exist, the specifics of the presented model/approach lacked detail/clarity, and there were multiple claims that lacked supporting evidence (see below).\n\n- lines 035-036 need a citation to defend the specific claim that NN's lack explicit structure. A few citations that potentially dispute that claim. Geiger et al. Finding alignments between interpretable causal variables and distributed neural representations, 2023. and Griffiths et al. Whither symbols in the era of advanced neural networks?, 2025.\n- lines 076-077 seem to be ignoring the fact that modern LLMs are able to compositionally generalize in many tasks\n- for the first paragraph in section 3.1, it might help to give a concrete example of why we might care about latent program networks. Provide a concrete problem that they are capable of solving or used to address. Do you maybe mean that they're optimized at \"training\" time, instead of \"test\" time? More elaboration would be helpful. You have the space for it.\n- in the Tasks section (lines 187-191) how does the model know what type of transformation to perform? Is it just supposed to match the statistics of the dataset?\n- what is a composed latent (line 211)? how is it constructed?\n- lines 256-257 need supporting evidence. why can't the model memorize the solution?\n- what does the model consist of? is it a multi-layer perceptron? a convolutional neural network?\n- difficult to interpret results with so much ambiguity surrounding the model"}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "35axtYGdMN", "forum": "BpQIvcNvnp", "replyto": "BpQIvcNvnp", "signatures": ["ICLR.cc/2026/Conference/Submission24243/Reviewer_AUD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24243/Reviewer_AUD1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797939576, "cdate": 1761797939576, "tmdate": 1762943014063, "mdate": 1762943014063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper drops the test-time latent optimization of Latent Program Network and adds a few regularization terms as losses to the latent \"program\" variables. The additional regularization terms are to train a composition operator of two \"latent programs\" following associativity and identity constraints. The model is evaluated on a few customized synthetic simple grid transformation programs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies models with latent programs."}, "weaknesses": {"value": "The model is only evaluated on simple synthetic grid transformation programs, not even ARC-1, and the model does not perform perfectly on those synthetic tasks.\n\nThe paper lacks details about models (like architectures of encoders and decoders) and datasets (e.g., the synthetic training & val datasets)."}, "questions": {"value": "* How does the model perform on ARC-1 and ARC-2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rMlMG2JY8L", "forum": "BpQIvcNvnp", "replyto": "BpQIvcNvnp", "signatures": ["ICLR.cc/2026/Conference/Submission24243/Reviewer_hwzr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24243/Reviewer_hwzr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045093642, "cdate": 1762045093642, "tmdate": 1762943013621, "mdate": 1762943013621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a modification Variational Autoencoders (VAEs) that include differentiable constraints on the latent based on category theory. The authors claim this method to be related to Latent Program Networks (LPNs). The proposed method is tested on the ARC challenge. Overall many details are missing."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea or regularizing latent variables with general category constraints is interesting."}, "weaknesses": {"value": "Some aspects of the presented method are unclear and details seem missing.\n\n1 .the methods are referred as \"Latent Program Networks (LPNs)\" but it also states \"latent programs are not directly predicted; instead, they are optimized at test time via gradient descent to minimize reconstruction error\" and \"In this work, we do not adopt test-time latent optimization.\". This seems like a big departure from LPNs which would be otherwise just a regular VAE with additional constraints.\n\n2. what architectures and sizes are used to parametrize the networks (both theta and psi ones)\n\nalso the experimental setup lacks needed hyperparameter sweeps (also, see questions below). \n\n1. No weights for the different constraints are explored, a and a single value of regularization weight beta is used. A reasonable setup would at least compare the full method and the baseline under a sweep of beta values on a held out set different from the ARC test set.\n\n2. results seem to be relatively fragile, with performance collapsing with small changes over the \"Full\" method. In this setting the parameter sweep is even more relevant."}, "questions": {"value": "the authors state \n\n> We selected the KL target empirically based on preliminary runs that balanced latent capacity and regularization\n\n1. I understand this as the value of beta=0.003 was determined based on initial runs. Was this on some held out set different from the ARC test? is this value set based on the \"Full\" run? optimal beta may be different for the different experiments particularly the \"Sequential VAE (free-run)\" which has less constraints and therefore less overall regularization. \n\n2. What is the behavior of the methods in table 2 under a sweep of the beta parameter."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rwrPffGxF7", "forum": "BpQIvcNvnp", "replyto": "BpQIvcNvnp", "signatures": ["ICLR.cc/2026/Conference/Submission24243/Reviewer_B8tF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24243/Reviewer_B8tF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104245444, "cdate": 1762104245444, "tmdate": 1762943013183, "mdate": 1762943013183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}