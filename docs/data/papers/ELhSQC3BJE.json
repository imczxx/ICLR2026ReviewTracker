{"id": "ELhSQC3BJE", "number": 2943, "cdate": 1757305047423, "mdate": 1759898118087, "content": {"title": "Holistic Token Efficient in Speculative Decoding: Post-use & Pre-cut", "abstract": "Large language models (LLMs) deliver strong generative performance but suffer from high inference latency. Speculative Decoding (SD) accelerates inference by allowing a fast draft model to propose tokens, which are then verified in parallel by a larger target model. While SD provides lossless acceleration while preserving identical generation quality, its key challenge lies in draft token efficiency: ensuring that as many drafted tokens as possible are converted into useful tokens in the final output. We present a holistic token-efficient SD strategy built on two complementary mechanisms. \\textit{Ex-post utilization (Post-use)} employs a token cache to recycle and reuse useful drafts in subsequent forward passes. \\textit{Ex-ante reduction (Pre-cut)} adaptively controls draft length, preventing overproduction when the marginal benefit falls below the cost. Together, these mechanisms both reuse what has been produced and eliminate what should not be produced. Experiments show 2.52–3.23$\\times$ overall speedup over auto-regressive decoding and over 20\\% higher token utilization than vanilla SD methods.", "tldr": "A drop-in mechanism to improve draft token utilization in Speculative Decoding for accelerate LLM inference losslessly", "keywords": ["large language models", "efficient decoding", "speculative decoding"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a63f228027763fef1eef44c36e77bdc1fb88d155.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a strategy to improve the token efficiency of Speculative Decoding. The proposed method consists of\n1. Token cache to recycle useful tokens from previous generation steps\n2. RL-based controller to adaptively shorten the draft length when the predicted benefit is low.\n\nExperiments show that this approach achieves a 2.52–3.23× speedup over autoregressive decoding and improves token utilization by over 20% compared to vanilla SD methods across various models and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Application of Reinforcement Learning for Adaptive Drafting: The paper introduces an RL-based controller with a Q-table to make dynamic \"stop\" decisions for drafting. While previous adpative drafting methods do not use RL.\n2. Complete Framework: The framework combines Post-use (reusing generated tokens) and Pre-cut (preventing wasteful generation) to address token efficiency from two different angles."}, "weaknesses": {"value": "1. Insufficient Differentiation of the Post-Use Mechanism from Prior Work: The paper describes its Post-use token cache with policies for ranking, admission, and eviction (lines 98). However, the high-level description is conceptually very similar to the \"phrase candidate pool\" used by the Ouroboros baseline.  The paper does not provide sufficient detail on its cache management algorithms to clearly differentiate this component from existing token reuse techniques, weakening the claim of novelty for the Post-use mechanism.\n2. Lack adaptive-length speculative sampling baselines."}, "questions": {"value": "1. The hyperparameter of the RL controller is not disclosed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4tzLVu0sdy", "forum": "ELhSQC3BJE", "replyto": "ELhSQC3BJE", "signatures": ["ICLR.cc/2026/Conference/Submission2943/Reviewer_Qyrp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2943/Reviewer_Qyrp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760593047637, "cdate": 1760593047637, "tmdate": 1762916452249, "mdate": 1762916452249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a holistic token-efficient Speculative Decoding (SD) strategy aimed at addressing the high inference latency in Large Language Models (LLMs) by maximizing both the Mean Accept Length (MAT) and draft utilization (Util), which are the primary determinants of SD speedup. This strategy functions as an on-the-fly, training-free SD plug-in that achieves lossless acceleration by combining two complementary mechanisms: Ex-post utilization (Post-use) and Ex-ante reduction (Pre-cut). The Post-use mechanism employs a token cache to recycle and reuse \"USEFUL\" tokens—those successfully verified by the target model—in subsequent draft and target model forward passes, thereby reducing the number of auto-regressive steps and increasing the yield from already generated drafts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a holistic token-efficient SD strategy that combines two complementary mechanisms: Ex-post Utilization (Post-use), which reuses already generated content through token caching, and Ex-ante Reduction (Pre-cut), which adaptively controls draft length to avoid generating unnecessary tokens. \n2. The method directly addresses the key problem of token wastage in SD while aiming to maximize token utilization and maintain high mean accepted length.\n3.  Extensive experiments verify the effectiveness and generality of the proposed method."}, "weaknesses": {"value": "1. The Pre-Cut strategy uses the product of draft token confidences as a predictor of expected accepted tokens, but the paper does not explain why this metric is preferable to alternatives (e.g., weighted average). A theoretical rationale or empirical comparison would strengthen the motivation for this choice.\n2. In Table 4, the token utilization rate for the CodeLlama 7B → 70B pair is only 6.23%, which is drastically lower than all other results (typically 64–74%). If correct, this requires a dedicated explanation in Section 6.5 to clarify the cause of such extreme inefficiency.\n3. The text on lines 54–55 (e.g., “Token Cache (Post-Use)”) appears directly above Figure 1 in a visually awkward position. Moving this text below Figure 1 would improve readability and professionalism.\n4. Section 4 should explicitly reference the correct subfigures in Figure 2. The discussion of the re-generated draft token ratio (near line 235) should cite Fig. 2(b), and the confidence product vs. acceptance rate correlation (near line 239) should cite Fig. 2(c)."}, "questions": {"value": "1. Adding a high-level architectural overview figure early in the paper would greatly improve clarity and help readers understand the contributions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rEwqXWcS5P", "forum": "ELhSQC3BJE", "replyto": "ELhSQC3BJE", "signatures": ["ICLR.cc/2026/Conference/Submission2943/Reviewer_P8gs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2943/Reviewer_P8gs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975526314, "cdate": 1761975526314, "tmdate": 1762916452055, "mdate": 1762916452055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes holistic token-efficient speculative decoding (SD) by utilizing token cache and adaptively choosing draft length for verification using trajectory confidence. The author aim at utilizing already drafted tokens rather than improving the draft models or darfting itself which is another axis in SD. The result shows improved speed-ups compared to the vanila SD methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper investigates another-axis for improving speculative decoding other than improving acceptance rate or better training recipe for drafter.\n* The proposed method is training-free and without extra computations.\n* Presentation is clear and ablation are properly studied."}, "weaknesses": {"value": "**Novelty of the method** : The proposed method basically combines two methods for better utilization of the drafted tokens. However, token reusing is already investigated in Ouroboros [1] as mentioned by the authors, difference between [1] and proposed token-caching strategy is not clearly stated and Table 1 shows only marginal improvements of the proposed method compared to Ouroboros even with pre-cut method. For pre-cut method iteself, there are many training-free methods for adaptively choosing draft length ([2], [3]). While confidence-based q-learninig method is interesting, comparison with other methods for adaptive draft-length is laccking and correlation between confidence and trajectory quality is already well-investigated in the literature so its not entirely novel ([4]). \n\n**Lack of experiments** : The algorithm should be tested on more datasets to prove its efficacy. Moreover, proposed method should be tested with the latest SD methods like in EAGLE-3 [5] (Appendix E only provides only theoretical analysis which might be far from the actual improvements). I recommend authors to test the method on other datasets in Spec-Bench [6].\n\n**Real serving scenarios** : I think utilizing token-caching has critical problem in real-world serving scenario where quries are heterogeneous and non-stationary in-nature. Moreover, it would be good if proposed method is experimented on batched inference."}, "questions": {"value": "Questions \n\n* For the experiment, can authors elaborate on why using greedy-decoding only is for fair comparison (ln 362)? (temperature sampling with SD is also lossless)\n\n* In ln 440, how's the Q_init is initialized? what's the prior knowledge here. If it contains some information in warm-up stage, i think it highly varies for diffrent domains. \n\n* Have you investigated other measures than naive product of confidence (as in Eq. 6) for Q-learning?\n\n[1] (Zhao et al.) Ouroboros: Generating longer drafts phrase\nby phrase for faster speculative decoding\n\n[2] (Zhang et al.) Draft Model Knows When to Stop: Self-Verification Speculative Decoding for Long-Form Generation\n\n[3] (Agrawal et al.) AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models via an Entropy-based Lower Bound on Token Acceptance Probability\n\n[4] (Fu et al.) Deep Think with Confidence"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rSW0QSUG7i", "forum": "ELhSQC3BJE", "replyto": "ELhSQC3BJE", "signatures": ["ICLR.cc/2026/Conference/Submission2943/Reviewer_8ZKg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2943/Reviewer_8ZKg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180102543, "cdate": 1762180102543, "tmdate": 1762916451850, "mdate": 1762916451850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This core contribution of this paper is a holistic strategy to improve token efficiency in speculative decoding through two complementary mechanisms. \n\n1. Post-use discussed a strategy to use cache that stores drafted tokens that were verified by the target model. Those tokens will be utilized in later forward passes to avoid regenerating recurring segments. This is a clever way as an alternative to Prompt Lookup Decoding where n-gram is provided by the context itself.\n2. Pre-cut learns a lightweight RL controller that sends binary decisions (continue/stop) based on recent acceptance statistics. \n\nOverall, I think this is a good paper that empirically unifies adaptive draft length decision and token caching but the weaknesses put it in a borderline position."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear, which is to maximize MAT and draft-token utilization. \n2. The post-use cache design preserves losslessness and offers a concrete path to multi-token gains per forward pass without training a new drafter. \n3. The empirical section covers multiple model families and tasks and reports end-to-end tokens/sec with clear speedup ratios, not just MAT."}, "weaknesses": {"value": "1. The novelty relative to prior token-reuse and adaptive drafting work is not significant, but still I appreciate the empirical efforts for proving the compatibility of both speculative decoding designs. \n2. Evaluation is relatively less comprehensive. Only HumanEval, MT Bench and GSM8K results are presented. Consider established benchmarks used in the community such as Spec-Bench.\n3. Lack of comparison against other adaptive draft length methods, such as SpecDec++ [1] and BanditSpec [2], especially considering the latter one also frames draft length selection in the RL framework.\n\n[1] SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths\n\n[2] BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms"}, "questions": {"value": "1. Can you provide component-wise wall-clock (Md forward, Mt forward, cache fetch/verify, rejection recompute, controller step) and memory overheads? This could help reviewers evaluate the source of the gains.\n\n2. Can you report seeded mean & stdev for Tables 1 to 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ru7SD0nfsj", "forum": "ELhSQC3BJE", "replyto": "ELhSQC3BJE", "signatures": ["ICLR.cc/2026/Conference/Submission2943/Reviewer_fr2t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2943/Reviewer_fr2t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762228455211, "cdate": 1762228455211, "tmdate": 1762916451342, "mdate": 1762916451342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}