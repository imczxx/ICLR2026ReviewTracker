{"id": "USyGD0eUod", "number": 13642, "cdate": 1758220301998, "mdate": 1759897422829, "content": {"title": "Automated Interpretability Metrics Do Not Distinguish Trained and Random Transformers", "abstract": "Sparse autoencoders (SAEs) are widely used to extract sparse, interpretable latents from transformer activations. We test whether commonly used SAE quality metrics and automatic explanation pipelines can distinguish trained transformers from randomly initialized ones (e.g., where parameters are sampled i.i.d. from a Gaussian). Over a wide range of Pythia model sizes and multiple randomization schemes, we find that, in many settings, SAEs trained on randomly initialized transformers produce auto-interpretability scores and reconstruction metrics that are similar to those from trained models. These results show that high aggregate auto-interpretability scores do not, by themselves, guarantee that learned, computationally relevant features have been recovered. We therefore recommend treating common SAE metrics as useful but insufficient proxies for mechanistic interpretability and argue for routine randomized baselines and targeted measures of feature 'abstractness'.", "tldr": "SAEs trained on random transformers achieve similar automated interpretability scores to trained models, showing that more targeted measures are needed.", "keywords": ["Sparse Autoencoders", "SAEs", "LLMs", "interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc874b236d423f35dabecdaff1458a8041c08366.pdf", "supplementary_material": "/attachment/312332fef14cd223b494fc1e67d79a917e08eb5b.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates automated interpretability metrics for SAEs (“fuzzing” and “detection”, explained variance, cosine similarity, L1, cross-entropy loss) trained on activations from randomly initialized (untrained) LLMs, and evaluates whether these SAEs produce similar scores compared with properly trained transformers and SAEs. The authors find that these commonly used automated interpretability metrics for SAEs are similar for conventionally trained transformers and randomly initialized transformers. The authors conclude that these metrics are therefore insufficient proxies for interpretability, and recommend token distribution entropy as a more consistent means of quantifying the fidelity and “abstractness” of SAE-extracted features.\n\n**Verdict**: This paper is clearly written and has broad relevance for the mechanistic interpretability community. While the paper represents a contribution to the mechanistic interpretability field, particularly in the scope of SAEs, the weaknesses listed below should be addressed in order to give more weight and proper context to the results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- S1: Since SAEs are very widely used tools for interpreting the internals of transformer models, the subject of investigation is of interest to the mechanistic interpretability community as a whole. \n- S2: The paper is largely well-written, with clear explanations and motivations for each experimental design choice. The Related Work section is extensive.\n- S3: A wide range of model sizes are tested (using the Pythia suite); the results and conclusions are therefore more likely to be generalizable."}, "weaknesses": {"value": "Major:\n- W1: Based on Figure 2, token distribution entropy seems to only separate trained versus randomized transformers in models >1.0B parameters. The authors don’t appear to comment on this, but greater discussion of the specific results of this token distribution entropy metric would be appreciated.\n- W2: Somewhat lost in this paper is the original purpose of automated interpretability metrics for SAEs: to find out if the SAE is properly trained, and to monitor its training. It would be a useful comparison to train an SAE on a normal transformer, but perhaps with clearly poor hyperparameter choices. In particular looking at “fuzzing” and “detection” auto-interp methods on this SAE would be interesting. \n- W3: Section 4 feels only loosely connected to the previous results, and the conclusions drawn are limited. It is too focused on the trained versus random transformers, which is less interesting than the main subject of the paper. As in W2, it would be useful to tie these results back into the original purpose of the SAE interpretability metrics.\n\nMinor:\n- W4: “Latents” is a term used frequently throughout the paper to refer to activations in the SAE latent space but is never properly defined. A short phrase defining this term at the beginning of the paper would help readability."}, "questions": {"value": "- Q1: Why does token distribution entropy only reasonably separate trained from random transformer models >1.0B parameters?\n- Q2: How do “improperly trained” SAEs (for properly trained transformers) perform on these automated interpretability metrics?\n- Q3: How do the results in Section 4 relate to the automated interpretability metrics explored in the previous sections?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lP8XwvfoP0", "forum": "USyGD0eUod", "replyto": "USyGD0eUod", "signatures": ["ICLR.cc/2026/Conference/Submission13642/Reviewer_P4Ex"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13642/Reviewer_P4Ex"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666237671, "cdate": 1761666237671, "tmdate": 1762924218925, "mdate": 1762924218925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors train SAEs on LLMs and on LLMs with randomized weights, apply EleutherAI's automated interpretability pipeline which uses an LLM to find monosemantic explanations for every feature given activating examples, and showed that both score similarly high.\n\nImportantly, this does NOT mean that SAEs fail to learn real, interesting computational features. Rather, the authors show that SAEs trained on random weights can still recover some simple structure and score highly. The paper's main message is simple: Don't use existing auto-interp scores as proof for SAE quality but they also go into great depth to explain why SAEs trained on random weights can still be sparse and contain apparently monosemantic features.\n\nI think the importance of the paper might be a bit limited because auto-interp scores are not usually used to benchmark SAEs; they are used to check the quality of the generated explanation. They never made sense to be used as SAE metrics as they don't measure the quality of an SAE feature."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the title: the title perfectly describes the paper's main finding\n- the methods are elegant and sound\n- the randomization, control, and training configuration (dataset, n tokens, buffer size, k, expansion, etc) is well-chosen.\n- entropy is a great metric to quantify the hypothesis that random SAE's features activate for token identities\n- the authors go into great depth and effort to present a hypothesis, experiments, and evidence that shows why SAEs trained on random weights may still exert, preserve, or amplify superposition."}, "weaknesses": {"value": "The primary reason why I think this paper is great but not exceptional is a possible limitation in its usefulness (that I'm happy to discuss during the rebuttal period). There could be two reasons for the papers main results: (1) SAEs or auto-interp methods are sus and we should not trust them, and (2), SAEs trained on random weights learn trivial features that are easy to guess and cause the high auto-interp scores. For example, SAEs in layer 0 (or even later) might learn 1 feature for every token (there should be roughly as many features as vocab size, and the embedding likely contains most of the variance and inference can be cut off reasonably well), making it very easy to find a high-scoring feature explanation. Real SAEs might learn features like \"I'm active at \"the\" so I want to predict a noun and I want to predict the object of the previous sentence\" which would be a great, generalizing feature but very hard to guess for the LLM explainer from a small feature dashboard. In fact, the paper argues that the second case is likely true, as they show that the entropy of real SAE features is much greater, especially in later layers where we expect and typically see abstract features.\nIf (2) was the reason for the paper's results, the take-away boils down to \"having great auto-interp scores doesn't mean you have a great model or SAE\". However, although auto-interp is important and used downstream for exploration or circuit tracing, I'm not aware that researchers use this metric to evaluate the quality of their SAE features, limiting the applicability of this paper. If I'm wrong here, could the authors link to papers where auto-interp scores have been used to benchmark SAE features?\n\nA stronger conclusion would be that because random-model SAEs achieve high auto-interp scores, SAEs may not discover meaningful, real features. But I'm not sure if this is true (probably not?).\n\n\nMinor:\n- Why was zero-ablation chosen instead of mean-ablation?\n- I read this paper thinking the authors' goal is to show that SAEs don't learn meaningful features. Although the paper never posits this conclusion and is precise in the abstract, I think that naturally, people reading and skimming the paper will assume that this is the central take away even if this wasn't explicitly written in the paper. It would be great if it could be made more clear what the author's interpretation of the results is and what it is not."}, "questions": {"value": "Do you think that auto-interp fails to distinguish random vs learned because (a) auto-interp works on real SAEs but the random SAE's features are simple (low entropy) and thus easy to guess so they get the same score or (b) real SAEs learn complex features that auto-interp methods fails to recover and they are thus as bad as random SAE's. (If b, I'd like to see more evidence for this)\n\nThroughout the paper, you assume that auto-interp scores are used as an SAE quality metric. However, auto-interp scores measure how well the explanation is that the LLM explainer found, and not how great the SAE feature is it measures. Are you saying that despite this, folk are using auto-interp scores as an SAE quality metric? Could you pinpoint to papers that use this metric like that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L4d6agZ9qz", "forum": "USyGD0eUod", "replyto": "USyGD0eUod", "signatures": ["ICLR.cc/2026/Conference/Submission13642/Reviewer_pFem"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13642/Reviewer_pFem"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979475716, "cdate": 1761979475716, "tmdate": 1762924218501, "mdate": 1762924218501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how SAE quality metrics, particularly a few automatic interpretability scoring methods, are effected by SAEs being trained on randomly initialised transformers. They show that similar auto-interpretability results are found for SAEs trained on random and trained transformers. They also provide a toy model of superposition in random networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The paper provides the most in-depth study of the relationship between auto-interpretability scoring methods and whether / how networks were randomly initialised that I am aware of. The use of a networks randomly initialised except for the embeddings space was a good choice and adds to the interestingness of the results since much of the interpretability of SAE features may come from association with and between tokens. \nQuality: The paper is comprehensive - comparing trained modles, randomly initialized models, randomly initialized except-for-embeddings models, step-0  models and \"control\" (which breaks association between tokens) models. They provide and analyse a toy model set up as well providing further insights there. \nSignificance: Token distribution entropy appears to capture something unique about trained models - and relates to previous observations in OpenAI's SAE paper (figure 22, showing 25% latents fire on a small set of tokens, and 75% fire on a wider variety or tokens). I think there is some significance in showing this property might distinguish SAEs trained on random / non-random networks. However, clearly some features should be fairly token aligned so in terms of scoring the \"interpretability\" of a given feature this metric may not be particularly useful."}, "weaknesses": {"value": "- Significance: While feature dashboards and auto-interpretability explanations are commonly used with SAEs, to my knowledge auto-interpretability scoring methods aren't generally considered to be very useful or meaningful (moreover, many SAE quality metrics are not particularly useful in practice). So while the result that these metrics may not be capturing something meaningful seems well supported - the significance is not particularly clear.\n\n- Soundness: The paper discusses auto-interpretability scoring in the aggregate for the most part - providing no examples of how interpretability scores associated with feature dashboards / max-activating examples. The results are less compelling because we don't see specific examples from each of the SAE-model combinations and the interpretability scoring metrics."}, "questions": {"value": "What would help improve this work?\n- Significance: I think the most interesting / significant thing about this paper is the token distribution entropy result. More detailed analysis of features which have high token distribution entropy and the kinds of properties which they capture may speak to the kinds of non-trivial features which are more difficult to automatically interpret or annotate. The current implications of the paper are somewhat but not very interesting. If the paper were to propose different SAE architectures (eg: inclusion of Matrioshka SAEs might help), this may provide actionable insights.\n- Presentation: It is odd to focus on the validity of auto-interpretability metrics and yet not conduct more analysis of feature activation patterns / kinds of features. The obvious implication of features on random networks being interpretable is that there may be interpretability illusions associated with SAE feature analysis (results we'd see on random models) - but the paper doesn't really go into this except some feature max activating examples in the appendix where we can't see the feature activation patterns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GiiLIdd8Js", "forum": "USyGD0eUod", "replyto": "USyGD0eUod", "signatures": ["ICLR.cc/2026/Conference/Submission13642/Reviewer_7Tpb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13642/Reviewer_7Tpb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992817951, "cdate": 1761992817951, "tmdate": 1762924218084, "mdate": 1762924218084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}