{"id": "Y8pXEF38EO", "number": 10182, "cdate": 1758163288203, "mdate": 1762941483097, "content": {"title": "Amortized-Precision Quantization for Early Exiting in Vision Transformers", "abstract": "Vision Transformers (ViTs) achieve state-of-the-art results across classification, detection, and segmentation, but their heavy computation hinders deployment on resource-constrained devices. Quantization is a common technique to improve efficiency, yet conventional approaches assume static inference and ignore the input-dependent utility of layers under dynamic strategies such as Early Exiting (EE). This mismatch leads to inefficient bit allocation: shallow layers may be over-provisioned while deeper exits, which dominate late-stage decisions, remain under-optimized. We introduce **Amortized-Precision Quantization (APQ)**, a new perspective that treats precision as a utilization-dependent resource, exposing depth‚Äìprecision and shallow-deep trade-offs. Building on APQ, we propose **Mutual Adaptive Quantization with Early Exiting (MAQEE)**, a bi-level optimization framework that jointly calibrates exit thresholds and reallocates bit-widths under risk control. We theoretically establish MAQEE's superiority over static quantization in dynamic inference, and empirically show that it reduces bit-operations by up to 95% while preserving accuracy, outperforming strong baselines by as much as 20% on ViT classification, detection, and segmentation benchmarks.", "tldr": "We introduce Amortized-Precision Quantization (APQ) and its implementation MAQEE, which adaptively allocate precision with early exiting in Vision Transformers, achieving 95%+ compute reduction while maintaining accuracy.", "keywords": ["Quantization", "Early Exiting", "Vision Transformers"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c9e78f1fc99f436b8f5481af6f99c949dd2b31ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Vision Transformers (ViTs) achieve strong performance across various computer vision tasks, yet their large parameter count and high computational cost make deployment on resource-constrained edge devices difficult.\nThis paper proposes Mutual Adaptive Quantization with Early Exiting (MAQEE) ‚Äî a compatibility-oriented framework that effectively integrates Quantization and Early Exiting (EE) to address this limitation.\nMAQEE resolves the trade-offs between depth‚Äìprecision and shallow‚Äìdeep that arise when combining quantization with dynamic inference. It adaptively modifies static inference paths based on input complexity, reducing computation while maintaining accuracy.\nThrough bi-level optimization, the framework searches for robust exit thresholds against quantization noise and dynamically reallocates bit-widths based on layer utilization and accumulated quantization distortion.\nThe proposed method achieves up to 95% reduction in BOPs with minimal accuracy loss, supported by four theoretical analyses (Theorems 1‚Äì4).\nOverall, this work introduces a new paradigm for efficient and robust ViT deployment under dynamic precision conditions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clarified Motivation:\nMAQEE explicitly identifies and formulates the mutual interference challenge between quantization and early exiting for ViT compression.\nTheoretical analyses (Theorems 1 and 2) mathematically formalize this issue, and the introduction of the Amortized-Precision Quantization (APQ) paradigm ‚Äî which allocates precision based on layer utilization ‚Äî is conceptually novel and potentially impactful for future research.\n\n - Robustness of the Framework:\nThe proposed bi-level optimization framework systematically separates the outer loop (exit-threshold optimization) and inner loop (bit allocation), presenting a clear and logically sound design.\nThe introduction of specific risk metrics ‚Äî Boundary Sensitivity Risk (BSR) and Quantization-Induced Drift (QID) ‚Äî provides a concrete and innovative approach to stabilizing dynamic quantization, effectively addressing the stated problem.\n\n - Experimental Superiority:\nThe experiments demonstrate that PTQ-based baselines (RepQ, ERQ, MPTQ) combined with EE often fail to reach the target accuracy (reported as N/A), while MAQEE achieves the lowest average exit depth (L) and BOPs across all tasks, maintaining the highest accuracy among all methods."}, "weaknesses": {"value": "- Lack of Comparison with QAT-based Methods:\nMAQEE includes a self-distillation recovery stage requiring ground-truth labels ùë¶  which introduces additional training cost compared to PTQ. Therefore, comparisons should also include Quantization-Aware Training (QAT) methods or QAT+EE approaches such as QuEE [1], ideally reported in the main result tables (Table 1 and Table 2).\n[1] Predicting Probabilities of Error to Combine Quantization and Early Exiting (QuEE), arXiv, 2024.\n\n - Limited Precision and Model Diversity:\nThe experiments mainly focus on 4-bit quantization (W4A4). Evaluations under lower (2-bit) or higher (6-bit / 8-bit) precision settings would provide a more complete understanding.\nSimilarly, additional results on smaller models (e.g., ViT-S, ViT-Tiny) and lightweight variants (e.g., DeiT, MobileViT) would enhance the generality of the claims.\n\n - Restricted Bit Range in MPQ:\nIn the baseline MPQ setup, the minimum and maximum bit widths are fixed at 3 and 5.\nTesting with a wider bit range would further highlight MAQEE‚Äôs advantage under highly dynamic inference paths.\n\n - Clarity of the Overall Framework:\nAlthough the mathematical formulation is solid, the overall pipeline of MAQEE is quite complex.\nAdding a high-level figure that visualizes the entire processing flow (outer and inner loop interactions) would help non-expert readers better understand the method."}, "questions": {"value": "The optimization superiority is mainly measured by theoretical metrics such as L (average exit layer) and BOPs.\nIs there a plan to evaluate MAQEE under real hardware constraints (e.g., GPU/NPU bandwidth and cache structure) that may affect actual latency and throughput?\n\nConsidering that modern GPUs support specific precision modes (e.g., INT8, INT4), how does MAQEE perform in practical settings such as 8/4 or 8/8 quantization?\nIs there any noticeable difference in real inference speed compared to static quantization?\n\nCould the authors provide an analysis of training cost?\nSince ViT-Base contains a large number of parameters, it would be valuable to report the additional computational overhead introduced by MAQEE‚Äôs bi-level optimization and self-distillation stages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EnQGiCwCCS", "forum": "Y8pXEF38EO", "replyto": "Y8pXEF38EO", "signatures": ["ICLR.cc/2026/Conference/Submission10182/Reviewer_56KL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10182/Reviewer_56KL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808580369, "cdate": 1761808580369, "tmdate": 1762921548219, "mdate": 1762921548219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank all reviewers for their constructive feedback.\nTo clarify, all hyperparameters in MAQEE are fixed and consistently applied across all models and tasks, with detailed configurations provided in the appendix. The quantization calibration is conducted only on a small calibration subset, followed by lightweight recovery training on a portion of the training data, ensuring that the overall optimization cost remains manageable. In the next version, we plan to extend the work with more comprehensive ablation studies, detailed analyses of training overhead, and experiments under varied quantization settings. \nGiven the current review timeline and the scope of necessary revisions, we have decided to withdraw the paper at this stage."}}, "id": "jQ5oEpnYuR", "forum": "Y8pXEF38EO", "replyto": "Y8pXEF38EO", "signatures": ["ICLR.cc/2026/Conference/Submission10182/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10182/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762938639449, "cdate": 1762938639449, "tmdate": 1762938639449, "mdate": 1762938639449, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work establishes a novel research direction by integrating model quantization with early exiting, proposing the Amortized-Precision Quantization paradigm, a dynamic inference-path quantization  framework. The authors introduce Mutual Adaptive Quantization with Early Exiting to optimize two critical trade-offs between quantization and early exiting at both global and local levels. Comprehensive theoretical analysis demonstrates the effectiveness and rationality of the proposed optimization. Applied to ViTs, this approach achieves an balance between accuracy and efficiency through coordinated quantization and early exiting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper formalizes the accuracy-efficiency trade-off inherent in integrating quantization with early exiting for Vision Transformers (ViTs), and proposes the Amortized-Precision Quantization (APQ) paradigm to shown its challenge.\n2. This paper shown the effectiveness and stability of the MAQEE optimization framework.\n3. Experimental results show that the method achieves superior accuracy compared to naive combinations of quantization and early exiting."}, "weaknesses": {"value": "1. The method introduces an excessive number of hyperparameters for optimization. Although theoretical guarantees are provided for MAQEE's stability, the proliferation of hyperparameters leads to sensitivity challenges in practical implementation and limits extensibility to other models or tasks. Furthermore, the absence of comprehensive ablation studies on these hyperparameters raises concerns that the reported performance may heavily depend on extensive parameter tuning.\n2. The computational cost of MAQEE's optimization process is not adequately addressed in the paper. This represents a significant consideration, as prolonged training requirements would substantially diminish the method's practical utility.\n3. The paper lacks individual ablation studies analyzing the contributions of MAQEE's constituent components. Without such analysis, the effectiveness of each optimization element remains questionable."}, "questions": {"value": "1. While the proposed method demonstrates effectiveness on the other tasks, its generalizability to other model architectures or vision tasks requires further investigation. What are the potential challenges in extending MAQEE to different domains or applications?\n2. Could the authors clarify the optimization procedure of MAQEE. Specifically, whether the components are optimized jointly or sequentially? Furthermore, is there any observed interference or conflict between the optimization objectives of different components?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6XwTUoDgqZ", "forum": "Y8pXEF38EO", "replyto": "Y8pXEF38EO", "signatures": ["ICLR.cc/2026/Conference/Submission10182/Reviewer_PBHV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10182/Reviewer_PBHV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816178120, "cdate": 1761816178120, "tmdate": 1762921547844, "mdate": 1762921547844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank all reviewers for their constructive feedback.\n\nTo clarify, all hyperparameters in MAQEE are fixed and consistently applied across all models and tasks, with detailed configurations provided in the appendix. The quantization calibration is conducted only on a small calibration subset, followed by lightweight recovery training on a portion of the training data, ensuring that the overall optimization cost remains manageable. In the next version, we plan to extend the work with more comprehensive ablation studies, detailed analyses of training overhead, and experiments under varied quantization settings. \n\nGiven the current review timeline and the scope of necessary revisions, we have decided to withdraw the paper at this stage."}}, "id": "8WQuCia34R", "forum": "Y8pXEF38EO", "replyto": "Y8pXEF38EO", "signatures": ["ICLR.cc/2026/Conference/Submission10182/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10182/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762938667096, "cdate": 1762938667096, "tmdate": 1762938667096, "mdate": 1762938667096, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes APQ, a quantization paradigm that aligns bit-width allocation with layer utilization in early exiting. To implement APQ, the authors design MAQEE, a bi-level optimization framework that jointly optimizes exit thresholds via a Boundary Sensitivity Risk metric, and per-layer bit-widths via Quantization-Induced Drift. Theoretical analyses demonstrate the mutual interference between quantization and early exits and prove MAQEE‚Äôs convergence. Experiments on multiple models and benchmarks show efficiency improvement while maintaining or exceeding baseline accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The method is well motivated and good framed.\n* The paper is well-structured and clear."}, "weaknesses": {"value": "* Experiment section needs to be significantly improved, for example, lacks of many important ablations including new introduced hyperparameters trade-off $\\alpha$ and $\\beta$, is the dataset selection sensitive, or is the initial bit distribution sensitive, how about the threshold grid? As the proposed method is somehow a little bit tricky, it‚Äôs very important to do comprehensive ablation study to further analyze the effectiveness.\n* Lack of the default setting of hyperparameters, learning rate, training epochs and more.\n* From my understanding, the process of quantization is extremely high, as it may need multiple iterations, and in each iteration, you need do a grid search (with a inner loop) and a fine-tuning step (distillation). Beside the better efficiency-accuracy trade-off after quantization, its important to provide more resources utilization details for quantization process, for an efficiency-related paper, these details are critical.\n* The results are reported without variance, its better to run multiple times to verify the effect of randomness."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e5cIMZR3uh", "forum": "Y8pXEF38EO", "replyto": "Y8pXEF38EO", "signatures": ["ICLR.cc/2026/Conference/Submission10182/Reviewer_YmZW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10182/Reviewer_YmZW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943453011, "cdate": 1761943453011, "tmdate": 1762921547391, "mdate": 1762921547391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}