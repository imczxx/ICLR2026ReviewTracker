{"id": "2AvjgGJg8U", "number": 24280, "cdate": 1758354888668, "mdate": 1759896772947, "content": {"title": "Regularization can make diffusion models more efficient", "abstract": "Diffusion models are one of the key architectures of generative AI. Their main\ndrawback, however, is the computational costs. This study indicates that the\nconcept of sparsity, well known especially in statistics, can provide a pathway to\nmore efficient diffusion pipelines. Our mathematical guarantees prove that sparsity\ncan reduce the input dimension’s influence on the computational complexity to that\nof a much smaller intrinsic dimension of the data. Our empirical findings confirm\nthat inducing sparsity can indeed lead to better samples at a lower cost.", "tldr": "Our mathematical guarantees prove that sparsity can reduce the input dimension’s influence on the computational complexity to that of a much smaller intrinsic dimension of the data.", "keywords": ["diffusion models", "Regularization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99ba9c22cad2a8c71ebf39566d65ffdfbbe82bfd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a regularization technique to accelerate diffusion models, with the core concept being the regularized denoising score-matching estimator (Equation 3). Based on this framework, the training efficiency of diffusion models is improved. The authors demonstrate the effectiveness of the method through both theoretical analysis and experimental validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A key strength of this paper lies in its theoretical rigor. The mathematical derivations of Theorem 1 are solid and clearly articulate the core conclusion. This high standard of rigor and clarity is also reflected in the mathematical sections and the explanation of the methods and motivations. Finally, the experimental results effectively validate this theoretical framework."}, "weaknesses": {"value": "**Major concerns**\n+ The TOY EXAMPLE in Figure 1 is confusing. The author states that “regularized denoising score matching predominantly adheres to the two-dimensional sub-manifold (along the Y and Z axes).” Can samples on both sides of the X-axis be effectively generated? Would this lead to a decrease in generated diversity?\n\n+ The paper's evaluation currently relies primarily on visual comparisons, which are insufficient on their own. As previously mentioned, the manuscript does not include common quantitative metrics for generative models, such as Precision and Recall. The authors should incorporate these (or other relevant) metrics to provide a more objective assessment of the method's performance, specifically regarding its ability to approximate the original data (fidelity, Precision) and generate diverse samples (diversity, Recall).\n\n> Improved Precision and Recall Metric for Assessing Generative Models. NeurIPS-2019\n\n+ The current diffusion models adopt the Flow Matching framework, which constructs different intermediate distributions and shifts the model's prediction target from a score function to a velocity field. Can the conclusions of this paper be generalized to this setting?\n\n> Flow Matching for Generative Modeling. arXiv:2210.02747\n\n**Minor concerns**\n+ The presentation of the figures could be improved. For instance, in Figures 2 and 3, enhancing the visual contrast between GT, baseline, and the proposed method would make the comparisons clearer and more impactful."}, "questions": {"value": "+ Regarding the practical implementation of equation 3, **it is unclear how the parameters are constrained to satisfy the requirements of equation  2**. Clarification on this mechanism is needed. If the author could provide pseudocode for the implementation, it would make the article more persuasive.\n\n+ Figure 5 shows relatively high FID values for CIFAR, while most diffusion models currently achieve FID values below 5. Could the authors explore more advanced network architectures to validate the method's generalizability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TRdMpyjKf7", "forum": "2AvjgGJg8U", "replyto": "2AvjgGJg8U", "signatures": ["ICLR.cc/2026/Conference/Submission24280/Reviewer_5MyW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24280/Reviewer_5MyW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760965288008, "cdate": 1760965288008, "tmdate": 1762943027752, "mdate": 1762943027752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how introducing l1-based regularization into score matching can make diffusion models more efficient, both theoretically and empirically. The authors assume that the true score function of the data distribution is approximately s-sparse, meaning that only a few dimensions matter for denoising. Under this sparsity assumption, they derive non-asymptotic bounds showing that the KL divergence scales with $s^2$ rather than $d^2$, and validate this through experiments on toy data and image datasets, where the regularized model achieves comparable or better quality with much fewer sampling steps."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Provides a clear theoretical link between sparsity regularization and improved non-asymptotic convergence, reducing dimensional dependence from $d$ to $s$.\n2. The proposed regularized score-matching objective is simple to implement and compatible with existing diffusion frameworks"}, "weaknesses": {"value": "The main theoretical guarantee hinges on the unverified assumption that the true score function is s-sparse, and given the experiments are limited to low-dimensional datasets, it remains unclear whether the same efficiency extends to large-scale diffusion models."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EJzrScx9Mf", "forum": "2AvjgGJg8U", "replyto": "2AvjgGJg8U", "signatures": ["ICLR.cc/2026/Conference/Submission24280/Reviewer_La1N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24280/Reviewer_La1N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961252150, "cdate": 1761961252150, "tmdate": 1762943027518, "mdate": 1762943027518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to accelerate score-based diffusion model inference by introducing sparsity regularization into the score function. Theoretically, the authors provide proofs supporting the feasibility of the approach; however, the experimental results do not sufficiently validate these claims. The paper would benefit from an expanded experimental section, removal of the “paper outline” section, and more compact figure layouts to better utilize space and improve readability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides detailed explanations for each assumption and theorem, offering clear intuition behind the proposed method. The theoretical derivations are comprehensive and rigorous, establishing a solid foundation for the concept of regularized score-based diffusion models."}, "weaknesses": {"value": "1. The paper does not provide code or implementation details, making it difficult to assess the practical feasibility of the method. Moreover, there is no analysis of the hyperparameters $s,r,\\kappa$, leaving the experimental section incomplete.\n\n2. From the presented results, the method only shows effectiveness on datasets with inherently sparse structures, such as MNIST. Even for MNIST, where sparsity is visually evident, the sparsity-inducing SGM struggles to generate high-quality samples with a small number of timesteps. In Figure 2 (right column), the generated images at T=50 and T=20 still contain noticeable noise, and this issue becomes even more pronounced in Figures 3, 4, and 5.\n\n3. Minor comment: The description of the diffusion process (Lines 29–34) should use consistent notation for the variable $x$ to avoid confusion."}, "questions": {"value": "1. The sparsity-inducing regularization requires solving the optimization problem in Eq. (3), which introduces additional computational overhead. Although the paper emphasizes inference-time acceleration, it is important to analyze how this optimization affects training time and training stability.\n\n2. Sparsity-inducing methods are typically applied for one of two purposes: (a) when genuine sparsity exists in the data or model structure with interpretable meaning, or (b) to improve computational efficiency in systems involving heavy matrix operations. However, diffusion processes do not inherently involve such high-order matrix computations. The authors should better justify the motivation. Why does enforcing sparsity in the score function improve inference efficiency, and under what conditions is this beneficial?\n\n3. Adding sparsity constraints might reduce the model’s generalization ability to fit data distributions, especially for high-dimensional modalities such as images or text. How does the proposed method balance sparsity with the ability to capture complex data distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "melGDkYWmS", "forum": "2AvjgGJg8U", "replyto": "2AvjgGJg8U", "signatures": ["ICLR.cc/2026/Conference/Submission24280/Reviewer_fKXu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24280/Reviewer_fKXu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128711494, "cdate": 1762128711494, "tmdate": 1762943027293, "mdate": 1762943027293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes adding a type of L^1 regularization to the score-matching objective in diffusion, and analyzes the iteration complexity of sampling from this model, and compares it to the standard score-matching objective -- it shows that convergence can be achieved using a number of iterations depending on the sparsity of the score function, rather than its ambient dimension. The paper shows empirical results on a synthetic task, MNIST and FashionMNIST to support its theoretical claims, and to support the assumption that score functions are sparse in practice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The addition of an explicit L^1 regularization to the score-matching objective, and a guarantee on sampling iteration complexity depending on the sparsity of the score functions is interesting -- it's further interesting that the new objective does seem to give reasonable results on (very) small-scale experiments. The fact that the new objective provides a reasonable approximation to the score even in the absence of sparsity is a plus. The paper itself is easy to read, and the contributions are laid out clearly.\n\nOverall, I like the contribution of this paper at a conceptual level, and the small-scale/initial empirical results are intriguing."}, "weaknesses": {"value": "From a technical perspective, the theory seems to largely be a rehashing of previous results with some modifications. The empirical results are unfortunately too small-scale to be convincing to practitioners -- even the CIFAR experiments included in the appendix were run using a non-standard 32 channel network instead of the standard 128 channels. \n\nAdditionally, it's not clear how the regularization parameter $r$ should be set in practice, and for instance, how sensitive the results are to this setting. The added complexity in the training could be a huge drawback for practitioners.\n\nThe theory could be made stronger by analyzing the *sample complexity* of learning the score using the new objective under sparsity constraints -- see [1] and [2]. I'd also be interested in knowing whether the recent more sophisticated sampling methods and analyses (see [3], [4]) translate to the sparse case analyzed in this paper.\n\n[1]: Generative modeling with denoising auto-encoders and langevin sampling. A Block, Y Mroueh, A Rakhlin. 2020\n\n[2]: Improved Sample Complexity Bounds for Diffusion Model Training. Shivam Gupta, Aditya Parulekar, Eric Price, and Zhiyang Xun. NeurIPS 2024.\n\n[3]: Faster Diffusion Sampling with Randomized Midpoints: Sequential and Parallel. Shivam Gupta, Linda Cai, Sitan Chen. ICLR 2025.\n\n[4]: Nearly -Linear Convergence Bounds for Diffusion Models via Stochastic Localization. J Benton, V De Bortoli, A Doucet, G Deligiannidis. ICLR 2024."}, "questions": {"value": "1) What is the sample complexity of learning sparse scores using the objective you have proposed?\n2) Can you achieve iteration complexity smaller than s^2 using the recent more sophisticated sampling algorithms and analyses?\n3) Do the results translate to larger scale? For example LSUN or Celeb-A?\n4) There are no quantitative results provided for many of the small scale experiments (MNIST, etc) -- can you report them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xDUOk4OYNs", "forum": "2AvjgGJg8U", "replyto": "2AvjgGJg8U", "signatures": ["ICLR.cc/2026/Conference/Submission24280/Reviewer_Hjd6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24280/Reviewer_Hjd6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131415892, "cdate": 1762131415892, "tmdate": 1762943027090, "mdate": 1762943027090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}