{"id": "lmQjnEIAv6", "number": 16421, "cdate": 1758264392873, "mdate": 1759897241627, "content": {"title": "Persuade with Reason: Enhancing Debate Persuasiveness through Accurate Persuasion Feedback Derived from Weak Supervised Labels", "abstract": "Existing methods for debate generation often struggle to provide convincing proof, lacking critical persuasiveness. More challengingly, directly fine-tuning or using RLHF on large language models (LLMs) can decrease the persuasiveness of the generated text, making it difficult to leverage advancements from state-of-the-art LLMs. We identify two key biases underlying this issue: reward hacking and reward sparsity. Reward hacking blurs the model's training objectives, causing the model to focus more on linguistic style and rhetoric while neglecting the essential logical reasoning and value shaping. Reward sparsity reduces the generalization and robustness of the reward model. To address these two problems, we propose a novel persuasiveness enhancement training method: $\\rm P^{3}$. Firstly, we introduce \\underline{\\textbf{P}}ersuasive reward estimation and modeling by separating persuasiveness scores from surface cues, addressing the reward hacking problem. Secondly, we solve the reward sparsity issue by employing \\underline{\\textbf{P}}ersuasive sample mining to extract persuasive annotation information from weakly supervised labels. Lastly, we design a new DPO algorithm tailored for \\underline{\\textbf{P}}ersuasiveness generation optimization, which modifying the objective function to mitigate the divergence problem on debate generation task. Extensive experimental results demonstrate that $\\rm P^{3}$ effectively alleviates the aforementioned issues, significantly enhancing the model's performance in debate and persuasion tasks, surpassing state-of-the-art closed-source commercial models, such as Gemini and Claude, in both automatic and human evaluations.", "tldr": "", "keywords": ["Debate Generation", "Reinforce Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/672d58afd19ab87b94d44482f396056d4cfc9066.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the lack of persuasiveness in the debate generation, especially after fine—tuning and RLHF. The main reason for that is reward hacking and reward sparsity. The authors propose P^3, a three-stage pipeline to improve LLM persuasion in debate-style generation: first, separate persuasiveness from surface/literal cues via an EM + Bradley–Terry inspired estimator, then mine persuasive samples from weak supervision (upvote−downvote scores) and filter by the estimated persuasiveness score, and finally, optimize with a modified DPO called PAPO to avoid DPO’s small-sample divergence. Experiments on ChangeMyView (CMV) show improved automatic (GPT-4 o1) and human evaluations; authors claim their 13B model outperforms much larger closed-source models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-structured and easy to follow\n- The paper includes both automatic and human evaluations, and shows the correlation between those metrics"}, "weaknesses": {"value": "- Lack of a more comprehensive analysis of the generated output. There is only one example described in the case study (section 4.3).\n- Unnecessary and redundant equations in section 2 that could have gone to the appendix and free up some space for the analysis of the results (the point above)"}, "questions": {"value": "* Why are the human evaluation scores and the o1-score on different scales? Wouldn't it be easier to report the automatic o1-score on the same scale as the human evaluations to facilitate direct comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CnMCqOcH0", "forum": "lmQjnEIAv6", "replyto": "lmQjnEIAv6", "signatures": ["ICLR.cc/2026/Conference/Submission16421/Reviewer_nFeL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16421/Reviewer_nFeL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946696057, "cdate": 1761946696057, "tmdate": 1762926541728, "mdate": 1762926541728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PAPO, a persuasion-aware preference optimization framework. Using the CMV dataset, the authors disentangle persuasiveness from style by modeling two latent components and optimizing through a score-weighted objective that addresses divergence issues in standard DPO. The model is evaluated both automatically and with limited human assessments, showing improved alignment with human judgments compared to existing preference optimization methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The study addresses a gap between language fluency and genuine persuasiveness in LLM outputs. Its attempt to separate logical argument quality is conceptually novel. The integration of theoretical justification and practical large-scale experiments make the paper methodologically grounded."}, "weaknesses": {"value": "Comment 1. The authors fit two MLPs (sd for persuasion, ss for surface) and learns a Bernoulli-mixture with EM. It’s unclear what sd truly captures. Does sd represent causal persuasiveness (logic, evidence) or merely represent latent artifacts?\nUsing a small subsample, the authors might test the following.\n(i) hold content constant while changing style; (ii) hold style constant while changing content. Check that sd is stable in (i) and changes in (ii); ss should do the opposite.\n\nComment 2. The Reddit-based supervision signal (upvotes minus downvotes), as the authors note, can conflate persuasiveness with unrelated noise, such as popularity and timing effects. This weak supervision may cause the model to reward factors other than genuine argumentative strength. This is particularly important as the entire algorithm hinges on this “weak” supervision.\nThe authors can probably include fixed effects for subreddit, posting time, and author karma, and then re-estimate the learned persuasion scores to test robustness.\nFurthermore, the authors can use CMV “delta” awards as clean persuasion labels, train on that subset, and compare whether PAPO still outperforms the traditional DPO in that subset.\n\nComment 3. I am not particularly convinced by the reliability of o1 scores. The authors’ argument is based on limited validation (only 100 human-rated samples).\nPotentially, the authors can consider increasing the human sampling stratified by topic, stance, and argument length. Importantly, it will be interesting to re-prompt the evaluator with alternative rubrics (logic-only vs. style-only) and show that model rankings remain stable.\n\nComment 4. The CMV dataset is known to have topic imbalances. To ensure that the authors’ results can generalize across different topics, the authors can split CMV data by topic clusters and show whether improvements hold uniformly or only in high-frequency topics. It will be worthwhile to investigate under which conditions (by topic or by any other systematic characteristics) the algorithm performs relatively better or worse.\n\nComment 5. The title of paper is currently missing in your submission."}, "questions": {"value": "See the above \"Weaknesses.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gt8S5LIgpk", "forum": "lmQjnEIAv6", "replyto": "lmQjnEIAv6", "signatures": ["ICLR.cc/2026/Conference/Submission16421/Reviewer_s2Y1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16421/Reviewer_s2Y1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981435080, "cdate": 1761981435080, "tmdate": 1762926541238, "mdate": 1762926541238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of generating persuasive debate text with large language models. The authors identify two major issues that cause existing methods, especially supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF), to fail in producing truly persuasive arguments. To address these, the authors propose P3, a three-stage training framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear Problem Identification: The paper convincingly argues that traditional RLHF pipelines are poorly aligned for debate/persuasion tasks due to reward misalignment (hacking) and data sparsity.\n\nWell-Motivated Framework: The P3 pipeline logically decomposes the overall goal into reward estimation, data selection, and strategy optimization. Each component addresses a specific, well-defined issue."}, "weaknesses": {"value": "1. Limited Dataset Scope: All experiments are restricted to the CMV dataset. While appropriate for persuasion, results on other argumentative or dialogue datasets (e.g., ConvAI, PersuasionForGood) would demonstrate generalization.\n\n2. Using GPT-4-o1 as the primary automatic evaluator introduces circular dependence, since GPT-4’s reward alignment may favor stylistic fluency over genuine logical rigor. The correlation coefficient (0.67) is reasonable but not perfect; more robust human-only evaluations would strengthen the conclusion."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "crLjP74QMS", "forum": "lmQjnEIAv6", "replyto": "lmQjnEIAv6", "signatures": ["ICLR.cc/2026/Conference/Submission16421/Reviewer_7znS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16421/Reviewer_7znS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113270601, "cdate": 1762113270601, "tmdate": 1762926540810, "mdate": 1762926540810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focus on persuasion generation, creating text that convinces a specific audience. The authors attribute the limitation of current persuasion generation to 2 main reasons: Reward hacking (objective design) and Reward sparsity (training data).\nTo tackle the deconstructed issues, the enhance pipeline, $P^3$, is proposed. The pipeline scores generations along literal and persuasiveness dimensions and trains using weakly supervised labels. Experiments on CMV demonstrate its effectiveness in improving persuasive quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Problem decomposition: deconstructing the weaknesses of persuasion generation along two axes, objective design and data scarcity, to diagnose failures and guide targeted remedies.\n2. Fine-grained scoring: decoupling the reward into literal fidelity and persuasiveness, enabling fine-grained optimization of persuasive quality.\n3. Data construction pipeline: propose a scalable data-construction pipeline that alleviates data scarcity and provides a practical recipe for weak/cheap supervision."}, "weaknesses": {"value": "1. Fig. 1 inconsistency (objective vs. paradigm).\nThe paper attributes failures to objective design, but Fig. 1 varies training paradigms (SFT vs. RL) rather than the objective itself. As a result, the figure does not isolate objective mis-specification. \n\n2. Limited evaluation scope (single dataset).\nThe primary evaluation is on CMV; broader experiments on additional persuasion/debate datasets would strengthen external validity and generality of the conclusions.\n\n3. Baseline parity (backbone vs. fine-tuned).\nIn Tab. 1 and Fig. 4, comparisons appear to pit the proposed fine-tuned system against general-purpose backbones. For a fair test of $P^3$’s contribution, including equally fine-tuned baselines can consolidate the final conclusion\n\n4. Strong model comparison.\nGiven the reported high correlation between o1-score and human annotations, GPT-4o seems to be a strong baseline models which is missing in the performance comparison.\n\n5. Missing specific metric was used for human evaluation.\nThe paper lacks a clear description or example of the human scoring process, making the evaluation criteria ambiguous. Providing concrete examples or a defined scoring rubric would clarify how the human judgments were obtained and interpreted.\n\n6. The title in the manuscript is inconsistent with the title provided in the submission page."}, "questions": {"value": "1. In Fig.2, more detailed statement is required. Why do such scatter plots demonstrate the deviation?\n2. In formula 3, what is the $x$ in $f_s(x)$ standing for\n3. same one in formula 5\n4. in Fig.3, given that the o1-score shows the highest correlation with human judgments, would a simple pipeline that uses GPT-4o1 to automatically annotate data and then trains SFT/RL models without any task-specific tailoring yield promising gains on persuasion generation?\n5. For automatic evaluations, what are the prompt templates (scoring rubric, temperature, top-p, max tokens)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k1IHI1BwiQ", "forum": "lmQjnEIAv6", "replyto": "lmQjnEIAv6", "signatures": ["ICLR.cc/2026/Conference/Submission16421/Reviewer_mPUe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16421/Reviewer_mPUe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117859246, "cdate": 1762117859246, "tmdate": 1762926540334, "mdate": 1762926540334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a persuasiveness-enhanced preference optimization framework that extends DPO to model persuasive reasoning under weak supervision. It leverages social feedback signals to learn from large-scale debate data and employs a dual-reward mechanism to separate genuine persuasion from superficial style. Experiments on persuasion and debate benchmarks show consistent gains in both automatic and human evaluations. Overall, the approach provides a simple yet effective way to align language models toward persuasive and content-driven generation under weakly supervised settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces a novel persuasiveness-aware extension of DPO that separates logical persuasion from surface-level style through dual reward modeling.\n2. The paper effectively leverages large-scale social feedback as weak supervision, demonstrating how noisy real-world data can be adapted for persuasive language modeling."}, "weaknesses": {"value": "1. The paper’s exposition is often unclear and difficult to understand.\n2. The experiments compare P³ mainly with general instruction-tuned LLMs (e.g., LLaMA, Claude, Gemini), but omit specialized baselines that also target persuasion or argumentation, such as: SFT-based methods, DPO or similar methods are focusing on reward hacking and reward sparsity.\n3. The paper reports only the O₁-score (GPT-4 evaluation) and limited human judgments. Even though some previous metrics have limitation, it can reflect something by using Bert score, Rouge/BLEU. Such as semantic alignment and style or phrasing pattern.\n4. The paper lacks both statistical and qualitative analyses of generated outputs. There is no examination of response length, lexical diversity, or argument structure, nor any examples of failure cases. Without such analysis, it is unclear why P³ performs better—whether due to improved reasoning or merely longer, stylistically refined responses.\n5. No enough ablation for hyper parameter. For example, α in expression (3) controlling the trade-off between persuasiveness and surface cues, yet no ablation or sensitivity study is provided to justify its selection or stability.\n6. Lack of clear definition of persuasiveness: The paper does not explicitly define what builds persuasiveness (e.g., length, strength of style, number of supporting arguments), making it unclear what aspects the model actually learns or optimizes."}, "questions": {"value": "1. Clarification on Figure 2\nThe paper would benefit from a clearer explanation of Figure 2. Specifically, please specify what the x- and y-axes represent in the scatter plot, and clarify the interpretation of the plotted metrics. It would also help to explicitly state whether higher values indicate better performance for each metric .\n\n2. Bias Mitigation\nThe paper briefly mentions several sources of bias but does not provide sufficient methodological detail on how they are addressed.\n(i) Data bias (Reddit): Please elaborate on how cultural and popularity biases are mitigated when training the MLPs, given that popularity signals (upvotes, downvotes) are not equivalent to genuine persuasiveness.\n(ii) Accumulated bias: The training pipeline may compound multiple biases — Reddit community bias → flawed supervision signals → MLP learns spurious correlations → biased reward f_d → misguided PAPO optimization.  It would strengthen the work to clarify what specific mechanisms are applied to break or mitigate this propagation.\n(iii) Evaluation bias: Since o1_score relies on GPT-based judgments, can you do some statistic analysis to show o1_score is diverse and would not give high score for some specific topic? What's could you please explain how to reduce the bias in the human evaluator during your experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "db8NA3AVle", "forum": "lmQjnEIAv6", "replyto": "lmQjnEIAv6", "signatures": ["ICLR.cc/2026/Conference/Submission16421/Reviewer_ChvZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16421/Reviewer_ChvZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144688973, "cdate": 1762144688973, "tmdate": 1762926538982, "mdate": 1762926538982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}