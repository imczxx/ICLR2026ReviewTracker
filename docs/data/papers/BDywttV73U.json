{"id": "BDywttV73U", "number": 374, "cdate": 1756736896322, "mdate": 1759898264535, "content": {"title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints", "abstract": "Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate  adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space, defining a fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting.", "tldr": "Knowledge injection method based on knowledge-oriented control, achieving precision adaptation and powerful retention.", "keywords": ["large multimodal model; knowledge injection; knowledge-oriented augmentation and constraint"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b34ae83d158103d1db1bc015e3955dd7a2e8f1c2.pdf", "supplementary_material": "/attachment/5f44960a128ed79df1f72b453057dda15d27eea1.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes a multimodal knowledge injection framework that integrates structured data augmentation with a null-space constraint. It aims to balance new knowledge learning and old knowledge preservation through automatic multimodal task generation and covariance-based regularization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is visually well-presented with clear figures and tables, and the writing is coherent and easy to follow."}, "weaknesses": {"value": "1.\tThe method uses GPT-4o to generate augmented data, which introduces the risk of incorporating external knowledge. This may lead to unfair comparisons with baselines that do not rely on external models. In other words, the performance gains could partly result from distillation from GPT-4o rather than from the proposed method itself.\n2.\tThe “Knowledge-Oriented Constraint” closely resembles AlphaEdit’s [1] null-space approach. The paper should clarify its conceptual or technical novelty beyond AlphaEdit.\n3.\tKORE-Augmentation and KORE-Constraint operate independently without clear interaction, making the framework appear as two parallel components rather than an integrated system.\n4.\tSeveral conceptually or methodologically related works were not discussed. The paper should include an analysis and discussion with [1][2][3].\n5.\tThe presentation of the theorems in the paper is not standardized and is difficult to follow. For example, in Theorem 1, the symbols are not defined within the statement itself but are introduced later in the proof. The theorem statements should be formalized and rewritten in a clearer, more rigorous manner.\n\n[1] AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models, ICLR\n\n[2] LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models, CVPR\n\n[3] LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation, COLM"}, "questions": {"value": "1. The roles of the two theorems in the paper are unclear; their purpose and contribution to the overall method are not explicitly explained.\n2. The LoRA rank used in this paper appears relatively high compared to prior works. How does the proposed method perform under lower-rank settings?\n3. In Table 5, the W/o Constraint variant performs about 3% better than KORE on the EVOKE dataset. Does this indicate that the proposed constraint may negatively affect performance on EVOKE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wmfm0hL7M6", "forum": "BDywttV73U", "replyto": "BDywttV73U", "signatures": ["ICLR.cc/2026/Conference/Submission374/Reviewer_Uqia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission374/Reviewer_Uqia"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761312498034, "cdate": 1761312498034, "tmdate": 1762915506539, "mdate": 1762915506539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the KORE method, which achieves a balanced integration of new and existing knowledge in large models through the collaboration of KORE-AUGMENTATION and KORE-CONSTRAINT. The former automatically generates multi-turn dialogues and multimodal task data to promote knowledge internalization, while the latter uses null-space constraints to prevent forgetting. Experiments demonstrate that KORE significantly enhances the knowledge learning and retention capabilities of multimodal models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- KORE-AUGMENTATION is highly innovative and serves as a reasonable and effective data augmentation approach.\n- The method achieves strong empirical results, reaching state-of-the-art performance."}, "weaknesses": {"value": "- The core idea of KORE-CONSTRAINT is quite similar to AlphaEdit [1] (both employ projection onto the null space to mitigate interference with prior knowledge), which weakens the originality of this work.\n- Experiments are conducted only on the EVOKE benchmark, while another equally important benchmark in this field, CoIN [2], is neglected.\n\n[1] [2025-ICLR] Alphaedit: Null-space constrained knowledge editing for language models  \n[2] [2024-NeurIPS] CoIN: A benchmark of continual instruction tuning for multimodel large language models"}, "questions": {"value": "- AlphaEdit [1] also employs projection onto the null space to mitigate interference with prior knowledge. Could you clarify how your approach differs from theirs?\n- SEFE [2] consists of two components, ASD and RegLoRA. Since the authors of SEFE did not apply ASD to the EVOKE dataset, did you fully reproduce ASD on the EVOKE benchmark for comparison, or are your SEFE results based solely on its RegLoRA component?\n\n[1] [2025-ICLR] Alphaedit: Null-space constrained knowledge editing for language models   \n[2] [2025-ICML] SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DrS1oXbHfT", "forum": "BDywttV73U", "replyto": "BDywttV73U", "signatures": ["ICLR.cc/2026/Conference/Submission374/Reviewer_WzoY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission374/Reviewer_WzoY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723638640, "cdate": 1761723638640, "tmdate": 1762915506346, "mdate": 1762915506346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the continual learning in Large Multi-Modal Models. To solve the balance of learning new knowledge and knowledge retention, they propose a KOPE. Extensive experiments demonstrate that KOPE achieves new knowledge injection performance and mitigates catastrophic forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method sounds reasonable, which makes new knowledge structured, and the covariance matrix keeps previous knowledge.\n\nThe experiment in the paper is sufficient, including multiple MLLMs and multiple downstream tasks."}, "weaknesses": {"value": "The survey is insufficient, e.g., related works and baselines have few papers in 2025.\n\nFor Figure 5, why is Full-FT lower than KOPE? Theoretically, full fine-tuning is an upper bound for any method.\n\nDuring the training, how to split the dataset into new knowledge datasets and old knowledge datasets?"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1x3VXJ2q4X", "forum": "BDywttV73U", "replyto": "BDywttV73U", "signatures": ["ICLR.cc/2026/Conference/Submission374/Reviewer_WrUD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission374/Reviewer_WrUD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879462797, "cdate": 1761879462797, "tmdate": 1762915506166, "mdate": 1762915506166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}