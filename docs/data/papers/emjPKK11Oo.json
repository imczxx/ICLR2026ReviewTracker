{"id": "emjPKK11Oo", "number": 12192, "cdate": 1758206242897, "mdate": 1759897526159, "content": {"title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful", "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. Despite broad use, recent studies indicate that, when faced with an explicit bias in their prompts, models often omit mentioning this bias in their output, revealing that this verbalized reasoning can sometimes give an incorrect picture of how models arrive at conclusions (unfaithfulness). In this work, we go further and show that unfaithful CoT can also occur on realistic, non-adversarial prompts without artificial bias. We find that when separately presented with the questions \"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We present preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies that aim to detect undesired behavior in LLMs via the chain of thought. More broadly, they indicate that while CoT reasoning can be a useful tool for assessing model outputs, it is not a complete and transparent account of a model's internal reasoning process, and should be used with caution, especially in agentic or safety-critical settings.", "tldr": "We show that Chain-of-Thought reasoning is not always faithful is frontier models, in unbiased contexts", "keywords": ["Chain-of-Thought", "Faithfulness", "AI Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98299141d455feb65ca22f7a849ebca77b4b5e65.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper systematically investigates the problem of unfaithfulness in CoT reasoning within LLMs. The authors identify two previously underexplored phenomena:\n\n1. Implicit Post-Hoc Rationalization — when faced with logically exclusive problems, the model generates seemingly plausible yet mutually contradictory reasoning chains to justify an underlying default answer bias.\n2. Unfaithful Illogical Shortcuts — in mathematical reasoning, the model arrives at correct answers through “shortcut” or erroneous reasoning steps that conceal logical inconsistencies within its CoT process.  \n\nExtensive experiments were conducted on 15 LLMs using a contrastive dataset of 4,834 questions, demonstrating the prevalence of unfaithful reasoning behaviors in mathematical tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a systematic analysis of the faithfulness problem in CoT reasoning, addressing a research question of significant value.\n2. Extensive experiments demonstrate that such faithfulness issues are prevalent across most models, including those equipped with reasoning capabilities, offering valuable insights and guidance for future large language model research."}, "weaknesses": {"value": "1. Although the paper identifies implicit biases, it does not uncover the underlying triggering mechanisms within the Transformer architecture. An internal analysis of such mechanisms would make the study more convincing and interpretable.\n2. The work lacks rigorous significance testing or bootstrap confidence intervals.\n3. The task coverage is limited — the dataset focuses mainly on comparative and mathematical reasoning, without including more complex scenarios such as multi-hop or conversational reasoning.\n4. The definition of faithfulness remains ambiguous, as it still conflates observable consistency with cognitive consistency; the theoretical boundary between the two requires clearer delineation."}, "questions": {"value": "1. Can the causal contribution of internal bias activation to the unfaithfulness rate be quantitatively measured? Could this be verified using logit-lens analysis or attention probing?\n2. In PutnamBench, is the phenomenon of Unfaithful Illogical Shortcuts significantly correlated with training data contamination?\n3. Could the proposed Consistency-with-Reversal regularizer be integrated into RLHF or DPO pipelines for empirical validation?\n4. Can the framework be extended to open-ended reasoning tasks such as multi-hop question answering or commonsense reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fvVlFxFaV4", "forum": "emjPKK11Oo", "replyto": "emjPKK11Oo", "signatures": ["ICLR.cc/2026/Conference/Submission12192/Reviewer_iSVc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12192/Reviewer_iSVc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754815570, "cdate": 1761754815570, "tmdate": 1762923142441, "mdate": 1762923142441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a study of chain-of-thought (CoT) unfaithfulness, where LLMs produce answers that do not necessarily reflect the reasoning in the predicted CoT. They conduct their study from two angles: one exploring the model's behavior when answering natural language questions containing anti-symmetric relations (e.g., is X less than Y? vs is Y less than X?). The second angle of their study is complex math word problem solving, where the model may produce invalid steps in order to justify a predetermined response."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The authors propose an interesting study to quantitatively measure the unfaithfulness of the model's CoT.\n - They conduct experiments on a wide range of models, including both thinking and non-thinking models, from disparate model families.\n - The paper is well-written and easy to understand."}, "weaknesses": {"value": "- There are a number of confounding factors that may explain some of the observed discrepancies in the model (see below for further details).\n - A validation study is missing for the LLM-as-a-judge approach in evaluation."}, "questions": {"value": "The example of whether the Ajay River is to the south of Salar de Arizaro (or vice versa) highlights another possible explanation for the model's unchanged response (no): Perhaps there are more than two possible answers. That is, there is at least one other answer in addition to \"Ajay River is located to the south of Salar de Arizaro\" and \"Salar de Arizaro is located to the south of the Ajay River\". This is because \"to the south of\" may not be a strictly anti-symmetric relation (i.e., a relation `r` is anti-symmetric if for any `x`, `y`, `r(x,y)` if and only if not `r(y,x)`). For example, this confounding factor arises in examples where r(x,y) and r(y,x) are both false (or both true), such as in \"is 2i larger than sqrt(-4)?\" It can also arise when there is ambiguity in the definition of the relation, as in \"is the magnetic south pole to the south of Antarctica?\" While I don't suspect these potential confounders would have a very significant effect on the paper's findings, they may explain some portion of the argument switching cases. To what extent are the relations in the World Models dataset unambiguously anti-symmetric? I am curious how the results would change if a third option (e.g., \"N/A\") were given to the models.\n\nI would not necessarily consider the example in Figure 1 to be one of unfaithfulness to the CoT. In both cases, the answer (no) is faithful with respect to the reasoning in the CoT. The main difference is that the CoTs provide very different arguments.\n\nThe authors raise the possibility of another potential confounding factor: sycophancy. While the authors did investigate the possibilty of RLHF-induced sycophancy being the explanation for the same answers being provided to logically opposite questions, they did not test for the presence of sycophancy in the base (pre-RLHF) model. However, the authors did generate data where \"yes\" and \"no\" answers are balanced. It would be interesting to compare the unfaithfulness metric on \"yes\" questions vs \"no\" questions.\n\nThe authors rely on LLMs as a judge for numerous parts of their study, without any validation experiments to ensure the correctness of these LLM-judges (i.e., or \"autoraters\"). They include a human annotation step in their evaluation pipeline but only for a specific subset of examples, and it is unclear to what extent this step alleviated the stated issues with the LLM judges. It is unclear how accurately these LLM judges perform when rating positive vs negative examples. The \"pitfalls\" that the authors mention on Line 328 are not elaborated upon.\n\nThe authors claim to control for data contamination by experimenting with data that was released after the trainng cutoff of the tested models. However, this does not guarantee that the model has not seen that data, since frontier models are frequently updated, possibly with additional post-training.\n\nThe paper is well-written with minimal grammatical or spelling errors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wo8K0R19Xn", "forum": "emjPKK11Oo", "replyto": "emjPKK11Oo", "signatures": ["ICLR.cc/2026/Conference/Submission12192/Reviewer_7nJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12192/Reviewer_7nJy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903525715, "cdate": 1761903525715, "tmdate": 1762923141555, "mdate": 1762923141555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper sets out to critique chain-of-thought reasoning in language models by providing a more refined taxonomy of unfaithfulness. They identify particular forms of unfaithfulness like post-hoc rationalizations and illogical shortcuts (perhaps hoping that the reader will be happy to identify these particular subspecies with \"unfaithfulness\"-full-stop as promised in the title), and they argue that these nuances deserve attention.\n\nTo be blunt, while I acknowledge that adding a bit of granularity to the conversation is marginally useful, I’m not particularly moved by this paper. It feels like just another entry in a long line of critiques that don’t really shift the needle; I've had to review three CoT critiques this batch and none of them have made an impression. Yes, the authors have carved out a more detailed definition of what \"unfaithfulness\" can look like, but they’re not really bringing anything groundbreaking or memorable to the table for me. Let's call this what it is: another minor addition to an already crowded field. I'll lean accept because I don't think it's worth a lengthy debate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Expands discussion of CoT unfaithfulness with hints of a systematic typology and empirical survey.\n- Engages existing literature and situates the problem in real-world prompts (“in the wild”).\n- Provides dataset and evaluation scripts for reproducibility."}, "weaknesses": {"value": "- No fundamentally new insight; incremental.\n- Analyses largely descriptive: new quantitative or causal tests.\n- Offers no solution or mitigation strategy.\n- Reads more like a position paper dressed as empirical work."}, "questions": {"value": "- What novelty is claimed beyond earlier works?\n- Are the “faithfulness” metrics predictive of task accuracy or just diagnostic labels?\n- Could dataset be used to train more faithful models, or is it purely evaluative?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3yiWnihxCv", "forum": "emjPKK11Oo", "replyto": "emjPKK11Oo", "signatures": ["ICLR.cc/2026/Conference/Submission12192/Reviewer_pcao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12192/Reviewer_pcao"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918059277, "cdate": 1761918059277, "tmdate": 1762923141116, "mdate": 1762923141116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on investigating the issue of \"Unfaithfulness\" in Chain-of-Thought (CoT) reasoning generated by Large Language Models (LLMs). Previous research on CoT unfaithfulness has primarily concentrated on adversarial prompts explicitly designed with human-engineered biases. This paper goes beyond that limitation and systematically demonstrates for the first time that CoT reasoning still exhibits unfaithfulness even when faced with more representative, realistic, and non-adversarial prompts. \"Unfaithful\" means that the verbalized reasoning steps (CoT) do not accurately reflect the internal computation or logical path the model actually used to reach the final answer. The authors designed a sophisticated multi-stage evaluation methodology to accurately identify and quantify these unfaithful reasoning steps and analyzed their \"Criticality\" to the final answer, thereby providing a more rigorous diagnostic tool for assessing LLM trustworthiness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper expands the study of CoT faithfulness from narrow adversarial scenarios to the broader \"In the Wild\" context, significantly enhancing the practical relevance and application value of this finding.\n2) he proposed fine-grained, multi-stage error classification and evaluation framework (as detailed in Appendix K.6, which uses multiple prompts to detect answer correctness, step incorrectness, step unfaithfulness, etc.) is very comprehensive and provides a valuable diagnostic tool for future interpretability research.\n3) This work serves as a strong reminder to researchers and developers that a model's decision reliability cannot be judged solely by its verbalized reasoning process. This is crucial for promoting the development of more transparent and trustworthy AI."}, "weaknesses": {"value": "1) The core contribution of the paper lies in shifting to \"realistic, non-adversarial prompts,\" but there is a lack of in-depth justification and formal definition regarding how the chosen benchmarks truly embody \"realism.\" For example, do the three selected datasets (as mentioned in Appendix K.6) merely exclude explicit adversarial bias, or do they statistically approximate the actual queries users submit to search engines or chatbots? The paper should provide a more rigorous analysis (e.g., comparison of perplexity or topic distribution against actual query logs) to enhance the generality and representativeness of the chosen benchmarks. Otherwise, the contribution of the term \"realistic\" may be diminished.\n2) The paper successfully diagnoses the prevalent existence of unfaithfulness but does not offer sufficient evidence to explain its internal mechanisms. Is unfaithfulness an inherent model flaw (e.g., a mismatch between reasoning and conclusion pairings in the training data)? Or is it simply a manifestation of output text fluency exceeding its logical rigour? The paper could attempt to establish causal relationships by systematically varying model architecture (e.g., ablation studies), training data characteristics, or decoding strategies (e.g., temperature, Top-p) to observe trends in unfaithfulness, rather than merely documenting the phenomenon.\n3) The paper clearly identifies a critical limitation but lacks adequate discussion or preliminary experiments on how to address or mitigate this limitation."}, "questions": {"value": "1) Regarding the root causes of \"unfaithfulness,\" have the authors tested for any internal diagnostic signals within the model? For instance, do the Attention Maps or the activation patterns of specific layers exhibit recognizable differences when the model generates an unfaithful step?\n2) Please provide more detailed information about the benchmark datasets. How were the selected datasets (such as the three mentioned in Appendix K.6) filtered to ensure they were \"non-adversarial\"? Can quantitative metrics (e.g., perplexity scores of the language model on real user query data) be provided to demonstrate that they are statistically closer to real-world usage scenarios?\n3) Please explicitly state whether the primary source for the \"Unfaithful\" and \"Critical\" labels was human annotation or LLM meta-evaluation. If human annotation, please provide the corresponding Kappa coefficient or F1 score to measure Inter-Rater Reliability (IRR). If LLM meta-evaluation was used, please explain how you ensured that the meta-evaluator model did not inherit or generate similar unfaithful biases as the model being evaluated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cZq442btXl", "forum": "emjPKK11Oo", "replyto": "emjPKK11Oo", "signatures": ["ICLR.cc/2026/Conference/Submission12192/Reviewer_3Ar4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12192/Reviewer_3Ar4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960293658, "cdate": 1761960293658, "tmdate": 1762923140519, "mdate": 1762923140519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}