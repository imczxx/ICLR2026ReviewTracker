{"id": "QywpTFx86x", "number": 7678, "cdate": 1758031587841, "mdate": 1759897839735, "content": {"title": "Polynomial, trigonometric, and tropical activations", "abstract": "Which functions can be used as activations in deep neural networks? This article explores families of functions based on orthonormal bases, including the Hermite polynomial basis and the Fourier trigonometric basis, as well as a basis resulting from the tropicalization of a polynomial basis. Our study shows that, through a simple variance-preserving initialization and without additional clamping mechanisms, these activations can successfully be used to train deep models, such as GPT-2 for next-token prediction on OpenWebText and ConvNeXt for image classification on ImageNet. Our work addresses the issue of exploding and vanishing activations and gradients, particularly prevalent with polynomial activations, and opens the door for improving the efficiency of large-scale learning tasks. Furthermore, our approach provides insight into the structure of neural networks, revealing that networks with polynomial activations can be interpreted as multivariate polynomial mappings. Finally, using Hermite interpolation, we show that our activations can closely approximate classical ones in pre-trained models by matching both the function and its derivative, making them especially useful for fine-tuning tasks. These activations are available in the torchortho library.", "tldr": "New activation functions based on orthonormal bases (like Hermite and Fourier) and Tropical polynomials can train deep models effectively, avoid gradient issues, and approximate standard activations.", "keywords": ["Orthogonal function bases", "Tropical polynomials", "Polynomial mapping", "Deep neural networks", "ImageNet-1K", "OpenWebText", "Transformers", "GPT2", "Convolutional networks", "ConvNeXt", "Initialization scheme", "PyTorch"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d199f086d1c261223b5038939a259f22dc1dfcd4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work reopens the discussion on which functions can be used in deep neural networks (or, more specifically, whether a few of the highlighted functions in this paper can). The paper covers theory and some empirical results. Chiefly, perhaps, is empirical work showing ConvNeXt and GPT-2 can be trained using orthogonal learnable activations, at least on specific shown datasets, which \"eliminates the need for additional mechanisms\"."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the theoretical underpinnings of Sec 3 are good -- 3.1 leads naturally to 3.2, 3.3, and 3.4. I may have missed _why_ Hermite, Fourier, Tropic activations are focused on. These families are unified cleanly.\n- The practical implementation in Sec 3.5 seems like it's reproducible. At least, no obvious flaws with that secion was found."}, "weaknesses": {"value": "- It's not *technically* a weakness, but convention seems to be moving away from older datasets like CIFAR-10, and older models like GPT-2. This does not sway the decision much, but it seems to upset 'the community' and it does limit the generalizability of your claims, especially to larger scales.\n- This is perhaps more of a question, but I'm flagging it as a weakness because lack of clarity on this seems to undermine one of the main points -- that exploding/vanishing gradients are addressed. I.e., it seems that equal-gain guarantees are only guaranteed at the initialization? What evidence is provided for stability long-term?\n- Not exactly a weakness either (to the extent to which it just describes an observation!) but the activations (esp Hermite) seem to add a high computational overhead."}, "questions": {"value": "- Why do you choose Hermite, Fourier, and Tropic activations? Are there other possibilities? What main problems with existing methods to these overcome?\n- Are there assumptions regarding distributions that aren't reasonable to expect, and to what extent do you need to check for them? E.g., are Gaussian or uniform distributions assumed by these activation functions, and would that be realistic?\n- The Fourier is described in sine-cosine form, but the recommended initialization seems to only make use of a_k? Which coefficients are actually initialized and trained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JbhjlSXERz", "forum": "QywpTFx86x", "replyto": "QywpTFx86x", "signatures": ["ICLR.cc/2026/Conference/Submission7678/Reviewer_3rgG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7678/Reviewer_3rgG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665839574, "cdate": 1761665839574, "tmdate": 1762919741955, "mdate": 1762919741955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a novel framework to enable learnable activation functions in deep neural networks. In particular, they focus on functions based on orthogonal bases and tropical polynomials. An initialization method for the activation functions  is introduced and the results showcase improvements over static functions. The efficacy of the method is benchmarked across vision and language tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The main idea is novel and well-motivated.\n\n- The thorough theoretical support on the initialization methods is a valuable contribution to the community.\n\n- I appreciate the benchmarking of the method across both text and vision tasks.\n\n- The latency analysis is an important addition."}, "weaknesses": {"value": "- I am missing an ablation over different backbones for both vision and language benchmarks.\n\n- Although not a major weakness, additional experimental support on challenging benchmarks would increase the impact of the paper, e.g., on COCO for vision related tasks.\n\n- A discussion on the application of the proposed activation functions for generative models (e.g., diffusion-based models) would be interesting.\n\nMinor:\n\n- The last sentence in ln. 485 seems to end abruptly."}, "questions": {"value": "I would appreciate if the authors address the issues raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "L8yMf4fJ5P", "forum": "QywpTFx86x", "replyto": "QywpTFx86x", "signatures": ["ICLR.cc/2026/Conference/Submission7678/Reviewer_4XrB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7678/Reviewer_4XrB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844414088, "cdate": 1761844414088, "tmdate": 1762919741554, "mdate": 1762919741554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces a family of learnable activation functions based on orthogonal function bases (Hermite and Fourier) and tropical polynomials. The authors propose a variance-preserving initialization scheme to ensure stable gradient propagation and demonstrate the feasibility of using these activations in deep architectures such as ConvNeXt and GPT-2. The paper combines theoretical analysis, efficient implementations, and empirical validation, suggesting that polynomial activations can indeed yield competitive results with proper initialization."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a rigorous variance-preserving initialization framework that unifies different activation families under an orthogonal function perspective. This is both mathematically elegant and practically meaningful.\n2. By addressing Hermite, Fourier, and tropical bases, the study gives a broad view of orthogonal and piecewise-linear activations, including insightful links to classical activations (ReLU, GELU).\n3. Experiments on ImageNet (ConvNeXt) and OpenWebText (GPT-2) convincingly demonstrate that the proposed activations can be trained stably and achieve comparable or slightly better performance than standard nonlinearities.\n4. The inclusion of recursive formulations, efficient kernels, and open-sourced code (torchortho) greatly improves the work’s reproducibility and potential impact."}, "weaknesses": {"value": "1. The reported 30–90% slower training speed (Section 6) is significant. The paper would benefit from more detailed timing analyses and GPU utilization comparisons to quantify the trade-off between performance and efficiency.\n2. The experiments focus on classification and next-token prediction tasks. Additional ablations (e.g., fine-tuning, transfer learning, adversarial robustness) could help demonstrate broader applicability."}, "questions": {"value": "1. The variance-preserving initialization ensures equal forward and backward gains for orthogonal activations. How sensitive is this balance to deviations from the assumed input distributions (e.g., non-Gaussian inputs during training)?\n2. When replacing activations in large pretrained models (e.g., GPT-2), how does initialization interact with layer normalization and residual scaling? Are any stability adjustments required?\n3. Have the authors explored methods to reduce computational cost, such as approximate polynomial evaluation (e.g., Chebyshev truncation, low-rank projection, or kernel-based approximation)? Could these reduce FLOPs while preserving stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EF8fHIARNh", "forum": "QywpTFx86x", "replyto": "QywpTFx86x", "signatures": ["ICLR.cc/2026/Conference/Submission7678/Reviewer_CZi7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7678/Reviewer_CZi7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962102566, "cdate": 1761962102566, "tmdate": 1762919741141, "mdate": 1762919741141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the utilization of orthogonal polynomial activation functions in neural networks, specifically, first deriving the variance preserving initialization for Hermite, Fourier, and Tropical activation function, then conducting experiments on image classification and NLP tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written, presented with a clear structure.\n2. The theorem-proof logic is clear and rigorious.\n3. The visualization helps explain the conclusion."}, "weaknesses": {"value": "Despite the strengths, here are some weaknesses:\n1. The motivation is not clear, is it just an exploration on activations? And I am not sure if the proposed activation functions solve any existing problems. (Although I do know that not all innovative thought must solve something discrete, but I do suggest the author to refine this part.) \n2. Since the paper is not the first to design a new kind of activation function, even not the first to use orthogonal  polynomials, I am not sure what is the core innovation.\n3. The experiments show very little improvements when Hermite activation function is used. The tropical and Fourier activation function even have worse performance then GELU. These results restrict the value of the paper.\n4. More experiments should be conducted, like, more tasks, more models, and more benchmarks.\n5. In Proposition C.3, when computing the expectation of $F(x)^2$, the paper uses $\\int F^2(x) \\dfrac{e^{-x^2/2}}{\\sqrt{2\\pi}}dx$. I do not think adding the $\\dfrac{e^{-x^2/2}}{\\sqrt{2\\pi}}$ term is rigorious, although it is based on the definition of Hermite. This problem is common for orthogonal polynomials, like Legendre, Hermite, and Chebyshev polynomials, that when adding the orthogonal term, the derivation is much easier.\n6. Although the newly designed functions have negligible redundent parameters, it may cause low numerical stability. I am not sure how the authors resolve this problem."}, "questions": {"value": "Please see the 'Weaknesses' section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uxd4dwfuwl", "forum": "QywpTFx86x", "replyto": "QywpTFx86x", "signatures": ["ICLR.cc/2026/Conference/Submission7678/Reviewer_MYHC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7678/Reviewer_MYHC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990251767, "cdate": 1761990251767, "tmdate": 1762919740049, "mdate": 1762919740049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}