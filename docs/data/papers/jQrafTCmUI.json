{"id": "jQrafTCmUI", "number": 10148, "cdate": 1758162065362, "mdate": 1759897671114, "content": {"title": "PRIVDISTIL: A Unified Framework for Accurate and Differentially Private Model Compression", "abstract": "Privacy has emerged as a paramount concern in the development and deployment of language models. While over-parameterized models deliver exceptional performance, their deployment on resource-constrained devices (such as mobile or embedded systems) remains challenging. Model compression techniques are widely adopted to address this, yet they introduce additional privacy risks. Although differential privacy (DP) provides rigorous theoretical safeguards against data leakage, the noise injection required during compression often severely degrades model utility. Balancing high performance with strong privacy guarantees in compressed models thus remains a critical open challenge.\nIn this work, we introduce Privdistil, a DP-aware model compression framework that redesigns the compression pipeline to preserve utility while protecting sensitive data. Privdistil begins by training a domain classifier via DP-SGD on a hybrid dataset of public and private samples, thereby identifying public data most aligned with the private domain. It then performs model compression exclusively on these selected public samples, followed by fine-tuning the compressed model on the private dataset using DP-SGD. By shifting the compression burden to public data, Privdistil minimizes noise requirements and boosts training stability. Extensive experiments show that Privdistil consistently surpasses state-of-the-art DP compression methods across diverse datasets and architectures, delivering an average accuracy gain of over 3\\% on GLUE benchmarks under a strict privacy budget of $\\varepsilon = 1$.", "tldr": "", "keywords": ["Differential privacy", "Knowledge distillation", "Model compression"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f02aa8cd7dcdfa506c07c5503185b8d69c69c736.pdf", "supplementary_material": "/attachment/cfb72d37bb9f0c543e17b52ba50b0fef16bb541a.zip"}, "replies": [{"content": {"summary": {"value": "Authors present PRIVDISTIL, a differential privacy (DP) aware model compression technique. Primary challenge that authors address is the following: a framework that compresses models without major regressions to task quality while preserving privacy. They do so by primarily distilling on \"selected public data\" and then eventually doing DP-SGD on the student model. The key to this process is data selection which is called importance sampling and ensuring diversity through clustering. Authors show that they have substantial gain in performance 3% gain in GLUE benchmark under a tight privacy budget ε= 1. The work is empirical, and while the technique does not seem highly novel appears efficient when compared to existing DP based compression techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clean and clear framework: the design is modular easy to implement, scale and has practical applicability. DP Domain Classifier -> Public Corpus Clustering -> Adjusted Importance Sampling -> Public KD followed by DPSGD on private data. The authors have explained each step and overall intuition behind each process in a detailed manner\n\n2. Strong empirical results: Compared to DPKD shows strong gains especially at smaller privacy budget\nBERTBASE/LARGE and ε∈{1,4}, PRIVDISTIL improves mean GLUE accuracy (79.2 vs. 76.1 at ε=1 with BERTBASE; 77.9 vs 75.9 )"}, "weaknesses": {"value": "1. Ablations and evals can be more rigorous: While there are tables on the impact of clusters and data selection, it is not apparent which part of the process is the giving the primary lift. For ex: AIS improves +0.8 avg compared to full data but 7.0 over random. It appears the major gains are coming from public KD\n\n2. Stat significance: All tables appear to be single run only. Seeing confidence intervals with difference seeds can provide more evidence\n\n3. While clipping params and epsilon 0.3 are specified noise multipliers, total steps per data set are not specified. \n\nMinor\n4. It is not specified what hardware is used for comparing speed ups\n\nOverall, since this is an empirical work (there is precedence around using public data along with private/DP mechanisms like private kNN, PATE) stronger ablations, and experiments are required. It is not clear from the paper what is providing the lift in quality metrics"}, "questions": {"value": "1. [Minor] Importance sampling seems to be a bit misleading. This is not importance sampling in classic sense but rather cluster based greedy selectiion, topk. Please clarify.\n\n2. In appendix F  it appears domain classifier has perfect separability with close to zero error rate. In real world scenarios this may not be likely. Is it possible to use more challenging data and present results?\n\n3. Ablations and evals require more rigor as stated above - it is not clear which part of the process is playing a role in improving quality. Please provide more experimentation\n\n4. Need more rigorous measurement of efficiency/speedup - is this latency? If yes under what conditions?\n\n5. [Good to have] While it is great to see results with BERT does this scale with larger models? Discussion would be if not for some preliminary experimentation.\n\n6. Statistically significant results are needed. Good to see results over multiple runs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ODBNvNGyr1", "forum": "jQrafTCmUI", "replyto": "jQrafTCmUI", "signatures": ["ICLR.cc/2026/Conference/Submission10148/Reviewer_TbmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10148/Reviewer_TbmF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978819685, "cdate": 1761978819685, "tmdate": 1762921518740, "mdate": 1762921518740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the process of fine-tuning and compressing LLMs under differential privacy. Compression is neccessary when LLMs are deployed on memory-constrained devices. The paper shows improved results over a simple baseline of compressing and fine-tuning using DP-SGD (Mirshegallah et al 2022). They propose using public data for the compression process, but instead of using the public data as is, they instead subsample the data to ensure closeness in domain to the private data and diversity within the subsampled data. The subsampling (called Adjusted Importance Subsampling) is their main technical contribution. WIth the access to public data + the subsampling strategy, they show large improvements over the simple baselien for 2 out of 4 tasks (between 3-6%) and smaller improvements (<1%) on the other 2 tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear improvement over a simple baseline. \n\n- Comprehensive experiments and ablation studies.\n\n- Results are presented in a clear way."}, "weaknesses": {"value": "- It would be good to see results at larger epsilon values e.g., eps=10, does the method still hold a significant advantage over the DPKD baseline?\n\n- I am still not convinced about the motivation for this problem. If the compressed model is to be deployed on device it is usually becasue it will process very sensitive data, e.g., text messages. Your method requires obtaining such data on the server side to perform the compression+fine-tuning, but collecting very sensitive data on the server side seems challenging. What kind of tasks/data are you considering your method would apply to?\n\n- I am also not clear about the importance of the subsampling from the public corpus, which is a key technical contribution. In Table 4, first row, am I understanding corresctly that this row corresponds to doing compression using the full data and then fine-tuning with the private data? If so, the gains of your data subsampling method over the first row are on the smaller side (1-1.5%) and I assume they would be smaller for larger epsilon. This means that the access to public data, without any data sampling strategy, is a key way in which you obtain the improved results over the DPKD baseline, which does not use public data, thus it seems to me an unfair comparsion to the baseline. \n\n- It is also not clear how performant your method will be when there is a large distribution gap between the public corpus used and the private corpus. If we are unable to obtain updated public data from the private data domain, such a large gap will eventually emerge.  \n\n- Finally DPKD is the simplest baseline in Mirshegallah et al, which they acknolwedge is on the weaker side, and they discuss other techinques such as initialization and pruning to increase peformance. Am I correct that in your comparsion you did not use any of their baselines involving pruning and initialization? \n\n- This is more minor, but in Table 1 I am still having a hard time understanding what each of the rows corresponds to (basides the DPKD and PrivDistil rows). I suggest clearly stating what is the teacher and student model and whether is any compression happenning."}, "questions": {"value": "Please see questions in the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mXTiFz2Oha", "forum": "jQrafTCmUI", "replyto": "jQrafTCmUI", "signatures": ["ICLR.cc/2026/Conference/Submission10148/Reviewer_bpHu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10148/Reviewer_bpHu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037533968, "cdate": 1762037533968, "tmdate": 1762921518355, "mdate": 1762921518355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the challenge of compressing language models under differential privacy (DP) without sacrificing too much utility. The authors propose PRIVDISTIL, (1) trains a DP domain classifier to estimate which public samples resemble the private domain, (2) clusters the public corpus using teacher embeddings, (3) performs Adjusted Importance Sampling (AIS) to pick relevant-and-diverse public data, (4) runs non-private distillation on those public samples to build a compact student, and (5) finally DP-fine-tunes the compressed student on private data. The whole process composes to an (ε,δ)-DP guarantee; in experiments on GLUE, PRIVDISTIL improves accuracy over DPKD at ε∈{1,4} and yields notable speedups via DistilBERT students."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Idea is nice: It is good to exploit public data for DP model compression. Training a DP domain classifier to rank public samples aligns selection with the private task while spending only a small slice of the privacy budget; the rest of compression occurs non-privately on selected public data. \n\nQuality and Clarity: The paper is very well written and the idea is clearly presented.  It has clear end-to-end privacy accounting. The paper gives a composition statement and uses a PRV accountant for tighter bounds; it also explains the budget split across domain-classifier training and final DP fine-tuning. \n\nPractical wins under realistic budgets. Gains at ε=1 and ε=4 on standard GLUE tasks, plus speedups from DistilBERT, make the method attractive in practice."}, "weaknesses": {"value": "The big concern is that the paper does not include an important related work that also tackles differential private model compression with public data selection. These two papers share similar idea and [1] seems achieving better utility on MNLI and SST-2 under private budget epsilon=4.\n\n[1] Yu et al., Selective Pre-training for Private Fine-tuning, TMLR 2024\n\nGiven the rapid movement of LLM, the paper does not include experiments on many strong autoregressive models, like Qwen3 0.6B or miniCPM4 0.5B."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Gj3oZLyj6V", "forum": "jQrafTCmUI", "replyto": "jQrafTCmUI", "signatures": ["ICLR.cc/2026/Conference/Submission10148/Reviewer_g2DN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10148/Reviewer_g2DN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070252826, "cdate": 1762070252826, "tmdate": 1762921517997, "mdate": 1762921517997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}