{"id": "EUTmFIQ9Jv", "number": 14623, "cdate": 1758240390306, "mdate": 1759897358960, "content": {"title": "Negative Sampling From the Ground Up: A Redesign for Graph-based Recommendations", "abstract": "Negative sampling is an important yet challenging component in self-supervised graph representation learning, particularly for recommendation systems where user-item interactions are modeled as bipartite graphs. Existing methods often rely on heuristics or human-specified principles to design negative sampling distributions. This potentially overlooks the usage of an underlying ``true'' negative distribution, which we might be able to access as an oracle despite not knowing its exact form. In this work, we shift the focus from manually designing negative sampling distributions to a more principled method that approximates and leverages the underlying true distribution. We expand this idea in the analysis of two scenarios: (1) when the observed graph is an unbiased sample from the true distribution, and (2) when the observed graph is biased with partially observable positive edges. The analysis result is the derivation of a sampling strategy as the numerical approximation of a well-established learning objective. Our theoretical findings are also empirically validated, and our new sampling methods achieve state-of-the-art performance on real-world datasets.", "tldr": "", "keywords": ["negative sampling", "recommendation system", "self-supervised learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4887e57c5ff2cb8dd5098204942ef0faa9e5a42d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a negative sampling method to improve graph-based recommendation models. Traditional strategies such as random or popularity-based sampling can introduce bias under imbalanced distributions, leading to deviations from the true data distribution. To address this, the authors theoretically revisit negative sampling in graph representation learning and derive an optimal framework grounded in the true negative distribution. They introduce a learnable exposure probability to correct observation bias and adjust sampling weights based on sample difficulty, giving higher importance to rarely exposed but informative samples. The effectiveness of the method is validated on multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1）The paper systematically derives the optimal negative sampling distribution from the optimization objective and builds a unified theoretical framework that incorporates traditional heuristic sampling methods into a single analytical system.\n\n2）The authors shift negative sampling from empirical design to a principled modeling approach based on the true negative distribution, introducing a learnable exposure probability to correct observation bias. This idea is relatively rare in existing work, reflecting methodological innovation and theoretical exploration value, though the overall contribution remains mainly theoretical.\n\n3）The paper is well-structured, with clearly defined formulas. Although the performance gains are modest, the results are stable across multiple datasets, demonstrating coherence between theory and empirical validation."}, "weaknesses": {"value": "1）The paper uses π+ and the number of negative samples k. However, the selection of π+ seems somewhat subiective and acks formal guidelines or an automated procedure for their setting. Both these parameters can significantly affect model performance, yet the paper does not delve into how they are chosen or how sensitive the model is to different values of these parameters.\n\n2）Although the paper constrains (φ) via regularization, it does not mention whether lower-bound clipping, smoothing, or temperature scaling is applied for numerical stability. Because inverse propensity weighting can amplify noise or cause gradient instability when (φ to 0 ), the absence of such details raises concerns about numerical robustness and convergence during training.\n\n3）While Section 6.4 presents training time trends, no quantitative metrics (e.g., GFLOPs, parameter counts, or average epoch time) are reported. Since an additional propensity estimation network ( \\phi ) is introduced, its forward-pass cost is unmeasured, making the claim of “computational controllability” unsupported.\n\n4）Experiments are conducted only on MovieLens, Pinterest, and LastFM datasets, without testing on social, knowledge, or heterogeneous graphs to assess generalization. Moreover, most cited works predate 2023, with only two from 2024, failing to reflect the latest advances in the field.\n\n5) Several of the figures(fig.2) in the paper are very small and difficult to read."}, "questions": {"value": "Q1. The paper models the negative sampling distribution as (q^-(v|u) ∝ p^-(v|u)·p_f(v|u)) and corrects bias via a learnable network. However, this approach still relies on an explicit factorization assumption. Have the authors considered using a generative modeling approach (e.g., a neural sampler or diffusion-based model) to directly learn the implicit distribution (q^-(v|u))? Such a method could better capture complex nonlinear dependencies and avoid the need for explicit factorization.\n\nQ2. The paper estimates exposure probabilities using a learnable network and applies regularization to stabilize the output. However, there is no mention of any lower bound or variance control for these values. Since inverse propensity weighting can amplify noise or cause over-compensation when approaching zero, could the authors clarify whether any truncation, smoothing, or other stabilization techniques were applied during implementation? If not, was numerical instability observed during training in practice?\n\nQ3. Section 6.4 provides time complexity analysis and training-time curves, suggesting the method is efficient. However, the results are mainly qualitative. Could the authors provide explicit quantitative comparisons, such as GFLOPs, parameter counts, or average training time per epoch, to show how the additional network affects computational cost relative to baselines like DNS or PNS?\n\nQ4. The paper introduces two correction factors based on the observed distribution: the exposure probability and the model prediction probability. However, both require additional estimation network outputs, whose stability and estimation errors could directly affect the sampling distribution shift. Compared with traditional observation-based methods, have the authors analyzed whether incorporating these two factors might amplify the cumulative bias or increase the sampling variance? In practice, did the experiments observe any deviation of the sampling distribution from the theoretical optimum due to model estimation errors?\n\nQ5. The paper mentions key hyperparameters such as π+ and the number of negative samples k, but their selection criteria are unclear. Were these parameters manually set or tuned automatically? How sensitive is model performance to these choices?\n\nQ6. In Figure 2, the caption mentions subfigure (d), but only three subfigures (a–c) are shown. Was one subfigure removed or merged in the final version? Please clarify or correct the numbering for consistency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eGJv0Mvkfp", "forum": "EUTmFIQ9Jv", "replyto": "EUTmFIQ9Jv", "signatures": ["ICLR.cc/2026/Conference/Submission14623/Reviewer_yxru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14623/Reviewer_yxru"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876257029, "cdate": 1761876257029, "tmdate": 1762925000508, "mdate": 1762925000508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of negative sampling in graph learning, and introduces a theoretically grounded framework that derives the optimal negative sampling distribution. Specifically, the authors argue for the existence of a true negative distribution and show that the optimal proposal distribution should be a reweighted version of this distribution. Several experiments are conduct based on some real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical analysis is interesting and complete. The proposed method can generalized across different graph machine learning applications.\n2. The experiments are comprehensive, including 10 baseline methods and 7 datasets."}, "weaknesses": {"value": "1. One of my main concerns lies in the performance of the proposed method. In most cases, the improvement over strong baselines is marginal and often falls within the range of standard deviation.\n2. My understanding is that recommendation interactions inherently contain noise, largely due to the mismatch between user preference and item exposure (i.e., users may not interact with items they are never exposed to). This exposure bias seems to contradict the core assumption of the proposed approach, such as the existence of an unbiased or recoverable true negative distribution.\n3. The performance gain on the Pinterest dataset is notably more substantial compared to the other datasets. It would be great if the authors could provide a deeper analysis or empirical justification for this phenomenon."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JWv4VIy2SY", "forum": "EUTmFIQ9Jv", "replyto": "EUTmFIQ9Jv", "signatures": ["ICLR.cc/2026/Conference/Submission14623/Reviewer_GEwg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14623/Reviewer_GEwg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974716745, "cdate": 1761974716745, "tmdate": 1762924999938, "mdate": 1762924999938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on the principled negative sampling method that approximates and leverages the underlying true distribution from the ground up. It expands this idea in the analysis of two scenarios: (1) when the observed graph is an unbiased sample from the true distribution, and (2) when the observed graph is biased with partially observable positive edges. The authors proposes a sampling strategy from these two scenarios as the numerical approximation of a well-established learning objective. Experiments on real-world datasets demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The motivation about understanding the underlying “true” negative distribution is valuable and access it as an oracle despite via the principled negative sampling is promising.\n2. The proofs are generally rigorous and thoughtful. Proposition 4.1 explicit proves that the best proposal negative distribution q− should take the simple form to optimize R_2(f) and Theorem 5.1 provides the desired form of the proposed negative sampling method, providing a sound theoretical foundation for the proposed approach.\n3. The experiments are extensive, which includes a wide set of baselines on three large benchmark datasets. Notably, Figure 3 demonstrates the efficiency of the proposed negative sampling method.\n4. The source code and the utilized datasets are released in anonymous Github repo."}, "weaknesses": {"value": "1. While the theoretical sections are generally rigorous, the notation is cumbersome, please streamline them or provide a table to present the details of rach notation.\n2. I noticed that the authors mentioned that the hyperparameters for the negative sampling baselines follow those reported in the original papers, which is reasonable. However, I suggest that the authors also try using the same settings as their own method (especially the number of negative samples) to ensure the fair comparison.\n3. Lack of some recent negative sampling works in the related work section:\n[1] Negative Sampling in Recommendation: A Survey and Future Directions, Arxiv\n[2] Adaptive hardness negative sampling for collaborative filtering, AAAI 2024\n[3] gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling, RecSys 2023\n[4]  Revisiting negative sampling vs. non-sampling in implicit recommendation, TOIS\n4. Where is sub-figure (d) in Figure 2."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KbFqGruci0", "forum": "EUTmFIQ9Jv", "replyto": "EUTmFIQ9Jv", "signatures": ["ICLR.cc/2026/Conference/Submission14623/Reviewer_fNH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14623/Reviewer_fNH3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996488888, "cdate": 1761996488888, "tmdate": 1762924998944, "mdate": 1762924998944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of negative sampling in self-supervised graph representation learning. Instead of relying on heuristic or manually-designed negative sampling strategies, the authors propose a principled framework that attempts to approximate an underlying ``true'' negative distribution. The authors provide theoretical derivations to support this perspective and develop new sampling strategies based on the resulting insights."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is interesting. The idea of approximating a latent true negative distribution instead of manually designing sampling heuristics is conceptually innovative.\n\n2. The paper provides theoretical analysis under two realistic data scenarios (unbiased and biased), and derives sampling strategies grounded in formal learning objectives."}, "weaknesses": {"value": "1. Although the paper claims to approximate a “true” or “unbiased” negative distribution for real-world recommendation, the datasets used in the experiments are public benchmarks that have already been heavily filtered, cleaned, and de-biased. In actual recommender systems, it is extremely difficult to define what constitutes a truly unbiased user–item graph, as user interactions are inherently affected by exposure bias, popularity bias, and system-level interventions. The authors should discuss whether the proposed method genuinely addresses real-world bias, or whether it only works under idealized, well-curated datasets. \n\n2. The authors should provide a quantitative analysis of the computational overhead introduced by the proposed sampling strategy. Although the method is described as lightweight, it remains unclear whether it increases training time, memory usage, or inference latency compared to heuristic-based negative sampling. This is particularly important for large-scale recommendation scenarios (e.g., millions of users and items), where even small increases in computation can significantly impact system efficiency."}, "questions": {"value": "3. Open question: The proposed method is developed and evaluated under an offline, static graph assumption. However, real-world recommender systems operate in online environments, where user–item interactions continuously evolve. The authors can discuss whether the method supports incremental or streaming updates? or whether it can be deployed in online learning or A/B testing scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IwtEmQVYtJ", "forum": "EUTmFIQ9Jv", "replyto": "EUTmFIQ9Jv", "signatures": ["ICLR.cc/2026/Conference/Submission14623/Reviewer_ALpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14623/Reviewer_ALpU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054408381, "cdate": 1762054408381, "tmdate": 1762924998200, "mdate": 1762924998200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}