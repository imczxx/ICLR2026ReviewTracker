{"id": "lNcc1TypMd", "number": 14671, "cdate": 1758241356940, "mdate": 1759897355991, "content": {"title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum", "abstract": "Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL).  While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the *model-capability continuum*. Near the *model-strong* end, prior-leaning objectives that downweight low-probability tokens (*e.g.,* $-p$, $-p^{10}$, thresholded variants) consistently outperform NLL; toward the *model-weak* end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability.", "tldr": "We revisit supervised fine-tuning (SFT) for large language models, introducing a model-capability continuum that shows negative log-likelihood is not universally optimal and characterizes when alternative objectives succeed or fail.", "keywords": ["Post-Training", "SFT", "training objectives"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/caf377d1b21cb4c946710cb70e7f8ee64c4aef95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper examines the effects of the negative log likelihood loss on various domains. The findings reveal that there is a relationship between the type of loss used and the model's priors on the task. Based on this, the paper proposes a model capability continuum as a way to formalize the spectrum of models' priors and their relationship to various probability-based learning priors. More specifically,y they find that models with weak priors on task tend to benefit more from NLL loss as compared to models with strong priors. This end of the spectrum benefits more from down-weighting low probability tokens. At the center, they find that no one objective function has a clear advantage."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is stated clearly in how SFT is applied to LLM alignment compared to classification training\n- Research questions are stated clearly \n- The paper lays out the theoretical background of the loss functions used for SFT and a generalized version of it\n\nMethod and Experiment:\n- Paper shows extensive experiments on the Model Strong and Model Moderate settings with a number of benchmarks\n- Ablation studies: The paper does extensive ablation on the high, low, and mid probability tokens' fine-tuning using various values of alpha"}, "weaknesses": {"value": "Related work Depth:\n\t\n- The paper has not examined existing literature exploring alternatives to CE loss. [1, 2]\n\t\n- It would be great to get a comparison to this work and a more comprehensive literature review of the existing landscape of alternative loss functions\n\nMethod and Experiment:\n- The experiments done on mode weak are not extensive. The choice of benchmark for model weak is much more restrictive compared to the other 2 settings\n\n1. Entropic Distribution Matching in Supervised Fine‑tuning of LLMs: Less Over‑fitting and Better Diversity\n2. Computer Vision Losses for Large Language Model Fine‑Tuning"}, "questions": {"value": "- Can the author discuss more about the connection with RL? This setting is similar to a policy with a strong prior setting in RL.\n- I would like to know if the results of model-weak still hold when using some other domain like coding, science, multi-lingual, etc (anyone could work).\n- For the model strong setting, it would be intresting to see results on a dataset that emphasizes knowledge memorization where the model has strong priors (Wikipedia, etc.).\n- Following on the previous question, does the proposed continuum still stand when we make a distinction of datasets that are reasoning/skills vs pure knowledge memorization? In other words is NLL loss sub-optimal choice for each class of the dataset when there strong prior and vice versa"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "awZR5tdSWm", "forum": "lNcc1TypMd", "replyto": "lNcc1TypMd", "signatures": ["ICLR.cc/2026/Conference/Submission14671/Reviewer_Fva5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14671/Reviewer_Fva5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710386261, "cdate": 1761710386261, "tmdate": 1762925043024, "mdate": 1762925043024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the standard practice of supervised fine-tuning (SFT) for large language models by questioning the negative log-likelihood (NLL) training objective. Authors propose a family of probability-based objectives that generalize NLL (which is the limit as $α→0$). They find alternative objectives (example $-p$ or $-(p^{10})/10$, which downweight low probability tokens) can outperform NLL on certain tasks. The key contribution is identifying a \"model capability continuum\": when the base model is strong (already has high prior knowledge on the task), prior-leaning objectives (that trust the model's prior) yield better generalization than NLL. When the base model is weak, the NLL objective is better for learning from scratch. In intermediate capability settings, no single objective is consistently better. The authors run comprehensive experiments across 7 models, 14 benchmarks, and 3 domains, demonstrating up to 16% accuracy gains with prior-leaning losses on strong models, whereas NLL remains best on weaker models. A theoretical analysis is provided to explain the performance of objectives. The work provides a practical guidance to choose objectives based on current model capability to improve generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Novel Perspective:** The paper offers a new viewpoint by questioning the default use of NLL for fine-tuning large pre-trained models. It introduces the concept of a model capability continuum, which is a clear way to understand how a model's prior knowledge should influence training strategy.\n\n* **Thorough Empirical Validation:** The experimental evaluation is very comprehensive. The authors conduct tests on 7 different LLM backbones (of varying sizes and domains) and 14 benchmarks covering diverse tasks (math problem solving, medical question answering, logic puzzles, etc.). This breadth gives good credibility to the findings, the continuum pattern (prior-leaning losses excel with strong models, NLL excels with weak models) is consistently observed, not just a one off result. Significant performance gains (sometimes doubling accuracy) are achieved in few settings using the new proposed objectives.\n\n* **Theoretical Insight:** Beyond empirical results the paper provides a theoretical analysis that supports its claims. The authors derive conditions under which one objective will outperform another, and show that these conditions flip between the \"model strong\" and \"model weak\" ends of the spectrum. This adds a lot of weight to the work it's not just \"we tried this new loss and it worked\" but why it works is partly explained through a formal lens.\n\n* **Clarity and Context:** The paper is well written and not hard to follow. It motivates the problem clearly (highlighting how long chain-of-thought supervision and strong pretrained priors violate assumptions of NLL's optimality). It also contextualizes the work in the literature: for example it contrasts its approach with reinforcement learning from human feedback (RLHF) and other recent techniques like PPO-inspired fine-tuning, importance sampling in SFT, and selective data training.\n\n* **Significance:** The findings have notable implications for the community. If NLL is not universally optimal for post-training, this could prompt many researchers and practitioners to reconsider their fine-tuning procedures. The idea that one should \"lean on the model’s knowledge when it's strong, and override it when it’s weak\" is a valuable guideline."}, "weaknesses": {"value": "1. **Objective Adaptation in the Intermediate Regime.**\n   The paper identifies that no single objective consistently works well in the model-intermediate regime, but does not propose a method to handle this case. This is a practical gap, since many real-world tasks likely fall in this zone.\n\n2. **Deciding Model Capability in Practice.**\n   The framework relies on knowing whether a model is \"model-strong\" or \"model-weak\" on a task, but the paper does not provide a way to assess this ahead of time. The current categorization is done post hoc.\n\n3. **Forgetting on Prior Tasks.**\n   The paper focuses on improving performance on new tasks during fine-tuning but does not study how different objectives affect retention of previously learned capabilities. This matters for applications where continual learning is important and accuracy needs to be high on the entire sequence of tasks being fine-tuned on."}, "questions": {"value": "1. Did the authors explore or consider adaptive objective schedules during training (example starting with NLL and transitioning to a prior-leaning loss)? If not what challenges do you expect to see in implementing such an approach?\n\n2. How should practitioners determine model capability before fine-tuning? Can simple metrics like zero-shot accuracy or mean token confidence be used reliably to choose the right objective?\n\n3. Did the authors measure or observe any differences in forgetting on prior capabilities when using prior-leaning objectives like $-p$ or $-p^{10}$ compared to NLL? Would you expect more or less forgetting in these cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Fbkn3XV6m", "forum": "lNcc1TypMd", "replyto": "lNcc1TypMd", "signatures": ["ICLR.cc/2026/Conference/Submission14671/Reviewer_nHnp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14671/Reviewer_nHnp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936111714, "cdate": 1761936111714, "tmdate": 1762925042634, "mdate": 1762925042634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that the standard negative log-likelihood (NLL) objective used in supervised fine-tuning (SFT) of large language models is not always the optimal choice during post-training. The authors observe that pretrained models already contain strong prior knowledge, and forcing them to imitate every supervision token can lead to overfitting and poor generalization. They introduce and study a broader family of probability-based training objectives that either emphasize or downweight low-probability tokens. Through experiments across multiple model sizes, datasets, and domains, they identify a “model-capability continuum”: in domains where the base model already has strong priors (e.g., math), objectives that downweight low-probability tokens (such as −p or thresholded −log p) outperform NLL. In domains where the model has weak priors (e.g., unseen puzzles), NLL performs better because it forces learning from unlikely tokens. In intermediate domains (e.g., medical reasoning), no objective clearly dominates. The paper further supports these findings with theoretical analysis showing how gradients and learning dynamics differ across capability regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and timely question in LLM post-training by re-examining the default SFT objective, which is usually taken for granted. The experimental evaluation is broad, covering multiple model families, diverse datasets, and capability levels, demonstrating the generality of the results. The conceptual introduction of a “model-capability continuum” provides an intuitive and practical framework for understanding when different objectives should be used. The empirical results are supported by gradient-based theoretical reasoning, which makes the findings more convincing. The paper has clear motivation, thorough ablations, and actionable insights for practitioners who want to improve fine-tuning outcomes."}, "weaknesses": {"value": "The classification of domains into “model-strong,” “model-intermediate,” and “model-weak” can feel somewhat heuristic and may not be straightforward to estimate for new tasks in practice. The proposed approach still requires manual selection of the objective based on the capability regime, and the paper does not yet provide an automated or adaptive method for doing this. While the theoretical explanation is suggestive, it relies on simplified assumptions and does not fully capture the complexity of real training dynamics. In some intermediate settings, the differences between objectives are small, which may limit the practical impact in many real-world SFT use cases. The paper also evaluates improvements mainly on reasoning-heavy tasks, so it is less clear how broadly the results generalize to conversational or stylistic alignment tasks."}, "questions": {"value": "The paper discusses a continuum from model-weak to model-strong domains, but the operationalization of this continuum is not fully specified. How should a practitioner determine where a new task sits on this continuum before training? Is there a quantitative diagnostic metric that can be computed prior to fine-tuning, rather than one derived from already trained or partially trained models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v5EhTOET6P", "forum": "lNcc1TypMd", "replyto": "lNcc1TypMd", "signatures": ["ICLR.cc/2026/Conference/Submission14671/Reviewer_ozBm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14671/Reviewer_ozBm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952610168, "cdate": 1761952610168, "tmdate": 1762925042177, "mdate": 1762925042177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs are usually post trained using SFT, where the model is taught to reproduce a reference answer token by token using NLL loss. The authors argue that once a model has been pretrained, NLL is no longer universally optimal because the model already encodes strong priors and SFT supervision can be noisy or irrelevant. The paper introduces a general family of objective functions that work under different conditions (MS, MI, MW). They show that through this formulation, they improved performance across 14 benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This paper introduces a general family of probability based objectives, it broadens the space of loss functions and connects NLL and accuracy as special cases.\n* The proposed idea of model capability continuum is neat, though the way to measure a models MS, MI and MW could be improved."}, "weaknesses": {"value": "* Experimental results focus on narrow domains (math, medical and puzzles) It would be good have results on some other general benchmarks (wild bench, arena hard, IF-eval, some code and agentic evals)\n* The continuum proposed by the paper relies on the mean predicted probability and pretraining coverage as proxies for prior strength. LLMs are often miscalibrated. Using a single scalar to rank tasks may overlook nuanced factors such as variance, entropy or distributional mismatch.\n* The paper does not study whether thresholding harms knowledge retention, fairness, or calibration.\n* The authors claim that RL‑inspired methods such as implicit reward learning, importance sampling and PPO‑style clipping are special cases of their prior leaning objectives. This should be backed with some empirical comparisons."}, "questions": {"value": "1. do you anticipate the same continuum behavior will hold for much larger LLMs (> 30B)? Could larger and more capable models potentially benefit even more from prior leaning objectives, or might new challenges (like optimization instability or diminished gains) arise at that scale?\n2. Have you considered using UQ methods as a more principled metric for assessing model capabilities. Such methods might better capture the epistemic vs aleatoric uncertainty and could help automate the classification of MS, MI and MW\n3. The experiments use a fixed threshold and show that training on the top 10 % of tokens yields strong improvements. How sensitive are the results to this choice?\n4. RL‑based methods such as RLHF, DPO, RPO and one‑token rollout also downweight low reward or low probability tokens by sampling. Could you provide a comparison between your probability based objectives and these RL approaches\n5. Does downweighting low‑probability tokens have any adverserial effects, like does it affect calibration or fairness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZOgxijcz1o", "forum": "lNcc1TypMd", "replyto": "lNcc1TypMd", "signatures": ["ICLR.cc/2026/Conference/Submission14671/Reviewer_FZpg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14671/Reviewer_FZpg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975274399, "cdate": 1761975274399, "tmdate": 1762925041687, "mdate": 1762925041687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}