{"id": "xxYPqm1qWz", "number": 4108, "cdate": 1757602032394, "mdate": 1763736418096, "content": {"title": "VideoAgentTrek: Computer-Use Pretraining from Unlabeled Videos", "abstract": "Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.", "tldr": "", "keywords": ["VideoAgentTrek"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f95b78f7b0f6d5dc1820f3b422fc35bbb329830.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Paper introduces a scalable pipeline, VideoAgentTrek to automatically mine training data for use in training of computer-use agents.\nThe work aims to address the challenge costliness of manually annotating such training data at scale. It does so by exploiting the large amount of screen-recorded tutorials available online and introduces a means of extracting explicit structured action labels from such unlabeled videos.\nTo achieve this, the authors designed VIDEO2ACTION, a two component inverse dynamics module (IDM) consisting of a video grounding model and a action content recognizer.\nThe module first performs dense action-event detection to segment clips and assign action labels, then a action parameterization model analyzes these segments to produce structured parameters such as pointer coordinates, typed text etc.\nUsing 39000 YouTube tutorials, the pipeline generates 1.52M interaction steps for large-scale pretraining."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The scalability of the method, producing over a million structured steps providing a way to generate large amount of annotated data required for training.\n\nClear establishment of need for large scale data via performance scaling evaluation to support motivation and need for automated scalable data collection. \n\nInclusion of cognitive-style reasoning using inner monologue generation process to extract rationale for steps i.e explicit the intent, the local plan, and the expected state change, enhances model interpretability and could improve models reasoning."}, "weaknesses": {"value": "It could be beneficial to show the performance across more vision-language models to see if the improvements are generalizable.\n\nFor the generation of inner monologue while effective, reliance on LLM-generated text raises concerns about consistency and reliability of the rationales. Were there any analysis on the outputs? \nQuantitative human study on a small subset could provide some insight.\n\nIn the performance evaluation of Action Event Detection, what is the threshold for temporal overlap to count as a hit?\n\nMinor comments not affecting rating:\nLine 266: \"ASR\" define abbreviations\\\nline 376 typo \"iff\""}, "questions": {"value": "Were the inner monologue used in the training?\n\nIn the first stage of action event detection, type a_k is predicted with the timestamps, why train the action parameterization model to again predict the type (line 245 - 247).\\\nit is mentioned in line 247 \"when available, we optionally condition on the detector’s $a_k$ to stabilize type predictions.\". How does this affect the performance of the parameterization model?\n\nFor the Stage 2 training, since it's stated that training was done on \"curated set of clean, human-annotated trajectories\" (line 310), were this the ones samples from Open CUA and AGUVIS.\nIf so how does the performance change if for stage 2, training data was drawn from only VideoAgentTrek generated trajectories and/or using a mix of human annotated and generated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y6azaAzfBu", "forum": "xxYPqm1qWz", "replyto": "xxYPqm1qWz", "signatures": ["ICLR.cc/2026/Conference/Submission4108/Reviewer_c9gy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4108/Reviewer_c9gy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806380333, "cdate": 1761806380333, "tmdate": 1762917180623, "mdate": 1762917180623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VIDEOAGENTREK, a scalable pipeline that automatically extracts GUI-action trajectories from unlabeled screen-capture videos (39 k YouTube tutorials → 1.52 M steps) and uses them for large-scale pre-training of computer-use agents. A learned inverse-dynamics module (VIDEO2ACTION) first localizes actions in time (event detector, F1=0.78) and then infers their parameters (click coordinates, typed text, etc.; 65.8 % human-verified accuracy). Continued pre-training on the mined data followed by supervised fine-tuning improves task success on OSWorld-Verified from 9.3 % (SFT-only) to 15.8 % (+70 % relative) and step accuracy on AgentNetBench from 64.1 % to 69.3 %. The authors release SCREENFILTER and VIDEO2ACTION as open-source tools."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel, timely problem: Leveraging the enormous volume of passive screen-capture videos for GUI-agent training is an appealing idea that addresses the current data bottleneck.\nEnd-to-end pipeline: From raw YouTube crawl to executable (screenshot, action, parameters) tuples, the system is fully automated and scales to web size.\nStrong empirical gains: Clear, statistically meaningful improvements over a pure SFT baseline on two independent benchmarks, plus positive scaling curves with data volume and test-time compute."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "This paper presents a timely and impactful contribution by introducing VIDEOAGENTREK, the first fully-automated pipeline that converts unlabeled, publicly available screen-capture videos into large-scale, training-ready trajectories for GUI agents. By equipping an inverse-dynamics module (VIDEO2ACTION) with dense event detection and parameter extraction, the authors bypass the expensive manual-annotation bottleneck and demonstrate clear downstream gains on both online and offline benchmarks. \nOne open question remains: will the complete codebase (SCREENFILTER, VIDEO2ACTION training & inference scripts, data-preparation pipeline) and the processed VideoAgentTrek dataset be publicly released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l1AMhFR7n8", "forum": "xxYPqm1qWz", "replyto": "xxYPqm1qWz", "signatures": ["ICLR.cc/2026/Conference/Submission4108/Reviewer_wmub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4108/Reviewer_wmub"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880724835, "cdate": 1761880724835, "tmdate": 1762917180320, "mdate": 1762917180320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VideoAgentTrek, which synthesizes GUI interaction trajectories from actionless videos to replace costly manual annotations. The core is the Video2Action inverse dynamics module: it first localizes events, then infers action parameters, and finally synthesizes a chain-of-thought. Training proceeds in two stages: supervised fine-tuning on the full dataset, followed by fine-tuning on a human-verified subset. Experiments report absolute gains of +6% on OSWorld-Verified and +5% on AgentNetBench. The authors release the ScreenFilter and Video2Action tools but do not release the video dataset or the trajectory dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow. The data processing pipeline, training setup, and results are clearly presented.\n- Applying VPT-style ideas to GUI agents is intuitive and scalable for data collection, and the idea is well-executed.\n- Clear improvements on two popular benchmarks.\n- ScreenFilter and Video2Action are released to support the reproduction of the video annotation pipeline."}, "weaknesses": {"value": "- While tools are open-sourced, the full video corpus and trajectory annotations are not available, which I believe could significantly increase the contribution of this work.\n- Potential data leakage is not quantified. The training data derived from public tutorials may overlap with OSWorld/AgentNetBench tasks, but the paper does not appear to include a rigorous deduplication or leakage analysis."}, "questions": {"value": "- Tutorial videos often contain a lot of noise (e.g., extraneous/meaningless mouse movements, hotkey usage). Is there any specific mechanism to handle this? For text-input actions, how does the model differentiate between a string-level typing action and individual key presses?\n- How are frames with no user action but visible UI changes handled? Does the Video2Action model have an explicit noop action?\n- Cursor icons can vary across operating systems and applications. What cursor types are supported by the cursor detection model?\n- Do event detection and cursor detection models require a standardized resolution/frame rate? How robust is the system across varying FPS and resolutions?\n- Can you provide an ablation on the effect of automatically generated inner monologue?\n- Is there a plan to release the full video dataset and extracted trajectories? Public release, especially of high-resolution videos with audio, would substantially strengthen the paper’s contribution. I would increase my score if this were to become available."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lbtq8kr1yM", "forum": "xxYPqm1qWz", "replyto": "xxYPqm1qWz", "signatures": ["ICLR.cc/2026/Conference/Submission4108/Reviewer_b4VU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4108/Reviewer_b4VU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979724643, "cdate": 1761979724643, "tmdate": 1762917179819, "mdate": 1762917179819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VideoAgentTrek, a scalable pipeline to pretrain computer-use agents from unlabeled, publicly available screen-recorded videos. The key challenge is that raw videos  lack structured action labels (types, timestamps, and parameters). The authors address this by introducing an inverse dynamics module to generate the parameterized action tuples. With the collected dataset and the proposed multi-stage training strategy, the final performance of the GUI Agent achieved SOTA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting and reasonable reframing of GUI action recovery as inverse dynamics from raw videos without (great) manual labels.\n\n2. The filter system leverages cursor detection to automatically focus on GUI-heavy segments; channel-coherence expansion for source discovery is practical and scalable.\n\n3. Clear improvements over a strong SFT-only baseline on both OSWorld-Verified and AgentNetBench, plus analysis showing benefits grow with pretraining scale and planning horizon.\n\n4. Pipeline is well-structured and explained (collection → filtering → detection → parameterization → rationale → training).\n\n5. Practical tools and release plans (if they actually release finally) can catalyze broader community work."}, "weaknesses": {"value": "1. While Stage 2 SFT mitigates noise (Parameterization accuracy, detection errors, etc), a more systematic study quantifying how parameter errors propagate to agent performance (e.g., via controlled corruption of parameters) would clarify robustness and failure modes.\n\n2. The dense detector shows weaker recall for keyboard/press actions, which can be crucial in many workflows. It would help to analyze how missing non-pointer actions affects task categories (e.g., auth flows, terminal usage) and to explore augmentations (ASR cues, keystroke audio hints) to close this gap. Also, since the author uses the pointer as one of the  video filters, I am very curious whether the trained model would bias to such pattern? Further discussion and analysis would help. \n\n3. More detailed release of manifests (video IDs, timestamps, filtering decisions) would further improve reproducibility within ToS boundaries.\n\n4.  The pipeline currently focuses on 2D screen recordings. Some OSWorld tasks involve subtle text/element detection under variable themes or require OCR robustness. An ablation with explicit OCR enhancements or higher-res sampling could clarify performance ceilings. Similarly, mobile platforms or non-English UIs are out-of-scope; discussing adaptation strategies would strengthen the broader impact story.\n\n5. In Table 9, the authors provided a cross-dataset comparison, which is good; But I do think there misses some important datasets. I would prefer a more complete comparison as done in AgentNet, Table 2. This will help the readers to localize. \n\n6. From the Figure, it seems the \"test-time scaling\" helps a lot, but I cannot find the further details."}, "questions": {"value": "1. See the weakness.\n\n2. Have you tried ensembling multiple parameter predictors or using confidence-based filtering to drop uncertain steps? How does selectively pruning low-confidence parameterizations affect Stage-1 benefits?\n\n3. Can you quantify how coordinate noise (e.g., ±k pixels) affects downstream success on benchmarks via a controlled perturbation study?\nImproving non-pointer actions:\n\n4. For press/type, did you explore fusing ASR transcripts or keystroke sounds to improve recall? Are there simple heuristics (e.g., stable cursor + text change) that boost detection without heavy supervision?\n\n\n5. Beyond releasing filter tools, can you also share:\n- Video ID lists and segment timestamps (without frames), with pass/fail reason codes.\n- A small, license-cleared demo subset (e.g., Creative Commons) to enable exact replication of Stage-1 on a miniature scale.\n\n6. Did video-pretraining improve performance uniformly across OSWorld app buckets (calc, chrome, vscode, etc.), or are gains concentrated in certain domains? \n\n7. Since mined text can include sensitive inputs (typed passwords, emails), how to deal with?\n\n8. Could you incorporate UI element detection (e.g., layout parsing) to normalize parameterization into element-centric actions rather than screen coordinates, improving cross-resolution robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IdbShqA93e", "forum": "xxYPqm1qWz", "replyto": "xxYPqm1qWz", "signatures": ["ICLR.cc/2026/Conference/Submission4108/Reviewer_Rbqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4108/Reviewer_Rbqb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983064881, "cdate": 1761983064881, "tmdate": 1762917179560, "mdate": 1762917179560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment to All Reviewers and the Area Chair"}, "comment": {"value": "We sincerely thank all reviewers and the area chair for their thoughtful, constructive feedback on our work. We are encouraged that reviewers agree on the novelty and importance of our goal: turning large-scale, unlabeled screen-capture videos into training-ready trajectories for computer-use agents. \n\nSeveral reviewers recognized our key strengths:\n\n- **Novel and timely approach** (`Rbqb`, `wmub`): First fully-automated pipeline converting unlabeled screen-capture videos into training-ready trajectories, addressing the data bottleneck for GUI agents\n- **Strong empirical validation** (all reviewers): Clear improvements over SFT-only baselines on OSWorld-Verified and AgentNetBench with positive scaling curves\n- **Practical impact and reproducibility** (`Rbqb`, `wmub`, `c9gy`): Comprehensive pipeline from collection to training, with open-source release of tools and data\n\nAt the same time, reviewers also raised several concerns, which we appreciate and believe we have fully addressed as follows:\n\n1. **Data Release and Reproducibility**: We provide concrete commitments with specific deliverables: Video ID manifest with full action metadata, reconstruction scripts, and CC-licensed subset with direct trajectories\n2. **Quality Validation** :\n    - provided quantitative validation results in appendix: 81% of generated inner monologue samples meet pre-training quality criteria.\n    - added OSWorld-G grounding evaluation to provide more training analyses.\n    - visualized pretraining data distribution and provided a comprehensive contamination study.\n3. **Additional Specific Clarifications**: Numerous clarifications requested by the reviewers, highlighted in blue in the revised version, have been added.\n\nThese updates strengthen our paper and directly address the core concerns while preserving the practical contributions of our work. We welcome further questions during the discussion period and thank all reviewers for their constructive feedback, which helped refine our claims, clarify limitations, and improve the overall presentation of VideoAgentTrek."}}, "id": "JS8ASxWUYh", "forum": "xxYPqm1qWz", "replyto": "xxYPqm1qWz", "signatures": ["ICLR.cc/2026/Conference/Submission4108/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4108/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission4108/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763737893222, "cdate": 1763737893222, "tmdate": 1763737893222, "mdate": 1763737893222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}