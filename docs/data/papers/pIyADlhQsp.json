{"id": "pIyADlhQsp", "number": 11830, "cdate": 1758204111664, "mdate": 1759897551941, "content": {"title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis", "abstract": "Multi-view diffusion models have shown promise in 3D novel view synthesis, but most existing methods adopt a non-autoregressive formulation. This limits their applicability in world modeling, as they only support a fixed number of views and suffer from slow inference due to denoising all frames simultaneously. To address these limitations, we propose CausNVS, a multi-view diffusion model in an autoregressive setting, which supports arbitrary input-output view configurations and generates views sequentially. We train CausNVS with causal masking and per-frame noise, using pairwise-relative camera pose encodings (CaPE) for precise camera control. At inference time, we combine a spatially-aware sliding-window with key-value caching and noise conditioning augmentation to mitigate drift. Our experiments demonstrate that CausNVS supports a broad range of camera trajectories, enables flexible autoregressive novel view synthesis, and achieves consistently strong visual quality across diverse settings.", "tldr": "CausNVS is an autoregressive diffusion model for next novel view synthesis with relative pose encoded attention (CaPE) and efficient KV cache inference, towards real-time world modelling, AR streaming and interactive online generation.", "keywords": ["multi-view diffusion", "novel view synthesis", "autoregressive generation", "world model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0009b3a6e2d64c2156bca90fd97f0818347e20e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed CausNVS, an autoregressive multi-view diffusion model for flexible NVS. Formally, CausNVS trained the multi-view diffusion with independent frame-wise noise to support the autoregressive generation. Then, CausNVS used CaPE to denote relative positional encoding, which is more robust than the popularly used absolute positional encoding (pluckr ray). Finally, the KV cache and window attention are used to improve the generation stability and reduce the inference cost. Experiments show the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper pointed out an interesting question for the autoregressive novel view synthesis, i.e., how to incrementally formulate the infinitely changed camera trajectories to AR models. Using relative positional encoding (CaPE) is somewhat effective, but this still suffers from some concerns, as detailed in the weaknesses.\n\n2. The presentation of this paper is clear and easy to follow. The authors provided sufficient implementation details in the appendix."}, "weaknesses": {"value": "1. This work presents an integrated system, but most of its core components (e.g., Diffusion Forcing, CaPE, KV cache, and window-wise attention map) have been previously proposed in existing literature, which weakens the paper’s novelty. The performance improvement is not significant (Table1 shows comparable and even inferior results compared to SEVA).\n\n2. A major concern is the CaPE’s design and efficacy for \"progressively larger poses\" for AR. \n\na) The first question is why CaPE does not require any normalization (\"scale sweeping\") as mentioned in Line 413-414? As formulated in Eq(2), CaPE is very similar to PRoPE[1] (it would be very helpful if the authors could discuss the relation between CaPE and PRoPE), while the unnormalized translation values will potentially undermine the numerical stability of attention computation. Depending on the metric methods and SfM pipelines used, some translation values could become excessively large, leading to unstable model training or inference. \n\n[1] Li R, Yi B, Liu J, et al. Cameras as relative positional encoding[J]. arXiv 2025.\n\nb) As shown in Figure 2, the first frame is consistently retained in the window-wise (frame-wise) attention. When the disparity between the first and last frames is extremely large (i.e., large translation values), the relative translation term in Eq. (2) will introduce distribution shifts between the training and inference processes. This shift may degrade the model’s generalization to address large-distance moving. So simply using the relative position encoding instead of the absolute one without proper normalization may not effectively address the large-pose challenge.\n\n3. Most experiments are just conducted on the in-domain dataset (real10k, dl3dv), while the out-of-distribution performance is not well studied. For example, the authors should consider datasets like Tanks-and-Temple, MipNerf360, and some stylized pictures to ensure generalization.\n\n4. To demonstrate the effect of AR and invariant pose encoding, the authors should include some convincing evidence, such as infinite or extremely long NVS generation along a specific way with good memory maintenance."}, "questions": {"value": "1. As shown in Figure 2, why is the first frame always included in the KV cache? I could not find any discussion for this point in the paper.\n\n2. The authors should clearly clarfiy why using CaPE is convincing to address the large pose issue for AR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IqBKM5dfSd", "forum": "pIyADlhQsp", "replyto": "pIyADlhQsp", "signatures": ["ICLR.cc/2026/Conference/Submission11830/Reviewer_67UX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11830/Reviewer_67UX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761391583310, "cdate": 1761391583310, "tmdate": 1762922849358, "mdate": 1762922849358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on novel view synthesis. The technical approach is learning-based, leveraging a multiview diffusion model equipped with augmentations including a Relative Camera Pose Encoding and causal mask. Experiments show that the proposed approach is comparable to state-of-the-art NVS models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The figure illustrations are clear. The attached website is informative."}, "weaknesses": {"value": "- I'm not too convinced by the results. In particular: \n(1) There is no comparison to camera-controlled video generation models such as https://research.nvidia.com/labs/toronto-ai/GEN3C/\n(2) There is no comparison to large NVS transformers, such as https://haian-jin.github.io/projects/LVSM/ and its follow-up works.\n(3) The video results in the website seem to have severe aliasing artifacts. Most examples have this issue.\n(4) The video results in the website seem to have blurry artifacts. For example, in the first section, first row, 3rd column, the chair becomes pretty blurry when the camera goes forward."}, "questions": {"value": "I think these may strengthen the submission:\n\n- Comparison to baselines from camera controlled video generation and large NVS models. \n- More video results and discussion on the aliasing/blurry artifacts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6cVnnRdRFS", "forum": "pIyADlhQsp", "replyto": "pIyADlhQsp", "signatures": ["ICLR.cc/2026/Conference/Submission11830/Reviewer_YFrZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11830/Reviewer_YFrZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771954844, "cdate": 1761771954844, "tmdate": 1762922848788, "mdate": 1762922848788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a causal diffusion model for novel view synthesis, which generates novel views sequentially. The proposed method adapts a multi-view diffusion model to a causal prediction setting by integrating several strategies, including causal masking in the attention layers, per-frame noise sampling, pairwise relative camera pose encodings, and a spatially-aware sliding-window mechanism with KV caching. Experiments demonstrate that this approach outperforms existing baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The motivation of synthesizing novel views in an autoregressive manner is a promising and important direction for achieving arbitrary-length view synthesis, which has significant practical value.\n* The proposed method achieves better results compared to the established baselines, demonstrating its effectiveness.\n* The paper is well-written, clearly structured, and easy to understand."}, "weaknesses": {"value": "* My primary concern is the paper's limited technical novelty. The method appears to be a skillful integration of several known techniques (causal masking[1], per-frame noise[1], relative pose encodings[2]) to adapt a non-causal model to a causal setting. While this combination is effective, the work does not seem to introduce new fundamental knowledge or concepts. The contribution lies more in the engineering and application of these components rather than in proposing a conceptually novel method.\n\n[1] Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural\nInformation Processing Systems (NeurIPS), 2024a.\n\n\n[2] Eschernet: A generative model for scalable view synthesis. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2024.\n\n* Regarding the baselines, I noticed the comparisons are focused primarily on other multi-view synthesis methods. I am curious how the proposed method stacks up against modern video generation models. Since these models excel at generating temporally coherent new frames, which is conceptually similar to sequential novel view synthesis. Could the authors provide a comparison with video-based novel view synthesis methods? This would help to better position the paper's contribution within the broader field.\n\n* The paper claims the ability to generate arbitrary-length sequences. However, the experiments seem to be conducted on relatively short sequences. I am skeptical about the model's ability to maintain long-term consistency. How does the model perform on much longer sequences (e.g., 100+ frames)? Does the quality degrade or do errors accumulate over time? An analysis or qualitative result on a long sequence would be necessary to fully substantiate this claim."}, "questions": {"value": "* I have a question regarding the use of CAPE for positional embeddings. This technique was originally designed for object-centric representations. How does it handle arbitrary camera paths in large, unbounded scenes? A discussion on the suitability of CAPE for this new, more general setting would be insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v6PVnzYYMa", "forum": "pIyADlhQsp", "replyto": "pIyADlhQsp", "signatures": ["ICLR.cc/2026/Conference/Submission11830/Reviewer_oyZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11830/Reviewer_oyZi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928324349, "cdate": 1761928324349, "tmdate": 1762922848359, "mdate": 1762922848359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an autoregressive framework for novel view synthesis to take into account previously generates frames. The approach introduces relative camera pose encodings to reuse the KV cache during inference. Additionally, it incorporates noisy frame conditioning and teacher forcing for autoregressive generation. Experiments are performed on RealEstate10K,  LLFF, DL3DV,  Long (Short) datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Teacher forcing and noisy frames during training a me noise conditioning during training encourage robustness to imperfect inputs and contexts. \n2. KV caching is used for efficiency at inference.\n3. The work proposes relative camera pose encodings (CaPE) which helps the model to adapt to shifting camera trajectory and reuse the KV cache efficiently."}, "weaknesses": {"value": "1. The focus is on improving the inference time performance and efficiency. Experiments do not show the inference time overhead of the models. The experiments do not report actual inference time, FLOPs, or latency metrics. Without these, it’s unclear whether the proposed design achieves meaningful speedups in practice.\n2. Quantitative results show that the models lags behind prior art in terms of PSNR metrics.\n3. What is the context window considered for autoregressive generation? How many past frames are considered?"}, "questions": {"value": "1. How does the approach compare in terms of inference time with respect to the prior work which generates views in parallel? How does this compare to the baselines without KV caching?\n2. How does the model adapt to long horizons. How is the drift mitigated with the increasing sequence length? \n3. Is it feasible to use this approach in real time applications? \n4. What is the minimum difference between the relative camera poses for the CaPE embeddings to be useful?\n5. Is it possible if the input frames are less than the generated that the model's performance can suffer compared to parallel approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AUtxLjhXIU", "forum": "pIyADlhQsp", "replyto": "pIyADlhQsp", "signatures": ["ICLR.cc/2026/Conference/Submission11830/Reviewer_xi5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11830/Reviewer_xi5L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041877287, "cdate": 1762041877287, "tmdate": 1762922847908, "mdate": 1762922847908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}