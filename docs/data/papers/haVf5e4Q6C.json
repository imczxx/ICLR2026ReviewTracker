{"id": "haVf5e4Q6C", "number": 21202, "cdate": 1758314857035, "mdate": 1759896935129, "content": {"title": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "abstract": "Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity—their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning.\nWe apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across four mathematical benchmarks—GSM8K, Math500, AMC and Minerva—achieving new state-of-the-art results for full-attention masked dLLMs.", "tldr": "IGPO, an RL method for diffusion LLMs that uses inpainting to inject partial reasoning hints when stuck with all-wrong responses, achieving SoTA results on math benchmarks for masked diffusion LLMs", "keywords": ["Diffusion Large Language Models", "Reinforcement Learning", "Inpainting", "Group Relative Policy Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61e14ceab88e89026729ae40e7c9fab42d02b6b0.pdf", "supplementary_material": "/attachment/2622f0929737944d17168a9c11a577c31eb75767.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces IGPO (inpainting guided policy optimization) for optimizing diffusion language models with RL which uses ground truth correct trajectories in order to learn in questions where all of the generations are incorrect. To do this, the algorithm resamples more trajectories for questions where all generations are wrong, but inserting text from the correct ground truth trajectory to be inpainted around. Followed by a standard GRPO update on it. This provides \"hints\" for the model during generation. The authors show that this increase performance in math reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- this paper is well written\n- this paper considers an important problem, both improving overall performance and utilizing ground truth responses.\n- the results of this paper are encouraging for the method."}, "weaknesses": {"value": "- while it does seem to work, the entropy-based gradient filtering seems like a method in order to deal with the off policy-ness of the hint responses. Being able to deal with this off-policy would be desirable instead (although harder).\n- the improvement is encouraging, but it is unclear whether it comes from increasing generation number or the hints (see questions)\n- My overall feeling is this seems to improve performance slightly, but there is more work to be done to show that this is the *right* way to use gold trajectories. Ie. How does it compare to distillation methods?"}, "questions": {"value": "- how many of these new responses get clipped? I feel like clipping would null many of these since the trajectory would be off policy?\n- I believe that the fair comparison would actually be to GRPO which oversampled on incorrect trajectories. Ie. Do hint tokens actually matter?\n- does `without Inpaint` in Figure 4 mean that one adds oversampled correct trajectories into the batch as well?\n- does SFT on the rewritten trajectories move it closer to on policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ixn5WrracJ", "forum": "haVf5e4Q6C", "replyto": "haVf5e4Q6C", "signatures": ["ICLR.cc/2026/Conference/Submission21202/Reviewer_LgBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21202/Reviewer_LgBR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494404473, "cdate": 1761494404473, "tmdate": 1762941612467, "mdate": 1762941612467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Inpainting-Guided Policy Optimization (IGPO), a reinforcement learning framework designed to enhance exploration in diffusion-based large language models under sparse reward settings. The key idea is to inject partial ground-truth reasoning traces as inpainting hints into masked regions during RL sampling, allowing the model to receive guided feedback while retaining its own generative reasoning. This strategy effectively mitigates the zero-advantage problem in group-based policy optimization methods such as GRPO, where uniformly incorrect responses yield zero gradients and inefficient learning. Experiments on mathematical reasoning datasets demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed use of inpainting for guided exploration in RL for dLLMs is a creative and well-motivated exploitation of architecture-specific inductive bias.\n\nThe approach is validated with thorough quantitative results on four widely recognized mathematical reasoning benchmarks,substantial gains over both prior masked dLLM methods and non-diffusion LLM baselines are reported\nAblation studies are thorough."}, "weaknesses": {"value": "The formulation of IGPO objective in the sampling procedure(Eq 5)  is difficult to follow. \n\nWhat does \"Advantages $A_{i}$ are computed normally\" mean?\n\nThe theoretical analysis is weak, provides limited justification for the proposed inpainting mechanism."}, "questions": {"value": "Could the authors provide theoretical analysis to justify why the proposed inpainting mechanism improves policy optimization performance?\n\nHow robust is the proposed trace rewriting method? Have the authors evaluated it against simpler rewriting heuristics or with different base models for inpainting?\n\nFor domains lacking high-quality reasoning traces, how does IGPO’s performance degrade or adapt under less informative supervision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "b34YLOV6FG", "forum": "haVf5e4Q6C", "replyto": "haVf5e4Q6C", "signatures": ["ICLR.cc/2026/Conference/Submission21202/Reviewer_L2eU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21202/Reviewer_L2eU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978747298, "cdate": 1761978747298, "tmdate": 1762941611318, "mdate": 1762941611318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces IGPO, a reinforcement learning framework that leverages the inpainting capability of masked diffusion large language models to overcome exploration inefficiency in RL finetuning. Instead of relying on full solutions, IGPO injects partial ground-truth reasoning hints during generation, guiding exploration toward promising reasoning paths while trying to maintain on-policy learning. Combined with a Length-Aligned Supervised Fine-Tuning stage using concise rewritten reasoning traces and entropy-based gradient filtering for stability, IGPO achieves state-of-the-art performance on GSM8K, Math500, AMC, and Minerva, improving sample efficiency and robustness over baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and thoughtfully presented, making its core ideas clear and easy to follow.\n\n1. The proposed algorithm elegantly exploits the unique bidirectional and inpainting capabilities of diffusion LLMs to form a guided exploration strategy that resolves the zero-advantage problem in RL finetuning.\n\n2. It demonstrates clear empirical gains, achieving state-of-the-art results across multiple mathematical reasoning benchmarks."}, "weaknesses": {"value": "1. The proposed method relies heavily on the unique characteristics of diffusion LLMs and is therefore not a general solution to the zero-advantage problem applicable to broader classes of language models.\n\n2. Compared with standard GRPO, IGPO additionally requires the RL dataset to include ground-truth reasoning paths, which further limits its applicability and use scenarios.\n\n3. The use of reasoning hints violates the on-policy requirement of underlying RL algorithms, potentially leading to bias in the optimization objective.\n\n4. Using reasoning hints during the RL process may constrain the model's ability to explore freely, making the final performance dependent on the quality of the provided expert reasoning hints."}, "questions": {"value": "1. IGPO assumes access to accurate ground-truth reasoning traces during RL. How robust is the method to imperfect or noisy reasoning annotations? Have the authors considered evaluating IGPO with reasoning traces of varying quality to assess its sensitivity?\n\n2. Would it be possible to include a discussion or comparison between IGPO and more recent approaches addressing the zero-advantage problem, such as [1]?\n\n[1] Le, Thanh-Long V., et al. \"No prompt left behind: Exploiting zero-variance prompts in llm reinforcement learning via entropy-guided advantage shaping.\" arXiv preprint arXiv:2509.21880 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1mzuqONykH", "forum": "haVf5e4Q6C", "replyto": "haVf5e4Q6C", "signatures": ["ICLR.cc/2026/Conference/Submission21202/Reviewer_raEu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21202/Reviewer_raEu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014159699, "cdate": 1762014159699, "tmdate": 1762941610634, "mdate": 1762941610634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a policy optimization for diffusion-based LLMs that bridges supervised fine-tuning and reinforcement learning. Specifically, it introduces Inpainting Guided Policy Optimization (IGPO), which leverages the unique inpainting capabilities of full-attention dLLMs by strategically injecting partial ground-truth reasoning traces into online policy optimization. With this design as well as some other techniques such as entropy-based filtering, they achieve better sample efficiency than GRPO and achieve SOTA among full-attention masked dLLMs across four mathematical reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper presents the first work to utilize the unique inpainting capabilities of diffusion LLMs\nfor reinforcement learning, which is novel.\n+ The method achieves SOTA results on four benchmarks for full-attention based dLLMs.\n+ The ablation studies demonstrate insights of the proposed method, such as how self-generated inpainted traces provide a better learning signal than ground truth traces."}, "weaknesses": {"value": "The proposed method is a hybrid between supervised learning (or imitation learning in the general RL field) and online reinforcement learning. The authors should compare or at least discuss other guided exploration methods in the general RL domain (especially in areas like diffusion policy) to give more insights into how this method is particularly good for dLLMs and how it might be or might not be transferable to tuning other diffusion-based models (say, video diffusion models or diffusion policies). Admittedly, the full-attention dLLMs have their unique properties."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Vmhs8ug9y", "forum": "haVf5e4Q6C", "replyto": "haVf5e4Q6C", "signatures": ["ICLR.cc/2026/Conference/Submission21202/Reviewer_NuRc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21202/Reviewer_NuRc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762296622068, "cdate": 1762296622068, "tmdate": 1762941609664, "mdate": 1762941609664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}