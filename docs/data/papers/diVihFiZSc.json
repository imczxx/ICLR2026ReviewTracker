{"id": "diVihFiZSc", "number": 5408, "cdate": 1757907716810, "mdate": 1763381455378, "content": {"title": "Continuity-Regularized Flow Matching for Offline Reinforcement Learning", "abstract": "Flow-matching policies have recently emerged as a powerful class of generative models for offline reinforcement learning (RL), capable of capturing complex, multi-modal action distributions from static datasets. However, standard training objectives are largely agnostic to the global properties of the generative path, permitting learned vector fields that are irregular and unstable, which can hinder performance. In this work, we introduce PDE-regularized Q-Learning (PQL), a novel algorithm that addresses this limitation by imposing a principled structure on the entire probability flow. PQL makes two synergistic contributions: first, a partial differential equation based regularizer derived from the continuity equation enforces global smoothness and stability on the flow. Second, to solve the complex optimization problem introduced by this regularizer, we propose a Beta-distributed timestep sampling strategy that focuses learning on the critical trajectory segments where the trade-off between imitation and smoothness is most acute. Through extensive experiments, we demonstrate that by structuring the generative journey and not just its destination, PQL achieves state-of-the-art performance on a wide range of challenging offline RL tasks.", "tldr": "", "keywords": ["Offline Reinforcement Learning", "Flow Policy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52bc04bd93f7292c0b218a7ccd5348848a764d6d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work puts forth PQL i.e. PDE regularized Q-learning for offline RL setting towards improving flow-matching policies. To this end, the paper identifies that standard flow-matching methods often ignore the trajectory path, while just ensuring the starting point and the target point of the generative process are correctly accounted for. PQL attempts to ameliorate the path inconsistency issue with a PDE assisted regularizer that essentially penalizes the learnt vector field to maintain smoothness and stability. A beta-distributed timestep sampler is introduced to specifically focus on the intermediate trajectory where the conversion from pure noise to a very meaningful action while \nbalancing between imitation and smoothness is critical at training time. Experimental evaluations are performed on relevant offline RL and IL task environments with comparison against benchmark algorithms and ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. PDE based regularization is an innovative approach towards enforcing generative path stability in flow matching policy algorithms.\n\n2. The adaptive timestep sampling strategy while presented in the context of handling the PDE regularizer, appears to be a principled way to improve solution tractability with flow-based constrained optimization setups.\n\n3. The practical utility of PQL is supported by improvements seen in benchmarking experiments. Hyper-parameter tuning analysis and ablation studies further strengthen credibility."}, "weaknesses": {"value": "1. It is not clear computational overhead added by the Jacobian-based regularizer and how does that component ultimately trade-off in performance gains.\n\n2. The Jacobian penalty while handles deformation of the flow, appears to be state agnostic. It deserves a separate study whether PQL would over-smoothen policies in critical intermediate states where complex, sharp actions were actually optimal.\n\n3. There are multiple typos and grammatical mistakes throughout the paper that need to be corrected."}, "questions": {"value": "In addition to the weakness comments, I request authors' response to the following questions :\n\n1. How does PQL's wall-clock training time compare to standard flow-matching algorithms ?\n\n2. The beta distributed adaptive seems like the intuitive next step beyond a uniform sampler. Is it worth investigating whether other distribution classes might work here ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dDbifdKQoU", "forum": "diVihFiZSc", "replyto": "diVihFiZSc", "signatures": ["ICLR.cc/2026/Conference/Submission5408/Reviewer_Y89o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5408/Reviewer_Y89o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760853178760, "cdate": 1760853178760, "tmdate": 1762918042866, "mdate": 1762918042866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a regularizer into the flow-matching-based policy to improve training stability and proposes a Beta-distributed time sampling strategy that enables stable and efficient optimization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments are thorough and well-executed, demonstrating solid empirical results.\n\n2. The paper provides sufficient theoretical justification and proofs."}, "weaknesses": {"value": "This paper appears to have been written somewhat hastily. In addition to several obvious typos and inconsistencies, there are also issues in understanding and experimental setup, as detailed below:\n\n1. There are several typos. For example, in the Introduction, the last item of the listed contributions does not include the method name, which looks like a placeholder from a template.\n\n2. The paper’s writing is sometimes difficult to follow, and the presentation could be improved.\n\n3. The authors claim that the introduced regularizer improves training stability, but in Section 5.2, it is not clearly shown that the model without the regularizer is unstable.\n\n4. The Introduction mentions that the Beta-distributed time sampling strategy also contributes to training stability, yet there is no direct evidence or analysis of stability in the experiments. Moreover, this Beta-distributed time sampling strategy is not discussed in the Method section.\n\n5. No code"}, "questions": {"value": "1. In Section 5.2, the results do not clearly show that the version without the regularizer is unstable—could the authors clarify this?\n\n2. Why is the Beta-distributed time sampling strategy mentioned in the Introduction and Appendix, but not described in the Method section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uUnyFb2UkR", "forum": "diVihFiZSc", "replyto": "diVihFiZSc", "signatures": ["ICLR.cc/2026/Conference/Submission5408/Reviewer_YF2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5408/Reviewer_YF2m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398656622, "cdate": 1761398656622, "tmdate": 1762918042567, "mdate": 1762918042567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PDE-regularized Q-learning (PQL), an extension of Flow Q-Learning (FQL) [1] by regularizing the Frobenius norm of the Jacobian $\\nabla_a u_\\theta(s, a, t)$ of the action policy $u_\\theta(s, a, t)$. The intuition is that by controlling the Frobenius norm, the 2-Wasserstein distance between the true marginal probability $\\rho_t^*(\\cdot|s)$ and $\\rho_t^\\theta(\\cdot| s)$. It provides a theoretical upper bound, provided the boundedness of the action policy. Furthermore, it introduces Hutchinson's trace estimator with JVP autodifferentiation to make memory-efficient computation. PQL is validated on D4RL, Adroit, and OGBench and demonstrates performance improvement in comparison with FQL.\n\n[1] Park, Seohong, Qiyang Li, and Sergey Levine. \"Flow q-learning.\" arXiv preprint arXiv:2502.02538 (2025).\n[2] Hoffman, Judy, Daniel A. Roberts, and Sho Yaida. \"Robust learning with Jacobian regularization.\" arXiv preprint arXiv:1908.02729 (2019)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Improvements in performance in different benchmarks\n2. Theoretical guarantees for path stability are provided\n3. Experiment validation is quite thorough\n4. The method is straightforward to implement"}, "weaknesses": {"value": "1. The idea of doing Jacobian regularization is not new e.g. [2]\n2. While the performance improvement exists, it seems to be only a marginal improvement.\n3. RL is always sensitive to hyperparameters. Introducing new regularizers likely increases the search space for tuning hyperparameters.\n4. Jacobian regularization does not guarantee the boundedness of the Lipschitz constant. The assumption might not be correct. Furthermore, if $J$ becomes large, the exponential term in the bound will make the bound too loose."}, "questions": {"value": "1. How large is the actual Lipschitz constant of the final policy? Could you quantify this empirically to show the correlation?\n2. It seems that we are just learning a robust policy that is robust to perturbation in actions rather than improving the training stability. Is this fixing training dynamics or just learning smoother policies?\n3. In line 073, you did not modify the name of your method.\n4. In Table 4, why do you think that beta sampling is improving the performance significantly?\n5. What is the overhead of computing the Jacobian?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yXLHSuxYpK", "forum": "diVihFiZSc", "replyto": "diVihFiZSc", "signatures": ["ICLR.cc/2026/Conference/Submission5408/Reviewer_ZP6n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5408/Reviewer_ZP6n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542431717, "cdate": 1761542431717, "tmdate": 1762918042305, "mdate": 1762918042305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the issue in previous Flow-matching methods, where point-to-point optimization often neglects the global properties and smoothness of the generative path, this work introduces a regularizer based on partial differential equations (PDEs) to constrain the learning process. Additionally, a Beta-distributed time sampling strategy is proposed to improve the optimization efficiency of this regularizer. Experimental results on multiple offline RL benchmarks, including D4RL, Adroit, and OGBench, demonstrate the effectiveness of the proposed approach. Furthermore, the authors provide theoretical justification for the method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The manuscript exhibits a coherent and logical structure, employs precise and formal language, and presents a clearly defined motivation that is readily understandable.\n2. The study includes a wide range of rigorous experiments, offering insightful analysis of the outcomes and thorough evaluation of the model’s structure and hyperparameters.\n3. The paper is grounded on a solid theoretical foundation, and the provided theoretical analysis offers principled support for the effectiveness of the proposed model."}, "weaknesses": {"value": "1. The experimental baselines do not include comparisons with diffusion-based methods such as Decision Diffuser, Diffuser, or Diffuser-Lite, which would provide a more comprehensive evaluation.\n2. The manuscript does not discuss the limitations of the proposed approach, which is important for understanding its scope and potential drawbacks.\n3. A placeholder remains in the Introduction: the third point summarizing the contributions still contains “[Your Method Name]” and should be properly updated."}, "questions": {"value": "1. Could the authors provide the performance results of their method on the Kitchen environment?\n2. The proposed approach introduces a relatively large number of hyperparameters. While the experiments analyzing hyperparameters in the paper are appreciated, could the authors provide practical guidelines or recommendations for selecting hyperparameters to facilitate rapid adaptation to new tasks?\n3. Have the authors considered alternative strategies to reduce the model’s sensitivity to hyperparameter choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rCpnkurOBu", "forum": "diVihFiZSc", "replyto": "diVihFiZSc", "signatures": ["ICLR.cc/2026/Conference/Submission5408/Reviewer_Mgme"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5408/Reviewer_Mgme"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940098061, "cdate": 1761940098061, "tmdate": 1762918042018, "mdate": 1762918042018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}