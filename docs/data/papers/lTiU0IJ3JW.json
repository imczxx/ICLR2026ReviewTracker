{"id": "lTiU0IJ3JW", "number": 24142, "cdate": 1758353278812, "mdate": 1759896779928, "content": {"title": "Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation", "abstract": "Diffusion models have emerged as powerful generative models for graph generation, yet their use for conditional graph generation remains a fundamental challenge. In particular, guiding diffusion models on graphs under arbitrary reward signals is difficult: gradient-based methods, while powerful, are often unsuitable due to the discrete and combinatorial nature of graphs, and non-differentiable rewards further complicate gradient-based guidance. We propose Graph Guided Diffusion (GGDiff), a novel guidance framework that interprets conditional diffusion on graphs as a stochastic control problem to address this challenge. GGDiff unifies multiple guidance strategies, including gradient-based guidance (for differentiable rewards), control-based guidance (using control signals from forward reward evaluations), and zeroth-order approximations (bridging gradient-based and gradient-free optimization). This comprehensive, plug-and-play framework enables zero-shot guidance of pre-trained diffusion models under both differentiable and non-differentiable reward functions, adapting well-established guidance techniques to graph generation. Our formulation balances computational efficiency, reward alignment, and sample quality, enabling practical conditional generation across diverse reward types. We demonstrate the efficacy of GGDiff in various tasks, including constraints on graph motifs, fairness, and link prediction, achieving superior alignment with target rewards while maintaining diversity and fidelity.", "tldr": "GGDiff performs graph diffusion via stochastic control, handling both differentiable and non-differentiable rewards for conditional graph generation, outperforming baselines on constraints, fairness, and link prediction tasks", "keywords": ["Graph Guided Diffusion", "Graph Conditional Generation", "Diffusion Models", "Generative Models for Graphs"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef82cfebbfbd469ea81a68aba0e81a8afee30b44.pdf", "supplementary_material": "/attachment/188b8f567c7b2e302616d0586a455469d7890c02.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework named Graph Guided Diffusion (GGDiff) to address a key challenge in conditional graph generation with diffusion models—specifically, the difficulty of effectively guiding the generation process when the reward signal is arbitrary and non-differentiable (e.g., combinatorial rewards). The framework is plug-and-play, enabling zero-shot guidance of pre-trained diffusion models and supporting both differentiable and non-differentiable reward functions, achieving a good balance among computational efficiency, reward alignment, and sample quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides an alternative to reinforcement learning (RL) for generation with non-differentiable scoring functions.  \n2. Theoretically sound and clearly written."}, "weaknesses": {"value": "1. Evaluation is limited, with comparisons against only a few baseline methods.  \n2. Generation efficiency under non-differentiable scoring functions is not assessed, which is critical for real-world applications."}, "questions": {"value": "1. The most impactful application of conditional graph generation is molecular generation. Please include results on standard benchmarks such as QM9 and ZINC.  \n2. Please provide a comparison of sampling efficiency with and without GGDiff.  \n3. Is it necessary to apply guidance at every diffusion step? Please include an ablation study where GGDiff is applied only during the final 10% of steps.  \n4. Please evaluate the sample efficiency of GGDiff. Given that many reward functions are computationally expensive, it is important to analyze the trade-off between the number of reward function evaluations and generation quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eks3xNes3n", "forum": "lTiU0IJ3JW", "replyto": "lTiU0IJ3JW", "signatures": ["ICLR.cc/2026/Conference/Submission24142/Reviewer_cZPL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24142/Reviewer_cZPL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761207218697, "cdate": 1761207218697, "tmdate": 1762942957041, "mdate": 1762942957041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of conditional graph generation and proposes a unified framework grounded in stochastic optimal control (SOC). The key contribution lies in introducing a gradient-free approach that avoids the intractability of gradient-based optimization through the use of zero-order optimization techniques.\n\nThe proposed framework can handle both differentiable and non-differentiable reward functions. Within this framework, the authors present a family of methods: a standard gradient-based approach and several gradient estimators based on single-point, best-of-N, and multi-point (average random search) strategies. The methods are evaluated on multiple datasets and tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Expect for the experimental section, the paper is clearly written and easy to follow, even for readers less familiar with stochastic optimal control. The presentation is structured and well-motivated.\n\nThe proposed method is conceptually sound and supported by rigorous derivations. The framework is general and theoretically appealing, providing a principled approach to conditional generation with non-differentiable objectives.\n\nThe empirical evaluation covers a wide range of datasets and tasks, illustrating the adaptability of the proposed approach."}, "weaknesses": {"value": "### Comparison with Standard Guidance Methods\nWhile the framework is motivated as an alternative to existing guidance-based conditional generation methods, the paper does not include direct comparisons with standard guidance approaches for graphs, such as classifier guidance or classifier-free guidance. Including these as baselines in the experimental section would substantially strengthen the empirical validation and support the paper’s claims.\n\nIn addition, the paper argues that these guidance methods require the differentiability of the target property. I respectfully disagree with this claim. Classifier-free guidance, in particular, does not inherently require differentiability of the targeted property. Even for classifier guidance, one can train a differentiable surrogate model to approximate a non-differentiable objective, which can serve as a practical workaround.\nIt would considerably reinforce the paper’s contribution to demonstrate empirically that such approaches fail or underperform under the same conditions, thereby highlighting the advantages of the proposed method more convincingly.\n\n### Evaluation and Baselines\nIn many experiments, the proposed method is compared against only a single alternative model, whereas several other strong guidance methods exist in the literature. Expanding the comparison to include more recent or diverse methods would provide a clearer picture of the strengths and weaknesses of the proposed approach.\n\nMoreover, Table 4 suggests that the method’s improved effectiveness may come at the cost of reduced diversity (as reflected by lower uniqueness). This trade-off deserves further analysis and discussion. It would also be helpful to specify which dataset was used for the experiments reported in Table 4, as this information is currently unclear.\n\n### Model Backbone\nThe experimental setup builds upon GDSS, a relatively early diffusion-based model for graphs. As the field has advanced significantly, it would strengthen the empirical validation to evaluate the proposed approach on more recent and robust backbones, such as Grum or CatFlow [1]. Demonstrating that the proposed guidance mechanism is effective across different backbones would further support its generality.\n\n### Methodological Limitations\nGraphs are inherently discrete structures, and recent state-of-the-art models (e.g., DeFog [2], SID [3]) are based on discrete diffusion. However, the proposed framework relies on continuous diffusion backbones, which limits its applicability to a subset of graph generation problems. Exploring extensions or adaptations for discrete generative processes could significantly broaden the impact of this work.\n\nAdditionally, the proposed multi-point estimators appear computationally demanding, as indicated in Appendix E. \n\n### Clarity and Presentation\nWhile the paper is well-written overall, it can sometimes be difficult to distinguish which components are novel contributions and which are directly drawn from the stochastic optimal control literature. Providing clearer separation between prior work and original developments would enhance the paper’s readability.\n\nUnlike the rest of the paper, the evaluation section is somewhat difficult to follow. Several pieces of information are missing (for example, the dataset used for Table 4), some elements are only indirectly referenced (such as the evaluation metrics described in Section 4.2), and certain notations (e.g., $\\Delta$ MMD) are not clearly defined. Improving the clarity and completeness of this section would greatly enhance the overall readability and impact of the paper.\n\n--------\n\n[1] Floor Eijkelboom, Grigory Bartosh, Christian A. Naesseth, Max Welling, and Jan-Willem van de\nMeent. Variational flow matching for graph generation. In The Thirty-eighth Annual Confer-\nence on Neural Information Processing Systems, 2024. URL https://openreview.net/\nforum?id=UahrHR5HQh.\n\n[2] Yiming Qin, Manuel Madeira, Dorina Thanou, and Pascal Frossard. Defog: Discrete flow matching\nfor graph generation. In Forty-second International Conference on Machine Learning, 2025. URL\nhttps://openreview.net/forum?id=KPRIwWhqAZ.\n\n[3] Y. Boget. Simple and critical iterative denoising: A recasting of discrete diffusion in graph generation. In Proceedings of the 42th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, July 2025."}, "questions": {"value": "Please, see suggestion in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BMwUTsUEZq", "forum": "lTiU0IJ3JW", "replyto": "lTiU0IJ3JW", "signatures": ["ICLR.cc/2026/Conference/Submission24142/Reviewer_KRg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24142/Reviewer_KRg4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761302823745, "cdate": 1761302823745, "tmdate": 1762942955991, "mdate": 1762942955991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GGDiff, a plug-and-play framework for controllable graph generation based on stochastic optimal control (SOC).\nInstead of training a new model, the authors reinterpret conditional diffusion as a control problem. They introduce a control signal Ut​ , during the sampling process to steer the trajectory toward graphs that satisfy a given reward or constraint.\nThe key idea is that this framework can handle both differentiable and non-differentiable rewards: for the former, they use gradient-based guidance; for the latter, they adopt zeroth-order optimization to approximate the control direction. The method builds on pre-trained graph diffusion models and does not require any retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel theoretical framing\nThe paper reformulates conditional graph diffusion as a stochastic optimal control (SOC) problem, which provides a unified theoretical view for guided generation. This is conceptually new in the graph domain.\n2. Training-free \nThe method only modifies the sampling process and does not require retraining or auxiliary classifiers, making it lightweight and compatible with existing pretrained diffusion models."}, "weaknesses": {"value": "1. Lack of stability or convergence analysis\nThe paper does not analyze how control strength (λ) or step size affects sampling stability.\nSince the control term directly modifies the diffusion dynamics, large or inconsistent Ut may cause sampling divergence, but this is not discussed.\n2. High variance in zeroth-order (ZO) gradient estimation\nFor non-differentiable rewards, ZO estimation introduces significant stochastic noise, especially in high-dimensional graph spaces.\nThe method lacks mechanisms (e.g., variance reduction, adaptive sampling) to mitigate this issue.\n2.No evaluation of computational efficiency\nZO optimization requires multiple reward evaluations per step, but the paper does not report runtime or sampling cost, leaving scalability unclear.\n4. Narrow fairness evaluation\nOnly ΔDP (dyadic parity) is reported. Other fairness metrics would make the evaluation more comprehensive."}, "questions": {"value": "1. On the choice of SOC vs. RL:\nSince reinforcement learning (RL) can also optimize non-differentiable rewards, what are the concrete advantages of the SOC formulation compared to an RL-based policy optimization framework?\nDid the authors attempt or consider an RL baseline for comparison?\n2. Variance in zeroth-order optimization:\nZeroth-order (ZO) gradient estimation can be quite noisy in high-dimensional graph spaces.\nHow sensitive is GGDiff to the number of sampled directions or to the noise level?\nHave the authors explored any variance reduction strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gzQl5BUn17", "forum": "lTiU0IJ3JW", "replyto": "lTiU0IJ3JW", "signatures": ["ICLR.cc/2026/Conference/Submission24142/Reviewer_vgtB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24142/Reviewer_vgtB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988743523, "cdate": 1761988743523, "tmdate": 1762942954439, "mdate": 1762942954439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}