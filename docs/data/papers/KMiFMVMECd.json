{"id": "KMiFMVMECd", "number": 21994, "cdate": 1758324494765, "mdate": 1759896891934, "content": {"title": "Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks", "abstract": "Enterprise teams rely on the Jira Query Language (JQL) to retrieve and filter issues from Jira.\nYet, to our knowledge, there is no open, real-world, execution-based benchmark for mapping\nnatural language queries to JQL. We introduce Jackal, a novel, large-scale text-to-JQL\nbenchmark comprising 100,000 natural language (NL) requests paired with validated JQL\nqueries and execution-based results on a live Jira instance with over 200,000 issues. To reflect\nreal-world usage, each JQL query is associated with four types of user requests: (i) Long NL,\n(ii) Short NL, (iii) Semantically Similar, and (iv) Semantically Exact. We release Jackal, a\ncorpus of 100,000 text-to-JQL pairs, together with an execution-based scoring toolkit, and\na static snapshot of the evaluated Jira instance for reproducibility. We report text-to-JQL\nresults on 23 Large Language Models (LLMs) spanning parameter sizes, open and closed\nsource models, across execution accuracy, exact match, and canonical exact match. In this\npaper, we report results on Jackal-5K, a 5,000-pair subset of Jackal. On Jackal-5K, the best\noverall model (Gemini 2.5 Pro) achieves only 60.3% execution accuracy averaged equally\nacross four user request types. Performance varies significantly across user request types:\n(i) Long NL (86.0%), (ii) Short NL (35.7%), (iii) Semantically Similar (22.7%), and (iv)\nSemantically Exact (99.3%). By benchmarking LLMs on their ability to produce correct and\nexecutable JQL queries, Jackal exposes the limitations of current state-of-the-art LLMs and\nsets a new, execution-based challenge for future research in Jira enterprise data.", "tldr": "We release Jackal, a 100,000 text-to-JQL benchmark with execution-based scoring and a Jira snapshot, evaluating 23 LLMs on execution accuracy, exact match, and canonical exact match.", "keywords": ["Semantic Parsing", "Text-to-Query", "Natural Language Interfaces", "Large Language Models (LLMs)", "Benchmarking", "Execution-based Evaluation", "Enterprise Data Systems", "Jira Query Language (JQL)", "Query Generation", "Text-to-SQL"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c25c6985c90ed1c690950e13fc8e4cdca4033183.pdf", "supplementary_material": "/attachment/d2ebcf8b1b69bd76903335e23f00ce4520160029.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents Jackal, the first large-scale, execution-based benchmark for evaluating large language models on the text-to-JQL task. It constructs 100,000 natural language–to–Jira Query Language (JQL) pairs validated on a live Jira instance with over 200,000 issues. Each JQL query is associated with four linguistic variants (Long NL, Short NL, Semantically Similar, and Semantically Exact) to reflect realistic query diversity. The benchmark evaluates 23 open and closed LLMs using execution accuracy as the primary metric, revealing that even the best models (e.g., Gemini 2.5 Pro) achieve only around 60 % overall accuracy, with strong sensitivity to phrasing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. Execution-based realism. Jackal grounds evaluation in actual Jira executions, providing a reproducible and practical measure of correctness beyond superficial string matching.\n\nS2. Scale and diversity. With 100 K pairs and four natural-language variants per query, it captures a wide spectrum of real-world phrasing styles rarely seen in prior text-to-DSL datasets.\n\nS3. Comprehensive benchmarking. Evaluating 23 LLMs—spanning proprietary and open-source models—offers a valuable cross-model comparison and baseline for future text-to-DSL research."}, "weaknesses": {"value": "W1. Examples of JQL appear too late in the paper.\nThe first concrete example only appears in Figure 2 and Section 3.2, while richer multi-clause JQLs are deferred to Appendix A (Table 2, p. 13). Most readers unfamiliar with JQL syntax cannot easily understand the benchmark’s linguistic or structural challenges until late in the paper. This weakens accessibility and motivation. A concise JQL primer with several example queries and their natural-language counterparts should be placed early (e.g., in the Introduction or Section 2) to help readers grasp what a JQL looks like and why translating to it is challenging.\n\nW2. Limited discussion of what makes text-to-JQL hard and how prompting was handled.\nAlthough the authors note that JQL involves custom fields, permissions, and linked-issue traversals, they do not deeply explain why mapping text to such constructs is particularly hard for LLMs. Moreover, the paper lacks clarity about the prompting setup during evaluation—what context (schema, field values, example queries) was given to each model. Without specifying whether models saw project-specific schemas or value enumerations, it is difficult to interpret why models fail on Short or Semantically Similar variants. Providing concrete prompt examples and an ablation on context richness would strengthen the technical depth\n\nW3. Unclear differentiation from text-to-SQL..\nSection 2.5 briefly mentions that JQL differs from SQL due to non-relational data, permissions, and linked-issue graphs, but the paper never empirically contrasts the two tasks. The benchmark could have highlighted distinct linguistic or structural phenomena (e.g., relative dates, visibility filters) through comparative analysis or shared-metric baselines. Without such evidence, the contribution risks appearing as a narrower variant of existing text-to-SQL work. For example, although text-to-SQL and text-to-visualization are very different tasks, techniques for them based on LLMs are very similar.\n\nW4. Missing comparison with simple RAG or retrieval-augmented baselines.\nMany of the difficult Short or Semantically Similar examples could plausibly be solved with retrieval of schema descriptions or example JQL templates, yet no RAG baseline is tested. A simple retrieval-plus-generation pipeline could provide a meaningful reference point, revealing whether LLM failures stem from lack of schema memory or reasoning ability. Including such baselines would clarify whether the challenge lies in recall or compositionality.\n\nW5. Lack of broader generalization analysis and verifiability.\nAll evaluations are conducted on a single Jira instance snapshot, which raises two interrelated concerns: generalization and reproducibility. Because each Jira instance has its own customized schema, permission structure, and field vocabulary, results derived from one snapshot may not hold for other configurations. Without experiments on multiple instances or unseen project schemas, the paper cannot convincingly claim model generalizability.\n\nMoreover, although the authors promise to release a static snapshot, the dataset and benchmark code are not yet publicly available. This makes it impossible for reviewers to independently verify the reported execution accuracies or replicate the evaluation pipeline. For a benchmark paper, reproducibility and openness are central to credibility. Unless the data, schema snapshot, and execution toolkit are released at submission time, the community has no way to trust that the reported results are accurate or that the benchmark functions as described."}, "questions": {"value": "See the above weaknesses W1-W5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fLApTNoPst", "forum": "KMiFMVMECd", "replyto": "KMiFMVMECd", "signatures": ["ICLR.cc/2026/Conference/Submission21994/Reviewer_B6kM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21994/Reviewer_B6kM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760763602511, "cdate": 1760763602511, "tmdate": 1762942011297, "mdate": 1762942011297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Jackal consists of 100,000 real-world natural language (NL) and JQL query pairs validated on a live Jira instance containing over 200,000 issues. Each JQL query is accompanied by four natural language variants—Long NL, Short NL, Semantically Similar, and Semantically Exact—to reflect diverse enterprise user intents. The benchmark includes an execution-based scoring toolkit and a reproducibility package."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "By establishing a reproducible and realistic evaluation framework for enterprise query translation, Jackal bridges an important gap between academic benchmarks and production-grade tasks."}, "weaknesses": {"value": "Although the dataset is described as real-world, the queries are generated programmatically and paraphrased by LLMs rather than collected from actual users. This may limit linguistic authenticity and introduce model biases into the data.\n\nGiven that JQL can differ across organizations due to custom fields and permissions, it remains unclear how models trained on Jackal would generalize to unseen instances. A cross-instance validation experiment, even at smaller scale, would make the benchmark more robust."}, "questions": {"value": "The dataset relies heavily on LLM-generated paraphrases. How do the authors ensure that the test data are not inadvertently contaminated by model-generated biases that might favor certain evaluated LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Mc07rwQIm", "forum": "KMiFMVMECd", "replyto": "KMiFMVMECd", "signatures": ["ICLR.cc/2026/Conference/Submission21994/Reviewer_vHic"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21994/Reviewer_vHic"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761014509179, "cdate": 1761014509179, "tmdate": 1762942011030, "mdate": 1762942011030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Jackal, a benchmark for natural-language-to-Jira Query Language (JQL) translation. The dataset includes 100K synthetic NL–JQL pairs generated with an LLM and validated on a live Jira instance. The authors evaluate 23 LLMs on a 5K subset (Jackal-5K) using execution accuracy, exact match, and canonical exact match. The key finding is that even the best proprietary models achieve only ~60% execution accuracy, exposing the difficulty of text-to-JQL translation. While the dataset is large and practical, the paper’s contribution is primarily engineering rather than research."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1. Constructing an execution-based benchmark for JQL is a useful community resource.\n\n- S2. The dataset appears systematically filtered and validated, with reproducibility in mind.\n\n- S3. The empirical evaluation covers a broad range of models and gives an approximate performance landscape."}, "weaknesses": {"value": "- W1 (No novelty). The benchmark design directly mirrors Text-to-SQL datasets (Spider, BIRD, Spider 2.0). There is no conceptual or methodological innovation—only a domain change.\n\n- W2 (Synthetic data). All natural-language queries are generated by an LLM and only lightly spot-checked. This makes the dataset less reliable for linguistic or semantic evaluation.\n\n- W3 (Weak analysis). The results section is mostly leaderboard numbers; there is no meaningful error taxonomy, model insight, or analysis of what linguistic phenomena cause failure.\n\n- W4 (Circular evaluation risk). Because an LLM generated the NL queries and another LLM is tested on them, there may be style leakage or artificial easiness/hardness not discussed.\n\n- W5 (Lack of research contribution). The paper reads like a technical report. There’s no new algorithm, learning strategy, or evaluation framework that would justify inclusion in a top ML venue."}, "questions": {"value": "- Q1. How diverse are the LLM-generated natural-language variants? Any quantitative measure of linguistic diversity or entropy across prompt types?\n\n- Q2. Are the results reproducible if the Jira instance evolves (e.g., issue deletions or field changes)?\n\n- Q3. Could the same methodology be applied to any DSL trivially? If yes, what scientific insight does JQL bring?\n\n- Q4. Did you analyze whether models simply memorize structural patterns of JQL rather than truly reasoning about semantics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Sk4DQO8DO6", "forum": "KMiFMVMECd", "replyto": "KMiFMVMECd", "signatures": ["ICLR.cc/2026/Conference/Submission21994/Reviewer_tpaw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21994/Reviewer_tpaw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983738362, "cdate": 1761983738362, "tmdate": 1762942010778, "mdate": 1762942010778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces \"Jackal,\" a novel, large-scale, execution-based benchmark for evaluating Large Language Models (LLMs) on the task of translating natural language to Jira Query Language (JQL). It effectively identifies a significant gap in existing resources and presents a comprehensive solution, complete with a dataset, an evaluation toolkit, and a reproducibility package. The empirical evaluation of 23 LLMs is thorough and reveals critical limitations in current state-of-the-art models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Fills a Critical Research Gap: The paper's primary contribution is addressing the notable absence of a high-quality, public benchmark for Text-to-JQL. It correctly identifies the limitations of existing small-scale, non-execution-based datasets. By providing 100,000 validated NL-JQL pairs, execution results on a live Jira instance with over 200,000 issues, and a full reproducibility package, Jackal enables rigorous, open research in an area with significant real-world enterprise relevance.\n\n2. Well-Designed and Realistic Benchmark Framework: The benchmark construction is thoughtful and reflects real-world usage: 1) The inclusion of four distinct user query variants (Long NL, Short NL, Semantically Similar, Semantically Exact) effectively captures the spectrum of how users naturally formulate requests, moving beyond literal translations. 2) The emphasis on execution accuracy as the primary metric is methodologically sound and aligns with best practices in semantic parsing, correctly avoiding the pitfalls of string-based matching (EM, CEM), which the results show to be nearly useless. 3) Grounding the evaluation in a real, large-scale Jira instance ensures the queries are valid and the results are practically meaningful.\n\n3. Comprehensive and Revealing Empirical Evaluation: The scale of the model evaluation is impressive, covering 23 proprietary and open-weight LLMs across various sizes. The results are compelling and clearly demonstrate that even the best models (e.g., Gemini 2.5 Pro at 60.3% accuracy) struggle significantly, especially with under-specified (Short NL) and paraphrased (Semantically Similar) queries. This analysis provides a valuable snapshot of the current capabilities and limitations of LLMs for this specific enterprise task."}, "weaknesses": {"value": "1. Limited Technical Novelty: The paper's core contribution is the benchmark itself rather than a novel methodological advancement. It applies established practices from Text-to-SQL (execution-based evaluation, multiple query types) to a new domain (JQL). The error analysis, while useful, remains somewhat high-level and does not delve deeply into the root causes of failure modes or propose specific, novel technical solutions beyond suggesting known directions like constrained decoding.\n\n2. Limitations in Evaluation Scope and Analysis: 1) The decision to report results only on the 5,000-pair \"Jackal-5K\" subset, while understandable for initial publication, underutilizes the full 100,000-pair dataset. A scaling analysis or results on the full set would have been more powerful. 2) The study focuses exclusively on LLMs. A comparison with older, rule-based or specialized semantic parsing approaches for JQL would have provided better context for understanding the absolute and relative progress represented by LLMs. 3) There is no discussion of the computational cost or latency involved in the execution-based evaluation, which is a practical concern for real-world deployment and future research using this benchmark."}, "questions": {"value": "1. will the execution of jql spend a lot of time?\n2. in text-2-sql, there are many stages, for example, schema linking, could your benchmark provide more studies on this like previous text2sql methods? e.g.[1,2]\n\n[1] Benchmarking the text-to-sql capability of large language models: A comprehensive evaluation.  \n[2] Pet-sql: A prompt-enhanced two-round refinement of text-to-sql with cross-consistency"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d6noZh5pjD", "forum": "KMiFMVMECd", "replyto": "KMiFMVMECd", "signatures": ["ICLR.cc/2026/Conference/Submission21994/Reviewer_rCnq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21994/Reviewer_rCnq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078882430, "cdate": 1762078882430, "tmdate": 1762942010427, "mdate": 1762942010427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}