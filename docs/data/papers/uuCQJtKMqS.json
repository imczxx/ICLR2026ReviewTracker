{"id": "uuCQJtKMqS", "number": 25556, "cdate": 1758369083307, "mdate": 1759896715813, "content": {"title": "AlienLM: Alienization of Language for Privacy-Preserving API Interaction with LLMs", "abstract": "We introduce $\\textbf{\\textit{AlienLM}}$, a framework that reinterprets encryption as language translation for large language models accessed exclusively through black-box APIs. Existing approaches based on secure inference or differential privacy and federated learning offer limited protection in API-only scenarios. $\\textbf{\\textit{AlienLM}}$constructs an Alien Language through a vocabulary-level bijection and employs API-only fine-tuning, thereby ensuring compatibility with commercial black-box services while requiring no access to model internals. Across four LLMs and seven benchmarks, $\\textbf{\\textit{AlienLM}}$ preserves more than 81\\% of the original performance, substantially surpasses substitution- and obfuscation-based baselines, and exhibits strong robustness against token-mapping and frequency-analysis attacks. $\\textbf{\\textit{AlienLM}}$ provides a deployable, low-overhead mechanism for safeguarding sensitive data in API-mediated applications such as healthcare, finance, and education. More broadly, our findings reveal a practical separation between linguistic representation and task competence, thereby motivating future work on composable privacy-preserving layers and formal characterizations of the learnability–opacity trade-off.", "tldr": "", "keywords": ["Encryption", "Obsfucation", "LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ba8aa4c34e3c77d90efc9a0bcf572ddb3a8dfec.pdf", "supplementary_material": "/attachment/1cbd73a2f5c259b75a14027365f72be5c57aa2e6.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies privacy protection during black-box API interactions with LLMs and proposes the AlienLM framework.AlienLM differs from traditional, simple encryption methods by reframing encryption as a language translation task for Large Language Models.\nAlienLM introduces a method based on vocabulary-level bijection to construct a secret language that balances confidentiality and LLM learnability. This approach provides a new perspective for integrating sensitive data with API-only fine-tuning scenarios. The method described in the paper achieves over 80% usability while maintaining confidentiality. Their experiments also reveal the trade-off between learnability and opacity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe idea of modeling encryption as language translation (encryption-as-language translation) is interesting, offering a new direction for protecting prompts and outputs in black-box API scenarios.\n\n2.\tThe paper presents a well-structured and logically consistent methodology. Each component of the proposed framework is motivated by sufficient justifications and accompanied by clear definitions.\n\n3.\tThe effectiveness of AlienLM is thoroughly discussed. The paper provides an extensive experimental analysis covering multiple benchmarks, various encryption ratios, and domain-specific EAT settings, offering a comprehensive understanding of the model’s behavior under different conditions.\n\n4.\tBy examining the trade-off between normalized edit distance and similarity-based distance, the paper proposes a novel approach to constructing an aligned language, which is both insightful and valuable as a reference."}, "weaknesses": {"value": "1. My major concern is about the practical usability. As the paper mentioned, AlienLM is proposed for sensitive domains such as healthcare, finance, and education. However, for the healthcare and financial domains, data should be strictly protected and uploading data to a remote LLM is prohibited. How would the author justify such cases?\n\n2.\tSome Typos. Typo in line 021, there should be a space between “AlienLM” and “provides”.\nTypo in line 239, there should be a space between “target model” and “M_{target}”.\nTypo in line 316, there should be a comma between “Table 1” and “Across”.\n\n3.\tMore discussion on real-world usage could be provided, for example, whether encryption would incur higher inference costs or affect reasoning speed.\n\n4.\tThe encryption method proposed in the paper still relies on a simple vocabulary mapping. My main concern is whether this approach is easily breakable. For instance, in API-based fine-tuning scenarios, the data provided to the model provider might be sufficient for them to reverse-engineer the encryption."}, "questions": {"value": "1. For the healthcare and financial domains, data should be strictly protected, and uploading data to a remote LLM is prohibited. How would the author justify such cases to use AlienLM?\n\n2. How does the AlienLM differ from previous encryption methods, such as Caesar ciphers?\n\n3. Would the Alien language still be inverted by the trained LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HDk6jQtDyl", "forum": "uuCQJtKMqS", "replyto": "uuCQJtKMqS", "signatures": ["ICLR.cc/2026/Conference/Submission25556/Reviewer_RnNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25556/Reviewer_RnNX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403030721, "cdate": 1761403030721, "tmdate": 1762943473214, "mdate": 1762943473214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies LLM text obfuscation inference (which is referred to as encryption/decryption in the submission, though its guarantees are far weaker than cryptographic security, making “obfuscation” a more accurate term here). \n\nIn particular, the submission proposes to (1) introduce additional \"encryption\" (encoder) and \"decryption'' (decoder) modules on top of the standard LLM API inference protocol, where the \"encryption\" module transforms plain text into human-unreadable representations (i.e.,  the “Alien Language”) that remain similar to the original plain text in the embedding space, and the \"decryption\" module converts the inference results back into readable text; and (2) to make the inference process feasible and meaningful, the LLM itself operates on the \"Alien Language\" and is fine-tuned (via API adaptation) on paired text data in \"Alien Language\".\n\nExperiments show that the proposed approach preserves much of the model’s utility while resisting simple recovery attempts. In particular, attacks based on nearest-neighbor alignment between \"Alien\" and target tokens fail to accurately reconstruct the original text."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is overall well structured, and the results are clearly presented.\n\n- It presents an practically relevant topic and interesting idea, and the approach is reasonable and easy to follow.\n\n- The experiments demonstrate some intriguing results."}, "weaknesses": {"value": "- The threat model is not explicitly defined. While it can be roughly inferred from the context, it remains unclear which specific adversarial settings the proposed method can resist. The current evaluation only considers relatively simple token-level mapping attacks (based on nearest-neighbor search in embedding or representation spaces).\nIn contrast to true cryptographic or secure multi-party computation schemes, where privacy is maintained even when the encryption and decryption algorithms are publicly known (following Kerckhoffs’ principle), this learning-based obfuscation approach relies heavily on the secrecy of the learned mapping. Thus, it does not offer formal or provable privacy guarantees. I would therefore recommend being more cautious and conservative in framing the method as “privacy-preserving”. The authors should also position it within the broader landscape of existing privacy-preserving inference methods, including cryptographic computation, differential privacy, and syntactic or semantic obfuscation approaches. Given the known fragility of heuristic, non-theoretically grounded obfuscation methods [1], the privacy level here appears closer to PII-scrubbing or surface-level anonymization, which already achieve \"near-perfect\" results in many cases. Overall, this may limit the method’s real-world value.\n\n[1] Carlini, Nicholas, et al. \"Is Private Learning Possible with Instance Encoding?\" 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021.\n\n- The idea of obfuscation used in this submission, i.e., making representations human-uninterpretable (by maximizing some kinds of distance in data space) yet machine-learnable (by minimizing some kinds of distance in embedding space), has been well established and is not particularly novel in the literature. This somewhat diminishes the method’s technical contribution."}, "questions": {"value": "- The paper seems to confuse the token set $S$ and the index set $I$. For example, in lines 198–199, the operation  $S\\cup (I \\backslash I_\\rho)$ is confusing.\n\n- The subscript notation for the target model is sometimes written as \"target\" and other times as \"tgt.\" It would be better to make this consistent throughout.\n\n- It would help to elaborate on how the proxy mapping $S(v) = τ_{proxy}(v, V)$ is constructed\n\n- The method section could benefit from a clearer and more detailed explanation of the core design ideas. It would improve clarity to explicitly state the design intent at the beginning of each subsection. For example, in Section 3.3:\nIt could be made clearer by linking the qualitative goals to quantitative objectives: “unreadable to humans” corresponds to maximizing edit distance, while “learnable by the model” corresponds to constraining embedding similarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qa5AgrXNG3", "forum": "uuCQJtKMqS", "replyto": "uuCQJtKMqS", "signatures": ["ICLR.cc/2026/Conference/Submission25556/Reviewer_uqLQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25556/Reviewer_uqLQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001063894, "cdate": 1762001063894, "tmdate": 1762943472847, "mdate": 1762943472847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AlienLM is a framework that conceptualizes encryption as language translation for large language models (LLMs) accessed through black-box APIs. The approach builds an “Alien Language” via a vocabulary-level bijection and employs API-only fine-tuning, enabling encrypted communication with commercial LLMs without access to model internals. Experiments across multiple models and benchmarks demonstrate that AlienLM retains over 81% of the original model’s performance while offering strong resistance to token-mapping and frequency-analysis attacks. The framework provides a deployable, low-overhead method for privacy-preserving applications and highlights the separation between linguistic representation and task competence, paving the way for future research on composable privacy-preserving mechanisms and the learnability–opacity trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Bijection-based Encrypted Language Layer:\nThe authors propose constructing an Alien Language and its translator through a vocabulary-level bijection, enabling bidirectional and lossless conversion between plaintext and encrypted alien text. This mechanism conceals model internals while providing a deployable encryption layer fully compatible with black-box LLM APIs.\n\n2. API-only Adaptation (EAT):\nThe authors introduce an API-only fine-tuning process, termed Encryption Adaptation Training (EAT), which allows the model to adapt to the Alien Language without accessing internal parameters. This adaptation consistently preserves over 80% of the model’s original performance, resulting in the Alien Language–adapted model, Malien.\n\n3. Domain-specific Adaptation:\nThe authors extend EAT to domain-focused scenarios, demonstrating that targeted fine-tuning improves performance in specialized areas such as coding and mathematical reasoning, while maintaining strong general-purpose capabilities across diverse benchmarks."}, "weaknesses": {"value": "1. Poor presentation and readability: The overall presentation is difficult to follow. In particular, Section 3 (Method) introduces a large number of mathematical formulations without sufficient explanation or intuitive interpretation, making it challenging for readers to understand the core ideas and methodological flow.\n\n2. Insufficient baselines for comparison: The evaluation includes only one baseline, SentinelLM (Mishra et al., 2024b), which is inadequate for demonstrating the effectiveness of the proposed privacy-preserving mechanism. Additional privacy-related baselines, such as approaches based on secure inference, differential privacy, should be included for a more convincing empirical validation.\n\n3. Lack of ablation study: The paper does not provide an ablation analysis to isolate the contributions of key components (e.g., the bijection design, the encryption ratio ρ, or the API-only fine-tuning step). Such experiments are necessary to clarify which elements of the framework drive the observed performance improvements."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hZAP8ZrcWo", "forum": "uuCQJtKMqS", "replyto": "uuCQJtKMqS", "signatures": ["ICLR.cc/2026/Conference/Submission25556/Reviewer_EeK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25556/Reviewer_EeK6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117163210, "cdate": 1762117163210, "tmdate": 1762943472668, "mdate": 1762943472668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AlienLM, a framework for privacy-preserving interaction with black-box LLM APIs by reinterpreting encryption as language translation. It constructs an “Alien Language” via a vocabulary-level bijection and adapts models through API-only fine-tuning (Encryption Adaptation Training, EAT). Experiments show AlienLM retains ~81% of original performance across seven benchmarks and four LLM backbones, outperforming naïve substitution and a simplified SentinelLM baseline. The authors claim robustness against token-mapping and frequency-analysis attacks and highlight tunable privacy–utility trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper works without white-box access, aligning with real-world constraints for commercial LLM APIs.\n- Evaluates across multiple models, benchmarks, and includes ablations on encryption ratio and domain-specific fine-tuning.\n- Greedy k-NN heuristic scales to large vocabularies and is computationally tractable.\n- Clear articulation of privacy–utility trade-off."}, "weaknesses": {"value": "- The “encryption” claim is misleading. The method is a deterministic substitution cipher with no formal cryptographic guarantees (e.g., IND-CPA), conflicting with NIST standards.\n\n- Threat model and security evaluation are incomplete. The paper ignores known/chosen-plaintext attacks and parallel-corpus alignment, which break substitution systems.\n\n- Strong alternatives like tokenizer-free models (CANINE[2], ByT5[3], Charformer[4]) are missing; SentinelLM [1] comparison is weakened."}, "questions": {"value": "Given the strengths, I have the following concerns:\n\n- While the method could be better framed as a prompt obfuscator for cloud LLMs, the claimed “encryption” provides no modern cryptographic security guarantees (e.g., IND‑CPA), which limits impact for actual compliance or regulated use-cases. Importantly, the method is a deterministic monoalphabetic substitution at the subword level. Classic results show such ciphers are inherently vulnerable to known‑plaintext and chosen‑plaintext/ciphertext attacks and provide no semantic security; the paper offers no formal analysis under standard definitions or an explicit adversary model. This significantly questions the \"encryption\" claim of the paper. How do you justify calling this “encryption” when it lacks formal properties like semantic security? Could you provide proofs?\n\n- The evaluations omit realistic known‑plaintext scenarios (e.g., leakage of any plaintext–alientext pairs through logs, user reports, or outputs), which would allow rapid mapping recovery. Reported “robustness” focuses on nearest‑neighbor alignment in embedding/LM‑head space or corpus‑level frequency matching on mismatched proxies, not on parallel‑text alignment, n‑gram/LM‑based decipherment, or oracle‑based adaptive attacks. Can the authors include these experiments to test the \"robustness\" of the proposed method?\n\n-  In its current form, there is no latency/throughput/serving cost analysis (client translation cost, server decoding/retokenization behavior, API tokenization pitfalls), no key‑rotation experiments, and no safety/alignment regressions assessment.\n\n- If the goal is privacy via obfuscation under black‑box APIs, a natural baseline is to compare against tokenizer‑free/byte‑level LMs (e.g., CANINE, ByT5, Charformer) that are robust to noise, spelling changes, and non‑standard scripts—often with no fine‑tuning or with lightweight adaptation. These are omitted. Can the authors compare stronger baselines: (i) SentinelLM in a white‑box setting (clearly acknowledging it is out‑of‑scope for API‑only, but useful for calibrating the upper bound); (ii) CANINE/ByT5/Charformer as target models; (iii) alternative span‑level reversible transformations.\n\n\n\n[1] Mishra et al. SentinelLMs: Encrypted Input Adaptation and Fine-Tuning of Language Models for Private and Secure Inference\n\n[2] Clark et al. Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation\n\n[3] Xue et al. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models\n\n[4] Tay et l. Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qUwHp47N8t", "forum": "uuCQJtKMqS", "replyto": "uuCQJtKMqS", "signatures": ["ICLR.cc/2026/Conference/Submission25556/Reviewer_PNXp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25556/Reviewer_PNXp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762417784487, "cdate": 1762417784487, "tmdate": 1762943471993, "mdate": 1762943471993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}