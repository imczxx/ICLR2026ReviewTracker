{"id": "0yLdDZMutq", "number": 225, "cdate": 1756731766775, "mdate": 1759898271033, "content": {"title": "GVote: Adaptive Per-request KV-Cache Compression without Manually Setting Budget", "abstract": "Large language models (LLMs) inference relies heavily on KV-caches to accelerate autoregressive decoding, but the resulting memory footprint grows rapidly with sequence length, posing significant efficiency challenges.\nCurrent KV-cache compression methods suffer from a Procrustes' bed problem: they force diverse workloads into fixed compression ratios, leading to suboptimal resource allocation and inference performance. \nTo this end, we present GVote, an adaptive per-request KV-cache compression scheme that eliminates manual budget specification while achieving superior accuracy-efficiency trade-offs. \nGVote operates on the principle that the important keys are the aggregation of keys required by future queries. \nGvote predicts future query attention demands by Monte-Carlo style sampling potential queries and aggregating selected keys to determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple benchmarks, including GSM8K, RULER and Longbench. \nCompared to baselines, GVote exhibits 2$\\times$ memory reduction while the accuracy maintains higher or comparable.", "tldr": "", "keywords": ["Large language models", "KV-Cache compression", "Budget"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/533fb78ad6425112140f9199de70e69be5be19c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GVote, a sampling-based method to determine the KV cache budget and the KV pairs to keep for efficient inference. With a couple minor hyperparameters, the authors show that GVote can automatically reduce memory and speed up inference on several tasks. While the algorithm seems sound, I mainly have some issues with the clarity and motivation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) Automatically determining the KV budget is an important problem, and this paper makes progress towards it.\n2) Sizable memory reduction and better prefill latency scaling."}, "weaknesses": {"value": "1) I think there is a lack of background/related work in the paper that makes the field seem too narrow. Given that the field of KV cache compression has matured over the past 2 years, there have been a lot more work than what the authors have included. For instance, many methods that use low-rank compression, sparse+low rank compression, or offloading KV caches to CPU are not touched upon, even though they all fall under the realm of KV caching algorithms. While the authors do not need to explicitly compare against these methods, I suggest a broader coverage of the literature.\n\n2) The claim in Figure 2 that the curves look Gaussian is not convincing visually, since they mostly seem like simple unimodal distributions. A quantitative metric would make this claim seem less suspect.\n\n3) Many (if not all) of the tasks are short generation tasks. To demonstrate the predictive power of GVote, it would better to show results on long generation tasks.\n\n4) Various typos:\n    - Line 129: backwards quotations\n    - Line 351: KVCache should be two words\n    - Line 365: We extensively tests -> We extensively test\n    - Equation 1: Should the index for the keys and values go from 1 to t? KV pairs from current and previous tokens are used, not just previous tokens."}, "questions": {"value": "1) Are models using chain of thought for GSM8K?\n\n2) Is there a way to tune the dial to prioritize accuracy vs. efficiency?\n\n3) Is there a way to constrain the efficiency? For example, if I want at least a 80% reduction in memory, is there a way to enforce this?\n\n4) Do you have throughput metrics? Or does this optimize only for latency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TpL0Q0iqsW", "forum": "0yLdDZMutq", "replyto": "0yLdDZMutq", "signatures": ["ICLR.cc/2026/Conference/Submission225/Reviewer_zbEY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission225/Reviewer_zbEY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760649529782, "cdate": 1760649529782, "tmdate": 1762915474722, "mdate": 1762915474722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an adaptive KV-cache compression scheme that operates per request. By moving beyond fixed-budget compression and choosing the compression ratio dynamically for each input, the approach targets lower memory and more efficient deployment without sacrificing accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The motivation is clear and well argued, and the experiments are thorough and well documented.\n* The distribution-aware synthetic-query mechanism is sound and convincing."}, "weaknesses": {"value": "* The reliance on K from the last ground-truth query to stabilize selection is intuitive but under-justified. Please add theory/ablations showing when this helps vs. hurts.\n* Figure 6 needs methodological clarity. If results are matched by accuracy, each method’s retained KV size / compression ratio and the corresponding latency/memory should be reported to ensure fair comparability.\n* Minor: duplicated sentence at L256–L257."}, "questions": {"value": "* Do you have empirical or theoretical evidence to justify using the K from the last ground-truth query?\n* In Fig. 6, at the matched-accuracy operating point, what are each method’s KV-cache size (or compression ratio) and latency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NNNl8NvDOY", "forum": "0yLdDZMutq", "replyto": "0yLdDZMutq", "signatures": ["ICLR.cc/2026/Conference/Submission225/Reviewer_A2eh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission225/Reviewer_A2eh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454178708, "cdate": 1761454178708, "tmdate": 1762915474586, "mdate": 1762915474586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of fixed-budget KV-cache compression, where a single preset budget is applied to all requests regardless of their contextual complexity. This uniform setting can either hurt accuracy on difficult reasoning tasks or waste memory on simpler ones.\n\nTo resolve this, the authors propose GVote, an adaptive per-request KV-cache compression scheme that automatically determines the necessary cache budget with low overhead.\nGVote is built on the observation that hidden states across the sequence approximately follow a Gaussian distribution. Leveraging this, the method samples synthetic future queries via Gaussian perturbation, aggregates their attention over existing keys through a Monte-Carlo voting procedure, and retains the union of key positions deemed important.\n\nAs a result, complex workloads (e.g., mathematical reasoning) naturally receive larger cache budgets, whereas simpler tasks (e.g., QA) are assigned smaller ones, enabling better accuracy–efficiency trade-offs. Experiments on Qwen2.5-7B-Instruct as well as Llama-based models across GSM8K, RULER, and LongBench demonstrate that GVote achieves higher or comparable accuracy at similar or lower memory usage compared to static-budget baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper clearly identifies the inefficiency of fixed-budget KV-cache compression under heterogeneous workloads and proposes a practical per-request budgeting mechanism.\n* By combining Gaussian-based synthetic query generation with Monte Carlo voting to approximate future queries, the method offers a technically interesting and novel bottom-up perspective.\n* The method demonstrates consistent accuracy–memory improvements across diverse benchmarks (e.g., GSM8K, RULER, LongBench) and model families (e.g., Qwen, Llama), supporting its generality.\n* By dynamically estimating per-request budgets, the approach achieves higher accuracy at comparable memory usage, or similar accuracy with lower memory, compared to fixed-budget baselines.\n* Sensitivity analyses on hyperparameters (e.g., p_nuc , S) provide useful guidance for applying the method in practice."}, "weaknesses": {"value": "* The assumption that hidden states follow a Gaussian distribution is only weakly supported; stronger theoretical justification or broader empirical evidence is needed to establish generality.\n* The paper provides limited analysis of failure cases in which synthetic queries fail to approximate future queries, leaving the robustness of the proxy insufficiently explored.\n* Although the method claims to eliminate manual budget specification, several key hyperparameters still require tuning.\n* Comparisons against recent adaptive KV-cache compression approaches are insufficient, making it difficult to assess the precise novelty of the contribution.\n* The paper does not adequately discuss how the proposed non-uniform KV layout would integrate with real-world inference systems (e.g., paged attention in vLLM), where memory layout constraints and paging mechanisms may complicate deployment.\n* The method focuses on single-request contexts and does not address how KV-cache should be maintained, shared, or incrementally updated across multi-turn dialogue scenarios, which are central in practical LLM applications."}, "questions": {"value": "* Could you provide broader empirical evidence showing that the Gaussian assumption for hidden states holds consistently across different models, layers, and task types?\n* In cases where synthetic queries fail to approximate future attention well, what conditions cause such failures, and how do they impact model performance?\n* Could you provide a quantitative evaluation of run-to-run variance to assess the stability of the Monte-Carlo sampling process?\n* How would the proposed non-uniform KV layout interact with paged-attention–based systems such as vLLM?\n* How should the method maintain and update KV-cache across multi-turn dialogue scenarios, which are common in real LLM applications?\n* Since the proposed method leverages hidden-state statistics rather than actual future tokens, would it be feasible to pre-compute a reusable pool of synthetic queries instead of re-sampling for every request?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NwGoDzQtl4", "forum": "0yLdDZMutq", "replyto": "0yLdDZMutq", "signatures": ["ICLR.cc/2026/Conference/Submission225/Reviewer_jWPe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission225/Reviewer_jWPe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932210995, "cdate": 1761932210995, "tmdate": 1762915474445, "mdate": 1762915474445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GVote, an adaptive KV Cache compression scheme that avoids manual budget setting.\nGVote relies on LLM hidden states’ Gaussian distribution: it uses Monte-Carlo sampling to generate synthetic future queries, aggregates their needed keys to dynamically determine optimal cache budgets, and forms a keep-set via voting.\nExperiments  show GVote reduces VRAM while keeping accuracy high or comparable to baselines (StreamLLM, SnapKV, AdaKV). It also has low generation latency and compatibility with modern kernels like FlashAttention."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ The proposed method maintains its advantage across different architectures (LLaMA, Qwen) and various model sizes (3B-14B), demonstrating that it is not dependent on a specific model structure and has a broad range of applicability."}, "weaknesses": {"value": "+ In the discussion of \"ARE SYNTHESIZED QUERIES GOOD PROXIES?\", there is no baseline for comparison. For instance, if tokens from the Prefix and Observation window, as used in SnapKV, are employed as proxies, what will the attention overlap be? This comparison is necessary to evaluate the effectiveness of synthesized queries as proxies.\n\n+ The experimental results in the paper are questionable. According to the reported results, StreamLLM's performance is only slightly worse than SnapKV and AdaKV. However, based on the results in the AdaKV paper, StreamLLM’s performance is significantly worse than the other two methods. This discrepancy is reasonable, as StreamLLM only retains attention sinks and short-term information, which likely limits its overall performance compared to the other approaches.\n\n\n+ The evaluation datasets used in the paper are not clearly specified. For example, the paper mentions evaluation on Longbench, but Longbench includes 21 different datasets, each with different metrics (such as Acc, F1, Rouge-L, Edit-Sim, etc.). The paper does not explain how the reported \"accuracy\" was derived.\n\n+ Adaptive adjustment of the budget is not necessarily an advantage, as it also means the budget becomes uncontrollable. In practical scenarios, this could lead to unexpected OOM issues.\n\n+ The paper has not been proofread, and there are numerous issues with the citation formatting."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1eSZdMSJ3W", "forum": "0yLdDZMutq", "replyto": "0yLdDZMutq", "signatures": ["ICLR.cc/2026/Conference/Submission225/Reviewer_RD3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission225/Reviewer_RD3X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996724565, "cdate": 1761996724565, "tmdate": 1762915474317, "mdate": 1762915474317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}