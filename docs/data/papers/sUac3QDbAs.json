{"id": "sUac3QDbAs", "number": 21384, "cdate": 1758316986160, "mdate": 1763726130038, "content": {"title": "Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment", "abstract": "Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment.", "tldr": "We propose a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space.", "keywords": ["Large language Model", "Alignment", "Reinforcement learning with human preference", "Wasserstein distance", "Sinkhorn distance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/02164bb72b1890eefaed092212ddf0e158e2cee5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Wasserstein Policy Regularization (WPR), a novel semantic-aware regularization framework for RLHF. Instead of using classical KL or f-divergence, the authors propose using an entropy-regularized Wasserstein distance that captures token-level semantic similarity in the embedding space. Theoretical derivations (dual formulation and tractable penalty) are rigorous, and experiments on Gemma-2B across two tasks (TL;DR summarization and HH-RLHF dialogue) show consistent improvements over KL- and f-divergence-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is theoretically rigorous and clearly presents the primal–dual equivalence and the functional role of the dual potential in policy regularization.\n2. The experimental design is clear and sound, with proper variable control and consistent setups across baselines. WPR demonstrates robust and superior performance across multiple tasks and hyperparameter settings.\n3. The idea is conceptually novel and insightful, providing a solution to alleviate the limitation of the traditional KL divergence in measuring semantically similar but support-disjoint samples."}, "weaknesses": {"value": "Major:\n1. The paper’s core claim is that the Wasserstein distance captures semantic relationships between tokens, but this claim is not empirically validated beyond a toy example. It would substantially strengthen the work to include quantitative metrics.\n2. The experiments are mainly conducted on a single base model (Gemma-2B) and two tasks. To support claims of generality and robustness, the study should include results from larger or alternative model architectures.\n3. While the framework is conceptually novel, the current experiments mainly demonstrate performance gains, not semantic effects.  \nQuantitative analysis showing **how semantic structures influence model behavior** would significantly raise the paper’s impact.\n\n\nMinor:\n1. The implementation of the Sinkhorn algorithm should be described in greater detail, for example\n- Stopping criterion and convergence conditions  \n- Truncation scheme (top-*k₁*, *k₂*) and numerical stability.\n2. The evaluation heavily depends on GPT-4 win rates, a widely used but subjective metric that offers limited robustness. From my own experience, the outcome of such evaluations can vary depending on how the GPT judging prompt is designed. \nIf feasible, incorporating **small-scale double-blind human evaluations** could further enhance credibility. (This is only a suggestion.)"}, "questions": {"value": "Overall, the paper is well-written. However, the central concept of “semantic awareness”—which distinguishes this work from prior RLHF regularization—remains vague and unclear to me. Specifically,\n\n1. Could you clarify the mechanism by which semantic awareness translates into improved alignment outcomes?  In tasks where the target distribution may diverge semantically from the base model.(e.g., filtering harmful or biased responses), does Wasserstein regularization still provide benefits, or could it hinder optimization?  \n\n2. Could you provide quantitative measures to illustrate the model’s semantic sensitivity?  (e.g. Is \"semantically close\" means high embedding correlation or something else?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CkjKoP0yQt", "forum": "sUac3QDbAs", "replyto": "sUac3QDbAs", "signatures": ["ICLR.cc/2026/Conference/Submission21384/Reviewer_AZq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21384/Reviewer_AZq3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761310797510, "cdate": 1761310797510, "tmdate": 1762941736482, "mdate": 1762941736482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Wasserstein Policy Regularization (WPR), a new semantic-aware regularization for RLHF framework based on the entropy-regularized Wasserstein distance. The experiments show WPR's outperformance to other baselines such as KL-divergence and f-divergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to read.\n2. For the regularization, both theoretical formulation and practical implementation are introduced in detail and analyzed via complexity perspective.\n3. The comprehensive experiments show the outperformance of new regularization and the effect of each component in the framework."}, "weaknesses": {"value": "1. The only base model in the experiments is the pre-trained Gemma-2B. The results of models from other LLM families will further validate the effectiveness of the regularization.\n2. The authors introduce the ignorance of semantic similarity in the previous regularization as the main motivation, and highlight that proposed WPR is a semantic-aware regularization. However, the analysis about semantic awareness is missed in the description of the regularization method. In the experiments, this is no corresponding results to prove that WPR captures semantic relationships (like the motivating example in Figure 1). I think the further discussion focusing on semantic awareness is needed if it is the key for outperformance.\n\nI would like to adjust my score if these concerns are addressed."}, "questions": {"value": "1. For the cost matrix $\\mathbf{C}$, is it constant? If it is constant, could it be possible to use some matrix decomposition such as SVD to speed up the computation of $\\exp(-\\lambda\\mathbf{C})$. If it is not, how is it initialized and updated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m4tydswrXK", "forum": "sUac3QDbAs", "replyto": "sUac3QDbAs", "signatures": ["ICLR.cc/2026/Conference/Submission21384/Reviewer_Ndnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21384/Reviewer_Ndnt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931110678, "cdate": 1761931110678, "tmdate": 1762941736220, "mdate": 1762941736220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Wasserstein policy regularization (WPR) for RLHF. the authors discuss that standard KL-based regularization only compares token probabilities at identical indices, ignoring semantic similarity across tokens, which may penalize reasonable semantic deviations. The authors propose using the entropy-regularized Wasserstein (Sinkhorn) distance between next-token distributions, leveraging token embedding geometry to induce semantic-aware regularization. They derive a dual-form penalty that integrates seamlessly into PPO, maintaining tractability via Sinkhorn scaling with sparsity tricks. Their experiments on Gemma-2B across TL;DR and HH-RLHF datasets show consistent improvements over KL and other f-divergences in GPT-4 win rate and MT-Bench scores."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- By replacing KL with a Wasserstein-based penalty that reflects token embedding geometry, the method tries to captures semantic similarity (e.g., “cat” vs. “kitten”), which could lead to more natural, aligned outputs rather than over-penalizing harmless semantic variations. But there are some concerns as detailed in weaknesses. \n\n- The paper derives a dual formulation enabling efficient Sinkhorn-based computation, making the method compatible with standard RLHF pipelines (e.g., PPO) and achieving empirical gains in practice."}, "weaknesses": {"value": "- Why does token-space geometry matter in practice? What is lost with KL, this rationale should be strengthened. The authors show win-rates but don’t link it explicitly to semantic alignment behaviour.\n\n- “Incorporates the geometry of the token space” this is unclear without explanation which deserves clarity. They rely on embedding-space distances, not true semantic grounding. This raises questions: Which embedding space? Frozen SFT model embeddings? Reference model embeddings? Is the embedding space stable or changing during fine-tuning?\n\n-  Is the overhead of proposed method is scalable to larger models ? What are the memory impacts during training? \n\n- Is it possible to get a DPO version of the proposed methods? \n\n- How hard is it to lean phi star in 15? \n\n- In (9), the definition of divergence depends on m, which eventually results in  an optimization variable for each n, and now the value of N changes from sample to sample, then how to utilize the same P* or any thoughts about it? \n\n- Except additional training overhead, is there any other price to pay for the proposed technique?"}, "questions": {"value": "Please refer to weaknesses,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WITLwpCBfX", "forum": "sUac3QDbAs", "replyto": "sUac3QDbAs", "signatures": ["ICLR.cc/2026/Conference/Submission21384/Reviewer_rU8j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21384/Reviewer_rU8j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055594506, "cdate": 1762055594506, "tmdate": 1762941735948, "mdate": 1762941735948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key limitation in standard RLHF: the use of Kullback-Leibler (KL) divergence and other f-divergences for policy regularization is \"semantically-blind,\" as it only compares token probabilities at identical indices . This means a policy that assigns high probability to \"kitten\" is penalized just as much as one that assigns high probability to \"table,\" when the reference token is \"cat\" .\n\nTo solve this, the authors propose Wasserstein Policy Regularization (WPR), a novel, semantic-aware regularizer for RLHF based on the entropy-regularized Wasserstein (Sinkhorn) distance . The core technical contribution is an elegant and tractable formulation: by leveraging the dual of the optimal transport problem, the Wasserstein regularization term becomes a simple, token-wise penalty applied to the reward. This penalty is derived from the optimal dual variables (ϕ*), which are computed efficiently using the standard Sinkhorn-Knopp algorithm.\n\nThis approach makes the Wasserstein-based penalty directly compatible with standard RL algorithms like PPO. The authors demonstrate empirically on summarization (TL;DR) and dialogue (HH-RLHF) tasks that WPR significantly outperforms a wide range of f-divergence baselines (including RKL, FKL, JS, and χ²) in terms of GPT-4 win rate and MT-Bench scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear, Compelling Motivation: The paper's greatest strength is its crystal-clear motivation. The \"cat/kitten/table\" example in Figure 1 immediately and intuitively communicates the flaw in existing methods and the rationale for the new one .\n\nElegant and Tractable Formulation: The core technical contribution is the derivation of a tractable algorithm (WPR) from a complex theoretical concept (Wasserstein distance). The insight to use the dual optimal variables (ϕ*) as a direct reward penalty (Theorem 2) is the key that makes this practical and compatible with existing PPO-based RLHF pipelines.\n\nStrong, Consistent Empirical Results: The proposed method (WPR) convincingly outperforms a comprehensive suite of f-divergence baselines on two standard alignment tasks (summarization and dialogue). The improvements are consistent across both GPT-4 win rates (Table 1) and the MT-Bench benchmark (Table 2).\n\nThorough Analysis and Ablation: The authors provide a strong set of analyses. The ablation study (Table 3) confirms the importance of the method's components (e.g., number of Sinkhorn iterations, λ). The sensitivity analysis (Figure 4) shows WPR is more stable across a wider range of the β hyperparameter than other divergences. The penalty analysis (Figure 5) provides insight, showing WPR is correlated with KL but \"more lenient\"."}, "weaknesses": {"value": "Cost Matrix C as a \"Black Box\": The entire semantic-awareness of the method hinges on the cost matrix C, which is built from the token embeddings of the reference SFT model. This is a reasonable choice, but its impact is not deeply explored. The ablation study only compares L2 vs. Cosine distance, but not the source or quality of the embeddings. The paper doesn't answer: what if the SFT model's embeddings are of poor quality? Would using embeddings from a more powerful, external model improve results?\n\nComputational Overhead: The paper claims the overhead is low (\"only 2.5%\" increase in training time). However, this is after applying two aggressive truncations: k1=512 for the cost matrix and k2=128 for the policy distributions. This O(k_2^2) complexity is still per-token, per-step, compared to the O(1) cost of the KL penalty. A more detailed wall-clock time comparison and an analysis of the performance/compute trade-off (e.g., how much better does it get if k2=256?) would be beneficial.\n\nNew Hyperparameter λ: The method introduces a new, important hyperparameter λ, the entropy regularization strength, which is set to 100. The ablation study shows a performance drop when decreasing it to 10. This parameter's tuning and sensitivity are critical to the method's success, and a more in-depth discussion of its impact and how to set it would improve the paper's practical utility."}, "questions": {"value": "On the Cost Matrix C: The semantic cost matrix C is fundamental to the method's success and is derived from the SFT model's embeddings. Have you experimented with using a different source for these embeddings, such as from a larger, more capable, and static model (e.g., GPT-4 embeddings)? Is it possible that a \"better\" or more semantically-grounded cost matrix C would lead to even larger gains?\n\nOn Computational Overhead: Could you please provide a more detailed breakdown of the computational overhead? The \"2.5%\" figure is encouraging, but a wall-clock time comparison per 1000 training steps (or per-epoch) against the RKL baseline would be very informative. How much does this overhead scale as the truncation parameter k2 is increased?\n\nOn the λ Hyperparameter: The entropy regularization parameter λ seems crucial. The ablation shows a notable drop when λ is set to 10. How was the value of λ=100 chosen? Was this tuned on a validation set, and how sensitive is the model's performance to this value (e.g., for λ=50, λ=200)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9SMQNLrUYT", "forum": "sUac3QDbAs", "replyto": "sUac3QDbAs", "signatures": ["ICLR.cc/2026/Conference/Submission21384/Reviewer_wx4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21384/Reviewer_wx4k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111055079, "cdate": 1762111055079, "tmdate": 1762941735574, "mdate": 1762941735574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}