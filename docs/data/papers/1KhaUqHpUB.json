{"id": "1KhaUqHpUB", "number": 22933, "cdate": 1758337249946, "mdate": 1759896839600, "content": {"title": "A Generative Approach to LLM Harmfulness Mitigation with Red Flag Tokens", "abstract": "Many safety post-training methods for large language models (LLMs) are designed to modify the model’s behaviour from producing unsafe answers to issuing refusals. \nHowever, such distribution shifts are often brittle and degrade performance on desirable tasks.\nTo address these pitfalls, we propose augmenting the model’s vocabulary with a special red flag token ($\\langle\\texttt{rf}\\rangle$) and training the model to insert this token whenever harmful content is generated or imminent. \nThis approach enables the model to explicitly learn the concept of harmfulness in its representations, with minimal impact on utility due to the marginal change in the generated distribution of natural language. \nMoreover, because the token is embedded in the model’s vocabulary, we can naturally leverage the LLMs' generalisation capabilities, such as in-context learning (ICL) and out-of-distribution generalisation to languages that are not formally supported (e.g., Japanese for Llama3). \nIn particular, we demonstrate that through ICL alone, the model can learn to initiate reflective reasoning upon generating $\\langle\\texttt{rf}\\rangle$ at inference, which steers the response away from harmful continuations or enables self-correction when the flag is raised falsely.\nThis approach is orthogonal and complementary to existing safety techniques—such as safety classifiers or standard safety training---and easier to evaluate in comparison to natural language refusals, as it does not require a human or automated judge to assess the harmlessness of the answers.", "tldr": "Teaching the model to generate a special red flag token at any point in the conversation when it turns harmful", "keywords": ["LLM Safety", "Adversarial Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/621a5eaad4f75d2111f3f4c76e0859e740e8bc8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a simple technique to enhance LLM safety through the introduction of a red flag token (⟨rf⟩) that the model emits when potentially generating harmful content. By training models to insert ⟨rf⟩ whenever harmful output is imminent, the approach enables intrinsic harmfulness detection without disrupting overall language generation quality. The method is validated across multiple open-source LLMs (e.g., LLaMA3.2, MistralV3, Phi-3.5), using standard utility and robustness benchmarks, as well as cross-lingual evaluations—including Japanese, a low-resource language."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed idea of incorporating a red flag token is new and technically simple. It represents a promising direction for improving LLM safety without sacrificing fluency or utility. Unlike traditional refusal-based or classifier-based systems, this generative mechanism directly embeds safety awareness within the model itself.\n\n1. The paper is well-written and well-organized. The structure flows logically from motivation to method, followed by rigorous evaluation. The figures (especially Figures 1 and 2) clearly illustrate both conceptual design and training dynamics.\n\n1. The authors provide a comprehensive overview of recent LLM safety research, which enables us to understand their contribution precisely. \n\n1. The method is technically sound and well-motivated. The loss formulation combines cross-entropy and KL-divergence terms to maintain distributional consistency, which demonstrates careful consideration of avoiding performance degradation while embedding the new safety signal.\n\n1. Experiments are conducted across multiple models and datasets. The results convincingly show improved defense success rates under both gray-box and white-box attacks, while maintaining nearly unchanged utility scores.\n\n1. Including results on Japanese (a low-resource, unsupported language for some models) adds considerable depth. This highlights the approach’s ability to generalize beyond English and provides valuable implications for multilingual safety research."}, "weaknesses": {"value": "1. I feel this paper lacks sufficient implementation details for reproducibility. The Appendix is rather brief and does not include enough low-level information.\n\n2. If I understand correctly, this paper does not explicitly discuss adversarial misuse or the potential for malicious users to manipulate or suppress the ⟨rf⟩ mechanism. Its threat model focuses on pre-filling and sampling attacks, but not on direct prompt-level manipulation of the red-flag behavior. Therefore, highlighting this as a limitation or open question in the revised manuscript would be valuable."}, "questions": {"value": "1. Could the ⟨rf⟩ token be extended or diversified (e.g., multiple “flag” tokens for different types of risk such as toxicity, bias, or misinformation)?\n\n1. Could you tell me the thoughts on Weakness 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q6OWxtUVYW", "forum": "1KhaUqHpUB", "replyto": "1KhaUqHpUB", "signatures": ["ICLR.cc/2026/Conference/Submission22933/Reviewer_PmRZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22933/Reviewer_PmRZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760943997937, "cdate": 1760943997937, "tmdate": 1762942444746, "mdate": 1762942444746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel approach to LLM harmfulness mitigation. Instead of performing a post-training (alignment) of LLMs for safety, the proposed approach trains LLMs to generate a red flag token <rf> during harmful generation. The idea comes from the authors’ claim that the post-training to prevent an LLM from producing harmful answers and force it to issue refusals causes a distribution shift, and it is a cause of the performance degradation. To prevent a significant distribution shift, the LLM is trained to produce <rf> when generating harmful content, while it is maintained not to go far from the original distribution without <rf>. The red flag token is then utilized at inference time to filter the response and replace it with a safe alternative, or trigger safety-oriented reflective reasoning. The experiments show competitive adversarial robustness to the existing adversarial training-based approach (CAT) and that the usefulness of the trained model is kept as high as the baseline model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A novel approach to detect harmful generations without causing a significant distribution shift. This idea itself may be used for different purposes."}, "weaknesses": {"value": "Preventing a distribution shift is a reasonable idea. However, for this purpose, one can simply use filtering approraches that pre-process inputs or post-process outputs. With a filtering approach, one does not need to post-train the LLM itself, leading to no change of distribution. The current defense mechanism is actually similar to output filtering techniques. A very naive baseline approach would train a classifier for harmfulness by using the dataset prepared for the proposed approach training and use the classifier to check whether the output is harmful or not. Compared to filter-based approaches, the advantage of this approach from the perspective of defense is questionable. An experimental comparison with filtering based approach is required. \n\nRobustness evaluation against automated jailbreaking attack such as GCG and PAIR is provided only for a small model (LLAMA3.2-3B-IT). Moreover, although I couldn’t find the statement, it seems that the result is a single training result, meaning that the result is not reliable. While the harmless performance is better than CAT, the robustness performance is worse (10% lower than CAT for PAIR)."}, "questions": {"value": "How to tune hyper-parameters, alphas, in (4)? Could you provide a sensitivity analysis?\n\nWhy does it make sense to solve (5)? Does the l2-neighborhood of adversarially generated prompts in the feature space contain adversarial prompts? Does prompts generated by GCG or PAIR stay close to their original prompts in the feature space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z5XXNGekhL", "forum": "1KhaUqHpUB", "replyto": "1KhaUqHpUB", "signatures": ["ICLR.cc/2026/Conference/Submission22933/Reviewer_qFhv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22933/Reviewer_qFhv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781227913, "cdate": 1761781227913, "tmdate": 1762942444261, "mdate": 1762942444261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They train the model to output the \\<rf\\> token when it generates something harmful. Unlike previous work, the model can output this token anywhere in the generation, not just in the beginning. Their loss encourages the model to give the same output as it would have without the \\<rf\\> token. They use few shot examples to show the model to use safety focused CoT reasoning after generating the \\<rf\\> token."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Their method is much more robust to prefill attacks than Fixed Pos RF, because that method can only output the rf token in the beginning.\n\nThey show their technique works much better than standard refusals in low resource languages, because a model trained in another language can generate the same \\<rf\\> token no matter the language.\n\nI like using the \\<rf\\> token to encourage the Safety CoT, since it’s novel as far as I know, and it allows the model to recover from a false positive \\<rf\\> token."}, "weaknesses": {"value": "The novelty of their core technique is relatively low, since the method is the same as previous work, but the model is able to output the \\<rf\\> token anywhere instead of only at the beginning.\n\nIt’s hard to tell if the technique has any benefit over CAT because their results are only a single run without error bars, and their results are similar to CAT.\n\nClarity\nI find the loss in equation 1 confusing. It looks like it choses a random i, and then the loss encourages the model to output \\<rf\\> for every token between k and i. Is this correct? If so, what’s the purpose of only doing this between k and i? Why not encourage the model to output \\<rf\\> for every token between k and the last token?\n\nFigure 4 should have whether the metric is DSR or utility in the figure itself, instead of just the caption (as figure 3 does)."}, "questions": {"value": "The paper says “To maintain model performance and reduce distribution shift as much as possible without increasing the likelihood of a harmful answer, we use a KL divergence on the tokens after the ⟨rf⟩:” If the goal is to have the model generate the same thing as it would have without the \\<rf\\> token, why not just avoid passing the generated \\<rf\\> token back to the model at all during decoding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gggwylG7sD", "forum": "1KhaUqHpUB", "replyto": "1KhaUqHpUB", "signatures": ["ICLR.cc/2026/Conference/Submission22933/Reviewer_kxPD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22933/Reviewer_kxPD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805527636, "cdate": 1761805527636, "tmdate": 1762942443850, "mdate": 1762942443850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a generative approach to mitigating the harmfulness of LLMs by introducing a special red flag token into the model’s vocabulary. This token is trained to be generated when the LLM generates or is about to generate harmful content, enabling the model to explicitly learn the concept of harmfulness with minimal impact on its overall utility. The approach is designed to be complementary to existing safety methods and offers flexibility, allowing the token to be used either as a hard filter for unsafe responses or as a soft signal to trigger reflective, safety-focused Chain-of-Thought reasoning. Experiments show that this method, particularly when combined with adversarial training (RF-AT), provides strong robustness against various attacks, including pre-filling and sampling, while also demonstrating good cross-lingual generalization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is (mostly - see [Note] in Weaknesses) well written and easy to understand\n- Mitigating harmfulness in LLMs is a relevant topic\n- The proposed method is well grounded and appears to achieve robust safety performance with a marginal compromise on model utility\n- Despite the weaknesses listed below, the empirical evaluation of the proposed method is acceptable - but could be better"}, "weaknesses": {"value": "- No content provided regarding limitations or future work; See questions for examples of potential limitations\n- Adding a comparison with defense methods from other categories (e.g., self-reflect and controlled text generation - example of a popular approach [1]) would make the paper's contribution more compelling\n- The performance of one of the benchmarks (CAT) appears to be very comparable, if not better in some cases, to the proposed method\n- Increasing the font size on some of the plots would significantly improve the paper's readability.\n- [Note] On page 9, line 464-465, you have \"As expected, translated attacks achieve break the model\", which I'm guessing you mean \"[...] translated attacks break [...]\" - proofreading the paper might be necessary.\n- Understandably, it is difficult to provide an anonymized version of the finetuned models. However, an anonymized repository could have been provided to assert the reproducibility of this work.\n\n[1] Yang, K., & Klein, D. (2021). FUDGE: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218."}, "questions": {"value": "- Although this approach appears to improve safety over gradient-based attacks, isn't it still vulnerable to them? \n- Wouldn't an external safety approach be more robust to jailbreak attempts? Perhaps including a benchmark with this type of safety mechanism would allow answering this.   \n- Is there a reason to use GPT-5 to judge refusals instead of models designed specifically to detect refusals like, for example, WildGuard?\n- How often is the RF token triggered on harmless prompts? Perhaps it would be useful to show how a \"false positive\" would affect the performance (utility-wise) of the model, compared to when the model correctly assesses no risk, and/or the base model version.\n- How is the DSR generalization across languages of CAT? How does your method affect utility across languages, as compared to the base model?\n- How frequently is the red flag token generated more than once in a response? How does allowing the generation of this token multiple times help to increase model safety?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SuVNjdx2Gi", "forum": "1KhaUqHpUB", "replyto": "1KhaUqHpUB", "signatures": ["ICLR.cc/2026/Conference/Submission22933/Reviewer_XkHf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22933/Reviewer_XkHf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850480999, "cdate": 1761850480999, "tmdate": 1762942443516, "mdate": 1762942443516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}