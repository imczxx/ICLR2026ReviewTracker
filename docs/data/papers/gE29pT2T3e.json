{"id": "gE29pT2T3e", "number": 3739, "cdate": 1757508970110, "mdate": 1759898072466, "content": {"title": "StereoCrafter-Zero: Zero-Shot Stereo Video Generation with Noisy Restart", "abstract": "Generating high-quality stereo videos requires consistent depth perception and temporal coherence across frames. Despite advances in image and video synthesis using diffusion models, producing high-quality stereo videos remains a challenging task due to the difficulty of maintaining consistent temporal and spatial coherence between left and right views.\nWe introduce \\textit{StereoCrafter-Zero}, a novel framework for zero-shot stereo video generation that leverages video diffusion priors without requiring paired training data. Our key innovations include a noisy restart strategy to initialize stereo-aware latent representations and an iterative refinement process that progressively harmonizes the latent space, addressing issues like temporal flickering and view inconsistencies.\nIn addition, we propose the use of dissolved depth maps to streamline latent space operations by reducing high-frequency depth information.\nOur comprehensive evaluations, including quantitative metrics and user studies, demonstrate that \\textit{StereoCrafter-Zero} produces high-quality stereo videos with enhanced depth consistency and temporal smoothness. In terms of epipolar consistency, our method achieves an $11.7\\%$ improvement in MEt3R score over the current state-of-the-art. Furthermore, user studies indicate strong perceptual gains over the previous arts, with an $8.0\\%$ higher perceived frame quality and $10.9\\%$ higher perceived temporal coherence.\nOur code will be made publicly available upon acceptance of this manuscript.", "tldr": "", "keywords": ["Stereo Synthesis", "Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d22ba284ebd21fdc01a1fe230ddfe94f768678a.pdf", "supplementary_material": "/attachment/09cd2888110d604043faf1b20c726f8096bf7877.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents StereoCrafter-Zero, a zero-shot stereo video generation framework that synthesizes consistent stereo video pairs (left-right views) from a single image and text prompt, without requiring stereo training data. Previous work only involved generating stereo images, whereas this paper extends to video for the first time. The method builds upon video diffusion priors (e.g., DynamiCrafter) and introduces three core innovations:\n1.\tNoisy Restart — a latent initialization and controlled noise injection strategy to improve temporal and inter-view coherence;\n2.\tIterative Refinement — repeated re-denoising of occluded regions to harmonize latent representations;\n3.\tDissolved Depth Maps — low-frequency depth representations designed to enhance latent-space warping stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe stereo video generation task remains underexplored yet highly relevant, addressing the growing demand for VR and immersive content creation. The paper provides a clear motivation for the necessity of joint spatial-temporal-stereo consistency, extending beyond previous single-view or 2D-to-3D conversion studies. The authors creatively identify a novel research problem and propose a training-free solution through StereoCrafter-Zero.\n2.\tTechnical innovations are clear. The Noisy Restart strategy is conceptually elegant, reusing the diffusion process' stochasticity for structural stabilization. The Dissolved Depth Map idea is intuitive yet effective-reducing high-frequency depth noise aligns well with the latent-space nature of diffusion models. Iterative Refinement is a simple but practical scheme to correct occluded regions without excessive computational overhead."}, "weaknesses": {"value": "1.\tWhile the stereo video generation task is inherently complex, most evaluation metrics are adapted from standard video generation benchmarks, which lack stereo-specific evaluation criteria. Currently, only subjective user studies demonstrate the performance of StereoCrafter-Zero, which limits the objectivity of the validation. It is recommended to conduct quantitative comparisons with stereo diffusion models on established stereo conversion benchmarks to substantiate the approach further.\n2.\tThe generated results exhibit certain artifacts. For example, in the teaser video, the left hand of Wukong shows spatial inconsistency, and the butterflies display temporal incoherence between left-right views. These issues highlight the challenges in maintaining stereo consistency for training-free frameworks."}, "questions": {"value": "The authors compare against stereo conversion methods but not (or insufficiently) against established stereo generation or stereo-view synthesis benchmarks. Could the authors report results on one or more standard stereo datasets (or convert to them) to allow more objective quantitative comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xgYTQCm8xF", "forum": "gE29pT2T3e", "replyto": "gE29pT2T3e", "signatures": ["ICLR.cc/2026/Conference/Submission3739/Reviewer_4dv8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3739/Reviewer_4dv8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890620365, "cdate": 1761890620365, "tmdate": 1762916955115, "mdate": 1762916955115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes StereoCrafter-Zero, a zero-shot pipeline for stereo video generation from a single image and text prompt. The method keeps left/right views temporally coherent by operating in the latent space of a video diffusion model, with key techniques combining: \n1. Noisy Restart to initialize stereo-aware latents\n2. Iterative Refinement to denoise the occluded regions\n3. Dissolved (low-frequency) depth maps to stabilize latent warping without relying on precise disparities. \n\nOn the proposed benchmarks, the proposed method attains the best quantitative performance and strong user-study scores for frame quality and temporal coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is with clear problem framing and novelty. It's very intuitive and beneficial to tie the generation and stereo conversion within diffusion latents rather than pixels. To enable the combination of low-frequency information and high-frequency details.\n\n2. Noisy Restart and Iterative Refinement components are simple, well-motivated, and ablated extensively in the experiment section.\n\n3. And the authors have conducted extensive experiments to show the superior performance of the proposed method, and the validness of each proposed components."}, "weaknesses": {"value": "1. The proposed pipeline involves depth estimation, repeated warping, and multiple diffusion passes. An end-to-end latency and compute report and the comparisons against baselines is needed, and current not systematically reported in the paper.\n\n2. Intuitively, the proposed dissolved depth operates on the latent space of the diffusion model, which is with naturally downsampled space. This seems especially bad for high-resolution details and small objects. It'd be good to at least provide a qualitative analysis upon this perspective.\n\n3. Authors note mismatches between screen metrics and VR preference. Since currently VR-related metrics are still not well established, it'd be good to provide some qualitative examples to support this claim, and make the readers be more aware of the desired direction for future research."}, "questions": {"value": "Mostly from the above weakness part, some additional questions:\n\n1. What are the wall-clock times and memory usage for a typical video, including depth + diffusion + refinements? How do Noisy Restart window K/L and refinement rounds N trade off quality vs. runtime and memory cost?\n\n2.How does performance change when swapping the current video model with other backbones? Any failure trends across certain content types, e.g. human, animals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OnItZPLj71", "forum": "gE29pT2T3e", "replyto": "gE29pT2T3e", "signatures": ["ICLR.cc/2026/Conference/Submission3739/Reviewer_6Lsc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3739/Reviewer_6Lsc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905611650, "cdate": 1761905611650, "tmdate": 1762916954845, "mdate": 1762916954845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents StereoCrafter-Zero, a method for zero-shot stereo video generation from a single image and text prompt. It introduces Noisy Restart and Iterative Refinement to improve stereo and temporal coherence, along with Dissolved Depth Maps to reduce noise."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel framework for zero-shot stereo video generation, combining techniques like Noisy Restart and Iterative Refinement to enhance stereo consistency and temporal coherence without the need for paired training data."}, "weaknesses": {"value": "1. The video results presented do not include significant camera movements, and the camera remains almost static. This setup does not effectively demonstrate the performance of the proposed method. In real-world scenarios, large camera movements are common, and it is questionable how well the proposed method would perform under such conditions. \n\n2. Given the artifacts in the results from DepthCrafter, it is unclear why the authors chose it as the prior model instead of VideoDepthAnything, which demonstrates much better performance. Additionally, the claim in Line 268 that high-precision depth leads to artifacts, as well as the comparison in Table 3 with different depth estimation methods, is confusing and weird. If lower-quality depth maps can lead to better results, this should be clarified with more explanation or supporting evidence instead of suspection.\n\n3. The paper claims that latent space warping benefits more from coarse geometry than fine depth, and unlike image-domain warping, latent space warping focuses on coarse geometry and semantic consistency rather than relying on high-precision depth maps. However, how does the method ensure fine-grained alignment of details? The authors demonstrate simple examples, but how does it perform with more complex fine details? Is the use of lower-quality depth maps a trade-off for better high-precision details, or is there another reason for this choice?"}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mJMD3O2gMc", "forum": "gE29pT2T3e", "replyto": "gE29pT2T3e", "signatures": ["ICLR.cc/2026/Conference/Submission3739/Reviewer_T9o9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3739/Reviewer_T9o9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989019189, "cdate": 1761989019189, "tmdate": 1762916954538, "mdate": 1762916954538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free stereo video generator from a single image + prompt by coupling a pre-trained video diffusion prior with three sampling-time mechanisms: Noisy Restart (re-inject noise at selected steps), Iterative Refinement (masked denoising on occluded regions), and Dissolved Depth Maps (low-frequency depth to guide latent warps). The method optimizes stereo consistency in latent space rather than pixel space and reports gains on MEt3R (epipolar consistency) and a small VR user study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The stereo content generation is an important but under-explored application domain. And the angle of generating stereo content generation in a training-free manner is neat.\n2. The latent-space stereo coupling (warp + masked refinement) is clean and avoids heavy training or pixel-space inpainting, the dissolved-depth idea is a reasonable inductive bias at sampling time.\n3. The evaluation is comprehensive with Stereoscopy-aware metrics, which is important for stereo content generation. The performance is state-of-the-art."}, "weaknesses": {"value": "1. The core technical contribution \"Noisy Restart\" mechanism is essentially a sampling-time stochasticity strategy. Conceptually, it closely parallels the Noise Re-Injection approach introduced in Time Reversal Fusion (Explorative Inbetweening of Time and Space, Feng et al. ECCV 2024), where noise is periodically reintroduced during denoising to stabilize bidirectional generation trajectories. While this work applies the idea in a stereo-latent context rather than bounded in-betweening, the underlying principle—injecting mid-trajectory noise to prevent degeneracy and enhance temporal smoothness—is largely shared. I think the authors should explicitly acknowledge this lineage, clarify what aspects are unique to the stereo setting (e.g., the interaction with disparity-guided warping and dissolved-depth refinement).\n2. The stereo effect is largely affected by the latent-space depth prediction and warping, I wonder how would the method work given common failure modes of monodepth model, like volumetric / transparent cases (flame, raindrops, glass)\n3. I think the proposed method should be compared with more stereo video generation baselines like monocular video to stereo generation (such as SpatialDreamer Lv et al. CVPR 2025)."}, "questions": {"value": "I would like to know the authors thoughts regarding my concerns in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x7WchuFpbv", "forum": "gE29pT2T3e", "replyto": "gE29pT2T3e", "signatures": ["ICLR.cc/2026/Conference/Submission3739/Reviewer_SeqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3739/Reviewer_SeqF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998175567, "cdate": 1761998175567, "tmdate": 1762916954255, "mdate": 1762916954255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}