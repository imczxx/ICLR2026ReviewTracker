{"id": "hxwV5EubAw", "number": 22528, "cdate": 1758332312923, "mdate": 1759896861379, "content": {"title": "Hippoformer: Integrating Hippocampus-inspired Spatial Memory with Transformers", "abstract": "Transformers form the foundation of modern generative AI, yet their key–value memory lacks inherent spatial priors, constraining their capacity for spatial reasoning. In contrast, neuroscience points to the hippocampal–entorhinal system, where the medial entorhinal cortex provides structural codes and the hippocampus binds them with sensory codes to enable flexible spatial inference. However, existing hippocampus models such as the Tolman-Eichenbaum Machine (TEM) suffer from inefficiencies due to outer-product operations or context-length bottlenecks in self-attention, limiting their scalability and integration into modern deep learning frameworks. To bridge this gap, we propose mm-TEM, an efficient and scalable structural spatial memory model that leverages meta-MLP relational memory to improve training efficiency, form grid-like representations, and reveal a novel link between prediction horizon and grid scales. Extensive evaluation shows its strong generalization on long sequences, large-scale environments, and multi-step prediction, with analyses confirming that its advantages stem from explicit understanding of spatial structures. Building on this, we introduce Hippoformer, which integrates mm-TEM with Transformer to combine structural spatial memory with precise working memory and abstraction, achieving superior generalization in both 2D and 3D prediction tasks and highlighting the potential of hippocampal-inspired architectures for complex domains. Overall, Hippoformer represents a initial step toward seamlessly embedding structured spatial memory into foundation architectures, offering a potential scalable path to endow deep learning models with spatial intelligence.", "tldr": "We propose Hippoformer, a hybrid architecture that integrates hippocampal-inspired structured spatial memory with transformers, enabling scalable spatial reasoning and outperforming existing models in 2D and 3D grid tasks.", "keywords": ["Hippocampus", "grid cell", "spatial reasoning", "relational memory", "transformer"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e6f9b23517211a7eeba4a8de62655cd5e19c094.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes mm-TEM, a variant of the Tolman-Eichenbaum Machine using\nmeta-MLP memory, and Hippoformer, which integrates mm-TEM with Transformers. The\nauthors evaluate these models on 2D grid prediction and 3D environment tasks,\nclaiming superior generalization and discovering that memory update frequency\naffects grid-like representation scales."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Implementation efficiency: mm-TEM trains faster than original TEM (in gradient * steps), making hippocampal-inspired architectures more practical - but see note below regarding \"efficiency\"\n* Integration with Transformers: The Hippoformer architecture combining structured spatial memory with Transformer's working memory is conceptually interesting.\n* Extensive empirical evaluation: The paper includes multiple experimental settings (varying context lengths, environment sizes, circular grids, 2D and 3D tasks).\n* Clear presentation: The paper is generally well-written with good figures illustrating the architecture and results.\n* Emergence of grid-like representations: The spontaneous development of grid patterns provides interesting connections to neuroscience."}, "weaknesses": {"value": "## Missing Foundational Literature on Hippocampal-Entorhinal Memory Systems\n\nThe introduction would be strengthened by acknowledging the broader foundational\nliterature on hippocampal-entorhinal contributions to episodic and relational\nmemory beyond spatial navigation. Several core ideas presented as novel to the\nTEM framework—namely, factorized MEC-LEC streams, hippocampal binding of\nstructural and sensory codes, and generalization across relational spaces—have\nsubstantial precedent in earlier empirical and computational work. The 'binding\nof items and context' (BIC) model (Eichenbaum et al., 2007) directly addresses\nhow the hippocampus binds contextual and item information. Lesion and recording\nstudies have established MEC vs LEC functional dissociations (Hargreaves et al.,\n2005; Knierim et al., 2014). The Complementary Learning Systems framework\n(McClelland, McNaughton, & O'Reilly, 1995) proposed separate systems for rapid\nrelational binding and structured representations decades earlier. Computational\nmodels have explicitly implemented factorized spatial and sensory\nrepresentations: Hasselmo et al. (2002) modeled grid cells for memory with\nstructural/contextual codes; Franzius, Sprekeler & Wiskott (2007) demonstrated\ngrid/place code emergence with associative mapping to sensory inputs; Bush et\nal. (2015) explicitly separated grid-cell path integration from memory\nassociation in recurrent networks. I recommend that the authors broaden their\nintroductory discussion to acknowledge this literature and clarify precisely\nwhat mm-TEM contributes beyond these established frameworks rather than\nappearing to introduce factorized hippocampal memory de novo.\n\n\n## Terminology: \"TEM Theory\" Should Be \"TEM Model/Framework\"\n\nThroughout the manuscript, the authors refer to \"TEM theory\" (e.g., lines 54,\n134). I suggest replacing this with \"the TEM model\" or \"the TEM framework.\" The\nTolman-Eichenbaum Machine (Whittington et al., 2020) is a computational\ninstantiation of existing theoretical ideas about cognitive mapping and\nrelational memory, rather than a full-fledged theory in itself. The underlying\ntheoretical principles, in particular that the hippocampal-entorhinal system\nimplements factorized memory for flexible generalization, predate TEM by decades\n(see comment above). Calling TEM a \"theory\" risks overstating its conceptual\nnovelty and may mislead readers about its epistemic status. The contribution of\nWhittington et al. (2020) was to provide an elegant computational implementation\nof these principles, not to propose the theoretical framework itself. This\ndistinction matters for accurately situating the current work in the scientific\nliterature.\n\n\n## Missing Related Work on Factorized Memory Architecturs\nThe paper would benefit from acknowledging a broader range of computational\nmodels that have explored factorized memory architectures combining structural\nand sensory representations. Beyond the TEM lineage cited, several prior works\nshare the core principle of separating spatial/structural codes from\nsensory/content codes for flexible memory retrieval. Hasselmo et al. (2002)\nmodeled how entorhinal cortex provides structural/contextual codes while the\nhippocampus performs binding operations. The Complementary Learning Systems\nframework (McClelland, McNaughton, & O'Reilly, 1995), while not specifically\nEC-HC focused, proposed separate systems for rapid relational binding versus\nstructured long-term representations—a conceptual predecessor to factorized\nmemory architectures. Franzius, Sprekeler & Wiskott (2007) demonstrated\ngrid/place code emergence with associative mapping to sensory inputs, providing\nan early computational analogue of separating structural representations from\nsensory mappings. Bush et al. (2015) explicitly implemented computational\nseparation of grid-cell-like path integration from memory association in\nrecurrent networks. Stachenfeld et al. (2017) modeled the hippocampus as a\npredictive map using successor representations, integrating spatial structure\nwith value predictions. Waniek (2020) proposed Transition Scale-Spaces that\nintegrate structural and sensory information in multi-scale frameworks. Even\nwork on sequence memory in recurrent networks (e.g., Rajan, Harvey, & Tank,\n2016) employs separate state/structure representations with readout layers,\nloosely corresponding to mm-TEM's structural versus sensory binding.\nAcknowledging these precedents would provide better context for understanding\nhow mm-TEM's specific implementation choices (meta-MLP memory, auxiliary losses,\nTransformer integration) extend versus replicate prior ideas, and would help\nreaders assess the true novelty of the contribution.\n\n\n## Imprecise Terminology: \"Novelty or Surprisal\"\nThe manuscript refers to \"novelty or surprisal\" in describing the fast-weight\nupdate mechanism (lines 171-172). In formal information theory, these are\ndistinct concepts: surprisal is a probabilistic quantity (−log P(x)), whereas\nnovelty is relative to memory or prior experience (Palm, 2012). What the model\nactually computes is the gradient of reconstruction loss, ∇_Θ L(k_t, v_t), which\nmeasures prediction error -- the mismatch between predicted and actual values. I\nrecommend clarifying which quantity is actually computed and adjusting the\nterminology to reflect the implementation accurately. The most precise term\nwould be \"prediction error\" or \"reconstruction error\" rather than the ambiguous\n\"novelty or surprisal.\" This precision matters for readers seeking to understand\nthe computational mechanism and for those attempting to reproduce or extend the\nwork.\n\n\n## Misleading Description of Hippocampal-Entorhinal Feedback\nIn Appendix A.2 (lines 630-632), the manuscript states: \"visual sensory cues\nprovide feedback from the HC to correct path integration errors in the MEC.\"\nThis description is biologically misleading. While feedback from the hippocampus\ncan indeed help stabilize MEC representations and correct errors in path\nintegration (Diehl et al. 2019, Mulas et al. 2016, and many others), the hippocampus does not send raw visual sensory information to\nMEC. Visual and other sensory inputs primarily reach MEC via cortical pathways\n(particularly through LEC from perirhinal and parahippocampal cortices). A more\naccurate description would be that the hippocampus provides relational or\nspatial feedback, likely in the form of conjunctive codes that bind spatial and\nsensory information, that can help recalibrate MEC structural representations,\nwhile MEC continues to integrate sensory cues from neocortical areas. I\nrecommend rephrasing this passage to clarify that the feedback represents a\ncomputational abstraction where HC provides spatial/relational corrections\nrather than literal sensory signals. This distinction is important for readers\ninterpreting the model in a neurobiological context and for understanding what\naspects of the architecture are biologically plausible versus computational\nconveniences.\n\n\n## Missing Critical Baseline: Direct Associative Memory Comparison\nThe paper's central claim is that factorizing memory into structural codes (via\npath integration) and sensory codes enables superior generalization compared to\nstandard architectures. However, the experimental evaluation lacks a critical\nablation: a direct associative memory baseline that learns state-transition\npairs (state_t, action_t) → state_{t+1} using the same meta-MLP architecture,\nwarm-up procedure, and auxiliary losses, but without structural factorization.\nThe current baselines (Transformer, Titans) differ from mm-TEM in multiple\nconfounded ways -- architecture, training procedures, and task formulation --\nmaking it impossible to isolate whether the performance gains stem from the\nfactorized representation itself or simply from having a meta-trained\nassociative memory.  The ablations in Fig. 3C only remove auxiliary losses, not\nthe core architectural choice of factorization. Furthermore, the claim of\n\"efficiency\" lacks rigor: no computational complexity analysis, wall-clock time\ncomparison, or memory footprint evaluation is provided, only gradient step counts\nagainst the original TEM. The comparison is further confounded by the warm-up\npre-training phase whose application to baselines is unclear. Finally, in small\ngrid worlds (8×8 to 11×11), a 64-step context likely covers a substantial\nfraction of reachable states, meaning the model may largely be performing\nwithin-episode memory retrieval rather than true generalization to novel spatial\nconfigurations. A proper ablation study isolating the contribution of structural\nfactorization is essential to validate the paper's core hypothesis.\n\n## Insufficient Mechanistic Explanation for Grid Scale vs. Update Frequency Relationship\nThe paper claims that the memory update frequency hyperparameter mb controls\ngrid scale through an \"effective prediction horizon\" mechanism (lines 252-256),\npositioning this as a novel insight into grid-scale diversity. However, this\nexplanation lacks mechanistic rigor and conflates multiple confounded factors.\nFirst, mb affects both training dynamics (how often gradients update the\nmeta-MLP weights) and inference behavior (how stale the memory becomes), but the\npaper does not disentangle which factor drives the grid scale effect. The\nobserved correlation could simply be an artifact of optimization\ndynamics. That is, sparser gradient updates naturally produce temporally smoother,\nlower, frequency representations through gradient accumulation, rather than\nreflecting a meaningful \"prediction horizon.\" Second, the feedback mechanism\nfrom relational memory to path integration (Appendix A.2) is central to\nunderstanding this effect but is incompletely described: the function f_delta is\nnot defined, the strength of feedback (α) is not analyzed, and how mb interacts\nwith this feedback is unclear. Third, no ablation studies isolate the causal\nmechanism. For instance, training with mb=1 but testing with mb=8, or analyzing\ngradient flow as a function of mb. Without this mechanistic analysis, the claim\nthat mb reveals insights into biological grid-scale diversity through\n\"multi-timescale predictions\" remains speculative correlation rather than\ndemonstrated causation. The authors should provide: (1) ablations separating\ntraining-time vs. test-time effects of mb, (2) gradient flow analysis explaining\nwhy different update frequencies produce different spatial frequencies, and (3)\ncomplete mathematical description of the feedback mechanism.\n\n## Missing Citation and Overclaimed Novelty on Grid Scale Mechanisms\nThe paper claims to reveal \"a novel mechanism for grid-scale diversity in\nMEC...as a natural consequence of multi-timescale predictions in the brain\"\n(lines 252-256), based on their observation that the memory update frequency mb\naffects grid scale. However, this is not novel -- Waniek (2020, \"Transition\nScale-Spaces: A Computational Theory for the Discretized Entorhinal Cortex\")\nanalytically derived that grid scales emerge from different prediction horizons,\nwith the spatial frequency directly related to the temporal prediction distance.\nThe current paper essentially re-discovers this relationship empirically without\nciting this prior work. Moreover, Waniek's analysis is rigorous in the sense of\nan analytical derivation, compared to the current paper's informal \"effective\nprediction horizon\" argument. Related work by Stachenfeld et al. (2017) and\nDordek et al. (2016) also established connections between temporal prediction\nscales and spatial grid scales. The authors should: (1) cite this prior\ntheoretical work, (2) clarify what is actually novel about their contribution\nbeyond empirical confirmation in a different architecture, and (3) either remove\nthe novelty claim or demonstrate what their mechanism adds beyond existing\ntheory.\n\n## Ambiguity in Generalization Claims Across Sections 3.1 and 3.2\nThe paper creates confusion about what \"generalization\" means across different\nexperiments. In Appendix A.1 (lines 600-602), the authors state that for \"all 2D\ngrid prediction tasks,\" evaluation is confined to previously visited positions\nbecause \"predicting observations at unvisited locations is not meaningful\" given\nthat observations are uncorrelated discrete IDs. However, in Section 3.2, which\nclaims to test \"generalization\" (line 260), this critical constraint is never\nrestated, leaving readers to infer that it still applies. This matters because\nSection 3.2's \"multi-step imagination\" is framed as testing the model's ability\nto \"generalize beyond its training horizon\" (line 268), but if evaluation is\nconfined to previously encountered positions, it's actually testing memory\nretrieval over longer sequences, not spatial generalization to novel states. The\npaper should explicitly clarify for each experiment: (1) whether evaluation\nincludes unseen positions, (2) what fraction of test positions were visited\nduring context, and (3) how \"accuracy\" is computed when positions are/aren't\npreviously seen. The distinction is crucial: the 3D experiments (Section 3.4)\nacknowledge that \"unvisited observations can be inferred from nearby spatial\ninformation\" due to continuous visual features, but no such acknowledgment is\nmade for 2D tasks where this fundamental difference in evaluation regimes should\nbe highlighted upfront in Section 3.2, not buried in the appendix.\n\n\n## Unjustified Causal Claims from Correlational Data (Section 3.2, Figure 4)\nThe paper claims that \"the presence of strongly grid-like cells is a key driver\nfor generalization\" (lines 346-347) based solely on a correlation between grid\nscore and prediction accuracy (r=0.647, Fig. 4A). This is a causal claim\nunsupported by the evidence. Correlation does not establish causality -- the\nrelationship could be due to reverse causation (good generalization enables\nbetter grid formation) or a confounding variable (successful learning produces\nboth regular representations and good generalization). Notably, the paper's own\ndata weakens the causal claim: Figure 4B shows models with low grid scores\nachieving high accuracy through \"alternative but still regular neural\nrepresentations,\" suggesting that regularity (not grid-ness specifically) may be\nwhat matters. To establish causality, the authors should conduct interventional\nexperiments: (1) inject hand-designed grid representations and test if\nperformance improves, (2) add regularization to suppress grid formation and test\nif performance degrades, or (3) explicitly bias learning toward grid patterns\nand compare against controls. As written, the claim that grids \"facilitate\" or\n\"drive\" generalization is speculative. The authors should either provide causal\nevidence or rephrase their claims to accurately reflect the correlational nature\nof their findings (e.g., \"grid scores correlate with generalization performance,\nsuggesting a potential relationship\").\n\n\n## Questionable Statistical Analysis in Figure 4A\nThe correlation analysis in Figure 4A raises several statistical concerns.\nFirst, while reporting r=0.647 (p=0.0002), the authors use linear regression\ndespite evidence of non-linearity: accuracy appears to show a ceiling effect\n(~0.95-1.0) and the relationship exhibits heteroscedasticity (variance in\naccuracy is much higher at low grid scores than high). This violates key\nassumptions of linear regression, making standard errors, confidence intervals,\nand p-values unreliable. Second, with only ~20-25 data points visible, the\nanalysis is sensitive to outliers and the wide confidence interval suggests\nsubstantial uncertainty. Third, r²≈0.42 means 58% of variance in accuracy\nremains unexplained, yet the authors make strong causal claims (\"key driver\")\nfrom this weak-to-moderate correlation. Fourth, the scatter plot shows several\ncounterexamples to the claimed relationship: models with grid scores around\n0.7-0.9 achieve accuracy >0.9, while models with grid scores ~1.0-1.1 have\naccuracy ~0.8. These observations, also noted in the text regarding \"low grid\nscores still achieve high accuracy\" (Fig 4B), directly contradict the linear\nrelationship implied by the regression line. The authors should: (1) test for\nnon-linear relationships (threshold models, saturation functions), (2) report\nnon-parametric correlations (Spearman's ρ) that don't assume linearity, (3) show\nresidual plots and test assumptions, (4) report prediction intervals to\ndemonstrate the large uncertainty in predictions, and (5) acknowledge that grid\nscore alone is a poor predictor of performance. The statistical evidence does\nnot support the strong causal claims made in the text.\n\n## Undefined Error Metric and Inconsistent Terminology in 3D Experiments (Section 3.4, Table 1)\nThe 3D environment evaluation suffers from unclear methodology and inconsistent\nreporting. Table 1 reports \"prediction error in units of 1e-3\" but never defines\nwhat error metric is used. Presumably MSE between predicted and ground truth\negocentric images, but this is not stated. Are errors computed in pixel space,\nnormalized space, or feature space? How are images preprocessed? The text\ncompounds confusion by referring to \"accuracy\" (lines 432-435) while the table\nshows \"error\". These are opposite metrics (higher accuracy vs. lower error is\nbetter). The classification of frames as \"visible\" vs \"not visible\" is\nundefined: what determines if a frame is considered previously seen in\ncontinuous 3D space with egocentric views? The reported standard deviations\nraise questions: one-step errors show std=0.00 across 3 seeds for all models\n(presumably due to rounding, but this should be clarified), while Hippoformer's\nmulti-step std (0.04) is 100× smaller than baselines (4-5). Why is Hippoformer so\nmuch more stable? Finally, without baseline comparisons (chance level, naive\npredictors) or interpretation of error magnitudes (e.g., \"0.001 corresponds to X\nper-pixel deviation\"), the numbers lack context. The authors should: (1)\nexplicitly define the error metric and computation procedure, (2) use consistent\nterminology (error or accuracy, not both), (3) specify the visible/not-visible\nclassification criterion, (4) explain the variance patterns across models, and\n(5) provide baselines and interpretation to make the magnitudes meaningful.\n\n\n## Discussion Section Overclaims Novelty and Omits Critical Limitations\nThe Discussion overclaims novelty and omits acknowledgment of significant\nlimitations identified throughout the paper. First, the claim that mm-TEM offers\n\"a new functional perspective on grid diversity\" (line 445) ignores Waniek\n(2020), who analytically derived that grid scales emerge from prediction\ndistances - the very relationship mm-TEM rediscovers empirically. Second, the\nRelated Work section omits foundational hippocampal-entorhinal literature\n(Eichenbaum et al., 2007; Hasselmo et al., 2002; Stachenfeld et al., 2017;\nFranzius et al., 2007; Bush et al., 2015) that established factorized memory\narchitectures and prediction-timescale-to-spatial-scale relationships decades\nbefore TEM. Third, while the authors acknowledge limited integration and\nsingle-layer design, they fail to address major methodological limitations: (1)\n2D evaluation confined to previously visited positions (Appendix A.1), meaning\n\"generalization\" is actually memory retrieval, not spatial inference to novel\nstates; (2) small environments (8×8 to 11×11) where 64-step context covers\nsubstantial state space; (3) no ablation isolating the contribution of\nfactorization versus direct associative memory; (4) causal claims about\ngrid-generalization relationship based solely on correlation (r=0.647); (5)\nundefined error metrics and inconsistent terminology in 3D experiments. Fourth,\nefficiency and scalability claims lack rigor: no computational complexity\nanalysis, wall-clock time comparisons, or large-scale demonstrations are\nprovided. The Discussion should thus (1) cite prior work and clarify what is\nnovel about the grid-scale finding beyond empirical confirmation, (2)\nacknowledge foundational neuroscience literature on factorized memory, (3)\nexplicitly discuss the limitation that 2D \"generalization\" is constrained to\npreviously visited positions (or clarify the text) (4) acknowledge that\ngrid-generalization causality remains unproven, (5) provide concrete criteria\nfor when mm-TEM/Hippoformer would be preferred over standard architectures, and\n(6) temper claims about efficiency and scalability until rigorous evidence is\nprovided."}, "questions": {"value": "1. Can you provide a direct ablation comparing mm-TEM against a flat associative memory (no factorization) with identical meta-MLP architecture, warm-up procedure, and auxiliary losses?\n2. Can you clarify the evaluation protocol for 2D tasks in Section 3.2? What percentage of positions in the \"imagination\" phase were previously visited during context?\n3. For the mb parameter effect on grid scales: Can you ablate training vs. test-time effects? (Train with mb=1, test with mb=8 and vice versa?)\n4. For Figure 4A: Can you provide non-parametric correlation measures (Spearman's ρ), test for non-linear relationships, and show residual plots?\n5. For Table 1: What specific error metric is used? How are \"visible\" vs. \"not visible\" frames classified in continuous 3D space?\n6. Can you provide wall-clock time comparisons and computational complexity analysis to support efficiency claims?\n7. How does performance scale to truly large environments where 64-step context covers <10% of reachable states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8tCKvyboPi", "forum": "hxwV5EubAw", "replyto": "hxwV5EubAw", "signatures": ["ICLR.cc/2026/Conference/Submission22528/Reviewer_3ncY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22528/Reviewer_3ncY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761218805961, "cdate": 1761218805961, "tmdate": 1762942262514, "mdate": 1762942262514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work leverages recent formulations of memory from the Titan framework to improve models of the HPC-EC system. This in turn 1) improves the memory efficiency of the HPC-EC models over long contexts, 2) yields insights into grid scaling, 3) leads to an architecture that performs better than standard Titan and Transformer architectures. The new memory structure is an MLP which is trained in-context to map from keys to values, in contrast to older models based on Hebbian and more recently softmax attention."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work is a significant and novel combination of recent ideas in neuroscience and machine learning. The improvement of tem-t using memory formulations from the recent Titans work is innovative and yields a fun result relating to grid scale. A hybrid architecture is proposed that blends the strengths of the titans-inspired model and transformer; however I would prefer it if the differential contributions of 1) the path-integration input (an old idea) and 2) the meta-MLP relational memory (new idea) were stated more clearly when comparing between models. The architecture has genuine promise to improve upon existing sequence models and I would be fascinated to see it deployed on language problems. \n\nThe figures are all clear and the text is very well written."}, "weaknesses": {"value": "Barely a weakness & perhaps more to do with framing, but my understanding is that both Transformer and Titan control models you implemented do not have a recurrent path-integrator. Therefore the improvements demonstrated over each are primarily due to the path-integrator token input that the model receives. Tem-t also has this advantage and so should also perform similarly well, which should be mentioned. It would be nice to see more comparisons between tem-t and mm-tem. The use of the meta-MLP memory instead of softmax attention is elegant, I would be curious to see the gains explored slightly more. Perhaps MLP is more brittle than softmax when faced with a novel but semantically familiar input? Or perhaps MLP imbues memory retrieval with generalisation capacity since it is itself a NN? And does this impact the learnt g representations of the outer-loop meta network? Maybe the MLP allows more interesting operations to be done on memories, e.g. contextual splicing of memories - if memorised spatial envs A and B and then encounter env C in which half the stimuli are from A states and half from B"}, "questions": {"value": "Typos:\n- 188, 194, 256, 735, 736\n\nDo we have a curve for fig2B tem-t?\n\nCan you clarify in main text that the transformer component of Hippoformer does not receive the path-integration input\n\nSome questions about mb:\n- What is the intuition for larger mb increasing the learnt grid size? A fun result that I don't fully understand! \n- Does mb = 1 really perform that poorly - especially since there is no noise in the observations? If so then it would be nice to see this somewhere (since this is essentially the basis of the argument for including transformer in Hippoformer)\n- Does smaller mb lead to better performance (assuming observations aren't noisy)? - If I understand correctly, a small mb ~ 1 is more similar to a transformer; would a network endowed with multiple memory MLPs each with different mb yield a similar performance to Hippoformer & also get grids at different scales?\n- Linked to question above, could mb relate to oscillation frequency in HPC? I believe there is a dorsal-ventral gradient of oscillation frequencies in hippocampus (matches nicely with the gradient of grid scales). Also wonder if there are interesting findings relating to mb & the discretisation of grid scales that is observed.\n\nAre there biological analogies to the hippoformer? \n\nI've always been curious to see what the path-integrator module adds when applied to tasks that are less obviously cognitive map-like. Have you tried this model on text-based tasks?\n\nAre there relations between the Titans meta-MLP and working memory (in contrast to transformer softmax which seems more episodic memory-like)? If so, maybe some kind of systems consolidation inspired ideas might apply here i.e. selectively exporting things out of hippocampal memory into neocortical memory\n\nLine 453 - should transformers and mm-TEM be the other way round?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dWXLaCynjH", "forum": "hxwV5EubAw", "replyto": "hxwV5EubAw", "signatures": ["ICLR.cc/2026/Conference/Submission22528/Reviewer_8yz6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22528/Reviewer_8yz6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861460668, "cdate": 1761861460668, "tmdate": 1762942262192, "mdate": 1762942262192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hippoformer, a hybrid architecture combining a novel hippocampus-inspired memory module, mm-TEM, with a Transformer. The work aims to address the lack of inherent spatial priors in standard Transformers. The central contribution lies in mm-TEM, a scalable variant of the Tolman–Eichenbaum Machine that employs a meta-MLP for relational memory, closely resembling the long-term memory module used in Titan. The authors conduct experiments in 2D and 3D environments, reporting that Hippoformer achieves superior generalization on long-horizon spatial prediction tasks compared to Transformer and Titan baselines.\n\nWhile the paper is well-written and tackles an important problem, I have major concerns regarding the experimental methodology and the clarity of the architectural contributions. The central claims about the superiority of the proposed relational memory are not sufficiently supported because the baseline comparisons appear to be confounded by critical differences in how positional information is handled."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Strong Motivation**: The paper is motivated by a clear and significant limitation of current generative models—their lack of structured spatial memory. The inspiration drawn from the hippocampal-entorhinal system provides a principled foundation for the architectural design.\n* **Interesting Component Design**: The mm-TEM module, with its meta-MLP and auxiliary relational losses, is an interesting and efficient take on prior hippocampus models. The analysis showing the emergence of grid-like representations is a valuable piece of evidence supporting the design."}, "weaknesses": {"value": "The paper's central claims hinge on the superior performance of mm-TEM and Hippoformer over strong baselines, particularly in length generalization. However, the experimental setup is insufficiently described and may contain confounding variables that invalidate these conclusions.\n\n1.  **Lack of Clarity on Positional Encoding in Baselines:** The most critical issue is the ambiguity surrounding the positional encoding (PE) used for the Transformer and Titan baselines. The mm-TEM model relies on a recurrently updated structural code, `g_t`, from a Path Integration Network, which effectively serves as a powerful, dynamic, and task-specific form of positional encoding. The paper does not specify whether the baselines have access to this same structural code.\n    *   If the baselines use standard, fixed positional encodings (e.g., sinusoidal) or no PE at all, the comparison is fundamentally flawed. The performance gains of Hippoformer could stem entirely from its superior positional information, not its relational memory. As shown by Kazemnejad et al. (NeurIPS 2023, \"The Impact of Positional Encoding on Length Generalization in Transformers\"), the choice of PE is a dominant factor in a Transformer's ability to generalize to longer sequences. To isolate the contribution of the relational memory, the baselines must be equipped with a similarly powerful and dynamic PE.\n    *   This lack of clarity makes it impossible to attribute the performance gains to the claimed source (the relational memory) versus a known, powerful factor (the positional encoding scheme).\n\n2.  **Unclear Architectural Novelty Compared to Titans:** The paper presents Hippoformer as a combination of a Transformer and the mm-TEM module (which is centered around a meta-MLP memory). The Titan architecture is also described as a model leveraging fast MLP weights. From the descriptions provided, the high-level architectural blueprint of Hippoformer appears very similar to that of Titans. The paper needs to explicitly detail the architectural and mechanistic differences. Is the primary novelty of Hippoformer simply the introduction of the auxiliary relational loss as a form of inductive bias for the meta-MLP? If so, the contribution should be framed more narrowly as a novel training objective for existing hybrid architectures on spatial tasks, rather than a fundamentally new architecture.\n\n3.  **Ambiguity in the Training of the Recurrent Module:** The mm-TEM module contains a recurrent update for the structural code `g_t`. This introduces dependencies across the entire sequence. The paper lacks crucial details about how this recurrence is handled during training. Is backpropagation through time (BPTT) performed over the full sequence length? Or is it truncated? This detail has significant implications for the model's computational cost, memory requirements, and its practical ability to capture the long-range dependencies it is being credited for."}, "questions": {"value": "**1. Clarifications on Baseline Models (Crucial for Rebuttal):**\nThis is the most important area. A clear response here could significantly change my assessment.\n\n*   **Question 1a (Positional Encoding):** How was positional/structural information provided to the Transformer and Titan baseline models? Specifically, did they take as input only `[s_t, a_t]`, or did they also receive the structural code `g_t` from the Path Integration Network, similar to mm-TEM?\n*   **Question 1b (Type of PE):** If the baselines did *not* use the Path Integration Network, what form of positional encoding was used (e.g., sinusoidal, learned, rotary, or none)?\n\n**2. Architectural and Contribution Framing:**\n\n*   **Question 2 (Hippoformer vs. Titans):** Could you please provide a more detailed, side-by-side comparison of the Hippoformer and Titan architectures? What are the key differences in their memory update rules, the interaction between the MLP-based memory and the self-attention component, and the flow of information? A diagram or table would be very helpful.\n\n**3. Implementation Details for Reproducibility and Analysis:**\n\n*   **Question 3 (Training Recurrent Module):** How is the gradient calculated for the recurrent mm-TEM module? Is backpropagation through time (BPTT) applied over the full sequence length (e.g., 256 steps in Fig. 2), or is it truncated to a smaller window? What are the implications of this choice for computational complexity and memory usage during training?\n\n\n**Suggestions**\n* If the Fig. 3A experiment corresponds to the length generalization task and does not use the Path Integrator Network for g_t or employ g_t as a positional encoding, the authors should evaluate multiple positional encoding methods and compare their results to those of mm-TEM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "K8XLMErwHd", "forum": "hxwV5EubAw", "replyto": "hxwV5EubAw", "signatures": ["ICLR.cc/2026/Conference/Submission22528/Reviewer_Ny5p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22528/Reviewer_Ny5p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993454903, "cdate": 1761993454903, "tmdate": 1762942261947, "mdate": 1762942261947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of prior hippocampal-inspired models (TEM's computational inefficiency) and modern architectures (Titans' lack of inherent spatial memory). By synthesizing these approaches, the authors propose **mm-TEM** (meta-MLP TEM), demonstrating better training efficiency and revealing a novel link between the memory update frequency and the emergence of biologically meaningful grid-like representations. Furthermore, they introduce **Hippoformer** (mm-TEM + Transformer), a hybrid architecture that effectively integrates structured long-term spatial memory with precise short-term working memory, achieving robust generalization across demanding 2D and 3D prediction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*   **Clarity and Organization:** The paper is well-structured with clear conceptual figures (e.g., Figure 1), making the overall architecture and training rationale highly intuitive.\n*   **Compelling Rationale:** The core rationale for the proposed method—integrating the computational efficiency of the meta-MLP memory (inspired by Titans) into the theoretically grounded TEM framework—is valid for overcoming scalability issues while retaining biological plausibility.\n*   **Strong Experimental Validation:** The systematic evaluation, including ablations and generalization tests across long context, multi-step imagination, and distribution shifts (circular-grid), robustly supports the claim that mm-TEM captures underlying spatial structure more faithfully than baseline models."}, "weaknesses": {"value": "**Literature Review Suggestion (Line 40):** The Introduction's discussion of the Transformer's associative memory perspective would be strengthened by citing recent, relevant works that formalize the Transformer's components (e.g., FFNs) as explicit memory systems, such as:\n- Geva, Mor, et al. (2020) on FFNs as key-value memories.\n- Ramsauer, Hubert, et al. (2020) on Hopfield networks' relation to attention.\n\n**Relational Loss Notation and Rationale:**\n- **Notation Clarity:** The auxiliary relational losses (Lines 184-190) require precise clarification. Please map the main text's definitions\\ \n$L_1$ and $L_2$ to the figure notations of $L_{x2g}$, $L_{g2x}$, $L_{g2g}$ and clarify the missing one.\n\n- **Missing Term Rationale:** The authors can define four potential combinations ($g \\to x$, $x \\to g$, $g \\to g$, $x \\to x$). But figure/main text uses 2/3 losses. What is the underlying reason for excluding the other possible terms from the relational losses?\n\n2.  **Path Integration Network Ablation and Role:**\n    *   The core TEM theory mandates a Path Integration (PI) network for structural code generation. Since the authors emphasize the **novel meta-MLP relational memory** as the main source of generalization, could the authors include an **ablation study on the PI network component itself** (e.g., replacing it with a simpler, non-integrated recurrent mechanism or removing the error correction loop)? This would help quantitatively disentangle the contribution of the *novel memory* from the *PI component*.\n    *   Given the PI network's role in generating the structural code ($g_t$) from actions, could this component be viewed as an advanced form of **learned positional encoding**?\n\n3.  **Grid Pattern Neuron Selection:**\n    *   For the analysis of emergent grid-like representations (Figure 2C, Figure 4B), the authors show visualizations of the high-gridness neurons. Could the authors explicitly state **from which module** (the Path Integration Network or the Relational Memory Network) the \"Top-5/Top-3\" high-gridness neurons were selected? Clarification on the specific origin of these analyzed neurons is crucial for interpreting the results.\n\n4.  **Table 1 Completeness (3D Task):**\n    *   Table 1 presents results for Transformer, Titans, and Hippoformer in the 3D environment. Given that **mm-TEM** is the core component providing the long-term generalization in Hippoformer, why are the performance results for the **mm-TEM** model alone **omitted**? (Figure 5 shows on par imagination capability between mm-TEM and hippoformer) Including mm-TEM's 3D performance would provide a necessary direct measure of the Transformer's specific contribution (abstraction) to the final Hippoformer architecture in this complex domain, thereby strengthening the claim of synergy."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XeXuCv81It", "forum": "hxwV5EubAw", "replyto": "hxwV5EubAw", "signatures": ["ICLR.cc/2026/Conference/Submission22528/Reviewer_Be6v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22528/Reviewer_Be6v"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997161354, "cdate": 1761997161354, "tmdate": 1762942261589, "mdate": 1762942261589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}