{"id": "uv9olmscIS", "number": 20246, "cdate": 1758304131544, "mdate": 1759896988672, "content": {"title": "OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand Open Knowledge Benchmarking", "abstract": "Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose OpenKnowledgeBench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on multiple open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.", "tldr": "", "keywords": ["LLM benchmarking", "dynamic evaluation", "knowledge updates", "automated benchmarks", "retrieval-augmented methods"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d52a9d158f165795cd0319acb0017251913bc204.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an automatic benchmark construction method for evaluating LLMs with multi-choice questions based on recent news that are expected to be not memorized by LLMs. The construction method uses LLM to generate questions and validate questions. 4 PhD students are asked to validate 200 questions with two panels: the answers have a correct rate of 94% in the first panel and 100% in the second panel. \n\nThe evaluate reports results of testing four LLMs, without context, without oracle context and with retrievers of BM25, DPR and ColBERT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Evaluating the performance of LLMs and LLMs with RAG is an interesting research problem. The dataset attempts to bridge this gap.\n\n2. The paper is well written. \n\n3. A lot of results on different LLMs, with and without context, and different retrievers are reported and analysed."}, "weaknesses": {"value": "1. The current evaluation focuses on different LLM-based QA models, but ignore the dataset construction method, and the key features of the dataset. \n\n2. I do not see any results on the efficiency and scalability of the benchmark construction method. High efficiency is required to ensure emerging LLMs. The validation of the question-answer quality depends on human annotation. This is time consuming. I also think this manual validation is required when new samples are generated, as the effectiveness of the LLMs for creating these samples may expire in dealing with new data. \n\n3. There is a shortage of incremental evaluation, e.g., results of datasets constructed in Jan 2025, Apr 2025, Jul 2025. This can observe the effectiveness of the data construction method in the time line."}, "questions": {"value": "1. As the questions are new (after the LLM is pre-trained), why some LLMs can still achieve as good performance as 50%? \n\n2. Is the following paper, which use new knowledge from Wikidata for evaluating LLMs, relevant? https://arxiv.org/abs/2412.17032 \n\n3. How do you check the licenses or copyright of the news data providers? Does the news data include any sensitive personal information?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "We need to check whether the authors satisfy the copyright of the news data providers, and whether the news data involve any personal information."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BH7Jy0Mn9w", "forum": "uv9olmscIS", "replyto": "uv9olmscIS", "signatures": ["ICLR.cc/2026/Conference/Submission20246/Reviewer_hfc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20246/Reviewer_hfc4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565497320, "cdate": 1761565497320, "tmdate": 1762933735927, "mdate": 1762933735927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for dynamic and decentralized evaluation of LLMs on evolving factual knowledge. The core contribution is OKBench, a fully automated, agentic pipeline that continuously generates, validates, and versions QA benchmarks from daily news streams. OKBench operates entirely autonomously, scraping fresh news articles, generating multiple-choice and open-ended questions with LLM agents, validating them through a second model, and assigning reproducible dataset signatures that ensure transparent version control.\n\nThe authors evaluate OKBench on both data quality and benchmark utility. Using OKBench, the paper conducts large-scale experiments across multiple open-source LLM families (e.g., Gemma, LLaMA, Qwen, Phi) and retrieval strategies (BM25, DPR, ColBERT v2)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and timely problem: the challenge of evaluating LLMs on factual knowledge without contamination from training data. The authors convincingly argue that testing on newly emerging knowledge is one of the most effective ways to mitigate data leakage, providing a well-motivated and practically relevant setting. \n\nA key strength lies in the design of a fully autonomous benchmark generation pipeline, which makes OKBench highly scalable and easily reproducible. The inclusion of a data versioning protocol with unique dataset signatures is an especially thoughtful feature, ensuring transparent and repeatable evaluations across time. \n\nThe paper also demonstrates strong attention to data quality, supported by a human validation study confirming that the majority of automatically generated questions are clear and factually correct. Furthermore, the authors provide a detailed cost analysis, showing that daily benchmark generation is affordable, an important consideration for community adoption. \n\nFinally, the experimental section is comprehensive and insightful, covering a broad range of models and retrieval-augmented generation methods, and yielding meaningful findings on knowledge freshness and retrieval effectiveness."}, "weaknesses": {"value": "OKBench's novelty claim as “the first fully automated factual QA benchmark” is somewhat overstated. Previous works such as TemporalWiki[1], WikiFactDiff[2], and especially WikiBigEdit[3] have already introduced fully automated benchmark generation pipelines for evaluating factual and temporal knowledge in LLMs. Although OKBench distinguishes itself by focusing on the news domain rather than structured knowledge graph updates, these earlier benchmarks should be explicitly acknowledged and included in Table 2, and the corresponding claim of being the first fully automated benchmark (end of Section 2) should be softened accordingly.\n\nSecond, the empirical results in Section 5.1 raise concerns about data contamination, which directly undermines the benchmark’s stated motivation. If the benchmark genuinely captures unseen, post-cutoff knowledge, models should not substantially exceed the 25 % random baseline in the no-context setting. However, the reported results show significantly higher accuracies, suggesting that a nontrivial portion of the data overlaps with the models’ pretraining corpora or with frequently reported background facts. A more thorough contamination analysis or explicit discussion of this limitation would strengthen the paper’s credibility.\n\nThird, the framework’s dependence on the underlying LLM used for question generation and validation is not explored experimentally. Since the agentic pipeline relies heavily on a single base model (GPT-4.1-2025-04-14), an ablation study varying the generation LLM would clarify the benchmark's robustness and reproducibility across different base models. Similarly, while the qualitative validation study is informative, a quantitative summary of the filtering rate (how many generated questions are discarded during validation) would help assess the pipeline’s efficiency and the actual yield of high-quality questions.\n\nFinally, there are a few presentation issues that could improve readability: Figure 2 is introduced early but only discussed in Section 5.2, making its placement suboptimal. Figure 4 would benefit from adopting the same layout as Figure 3 (four subplots in a single row) with a less prominent color for the no-context baseline (this is more of a suggestion than an actual weakness). \n\n[1] Jang et al. (2022): TemporalWiki: A lifelong benchmark for training and evaluating Ever-Evolving language models.\n[2] Khodja et al. (2024): Wikifactdiff: A large, realistic, and temporally adaptable dataset for atomic factual knowledge update in causal language models.\n[3] Thede et al. (2024): WikiBigEdit: Understanding the Limits of Lifelong Knowledge Editing in LLMs"}, "questions": {"value": "1) Novelty and Relation to Prior Work: How does OKBench fundamentally differ from WikiBigEdit, which also features a fully automated factual QA generation pipeline? Could the authors clarify whether they view OKBench as complementary to or extending this line of work, and why it was omitted from Table 2 and the related work section?\n2) Data Contamination Analysis: Given that models achieve substantially higher than random accuracy in the no-context setting, how do the authors explain this performance?\n3) Pipeline Robustness: Since the benchmark generation heavily relies on a specific base LLM (GPT-4.1-2025-04-14), how stable is the pipeline when using different generation or validation models? \n4) Filtering Statistics: Can the authors quantify how many of the initially generated questions are filtered out during the validation stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7BET5mlLHh", "forum": "uv9olmscIS", "replyto": "uv9olmscIS", "signatures": ["ICLR.cc/2026/Conference/Submission20246/Reviewer_wKmq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20246/Reviewer_wKmq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589645519, "cdate": 1761589645519, "tmdate": 1762933735677, "mdate": 1762933735677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes OKBench as a fully automated framework for generating dynamic benchmarks.  It can be automatically generated and used for the evaluation of retrieval-augmented methods. On experiments with multiple open-source and proprietary LLMs, it finds multiple observation about the model behaviors toward novel information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This work proposes an approach to construct an on-demand knowledge base from the Internet text source. It can be used for evaluation of RAG systems. \n\n2. This work experiments LLMs’ behaviors on the dynamic corpus and discovers multiple intriguing observations."}, "weaknesses": {"value": "1. The dataset novelty is limited. \n- As summarized in Table 1, the novelty of the dataset is not clear compared to the existing ones. \n- One key novel feature is “any time” in the update frequency, but making other datasets anytime too is not challenging. \n- As such, no prominent novel features can be found for this dataset.\n\n2. The novelty of the proposed data collection pipeline is limited. \n- As illustrated in Fig. 1 and section 3.1, there is no novelty in the benchmark construction pipeline, as it is quite standard and straightforward. \n\n3. The usability of the dataset is limited. \n- As shown in Fig. 3, once the oracle document is given, most models attain very high accuracy (near 100%). \n- It may implicate that once the retrieval is successful, the QA part could be very easy to solve. Then, this benchmark may evaluate mostly the retriever’s performance rather than LLMs’ QA capability. \n- As reported in Fig.4, four different LLMs show almost similar performance. It could be a piece of evidence that the choice of an LLM does not matter to solve this benchmark. Only retriever selection matters. \n\n4. Only three basic retrievers are tested."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The data sources (news articles) summarized in Table 2 are under copyright. This work mentions fair-use but does not obtain permission from them. The proposed dataset construction pipeline may be under a risk of copyright infringement."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "688wnbvLp9", "forum": "uv9olmscIS", "replyto": "uv9olmscIS", "signatures": ["ICLR.cc/2026/Conference/Submission20246/Reviewer_Ub36"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20246/Reviewer_Ub36"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809793044, "cdate": 1761809793044, "tmdate": 1762933735272, "mdate": 1762933735272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OKBench, a dynamic and knowledge-intensive benchmark designed to automatically evaluate large language models on their ability to handle factual, up-to-date information."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This paper proposes OKBench, a dynamic, knowledge-intensive benchmark that is automatically updated.\n- The authors test models of various sizes and compare their performance on MMLU Pro and OKBench, analyzing differences in memorization and adaptability to newly introduced information."}, "weaknesses": {"value": "**Limited novelty:**\n- Although the paper claims that OKBench is “the first fully automated benchmark for evaluating factual question answering ability” (L146), similar dynamic benchmarks have already been proposed in prior work [1, 2, 3].\n- These existing works also describe pipelines for continual updates, whereas this paper presents only the initial construction pipeline, without demonstrating an actual update process. The lack of comparison to prior dynamic benchmarks may mislead readers about the paper’s contribution.\n- The paper asserts that it introduces an agentic framework for benchmark construction (L153). However, the pipeline illustrated in Figure 2 lacks a clear agentic component; each step appears to be manually designed, without autonomous decision-making or agent-based iteration.\n\n\n**Benchmark difficulty and saturation:**\n- The results in Figure 4 show that BM25 Context performance (≈90–95 for Gemma) is nearly identical to Oracle performance (≈95). Even accounting for BM25 being a simple lexical retriever, this narrow gap suggests that the benchmark questions may be too easy.\n- Moreover, BM25 Context significantly outperforms DPR Context (90–95 vs. 75–80), indicating that word matching alone suffices to answer most questions.\n- This also implies that questions might have been generated from single factual sentences, often reusing phrases directly from the source text.\n Such design choices reduce the benchmark’s ability to evaluate deeper reasoning and may compromise its validity.\n\n[1] Ko et al., \"GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge?\", ACL 2024.    \n[2] Lin et al., \"DynaQuest: A Dynamic Question Answering Dataset Reflecting Real-World Knowledge Updates\", ACL 2025 Findings.    \n[3] Ouyang et al., \"HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation\", ArXiv 2025."}, "questions": {"value": "- The DPR retriever shows much lower performance than BM25 or ColBERTv2, and inconsistent results across the 1-, 5-, and 10-day corpus.\nHave the authors considered evaluating stronger retrievers, such as Qwen-3-Embedding or E5, which are known to perform better on factual retrieval tasks? Including such models could clarify whether the issue lies with the retriever’s capability or with the benchmark’s inherent design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xrQLQJMZTP", "forum": "uv9olmscIS", "replyto": "uv9olmscIS", "signatures": ["ICLR.cc/2026/Conference/Submission20246/Reviewer_osHV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20246/Reviewer_osHV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960584956, "cdate": 1761960584956, "tmdate": 1762933735012, "mdate": 1762933735012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}