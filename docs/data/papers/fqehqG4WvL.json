{"id": "fqehqG4WvL", "number": 23031, "cdate": 1758338423818, "mdate": 1759896835268, "content": {"title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "abstract": "Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents—such as robots and self-driving cars—typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding (SU). To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% and 56% average improvements on multi-choice QA and absolute distance estimation, respectively. Ego3D-VLM can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level SU in real-world, multi-view environments. Code is available in the supplementary materials.", "tldr": "", "keywords": ["Vision Language Model", "Spatial Reasoning", "Multiview Images"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/151dfcd5ef9bc2c15c5d4ea404dfac08411d2bee.pdf", "supplementary_material": "/attachment/f8561c148b392ab2329706024a8d7b3ec1eca5d4.zip"}, "replies": [{"content": {"summary": {"value": "the paper introduces Ego3D-Bench, a new benchmark with over 8,600 human-annotated QA pairs built from outdoor datasets like NuScenes, Waymo, and Argoverse 1. The benchmark tests five categories of spatial tasks from both ego-centric and object-centric perspectives, including absolute/relative distance, localization, motion reasoning, and travel time. An evaluation of 16 SOTA VLMs reveals a significant performance gap compared to human-level scores. To bridge this gap, the paper proposes Ego3D-VLM, a plug-and-play post-training framework. This method uses external REC and metric depth estimation models to identify objects and their 3D coordinates , transforms them into a global reference frame , and generates a \"textual cognitive map\" that is fed to the VLM as additional context along with the images. The authors report that this framework improves average multi-choice QA accuracy by 12% and absolute distance estimation by 56%."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes Ego3D-Bench, which is well-constructed with over 8,600 QA pairs. It's built on real-world datasets. Critically, it also involves human annotators for filtering, captioning, and final review, ensuring high-quality, non-trivial questions.\n\n2. The evaluation is thorough, testing 16 SOTA VLMs with important ablations.\n\n3. The paper proposes Ego3D-VLM, a plug-in framework. This approach generates a textual cognitive map, is presented as an efficient alternative to computationally expensive methods like full point-cloud reconstruction. Even if it did not improve the spatially reasoning ability of the base VLM.\n\n4. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. A minor problem is that the proposed Ego3D-VLM is repeatedly called a \"post-training framework\", which is misleading. Post-training usually implies fine-tuning or adapting model weights. The Ego3D-VLM framework is a purely inference-time pipeline that prepends the cognitive map to guide a frozen, underlying VLM.\n\n2. The Ego3D-VLM is essentially transforming the 3D vision problem into a textual reasoning problem. As the qualitative examples show in Fig. 9, 10, 11, the baseline VLM is visually analyzing distances while the Ego3D-VLM version is simply given the 3D coordinates in text and performs Euclidean distance calculations. So the improvement is not on the VLM's visual spatial reasoning ability, but more like a work-around, that reads more accurate answers from a pre-processed text map. \n\n3. The paper can be improved by providing more detailed information about the dataset. Like the distribution of types of objects involved (people, cars, or buildings). And some details on how many works human annotators need to be involved (what percentage of data needs to be filtered out?)\n\n4. For egocentric VQA questions, what if instead of giving each image separately as input, we concat them next to each other to form one larger picture as input? what will happen in this case? Will it be better than inputing several images at the same time?"}, "questions": {"value": "Please refer to the weakness.\n\nWill this dataset be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wge4hmcV8v", "forum": "fqehqG4WvL", "replyto": "fqehqG4WvL", "signatures": ["ICLR.cc/2026/Conference/Submission23031/Reviewer_QSvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23031/Reviewer_QSvq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774598707, "cdate": 1761774598707, "tmdate": 1762942483870, "mdate": 1762942483870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Ego3D-Bench, a novel benchmark for evaluating the spatial reasoning capabilities of VLMs using ego-centric, multi-view outdoor data. It evaluates 16 SOTA VLMs on this new standard. Furthermore, the paper proposes Ego3D-VLM, a post-training framework that employs a cognitive map derived from 3D coordinates to enhance 3D spatial reasoning significantly. The experimental results show that this framework achieves clear and substantial improvements in performance on multi-choice QA and absolute distance estimation tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The Ego3D-Bench is highly valuable as its focus on outdoor, ego-centric data directly addresses and fills a critical gap in the current landscape of VLM spatial understanding benchmarks.\n2. The proposed post-training framework, Ego3D-VLM, is shown to be effective, adaptable across various VLM architectures, and demonstrably boosts the spatial reasoning abilities of several SOTA models.\n3. The paper provides a comprehensive and convincing experimental analysis of Ego3D-VLM, covering different VLM families, comparisons against other benchmarks, and inference time analysis in both the main body and the appendix, making the claims credible."}, "weaknesses": {"value": "1. The paper lacks sufficient transparency regarding the human effort involved in the benchmark construction. Specifically, it should disclose the origin of the hired annotators, the total number of human hours dedicated to the Ego3D-Bench creation pipeline, and the compensation structure used for the annotators.\n2. The Ego3D-VLM explicitly reconstructs 3D points based on estimated depth, meaning the robustness of the entire pipeline is highly dependent on the accuracy of an external, potentially unreliable, outdoor metric depth estimator. The overall pipeline involves a seemingly over-complex process (multi-view images → 3D reconstruction → 2D cognitive map). The intuitive justification for mapping 3D information *back* to a 2D text representation seems questionable without further theoretical justification."}, "questions": {"value": "1. In the Ego3D-Bench construction pipeline, annotators write descriptive captions for unique objects. Do these captions follow specific structural or linguistic templates, and how is the objectivity, reliability, and unbiased nature of these captions ensured?\n2. Given the complex, multi-step process (multi-view images → 3D coordinates → 2D textual cognitive map), can the authors provide an intuitive explanation for why the Ego3D-VLM framework proves to be effective? What is the core cognitive insight this result provides regarding how VLMs process spatial information?\n3. The results in Table 1 show that VLM performance on Ego-Centric (Ego.) vs. Object-Centric (Obj.) Absolute Distance, Motion, and Relative Distance tasks is not consistently worse for Ego-Centric problems. Does this result contradict the expected challenge of Ego-centric problems, and if not, how should this be interpreted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RdChFA83Pg", "forum": "fqehqG4WvL", "replyto": "fqehqG4WvL", "signatures": ["ICLR.cc/2026/Conference/Submission23031/Reviewer_ecVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23031/Reviewer_ecVi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890028703, "cdate": 1761890028703, "tmdate": 1762942483423, "mdate": 1762942483423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Ego3D-Bench, a new benchmark designed to evaluate the 3D spatial reasoning capabilities of VLMs using ego-centric, multi-view images from outdoor driving scenes. Comprising over 8,600 quality-controlled question-answer pairs, the benchmark reveals a significant performance gap between humans and state-of-the-art VLMs. To address this gap, the authors propose Ego3D-VLM, a training-free framework that enhances a given VLM's spatial understanding by generating a textual cognitive map. This map integrates 3D coordinate information of referred objects from multiple camera views and is provided as additional context to the VLM. Experimental results demonstrate that Ego3D-VLM consistently improves the performance of various VLMs on the proposed benchmark. The main contributions are the Ego3D-Bench benchmark and the plug-and-play Ego3D-VLM enhancement method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The focus on ego-centric multi-view fusion is well-aligned with the needs of embodied AI and vehicular applications. The significant human-model performance gap observed on the benchmark provides a valuable measuring stick for future research.\n\n2. The method of constructing a textual/JSON cognitive map at inference time using tool-based models is straightforward and effective. The comparative analysis of different cognitive map formats also offers useful insights.\n\n3. The paper is well-structured, and the figures and examples are clear and illustrative."}, "weaknesses": {"value": "1. Line 77: The claim that \"Ego3D-Bench is the first benchmark to evaluate spatial reasoning of VLMs given ego-centric multi-view inputs\" overstates its novelty. The related work section itself acknowledges existing multi-view benchmarks (e.g., All-Angle Bench, VSI-Bench), differing primarily in their setup. The contribution would be more accurately framed as a valuable extension within the specific ego-centric setting. The authors should more clearly articulate the unique challenges and benefits inherent to the ego-centric paradigm.\n\n2. Line 86: The claim that point-cloud/BEV methods \"significantly increase inference time—often by a factor of ten\" is not sufficiently supported. The cited work (Zhu et al., 2024) primarily deals with indoor scenes. In outdoor autonomous driving contexts, point clouds are sparser, and numerous real-time methods exist. The authors provide no direct, comparable efficiency comparison with such methods. Furthermore, Table 11 shows that Ego3D-VLM itself introduces non-negligible latency.\n\n3. Line 89: Describing Ego3D-VLM as a \"post-training method\" is a misnomer. The approach is fundamentally an inference-time augmentation or a form of prompt engineering.\n\n4. Line 266: The global scaling method based on common-sense object heights (e.g., person ≈ 1.7m) is highly susceptible to errors in dynamic traffic scenes due to pose, occlusion, and detection inaccuracies. The support for its reliability is limited (a single RMSE improvement in ablations), lacking analysis of error propagation or failure cases.\n\n5. Table 8: While the results indicate textual/JSON cognitive maps outperform visual ones, the authors provide no analysis for why this is the case. An investigation into the reasons behind this performance difference (e.g., alignment with pre-training, information density) is missing.\n\n6. The core concept of a \"cognitive map\" as a structured representation to aid spatial reasoning is not novel and bears similarity to ideas in prior works like VSI-Bench. The distinction and specific advancements of the proposed cognitive map should be more clearly delineated."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yLqFvF6R4v", "forum": "fqehqG4WvL", "replyto": "fqehqG4WvL", "signatures": ["ICLR.cc/2026/Conference/Submission23031/Reviewer_f2RE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23031/Reviewer_f2RE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983041113, "cdate": 1761983041113, "tmdate": 1762942483149, "mdate": 1762942483149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In contrast to prior passive perception in static indoor scenes, this paper presents Ego3D-Bench, a benchmark for evaluating 3D spatial reasoning in ego-centric multi-view outdoor scenes, which comprises 8,600+ QA pairs from nuScenes, Waymo, and Argoverse. The authors also propose Ego3D-VLM, a framework that enhances VLMs via a textual cognitive map. Experiments on 16 SOTA VLMs (e.g., GPT-4o, InternVL3) show a large performance gap between humans and VLMs; and Ego3D-VLM narrows this gap, achieving 12% higher accuracy on multi-choice QA and 56% lower RMSE on absolute distance estimation, with adaptability to diverse multi-view scenarios. Moreover, the authors conduct detailed ablation studies regarding various model classes and model components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Unlike prior benchmarks (single-image/indoor video), this paper addresses a critical real-world gap by proposing Ego3D-Bench, which targets ego-centric multi-view outdoor scenarios, aligning with embodied AI needs such as robotics and autonomous driving.\n- Ego3D-VLM is efficient and flexible. The textual cognitive map can effectively represent spatial information to aid the 2D visual input. And the experimental results turn out to be very effective.\n- Extensive experiments and rigorous evaluations. This paper tests 16 kinds of VLMs (including generalist, 3D-specific, and depth+REC) and delivers ablation studies regarding various model components."}, "weaknesses": {"value": "- The reliance on external perception modules (depth + REC) may complicate the inference pipeline. And such a design cannot train the inherent spatial perception capability of VLM in an end-to-end manner. It is more like providing extra input information by external modules though it does help.\n- Relatively weak transfer results to other scenes (All-angles and VSI-Bench). This raises my concerns on the generalizability of the proposed method. How could you address this issue, by polishing the methodology, or integrating more data?\n- The claim of \"post-training\" framework can be misleading. Post-training often relates to RL or RFT. However, I found no RL-related objective. This paper appears to only involve creating new data and SFT with that data."}, "questions": {"value": "- What is the usage of scaled 3D points (Line 267)? Seems like it is used by default, rather than the global points, but the equation (4) is not."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l9edw9WP0j", "forum": "fqehqG4WvL", "replyto": "fqehqG4WvL", "signatures": ["ICLR.cc/2026/Conference/Submission23031/Reviewer_5Jz1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23031/Reviewer_5Jz1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998002229, "cdate": 1761998002229, "tmdate": 1762942482966, "mdate": 1762942482966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}