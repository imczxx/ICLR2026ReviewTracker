{"id": "yzHwT3gfaE", "number": 15805, "cdate": 1758255474594, "mdate": 1759897280916, "content": {"title": "Toward Conservative Planning from Preferences in Offline Reinforcement Learning", "abstract": "We study offline reinforcement learning (RL) with trajectory preferences, where the RL agent does not receive explicit rewards at each step but instead receives human-provided preferences over pairs of trajectories. Despite growing interest in preference-based reinforcement learning (PbRL), contemporary works cannot robustly learn policies in offline settings with poor data coverage and often lack algorithmic tractability. We propose a novel **M**odel-based **C**onservative **P**lanning (MCP) algorithm for offline PbRL, which leverages a general function class and uses a tractable conservative learning framework to improve the policy upon an arbitrary reference policy. We prove that, MCP can compete with the best policy within data coverage when the reference policy is supported by the data. To the best of our knowledge, MCP is the first provably sample-efficient and computationally tractable offline PbRL algorithm under partial data coverage, without requiring known transition dynamics. We further demonstrate that, with certain structural properties in PbRL dynamics, our algorithm can effectively exploit these structures to relax the partial data coverage requirement and improve regret guarantees. We evaluate MCP on a comprehensive suite of human-in-the-loop benchmarks in Meta-World. Experimental results show that our algorithm achieves competitive performance compared to state-of-the-art offline PbRL algorithms.", "tldr": "We propose a novel model-based conservative planning algorithm with both sample and computational efficiency guarantees.", "keywords": ["reinforcement learning", "sample complexity", "model-based planning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bca8c52d8a28ac0f044075b0a814df4a2449d2b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors study offline reinforcement learning (RL) with trajectory preferences, where the RL agent does not receive explicit rewards at each step but instead receives human-provided preferences over pairs of trajectories. They propose a novel Model-based Conservative Planning (MCP) algorithm for offline PbRL, which leverages a general function class and uses a tractable conservative learning framework to improve the policy upon an arbitrary reference policy. They prove that, MCP can compete with the best policy within data coverage when the reference policy is supported by the data. Lastly, the authors conduct some empirical evaluations of MCP."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of conservative planning in RLHF is an important and interesting problem.\n2. The technical part of this paper is solid, the proof looks correct to me.\n3. There are simulation results supporting the theoretical results."}, "weaknesses": {"value": "1. My main concern is about the algorithm. The hyperparameter $\\lambda_1$ and $\\lambda_2$ depend on the concentrability coefficients, which further depends on the real reward or transition kernel. Would you please explain how to calculate (or estimate) such values efficiently?\n\n2. Furthermore, $\\lambda_2$ also depends on $M_P$, which is a max over all the intermediate steps during the training. How could the algorithm know this value before the algorithm starts?\n\n3. The idea behind line 5 and 6 of alg.1 is interesting. Would you please analyze the computational complexity of solving the optimization problem in line 5?"}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hF6GxLNMDl", "forum": "yzHwT3gfaE", "replyto": "yzHwT3gfaE", "signatures": ["ICLR.cc/2026/Conference/Submission15805/Reviewer_Ec41"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15805/Reviewer_Ec41"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796158863, "cdate": 1761796158863, "tmdate": 1762926036645, "mdate": 1762926036645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Model-based Conservative Planning (MCP), a framework for offline preference-based reinforcement learning (PbRL) that integrates model learning with conservative optimization to ensure reliable performance under partial data coverage. The authors provide generalization guarantees under general function approximation and extend the analysis to structured settings such as kernelized nonlinear regulators. Empirically, the paper evaluates MCP on the Meta-World medium-replay benchmark, showing improved success rates over several preference-based baselines (PT, IPL, DPPO, APPO)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and logically structured.\n\n- The paper provides a solid theoretical framework for offline preference-based reinforcement learning under partial coverage. It establishes generalization guarantees for model-based conservative planning with general function approximation, which, to the best of my knowledge, has not been analyzed in prior PbRL literature."}, "weaknesses": {"value": "The experimental evaluation exhibits certain limitations.\n\n- The experiments are conducted solely on the Meta-World medium-replay benchmark, which primarily contains deterministic, low-dimensional robotic tasks. This restricted evaluation makes it difficult to assess how well the proposed conservative planning approach generalizes to more diverse or high-dimensional offline preference-based RL settings. Including broader datasets such as D4RL or human-feedback benchmarks (e.g., Atari or MuJoCo preference datasets) would strengthen the empirical evidence.\n\n- The paper lacks ablations isolating the contribution of each design component, such as the relative performance regularization, model-based planning, and the choice of regularization weights.  Without these analyses, it is unclear how much improvement stems from the conservative objective itself versus other implementation factors. Including sensitivity studies or component removals would better validate the claimed effectiveness."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RkPuy6ARaS", "forum": "yzHwT3gfaE", "replyto": "yzHwT3gfaE", "signatures": ["ICLR.cc/2026/Conference/Submission15805/Reviewer_GUzu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15805/Reviewer_GUzu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893976527, "cdate": 1761893976527, "tmdate": 1762926036323, "mdate": 1762926036323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes MCP, a novel model-based conservative planning algorithm for offline preference-based RL that is both sample-efficient and computationally tractable. Under partial coverage, MCP does not require additional structural assumptions and provides a PAC guarantee with a tractable implementation in model-based RL. To the best of the reviewerâ€™s knowledge, this is the first work in model-based RL that addresses the limitations of PbRL. In experiments, MCP demonstrates high performance compared to other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors derive PAC bounds for the three variants of MCP (original, factored, and KNR). The paper is clearly structured, making it easy for readers to follow the overall flow and reasoning.\n\n2. This work is a natural extension to the model-based setting and effectively addresses the limitations of previous approaches."}, "weaknesses": {"value": "1. In Figure 1-(a), MCP is compared only with MR and Oracle. It would strengthen the validity of the theoretical claims if the experiments also included a comparison with APPO, as was done in the theoretical analysis regarding sample efficiency (Line 310).\n\n2. The training curves are presented only for MCP. It would be beneficial to include those of other baselines as well, so that the results highlight not only the final performance but also the faster convergence of MCP, demonstrating the advantage of the model-based RL approach."}, "questions": {"value": "1. The authors mention that MCP provides an implicit way of encoding conservatism, mainly through the minimax objective function. However, it is somewhat difficult to intuitively understand how this mechanism works in practice. Could the authors provide a simple or toy example to illustrate how the minimax structure implicitly enforces conservatism?\n\n2. In Table 5, $\\lambda_{1}$, $\\lambda_{2}$, and $\\lambda_{3}$ are important parameters for MCP. Therefore, it would be helpful to report their exact values for each dataset. Are these parameters highly sensitive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7P04r3PteC", "forum": "yzHwT3gfaE", "replyto": "yzHwT3gfaE", "signatures": ["ICLR.cc/2026/Conference/Submission15805/Reviewer_6XMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15805/Reviewer_6XMk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986465543, "cdate": 1761986465543, "tmdate": 1762926035903, "mdate": 1762926035903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles incomplete data coverage and high computational cost in offline preference-based RL by introducing Model-based Conservative Planning (MCP). MCP uses a model-based planning framework that implicitly enforces conservatism, enabling sample-efficient and computationally tractable policy learning with general function approximation. Theoretically, MCP competes with the best policy supported by the dataset under partial coverage, with regret bounds improved via dynamic structures (e.g., kernelized nonlinear regulators, factorized models). Empirically, across 8 Meta-World tasks, MCP outperforms baselines like APPO and IPL in average rank and shows strong robustness in low-data regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. simultaneously achieve sample efficiency and computational tractability in offline PbRL with unknown dynamics and partial coverage. It encodes conservatism via relative performance instead of explicit confidence sets or extra value modeling, simplifying the framework and overcoming the computational bottlenecks of methods like FREEHAND and Sim-OPRL.\n\n2. Supports general function approximation (linear models, neural networks, etc.), and derives adaptive concentration coefficients and regret bounds for structured dynamics (e.g., kernelized nonlinear regulators and factorized models), mitigating the curse of dimensionality and broadening theoretical applicability."}, "weaknesses": {"value": "1. Compared with the APPO algorithm, this paper mainly introduces a model with conservative terms, but the experimental results do not show much improvement, while introducing additional model computational overhead.\n\n2. What is the robustness of MCP when there is noise in preference labels? Quantitative analysis of the impact of label noise on performance can be conducted to prove that the design of MCP can have better generalization.\n\n3. Each time a policy is updated, a new MDP model should be learned to challenge it, which is very wasteful in terms of compuational cost."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vwkWdTXjML", "forum": "yzHwT3gfaE", "replyto": "yzHwT3gfaE", "signatures": ["ICLR.cc/2026/Conference/Submission15805/Reviewer_wXXH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15805/Reviewer_wXXH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988905734, "cdate": 1761988905734, "tmdate": 1762926035396, "mdate": 1762926035396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}