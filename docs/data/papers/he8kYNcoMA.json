{"id": "he8kYNcoMA", "number": 24671, "cdate": 1758359146098, "mdate": 1759896755627, "content": {"title": "ST-SimDiff: Balancing Spatiotemporal Similarity and Difference for Efficient Video Understanding with MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) face significant computational overhead when processing long videos due to the massive number of visual tokens required. To improve efficiency, existing methods primarily reduce redundancy by pruning or merging tokens based on importance or similarity. However, these approaches largely overlook a critical dimension of video content, i.e., changes and turning points, and they lack a collaborative model for spatio-temporal relationships.\nTo address this, we propose a new perspective: similarity is for identifying redundancy, while difference is for capturing key events. Based on this, we designed a training-free framework named ST-SimDiff. We first construct a spatio-temporal graph from the visual tokens to uniformly model their complex associations. Subsequently, we employ a parallel dual-selection strategy: 1) similarity-based selection uses community detection to retain representative tokens, compressing static information; 2) temporal difference-based selection precisely locates content-changing points to preserve tokens that capture key dynamic shifts. This allows it to preserve both static and dynamic content with a minimal number of tokens. Extensive experiments show our method significantly outperforms state-of-the-art approaches while substantially reducing computational costs.\nOur code is available in [https://anonymous.4open.science/r/ST-SimDiff-7225](https://anonymous.4open.science/r/ST-SimDiff-7225).", "tldr": "We propose a training-free method that builds a spatio-temporal graph to efficiently select video tokens by incorporating similarity for redundancy reduction and difference for key event detection.", "keywords": ["Video Understanding", "Visual Token Reduction", "Multimodal Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f03bd31fe6be94f7bb0c7595ca9da1ac8963afa3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the problem of extremely high computational costs caused by the massive number of visual tokens when multimodal LLMs process long videos. It makes three main contributions: 1) It is the first to focus on inter-frame differences in videos, emphasizing that change is key to video understanding. 2) It proposes a method for constructing spatiotemporal graphs that uniformly models spatial similarity and temporal continuity. 3) It introduces a new token selection strategy: for token clusters with high similarity in the graph, only a few representative tokens are retained, while tokens with extremely high dissimilarity on temporal edges are preserved."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper proposes ST-SimDiff, a training-free framework for video token compression in multimodal large language models. The core innovation lies in simultaneously leveraging similarity (to identify redundancy) and dissimilarity (to capture key events) for token selection. The method constructs a spatiotemporal graph and achieves dual-path parallel selection by combining community detection and temporal difference detection. Experiments demonstrate state-of-the-art performance on VideoMME, LongVideoBench, and EgoSchema, while significantly reducing computational costs. The work exhibits strong novelty, effectively achieves its research objectives, and thoroughly validates the method's effectiveness across multiple models and datasets."}, "weaknesses": {"value": "1.Does the compressed video still retain the audio modality? If yes, how is the alignment between video and audio achieved? If not, does the absence of audio bring negative impacts? If so, how can the negative effects caused by the missing audio be mitigated?\n2.For the same input, this framework seems to produce only one type of output regardless of how the text query instruction varies (rather than automatically adapting based on instruction changes). For different video analysis objectives, is this fixed, unified video pruning framework insufficiently flexible?\n3.Constructing the spatiotemporal graph requires traversing all visual tokens. For videos with extremely high resolution or extremely long duration, is this framework still applicable? In other words, what is the estimated upper limit of visual tokens that this framework can handle?\n4.Considering only changes between adjacent frames may lead to the model being insufficiently sensitive to slowly changing scenes."}, "questions": {"value": "1. Does the community detection algorithm actually use Louvain or connected components? If it's the latter, why not use the superior Louvain algorithm?\n2. In Table 1, the ST-SimDiff results at Token Retain Ratio (r=50%) even exceed the Token Retain Ratio (Full Performance) results (63.3). Why does this phenomenon occur where the compressed video performs better than the original video? The paper should provide a discussion on this.\n3. Could you provide some concrete examples showing which tokens are selected by SRTS and which are selected by DETS?\n4. When selecting difference frames, the video only considers changes between a few adjacent frames, which may lead to the model being insufficiently sensitive to slowly changing scenes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yAxP96Cg4N", "forum": "he8kYNcoMA", "replyto": "he8kYNcoMA", "signatures": ["ICLR.cc/2026/Conference/Submission24671/Reviewer_34qR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24671/Reviewer_34qR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794175314, "cdate": 1761794175314, "tmdate": 1762943158670, "mdate": 1762943158670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the computational overhead MLLMs face when processing long videos. The authors propose ST-SimDiff, a training-free token reduction framework. The core idea is that video compression must balance two aspects: using similarity to identify and compress redundant static content, and using difference to preserve key dynamic events or turning points. The method constructs a spatio-temporal graph to model token relationships. It then uses community detection for similarity and temporal difference thresholding for key events to generate a compact, information-rich token subset. Experiments show SOTA performance on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and clearly articulates a significant and practical problem. The core concept of striking a balance between spatiotemporal similarity and temporal difference constitutes a valuable contribution to this field.\n2. The proposed framework is training-free, making it highly practical, and easily applicable as a plug-and-play module. The use of a spatio-temporal graph to uniformly model complex token relationships is good way to implement the core idea."}, "weaknesses": {"value": "1. Discrepancy in baseline performance. The paper's validation on Qwen2.5-VL (Appendix, Table 4) suffers from a baseline inconsistency. The paper reports the \"Upper Bound (Full Performance)\" for Qwen2.5-VL as 62.9 on VideoMME and 59.2 on LongVideoBench. However, the official Qwen2.5-VL technical report states a score of 65.1 on VideoMME and 56.0 on LongVideoBench. The authors should address or explain this discrepancy.\n2. Lack of Image-based Validation. The paper focuses exclusively on video datasets. However, the core mechanism, particularly the Similarity-based Representative Token Selection (SRTS, is designed to handle spatio-temporal redundancy. The spatial component of this logic should be directly applicable to compressing redundant tokens in static images. Experiments on standard image-based VLM benchmarks could showcase the robustness of the similarity-based compression module.\n3. Qualitative Visualization. The paper's core hypothesis relies on the intuitive concepts of \"similarity\" (static content) and \"difference\" (key events). While the quantitative results are strong, the paper provides no qualitative visualizations. It would be highly beneficial to include figures that show, for specific video examples, which tokens are selected by the SRTS module versus the DETS module. This would provide direct and intuitive evidence that the framework is truly capturing static backgrounds and dynamic turning points as intended."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3WsMIJacnE", "forum": "he8kYNcoMA", "replyto": "he8kYNcoMA", "signatures": ["ICLR.cc/2026/Conference/Submission24671/Reviewer_TViS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24671/Reviewer_TViS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825909875, "cdate": 1761825909875, "tmdate": 1762943158484, "mdate": 1762943158484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ST-SimDiff, a balanced framework that explicitly models the trade-off between spatial-temporal similarity and difference in video representations. The method aims to improve video understanding by aligning similar semantics while preserving meaningful diversity across frames. The authors introduce novel similarity–difference guided modules that can be integrated into existing video backbones with minimal modifications. Extensive experiments on standard benchmarks demonstrate consistent improvements over strong baselines, showing superior performance across multiple tasks with manageable computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This well-executed, clearly written paper addresses the critical issue of balancing temporal consistency and diversity in video representation learning. The ST-SimDiff framework is elegant, conceptually sound, and easily integrates into existing architectures, delivering a significant performance boost without the need for additional supervision or costly retraining.\nThe paper effectively highlights how previous methods either overemphasize similarity or difference, while ST-SimDiff strikes a balanced, principled approach. The motivation is compelling, the technical formulation is clear, and the presentation flows smoothly.\nThe design of interpretable components effectively clarifies how spatial-temporal cues interact. The experimental validation is comprehensive, covering diverse datasets, multiple backbones, and various evaluation metrics, with ablations that isolate the contribution of each component. Results are strong and consistent."}, "weaknesses": {"value": "The paper lacks crucial qualitative visualizations. It does not show failure cases or ambiguous scenarios, making it difficult to understand the mechanism's practical limitations or how it behaves when it produces an incorrect interpretation.\nThe framework's performance boundaries are not explored. The paper fails to investigate video types or specific tasks where the proposed balancing paradigm might be suboptimal or ill-equipped.\nThe paper does not address the critical misalignment problem that arises from sequence pruning. While pruning/dropping is a common efficiency method, it creates a gap between the dense data the model was trained on and the sparse data it receives at inference, for which the model has no explicit handling mechanism.\nThe paper lacks a method for error attribution, making it difficult to determine whether a specific failure originates from the similarity module, the difference module, or their interaction."}, "questions": {"value": "Can the authors provide qualitative visualization results, especially highlighting where the similarity–difference mechanism fails or produces ambiguous interpretations?\nWhere are the performance boundaries of this framework? Are there specific video tasks/types that ST-SimDiff is ill-equipped to handle?\nHow does the framework solve the input misalignment problem after pruning?\nIf the model makes an incorrect judgment on a video, how can you attribute the failure to the similarity module versus the difference module?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "phVmksTTrH", "forum": "he8kYNcoMA", "replyto": "he8kYNcoMA", "signatures": ["ICLR.cc/2026/Conference/Submission24671/Reviewer_f9gs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24671/Reviewer_f9gs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834811761, "cdate": 1761834811761, "tmdate": 1762943158255, "mdate": 1762943158255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel and insightful framework for video token compression, effectively balancing similarity-based redundancy reduction with difference-based key event capture. This training-free, dual-path approach is technically sound and demonstrates strong, generalizable performance across multiple models and significant computational savings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper makes a significant contribution by addressing the need for efficient video understanding through the dual focus of redundancy compression (via similarity) and key event capture (via difference). This approach moves beyond prior work that primarily focused on similarity or importance-based pruning alone.\\\n2.The proposed ST-SimDiff framework is technically sound, employing a spatio-temporal graph to model token relationships uniformly. The dual-path SRTS and DETS effectively implement the core motivation, demonstrating significant and consistent performance improvements across a range of state-of-the-art, model-agnostic baselines, including LLaVA-Video, NVILA-Video, and Qwen2.5-VL.\\\n3.As a training-free framework, ST-SimDiff introduces minimal overhead and operates with substantially improved efficiency. Its computational complexity is analyzed as O(Nd), much more efficient than the $O(N^2d)$ self-attention it aims to reduce. Empirical results show significant reductions in inference time and peak GPU memory usage."}, "weaknesses": {"value": "1.The final pruning step, where the initial candidate set exceeds the target budget, is underexplained in Section 4.5. It is unclear which layers are selected, how the scores are aggregated, and the impact of this step on performance.\\\n2.The paper lacks qualitative examples, such as visualizations showing which tokens are selected. This makes it challenging to build intuition about the method’s behavior and understand its potential failure modes.\\\n3.There is no comparative analysis or justification to explain why the connected components algorithm is chosen in SRTS. The impact of this choice is less investigated."}, "questions": {"value": "1.Which exact shallow layers are used in the final attention-based pruning step? What is the specific aggregation function for attention scores across heads and layers?\\\n2.Provide qualitative visualizations that highlights which tokens are preserved by the proposed method to help build intuition and identify failure modes.\\\n3.Why do you choose the connected components algorithm compared with other mainstream community detection methods, especially in terms of both understanding performance and processing efficiency?\\\n4.Could the framework's performance be further enhanced by making parts of it learnable? What are the potential benefits versus the cost?\\\n5.The SRTS and DETS paths are currently combined, seemingly uniformly. Is it possible to dynamically weight these two paths for better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K5zlDLx05w", "forum": "he8kYNcoMA", "replyto": "he8kYNcoMA", "signatures": ["ICLR.cc/2026/Conference/Submission24671/Reviewer_1bh7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24671/Reviewer_1bh7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839895329, "cdate": 1761839895329, "tmdate": 1762943158045, "mdate": 1762943158045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}