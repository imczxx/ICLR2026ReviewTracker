{"id": "hfNnQHkTtv", "number": 21659, "cdate": 1758320211506, "mdate": 1759896910173, "content": {"title": "Nudging the Boundaries of LLM Reasoning", "abstract": "Current online reinforcement learning (RL) algorithms like GRPO share a key limitation in LLM reasoning: they cannot learn from problems that are \"unsolvable\" to the model. In other words, they can only improve performance on problems where the model is capable of exploring the correct answer. If a problem is too difficult -- such that even hundreds of attempts never produce a correct solution -- the model cannot learn from it. Consequently, the model's \"upper limit\" remains unchanged after RL training, even though the likelihood of solving easier, solvable problems may increase. These hard, unsolvable samples -- though potentially rich in learning signal -- cannot contribute to training, as no rollouts yield rewards and thus no gradients are produced. To unlock learning from these hard samples, we propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM reasoning using self-generated hints, i.e., abstract cues that help reduce the problem difficulty for the model. Given a question and its gold answer, the model generates a Chain-of-Thought (CoT) and then produces a hint containing the core knowledge needed to solve the problem. During online RL training, we generate G rollouts from the base policy and use the pass rate to decide whether the hint should be injected. For hard samples with a 0% pass rate, we inject the offline-generated hint and regenerate a new batch of trajectories. This yields two benefits: (1) the hint boosts pass rates (from 0% to non-zero), thereby introducing training signals for previously unsolvable samples, and (2) the hints are self-generated (conditioned on the gold answer), avoiding distributional shift and do not rely on external models. Compared to standard GRPO, NuRL achieves consistent improvements across six diverse benchmarks and three models, while remaining complementary to test-time scaling. Notably, NuRL can raise the model's upper limit, whereas GRPO leaves pass@1024 unchanged from the base model. Furthermore, we present a systematic study of what makes an effective hint and when hints are most useful. Interestingly, the best hints are abstract and high-level -- as revealing gold answers actually hurt performance -- and are most beneficial when applied necessarily and after GRPO has converged.", "tldr": "", "keywords": ["Nudging LLM", "LLM Reasoning", "GRPO"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a23a3bdb80fd41119e91b31d39fbfab494721422.pdf", "supplementary_material": "/attachment/12906afdf28e4cc615dc9e51443934dd1c2252e2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces NuRL, a method to leverage off-policy hints (either self-generated or generated by teacher) that are (i) abstractions (ii) partial solutions (iii) explanations and (iv) ground truth answers. These are done for difficult problems when accuracy is zero.\n\nInterestingly, by injecting these hints during GRPO when rewards are all zero, the model manages to achieve superior performance during RL (do they show superior performance on specifically these hard problems? see my question in the Question section below).\n\nComparison is done against vanilla GRPO and there seems to be a 1-2% increase on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The problem setting is novel and interesting - getting GRPO to sample non-zero rewards on difficult problems\n- The method is simple and effective\n- very nice diagrams\n- paper well-written, easy to understand"}, "weaknesses": {"value": "- not enough difficult benchmarks, esp. for Qwen3-4B-Instruct which is already at 70% performance even before RL\n- I'd like to see some more qualitative rollout examples, mentioned in questions\n- more comparison against other methods"}, "questions": {"value": "1. Qwen3-4B seems to achieve already such a high accuracy on the benchmarks. I believe it would strengthen your paper if you can evaluate on more difficult benchmarks. For the rebuttal, I'd like to see results on ALL benchmarks listed in the final answer competitions in the standard Math Arena: https://matharena.ai/?comp=overall--final-answer_competitions. In addition, please add evaluations on OlymMath-EN: https://github.com/RUCAIBox/OlymMATH/tree/main/data. If these are still too easy for Qwen3-4B base (>50% accuracy), you can also look into evaluating on Omni-MATH.\n2. What's the difference between before hint and after hint in figure 6? Can I see another curve which evaluates the model's capability *without the hint*, after training on rollouts *conditioned on the hint*?\n3. Related to the above question, why do you think training on rollouts conditioned on the hint should improve rollouts without the hint? If the hint tokens are not upweighted, then I don't understand why they would be sampled at test time. And if they are not sampled, the rollouts shouldn't improve?\n4. Can you compare and contrast example rollouts on difficult problems (i) without the hint, (ii) given the hint, and (iii) after training on the hint-conditioned rollouts, without the hint?\n5. In figure 4, what is the performance of the models without any hints? Could you add a bar next to them showing that\n6. How do you compare against baselines where you SFT on the partial solutions? How about against other methods that learn directly on correct traces such as LUFFY (Gemini generated traces) and Critique-GRPO (self-revised correct races)?\n\nIf you can make this paper more sound and answer the questions above with solid results, I will raise my score and vouch for you."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vqFy4dmL1n", "forum": "hfNnQHkTtv", "replyto": "hfNnQHkTtv", "signatures": ["ICLR.cc/2026/Conference/Submission21659/Reviewer_1SjR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21659/Reviewer_1SjR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685562509, "cdate": 1761685562509, "tmdate": 1762941874544, "mdate": 1762941874544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NuRL, a reinforcement learning method designed to push the reasoning capability limits of LLMs by enabling learning from tasks that are currently unsolvable under the base policy. Built on GRPO, NuRL incorporates self-generated abstract hints that are injected only when all rollouts fail, turning zero-reward samples into informative training signals. Through experiments on six reasoning benchmarks (MATH 500, MATH Hard, AIME 2024, GPQA, MMLU-Pro, Date Understanding) and three 3–4B models (Llama, OctoThinker, Qwen), the authors demonstrate consistent improvements in pass@1 and pass@k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a timely and important limitation of reasoning-oriented RL methods.\n- The experimental evaluation is comprehensive, spanning diverse tasks and multiple model scales.\n- The analysis is thorough, including insightful studies on hint type, hint timing, pass@k, and solvable-problem curves.\n    - I really like Figure 4. It clearly demonstrates the relative effectiveness of different hint types.\n- The proposed two-stage selective-hint strategy is novel, well-motivated, and addresses the problem effectively."}, "weaknesses": {"value": "- The evaluation is limited to models up to 4B parameters. As model capacity increases, the proportion of unsolvable problems naturally decreases. It is unclear whether the method would still be effective when applied to larger, more capable models.\n- There is a clear train and test distribution shift where hints are provided during training but absent during evaluation.\n- The Self-Consistency method lacks sufficient detail. Although it is not the focus of the paper, additional implementation details would provide clearer context."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fuSTgMyivC", "forum": "hfNnQHkTtv", "replyto": "hfNnQHkTtv", "signatures": ["ICLR.cc/2026/Conference/Submission21659/Reviewer_RKzS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21659/Reviewer_RKzS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944445848, "cdate": 1761944445848, "tmdate": 1762941874095, "mdate": 1762941874095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NuRL (Nudging LLMs with Reinforcement Learning), a method designed to extend the reasoning boundaries of large language models (LLMs) by enabling learning from previously unsolvable problems. Standard online RL methods like GRPO fail on such problems because all rollouts receive zero rewards, preventing gradient updates. NuRL introduces self-generated hints to “nudge” the model and make these problems solvable. During training, hints are injected only when all rollouts for a problem fail, and only for some rollouts to avoid overfitting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem is important, and the proposed idea is interesting: knowledge outside of rl training can help the model further improve the performance.\n\n2. The evaluation shows that the proposed method achieves consistent performance improvement compared to GRPO.\n\n3.  Ablation studies carefully examine what types of hints help (abstract vs. explicit), when to apply them."}, "weaknesses": {"value": "1. One of my concerns is where the accuracy improvement comes from. As the authors mentioned: “In practice, such uninformative problems (both too hard and too easy) are often discarded to improve training efficiency”. However, besides discarding those examples, a common practice (e.g., DAPO), will resample and fill the batch (dynamic sampling). Simply discarding the example can hurt the training performance. That’s also why dynamic sampling can improve the model performance and stabilize training. A side effort of NuRL is to address this zero advantage issue, so the performance improvement may come from more stable optimization rather than hints. It would be great to compare NuRL and dynamic sampling to see how those hints can improve the performance.\n\n2. Besides hints from the base model, the paper also proposes to use another more powerful model (e.g., GPT-4o) to generate hints. Such an offline method has already been studied in [1]. It would be great to include it as a baseline for comparison.\n\n\n[1] Yan, Jianhao, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. \"Learning to reason under off-policy guidance.\" arXiv preprint arXiv:2504.14945 (2025)."}, "questions": {"value": "listed in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ONQy00a2cp", "forum": "hfNnQHkTtv", "replyto": "hfNnQHkTtv", "signatures": ["ICLR.cc/2026/Conference/Submission21659/Reviewer_GxNS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21659/Reviewer_GxNS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974812722, "cdate": 1761974812722, "tmdate": 1762941873729, "mdate": 1762941873729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission introduces NuRL, which augments typical LLM RL training by providing hints when all responses to a query are incorrect. Experiments consistently show that this method can address the problem of zero gradients for such questions, allowing the model to solve more problems. Ablation studies illustrate helpful properties of hints and support implementation choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**  The hinting implementation's details are ablated and investigated, clarifying how to provide helpful hints and providing potential explanations for why specific hinting schemes work. The method is evaluated on a diverse range of reasoning problems using models with different RL behavior profiles, which is uncommon in RL studies and adds rigor.\n\n**Quality** Experiments are well designed and rigorously test questions of interest. For example, the selection of models tested is well motivated (line 246). \n\n**Clarity** The paper is well written. Figures and tables clearly communicate the method and experimental results. Background work is appropriately referenced. \n\n**Significance** The method improves pass@1 and pass@1024 performance on key RL tasks, suggesting an improved ability to solve hard problems, which the community will be interested in. Ablation studies that characterize properties of good hints can guide future work in this area."}, "weaknesses": {"value": "There are some highly related works, namely Huang et al. (2025), which the authors cite. The submission is still novel, however, as it focuses on LLMs instead of MLLMs, self-generated hints instead of relying on an external model or dataset, and investigation of the properties of good hinting schemes."}, "questions": {"value": "I would be happy to improve my rating after some minor changes that reinforce the paper's existing strengths, which are its exploration of various hint implementations and careful experimental designs.\n\n- Line 136: For RL on LLMs with experience replay buffers, in addition to Lu et al. (2025), I would cite Bartoldson et al. (2025), “Trajectory Balance with Asynchrony”.\n\n- Line 205: How is the amount of information measured in order to support this statement about the ordering? \n\n- Line 313: Could you please explain this procedure more clearly? My understanding is that a model is trained with GRPO until performance plateaus, then it's split into two copies: one that NuRL training is applied to, and another that continued GRPO training is applied to. If this is correct, would it make sense to create another GRPO baseline that -- at the halfway point -- also switches to the 8 responses per query that NuRL uses?\n\n- Line 363: “complementary”\n\n- Figure 3: why exclude Qwen from this experiment?\n\n- Figure 6: it looks like NuRL performance would continue to improve with longer training. For example, if you train for 1000 steps, do you see a plateau with NuRL?\n\n- Figure 6: it would be interesting to augment this study to test if more direct hints improve the solvable fraction on train data but not test data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gLWYIwK6me", "forum": "hfNnQHkTtv", "replyto": "hfNnQHkTtv", "signatures": ["ICLR.cc/2026/Conference/Submission21659/Reviewer_1qwq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21659/Reviewer_1qwq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109963076, "cdate": 1762109963076, "tmdate": 1762941873455, "mdate": 1762941873455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}