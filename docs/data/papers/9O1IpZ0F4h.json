{"id": "9O1IpZ0F4h", "number": 8345, "cdate": 1758079346918, "mdate": 1759897790419, "content": {"title": "NFPO: Stabilized Policy Optimization of Normalizing Flow for Robotic Policy Learning", "abstract": "Deep Reinforcement Learning (DRL) has experienced significant advancements in recent years and has been widely used in many fields. In DRL-based robotic policy learning, however, current *de facto* policy parameterization is still multivariate Gaussian (with diagonal covariance matrix), which lacks the ability to model multi-modal distribution. In this work, we explore the adoption of a modern network architecture, i.e. Normalizing Flow (NF) as the policy parameterization for its ability of multi-modal modeling,  closed form of log probability and low computation and memory overhead. However, naively training NF in online Reinforcement Learning (RL) usually leads to training instability. We provide a detailed analysis for this phenomenon and successfully address it via simple but effective technique. With extensive experiments in multiple simulation environments, we show our method, NFPO could obtain robust and strong performance in widely used robotic learning tasks and successfully transfer into real-world robots.", "tldr": "", "keywords": ["Reinforcement Learning", "Policy Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f70c5fb8987b9433fd11184a751e90006a8fd27.pdf", "supplementary_material": "/attachment/7eac9b9bc1615d5bc1e1893d80dea101b17ca869.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes NFPO, an on-policy RL method that replaces the standard Gaussian policy with a normalizing flow (specifically, RealNVP) inside PPO. The authors experimentally analyze why naïvely plugging a flow into PPO is unstable, and proposed techniques to stabilize training by bounding the scale network with a tanh transform and a limit parameter l. Experiments span URG and MuJoCo Playground, and compare NFPO against PPO baselines. NFPO often matches or beats PPO on several locomotion/manipulation tasks; qualitative studies (gridworld and UR10 reaching) suggest more multi-modal behavior; and they show sim-to-real transfers to Unitree robots. Wall-clock cost increases by ~19% vs. PPO on g1."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed technique is simple and clearly presented, with writing that is easy to follow.\n- The paper includes detailed studies on different design choices (e.g., different methods to stablize scaling function, different hyper-parameter choices).\n- NFPO offers slight performance gains compared to PPO, especially for certain control tasks such as g1-joystick, h1-joystick, and G1JoyStickRoughTerrain."}, "weaknesses": {"value": "- While the paper may be among the first to pair normalizing flows with on-policy RL, similar ideas have been explored extensively in off-policy settings, e.g., [1-3]. Without a clearer technical distinction or theoretical contribution beyond the training stabilizations, the novelty of this work seems modest.\n\n- Reported gains are small and the comparisons are not run on widely accepted benchmarks with commonly adopted baselines (e.g., mixture Gaussian PPO variants, off-policy methods). As a result, it’s hard to assess the practical significance of the method.\n\n- The sim-to-real experiments omit key operational specifics, e.g., task definitions, control frequency/latency, domain-randomization settings, and failure rates, making reproducibility and reliability difficult to judge.\n\n---\n\n**Minor Errors:**    \n- “the $\\exp(s_\\theta (x_d))$ used in RealNVP apply …” → “the $\\exp(s_\\theta (x_d))$ used in RealNVP applies …” in Line 171.   \n- “overfiting” → \"overfitting\"; appears in Lines 162-174.   \n- “logprobability” → \"log-probability\" (hyphen) in Line 177."}, "questions": {"value": "- Please provide the state/action space definitions and concise descriptions of each evaluated environment. Also explain why NFPO strongly outperforms PPO on G1JoyStickRoughTerrain yet underperforms on MJP-PandaOpenCabinet—what properties of these tasks favor or hinder flow policies?\n\n- Please include learning curves on common MuJoCo benchmarks (e.g., Hopper, HalfCheetah, Walker2d, Ant, Humanoid) and compare against widely used baselines (e.g., SAC, TD3) to contextualize performance.\n\n- The experiment presented in Fig. 8 appears to modify baseline defaults without retuning. Because online RL is sensitive to hyperparameters, fair comparisons should use the best settings found within a shared search space. How were the baseline hyperparameters chosen, and why is that selection representative?\n\n- Section 5.5 claims NFPO supports deterministic action sampling, but for non-volume-preserving flows the mode (argmax density) is generally nontrivial [3]. How is deterministic sampling implemented for RealNVP in practice?\n\n- Fig. 5 suggests NFPO and PPO achieve similar returns, while Table S2 reports ~19% sampling overhead for flows. Under what conditions (task characteristics, exploration regime, multimodality) are normalizing-flow policies clearly preferable than Gaussian policies?\n\n- Beyond the reported settings, how sensitive is performance to flow depth (e.g., 2/6/8 layers) and hidden sizes? Please include a brief ablation.\n\n- Did you evaluate other efficient multimodal policy families (e.g., consistency models [4,5] or related diffusion variants)? How do they compare to NFPO in stability, sample efficiency, and runtime?\n\n---\n\n**References:**\n\n[1] Haarnoja et al. Latent space policies for hierarchical reinforcement learning. ICML 2018. \\\n[2] Mazoure et al. Leveraging Exploration in Off-policy Algorithms via Normalizing Flows. CoRL 2019. \\\n[3] Chao et al. Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow. NeurIPS 2023. \\\n[4] Song et al. Consistency Models. ICML 2023.\\\n[5] Ding et al. Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning. ICLR 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UDVjqVzCfR", "forum": "9O1IpZ0F4h", "replyto": "9O1IpZ0F4h", "signatures": ["ICLR.cc/2026/Conference/Submission8345/Reviewer_AJ7p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8345/Reviewer_AJ7p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620955583, "cdate": 1761620955583, "tmdate": 1762920262283, "mdate": 1762920262283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to integrate the multi-modal modeling capabilities of Normalizing Flows (NF) into robotic PPO policies. The authors first identify the training instability of combining NF directly with PPO, attributing it to the exploding Jacobian determinant caused by the $exp(s)$ term in RealNVP.  To solve this, the authors propose NFPO, which uses an ${tanh}$ activation function to constrain the output of $s$, thereby stabilizing training. Experiments show that NFPO performs robustly on several simulation tasks, matching or exceeding  PPO 3, and was successfully transferred to a real-world robot."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Problem Diagnosis and Solution: The paper clearly diagnoses the root cause of instability when combining NF with PPO (exploding determinant) and proposes a simple, effective solution ($tanh$ activation) .\n\nSolid Experimental Validation: The paper provides comprehensive benchmarks on 9 tasks across multiple simulators (IsaacGym, Mujoco Playground) 7and includes thorough ablation studies.\n\nReal-World Deployment: The policy was successfully transferred from simulation to a real Unitree G1 robot, strongly demonstrating the algorithm's robustness and practical value."}, "weaknesses": {"value": "Limited Innovation: The work is primarily an application and engineering-level adaptation of RealNVP for policy optimization, rather than a fundamental algorithmic innovation. The core stabilization technique ($s\\_tanh$) is a known trick.\n\nAmbiguous Multi-modal Advantage: Although multi-modality was shown in specific tasks (Sec 5.3), its direct link to the performance gains in the main benchmarks (Sec 5.2) is not clear."}, "questions": {"value": "Regarding Real-World Deployment: You mentioned using a \"deterministic version\" in Sec 5.5.a) How was this \"deterministic version\" obtained (e.g., $z=0$)? Does this imply that the stochastic or multi-modal policy is unstable in the real world?\n\nRegarding FPO/Meow Comparison: In Sec 5.4, NFPO outperformed FPO and Meow in robustness. Were FPO/Meow given the same level of hyperparameter tuning as NFPO, or were their default parameters used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cBmzgfKPPA", "forum": "9O1IpZ0F4h", "replyto": "9O1IpZ0F4h", "signatures": ["ICLR.cc/2026/Conference/Submission8345/Reviewer_47UP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8345/Reviewer_47UP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924518838, "cdate": 1761924518838, "tmdate": 1762920261913, "mdate": 1762920261913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NFPO algorithm, which parameterizes the policy-network with a Normalizing Flow to capture the multi-modal action distributions. The authors diagnose the instability in RealNVP-based flows inside PPO, and provide a simple but effective solution: normalizing the $s_θ(x)$ output to make it in a proper range. Extensive experiments on multiple robotics simulators and with multiple seeds show a stronger and more stable performance on several locomotion tasks, with mixed results on some manipulation tasks and sim-to-real deployments on Unitree hardware."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose NFPO, a new framework that integrates Normalizing Flows (NF) into PPO for robotic multi-modal policy learning, and further analyze the causes of its training instability and introduce effective stabilization techniques. The authors provide a clear problem formulation for the multi-modal action distribution for on-policy control, and a simple and reproducible solution by swapping policy head and bounding the scale output of the flow.\n\n- Comprehensive experiments are conducted on several widely used simulation environments. With the same configuration settings, NFPO achieves competitive performance compared with state-of-the-art Gaussian-based PPO implementations. Real-world validation also demonstrates that policies trained with NFPO can be successfully transferred to physical robots. These extensive experiments on multiple robotics simulators and deployments show the effectiveness of NFPO in capturing the multi-modal action distributions and stabilizing learning."}, "weaknesses": {"value": "- Considering this is the ICLR submission, the theoretical analysis may be more important than the engineering implementation and results. But the theoretical analysis for the algorithm and mathematical proofs in this paper are limited, e.g., one may expcet to see the analysis on the stability of NFPO and the reason why adding entropy loss in NFPO does not bring a significant performance difference.\n\n- In some tasks like  MJP-PandaOpenCabinet and MJP-Go1JoystickRoughTerrain, NFPO fails to learn a good policy, which shows the limitation on the generalization ability of NFPO.\n\n- Runtime overhead is reported but not decomposed. And there is no complexity analysis vs. the action dimension or number of coupling layers."}, "questions": {"value": "- As stated in weaknesses, the theoretical analysis and mathematical proofs in this paper are limited. Can the authors provide more theoretical analysis on why NFPO can work better? \n\n- Why tanh is more robust than clip in the authors' implementation? \n\n- Can the authors provide more experiments with different hyperparameter combinations to show the robustness of NFPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0mBfkhjxR0", "forum": "9O1IpZ0F4h", "replyto": "9O1IpZ0F4h", "signatures": ["ICLR.cc/2026/Conference/Submission8345/Reviewer_MJyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8345/Reviewer_MJyh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966668050, "cdate": 1761966668050, "tmdate": 1762920261446, "mdate": 1762920261446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}