{"id": "BZhxEZfznH", "number": 4636, "cdate": 1757731753781, "mdate": 1763020593966, "content": {"title": "Representing Sentence Structure in a Tree Metric Space", "abstract": "This paper proposes building a sentence tree metric space through\nrepresentation learning of sentence structure. Our method \nrepresents every sentence tree structure as a vector, with the\nEuclidean distance applied to construct the sentence tree metric. In\ncomparison with the previous, representative tree-metric methods of\nthe (tree edit distance) TED, tree kernels, and PQ-grams, our method  has\nthe best computational complexity, scaling to handle a million trees,\nyet it performs well in predicting tree structure and learning\nTED-like distances, even without TED for supervision. Our large-scale\nsentence metric space analyses provide novel ways to study sentence\nstructures from recent language technology, by evaluating parsers and\ntree-annotated corpora, and with tree structures acquired by recent\nlarge language models (LLMs). These analyses also address the nature\nof natural language trees not only within languages but in comparison\nwith random trees.", "tldr": "We propose a fast tree representation learning method that constructs a sentence metric space. The approach jointly predicts tree structure and class, offering benefits for evaluating LLMs and parsers, and analyzing sentences w.r.t. random trees.", "keywords": ["tree metric space", "tree representation learning", "Transformer", "tree analyses", "tree metric", "random trees"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2f31cf1504a703c535fd63d43859c8057c8a1497.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new framework for representing sentence structures in a tree metric space via representation learning. Each dependency tree is embedded as a fixed-dimensional vector through a Transformer encoder trained to predict both intra-tree relations and language classes.\nThe Euclidean distance between embeddings defines a metric over trees, and the Wasserstein distance is used to compare sets of trees.\nCompared to classical tree distance methods such as Tree Edit Distance (TED) and PQ-grams, the proposed model achieves orders-of-magnitude faster computation while preserving TED-like properties.\nThe authors demonstrate applications to large-scale linguistic analysis across various languages, parser evaluation, and assessing the structural similarity of LLM-generated trees.\nThey also reveal empirical evidence that natural language trees are not context-free and visualize cross-linguistic clusters in the learned metric space."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-  The idea of constructing a learned metric space for sentence trees is conceptually new and elegant. It bridges structural linguistics and modern representation learning.\n\n-  The proposed approach reduces the cubic cost of TED to linear time in tree size, enabling analysis of up to one million trees, a strong scalability result with practical implications.\n\n- The experiments span 21 natural languages, random and context-free trees, and multiple LLM generations. This breadth convincingly demonstrates generality and interpretability.\n\n- The paper covers broad related work, and overall writing is clear."}, "weaknesses": {"value": "- My primary concern lies in the impact and benefit of the proposed approach to the broader community. Although the paper presents a technically sound method and Section 6 provides interesting analyses, such as parser evaluation, LLM syntax assessment, and linguistic typology visualization, these results mainly serve as illustrative examples rather than evidence of broader methodological or representational impact. The strong correlation with TED is interesting, but the paper does not clearly articulate why reproducing this metric is valuable for representation learning or how the resulting tree metric space could drive new progress in downstream tasks. A more elaborate discussion or empirical demonstration of potential applications where the learned metric brings further benefits would strengthen the paper.\n\n- The authors compare with classical tree metrics (TED, tree kernels, PQ-grams) and tree-structured neural models (Tree-LSTM, Tree-Transformer), but omit modern graph embedding or graph neural network (GNN) baselines that could, in principle, also encode tree structures. While the authors justify this choice (line 98) by citing Zhang et al. (2024) that standard GNNs without explicit structural encodings fail to capture tree geometry,  this claim remains unverified in their own setting. \nAs ICLR emphasizes representation learning, it would be valuable to empirically demonstrate how standard GNNs fail to represent tree topology here, showing that the proposed model’s success arises from its representational inductive bias rather than from task-specific tuning."}, "questions": {"value": "- How does the proposed tree metric yield broader representational or practical benefits beyond reproducing TED and illustrative analyses?\n\n- Could the authors provide a more elaborated discussion or empirical analysis showing how standard GNNs fail to capture tree structure, clarifying whether the proposed model’s advantage stems from its inductive bias or specific design choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nICxSwSxVl", "forum": "BZhxEZfznH", "replyto": "BZhxEZfznH", "signatures": ["ICLR.cc/2026/Conference/Submission4636/Reviewer_PzR7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4636/Reviewer_PzR7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859536991, "cdate": 1761859536991, "tmdate": 1762917481192, "mdate": 1762917481192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YrP9WJoou0", "forum": "BZhxEZfznH", "replyto": "BZhxEZfznH", "signatures": ["ICLR.cc/2026/Conference/Submission4636/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4636/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763020593186, "cdate": 1763020593186, "tmdate": 1763020593186, "mdate": 1763020593186, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of tree metric space and proposes a novel method that builds a sentence tree metric space through representation learning of embedding trees in a vector space. It represents every sentence tree structure as a vector, with the Euclidean distance applied to construct the sentence tree metric. The experimental results show that the proposed method achieves lower time complexity than previous approaches and exhibits strong capability in predicting tree structures and classes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel sentence trees representation learning approach by using deep learning to construct a metric space.\n\n2. The proposed method demonstrates good efficiency and scalability."}, "weaknesses": {"value": "1. The motivation of the proposed method should be strengthened.\n\n2. The authors should enhance the expressiveness of the paper and avoid unclear or ambiguous descriptions.\n\n3. The number of comparison methods is too limited."}, "questions": {"value": "1. How does the time cost of the proposed method compare with that of TrLSTM and TrTransformer?\n\n2. Considering the resources and time required to train the proposed method, is it computationally economical?\n\n3. I would like to see further experimental analyses, such as the ablation study of distance function.\n\n4. The authors should improve the quality of the figures. For example, Figure 1 contains red wavy lines, Figure 2 shows inconsistent font sizes and an unclear workflow, and in Figure 5, \"Norwegian-Bokmaal\" extends beyond the box.\n\n5. The authors should cite more recent papers, as only 3/48 of the current references are from the past three years (2022)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EMVOagZsfZ", "forum": "BZhxEZfznH", "replyto": "BZhxEZfznH", "signatures": ["ICLR.cc/2026/Conference/Submission4636/Reviewer_P7K3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4636/Reviewer_P7K3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918186893, "cdate": 1761918186893, "tmdate": 1762917480889, "mdate": 1762917480889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a paper on embedding a tree structure of a sentence. While not using terminals, tree shapes are widely different from sentence to sentence and language and language, its accurate embedding is (if properly conducted) an important problem.\nAlthough the method itself is quite vaguely represented and no guarantees for being a \"metric\" is presented, empirically the proposed embedding works better than various baselines and languages; however, how different these shapes actually are and how much these differences are reflected as a \"metric\" is not presented in this paper."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Tree shapes are of course important theme for natural language processing and linguistics, and their study through appropriate embeddings is also beneficial.\nTree shapes will be statistically different from language to language, thus experimental evaluations on many different languages and random trees (and LLM trees) are very interesting and important."}, "weaknesses": {"value": "There are several drawbacks in the current form of this paper.\n\n- First of all, the proposed embedding method is presented quite ambigously. There are many pages of results in this paper, but the central part of Section 3.2 is less than a page and without almost any mathematical explanation. Could you explain Figure 2 in words and equations? What is \"cross-entropies of the positions of every node\"? (i.e. are you using nonterminals associated with each node or completely concentrate on vanilla tree shapes?) There are no argument to regard this method of embedding better than other possible ways.\n- Second, is the proposed method actually \"metric\"? There are no arguments that the resulting embeddings become a metric: just using Euclidean distance as a \"metrtic\" seems to be an overstatement. \"Metric\" also appears in the title, and you can be more careful for using that kind of mathematical statements.\n- Finally, depth-first traversal also seems to be a heuristic. Since a tree structure can be represented as a binary matrix of connections (imagine CYK parsing), directly embedding that matrix seems to be more mathematically grounded way for embedding tree structures.\n\nAt least, this paper seems to need two pages for presenting the basic theory of embedding and its theoretical conclusions."}, "questions": {"value": "See \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C9SjRdlPMS", "forum": "BZhxEZfznH", "replyto": "BZhxEZfznH", "signatures": ["ICLR.cc/2026/Conference/Submission4636/Reviewer_2fux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4636/Reviewer_2fux"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049008642, "cdate": 1762049008642, "tmdate": 1762917480367, "mdate": 1762917480367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach to constructing a sentence tree metric space through representation learning of sentence structures. The proposed method embeds each sentence’s syntactic tree into a continuous vector space, where Euclidean distance is employed to define a metric over the sentence trees. The analysis further investigates the structural characteristics of natural language trees, comparing them not only across different languages but also against randomly generated trees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is supported by experimental evaluations that demonstrate the applicability of the proposed approach. The authors provide analyses that help clarify the relationship between linguistic structures and their learned representations."}, "weaknesses": {"value": "Despite its merits, several issues should be addressed to improve the paper’s clarity and impact:\n\na. Limited applications of the proposed framework. The paper primarily focuses on tree-based representations of sentences. While a few application examples are provided, the practical utility of the sentence tree metric space remains somewhat restricted. Moreover, in natural language, many sentences exhibit incomplete or ambiguous syntactic structures, which may limit the proposed approach.\n\nb. Insufficient comparison with classical NLP methods. Although various approaches exist for sentence representation in classical NLP, the experimental section includes only a small subset. Incorporating additional well-established methods would provide a more comprehensive evaluation and strengthen the empirical findings.\n\nc. Lack of comparison with LLM-based sentence representations. Given the strong generalization capabilities of large language models (LLMs), they could serve as natural baselines for sentence representation—either through fine-tuning or prompt engineering. Including such comparisons would help contextualize the proposed method within the current NLP landscape.\n\nd. Unclear definition of the tree metric space. In Section 3.2, the formal definition of the sentence tree metric space is vague. The paper lacks a rigorous mathematical specification of the metric and does not clearly demonstrate how the metric space properties are utilized in practical applications. Clarifying this section and providing theoretical justification would enhance the paper’s rigor."}, "questions": {"value": "To strengthen the contribution, the authors are encouraged to (1) include additional baselines encompassing both classical NLP and LLM-based approaches, (2) explore broader or more realistic applications of the proposed tree metric space, and (3) refine the formal definition and theoretical grounding of the metric space."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rteYRpN22S", "forum": "BZhxEZfznH", "replyto": "BZhxEZfznH", "signatures": ["ICLR.cc/2026/Conference/Submission4636/Reviewer_hwzd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4636/Reviewer_hwzd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115915898, "cdate": 1762115915898, "tmdate": 1762917480037, "mdate": 1762917480037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}