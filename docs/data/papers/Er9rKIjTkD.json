{"id": "Er9rKIjTkD", "number": 13891, "cdate": 1758224447969, "mdate": 1759897405922, "content": {"title": "SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis", "abstract": "The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing **SANEval** (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than that of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.", "tldr": "We introduce SANEval, an open-vocabulary T2I benchmark that not only scores compositional accuracy but also provides fine-grained diagnostic actionable feedback", "keywords": ["Benchmarking and Evaluation", "Text-to-Image", "Generation", "Open-Vocabulary Object Detection", "Model Interpretability", "Alignment", "Explanability"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8687545a7fd2f293d84489e98b34a734658a7e87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SANEval, a new benchmark designed to evaluate compositional text-to-image (T2I) generation along three axes: spatial relations, attribute binding, and numeracy. The core innovation is an open-vocabulary evaluation pipeline that combines a language model-based prompt understanding module with an LLM-enhanced object detection module (using YOLO-E) to handle objects beyond fixed vocabularies. SANEval provides not just quantitative scores but also interpretable, structured feedback diagnosing missing, spurious, or misbound elements. The authors evaluate six state-of-the-art T2I models and demonstrate that SANEval aligns better with human judgment than existing closed-vocabulary benchmarks such as CompBench++."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely and relevant topic – Evaluation of compositional T2I generation is an important and underexplored problem, especially as generative models become more capable yet still fail on fine-grained prompt adherence.\n\n2. Open-vocabulary focus – The attempt to overcome vocabulary limits of prior OD-based benchmarks (e.g., those tied to COCO classes) is a real step forward. Integrating LLM synonym reasoning with detection to match rare objects is a practical and impactful idea.\n\n3. Structured feedback – Providing interpretable diagnostic feedback (e.g., missing or swapped objects) is valuable, addressing a longstanding complaint that most benchmarks yield only opaque scalar scores.\n\n4. Thorough evaluation – The experiments cover multiple models, diverse prompt categories, and statistical comparisons with existing benchmarks. The degradation analysis with increasing object count is especially insightful.\n\n5. Strong engineering – The system is well implemented, modular, and reproducible, with clear illustrations (e.g., Figures 1–4) explaining each evaluation component."}, "weaknesses": {"value": "1. Incremental conceptual novelty – While well-engineered, the main idea, combining LLM reasoning with open-vocabulary detection for compositional evaluation, is a relatively straightforward extension of prior hybrid pipelines like Geneval or CompBench++. The paper oversells its conceptual novelty relative to what is essentially a systematic engineering improvement.\n\n2. Dependence on proprietary APIs – The reliance on commercial APIs (Gemini 2.5 Flash, GPT, etc.) undermines claims of “open” benchmarking and reproducibility. The evaluation cannot be replicated without access to these closed systems.\n\n3. Data quality concerns – Much of the dataset is LLM-generated and then “validated by humans,” but the validation process is not clearly described. It’s unclear how reliable or diverse the resulting prompts and labels are, especially for rare or ambiguous compositions.\n\n4. Potential circularity – Because the same families of LLMs are used both to generate prompts and evaluate outputs, the benchmark risks circular evaluation biases. This is particularly problematic if the evaluated models share architectures or training corpora with the evaluators.\n\n5. Limited human validation – The claim that SANEval correlates better with human judgments is asserted but not deeply substantiated. There’s no large-scale human evaluation or statistical analysis showing correlation coefficients with human preferences.\n\n6. Interpretability claim is overstated – The feedback is structured and informative, but not necessarily “interpretable” in a cognitive sense. The system still relies on opaque LLM reasoning, so interpretability here is more syntactic than semantic.\n\n7. Paper tone – The writing is overly confident and somewhat verbose, with repeated claims of “first,” “scalable,” and “comprehensive.” While the contributions are solid, they don’t quite justify that level of rhetoric."}, "questions": {"value": "1. How robust is SANEval to the choice of LLMs or detectors? Have you tested with open-source alternatives to Gemini or GPT to ensure consistency?\n\n2. Could you provide quantitative human evaluation results to validate your claim of stronger human correlation?\n\n3. How do you handle prompts with ambiguous or relational adjectives (e.g., “a tall man next to a shorter man”)?\n\n4. What mechanisms prevent SANEval from rewarding models that overfit to common visual priors rather than compositional fidelity?\n\n5. How expensive (in terms of GPU or API calls) is the full benchmark run for one model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BmEDayaCuv", "forum": "Er9rKIjTkD", "replyto": "Er9rKIjTkD", "signatures": ["ICLR.cc/2026/Conference/Submission13891/Reviewer_DbSq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13891/Reviewer_DbSq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927389582, "cdate": 1761927389582, "tmdate": 1762924401818, "mdate": 1762924401818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes SANEval, an automated benchmark and scoring framework for evaluating compositional faithfulness in text-to-image generation. The core claim is that current evaluators either (i) rely on fixed vocabularies, which miss rare or fine-grained objects, or (ii) provide only coarse scalar scores, which are not actionable. SANEval addresses this by (1) decomposing prompts into explicit requirements (objects, attributes, spatial relations, and numeracy), (2) using synonym expansion plus an open-vocabulary detector to localize objects in generated images, (3) using a VLM to judge fine-grained attributes on detected crops, (4) checking spatial layout and object counts, and (5) generating structured, diagnostic feedback for each failure mode. The benchmark is applied to a suite of state-of-the-art text-to-image models, and per-dimension scores (attribute binding, spatial relationships, numeracy) are reported along with claims of improved alignment with human judgments and complementary behavior to prior benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Problem importance. The paper focuses on a real bottleneck in current T2I systems: controllability. Capturing whether a model got “two red cars to the left of a blue bus” right is directly relevant to downstream productization and safety of generative vision systems. Framing spatial relations, numeracy, and attribute binding as three core controllability axes is well-motivated. \n\n2. Pipeline design / interpretability. The evaluation stack is modular and conceptually clean: prompt parsing → synonym expansion → open-vocabulary detection → attribute judgment on crops → spatial / count checks → textual feedback. This goes beyond a single scalar metric and produces human-readable explanations (“missing the second penguin,” “shirt color does not match prompt”), which is valuable both for benchmarking and for training-time improvement (e.g. RL with AI feedback). \n\n3. Attempt at open-vocabulary evaluation. Traditional benchmarks are constrained to a small, fixed label set (e.g. COCO categories). SANEval explicitly attempts to escape that bottleneck via synonym expansion and an open-world detector, claiming to evaluate long-tail objects and fine-grained attributes. This, if shown reliable, is a meaningful step forward for evaluation coverage. \n\n4. Multi-model comparison and failure characterization. The paper evaluates several strong text-to-image systems and surfaces distinct weaknesses (e.g., some models better at spatial layout, others better at numeracy, etc.). This makes SANEval look like a diagnostic tool rather than just a leaderboard generator. Tables in the main text highlight degradation under increasing prompt difficulty and differences across models."}, "weaknesses": {"value": "1. Reproducibility and stability are underdeveloped.\nThe benchmark depends on proprietary or partially described components (e.g. Gemini-2.5-Flash for prompt parsing and attribute judgment, YOLO-E for open-vocabulary detection), some of which are not publicly reproducible. The paper promises release of data and code but does not convincingly demonstrate that the community will be able to run the full pipeline without access to closed-source commercial systems.\n\n2. Limited validation of metric correctness.\nThe paper claims “strong alignment with humans,” but key details are missing: exact study size, annotator protocol, inter-annotator agreement, and per-dimension correlation (spatial / numeracy / attribute binding) between SANEval scores and human judgments at the image level. Current quantitative results emphasize differences across models and p-values vs. CompBench++, but do not provide clear, per-sample reliability numbers for SANEval itself. Without those, it remains unclear whether SANEval is actually accurate in judging success/failure, or just “plausible and convenient.”\n\n3. Open-vocabulary claim is not yet airtight.\nThe core technical sell is that synonym expansion plus open-world detection solves “vocabulary mismatch.” However, no error analysis is reported for that step: How often does the system over-credit partial matches (“generic bird” instead of “albatross”)? How often does it under-credit genuinely correct rare objects because the synonym set was incomplete? This is an obvious potential criticism because it goes directly at the headline claim (“open-vocabulary evaluation”). \n\n4. No ablation / robustness analysis across pipeline stages.\nThe pipeline has multiple learned/modules stages, and failure in any stage could cascade. The paper does not report sensitivity to swapping the LLM in the Prompt Understanding Module, ablating synonym expansion, or replacing the attribute-judging VLM with an alternative. There is also no quantitative discussion of false positives / false negatives in spatial and numeracy checks caused by upstream detection errors. This makes it easy to argue the metric may be brittle, and therefore risky to trust for fine-grained leaderboard decisions.\n\n5. Statistical framing against prior benchmarks is weak.\nThe comparison to CompBench++ uses significance tests on rank correlations to argue SANEval measures something different. Only reporting p-values, without effect sizes or the actual Spearman coefficients (ρ), invites criticism. A low correlation may mean “captures complementary aspects,” but it could also mean “noisy / inconsistent.” The paper currently does not disambiguate those possibilities.\n\n6. Human prompts vs synthetic prompts.\nPrompt sets are at least partially LLM-generated and curated. The paper does not quantify how similar these prompts are to natural user requests “in the wild.” A skeptical reader may ask whether the benchmark is indirectly overfit to LLM-style phrasing, and whether that inflates apparent evaluator reliability."}, "questions": {"value": "1. Benchmark stability and openness\n    - Will an end-to-end reference implementation using only openly available models (for prompt parsing, open-vocabulary detection, and attribute judging) be released?\n    - If such a “SANEval-lite” pipeline is substituted for the proprietary backbone, how similar are model rankings and per-dimension scores?\n\n2. Human agreement and study design\n\n    - How large is the human evaluation set used for validating SANEval?\n\n    - What instructions were annotators given for spatial, numeracy, and attribute correctness?\n\n    - What inter-annotator agreement was observed?\n\n    - What are the per-dimension image-level correlations (e.g., Spearman’s ρ / accuracy / F1) between SANEval and human judgments?\n\n3. Error analysis for open-vocabulary claims\n\n    - On a held-out subset with human-labeled boxes and attributes, what is precision/recall of SANEval’s object existence and attribute binding scores for long-tail categories?\n\n    - How often does synonym expansion lead to false credit for “nearby but wrong” categories?\n\n4. Robustness / ablations\n\n    - How sensitive is SANEval to swapping the LLM used in the Prompt Understanding Module or the attribute-binding checker?\n\n    - How sensitive are final scores to removing synonym expansion or constraining detection to a fixed-label detector (COCO-style)?\n\n    - Can a single failing sub-module (e.g., missed detection) flip an otherwise-correct image from “pass” to “fail,” and how often does that happen?\n\n6. Statistical interpretation\n    - Table 3 reports significance values when comparing to CompBench++, but does not report actual effect sizes. Could the paper include rank correlation coefficients and a short interpretation (e.g., “ρ = 0.2 indicates weak monotonic agreement, suggesting that SANEval captures aspects of controllability that CompBench++ does not emphasize”)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JXcyfLKHQ2", "forum": "Er9rKIjTkD", "replyto": "Er9rKIjTkD", "signatures": ["ICLR.cc/2026/Conference/Submission13891/Reviewer_hFs4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13891/Reviewer_hFs4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953794891, "cdate": 1761953794891, "tmdate": 1762924401433, "mdate": 1762924401433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SANEval, a new benchmark for evaluating compositional faithfulness in text-to-image (T2I) generation models. Unlike existing benchmarks that depend on closed-set object vocabularies or yield opaque single-number scores, SANEval introduces an open-vocabulary, diagnostic, and interpretable evaluation pipeline. It consists of two core modules: a Prompt Understanding Module that uses an LLM to extract objects, attributes, spatial relationships, and quantities from prompts, and an Enhanced Object Detection Module that combines an open-vocabulary detector (YOLO-E) with LLM-based synonym reasoning to reduce vocabulary mismatch. SANEval evaluates images along three axes—attribute binding, spatial relationships, and numeracy—producing not only quantitative scores but also fine-grained, structured feedback that identifies missing, hallucinated, or incorrectly bound elements. Experiments conducted on six state-of-the-art T2I models show that SANEval aligns more closely with human judgment than prior metrics and captures compositional failure modes that existing benchmarks often miss. The authors further commit to releasing the dataset, evaluation pipeline, and annotations to support future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong diagnostic capability:\nSANEval goes beyond providing a single score—it outputs structured, interpretable feedback, explicitly identifying missing objects, incorrect attribute bindings, and count mismatches. This makes it highly useful for debugging and improving T2I systems.\n\nOpen-source commitment:\nThe authors plan to release the dataset, prompts, annotations, and full evaluation pipeline, which will greatly facilitate reproducibility and help standardize compositional evaluation in the community."}, "weaknesses": {"value": "Limited robustness analysis:\nThe paper does not thoroughly examine how LLM parsing errors or object detection failures (e.g., hallucinations or missed detections) propagate through the pipeline and affect final scoring reliability.\n\nHigh computational cost:\nThe evaluation pipeline requires multiple rounds of LLM calls and YOLO-E inference per image, which may make it expensive and impractical for large-scale evaluation on millions of samples.\n\nInsufficient prompt diversity:\nThe dataset’s ~5000 prompts are largely synthetically or structurally composed, lacking coverage of real-world human-written instructions with figurative language, emotional tone, ambiguous phrasing, or complex logical relationships."}, "questions": {"value": "LLM reliability\nHow do you ensure that the LLM does not introduce hallucinations or misinterpretations during prompt parsing or attribute evaluation? Have you considered combining rule-based logic with LLMs (a hybrid approach) to reduce such errors?\n\nObject detection uncertainty\nIf the detector fails to identify a small, occluded, or stylized object, could the system incorrectly penalize a correct image? Do you plan to incorporate uncertainty estimation or confidence propagation to handle these cases?\n\nGeneralization to real-world prompts\nCan you demonstrate how SANEval performs on natural, human-written prompts involving negation, multiple clauses, metaphors, or conditional instructions—rather than primarily template-based prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ReaPLJImeZ", "forum": "Er9rKIjTkD", "replyto": "Er9rKIjTkD", "signatures": ["ICLR.cc/2026/Conference/Submission13891/Reviewer_NsRt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13891/Reviewer_NsRt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965126962, "cdate": 1761965126962, "tmdate": 1762924401154, "mdate": 1762924401154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SANEval, an open-vocabulary benchmark for evaluating compositional reasoning in text-to-image (T2I) models. The authors argue that current benchmarks rely on closed-set vocabularies and provide limited diagnostic feedback. To address this, they design a modular evaluation framework combining a large language model for prompt understanding with an open-world object detector, enabling fine-grained assessment of spatial relations, attribute binding, and numeracy. SANEval outputs both quantitative scores and interpretable feedback to pinpoint specific compositional errors. The benchmark is validated against human annotations and shows complementary insights compared to prior methods like CompBench++, establishing it as a scalable and diagnostic tool for T2I evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a well-structured benchmark that separately evaluates spatial reasoning, attribute binding, and numeracy, offering a more interpretable breakdown of compositional performance than prior holistic metrics.\n\n* By integrating LLM-based synonym expansion with an open-world detector (YOLO-E), SANEval effectively overcomes the fixed-class limitations of existing object-detection-based benchmarks.\n\n* The framework provides structured, human-readable feedback that identifies missing, incorrect, or extra objects and attributes, making it highly useful for model debugging. The benchmark demonstrates statistically distinct and complementary insights compared to established baselines, showing that SANEval captures novel aspects of compositional reasoning."}, "weaknesses": {"value": "- The benchmark heavily relies on proprietary LLMs (e.g., Gemini-2.5-Flash) for both prompt parsing and evaluation, which may limit reproducibility.\n\n- The qualitative feedback examples remain limited, and it is unclear how consistently the diagnostic outputs generalize across diverse prompt domains.\n\n- The benchmark’s prompts are synthetically constructed and might not reflect the real-world user prompts especially the diversity aspect."}, "questions": {"value": "How robust is SANEval to variations in the underlying LLM or detector, would the evaluation results remain consistent if different models (e.g., GPT-4o or open-source detectors) were used in place of Gemini?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GaN49Iv2PN", "forum": "Er9rKIjTkD", "replyto": "Er9rKIjTkD", "signatures": ["ICLR.cc/2026/Conference/Submission13891/Reviewer_aNif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13891/Reviewer_aNif"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123164867, "cdate": 1762123164867, "tmdate": 1762924400753, "mdate": 1762924400753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}