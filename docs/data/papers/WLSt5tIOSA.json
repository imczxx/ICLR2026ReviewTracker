{"id": "WLSt5tIOSA", "number": 2538, "cdate": 1757140461772, "mdate": 1759898142394, "content": {"title": "Fine-tuning VLMs Without Forgetting Is Easier Than You Think", "abstract": "*This paper does not propose a new method; rather, we find that simple adjustments of the fine-tuning recipes of vision language models (VLM) are sufficient to mitigate catastrophic forgetting.* Using visual question answering tasks, we design a 2×2 experimental framework to assess model performance across in-distribution and out-of-distribution image and text inputs. Our results show that appropriate regularization, such as constraining the number of trainable parameters or adopting a low learning rate, effectively prevents forgetting when dealing with out-of-distribution images. However, we uncover a distinct form of forgetting in settings with in-distribution images and out-of-distribution text. We attribute this forgetting as task-specific overfitting and address this issue by introducing a data-hybrid training strategy that combines datasets and tasks. Finally, we demonstrate that this approach naturally extends to continual learning, outperforming existing methods without the need for complex auxiliary mechanisms. In general, our findings challenge the prevailing assumptions by highlighting the inherent robustness of VLMs and providing practical guidelines for adapting them while preserving their general-purpose capabilities.", "tldr": "", "keywords": ["VLM Finetuning", "Catastrophic Forgetting", "Continual Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4de99b6441d5bafde70e933c1b4d4bcc4eafdb33.pdf", "supplementary_material": "/attachment/d1387f2719f7408715ac3f10f5d36232b35c2499.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a systematic empirical study of catastrophic forgetting in vision-language model (VLM) fine-tuning. Rather than introducing a new algorithm, the authors argue that simple fine-tuning practices—notably small learning rates and parameter-efficient updates (e.g., LoRA)—are sufficient to prevent most forms of forgetting. Using a carefully designed 2×2 evaluation matrix (in/out-of-distribution text × image), they show that VLMs retain generalization across domains when regularized appropriately.\nA notable exception is observed for in-distribution (ID) images with out-of-distribution (OOD) text, where forgetting manifests as task-specific overfitting—the model learns to ignore new prompts while retaining prior visual associations. To address this, the authors propose a data-hybrid training strategy that mixes small amounts of diverse instruction data to prevent overfitting. Extending to a continual learning benchmark (MLLM-CL), the same simple recipe outperforms or rivals complex continual-learning baselines without auxiliary mechanisms."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Comprehensive and well-controlled experimental design.** The 2×2 ID/OOD evaluation setup is elegant and isolates different axes of forgetting (image vs. text). The authors’ inclusion of both small- and large-scale VLMs (Qwen2.5-VL, LLaVA) strengthens the generality of the conclusions\n- **Somewhat surprising empirical finding.** The claim that catastrophic forgetting in VLMs is largely overstated is both impactful and counterintuitive. It provides a fresh perspective likely to influence future methodology in multimodal fine-tuning.\n- **Clear diagnosis of a new failure mode.** Identifying the ID-image / OOD-text regime as the key vulnerability (task-specific overfitting rather than capacity loss) is insightful and backed by solid diagnostic experiments (e.g., the class-label distractor test, Figure 3).\n- **Simplicity and reproducibility.** The study uses minimal hyperparameter tuning, accessible setups (LoRA, low learning rates), and open-sourced code/datasets. This all ensures the paper’s transparency and educational value.\n- **Strong continual-learning results.** The finding that simple sequential fine-tuning can match or outperform complex replay-based methods is remarkable and challenges current assumptions in the field"}, "weaknesses": {"value": "- **Limited insightful discussion.** While the empirical evidence is strong, the paper lacks a deeper explanation why VLMs are inherently robust to forgetting. Some discussion on model overparameterization, modularization between modalities, or representational sparsity would help ground the claims.\n- **Missing connections to uncertainty-aware or Bayesian fine-tuning.** The discussion of regularization could benefit from referencing prior works that explicitly model uncertainty during (continual) fine-tuning, such as VPT [1] or CLAP4CLIP [2], which perform uncertainty-aware adaptation. This connection would situate the authors’ findings in a broader context of uncertainty-aware regularization.\n- **Limited coverage of downstream tasks beyond VQA.** The study focuses on ImageNet-derived VQA formulations. It would strengthen the generality claim to evaluate on multimodal reasoning tasks (e.g., captioning, retrieval, grounded QA) where linguistic diversity is higher.\n- **Contributions framing.** The paper repeatedly emphasizes “no new method,” but this undersells the significance of the findings. Perhaps, a reframing toward “rethinking the design space of multimodal adaptation” could better reflect its contribution.\n\n**References:**\n\n[1] Derakhshani *et al.* \"Variational prompt tuning improves generalization of vision-language foundation models.\" Me-FoMo workshop, ICLR 2023.\n\n[2] Jha *et al.* \"CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models.\" NeurIPS 2024."}, "questions": {"value": "- How does the proposed recipe interact with adapter stacking or mixture-of-LoRA methods (e.g., MR-LoRA, O-LoRA)? Would the same stability hold when multiple tasks are blended dynamically?\n- In continual learning, how do your models behave under domain interference (e.g., conflicting visual distributions)? Would the same low learning rate recipe suffice without task identifiers?\n- Can you quantify the epistemic uncertainty or representation drift across fine-tuning steps to empirically validate that “forgetting is not capacity-limited”? (This could also help connect well with the VPT/CLAP4CLIP uncertainty-aware continual learning literature.)\n- Does the data-hybrid strategy work if the additional text data is synthetic (e.g., generated by GPT-4V)? This could test whether linguistic diversity or semantic content is the key factor."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YYmeAD3pz3", "forum": "WLSt5tIOSA", "replyto": "WLSt5tIOSA", "signatures": ["ICLR.cc/2026/Conference/Submission2538/Reviewer_4KR7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2538/Reviewer_4KR7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760882696762, "cdate": 1760882696762, "tmdate": 1762916272562, "mdate": 1762916272562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides experimental results to support that with appropriate regularization (low learning rate, constraining trainable params), VLMs prevent forgetting. Extensions to continual learning are shown, and practical guidelines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Paper considers an interesting evaluation protocol.\n- Base models considered are near state of the art.\n- Results on continual learning seem good."}, "weaknesses": {"value": "- Recent published works, see [1] and references therein, demonstrate somewhat contradictory results to this paper. Through extensive empirical analysis across four state-of-the-art vision-language models and five fine-tuning techniques, a strong linear relationship holds: tasks with greater in-domain performance improvements suffer from more pronounced out-of-domain degradation, with some parameter-efficient fine-tuning (PEFT) methods exhibiting severe forgetting. A predictive measure (IIMM) also has been derived to capture when a VLM is expected to suffer more or less from forgetting. Even figure 2 in the paper shows there is some degree of forgetting.\n\n[1] L. Niss et al, \"The Inter-Intra Modal Measure: A Predictive Lens on Fine-Tuning Outcomes in Vision-Language Models\", ICCV 2025, https://arxiv.org/abs/2407.15731\n\n- No theoretical justification for results.\n- Training recipes considered are well known existing works.\n- Limited novelty contributions. The takeaways are overstated."}, "questions": {"value": "- How does the degree of forgetting on off-target tasks correlate with on-target accuracy?\n- For fine tuning tasks that yield large improvements post-fine tuning, what is the forgetting accuracy on previous general tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0TKxUfyKIa", "forum": "WLSt5tIOSA", "replyto": "WLSt5tIOSA", "signatures": ["ICLR.cc/2026/Conference/Submission2538/Reviewer_yCvb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2538/Reviewer_yCvb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761083778963, "cdate": 1761083778963, "tmdate": 1762916271722, "mdate": 1762916271722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates catastrophic forgetting in the fine-tuning of vision–language models (VLMs) and shows that it can be largely mitigated through simple adjustments such as using smaller learning rates or limiting the number of trainable parameters. The authors introduce a 2×2 evaluation framework that systematically tests model performance across ID/OOD image and text settings. Their results confirm that these fine-tuning strategies effectively prevent forgetting in most cases, with the only notable failure occurring in the OOD-text / ID-image quadrant. Further analysis attributes this phenomenon to task-specific overfitting, which the authors mitigate using a data-hybrid training strategy that mixes diverse instruction data.Finally, the authors extend the fine-tuning strategies to continual learning setups and demonstrate the methods demonstrate strong performance among prior arts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper's presentation is both smooth and adequate, with experimental setups and core findings laid out nicely.\n* The paper provides strong and comprehensive empirical evidence showing that catastrophic forgetting in VLMs is not inevitable and can be effectively mitigated with simple fine-tuning strategies such as small learning rates or parameter-efficient updates.\n* The identification and in-depth analysis of the failure mode involving in-distribution images and out-of-distribution text are particularly insightful and offer an interesting understanding of task-specific overfitting within multimodal fine-tuning."}, "weaknesses": {"value": "* Although the study covers a wide range of models and datasets, its fine-tuning experiments are largely restricted to classification-style tasks, which limits the generality of the conclusions. Such tasks may be relatively easy for modern VLMs, potentially requiring only minimal parameter updates to reach high accuracy. As shown in Figure 2, the model already achieves strong in-distribution performance even before fine-tuning, suggesting that the observed “robustness” may partly reflect task simplicity rather than intrinsic resistance to forgetting.\n\n* The core finding that smaller learning rates or restricted parameter updates reduce forgetting—is not conceptually novel. For example, in [1] (which the paper cites), the author notice that \"forgetting increases as a shifted power law in the number of parameters\nfine-tuned and the number of update steps.\" and in [2], the authors find that using LoRA can mitigate catastrophic forgetting or stop (if limiting the training budget) in a lot of settings including instruction-tuning setups. Apart from these, modulating learning rate to mitigate catastrophic forgetting is also not a new topic to the continual learning domain. \n\n[1] Kalajdzievski; Scaling Laws for Forgetting When Fine-Tuning Large Language Models.\n[2] Biderman et al; LoRA Learns Less and Forgets Less."}, "questions": {"value": "It is somewhat surprising and counterintuitive that in the continual learning experiments, the proposed methods achieve better performance without a replay buffer than with one, whereas all other baselines experience a performance drop under the same condition. Could the authors clarify the underlying reason for this behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qSUr2CVuVS", "forum": "WLSt5tIOSA", "replyto": "WLSt5tIOSA", "signatures": ["ICLR.cc/2026/Conference/Submission2538/Reviewer_W4xs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2538/Reviewer_W4xs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776537841, "cdate": 1761776537841, "tmdate": 1762916271187, "mdate": 1762916271187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that catastrophic forgetting in VLM fine-tuning has been overstated. Using a 2×2 evaluation framework (ID/OOD images × ID/OOD text), the authors show that simple methods, such as low learning rates (1e-6) or LoRA, can effectively preserve zero-shot capabilities while fine-tuning on ImageNet-VQA. They identify one failure mode: ID images with OOD text cause \"task-specific overfitting\" where models memorize templates instead of following instructions, which can be fixed by mixing diverse training data. In continual learning experiments, their simple approaches match or beat specialized methods, especially without replay buffers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **The 2×2 evaluation framework is genuinely useful.** Separately varying image and text distributions provides much finer-grained insights than typical \"ID vs OOD\" evaluations. \n- **The systematic ablations are thorough and well-designed.** Table 1 shows that most regularization strategies fall within a ±3pp margin, which is exactly the kind of detailed analysis needed to support the claims. They also test across different model sizes (3B, 7B), architectures (Qwen2.5-VL, LLaVA), and rare domains (microscopy, surgery).\n- **The practical value is real.** If the findings hold more generally, this could save researchers and practitioners a lot of unnecessary complexity. The message \"just use a low learning rate\" is actionable in a way that \"implement this 10-component architecture\" is not."}, "weaknesses": {"value": "1.  **Limitations of classification tasks**: The paper's central conclusions (Findings I-III) are primarily validated on a single-task, classification-style VQA setup (ImageNet-VQA). While the continual learning experiments in Section 5 introduce some diversity, they do not sufficiently demonstrate that the proposed simple recipe generalizes to *single-task* fine-tuning on other complex, non-classification tasks (e.g., captioning, reasoning). This casts doubt on the universality of the main claim.\n\n2.  **Conclusion can't be verified**: The attribution of the OODᵀ–IDᴵ failure to \"task-specific overfitting\" in the language module, while supported by a distractor experiment, remains somewhat indirect. The analysis does not fully rule out co-occurring factors, such as a potential degradation or narrowing of the visual representations for non-classification features. A more direct analysis (e.g., probing visual features or conducting ablations with a frozen vision encoder) would solidify this point.\n\n3.  **Lack of Mechanistic Explanation**: The paper provides a strong empirical study but offers little insight into the *underlying reasons* for VLMs' apparent robustness compared to LLMs. Is it the inherent stability of the pre-trained vision encoder, the regularizing effect of the projector, or the multi-modal nature of the tasks themselves? \n\n4.  **Insufficient Discussion of Related Work**\n    The paper lacks a thorough discussion of several contemporary works that directly address catastrophic forgetting in multimodal models. For instance:\n    - **\"Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models\"** (ICML 2024) \n    - **\"LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models\"** (CVPR 2025)\n    - **\"Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs\"** (Arxiv, 2025)"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "I5kSanVjSL", "forum": "WLSt5tIOSA", "replyto": "WLSt5tIOSA", "signatures": ["ICLR.cc/2026/Conference/Submission2538/Reviewer_JeX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2538/Reviewer_JeX7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762172798143, "cdate": 1762172798143, "tmdate": 1762916270977, "mdate": 1762916270977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}