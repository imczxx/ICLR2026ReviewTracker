{"id": "4vXNp4wujY", "number": 6570, "cdate": 1757989275199, "mdate": 1759897907415, "content": {"title": "AMS-Quant: Adaptive Mantissa Sharing for Floating-Point Quantization", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various kinds of tasks, while the billion or even trillion parameters bring storage and efficiency bottlenecks for inference. Quantization, particularly floating-point quantization, is known to be capable of speeding up LLM inference by reducing memory footprint and data movement during the inference process. For the first time, we advance the floating-point quantization exploration from integer bit-widths to non-integer bit-widths, namely AMS-Quant, to further approach the quantization sweet spot. AMS-Quant incorporates two novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing, which groups k quantized weights and lets them share the least significant mantissa bit, allowing us to further approach the minimum quantization bit-width without accuracy loss. (2) It introduces Adaptive Searching, which employs an offline optimization strategy to minimize the accuracy degradation introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA Linear kernels, which translates memory savings into wall-clock latency reduction by reducing memory access. Extensive experiments on large-scale datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3 and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16 inference (2.8$\\times$ and 3.2$\\times$), with negligible accuracy loss.", "tldr": "we proposed a novel weight-only floating-points quantization technique to achieve overall non-integer quantization bit-width  e.g., FP5.3, by encoding multiple quantized weight parameters with Adaptive Mantissa-bit Sharing (AMS)", "keywords": ["Large Language Model", "Quantization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/350ed263334e32ea236d0d054432dbd04017de13.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AMS-Quant, a novel floating-point quantization method that enables non-integer bit-width representations for LLMs. The method is based on two key components:\n- Mantissa-bit Sharing: groups of k quantized weights share their least significant mantissa bit, reducing the effective bit-width.\n- Adaptive Searching: an offline optimization that selects the shared bit value minimizing mean squared error across the group.\n\n\nAuthors complement the algorithm with an efficient CUDA kernel implementation that restore quantized weights using bit-level operations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Idea is novel and timely. While existing works have explored sharing exponent bits or scaling factors, mantissa sharing is surprisingly unexplored. \n- The paper is aptly written and clearly organized.\n- The proposed adaptive searching for mantissa bits provides an effective mechanism to manage accuracy loss.\n- Authors provide substantial experimental results to validate their proposed method."}, "weaknesses": {"value": "- Authors did not report search time for adaptive mantissa sharing algorithm. While it is run offline, it is essential to ascertain the computational cost associated with the search process.\n- Non-integer bit-widths based on packing and unpacking bits cannot be integrated with inference frameworks like TensorRT. Likewise, deployment on other hardware devices like TPUs, NPUs etc may need extra effort."}, "questions": {"value": "- What is the time and compute cost for running adaptive mantissa search, especially for large models?\n- Can this method be combined with existing methods like GPTQ or AWQ with ease?\n- Can this method be extended to activation quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "010tKaXCZw", "forum": "4vXNp4wujY", "replyto": "4vXNp4wujY", "signatures": ["ICLR.cc/2026/Conference/Submission6570/Reviewer_k41G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6570/Reviewer_k41G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497320800, "cdate": 1761497320800, "tmdate": 1762918908185, "mdate": 1762918908185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AMS-Quant, a weight-only floating-point quantization framework for LLMs. It includes two major techniques, i.e., mantissa-bit sharing to boost compression and adaptive searching to reduce quantization error. A custom GPU kernel is developed to realize practical speedups. Experiments show that AMS-Quant achieves these gains with only minimal accuracy degradation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and the core ideas are clear to understand.\n2. The major contributions of this quantization framework are mantissa sharing and adaptive searching, which are both sound for improved floating-point quantization.\n3. The experimental results not only show the quantization accuracy but also the actual acceleration enabled by the proposed methods."}, "weaknesses": {"value": "1. This work is for weight-only quantization, if it can be further extended to an activation and weight quantization, more efficiency gains should be obtained.\n2. The mantissa-sharing operation is analogous to block floating-point quantization and micro-scaling formats (e.g., MXINT, MXFP), which share exponents instead. Additional experiments should be conducted to more thoroughly validate the superiority of the proposed methods.\n3. Mantissa sharing is applied along the input-channel dimension of the weight tensor. However, this choice is less cache- and memory-friendly than sharing along the output-channel dimension. While the paper justifies the design by noting that activation outliers are typically aligned with input channels, it may be worthwhile to incorporate a small calibration set into the mantissa-sharing and adaptive search procedure to reduce quantization loss, while enabling mantissa sharing along the output-channel dimension for more efficient memory access.\n\nOverall, this is a good-quality paper and I will raise my score if my concerns are well addressed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9sNSxnlXvx", "forum": "4vXNp4wujY", "replyto": "4vXNp4wujY", "signatures": ["ICLR.cc/2026/Conference/Submission6570/Reviewer_5aq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6570/Reviewer_5aq1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810776216, "cdate": 1761810776216, "tmdate": 1762918907651, "mdate": 1762918907651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on weight-only quantization by leveraging mantissa-bit sharing, resulting in non-integer bit widths for individual weight elements. It also proposes an adaptive search strategy to determine the optimal shared mantissa value. Based on these techniques and a bit-slicing packing strategy, the authors implement CUDA linear kernels and demonstrate improved latency through more efficient memory access."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "They implement CUDA kernels using an efficient restoration algorithm based on the bit-slicing technique."}, "weaknesses": {"value": "A major limitation of this paper is the lack of comparison with existing methods. Numerous prior works achieve higher (even 2-bit in QuIP or AQLM) compression rates while maintaining comparable accuracy. Without extensive benchmarking against these methods, it is difficult to evaluate the advantages of the proposed strategy. Furthermore, many weight-activation quantization (e.g., Quarot) approaches exist that effectively address KV cache and high-precision computation overhead in attention mechanisms, which makes the proposed method less compelling.\n\nAdditionally, the rationale for mantissa-bit sharing is not clearly justified. The method - merely sharing the least significant bits of the mantissa and selecting the optimal value via brute-force search - appears simplistic and does not convincingly constitute a novel contribution. From a practical perspective, applying techniques that reduce additional 1 or 2 bits more substantially might be more impactful than the minor gains achieved through mantissa sharing.\n\nRegarding the packaging and restoration techniques, the approach shows limited novelty compared to the TC-FPx framework and appears largely as an engineering implementation. Finally, the figures do not effectively convey the core concepts and seem to underutilize the available space, reducing their clarity and impact."}, "questions": {"value": "What are the advantages of using non-integer bit widths and the mantissa-sharing strategy compared to existing quantization methods? Please clarify how this approach provides benefits in terms of accuracy, compression, or computational efficiency relative to prior work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CHAGtNuL4i", "forum": "4vXNp4wujY", "replyto": "4vXNp4wujY", "signatures": ["ICLR.cc/2026/Conference/Submission6570/Reviewer_j2b7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6570/Reviewer_j2b7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891445257, "cdate": 1761891445257, "tmdate": 1762918907177, "mdate": 1762918907177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose mantissa-bit sharing, sharing the least significant mantissa bit across groups of k quantized weights, resulting in formats like FP5.33-e2m3 and FP4.25-e2m2 (1:3 / 1:4 sharing). Per group 1-bit search minimizies the quantization MSE. Further, they implement bit-packing + register level ops for restoring FP16 for CUDA linear kernels. Speedup over FP16 kernels are demonstrated. Finally, their FP5.3(e2m3) hits the sweet-spot across IFEval GSM8k and MMLU on 3 models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Simple to implement and leverage\n- design is sound, kernel also obtains a speedup in memory-bound settings at the kernel-level."}, "weaknesses": {"value": "- baselines seem to be limited, AWQ.GPTQ, NF4 should be discussed.\n- Are the speedups only kernel-level? it is very important to see full decode latency,  specifically is that a speedup for the model, or for just the kernel in Figure 6? I think its the latter so it might be better to not label them as real model speedups."}, "questions": {"value": "- Beyond notes on the weaknesses, it would be interesting to see an ablation of increasing k."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p3z9zAmqra", "forum": "4vXNp4wujY", "replyto": "4vXNp4wujY", "signatures": ["ICLR.cc/2026/Conference/Submission6570/Reviewer_VgPA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6570/Reviewer_VgPA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944566826, "cdate": 1761944566826, "tmdate": 1762918906846, "mdate": 1762918906846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}