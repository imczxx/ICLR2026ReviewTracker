{"id": "eDJsc3KXZC", "number": 7932, "cdate": 1758044367240, "mdate": 1759897821337, "content": {"title": "Black-box Detection of LLM-generated Text Using Generalized Jensen-Shannon Divergence", "abstract": "We study black-box detection of machine-generated text under practical constraints: the scoring model (proxy LM) may mismatch the unknown source model, and per-input contrastive generation is costly. We propose SurpMark, a reference-based detector that summarizes a passage by the dynamics of its token surprisals. SurpMark quantizes surprisals into interpretable states, estimates a state-transition matrix for the test text, and scores it via a generalized Jensen–Shannon (GJS) gap between the test transitions and two fixed references (human vs. machine) built once from historical corpora. We prove a principled discretization criterion and establish the asymptotic normality of the decision statistic. Empirically, across multiple datasets, source models, and scenarios, SurpMark consistently matches or surpasses baselines; our experiments corroborate the statistic’s asymptotic normality, and ablations validate the effectiveness of the proposed discretization.", "tldr": "We propose a reference-based detector using surprisal-state Markov transitions and GJS score to flag AI text—fast, accurate, no regeneration.", "keywords": ["Text Detection", "Likelihood-free Hypothesis Testing", "Generalized Jensen-Shannon Divergence"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a7fcc3c32d5c01c289bd763c32b2608f62faba2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is well written and interesting. It seeks to improve the common approach of using model log-likelihood of a text as a statistic for discriminating between human and machine generated text. The proposed improvement is do as follows: 1) discretize log-likelihood by putting values of log-likelihood into bins, 2) use human and machine generated reference texts to compute approximate Markov transition probabilities from bin to bin for human text and machine text.3) Use generalized Jensen-Shannon divergence to classify whether a sequence looks more like it was generated from the human-reference Markov process or machine-reference Markov process. Use this as a classifier for human/machine generated text. This is an interesting and novel approach.\n\nSince I will be fairly direct in my criticism of the paper below, let me reiterate here that I think the essential idea of the paper is nice, and congratulate the authors on that."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The core idea of the paper is (I think) novel and interesting. It is a fairly simple approach and seems to improve upon the baselines it studies. It is a simple idea which is worthy of study. I am interested in the idea of using k-means clustering, as opposed to other methods, to group the set of next token probabilities.\n\nSome nice mathematics is done around the question of detecting which of two Markov processes generated a sequence."}, "weaknesses": {"value": "1. A lot of interesting mathematics is done, but I don't think this addresses the crucial questions of the effectiveness of the detector. For example, I think the mathematics all concerns the white box case, where machine texts are generated from the exact Markov transition probabilities used in the detector. Similarly, theorem 4.4 (asymptotic normality) is a nice piece of mathematics, but I think it risks giving false confidence in the test, e.g. because it ignores the fact that different language models have different statistics, and that different humans generate text in different ways (non-native speaker bias etc.), that neither humans nor machines generate language according to a one-step Markov process. I think there's a risk that the casual reader takes the very substantial mathematical content of this article as a mathematical proof of the effectiveness of the algorithm, which it isn't. \n\n2. A second point on the mathematics, which I raise for the authors but do not use in calibrating my score since I'm unable to fully defend it. Most of your mathematics concerns the question, 'Given two Markov processes P and Q over the same state space, and a sequence w generated either by P or Q, how can we determine the probability of w being generated by P vs generated by Q'. Given the ubiquity of Markov processes in science, I would be astonished if this question is not extremely well treated in the statistics literature already. There is no need to respond to this comment since I haven't justified it with evidence, it does not affect my score.\n\n3. I know (and dislike that) referees always request more baselines, but I don't think that your choice of baseline detectors is representative of the state of the art. Specifically, what your detector does nicely is use patterns in the structure of log-likelihood, rather than just average over the sequence of log-likelihood. But there are other detectors which do this, for example [Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood](https://aclanthology.org/2024.emnlp-main.564/) (Xu et al., EMNLP 2024)]. Binoculars makes an (apparently strong) claim to outperform Fast-detectGPT, so I think would be useful to include. There are probably many more of which I'm unaware. This is a really crucial point for me, your idea of spotting patterns in log-likelihood rather than averages of log-likelihood is not entirely new, and so I really need to see that you have compared your approach to all of the other methods which try to do this. \n\n4. There are plenty of ways that you could have discretized the space of token probabilities. Do you know whether your k-means clustering represents a sensible chocie? Does it outperform binning, for example, into k-bins which occur with roughly equal frequency in the reference texts?"}, "questions": {"value": "Minor comment: I don't think it's fair to characterize Fast-Detect GPT as requiring substantial compute. The novelty of their algorithm is that, rather than computing likelihood of hundreds of perturbations of the original text w_1...w_n, they use only (at time n) the likelihoods of different choices of token n, given the true values of w_1...w_{n-1}. This is information that you get almost for free when computing p(w_n|w_1..w_{n-1}), and so the compute time is comparable with that of just computing the likelihood of the sequence w_1...w_n.\n\n2. Your criticism of classifiers is that they must be retrained for different domains. Do you retrain your baseline Markov transition probabilities for different domains [Edit: I think I get an answer in section 5, that you do retrain for each, using 300 texts. I think you are saying that GPT2-Large is used to compute the machine transition-probabilities in each case, could I double check that this is true. In particular, can I make 100% sure that you do not use the generator model in computing transition-probabilities]? \n\nDo you have a robust claim that retraining these probabilities is easier than for the classifiers? Do you know how susceptible to domain shift your detector is (in circumstances where there is not a good corpus of domain texts to generate your transition probabilities]?\n\n3. (related to 2) On page 3 you mention that you discretize log-likelihoods using k-means clustering. This is the absolutely crucial bit of your algorithm, I would like to know at this point what data you are using to do this clustering. Similarly for the modelling as a Markov chain. In the following section you mention that 'reference texts' t_P and t_Q are used, are the reference texts specific to the language model? Specific to the domain of text generation? This is crucial information about what your algorithm does that shouldn't be buried in the appendix (and I couldn't even find it in the appendix).\n\n4. When we discuss algorithms to detect machine generated text, it is crucial to know how the text was generated. (e.g. Temperature/top-k/top-p?) These generation strategies have an enormous effect on the effectiveness of detectors (as noted in the work of Ippolito et al that you cite). I can't find any reference in your document to how text was generated, it is missing from section A3.1. \n\n5. Please include information on the length of human and machine texts in your experiments. Do you use the full length of human texts from the passages, and cap the machine text at a certain length? I suspect that FastDetect-GPT handles passages of different lengths very poorly, due to the way they normalise their statistic. Does your comparison with FastDetect-GPT hold up if, for example, you ensure that every machine and human text in your test set is exactly 150 tokens in length?\n\n6. If I understand correctly, your main experiments are run on collections of 150 texts. I'm not sure that you should report AUROC to 2dp when you have so few samples. \n\nMinor Comments: \n7. On the first page you distinguish between 'Global statistics' and 'distributional statistics'. I'm not sure I understand the distinction or the terminology. The likelihood statistic is computed by summing log likelihood along a sequence. The fast-detect statistic is computed by summing log-likelihood minus expected log likelihood along an orbit. In what sense is one 'global' and one 'distributional'. \n8. You claim (p3) to have built a 'likelihood free' hypothesis test. What do you mean by this? You are still using the sequence of (black box model) likelihoods as the primary input. Do you use it only to say that you don't have access to the underlying true human or model likelihoods? If so, this is true of every black box detector. Which is fine, but I misunderstood your wording to mean something stronger."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B14Dq98jhQ", "forum": "eDJsc3KXZC", "replyto": "eDJsc3KXZC", "signatures": ["ICLR.cc/2026/Conference/Submission7932/Reviewer_rzH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7932/Reviewer_rzH5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761131917764, "cdate": 1761131917764, "tmdate": 1762919955462, "mdate": 1762919955462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SurpMark, a black-box, reference-based detector for identifying LLM-generated text by measuring its statistical proximity to human and machine reference corpora using the Generalized Jensen-Shannon (GJS) divergence. The method first uses a proxy language model to compute the token surprisal sequence for both the reference texts (from human and machine sources) and the test text. A shared quantizer, derived from the reference surprisals via k-means clustering, is then used to discretize these continuous surprisal values into a finite number of interpretable states. The core of the method summarizes each text—references and test—by estimating its first-order Markov transition matrix over these discrete surprisal states, capturing local token dynamics. Finally, the test text is assigned a GJS score that measures the divergence between its transition pattern and those of the human and machine references, with the decision rule based on this differential score, effectively framing detection as a likelihood-free hypothesis test. The numerical experiment shows the efficiency of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe authors propose SurpMark, a novel reference-based detection framework that formulates the identification of LLM-generated text as a likelihood-free hypothesis test using Generalized Jensen-Shannon (GJS) divergence, eliminating the need for per-instance regeneration or classifier training.  \n2.\tA key strength of the method is its principled integration of token surprisal dynamics via discretized first-order Markov transition matrices, which captures local structural patterns in text while ensuring robustness to calibration mismatches between the proxy and source models.  \n3.\tThe paper provides a rigorous theoretical analysis justifying the choice of discretization bins through a bias-variance trade-off, proving the GJS statistic's equivalence to a normalized log-likelihood ratio, and establishing its asymptotic normality.  \n4.\tExtensive experiments across multiple datasets, LLMs, and attack scenarios—including paraphrasing—demonstrate strong and consistent performance, with ablation studies validating design choices and confirming theoretical predictions."}, "weaknesses": {"value": "1.\tThe experimental validation of the first-order Markov assumption in Figure 2(b) is potentially biased, as it compares models of different orders without clarifying whether higher-order models are provided with proportionally larger reference corpora to mitigate their severe data sparsity. Since the number of parameters grows exponentially with the order, higher-order models inherently suffer from greater statistical estimation error, including row-wise transition noise and missing transitions error, nder fixed reference size, making their lower performance likely an artifact of insufficient data rather than evidence of inferior modeling capability; the authors should provide details on the reference data size used for each model order to ensure a reasonable and unbiased comparison.\n2.\tThe definition of the symbol $ N $ in the paper appears to be inconsistent. In the theoretical analysis (e.g., Theorem 4.2), $ N $ should denote the total number of reference transitions, as the statistical error bounds—such as row-wise transition noise and missing transition error—depend on this aggregate quantity. However, on line 245, $ N $ is defined as the length of a single reference library, and in Algorithm 1 (line 5), it is described as the \"total number of transitions,\" which may lead to ambiguity. Furthermore, in the experimental results (e.g., Figures 4 and 5), the \"reference length\" is varied at small scales, suggesting a per-sequence interpretation. The authors should explicitly clarify whether $ N $ refers to the length per sequence or the total aggregated number of transitions across all sequences to avoid confusion in both theoretical and empirical contexts.\n3.\tThe paper positions kernel-based reference methods like R-Detect[A] and MMD-MP[B] as requiring costly kernel training and permutation testing, yet fails to include a direct and comprehensive comparison in the main results, relegating it to an appendix without full experimental transparency. Moreover, the emphasis on the need for training kernel parameters in these methods is misleading, as their offline training costs can be amortized over large-scale deployment. The authors should provide a head-to-head comparison with R-Detect under identical reference corpus sizes and test conditions in the main tables, along with empirical runtime measurements to substantiate claims about computational efficiency.\n4.\tThe paper does not sufficiently evaluate the robustness of SurpMark in scenarios where test samples exhibit significant distributional shifts relative to the fixed reference corpora, which may limit its reliability in real-world deployments with heterogeneous input streams. Given that the method relies on discretized Markov dynamics different from R-Detect[A] and MMD-MP[B], the authors should provide additional experiments assessing performance under domain mismatch or dynamic data drift to better validate its practical generalizability.\n5.\tThe paper's figures, while visually informative, are not provided in vector format, resulting in noticeable blurriness—particularly in dense plots such as Figure 4 and Figure 5. Several figures suffer from suboptimal formatting choices: Figure 7 uses an overly small x-axis font, and the legend in Figure 5(c) is too small to be easily readable. Moreover, the tables in the appendix lack typographic emphasis.\n6.\tFor practical online detection scenarios, the latency for detecting a single text instance is a key concern. The paper claims that avoiding expensive per-instance regeneration is one of its main motivations, which is reasonable in principle. However, Figure 6 shows that SurpMark only begins to amortize its cost advantage after processing around 300 samples in batch mode. This implies that for single-instance detection, SurpMark is likely slower than Fast-DetectGPT. Therefore, the paper’s central motivation—achieving superior computational efficiency over regeneration-based methods—is not convincingly substantiated by the current experimental design.\n7.\tIn Section 3, the paper cites Khandelwal et al. (2018)[C] to justify that “LM predictions rely mainly on short range context,” thereby motivating the use of a first-order Markov model. This claim is no longer persuasive in the era of large-scale LLMs such as LLaMA 3 and GPT-5, whose predictions depend heavily on extended context. The authors should consider weakening or removing this outdated reference and instead emphasize the empirical evidence presented in Figure 2(b) as the primary justification for adopting a first-order approximation."}, "questions": {"value": "1.\tThe paper does not discuss how to set the tunable threshold τ, which is crucial for practical deployment. Could the authors clarify how τ should be chosen (e.g., via validation or theoretical guidance) and provide empirical analysis on its impact on detection performance?\n2.\tThe empirical validation of the asymptotic normality of the GJS statistic, while visually supported by histograms in Figure 2(c) and Figure 7, would be strengthened with formal statistical tests for normality. Could the authors provide quantitative assessments such as Shapiro-Wilk test results or other normality metrics to more rigorously confirm the distributional assumptions?\n3.\tThe paper does not specify the length distribution of the human and machine reference texts used to build the transition matrices, nor does it clarify whether short or low-quality references were filtered out during preprocessing. Could the authors provide details on how reference text lengths were controlled and whether a minimum length threshold was applied to ensure reliable estimation of the Markov transition statistics?\n4.\tTable 1 shows that the proposed method exhibits a considerable advantage on closed-source models, while Table 2 indicates much smaller gains on open-source ones. This is a noteworthy phenomenon. Could there be deeper underlying factors—such as tokenization mismatches, proxy-model bias, or domain differences—that explain this disparity?\n5.\tSection 5.2, labeled as “Ablation Studies” is more accurately a hyperparameter sensitivity analysis. A proper ablation study should explicitly answer the following questions:\n(1)\tNecessity of the Markov chain: If the transition matrix is removed and only the stationary distribution of surprisal states (i.e., the 1-gram distribution, a $k$-dimensional vector) is used as a feature, how much does performance degrade?\n(2)\tNecessity of the GJS divergence: If the Generalized Jensen–Shannon (GJS) divergence is replaced by a simpler distance metric such as the $L_1$ or $L_2$ norm, what is the performance impact?\n(3)\tNecessity of k-means quantization: If k-means clustering is replaced by simpler schemes such as equal-width or equal-mass binning, how does performance change? \n\n[A] Deep Kernel Relative Test for Machine-generated Text Detection. ICLR 2025.\n\n[B] Detecting machine-generated texts by multi-population aware optimization for maximum mean discrepancy. ICLR 2024.\n\n[C] Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5JZvRYO4vI", "forum": "eDJsc3KXZC", "replyto": "eDJsc3KXZC", "signatures": ["ICLR.cc/2026/Conference/Submission7932/Reviewer_EQL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7932/Reviewer_EQL7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746728142, "cdate": 1761746728142, "tmdate": 1762919954966, "mdate": 1762919954966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SurpMark, a novel, reference-based method for detecting machine-generated text. Instead of relying on brittle, absolute surprisal values, it models the dynamics of surprisal. This paper suggests quantizing surprisals into $k$ states and modeling the text as a first-order Markov chain of these states."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors propose a novel framework to learn state transitions of token surprisal as a first-order Markov chain. It cleverly abstracts the detection signal away from the specific tokens to state transitions internally within LLMs.\n\nThe propositions defined in the paper are also very extensive and highly appreciated."}, "weaknesses": {"value": "- The theoretical justification for the core framework (i.e. L178, which cites Khandelwal et al.), which relies on a first-order Markov chain assumption, is outdated. The paper cited only discusses older architectures such as LSTMs, and it does not align with the assumption stated here with LLMs.\n-  In Theorem 4.2, the assumption $\\pi_{\\min} \\gtrsim 1/k$ appears questionable. The surprisal distribution is rarely uniform in practice, making this assumption unrealistic and invalidating the proof in the Appendix. \n- This indirectly casts doubt on the validity of the subsequent theorem regarding the optimal $k$. The authors mention that they compute this theoretical optimum and then fine-tune around it; however, it would be helpful to report whether the experimentally observed optimum diverges significantly from the theoretical value. (Some later experiments in Section 5.2 do show some divergence.)\n- The choice of K-means clustering requires an ablation study. K-means is sensitive to outliers, which is particularly problematic in the stated settings. Additionally, are the interpretive labels such as \"Very predictable\" validated or grounded in any sort of analysis or were these labels assigned by the authors?\n- The method relies on a Markov model to enhance detection - which is very prone to adversarial attacks (apart from just paraphrasing). It would be valuable to evaluate the robustness of this framework under simple prompt-engineered adversarial attacks."}, "questions": {"value": "- Although mathematically sound, can the authors intuitively explain the GJS divergence equation (above Equation 2)?\n\nAll the questions and suggestions have been listed in the weaknesses. I am willing to increase my score if the authors address these concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lKF4wuD0FF", "forum": "eDJsc3KXZC", "replyto": "eDJsc3KXZC", "signatures": ["ICLR.cc/2026/Conference/Submission7932/Reviewer_ex8a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7932/Reviewer_ex8a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771627090, "cdate": 1761771627090, "tmdate": 1762919954564, "mdate": 1762919954564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SurpShark, where next-token log-likelihoods of a text under a proxy model are collected (surprisals), quantized and modeled as Markov chains. By comparing the transition kernel between a reference collection of texts and a testing text, SurpShark effectively detects whether the testing text is machine-generated. The paper also provides theoretical justification for the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation of the methodology of SurpMark (Section 3) is straigtforward and easy to understand, with good discussions and intuitions to drive the construction. \n2. The main experiment results show significant improvement of SurpMark over existing methods. \n3. Detailed and extensive ablations are performed to understand the effects of hyperparameter k, reference length, etc. on model performance, backing intuition behind the method and strongly supporting its effectiveness."}, "weaknesses": {"value": "1. My biggest problem with the paper is the theoretical section, which in my current opinion greatly weakens the paper because it: a) is founded on a false assumption, b) does not provide theorems that go beyond what's readily intuitive, and c) is loosely connected to experiments. Unless there's some gross misunderstanding on my part, I cannot comfortably recommend acceptance given the current state of how the theory is presented, especially since the results are presented as a main contribution. See below for detailed justifications for these arguments: \n\n    a) The theory section bases its analysis on a single false assumption: that there is an underlying Markov transition kernel for the surprisal sequences (Page 5, Line 216). Realistically, the surprisal sequence follows a partially observed Markov chain, where the underlying transition is powered by the ground truth generation process $P(x_n | [x_t]_{1}^{n-1})$, and a hidden state is the collection of texts up to a certain token. It is fairly unreasonable to assume surprisals can be simplified into a Markov process, since by themselves the surprisals hide significant semantic information. I personally would allocate SurpMark's superior performance to its first-order **approximation** of the underlying multi-order transition kernel, and that this approximation is sufficient in differentiating between texts generated from machines vs. humans. Related to this, the authors attempted to justify this first-order choice empirically by Figure 2(b) and the argument \"The first-order Markov assumption suits our setting because LM predictions rely mainly on short range context (Khandelwal et al. (2018)).\" on Lines 177-178, but the figure has no experimental details attached as far as I can find (it's also very questionable why the AUROC would decrease as the order increases), and the quoted 2018 paper does not support first-order assumptions, rather that shallow contexts are more \"sharply distinguished\". \n\n    b,c) For discussion's sake let's say here the first-order Markov assumptions are true. In Section 4.2, the theoretical suggestion for the choice of k has a constant term with unknown magnitude from both Proposition 4.1 and Theorem 4.2, reducing the reliability of this theoretical suggestion $k^*$. Just to be clear, I'm not arguing that the whole analysis is pointless; I think it provides a good connection between theory and empirics regardless, but I would suggest not framing the theorem's prediction as a good basis for empirical studies (as in Section 5.2), but a validation or sanity check that the intuitions behind the method is correct (supported by the optimal k values in Figure 4(a)). In Section 4.3, the main result is the asymptotic normality of $\\Delta GJS_n$, which despite the technical difficulty is quite straightforward. It would be better if the exact convergence terms (namely $sigma_H^2$) can be analyzed and compared to empirical results. More importantly, in my view these technically impressive results unfortunately do not answer the most important question: can first-order Markov chain approximate the underlying partially observed Markov chain? \n\n2. There are some other minor writing issues, see Questions 2-3."}, "questions": {"value": "Q1. Can you explain how you obtained Figure 2(b) and why the performance gets worse with larger order? \n\nQ2. The definition of the surprisal sequence goes from text t to token sequence x. To clarify, shouldn't x be the exact same as t? Saying the proxy model $F_{\\theta}$ performs inference make it seem like x is obtained as a rollout. \n\nQ3. The text in Lines 198-204 is a bit rough. The main issue is that $\\Delta GJS_n$ is referenced before defined. \n\nQ4. [**important**] Can you discuss the assumption of first-order Markov chain, both its theoretical and empirical implications? \n\nQ5. For the optimal $k^\\*$ determined through Section 4.2, how would you handle the constant C present in the theories when applying the suggested $k^\\*$ to empirical studies? \n\nQ6. What can be implied quantatively by Theorem 4.4 beyond a qualitive assessment of asymptotic normality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uRvFr70K7E", "forum": "eDJsc3KXZC", "replyto": "eDJsc3KXZC", "signatures": ["ICLR.cc/2026/Conference/Submission7932/Reviewer_gbAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7932/Reviewer_gbAS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986197224, "cdate": 1761986197224, "tmdate": 1762919953960, "mdate": 1762919953960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}