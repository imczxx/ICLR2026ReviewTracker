{"id": "Hs4WbkJqsm", "number": 21065, "cdate": 1758313342874, "mdate": 1759896943916, "content": {"title": "Wef-GNN: A Generalizable Graph Neural Network for Crystalline Material Property Prediction", "abstract": "Graph neural networks (GNNs) have shown great promise for predicting properties of crystalline solids. However, existing models struggle to generalize across crystals of varying sizes, and there is a lack of high-fidelity $\\textit{ab initio}$ training data. Here, Wef-GNN addresses the problem of generalizability by introducing a multi-head temporal attention mechanism in the graph update function and a crystalline graph representation scheme that is more size-agnostic compared to the traditional primitive unit cell-based graph representation. Further, it was found that a single Wef-GNN layer can be recycled for all graph convolution steps without considerable loss in accuracy; this leads to deep receptive fields without additional parameters. Wef-GNN outperforms all prior models in a standard band gap prediction benchmark while having much fewer parameters. To address the challenge of high quality $\\textit{ab initio}$ training data, a high-fidelity dataset was curated by performing 10,522 high-accuracy Density Functional Theory (DFT) calculations. Wef-GNN was pre-trained on a standard large dataset of lower-accuracy DFT calculations then fine-tuned with the high-accuracy DFT dataset. The resulting model matches experimental band-gap values much better than other GNNs, and even outperforms the underlying low-accuracy DFT calculations.", "tldr": "A GNN architecture and training scheme that enables efficient and generalizable crystalline material property prediction", "keywords": ["Graph Neural Network", "Density Functional Theory", "Material Science", "AI for Science"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7e56d5ef4972ef77160f374f13e43dab1185b24.pdf", "supplementary_material": "/attachment/573ef28aaf71fd4113054d77c146acaae25c8693.zip"}, "replies": [{"content": {"summary": {"value": "This paper builds a GNN and demonstrates it on bandgap prediction. There are three mentioned contributions:\n\n- A new multi-headed temporal attention mechanism which is proposed to improve treatment of different structure sizes\n- An empirical observation that the same weights could be re-purposed across message passing layers\n- Training on low- and fine-tuning on high-fidelity data"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "This paper is very thin. Some of the space is spent on fairly simple and well-known effects in the materials modelling community, for example using a cutoff radius to connect a graph neural network. \n\nThe background section of this paper is outdated. A wealth of advanced MLIPs have appeared in the past three years, trained on vast ranges of high-fidelity data. Many of the claims made in the background section of this paper no longer hold.  \n\nTraining on low-fidelity data and finetuning on high-fidelity data is not new. The empirical finding that the weights can be recycled across multiple layers is mildly interesting, but even at empirical level it is not well explored. \n\nThere are extremely few results (Table 1) and no ablation studies of the architecture.\n\nI think this work is suitable for a workshop in order to further develop the core ideas."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9N5IDb1XLF", "forum": "Hs4WbkJqsm", "replyto": "Hs4WbkJqsm", "signatures": ["ICLR.cc/2026/Conference/Submission21065/Reviewer_eowe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21065/Reviewer_eowe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760831066317, "cdate": 1760831066317, "tmdate": 1762940636692, "mdate": 1762940636692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds a GNN and demonstrates it on bandgap prediction. There are three mentioned contributions:\n\n- A new multi-headed temporal attention mechanism which is proposed to improve treatment of different structure sizes\n- An empirical observation that the same weights could be re-purposed across message passing layers\n- Training on low- and fine-tuning on high-fidelity data\n\nEdit: I acknowledge the author's response. At best, this paper seems to be a placeholder. Please submit a completed papers for review  in the future, as according to ICLR policy, new experiments added to the paper during the review process should not substantially change its content."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "This paper is very thin. Some of the space is spent on fairly simple and well-known effects in the materials modelling community, for example using a cutoff radius to connect a graph neural network. \n\nThe background section of this paper is outdated. A wealth of advanced MLIPs have appeared in the past three years, trained on vast ranges of high-fidelity data. Many of the claims made in the background section of this paper no longer hold.  \n\nTraining on low-fidelity data and finetuning on high-fidelity data is not new. The empirical finding that the weights can be recycled across multiple layers is mildly interesting, but even at empirical level it is not well explored. \n\nThere are extremely few results (Table 1) and no ablation studies of the architecture.\n\nI think this work is suitable for a workshop in order to further develop the core ideas."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9N5IDb1XLF", "forum": "Hs4WbkJqsm", "replyto": "Hs4WbkJqsm", "signatures": ["ICLR.cc/2026/Conference/Submission21065/Reviewer_eowe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21065/Reviewer_eowe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760831066317, "cdate": 1760831066317, "tmdate": 1763760183730, "mdate": 1763760183730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a GNN layer and a high-accuracy DFT materials bandgap dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper achieves state-of-the-art performance on band gap prediction accuracy on the MatBench benchmark, beating the previously best model by a substantial margin.\n\nIt is very surprising that the weight sharing across layers, i.e., the use of a fully recurrent graph neural network, does not lead to a performance drop. It would be good to further analyze this behavior. Would the same be true if the temporal attention-based update function were used in other MPNN models?"}, "weaknesses": {"value": "The suggestion of going from primitive unit cells to conventional unit cells to improve generalizability is unclear. If periodic graphs are initialized correctly, there is no difference in node updates and node embeddings between a primitive unit cell and a conventional one, as the underlying periodic graph, including all individual atom environments, is exactly the same. Thus, the graph representation after readout and the prediction are identical. If this is different for WefGNN, then the authors should provide more information about this.\nFurthermore, the argument that this helps in generalization to larger unit cells also seems misleading. Generalization in ML is defined on in-distribution data, not out-of-distribution data. There is no guarantee for any performance on out-of-distribution data. Simply increasing the unit cell size periodically (the materials' properties are invariant to this) without adding any additional materials with actually larger primitive unit cells to the training data will not increase the performance on materials with large primitive unit cells.\n\nThe WefGNN model is only benchmarked on one of the tasks in MatBench. What about all other tasks? \n\nSection 5 (hybrid training) is interesting but not really relevant for a machine learning audience. It introduces a new dataset with more accurate DFT calculations, trains (or fine-tunes) the WefGNN model on this dataset, and shows that this leads to better agreement with experimental data. This is not surprising, as the newly generated DFT data has higher agreement with experimental data. The test set error of Model B vs. the HSEE06 values is not reported and should be added. \n\nOverall, the paper is lacking a lot of analysis, further benchmarks, ablations, and also some basic information about the model itself."}, "questions": {"value": "- Why was WefGNN only benchmarked on band gaps?\n- Please show an ablation study that supports that not using primitive unit cells actually improves model performance\n- What is the aspect in Algorithm 1 that differentiates WefGNN from other GNN models? The first loop just seems to be a way to preprocess the geometry to find edges, and the second part is a conventional aggregation, updat,e and readout cycle. Please provide further ablation studies of your components (temporal attention, ...) to demonstrate the effect of each of the design choices.\n- What are the initial feature vectors for nodes and edges? How is the geometry of the unit cell used? Is the model invariant to rotations/translations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "X46CYhG043", "forum": "Hs4WbkJqsm", "replyto": "Hs4WbkJqsm", "signatures": ["ICLR.cc/2026/Conference/Submission21065/Reviewer_eAnW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21065/Reviewer_eAnW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761411549960, "cdate": 1761411549960, "tmdate": 1762940636232, "mdate": 1762940636232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Wef-GNN, a graph neural network designed for predicting crystalline material properties with a focus on improving generalizability across crystals of varying sizes and achieving higher accuracy than traditional PBE-level DFT datasets. The proposed model incorporates several key innovations: a multi-head temporal attention mechanism in the update step that enables each node to attend to its historical representations, parameter recycling across message-passing layers to expand receptive fields without increasing the number of learnable parameters, and the use of conventional unit cells instead of primitive ones to enhance consistency and generalization across different crystal structures. Additionally, the authors propose a hybrid training strategy that involves pretraining the model on large-scale, lower-accuracy PBE datasets followed by fine-tuning on a smaller, high-fidelity HSE06 dataset. Through this combination of architectural and training improvements, Wef-GNN achieves a mean absolute error (MAE) of 0.117 eV on the Matbench band-gap prediction task, surpassing existing models while using significantly fewer parameters."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors clearly identify two limitations in existing GNN-based materials models — lack of generalization and limited accuracy due to low-fidelity DFT datasets.\n- Introducing temporal attention within GNNs for crystalline materials and the weight recycling strategy is interesting and computationally efficient.\n- Code and dataset availability are mentioned, which aligns with ICLR’s reproducibility requirements.\n- Achieves state-of-the-art performance (0.117 eV MAE) on Matbench Benchmark."}, "weaknesses": {"value": "- The paper shows strong empirical results but lacks theoretical explanation or ablation to justify why temporal attention or weight recycling improves generalization.\n\n\n- Most design elements—attention, message passing, and parameter sharing—are already well-established; their combination is effective but not fundamentally novel.\n\n\n- Ablation study comparing temporal attention vs. static attention is missing, leaving the claimed benefit unsubstantiated.\n\n\n- The paper reads more like a well-structured technical report than a concise ICLR-style paper. The submission appears incomplete, ending at around 6.5 pages with large images, whereas ICLR papers typically utilize the full 9-page limit."}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XnVykMFFI8", "forum": "Hs4WbkJqsm", "replyto": "Hs4WbkJqsm", "signatures": ["ICLR.cc/2026/Conference/Submission21065/Reviewer_d7FY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21065/Reviewer_d7FY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905225091, "cdate": 1761905225091, "tmdate": 1762940635791, "mdate": 1762940635791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Wef-GNN: A Generalizable Graph Neural Network for Crystalline Material Property Prediction” presents a lightweight, size-agnostic GNN architecture for predicting material properties, particularly band gaps. Wef-GNN integrates a multi-head temporal attention mechanism, recycled GNN layers, and a conventional-cell representation to enhance generalizability across crystals of varying sizes. The experimental results demonstrate strong and promising performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes Wef-GNN, a graph attention–based model that incorporates a multi-head temporal attention mechanism, GNN layer recycling, and a novel graph representation scheme for crystal structures.\nThis work demonstrates promising results for the band gap property through the introduction of a novel multi-head attention mechanism within Wef-GNN."}, "weaknesses": {"value": "The GNN models commonly used for crystalline materials, such as Matformer [1], and PotNet [2], are not referenced in the paper. Please include these pioneering works in the related work section.\nThis paper notes that the MP and OQMD datasets suffer from DFT-induced error bias. However, recent studies such as CrysDiff [3] and CrysGNN [4] have demonstrated that pretraining GNNs can effectively mitigate this bias by incorporating a small amount of experimental data alongside DFT-generated data during training. Consequently, the necessity of creating a new HSE dataset is questionable. Furthermore, these techniques are applicable to a wide range of material properties, not just band gap prediction.\nHere, multi-head attention is used for aggregation. However, transformer-based methods are now widely adopted for crystal property prediction. Why did the authors choose to use the older graph attention variant instead? It would be helpful to include an ablation study comparing the effectiveness of transformers versus graph attention for this specific task. Moreover, the results section currently lacks any ablation analysis. Additionally, the meaning of “Wef” in the model name Wef-GNN is unclear and should be clarified.\nCould the authors provide statistics on how many materials contain fewer than two atoms? Since crystals with fewer than two atoms are chemically insignificant, this raises the question of why varying the number of message-passing layers is necessary.\nSince this paper focuses solely on band gap prediction, the authors should also evaluate other key material properties—such as formation energy, total energy, Ehull, shear modulus, and bulk modulus—to better demonstrate the generalizability of Wef-GNN. Moreover, they should validate their model on established benchmark datasets like JARVIS [5] and MP-2018. Additionally, although the paper introduces a new dataset, it lacks any analysis of its characteristics, such as the distribution or number of atoms per structure.\nFrom Table 1, it is evident that Wef-GNN performs well; however, the results remain incomplete since the authors do not compare it with current supervised state-of-the-art models such Matformer. Furthermore, given their use of pre-training, they should have also included comparisons with recent pre-trained models like CrysGNN and CrysDIFF. In addition, the paper appears unfinished, ending at 7.5 pages and lacking any ablation studies.\n\n\n\nReferences:\n\n[1] Yan, K., Liu, Y., Lin, Y. and Ji, S., 2022. Periodic graph transformers for crystal material property prediction. Advances in Neural Information Processing Systems, 35, pp.15066-15080.\n\n[2] Lin, Y., Yan, K., Luo, Y., Liu, Y., Qian, X. and Ji, S., 2023, July. Efficient approximations of complete interatomic potentials for crystal property prediction. In International conference on machine learning (pp. 21260-21287). PMLR.\n\n[3] Song, Z., Meng, Z. and King, I., 2024, March. A diffusion-based pre-training framework for crystal property prediction. In Proceedings of the AAAI Conference on Artificial Intelligence(Vol. 38, No. 8, pp. 8993-9001).\n\n[4] Das, K., Samanta, B., Goyal, P., Lee, S.C., Bhattacharjee, S. and Ganguly, N., 2023, June. Crysgnn: Distilling pre-trained knowledge to enhance property prediction for crystalline materials. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 7323-7331).\n\n[5] Choudhary, K., Garrity, K.F., Reid, A.C., DeCost, B., Biacchi, A.J., Hight Walker, A.R., Trautt, Z., Hattrick-Simpers, J., Kusne, A.G., Centrone, A. and Davydov, A., 2020. The joint automated repository for various integrated simulations (JARVIS) for data-driven materials design. npj computational materials, 6(1), p.173."}, "questions": {"value": "See the limitations"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "onjrwZlKuu", "forum": "Hs4WbkJqsm", "replyto": "Hs4WbkJqsm", "signatures": ["ICLR.cc/2026/Conference/Submission21065/Reviewer_GSV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21065/Reviewer_GSV2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091024534, "cdate": 1762091024534, "tmdate": 1762940635290, "mdate": 1762940635290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}