{"id": "atoLVj3fZY", "number": 24332, "cdate": 1758355575787, "mdate": 1759896770929, "content": {"title": "Spectral Annealing for Scalable Ising Model Optimization", "abstract": "Large-scale Ising optimization underlies numerous machine learning tasks from correlation clustering to feature selection. Yet existing approaches face a fundamental scalability-quality trade-off: exact methods guarantee optimal solutions but become computationally prohibitive at scale, while metaheuristics achieve scalability by sacrificing solution quality. Spectral relaxations offer a promising middle ground with efficient scaling, but suffer from a critical limitation‚Äîthey solve exactly one eigenvalue problem, constraining solution quality through lack of exploration.\nWe introduce \\textit{spectral annealing}, a systematic approach that transforms spectral methods from single-shot to exploration-based optimization while preserving computational efficiency.  Our method explores a continuous family of spectral relaxations $\\mathbf{N}_\\alpha = \\mathbf{D}_\\alpha^{-1/2} \\mathbf{A} \\mathbf{D}_\\alpha^{-1/2}$ parameterized by $\\alpha \\in [0,1]$, interpolating from raw adjacency to full normalization. Our key innovation is a \\textit{diagonal predictor}, which enables efficient warm-starting by exploiting the algebraic structure of the $\\alpha$-parameterized family. We provide rigorous eigen-drift analysis for principled step-size control and GPU implementation with efficient parallelization techniques. Experiments on instances up to 262,000 spins demonstrate favorable time-quality trade-offs. Systematic $\\alpha$-exploration provides 2-5√ó quality improvements over single-parameter spectral methods while maintaining computational efficiency.", "tldr": "Smooth transition of spectral partitioning for scalable Ising model solvers, achieving the efficiency of spectral methods with solution quality of expensive solvers", "keywords": ["Spectral Annealing", "Ising Model", "Combinatorial Optimization", "Pertubation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0cbef67ff876a9b8284eaba1ba6989f0abb49df8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The method tracks the top eigenvector of an Œ±-normalized matrix that interpolates from the raw adjacency to the fully normalized form. \nAfter finding each spectral solution it rounds to spins and applies local search. \nThey key idea is that rather than recomputing the spectral problem from scratch at every Œ±, it predicts a warm start after small Œ± change for finding the spectral solution, which reduces the cost of the computation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The approach is conceptually simple, a natural generalization, and appears effective.\nThe warm-start idea reduces the eigenvalue solve by a factor of roughly 3-10x.\nThe paper provides theoretical guarantees for the predictor and uses them to set safe step sizes.\nComplexity and memory usage are discussed.\nThere is discussion about making the GPU implementation efficient.\nThe benchmarks cover a range of solvers, including state-of-the-art heuristics of different types (MCMC and dynamics on relaxed states)."}, "weaknesses": {"value": "The benchmarking presentation could be clearer. Instead of showing heterogeneous gaps and runtimes across methods\n(which is hard to interpret when a heuristic has a better gap but worse time), it might be clearer use the gap achieved by the proposed method as a target and record the time each competitor heuristic needs to reach this target."}, "questions": {"value": "1) How are the heuristic parameters set on the large real-world instances (Table 1)? This can have a strong impact on performance. I understand that a key advantage of the spectral approach is minimal tuning, but please state clearly how defaults parameters of heuristics were chosen and why they are reasonable. For example, for simulated bifurcation: step size, damping, and time steps?\n\n2) For Table 2 on GSET, heterogeneous time and gap are hard to read and compare. A clearer presentation would be time-to-target. \nFor example, would it be clearer to use the gap found by the proposed spectral method as the reference and run each heuristic until it reaches this gap (or better), then report the times ?\n\n3) The same applies for Figure 3.\n\n4) How does numerical precision affect the spectral relaxation and the overall pipeline? This is important for the GPU implementation.\n\n5) If possible, put in bold the best results in tables (if it is possible to choose) to aid interpretation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SslCzSCCa2", "forum": "atoLVj3fZY", "replyto": "atoLVj3fZY", "signatures": ["ICLR.cc/2026/Conference/Submission24332/Reviewer_5i2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24332/Reviewer_5i2q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801841046, "cdate": 1761801841046, "tmdate": 1762943045951, "mdate": 1762943045951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a central limitation of spectral methods for Ising optimization: performance is highly sensitive to the matrix $M$ used in the relaxation. \nTo address this sensitivity, the authors construct an annealing (homotopy) path that continuously interpolates between the original adjacency matrix $A$ and a fully *smoothed* (*degree-normalized*) matrix. They then solve a sequence of eigenvalue problems along this path, rounding and refining at each step, thereby avoiding a brittle, single-shot choice of $M$. To make the sequence efficient, this paper proposes a warm-start strategy that predicts eigenvectors when $\\alpha$ is incremented by $+\\Delta$, together with a theoretical bound on the largest admissible  $\\Delta$ that controls drift. The paper also discusses practical accelerations, including GPU-friendly implementations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-motivated generalization of spectral relaxations.** Rather than treating the choice of $M$ as a brittle hyperparameter, the method reframes it into a continuation over $\\alpha \\in [0, 1]$ that interpolates from $A$ to a fully normalized matrix. This addresses the well-known sensitivity of spectral heuristics to normalization choices in an algorithmic manner.\n- **Warm-start with explicit eigenvector prediction.** The derivation of a predictor for the top eigenvector under the update $\\alpha \\gets \\alpha + \\Delta$ is both technically appealing and practically functional. The accompanying analysis of perturbation with respect to $\\Delta$ provides a theoretical foundation for the continuation, rather than treating it as a purely heuristic speedup."}, "weaknesses": {"value": "- **Notation and clarity early in the paper.** Some quantities (e.g., the spectral matrix $M$ and $nnz(A)$ appear in the introduction without definition. This hurts readability for non-specialist readers and should be fixed by introducing all symbols at first mention.\n- **Missing baselines that use continuous relaxations.** Regarding the statement *\"enabling optimization of instances with $n \\ge 10^{5}$ that remain intractable for exact methods while achieving competitive solution quality.\"*, If the intended message is that the method is **competitive** on large instances, it is important to compare it with other recent continuous-relaxation solvers, such as QQA [1] (accepted at ICLR2025) and iSCO [2]. Like your approach, these methods employ annealing/continuation ideas and run efficiently on GPUs. At a minimum, they should be discussed in Related Work; ideally, include empirical comparisons on representative tasks to position your speed-quality trade-off relative to theirs.\n- **Incrementalness concern.** The normalization schemes themselves are not novel. The primary contribution lies in annealing along this path and repeatedly solving the associated relaxations. Although effective, this may be viewed as an incremental extension of established spectral pipelines. More apparent differentiation, via new theory (e.g., approximation or rounding guarantees) or broader empirical comparisons, would strengthen the work.\n- **Applications and impact in ML/optimization practice.** This paper would benefit from clarifying the application domains in which fast, approximate Ising solvers are in demand. For instance, in RBM training, the mainstream continues to minimize contrastive divergence and mean-field variants. It would be helpful to articulate concrete scenarios where the proposed spectral annealing has a clear advantage, and to quantify its performance relative to mean-field approximations along the speed‚Äìquality frontier.\n\n### References \n\n- [1]: Yuma Ichikawa and Yamato Arai, Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based Sampling, ICLR2025.\n- [2]: Haoran Sun et al., Revisiting Sampling for Combinatorial Optimization, ICML2023."}, "questions": {"value": "- **Meaning of $nnz(A).$** Does $nnz(A)$ denote the number of nonzero entries of $A$? If so, please define it at first use to avoid confusion.\n- **Role and scalability of the 1‚Äëflip local update.** The algorithm includes a 1‚Äëflip local search after rounding. How essential is this step at a large scale? Do you observe diminishing returns or stability issues in very high dimensions? An ablation that shows solution quality and runtime with/without local search over several graph sizes would clarify its value.\n- **Warm-start efficiency in practice.** What is the typical average iteration count $\\bar{I}$ after warm starts across datasets? Beyond empirical plots, can you provide a simple theoretical bound relating $\\bar{I}$ to the spectral gap and your step-size rule for $\\Delta$? Even an asymptotic statement would make the continuation schedule more principled."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h2D5lnOAjA", "forum": "atoLVj3fZY", "replyto": "atoLVj3fZY", "signatures": ["ICLR.cc/2026/Conference/Submission24332/Reviewer_diKW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24332/Reviewer_diKW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954021124, "cdate": 1761954021124, "tmdate": 1762943045604, "mdate": 1762943045604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces spectral annealing for large-scale Ising optimization. The core idea is: instead of solving one spectral relaxation, it explores a family of spectral matrices N_Œ± = D_Œ±^{-1/2} A D_Œ±^{-1/2} parameterized by Œ± ‚àà [0,1], which interpolates between raw adjacency (Œ±=0) and full normalization (Œ±=1). The major contribution is a diagonal predictor that enables efficient warm-starting. The authors provide analysis for step-size control and demonstrate favorable time-quality trade-offs on instances up to 262K spins, achieving 2-5√ó quality improvements over single-parameter spectral methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is technically sound. The theoretical analysis, such as drift bounds, step-size control, and positive definiteness guarantees, is reasonable. The experiments are reasonably comprehensive, covering three different dataset types (real-world, benchmark, synthetic) and comparing against multiple baselines. The ablation study effectively demonstrates the value of the implementation optimizations.\n\nThe paper is well-organized and mostly easy to follow. The background section clearly explains the normalization dilemma. The algorithm presentation is clear.\n\nThe problem is important for ML applications in correlation clustering, MAP inference, and energy-based models. The method scales to large instances where exact methods fail and maintains better quality than pure spectral methods."}, "weaknesses": {"value": "The paper mentions Goemans-Williamson but doesn't compare against modern SDP solvers. This is a major gap since SDPs provide stronger relaxations. Also, the paper cites SPONGE as related work, but never compares against it experimentally. But SPONGE is also optimizing over spectral parameters.\n\nWere SA, SB, and PT tuned for these specific instances? The paper doesn't clarify hyperparameter selection for baselines, which affects fair comparison.\n\n\"Unbearable computational time\" is vague. How many iterations did SA use? The paper should report iteration counts for all metaheuristics.\n\nFigure 1 shows that optimal Œ± varies by instance, and this motivates the proposed method. But the paper never analyzes the root cause. What graph properties predict whether Œ±=0 or Œ±=1 is better? When does sweeping help most? This understanding would make the contribution more principled than simply trying everything."}, "questions": {"value": "The paper applies local search to spectral annealing's solutions, but it's unclear if the same refinement is applied fairly to all baseline spectral methods. Table 1 shows \"SpecAdj\" and \"SpecSignedLap\", but do these include local search?\n\nUnlike Goemans-Williamson (0.878 for max-cut) or Trevisan (0.531), this method has no worst-case approximation ratio. Can you derive any instance-dependent bounds?\n\nTheorem 2 requires Œ≥(t) > 0 for all t, but this may not hold. What happens at degeneracy points?\n\nThe method only applies to Ising optimization. The conclusion mentions \"extending to other combinatorial problems\" but provides no concrete direction. Which problems have a similar Œ±-parameterization structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qANLsE4l51", "forum": "atoLVj3fZY", "replyto": "atoLVj3fZY", "signatures": ["ICLR.cc/2026/Conference/Submission24332/Reviewer_GE9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24332/Reviewer_GE9a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018870834, "cdate": 1762018870834, "tmdate": 1762943045282, "mdate": 1762943045282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles large-scale Ising optimization problems (used in graph partitioning, correlation clustering, etc.).\n\nTraditional spectral methods solve only one eigenvalue problem‚Äîchoosing a single matrix normalization‚Äîand then round that result to get a binary solution. That approach is fast but very sensitive to which normalization you pick.\n\nThis work turns that single step into a smooth ‚Äúannealing‚Äù process. It continuously changes the normalization parameter ùõº from 0 to 1. so the algorithm gradually moves from the raw adjacency matrix to the fully normalized version.\n\nBoth theoretical and empirical results are provided to support the improvement of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a well-motivated and practically important problem: improving the scalability and robustness of spectral methods for large Ising optimization. The proposed idea‚Äîcontinuously annealing the normalization parameter rather than fixing it‚Äîis simple, elegant, and easy to understand, yet it leads to a meaningful performance gain. The approach is also highly practical: it retains the efficiency of standard spectral methods, is straightforward to implement, and can naturally leverage GPU acceleration. The theoretical analysis is solid, providing clear stability guarantees and drift bounds, and the experimental evaluation is thorough, covering both real-world and synthetic datasets with convincing results. Overall, the method is conceptually clean, empirically validated, and potentially very useful for large-scale optimization or as a building block for future algorithms."}, "weaknesses": {"value": "While the technical content is solid, the presentation could be improved. The writing sometimes assumes familiarity with optimization-oriented formulations of the Ising model, which may confuse readers who come from the statistical physics or probabilistic inference side, where Ising models are usually discussed in terms of likelihoods or partition functions. The paper quickly transitions into the optimization framing without first clarifying the connection to the traditional probabilistic view. A brief background or bridging discussion would make the work more accessible and help a broader audience appreciate the significance of the method. In addition, certain sections (especially theoretical derivations) are dense and could benefit from clearer exposition and more intuitive explanations.\n\nWhile the paper presents a clean and well-motivated idea, I am not fully confident about its novelty relative to prior spectral relaxation or continuation methods. The key insight‚Äîsweeping the normalization parameter Œ± and warm-starting successive eigenproblems‚Äîfeels conceptually straightforward. That said, the authors provide solid theoretical backing, practical acceleration, and convincing large-scale results, which make this a meaningful engineering and methodological advance, even if not a fundamentally new paradigm."}, "questions": {"value": "Since both the starting and ending points of the Œ±-path correspond to existing spectral formulations (raw adjacency and normalized Laplacian), could the authors elaborate on why sweeping Œ± is a substantial advancement rather than a straightforward interpolation between known variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2Bqn7vzrEY", "forum": "atoLVj3fZY", "replyto": "atoLVj3fZY", "signatures": ["ICLR.cc/2026/Conference/Submission24332/Reviewer_Dnia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24332/Reviewer_Dnia"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024756076, "cdate": 1762024756076, "tmdate": 1762943045018, "mdate": 1762943045018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}