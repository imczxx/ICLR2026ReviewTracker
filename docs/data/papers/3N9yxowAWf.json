{"id": "3N9yxowAWf", "number": 17925, "cdate": 1758282075854, "mdate": 1759897144821, "content": {"title": "Sparse Disentangled VAE for Treatment Effect Estimation with Irrelevant Variables", "abstract": "Treatment effect estimation from imbalanced observational data is challenging, requiring balanced latent representations to reduce selection bias and enable accurate causal estimates. Many state-of-the-art methods employ VAEs with predetermined latent dimensionality, but this often causes over- or underfitting since too little relevant or too much irrelevant information is encoded. As cross-validating latent dimensionality is impractical for complex models and high-dimensional data, automatic determination is needed. We address this by learning sparsity-inducing masks that sub-select dimensions for each task, using a differentiable $L_0$ objective to penalize active dimensions and a mutual exclusivity regularizer to prevent overlap, ensuring independent and disentangled representations. Conflicting goals of accuracy and sparsity are balanced via Generalized ELBO with Constrained Optimization (GECO), optimizing sparsity only once prediction quality exceeds a threshold. Our method thus infers task-relevant latent factors, yields compact representations, and isolates irrelevant variables in challenging high-dimensional data. Experiments on real-world and synthetic datasets demonstrate improved predictive accuracy, compactness, and disentanglement compared to state-of-the-art baselines.", "tldr": "A VAE-based method that learns compact, disentangled representations for accurate treatment effect estimation.", "keywords": ["Treatment effect estimation", "Irrelevant variables", "Variational autoencoder", "Disentangled representations", "Sparsity", "Causal inference"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f17616e282af58f1f6f46c449616888a0c4094c.pdf", "supplementary_material": "/attachment/50b3e6481e69c6c95dcc68606ded80947d4b8540.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces GLOVE-ITE, a novel VAE-based framework for treatment effect estimation from high-dimensional observational data where irrelevant variables are present. The paper's core contribution is tackling a key limitation of existing VAE methods: the reliance on a pre-defined, fixed latent bottleneck width. The authors argue this leads to under- or over-fitting, as the dimensionality is critical but impractical to cross-validate.\n\nGLOVE-ITE automatically learns a sparse, disentangled representation by integrating three key techniques:\n\n1. Differentiable $L_0$ Sparsity: It employs $L_0$ regularization with a Binary Concrete relaxation to learn sparse, task-specific masks for different latent factors (instrumental $\\Gamma$, confounding $\\Delta$, adjustment $\\Upsilon$, and irrelevant $\\Omega$). This effectively performs automatic dimension selection.\n\n2. GECO (Generalized ELBO with Constrained Optimization): To balance the conflicting objectives of prediction accuracy (MSE) and sparsity ($L_0$), the method uses GECO. This reframes the outcome prediction loss as a constraint, ensuring that sparsity is only optimized once the prediction error drops below a specified tolerance threshold $\\tau$.\n\n3. Exclusivity Regularizer ($\\mathcal{L}_{excl}$): A novel entropy-based loss is introduced to enforce that the learned masks are mutually exclusive (non-overlapping). This prevents information leakage between factors and promotes stronger disentanglement, particularly in isolating the irrelevant variables $\\Omega$.\n\nThe authors conduct extensive experiments on synthetic data and two real-world benchmarks. The results demonstrate that GLOVE-ITE achieves state-of-the-art or competitive accuracy while using a significantly more compact latent representation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel and Sound Methodology**: The primary strength is the elegant methodological design. The use of GECO to dynamically balance the conflicting accuracy and sparsity objectives is a key innovation that moves beyond simple weighted losses. This is powerfully supported by the $\\mathcal{L}_{excl}$ loss, which ensures the learned sparse dimensions are also meaningfully disentangled.\n\n- **Solves a Critical Practical Problem**: The paper successfully automates the selection of latent dimensionality, a notoriously difficult hyperparameter to tune. The compression analysis in Table 3 provides strong evidence that the model can discover the \"true\" intrinsic dimension of the data."}, "weaknesses": {"value": "- **Sensitivity to GECO's $\\tau$ Hyperparameter**: The paper elegantly removes the need to tune the latent dimensionality, but it introduces a new, and potentially equally sensitive, hyperparameter: the MSE tolerance $\\tau$ for GECO. Figure 6 explicitly shows that both the final PEHE and the number of active dimensions are highly dependent on the choice of $\\tau$. The paper provides the $\\tau$ values used, but does not discuss how these values were chosen, which risks trading one difficult hyperparameter for another.\n\n- The study primarily relies on two small-scale real-world datasets and one synthetic dataset. Although the paper claims its method has advantages in \"high-dimensional data\" and \"large data settings,\" the real-world datasets used have very small sample sizes. This weakens the conclusions regarding the method's scalability and robustness on large-scale problems."}, "questions": {"value": "- Could you elaborate on the process for selecting the GECO tolerance hyperparameter $\\tau$? Figure 6 suggests performance is quite sensitive to it. How were the values of 0.4 (for IHDP/Jobs) and 0.01 (for Synthetic) determined? Is there a principled way to set $\\tau$ that avoids the same level of exhaustive tuning that the paper sought to eliminate for bottleneck width?\n\n- The paper claims to address the issue of \"predetermined latent dimensionality\" and \"eliminate the need for predefined or manually tuned bottleneck width.\" However, in practice (as shown in Table 3), the model still requires the user to specify an initial total latent dimension $d$ (e.g., 60, 68, 76, 84). GLOVE-ITE learns a sparse active subset from this initial $d$-dimensional space, rather than determining the dimensionality from scratch. This initial $d$ remains a critical hyperparameter that must be defined by the user. The paper does not discuss how the model’s performance might be affected if this initial $d$ is set improperly (e.g., if it is smaller than the true intrinsic dimensionality).."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b5JYuBI7Y6", "forum": "3N9yxowAWf", "replyto": "3N9yxowAWf", "signatures": ["ICLR.cc/2026/Conference/Submission17925/Reviewer_C5JF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17925/Reviewer_C5JF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372291452, "cdate": 1761372291452, "tmdate": 1762927738714, "mdate": 1762927738714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GLOVE-ITE, a VAE-based framework designed to estimate individual treatment effects in high-dimensional observational data that may contain irrelevant or confounding variables. The authors claim that this framework allows the model to automatically determine the optimal latent dimensionality and improve interpretability while maintaining competitive prediction performance. Experiments on synthetic, IHDP, and Jobs datasets show modest improvements in PEHE and policy risk over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addressing the impact of irrelevant variables in high-dimensional ITE estimation is a relevant and practically important topic.\n2. The experiments follow standard benchmarks, and the results indicate some improvement over prior VAE-based baselines."}, "weaknesses": {"value": "1. The main components of the proposed method are well-established techniques. Their combination is incremental and lacks clear conceptual innovation. There is no new causal modeling insight beyond architectural tuning.\n2. The paper claims to address causal disentanglement, but the proposed method operates purely at the representational level without formal causal guarantees.\n3. The use of GECO and L0 regularization is motivated empirically but not theoretically linked to causal identifiability or bias reduction. The mathematical formulations are standard VAE objectives with additional penalty terms, not fundamentally new learning principles.\n4. The experiments are limited to small-scale datasets (IHDP, Jobs) and synthetic examples. There is no evidence that the method scales to realistic high-dimensional or nonlinear causal systems.\n5. The visualization and discussion of “latent causal factors” are not convincing. The masks and sparsity patterns are not compared against ground-truth causal variables, making it unclear whether the model truly learns meaningful disentanglement."}, "questions": {"value": "See Weaknesses Part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Bg5Ui7YL3", "forum": "3N9yxowAWf", "replyto": "3N9yxowAWf", "signatures": ["ICLR.cc/2026/Conference/Submission17925/Reviewer_ukKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17925/Reviewer_ukKR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761480104860, "cdate": 1761480104860, "tmdate": 1762927737882, "mdate": 1762927737882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a variational autoencoder-based approach for estimating treatment effects from observational data, focusing on disentangling latent factors to improve causal inference. Key components include:\n\n- **Learnable Masks via L0 Regularization**: Binary masks are used to partition the latent space into four distinct groups—instrumental variables, confounding factors, adjustment variables, and irrelevant dimensions—allowing selective access to relevant latents for different modeling tasks (e.g., treatment assignment and outcome prediction).\n\n- **Generalized ELBO with Constrained Optimization (GECO)**: to trade off reconstruction fidelity, outcome prediction accuracy, and sparsity in the masks.\n\n- **Entropy Regularizer for Mutual Exclusivity**: A regularization term that penalizes overlap among the masks by promoting high entropy in their assignments, thereby enforcing separation of the latent groups and reducing information leakage between causal factors."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The integration of these components—masks for latent partitioning, GECO for multi-objective optimization, and entropy-based separation—appears novel in its specific application to causal disentanglement for treatment effect estimation.\n- Experiments demonstrate empirical gains, including improved Precision in Estimation of Heterogeneous Effects (PEHE) scores and visualizations of well-separated latent representations across groups."}, "weaknesses": {"value": "### Unidentifiable Representations\n\nThe core issue lies in the unidentifiability of the learned representations, especially the mask-based separation. Identifiable representations are tied to the data(-generating process) and remain invariant to training procedures (see [1] for a detailed framework). Here, even with the pre-trained \"full representation\" fixed, the learnable masks yield inconsistent outputs. For instance:\n\n- Varying the hyperparameter $\\lambda$ produces different mask matrices,\n- Changing the random seed during training likely results in varied masks.\n\nHowever, the underlying causal graph of covariates being unique (or equivalent up to Markov equivalence classes for treatment effect estimation).\nThis means the method fails to recover the true causal structure reliably.\n\n### Potential Dependence in Selected Dimensions\n\nEven if masks select dimensions, dependence can persist if the pre-separated \"full representations\" are not independent. The resulting instrumental, confounding, and adjustment latents may not be fully disentangled. To achieve independence, an identifiable VAE (iVAE) framework [1] is necessary, with direct applications to PEHE estimation available in [2].\n\n### Limited Novelty\n\nThe primary ideas are taken (or straightforward adaptations) from prior work:\n\n- Binary Concrete distributions for implementing sparse masks.\n- GECO as a regularization to penalize outcome prediction errors.\n- Partitioning latents into instrumental, confounding, and adjustment categories.\n- L0 regularization via masks reduces to selecting latent dimensions for causal roles.\n\n### Arbitrary Method Design Choices\n\nSeveral decisions seem ad hoc or suboptimal:\n\n- **Irrelevant Representations**: Why introduce explicit masks for irrelevant dimensions? Any dimension not assigned to the three causal masks (instrumental, confounding, adjustment) is inherently irrelevant, making this redundant.\n- **GECO vs. Simpler Alternatives**: Instead of GECO, one could multiply the mask loss by a hyperparameter to balance sparsity and prediction error—simpler and equally effective. Alternatively, incorporating an outcome likelihood term (analogous to those for covariates $X$ and treatment $T$) would more naturally integrate prediction without custom constraints.\n- **Dependence on $\\lambda$**: The method relies on tuning the error threshold $\\lambda$ for sparsity, akin to manually selecting latent dimensionality as a bottleneck—both are heuristic ways to control model capacity without deeper justification.\n\n\n### Experiments\n\n- **Table 1**: Standard deviations for baseline methods are unusually large, suggesting a potential bug.\n- **Figure 6**: I think PEHE accuracy should degrade for very small $\\lambda$ due to overfitting from insufficient sparsity.\n\n\n### Writing Issues\n\nThe writing is often unclear, imprecise, or repetitive. Specific examples:\n\n- **Abstract**: \"Treatment effect estimation from imbalanced observational data is challenging…\" (First Sentence) and later parts of the paper mention balancing or balanced representations repeatedly, but this is not a novel contribution and creates confusion about the paper's focus.\n- **Introduction**: Vague and redundant phrases like \"entanglement between distinct latent causal factors interferes with the model’s causal structure\" and \"information leakage between latent factors can lead to inaccurate inference of latent factors.\" Terms such as \"dedicated mask that separates irrelevant factors into a distinct latent subspace\" and \"mutual exclusivity regularization across all masks that prevents information leakage\" are imprecise without later context.\n- **Binary Concrete Distribution (for Masks)**: Unclear details—e.g., are $\\alpha$ and $\\beta$ hyperparameters? How does $\\alpha$ influence the process? The transition from Equation (2) to (3) lacks explanation, and the rationale for the \"stretching\" equation is absent.\n- **Line 234**: \"Other tasks perform better than a threshold\" uses plural \"tasks\" (likely meaning outcome prediction), which is confusing.\n- **Equation Ordering**: Presenting incomplete Equation (4) before (9), while referencing (9) early (Line 248), disrupts logical flow.\n- **Optimization Details**: The min-max optimization scheme is undefined—e.g., does it have a stochastic gradient version? Which optimizer was used?\n\n### Missing Related Work\n\nThe paper omits key references on identifiable and causal VAEs:\n\n- [1] Khemakhem, Ilyes, et al. \"Variational autoencoders and nonlinear ICA: A unifying framework.\" *International Conference on Artificial Intelligence and Statistics*. PMLR, 2020. (For foundational identifiability in VAEs.)\n- [2] Wu, Pengzhou Abel, and Kenji Fukumizu. \"$\\beta$-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap.\" *International Conference on Learning Representations* (2022). (For iVAE applications to PEHE and disentanglement.)"}, "questions": {"value": "Please refer to the points in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LiJEamexou", "forum": "3N9yxowAWf", "replyto": "3N9yxowAWf", "signatures": ["ICLR.cc/2026/Conference/Submission17925/Reviewer_LaRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17925/Reviewer_LaRH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652461764, "cdate": 1761652461764, "tmdate": 1763003059309, "mdate": 1763003059309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GLOVE-ITE, a VAE-based framework for individual treatment effect (ITE) estimation that learns a compact, disentangled latent space in the presence of irrelevant variables. It adopts the L0 sparsity objective that learns masks over a shared latent to activate only task-relevant dimensions and utilizes GECO to prioritize outcome prediction while sparsifying. Experiments on IHDP, Jobs, and a structured synthetic dataset show better performance with fewer active latent dims than VAE baselines, supported by ablations and qualitative analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem, underlying factors, and losses are clearly laid out with equations and a helpful architecture figure.\n- Automatic bottleneck sizing approach in this context is interesting.\n- Claims are supported by extensive experimental results.\n-  Key hyperparameters and datasets are sufficiently described."}, "weaknesses": {"value": "- Novelty appears to be incremental. Prior work combined GECO + sparsity in VAEs; a clearer positioning vs. (Boom et al. 2020) is needed. \n- Baselines are VAE-centric; adding strong non-VAE ITE methods (e.g., modern CFR variants or representation learners without generative modeling) would better calibrate performance gains. \n- While (\\tau) effects are shown, broader sweeps for $\\beta$, $\\gamma$ , $\\zeta$, and $\\kappa$ are missing."}, "questions": {"value": "1. Beyond the target domain and exclusivity loss, what prevents (Boom et al. 2020)-style approaches from achieving similar behavior? Can you ablate exclusivity against simple orthogonality penalties? \n2. Please include non-VAE ITE SOTA baselines (e.g., strong CFR/DR learners).\n3. Please report sensitivity to ($\\beta,\\gamma,\\zeta,\\kappa$); provide recommended ranges to avoid degenerate solutions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WhwDU6X48q", "forum": "3N9yxowAWf", "replyto": "3N9yxowAWf", "signatures": ["ICLR.cc/2026/Conference/Submission17925/Reviewer_JcSA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17925/Reviewer_JcSA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853685689, "cdate": 1761853685689, "tmdate": 1762927737181, "mdate": 1762927737181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}