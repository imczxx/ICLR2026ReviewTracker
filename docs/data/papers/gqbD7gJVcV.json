{"id": "gqbD7gJVcV", "number": 21079, "cdate": 1758313504158, "mdate": 1763497902501, "content": {"title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases", "abstract": "Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce $\\textit{BiomedSQL}$, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples generated from templates and grounded in a harmonized BigQuery knowledge base that integrates gene–disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases.", "tldr": "We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base.", "keywords": ["Benchmark", "Text-to-SQL", "LLM", "Scientific reasoning", "Biomedical data"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5f57474a2149e9f14d5121e2829ecee16520a78.pdf", "supplementary_material": "/attachment/d129a9a6605fcb1af5b7cf479cb988e904c5165f.zip"}, "replies": [{"content": {"summary": {"value": "The paper “BiomedSQL: Text-to-SQL for scientific reasoning on biomedical knowledge bases” presents a large-scale benchmark aimed at evaluating scientific reasoning in text-to-SQL systems. The authors harmonize several major biomedical knowledge bases (OpenTargets, ChEMBL, GWAS Catalog, omicSynth) into a unified BigQuery schema (~10 core tables), then create roughly 68 000 natural-language question / gold SQL / answer triples derived from 40 expert templates.\n\nThe benchmark is used to evaluate numerous LLMs (open + closed models) across single-turn and multi-step paradigms (ReAct, Schema Indexing, DAIL-SQL, and their own BMSQL agent). The work also introduces BioScore, an LLM-based judge for assessing natural-language answers when multiple SQL solutions can be correct.\n\nOverall, BiomedSQL fills an important gap at the intersection of biomedical NLP and structured reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "## Strengths\n\n- **Novel and impactful problem scope**  \n  Text-to-SQL has been widely studied, but very few benchmarks target scientific reasoning or biomedical domains where implicit conventions (e.g., significance thresholds, trial phases) matter. This benchmark addresses that gap.\n\n- **Realistic, large-scale data integration**  \n  The authors construct a multi-table biomedical schema from authentic public datasets, which lends realism far beyond toy databases used in Spider-style corpora.\n\n- **Extensive empirical coverage**  \n  Evaluation spans many modern LLMs and multiple reasoning paradigms. Metrics include execution accuracy (EX), Jaccard similarity (JAC), schema-error rate (SER), and BioScore, giving a well-rounded picture.\n\n- **Clear technical writing and reproducibility reporting**  \n  The paper gives schema diagrams, query examples, and compute details. Appendix sections provide enough detail for independent reproduction.\n\n- **Valuable analysis**  \n  The authors break down performance by reasoning category (aggregation, multi-table joins, scientific thresholds, etc.) and analyze failure patterns."}, "weaknesses": {"value": "1. **Security / artifact hygiene**  \n   The supplementary materials appear to contain active service-account credentials for BigQuery access. Even if these are limited in scope, publishing any cloud credentials is a critical security issue.  \n\n2. **Licensing and redistribution clarity**  \n   Several component datasets (e.g., ChEMBL) carry CC BY-NC 4.0 licenses that restrict commercial redistribution. The paper should explicitly list the license for each source table and explain what will be hosted directly versus reconstructed locally.  \n\n3. **Template-based generation and linguistic diversity**  \n   All 68,000 questions are created from 40 expert templates. That’s acceptable for structured benchmarking, but the paper should quantify linguistic variability and discuss how well templated language covers real user queries. If possible, evaluate a small human-written set to test generalization.\n\n4. **LLM-as-judge (BioScore) transparency**  \n   BioScore is interesting but somewhat underspecified. The paper briefly mentions a small human correlation study; sample size and agreement statistics should be provided to establish reliability. Even basic numbers (n, ρ/κ) would help readers assess the trustworthiness of this metric.\n\n5. **Accessibility / BigQuery dependence**  \n   Currently all queries execute in BigQuery. For full reproducibility, a local version (e.g., SQLite or CSV dump) should be released or an automated script provided. Otherwise replication requires commercial cloud access."}, "questions": {"value": "1. Can you confirm whether the BigQuery credentials in the supplement are active or dummy placeholders?  \n\n2. Please provide a clear per-table license map indicating which datasets are redistributed vs. reconstructed.  \n\n3. Will you release a local/offline version (e.g., SQLite or CSV) or reproducible build scripts for the database?  \n\n4. Could you elaborate on BioScore reliability — sample size, human–LLM agreement, and any bias checks?  \n\n5. How diverse are the 40 question templates linguistically, and do you have results on any human-written queries?  \n\n6. Please confirm that all data are public, aggregated, and contain no personally identifiable information."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LfvlulO4pN", "forum": "gqbD7gJVcV", "replyto": "gqbD7gJVcV", "signatures": ["ICLR.cc/2026/Conference/Submission21079/Reviewer_sdka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21079/Reviewer_sdka"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761222010532, "cdate": 1761222010532, "tmdate": 1762999986322, "mdate": 1762999986322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new, domain-specific Text-to-SQL dataset focusing on the\nbiomedical domain and on a BigQuery database. The dataset is bootsrapped from\n40 manual created Text-Query pairs in a template based approach to 68K data\npoints. The database is created by combining several existing related data\nsources. The evaluation of the dataset is extensive, covering several open and\nclosed-source models and interaction paradigms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: New combined biomedical database that can be used by Text-to-SQL researchers.\n\nS2: Clear motivation and paper structure.\n\nS3: Dataset seems challenging on evaluated Text-to-SQL approaches"}, "weaknesses": {"value": "W1: Creating 68K data points from only 40 seed questions in a template based approach will lead to very similar question, just differentiating in small parts (e.g., different value in WHERE filter).\n\nW2: The authors show that existing Text-to-SQL systems do not work well on their dataset, which is surprising as with the template based approach the dataset contains lots of repetition and structurally equal questions. A similarity-based few-shot approach should dramatically boost performance.\n\nW3: The template-based approach used by the authors is rather simple. What about multiple languages or question paraphrasing?"}, "questions": {"value": "Q1: Can the authors provide an ER Diagram of the database schema?\n\nQ2: Have the authors used PK and FK constraints in their database?\n\nQ3: Can the authors elaborate on few-shot experiments and show results for sparse (e.g., BM25) and dense retrieval?\n\nQ4 How does the dataset compare against other domain-specific dataset in the biomedical domain such as ScienceBenchmark [1] in terms of query complexity, DB complexity and data size?\n\n[1] Zhang, Y., Deriu, J. M., Katsogiannis-Meimarakis, G., Kosten, C., Koutrika, G., & Stockinger, K. (2024). ScienceBenchmark: a complex real-world benchmark for evaluating natural language to SQL systems. Proceedings of the VLDB Endowment, 17(4), 685-698."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qf4uokp47e", "forum": "gqbD7gJVcV", "replyto": "gqbD7gJVcV", "signatures": ["ICLR.cc/2026/Conference/Submission21079/Reviewer_EjqL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21079/Reviewer_EjqL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755528221, "cdate": 1761755528221, "tmdate": 1762999986571, "mdate": 1762999986571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BiomedSQL, a text-to-SQL benchmark aim to test scientific reasoning over a real biomedical knowledge base rather than only syntactic schema translation. The dataset contains about 68k question–SQL–answer triples and runs on a harmonized BigQuery database that integrates OpenTargets and ChEMBL drug and target data, GWAS summary stats for Alzheimer’s and Parkinson’s disease, and SMR causal signals from omicSynth. Questions require domain rules like genome-wide significance thresholds or trial phase filters, which general text-to-SQL benchmarks do not probe. Evaluations on various LLMs using different prompting strategies and interaction paradigms reveal performance gaps: top models like GPT-o3-mini achieve 59.0% execution accuracy with advanced prompts, while a custom multi-step agent (BMSQL) reaches 62.6%, far below the 90.0% expert baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- First benchmark explicitly targeting scientific reasoning in text-to-SQL for biomedicine, going beyond syntactic translation in general benchmarks or clinical ones.\n- Highlights implicit domain conventions like significance thresholds, effect directionality, and multi-omic causal inference, which are critical for real-world biomedical queries but underexplored in prior work.\n- Large number of data samples with 68,000 triples and a large-scale database (e.g., 21M+ rows in GWAS tables).\n- Thorough experiments across 10+ models, multiple prompting variants, interaction paradigms, and various evaluation metrics."}, "weaknesses": {"value": "- Generating 68K samples from 40 seed questions (drawn from CARDBiomedBench) may result in redundant patterns, potentially overestimating model generalization. Is there a specific reason for scaling to 68K rather than a smaller subset? The expansion process appears limited to entity substitution, without syntactic expansions (if I understand correctly), which could reduce real-world variability as seen in crowdsourced benchmarks."}, "questions": {"value": "- Providing costs for running all 68K samples for different models and interaction patterns would be helpful to potential readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IodM7KeK9D", "forum": "gqbD7gJVcV", "replyto": "gqbD7gJVcV", "signatures": ["ICLR.cc/2026/Conference/Submission21079/Reviewer_fKjE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21079/Reviewer_fKjE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979521923, "cdate": 1761979521923, "tmdate": 1762940803516, "mdate": 1762940803516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BiomedSQL, a new large-scale benchmark with 68,000 question/SQL/answer triples designed to evaluate the scientific reasoning of text-to-SQL systems. The authors argue that standard models fail in biomedical research because they only translate syntax, but scientific questions require implicit domain knowledge. This includes inferring non-obvious criteria like genome-wide significance thresholds or filtering by clinical trial phases, which are not defined in the database schema. Experiments on a real-world BigQuery knowledge base revealed a substantial performance gap. Human experts achieved a much higher accuracy than the best-performing model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper addresses a impactful aspect in the field of structured reasoning in science, specifically the ability to use implicit, domain-specific knowledge.\n\n* The benchmark is built on a \"real-world biomedical knowledge base\", harmonizing large, authentic public datasets like OpenTargets, ChEMBL, and GWAS Catalog data. This provides a more realistic challenge than many existing text-to-SQL corpora.\n\n* The authors assess performance by reasoning type (like aggregation or multi-table joins) and also analyze error patterns."}, "weaknesses": {"value": "* The entire dataset of 68,000 questions is generated from only 40 expert-written templates, meaning quite high homogeneity.\n\n* The paper introduces and uses BioScore, an LLM-as-a-judge (using GPT-4o), to evaluate natural language answer quality. Although the authors provide a validation study showing high correlation with a domain expert,  this does not capture the full picture of the metric's reliability, as the paper itself acknowledges the \"concern over LLM-as-a-judge metrics yielding unstable assessment results\".\n\n* The database is a collection of sources with different licenses, including CCO 1.0, CC BY 4.0, and CC BY-NC 4.0. This creates potential complexities for redistribution and use, particularly for any commercial applications, which the paper does not fully resolve."}, "questions": {"value": "* You provide a validation study for your LLM-as-a-judge metric, BioScore, showing a high (0.91) Spearman correlation with a domain expert on a sample of 40 responses. How confident are you that this high agreement holds across the thousands of queries where models fail in different ways?\n\n* The Appendix notes that one of your data sources is under a CC BY-NC 4.0 license, which restricts commercial use. How do you see this license affecting the adoption of BiomedSQL by industry research labs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DTYZ3SgmDh", "forum": "gqbD7gJVcV", "replyto": "gqbD7gJVcV", "signatures": ["ICLR.cc/2026/Conference/Submission21079/Reviewer_TV3c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21079/Reviewer_TV3c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981325420, "cdate": 1761981325420, "tmdate": 1762940739713, "mdate": 1762940739713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}