{"id": "N0Wh7ND71X", "number": 10849, "cdate": 1758183258257, "mdate": 1759897624881, "content": {"title": "Rethinking Learning-based Symmetric Cryptanalysis: a Theoretical Perspective", "abstract": "The success of deep learning in cryptanalysis has been largely demonstrated empirically, yet it lacks a foundational theoretical framework to explain its performance. We bridge this gap by establishing a formal learning-theoretic framework for symmetric cryptanalysis. Specifically, we introduce the Coin-Tossing model to abstract the process of constructing distinguishers and propose a unified algebraic representation, the Conjunctive Parity Form (CPF), to capture a broad class of traditional distinguishers without needing domain-specific details. Within this framework, we prove that any concept in the CPF class is learnable in sub-exponential time in the setting of symmetric cryptanalysis. Guided by insights from our complexity analysis, we demonstrate preprocessing the data with a flexible output generating function can simplify the learning task for neural networks. This approach leads to a state-of-the-art practical result: the first improvement on the deep learning-based distinguisher for $S{\\scriptsize PECK}$32/64 since 2019, where we enhance accuracy and extend the attack from 8 to a record 9 rounds.", "tldr": "This paper constructs a theoretical bridge between learning theory and deep learning-based symmetric cryptanalysis, and achieve state-of-the-art results on relevant tasks guided by the theory.", "keywords": ["Symmetric Cryptanalysis", "Neural Distinguisher"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/552b23c3385bccf15f8b2cd687be20ec54fb1d16.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a theoretical framework for explaining the performance of neural distinguishers, consisting of the Coin-Tossing (CoTo) model and the Conjunctive Parity Form (CPF). The CoTo model formalizes the data generation process, while the CPF provides a unified representation of conventional distinguishers. The authors prove that any concept belonging to the CPF class is learnable in sub-exponential time. Finally, based on this result, they construct a neural distinguisher for SPECK32/64, extending existing neural attacks from 8 to 9 rounds for the first time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the CPF, which unifies a wide range of existing distinguishers, and theoretically proves its learnability.\n- It improves the performance of neural distinguishers on SPECK32/64."}, "weaknesses": {"value": "- The approach might ultimately only cover the scope of existing conventional distinguishers.\n- For the 9-round results, the paper does not include a comparison that incorporates the differential-linear results used to determine the input generation function, so the effectiveness of the proposed method remains unclear.\n- Theoretical results only demonstrate the existence of a learning algorithm, without providing any concrete construction or procedure."}, "questions": {"value": "## The neural distinguisher might still lie within the scope of conventional distinguishers.\n\nExpressing conventional distinguishers in the unified form of CPF and proving their learnability has theoretical value. However, because the CPF framework is designed to include conventional distinguishers, it raises the concern that the neural distinguisher may simply be re-learning what classical distinguishers already capture. For example, Benamira et al. demonstrated that results similar to Gohr's neural distinguisher can be achieved through traditional cryptanalytic methods. In Example 1 and Example 2, the paper shows that CPF concepts can represent classical differential and differential-linear distinguishers, but this also implies that the neural network may merely be learning these existing structures. A particularly concerning point is that, in order to generate training data for the neural distinguisher, one still needs to rely on conventional cryptanalysis to design appropriate input generating functions and output generating functions.\nIdeally, these should also be learned automatically, but since they are determined using classical analysis, the neural distinguisher may in fact just be imitating conventional distinguishers. If that is the case, the necessity of employing machine learning at all becomes questionable.\n\n## The effectiveness of the 9-round neural distinguisher on SPECK32/64 is unclear because the corresponding differential-linear results are not reported.\n\nThe paper lists the extension of the neural distinguisher to 9 rounds as one of its main contributions. However, to estimate the input difference $\\Delta$, the authors used the MILP/MIQCP-based method proposed by Bellini et al. (2023a). During this process, the corresponding $\\Gamma$ is also computed, meaning that a differential-linear distinguisher using this $(\\Delta, \\Gamma)$ pair can be obtained. Therefore, Table 2 should include a comparison of the accuracy, TPR, and TNR achieved by this differential-linear distinguisher. If its results are almost the same as those of the proposed neural distinguisher, then, as noted in Concern 1, the neural approach would not go beyond the scope of existing distinguishers, and its advantage would be unclear. Moreover, the claimed contribution of extending the distinguisher to 9 rounds might simply reflect the reuse of a conventional differential-linear distinguisher rather than a genuinely new capability of the neural model.\n\n## Theorem 3 only proves the existence of a learning algorithm without showing how to construct it.\n\nThis limitation may be inherent to the PAC-learning framework, but the paper only establishes the existence of a learning algorithm, without specifying how such an algorithm could be concretely constructed. From a cryptanalytic perspective, researchers are also interested in how a distinguisher can be explicitly built. Providing even a brief description or outline of how the learning algorithm might be realized would considerably strengthen the paper's practical value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3Xiv1REDTj", "forum": "N0Wh7ND71X", "replyto": "N0Wh7ND71X", "signatures": ["ICLR.cc/2026/Conference/Submission10849/Reviewer_JatA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10849/Reviewer_JatA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736131505, "cdate": 1761736131505, "tmdate": 1762922049714, "mdate": 1762922049714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical framework for analyzing learning-based cryptanalysis of symmetric primitives, particularly SPNs. It defines a family of distinguishers $\\Theta_\\lambda$ through an advantage constraint and introduces concepts to formalize the success of learning-based adversaries independently of specific neural architectures. The work focuses on conceptual unification. The application section illustrates how the proposed theoretical framework can guide the design of neural distinguishers for practical ciphers such as SPECK32/64. While the reported accuracy improvements are modest, these experiments serve to demonstrate the potential applicability of the Coin-Tossing model and CPF formalism to real-world neural cryptanalysis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Its main strength is the theoretical unification and conceptual rigor it brings to an area that has mostly been explored empirically."}, "weaknesses": {"value": "While the paper presents a rigorous theoretical model of learning-based cryptanalysis, its focus appears primarily cryptographic rather than machine-learning-oriented. The contribution centers on formalizing a generalized attack framework for SPNs and analyzing properties of distinguishers, but it remains unclear how these abstractions translate into improved or novel neural-network–based attacks in practice. The authors may wish to demonstrate how the proposed generalization informs or enhances current NN-based cryptanalytic methods."}, "questions": {"value": "Q1: The definition of the function class $\\Theta_\\lambda$ is somewhat ambiguous and appears to deviate from standard set-roster notation. It is unclear whether it is meant to denote (i) a specific model parameterized by $\\lambda$, or (ii) the set of all functions realizable under the constraint (1). If the latter is intended, the notation would benefit from an explicit clarification. Making this precise would also make Remark 1 (closure under output flipping) more transparent.\n\nQ2: There is some ambiguity in the notation for $\\mathcal{Y}$ and $\\mathcal{R}$. Earlier, these symbols denote finite subsets of the training dataset (structured and random samples, respectively), but later they appear to refer to the underlying distributions from which the samples are drawn, as said after Eq. (1): \"Here, y and r are randomly drawn from $\\mathcal{Y}$ and $\\mathcal{R}$, respectively\" (line 188). \n\nMinor remark: in the definition of Oracle_CT in line 197 the operator I_1 appears unnecessarily."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2tyC04w8f4", "forum": "N0Wh7ND71X", "replyto": "N0Wh7ND71X", "signatures": ["ICLR.cc/2026/Conference/Submission10849/Reviewer_v52A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10849/Reviewer_v52A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823194724, "cdate": 1761823194724, "tmdate": 1762922049278, "mdate": 1762922049278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical framework for symmetric cryptanalysis. \n* The authors introduce the Coin-Tossing (CoTo) model to formalize the construction of distinguishers and also propose the Conjunctive Parity Form (CPF) as a unified algebraic representation for distinguishers. \n* They provide a proof that any CPF concept is learnable in sub-exponential time and even in polynomial-complexity time in certain cases.\n* The theoretical findings lead to a practical improvement in neural distinguishers for a standard block cipher by reducing the problem’s complexity upper bound."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This theoretical framework is novel with respect to cryptanalysis, addressing a gap in the literature.\n* The complexity results are interesting and directly lead to practical impact for designing better neural distinguishers, as demonstrated by the results on SPECK 32/64.\n* The definitions, theorems, and proofs are well-presented. The authors also provide enough background for readers who may not be familiar with the field."}, "weaknesses": {"value": "* Other cryptanalysis works have shown that there can be a gap between theory and practice (e.g. Benchmarking Attacks on Learning with Errors by Wenger et al.). In this case, the work establishes a theoretical upper bound, but it seems unclear as to whether this reflects actual neural network performance.\n* The compression technique reduces problem complexity but also would eliminate some features. Can you discuss in more detail the trade-offs of this technique and in which cases it could affect distinguisher performance? \n* Some minor typos (e.g. missing space in line 28 and “Boolen” in line 234)"}, "questions": {"value": "How would the practical results generalize to other ciphers with different structural properties? Are there certain cipher structures where this technique would perform better (or worse)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1EJ6whiQii", "forum": "N0Wh7ND71X", "replyto": "N0Wh7ND71X", "signatures": ["ICLR.cc/2026/Conference/Submission10849/Reviewer_yb2Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10849/Reviewer_yb2Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955461608, "cdate": 1761955461608, "tmdate": 1762922048817, "mdate": 1762922048817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a general Coin-tossing model that connects the LPN problem to cryptanalysis learning tasks. The model operates at a high level of abstraction, which ensures it covers a variety of cryptanalysis schemes. The complexity of the coin-tossing model is then explored and as a result, authors propose using a different output-generating function that improves the accuracy for 8 and 9 round Speck over Gohr's results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The generality of the coin-tossing model to unify various cryptanalysis approaches is nice.\n- The connection to the LPN problem and the analysis of learning complexities are interesting.\n- The accuracy improvement on the 8-round Speck is impressive."}, "weaknesses": {"value": "- Varying output generating functions is not novel. This has been explored by several other works, including precisely the proposed compression function (the Multi-output Difference in [1]). Several more approaches have been studied; this is referred to as feature engineering in this survey[2].\n- While the coin tossing model is very general, the experimental sections only test two ciphers in very narrow scenarios. Evaluating whether different output-generating functions also help in other cases (i.e., Simon, which is a common target across the literature) would help in evaluating the generality of insights.\n- The performance improvement in the 8/9 round Speck case is impressive, but neural distinguishers with higher accuracies have been found by using more ciphertexts (or their compressed version[1], see the tables in[2]).\n\nReferences:\n[1]:  Hou, ZeZhou, Ren, JiongJiong, Chen, ShaoZhen, Improve Neural Distinguishers of SIMON and SPECK, Security and Communication Networks, 2021, 9288229, 11 pages, 2021. https://doi.org/10.1155/2021/9288229 \n\n[2]: Gerault, D., Hambitzer, A., Huppert, M., & Picek, S. (2024). SoK: 6 Years of Neural Differential Cryptanalysis. Cryptology ePrint Archive."}, "questions": {"value": "- W2: As neural distinguishers with m>2 can perform better than those with m=2 in some cases [2], would the output generating functions also help in those cases? \n- While using different output-generating functions is motivated by the theoretical results, the generality of reducing input size n being better for complexity seems dependent on the specific cipher being analyzed. Does the coin-tossing model help us construct these output-generating functions in a more principled manner?\n- The improved performance in Table 2 over Gohr(2019) has much more imbalanced TPR vs TNR. Does this affect key recovery?\n- Do models trained with compression function require less training data/epochs vs models trained on the standard output differences?\n- What was the performance of ND^1_comp with the original output difference from Gohr?\n\n\nMinor/Typos: \n- I_0 and I_1 are used in appendix B.2 as input generating functions over \\alpha_0 and \\alpha_1. (there is also an I_1 left in Definition 2)\n- Page 2 The learning theory and LPN problem -> Learning Theory and the LPN problem.\n- Table 2: Nerual Distinguisher\n- L899: nature->natural\n- L912: \"and Transformer\" there are no Transformers in the figure, also the citation should maybe be for the published version if included.\n- L1028: unclear sentence\n\n\n\nReferences:\n[1]:  Hou, ZeZhou, Ren, JiongJiong, Chen, ShaoZhen, Improve Neural Distinguishers of SIMON and SPECK, Security and Communication Networks, 2021, 9288229, 11 pages, 2021. https://doi.org/10.1155/2021/9288229 \n\n[2]: Gerault, D., Hambitzer, A., Huppert, M., & Picek, S. (2024). SoK: 6 Years of Neural Differential Cryptanalysis. Cryptology ePrint Archive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wNKYpyKTZM", "forum": "N0Wh7ND71X", "replyto": "N0Wh7ND71X", "signatures": ["ICLR.cc/2026/Conference/Submission10849/Reviewer_3K9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10849/Reviewer_3K9D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992229104, "cdate": 1761992229104, "tmdate": 1762922048296, "mdate": 1762922048296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}