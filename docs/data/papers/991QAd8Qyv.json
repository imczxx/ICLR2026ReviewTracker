{"id": "991QAd8Qyv", "number": 2784, "cdate": 1757248671877, "mdate": 1762958677653, "content": {"title": "Plug-and-Play Global Memory via Test-Time Registers", "abstract": "A well-known challenge in large attention-based architectures, e.g., Vision Transformers (ViTs) and large language models (LLMs), is the emergence of attention sinks, where a small subset of tokens disproportionately attracts attention and ultimately degrades downstream performance. Prior work typically casts these high-norm tokens in ViTs as \"computational scratchpads\" and analogously characterizes sinks in LLMs as pressure valves that absorb surplus attention with little semantic content. This work revisits that view from the perspective of vision models. It is shown that, in large-scale vision architectures, register features in fact encode global, task-relevant information and can act as a plug-and-play global memory. Building on this insight, a training-free framework is proposed that leverages test-time register tokens as global priors to enhance both dense prediction and generative tasks, while mitigating adverse sink effects. In contrast to heuristic placement rules, a theoretically grounded token--neuron interpolation rule is introduced for robust, model-agnostic insertion. Experiments demonstrate improved generative quality and stronger text-image alignment for one-dimensional token generation with ViTs, reflected by gains in FID, IS, CLIPScore, and SigLIP metrics.", "tldr": "Plug-and-Play Global Memory", "keywords": ["Attention Sink", "Image Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/16079438ac60bf2e8a4c25a0b968dfb20a78940a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Plug-and-Play Global Memory via Test-Time Registers, a training-free framework that utilizes Vision Transformer register tokens as global priors during inference. It argues that register features encode global, low-frequency, task-relevant information rather than acting as noise or “attention sinks.” The method introduces a theoretically motivated token–neuron interpolation rule to insert registers robustly across architectures, yielding measurable improvements in generative quality and text-image alignment on benchmarks such as OpenCLIP and DINOv2."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a perspective on register features, demonstrating that they serve as a compact global memory by concentrating low-frequency, global information distinct from [CLS] and local patch representations. \n2. Its plug-and-play nature enables performance improvements in generation without altering pretrained weights."}, "weaknesses": {"value": "1. The paper lacks ablation experiments to clarify the individual contribution of components like the NFN layer selection, TokenRank gating, and the interpolation rule.\n2. The method introduces many hyperparameters (e.g., k, n, w, s, α) without clear guidance or justification for their choices.\n3. Compared to the strong baseline Vision Transformers Don’t Need Trained Registers (Jiang et al., 2025), the performance improvements are minor (FID 21.5->20.3, IS 283->287), raising questions about novelty and impact.\n4. There is limited analysis of sensitivity or generalization, making it unclear how robust the method remains across architectures or tasks."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O9mlgD8nPa", "forum": "991QAd8Qyv", "replyto": "991QAd8Qyv", "signatures": ["ICLR.cc/2026/Conference/Submission2784/Reviewer_zmKk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2784/Reviewer_zmKk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381529360, "cdate": 1761381529360, "tmdate": 1762916375619, "mdate": 1762916375619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "WVDj5PAW7v", "forum": "991QAd8Qyv", "replyto": "991QAd8Qyv", "signatures": ["ICLR.cc/2026/Conference/Submission2784/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2784/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762958676955, "cdate": 1762958676955, "tmdate": 1762958676955, "mdate": 1762958676955, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes that attention sinks/register tokens encode global information in Vision Transformers and can be used to control generation. The authors use NFN (Normalized Feature Norm) to identify outlier neurons and transfer outlier activations to inserted register tokens. They validate the approach on OpenCLIP and DINOv2, and demonstrate generation results using HCT decoding, showing modest improvements over prior methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Statistical analysis of information across different tokens \n2. The proposed register tokens effectively absorb outliers and attention sinks"}, "weaknesses": {"value": "1. The claim about \"encoding global information\" has no direct relationship with the subsequent method.\n2. Although the authors criticize prior work for being architecture-specific with many hyperparameters, the proposed method appears more complex and requires even more hyperparameters.\n3. Despite claiming that registers can serve as \"priors\" or \"encode global, task-relevant information and can be repurposed as reusable memory,\" no direct experiments substantiate these claims.\n4. Unclear analysis:\n  - Which specific vectors are used for FFT to obtain frequency components?\n  - Why do modules with low NFN indicate outlier neurons?"}, "questions": {"value": "##  1. Section 3.1 FFT analysis details: \n\n1.1 Line 195 mentions applying FFT to \"token features.\" Given that channels are exchangeable, is it reasonable to perform FFT across the channel dimension? \n\n1.2 Line 171 mentions using \"PCA-projected features.\" Which implementation is actually used—FFT on raw features or on PCA-projected features?\n\n1.3 If FFT is applied after PCA projection, does this have statistical meaning, and how should the results be interpreted?\n\n## 2. Why does low NFN indicate register tokens?\n\nThe paper selects layers with the lowest NFN scores to identify register/sink phenomena. However, wouldn't NFN >> 1 better indicate that most neurons are activated by specific input directions? \n         \n## 3.  How are register tokens used as global/task-relevant information in experiments? \n\nThe paper claims register tokens encode \"global, task-relevant information,\" but how is this demonstrated experimentally?\n In Section 5.2, are the register features extracted from the same image being generated, or from different images? If from the same image, how is this a \"prior\" rather than self-conditioning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NxuoXcIQ6h", "forum": "991QAd8Qyv", "replyto": "991QAd8Qyv", "signatures": ["ICLR.cc/2026/Conference/Submission2784/Reviewer_zV3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2784/Reviewer_zV3Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624071001, "cdate": 1761624071001, "tmdate": 1762916375430, "mdate": 1762916375430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method to rework, add, or retrofit test time registry tokens [1] in pre-trained foundation models to act as latent context tokens, similar to existing latent token approaches such as TiTok [2]. It is a little unclear what the authors intend at the outset of the manuscript, since they outline a study of existing registry tokens, yet seemingly wants to construct additional registry tokens at test time via interpolation, which this reviewer reads as the main contribution of the work. The final explicitly claimed contribution is made regarding generative tasks. \n\nThe aforementioned study (the first contribution) is motivated with arguments from statistical signal processing using 1000 images from ImageNet, which the authors thereby claim demonstrates how registry tokens have “more low frequency energy”.  The authors argue that this shows evidence for “global memory”.  The motivation and central methodology of this study comes across as unclear, potentially flawed, and seemingly unrelated to the central contribution of the paper. \n\nThe authors then present a method based on existing work (NFN and TokenRank) providing a method for identifying where and how to insert test-time register tokens as a kind of plug-and-play global memory without retraining. This is performed via an interpolation method, which is what this reviewer reads as the central contribution. The third contribution is an evaluation of generative approaches with HCT [3] (this abbreviation is not connected to a citation in the related work, and is inferred form the readers perspective). \n\nOverall, the paper comes across as unclear or underwritten, with experiments and studies that seem only tangentially related to the key contributions. \n\n[1] [Darcet et al. 2024 - Vision Transformers need Registers](https://arxiv.org/abs/2309.16588)\n\n[2] [Yu et al. 2024 - An Image is Worth 32 Tokens for Reconstruction and Generation](https://arxiv.org/abs/2406.07550)\n\n[3] [Beyer et al. 2025 - Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The most plausible source of novelty here is the projection and conservation interpolation rule (Sec. 4.3) (Eqs. 8–10), which (if this reviewer understands the work correctly) proposes to shift register-subspace energy from outlier patches into a test-time register, while stabilizing scales. By contrast, the NFN layer selection and TokenRank gating are repurposed diagnostics from prior work. \n2. In a more generous reading, the authors seem to attempt to connect token- and neuron-level diagnostics into a single pipeline. That kind of idea is conceptually interesting as an effort toward unifying perspectives across the two dimensions. \n\nIf the “training-free” fitting of inference time latents-from-registers fully live up to the authors stated claims, then this is a clear strength. However, the presentation in the current manuscript is currently too unclear for this reviewer to agree that this is convincingly established as a result."}, "weaknesses": {"value": "1. The motivation for the statistical image processing study is not clearly related to the contributions of the work. The authors seek to establish properties with train-time registry tokens, however, since the proposed method is exclusively based on new test-time registry tokens, this link is not clear to this reviewer. What exactly does this study tell us about the new “plug-and-play” memory tokens the authors propose?\n2. The theoretical basis and methodology used in the motivating study is dubious. The authors are mixing concepts in interpreting PCA as a generalised Fourier transform (a valid equivalence in specific circumstances) with harmonic analysis of the PCA over 1000 sampled image tokens. This latter view is problematic, since taking the FT of the PCA over tokens imposes an order on the dimensions of the embedding which is not inherently meaningful, and does not correspond to spatial frequency in a manner which the authors argue. Either this reviewer is missing something, or this is a flawed method for showing some vague “global” property in the registry tokens.\n3. There seems to be a bit of circular reasoning with regard to the initial study; the authors assume registers encode “global context,” then define “low-frequency” as global, and unsurprisingly find that registers are “low-frequency.” It’s a form of self-confirming analysis.\n4. The sample size for the study is arbitrarily limited to 1000 images; which says very little about ImageNet as a whole. At a minimum, the study could easily be extended to 50k to 100k samples without much computational overhead, matching the validation fold as a bare minimum. A PCA using less than 0.1% of samples is prone to noise and spurious correlations. Also, this reviewer cannot seem to find a reference to HOW these were sampled; making the confusion even worse. Are all classes represented? Is it uniformly sampled? The reader is left guessing, which makes the study difficult to accept as valid. \n5. The explanations behind the key method does not provide a very clear picture of the main contribution. Section 4 makes an attempt to mathematical exposition the key components in the method, but misses the mark. Figure 1 did not assist this reviewer in understanding the method either. \n6. There is a worrying ambiguity between the claimed contributions in the paper and existing work. The result is confusing; the reader can’t make heads or tails of what the central contributions are, and what has already been proposed by other researchers.\n    - A central method HCT is not properly introduced. The authors seem to treat it as shorthand for “Highly Compressed Tokenizer” [3], but it’s never defined. Likewise, “Fourier-VLM” and “TexTok/REG” are mentioned only as narrative hooks, with no bibliographic entries or in-text citations  despite the reference list including papers with those titles, the connections aren’t established in-text.\n    - The central contribution of the current paper is arguably the interpolation rule (projection and conservation) in addition to the pre-head normalisation and stability (Sec 4.3). This comes after establishing NFN and TokenRank as central components, which makes it difficult to parse where the contribution of the authors actually lie. This can likely be fixed easily, but in the context of ambiguous citations and unclear contributions, it comes across as unfinished.\n7. This reviewer has a central concern with the evaluation of the work, as it seems that the evaluation lacks coherence with the stated motivation. The authors measure token diagnostic metrics, and generative performance. The diagnostic metrics (token norms, outlier ratios) are internal to ViTs and confirm only that injected registers alter attention statistics. The subsequent generative tests, performed through HCT/VQ-LL-32 decoding pipeline, are disconnected from this analysis and only marginally support the “global prior” claim. No evidence is provided that the proposed interpolation rule improves standard vision tasks (classification, retrieval, or segmentation), nor is the choice of HCT justified or reproducible. As a result, the evaluation does not convincingly demonstrate the practical utility of the method at the level that is currently claimed."}, "questions": {"value": "1. What is the intended role of the PCA–FFT analysis in motivating or validating the proposed test-time registers? How does this study on train-time registers inform the design or behaviour of the new “plug-and-play” memory tokens introduced later?\n2. The paper risks circularity in equating “low-frequency” with “global” and then concluding that registers are global because they are low-frequency. How do the authors operationalize “global” without invoking the frequency proxy (e.g., explicit global attribute metrics or controls) to avoid this loop?\n3. Could the authors clearly state which components of the proposed method are novel? For instance, is the projection-and-conservation interpolation rule in Section 4.3 (Eqs. 8–10) entirely original, or partly adapted from prior work? How do NFN and TokenRank differ from their original formulations?\n4. How sensitive are the reported results to the inclusion of each component? Have the authors tested variants with only NFN, only TokenRank, or only the interpolation rule to isolate their respective effects?\n5. Can the authors confirm that HCT refers to “Highly Compressed Tokenizer” [3]? Why was this pipeline chosen as the evaluation setting for “global priors,” and what are its implementation details (codebook, optimization steps, β / γ sweeps, seeds)? Why aren’t other baselines evaluated?\n6. Are the small reported improvements in Table 3 statistically significant? How many random seeds were used, and what is the variance across runs?\n7. In the PCA-to-FFT study, over what variable is the one-dimensional FFT computed, and why is that ordering semantically meaningful? Would whitening not remove the very correlations being interpreted as low-frequency energy?\n8. How were the 1000 ImageNet images sampled for that analysis? Do the results hold when the sample size is increased?\n9. If register tokens are hypothesized to encode “global” visual attributes, have the authors tested this directly? For example, by quantifying changes in global image properties (illumination, color, layout) when injecting their registers versus a [CLS] or random prior? Can the authors convincingly include such experiments?\n10. Have the authors examined whether the proposed method improves any discriminative or dense prediction tasks, such as classification, retrieval, or segmentation, where attention structure is directly relevant? If not, why is this omitted, and why is the evaluation on generative tasks related to the study on registry tokens in foundation models?\n11. Several aspects of the implementation and evaluation remain unclear. Could the authors comment on whether they believe the study is reproducible from the information currently provided in the text?\n\n### Suggestions\n\nSeparate from these questions, this reviewer would suggest the authors strive to rhetorically and logically connect the contributions to the studies and experimental results of the paper. The authors are also encouraged to demonstrate the feasibility of their method to non-generative tasks, which seem somewhat \"bolted on\" with regards to the theoretical motivation in the paper. Lastly, restructuring the mathematical exposition by having a small summary in prose before each section would help readability, and clarify the goals as the method is developed throughout the text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tWze3sFTKr", "forum": "991QAd8Qyv", "replyto": "991QAd8Qyv", "signatures": ["ICLR.cc/2026/Conference/Submission2784/Reviewer_3pSt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2784/Reviewer_3pSt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891124799, "cdate": 1761891124799, "tmdate": 1762916375178, "mdate": 1762916375178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits “attention sinks” in large attention-based models and argues that register tokens encode global, low-frequency, task-relevant information that can be used as a plug-and-play global memory at test time. The authors propose a training-free pipeline that (i) selects intervention layers with Normalized Feature Norms, (ii) extracts a register subspace by ranking high-activation channels around outlier patches, (iii) introduces a token–neuron interpolation rule that conserves energy while shifting content from outlier patches into new test-time register tokens, and (iv) quantifies/controls token influence with TokenRankcoupled with a value “write mass” measure. They then use the resulting register vector as a global prior in 1-D token generative decoders, reporting small but consistent gains on FID, IS, CLIPScore, SigLIP and reductions in outlier patches / faster attention mixing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1)\tThe paper’s core technical novelty is concrete and useful: it introduces an NFN-guided placement plus a token–neuron interpolation rule that bridges token-level sinks and neuron-level activations. The proposed method clearly explains several key decisions including 1) how to locate layers, 2) which channels to move, and 3) how to move them.\n2)\tThe work proposes TokenRank with a value write-mass measure. This value indicates whether the inserted registers truly become global absorbers and whether patch outliers are suppressed.\n3)\tThe framework is training free. The framework adds only a few tokens, and uses forward-pass statistics. This makes it easy to adopt in frozen pipelines."}, "weaknesses": {"value": "1. Your pipeline is close to Jiang et al., 2025 (test-time registers) and Darcet et al., 2024 (trained registers); the distinct elements are NFN-guided placement, token–neuron interpolation, and TokenRank gating. The impacts of the proposed elements are not clearly presented in the paper. To attribute your contribution and quantify its unique impact, could you run ablations to explain the effectiveness of each components?\n2. To position your method as the preferable test-time control under matched conditions, could you include these training-free baselines on the same Fourier-VLM/HCT/TiTok (referred in Section 2 in your paper) pipeline and report quality–latency trade-offs?\n3. The paper argues generality (LLMs/LMMs/DiT) but evaluates only ViTs and 1D token decoders. Could you provide supported material for this claim?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CuYmA72BR9", "forum": "991QAd8Qyv", "replyto": "991QAd8Qyv", "signatures": ["ICLR.cc/2026/Conference/Submission2784/Reviewer_bken"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2784/Reviewer_bken"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972309021, "cdate": 1761972309021, "tmdate": 1762916375014, "mdate": 1762916375014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}