{"id": "a8uipkMIZN", "number": 10530, "cdate": 1758174770921, "mdate": 1759897645411, "content": {"title": "A Theoretical Framework for Escaping Local Optima in MSE toward Global Convergence", "abstract": "Deep learning models are trained by minimizing loss functions such as mean squared error (MSE) or cross-entropy, but these objectives are highly non-convex. As a result, optimization often encounters local optima, saddle points, or sharp valleys that hinder convergence and generalization. Although many heuristic approaches, such as momentum, Adam, help mitigate these issues, they provide limited theoretical understanding.\nIn this work, we present a theoretical study of the optimization of MSE. We first provide a mathematical characterization of local optima under MSE and contrast them with those of cross-entropy, identifying when and how they arise. Building on this analysis, we introduce a modified optimization algorithm that explicitly accounts for these properties. Unlike heuristic methods, our approach offers theoretical guarantees for avoiding spurious local traps.\nOur experiments show that the proposed method reliably avoids local optima and converges more effectively than existing optimizers in MNIST, CIFAR10 and CIFAR100 with simple CNN. Our work provides both new insight into MSE optimization for training deep networks.", "tldr": "We provide a theoretical analysis of local optima in MSE loss and propose a principled optimizer that avoids traps, offering both mathematical guarantees and improved training performance.", "keywords": ["Mean Square Error", "Local Optimum", "Linear Algebra"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba0c46179f16c18aa6c933a6f3653ab145a343ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is a theoretical study of the optimization landscapes of loss functions like MSE and cross-entropy. The authors present a new loss based on known claims in the field and show experimental results to support the claim that their new loss function avoids local optima and converges more effectively than existing optimizers on a specific set of datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "I believe authors have attempted to study a interesting problem of escaping local optima and analyzing existing optimizers characteristics. I feel the authors have an interesting idea but the whole presentation lacks rigor and is confusing with many errors."}, "weaknesses": {"value": "This paper is definitely not at publishable quality right now.\n\n1) Lot of grammatical errors, QMSE - only used as abbreviation no concrete explanation.\n2) Missing related work.\n3) I also find many logical flaws in the motivation and general study here. The issues identified are really about the neural network optimization landscape, not MSE specifically. I am not sure if the authors are confused between ideas of optimization surface and loss function. Ex: The paper states MSE loss is 'non-convex due to the model's activations', but then they say that the issue is in with the MSE?\n4) Additionally, central claim that MSE has inherent local optima problems is not well-supported. \n5) The supporting mathematical claims has errors and there is lack of detail on QMSE.\n\nThe issues discussed are really about the neural network optimization landscape (the Jacobian structure, linear dependencies in gradients), not something inherent to MSE specifically. I am not sure where are the authors are going with the claims. What is $f$ here? How is it defined?"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8WsDYpQOeC", "forum": "a8uipkMIZN", "replyto": "a8uipkMIZN", "signatures": ["ICLR.cc/2026/Conference/Submission10530/Reviewer_uHc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10530/Reviewer_uHc4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725216422, "cdate": 1761725216422, "tmdate": 1762921809726, "mdate": 1762921809726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the local optima problem in the optimization of MSE loss in deep learning. It first provides a mathematical characterization of local optima for MSE (compared with cross-entropy) and identifies their root cause (the null space of the Jacobian matrix leading to zero linear combination of gradients). Building on this analysis, the authors propose QMSE, a modified MSE loss. Experiments on MNIST, CIFAR-10, and CIFAR-100 with a simple CNN show that QMSE outperforms vanilla MSE in training accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "QMSE is a lightweight modification of MSE, requiring no major changes to the training pipeline."}, "weaknesses": {"value": "- Most theoretical analyses are straightforward and largely well-known, and some descriptions lack rigor (e.g., Line 157).\n\n- Experimentally, QMSE improves training accuracy but achieves similar validation accuracy compared to vanilla MSE (e.g., Fig. 2(a)).\n\n- Both theoretical analysis and experimental results are insufficient to support this method."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2hlMcjjO2", "forum": "a8uipkMIZN", "replyto": "a8uipkMIZN", "signatures": ["ICLR.cc/2026/Conference/Submission10530/Reviewer_ebWV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10530/Reviewer_ebWV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904087434, "cdate": 1761904087434, "tmdate": 1762921809160, "mdate": 1762921809160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal is to provide a theoretical framework for escaping local optima in MSE toward global convergence. This is a well motivated problem as non-linear optimization on MSE loss are generally notoriously hard to optimize. The authors address this through standard analysis of MSE and Cross-entropy losses. The main contribution is the presentation of a QMSE, a modified MSE loss that uses a quadratic norm instead of the standard L2 norm. Standard analysis of this loss supports some beneficial properties for optimization. Numerical investigation training CNNs on MNIST, CIFAR-10, and CIFAR-100 indicate modest to negligible practical impact on training and validation loss.\nThe submission is generally clear, though several writing issues are present. Several variables are not defined, some equations are mis-referenced. In particular, I found the text associated with Section 3.1, which presents the QMSE loss, to be difficult to read. Some claims are simply not supported by the presented results (e.g., QMSE has enhanced sensitivity)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The introduction of QMSE is potentially impactful and is the main contribution."}, "weaknesses": {"value": "There is no formal analysis of the optimization dynamics of the error functions\nThere is no numerical analysis of the critical points of the error functions\nThe demonstrated practical effects of the proposed loss are negligible\nOnly small models are presented.\nThe references are extremely spars\nThe writing needs to be clarified in many places.\nSome of the observations about MSE loss have been made before (e.g., Frye et al., Neural Computation).\nNot enough detail is provided on the numerical experiments to enable reproduction\nNo software is provided."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mv4iF8rJ5k", "forum": "a8uipkMIZN", "replyto": "a8uipkMIZN", "signatures": ["ICLR.cc/2026/Conference/Submission10530/Reviewer_tuQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10530/Reviewer_tuQd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919883338, "cdate": 1761919883338, "tmdate": 1762921808675, "mdate": 1762921808675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies optimization with MSE in deep networks. It derives a first-order stationarity condition for MSE and argues that spurious local optima can occur when the parameter vector falls into the null space of the Jacobian. To mitigate this, it proposes a “QMSE” objective that replaces the usual ℓ2 norm with a data-dependent quadratic form $Q=(y-f+v)(y-f+v)^T$, where v is is a random unit vector chosen so that $J^Tv\\neq 0$. Experiments on MNIST, CIFAR-10, and CIFAR-100 with a simple CNN claim that QMSE attains higher training accuracy (and sometimes validation accuracy) than vanilla MSE, especially without momentum."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. A simple, implementable idea (injecting a random direction v) that—at least heuristically—ensures a non-zero projected gradient and can be realized via efficient VJP/JVP operations."}, "weaknesses": {"value": "1. “Theoretical guarantees” are claimed but not provided. The abstract says the method “offers theoretical guarantees for avoiding spurious local traps,” but the body contains no formal theorems, assumptions, or proofs establishing convergence or even non-vanishing gradients beyond heuristics.\n2. Method novelty and positioning. The proposal effectively adds a random directional component to the residual so that $J^Tv\\neq0$, which resembles perturbed/annealed gradients or gradient noise injection, widely studied for escaping saddles. The paper does not position QMSE relative to these families of methods nor to modern sharpness-aware or entropy-based objectives. This raises novelty concerns.\n3. Limited scope and baselines. Only a “simple CNN” and small datasets are used; no modern architectures (e.g., ResNet-18) or larger scales. Comparisons exclude common baselines that also encourage escape from sharp/degenerate regions (Adam, RMSProp, SGD+noise, Lookahead, SAM, label smoothing, CE).\n4. Focus on training accuracy with weak generalization gains. The paper itself notes validation accuracy is similar to MSE on CIFAR-10, with the main gains in training accuracy; on CIFAR-100 the model underfits badly (train acc ~0.15–0.35). Thus, evidence that QMSE improves useful convergence (i.e., generalization) is limited.\n5. Clarity & writing. Numerous grammatical issues and ambiguous phrasing (e.g., “mean squrared,” “modi-fied,” “keep the gradient moving artificially”) and inconsistent notation formatting. This affects readability and precision. (Examples across the Abstract, Intro, and Sections 2–3.)"}, "questions": {"value": "1. The delineation between global and local minima is too coarse and offers limited insight. Prior work [1] analyzes MSE in an unconstrained feature model and proves there are no spurious saddle points—every local minimum is global. This suggests your Cases 2 and 3 effectively collapse to Case 1. Do you contend that [1] is incorrect, or do your assumptions differ in a way that invalidates its result? Please clarify and provide a formal basis (e.g., a counterexample, differing setting, or additional constraints and corresponding proof to support your claim).\n\n[1] Zhou et al. On the optimization landscape of neural collapse under mse loss:Global optimality with unconstrained features."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "032W49Rni8", "forum": "a8uipkMIZN", "replyto": "a8uipkMIZN", "signatures": ["ICLR.cc/2026/Conference/Submission10530/Reviewer_vo6u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10530/Reviewer_vo6u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946959930, "cdate": 1761946959930, "tmdate": 1762921808025, "mdate": 1762921808025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}