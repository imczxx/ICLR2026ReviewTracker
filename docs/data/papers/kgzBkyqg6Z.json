{"id": "kgzBkyqg6Z", "number": 5797, "cdate": 1757935616432, "mdate": 1759897952613, "content": {"title": "Towards a Collaborative Memory for Agentic Workflow: Breaking the Prefix Barrier with Segment-Level KV Cache Sharing", "abstract": "In LLMs-based multi-agent systems, the Key-Value (KV) cache serves as a critical carrier of agents' working memory, and its efficient reuse is paramount for enhancing the service throughput and inference efficiency. However, prevailing KV cache reuse methods rely heavily on a rigid prefix matching mechanism, which mandates exact equivalence between the query request and the cached prefix. This inflexible matching scheme struggles to accommodate the highly heterogeneous instruction prompt template in multi-agent environments, thereby severely constraining the overall system throughput. To overcome these limitations, this paper introduces a novel collaborative memory approach, underpinned by a Segment-Level KV Cache Sharing mechanism. This method decomposes the cache unit into fine-grained semantic segments, enabling agents to dynamically reuse KV cache segments generated by any other agent at arbitrary positions, without relying on sequential prefix consistency. Our approach not only significantly boosts the inference efficiency of LLMs in agentic workflows but also achieves genuine working memory sharing and collaboration, thereby enhancing cooperative capabilities among agents. Our implementation is built upon the vLLM framework and leverages the PageAttention mechanism. Extensive experimental results demonstrate that the proposed method markedly reduces redundant computation, increases system throughput, and even improves the performance of agentic workflow on benchmark tests through effective working memory sharing.", "tldr": "", "keywords": ["KV Cache Sharing", "Multi-Agent", "Working Memory"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08c97bac12f3bd92c053a321cedab68f161c64a2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the inefficiency of KV cache reuse in multi-agent LLM systems. Existing methods rely on strict prefix matching, making cache reuse rare under heterogeneous prompts. The authors propose a Segment-Level KV Cache Sharing mechanism that decomposes the cache into semantically coherent segments, enabling agents to reuse KV segments across contexts without prefix alignment.\nThey implement a high-performance prototype, CrossKV, on top of the vLLM engine using PageAttention, which introduces a memory table for segment-level KV aliasing and retrieval.\nExtensive experiments on multiple models (Qwen2.5, Llama3) and agentic workflows (AutoGen, MAD, Solver) demonstrate up to 4.6× TTFT speedup and even performance gains in several reasoning benchmarks. The paper also analyzes the effect of positional encoding (RoPE) and proposes adaptive partial recomputation strategies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Segment-level KV cache sharing breaks the prefix-matching bottleneck in LLM inference.\n2. CrossKV is a complete, model-agnostic prototype built on vLLM.\n3. Includes in-depth discussion on positional encoding, recomputation, and memory overhead."}, "weaknesses": {"value": "1. Lack of formal theory explaining semantic stability of reused KV segments.\n2. Limited comparison to other advanced caching methods (e.g., KVShare, CacheBlend).\n3. RoPE correction introduces extra memory and computation overhead, and long-segment reuse may require partial recomputation, increasing system complexity.\n4. Absence of large-scale real-world multi-agent case studies."}, "questions": {"value": "1. How are the semantic boundaries of segments detected or defined in practice?\n2. Could segment aliasing introduce semantic drift or context leakage across agents?\n3. How would CrossKV behave in cross-lingual or high-noise agent communication?\n4. Is there any safeguard to prevent incorrect segment matching (hash collision or semantic mismatch)?\n5. Could this approach be integrated with retrieval-augmented or external-memory systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Iul3m8ArZd", "forum": "kgzBkyqg6Z", "replyto": "kgzBkyqg6Z", "signatures": ["ICLR.cc/2026/Conference/Submission5797/Reviewer_taJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5797/Reviewer_taJt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719578480, "cdate": 1761719578480, "tmdate": 1762918267940, "mdate": 1762918267940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to address a key inefficiency in LLM-based multi-agent systems (MAS): the reliance of KV cache reuse systems on rigid prefix matching. This mechanism fails in MAS environments where agents have diverse prompt templates and contexts, leading to rare cache hits and significant redundant computation.\n\nTo solve this, the authors propose a Segment-Level KV Cache Sharing mechanism. This approach decomposes the KV cache into fine-grained \"semantic segments\". It allows any agent to reuse a cached segment from any other agent, regardless of its position in the new query, thereby enabling a collaborative working memory.\n\nThe paper also investigates the critical technical challenge of positional encoding (RoPE) mismatches and proposes an adaptive recomputation strategy as a solution. Experiments show the method significantly increases inference speed and, in some cases, improves task performance by enabling this working memory sharing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper correctly identifies a highly relevant and practical problem. As multi-agent systems become more common, the limitations of prefix-only caching become a critical bottleneck. The idea of a \"collaborative memory\" is a strong conceptual framing.\n* The proposed mechanism has the intended effect of improving the cache hit rates and prefill speeds."}, "weaknesses": {"value": "* The paper itself notes a lack of theoretical justification as a limitation. The fundamental assumption that a segment's KV cache is locally concentrated and largely context-independent is justified by citing sparsity literature and a single visual analysis (Fig. 1), but it's not deeply explored.\n* The results are presented only with dense architectures, not with other more prevalent architectures like MoE. In general, such a technique, without theoretical justification, is hard to take for granted that it will generalize broadly.\n* It is also not clear how the semantic segments are identified in practice. The LLM identifying reusable parts of its context upon prompting seems brittle.\n* In the Table 1, it is not clear why certain results will actually improve upon reusing the cached KVs. Reusing cached KVs should strictly be a inference time win, not qualitative win.\n* The manuscript in its current form is quite repetitive. The problem, solution, and contributions are restated in similar terms across the Abstract, Introduction, and Conclusion, diluting the paper's impact. The paper could be substantially shorter without losing any of its core technical merit."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PT6fDa1AFE", "forum": "kgzBkyqg6Z", "replyto": "kgzBkyqg6Z", "signatures": ["ICLR.cc/2026/Conference/Submission5797/Reviewer_Ge4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5797/Reviewer_Ge4H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892540436, "cdate": 1761892540436, "tmdate": 1762918267740, "mdate": 1762918267740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the issue of redundant computation in multi-agent LLM workflows, where agents often repeat chunks of text but prefix caching (strict reuse of KV cache based on prompt) rarely helps because prompts differ across roles and turns. The authors propose CrossKV, which shares segment-level KV cache. When a new query contains a previously generated span (i.e. continuous tokens), the system looks up that span’s hash in a \"memory table\" and aliases the current logical hashes to the same physical KV blocks in vLLM. This enables reuse even when prefixes don’t match. The evaluation take-away is: this content-driven reuse reduces redundant prefill/decoding across agents while remaining compatible with standard attention; the authors claim that positional (RoPE) shifts are usually negligible, and for rare long/complex cases the system caps sharing length and partially recompute to re-anchor positions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Very timely and important research problem. Most prior work focuses on lossless and full KV cache reuse, e.g. prefix caching. CrossKV is a type of lossy and partial KV cache reuse, and we are already seeing this paradigm emerge in popularity recently. I believe this is an under-appreciated topic in KV cache related research that deserve more attention, given the rise of agentic workload; in agentic AI, strict prefix caching becomes less useful due to increased prompt diversity, and I appreciate the authors for motivating this emerging scenario and issue with existing cache reuse methods. \n\n2. Solid and efficient system design. The proposed design of segment-level KV cache reuse eliminates additional I/O or data movement (e.g. duplicating KV cache blocks in GPU memory), which happens a lot in related work when positional encoding alignment is needed for partial KV cache reuse. The memory table design is efficient and scalable, as there is no need to store actual KV cache related tensors for efficient lookups. Microbenchmarks in the evaluation section are extensive and contribute to the point being made by the authors regarding efficiency."}, "weaknesses": {"value": "1. As the authors acknowledge (e.g., in Section 3.3), misalignment in positional encoding can be a significant concern that leads to issues such as accuracy degradation; this also seems to be the main reason why CrossKV accuracy is often lower than \"vanilla\" in Table 1. While I agree that understanding its impact theoretically may be challenging, a more thorough empirical analysis is essential before this framework can be safely adopted in practice. Specifically, it would be helpful to examine the potential consequences when positional encodings are misaligned: What are the worst-case scenarios? Could LLMs produce irrelevant or even harmful outputs? Which types of queries or benchmarks are most sensitive to positional misalignment? Is it possible to identify when (segment-level) KV cache reuse would lead to accuracy degradation and switch to recompute instead? \n\n2. The evaluation section of this paper lacks comparison to major baselines in the direction of lossy or partial KV cache reuse, like CacheBlend and KVShare, even though these work are mentioned in \"related work\"."}, "questions": {"value": "Thank you for submitting this paper to ICLR! Please refer to \"weaknesses\" for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "40KvcY7hrz", "forum": "kgzBkyqg6Z", "replyto": "kgzBkyqg6Z", "signatures": ["ICLR.cc/2026/Conference/Submission5797/Reviewer_j1n9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5797/Reviewer_j1n9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947278489, "cdate": 1761947278489, "tmdate": 1762918267331, "mdate": 1762918267331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CrossKV, a segment-level KV cache sharing mechanism that enables flexible reuse of intermediate computations across agents in multi-agent workflows without requiring prefix alignment. Built on vLLM, it decouples cache reuse from rigid prefix matching, allowing agents to share semantic segments at arbitrary positions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The segment-level sharing mechanism effectively overcomes the limitations of prefix-based caching in multi-agent systems.\n2. The implementation demonstrates practical system-level gains, with notable improvements in both throughput and task performance."}, "weaknesses": {"value": "1. Section 3.1.2 directly presents the attention map visualization comparing cases with and without segment-level sharing in Figure 1. However, at this point, the mechanism of segment-level KV sharing remains unclear—under what conditions (e.g., a certain degree of token similarity) does sharing occur, and what are the typical patterns of such segments? Section 3.1.1 also lacks rigorous and interpretable formulations, relying solely on textual descriptions, which makes Figure 1 difficult to understand.\n2. It is still unclear how CrossKV is integrated into existing multi-agent frameworks. Does it replace the original natural language communication among agents, or do agents still communicate via natural language while additionally performing KV sharing?\n3. CrossKV seems inapplicable to heterogeneous LLM-based multi-agent systems, as it does not address potential discrepancies in hidden state dimensions or distributions across different backbone LLMs. Restricting the method to homogeneous MASs substantially limits its contribution.\n4. How are the `<reuse begin>` and `<reuse end>` tags obtained? If they are generated by the model itself, how is the correctness or reasonableness of their positions ensured?\n5. When these KV caches are directly reused, does this lead to semantic discontinuity? Directly embedding the cached segments from one agent into another agent’s input without adaptation seems problematic."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DONBCxZhYw", "forum": "kgzBkyqg6Z", "replyto": "kgzBkyqg6Z", "signatures": ["ICLR.cc/2026/Conference/Submission5797/Reviewer_V51C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5797/Reviewer_V51C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969060809, "cdate": 1761969060809, "tmdate": 1762918267009, "mdate": 1762918267009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}