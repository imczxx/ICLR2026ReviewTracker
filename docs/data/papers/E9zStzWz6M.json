{"id": "E9zStzWz6M", "number": 10925, "cdate": 1758184886124, "mdate": 1763103003247, "content": {"title": "GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models", "abstract": "Recent years have seen impressive advances in text-to-image generation, with image generative or unified models, generating high-quality images from text. Yet these models still struggle with fine-grained color controllability, often failing to accurately match colors specified in text prompts. While existing benchmarks evaluate compositional reasoning and prompt adherence, none systematically assess the color precision. Color is fundamental to human visual perception and communication, critical for applications from art to design workflows requiring brand consistency. However, current benchmarks either neglect color or rely on coarse assessments, missing key capabilities like interpreting RGB values or aligning with human expectations. To this end, we propose GenColorBench, the first comprehensive benchmark for T2I color generation, grounded in color systems like ISCC-NBS and CSS3/X11, including numerical colors which are absent elsewhere. With 44K color-focused prompts covering 400+ colors, it reveals models’ true capabilities via perceptual and automated assessments. Evaluations of popular T2I models using GenColorBench show performance variations, highlighting which color conventions models understand best and identifying failure modes. Our GenColorBench assessments will allow to guide improvements in precise color generation. The benchmark will be made public upon acceptance.", "tldr": "", "keywords": ["Diffusion models", "text-to-image generation", "colors"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/282016460388444436f6bf1e83010984dd6209ce.pdf", "supplementary_material": "/attachment/885291b7985475bc20830d4d6489d4e8040ed5ea.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a benchmark for evaluating the capability of T2I models in faithfully rendering the specified colors on generated objects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To the best of my knowledge, color evaluation is indeed an overlooked aspect in existing T2I benchmarks, and this work therefore fills an important gap.\n2. The construction method, and especially the color identification protocol (lines 288-314), seems well-thought-out. However, as I am not an expert in color systems, I am not in a position to judge the reasonability, correctness, and professionalism of this specific design.\n3. The benchmark covers multiple dimensions of evaluation, which I find to be reasonable and useful.\n4. The findings in Section 4 are informative. While some of these findings might seem straightforward or have been observed in prior works, it is highly valuable that the proposed benchmark allows these informal observations to be quantitatively validated."}, "weaknesses": {"value": "1. I suggest designing a hierarchy of evaluation protocols with increasingly fine-grained color divisions. At a minimum, I would recommend adding a protocol that only involves ISCC-NBS Level 1 color names. The underlying rationale is that highly fine-grained color specification currently seems to be a niche demand, and such evaluation might be more relevant for specialized models. Subjecting general-purpose models to such strict criteria may not be necessary. A protocol with a coarser color division, perhaps coupled with greater tolerance, could provide more instructive scores for these models.\n2. (This is a suggestion, not a mandatory requirement.) I would encourage the authors to perform a correlation analysis between the models' scores on GenColorBench and their scores on other benchmarks, such as GenEval or T2I-CompBench. Such an analysis could help derive interesting conclusions, for example, whether models with strong compositional generation or instruction-following capabilities necessarily exhibit stronger color-adherence abilities."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UM8on1KxEP", "forum": "E9zStzWz6M", "replyto": "E9zStzWz6M", "signatures": ["ICLR.cc/2026/Conference/Submission10925/Reviewer_v14i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10925/Reviewer_v14i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728200221, "cdate": 1761728200221, "tmdate": 1762922126718, "mdate": 1762922126718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "DHL6CjUfEW", "forum": "E9zStzWz6M", "replyto": "E9zStzWz6M", "signatures": ["ICLR.cc/2026/Conference/Submission10925/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10925/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763103002244, "cdate": 1763103002244, "tmdate": 1763103002244, "mdate": 1763103002244, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GenColorBench, the first large-scale benchmark explicitly designed to evaluate color generation accuracy and understanding in text-to-image (T2I) and unified multimodal generative models. It identifies a gap in current evaluation benchmarks (e.g., GenEval, T2I-CompBench++, TIFA) which assess compositionality, faithfulness, or reasoning but ignore fine-grained color controllability.\n\n- Comprehensive Design:\n44K+ prompts across 5 tasks; supports linguistic and numerical color representations; standardized with ISCC–NBS and CSS3/X11 mappings.\n\n- Evaluation Protocol:\nIntroduces a hybrid perceptual–metric approach using CIELAB color space, PCA-based dominant hue extraction, and multiple JND-thresholded metrics.\n\n- Empirical Insights:\nEvaluation of major diffusion, autoregressive, and multimodal models (e.g., Flux, SD3.5, BLIP3o, OmniGen2) reveals:\nModels perform well on basic colors but fail on intermediate or modifier-based colors (e.g., greenish blue).\nColor–object association and multi-object composition remain unsolved.\nNumeric color control (RGB/hex) is particularly poor across models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear idea, addressing an interesting aspect of text-to-image evaluation — color understanding.\n2. The writing is generally clear and structured, with sufficient methodological detail and logical flow.\n3. The work offers a novel a benchmark contribution, supported by sound experimental design and comprehensive analysis across multiple models."}, "weaknesses": {"value": "1. Each object is evaluated with only a single dominant color, which may oversimplify real-world cases where objects naturally exhibit multiple colors or textures.\n\n2. There is some concern about the practical relevance of the benchmark—generative models may not need to distinguish over 400 colors, many of which are not practical or barely perceptible even to humans."}, "questions": {"value": "- 1. I find the description of object colors in the benchmark somewhat confusing. In reality, many objects naturally have multiple colors. For instance, in Figure 1, the pink car contains several other colored parts, and the “yellow cat” clearly shows a mix of yellow and white. How does the benchmark handle single objects that contain multiple colors? Has any evaluation been conducted for such cases? How to select the dominant color?\n\n\n- 2. Quality control is crucial for a benchmark. However, the paper provides insufficient discussion regarding human quality control. Human judgments of color can be subjective and imprecise. The authors should clarify how they address these issues, as well as important aspects such as the proportion of data retained after manual quality inspection.\n\n- 3. I’d really appreciate it if you could replay to the two concerns mentioned in the “Weaknesses” section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7vJy0KsGAm", "forum": "E9zStzWz6M", "replyto": "E9zStzWz6M", "signatures": ["ICLR.cc/2026/Conference/Submission10925/Reviewer_wWRc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10925/Reviewer_wWRc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966513034, "cdate": 1761966513034, "tmdate": 1762922126364, "mdate": 1762922126364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GenColorBench, the first large-scale benchmark designed to systematically evaluate color generation capabilities in text-to-image (T2I) models. \nThe authors identify a critical gap, as existing benchmarks often overlook color precision. \nGenColorBench features over 44,000 prompts grounded in established color science systems (ISCC-NBS, CSS3/X11) and uniquely incorporates numerical color formats (RGB/hex). \nIt proposes a multi-faceted evaluation framework across five distinct tasks, using a sophisticated pipeline involving VQA for object detection, Grounded SAM for segmentation, and a perceptually-grounded metric in CIELAB space to score color accuracy. \nThe extensive evaluation of state-of-the-art models reveals profound and widespread difficulties in achieving precise color control, particularly for complex prompts, and suggests that models rely heavily on dataset biases rather than true compositional understanding. \nThe work provides a valuable resource and a strong baseline for future research on color fidelity in T2I generation."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Addresses a Critical Gap: \nThe paper tackles a well-motivated and highly important limitation in current T2I evaluation. \nPrecise color control is a fundamental requirement for many practical applications, and this work provides the first systematic, large-scale tool to measure it.\n- Theoretically Grounded Methodology: \nThe benchmark's design is well-founded in color science. \nGrounding the evaluation in established, perceptually uniform color systems like ISCC-NBS and employing the 'dominant hue' concept for analysis represents a significant advance over simplistic VQA or CLIP-based scoring.\n- Comprehensive Scale and Coverage: \nWith over 44,000 prompts, 400+ colors, and five distinct evaluation tasks (including the novel Numerical Color Understanding task), the benchmark provides unprecedented scale and specificity, enabling a deep and nuanced assessment of model capabilities.\n- Insightful Empirical Findings: \nThe paper does more than just introduce a benchmark; it uses it to deliver key insights. \nThe findings—that all models struggle significantly, that performance is tied to semantic categories, and that models reflect training data biases—are valuable contributions to the field's understanding of current model limitations.\n- Exceptional Clarity and Presentation: \nThe paper is exceptionally well-written, logically structured, and easy to understand. \nThe motivation is clear, and the figures and tables (especially Table 1) are highly effective at communicating the core contributions and results."}, "weaknesses": {"value": "- Benchmark Calibration Concerns: \nThe performance scores across all evaluated models are extremely low (highest average is 22.42%). \nWithout a human performance baseline or inter-annotator agreement study, it is difficult to ascertain whether these scores reflect genuine, severe model limitations or overly stringent evaluation criteria. \nThis lack of calibration makes the absolute scores hard to interpret.\n- Arbitrary Thresholding in Evaluation Metric: \nThe Just-Noticeable-Difference (JND) threshold, a critical parameter in the scoring mechanism, is stated as 'typically 5' without strong justification. \nThe paper lacks a sensitivity analysis for this parameter, which could significantly impact the final results and conclusions.\n- Reliability of the Evaluation Pipeline: \nThe framework's accuracy depends on a chain of pre-trained models (Janus-1.3B for VQA, Grounded SAM for segmentation), which have their own failure modes. \nThe paper itself shows that VLLMs struggle with fine-grained color, raising concerns about the reliability of using one for object detection gating without a thorough analysis of potential error propagation.\n- Limited Analysis of Failure Modes: \nWhile the benchmark effectively reveals that models perform poorly on numerical color understanding (most scoring under 10%), there is insufficient analysis into *why* this task is so challenging or what specific aspects of RGB/hex processing are most problematic for current architectures.\n\n---\n\n- General Limitations:\n    - The benchmark focuses on English-centric color names and systems. Its applicability to non-English languages or cultures with different color taxonomies is not discussed.\n    - A discussion on the potential negative societal impacts is missing. For example, a benchmark that perfects model adherence to specific colors could be used for malicious purposes, such as flawlessly replicating copyrighted brand identities or creating more convincing disinformation.\n    - The evaluation protocol does not explicitly control for hyperparameters like sampling steps or guidance scale across models, which could affect the fairness of the comparison."}, "questions": {"value": "- Given the very low scores across all models, what do you consider a 'good' or 'reasonable' performance level on this benchmark? \nWould you consider conducting a human evaluation study on a subset of the prompts to establish a performance baseline and help calibrate the benchmark's difficulty?\n- Could you provide a justification for choosing a JND threshold of 5? \nHow sensitive are the reported model rankings and overall conclusions to variations in this threshold (e.g., changing it to 3 or 7)?\n- How does the known unreliability of VQA models for fine-grained tasks impact the initial object detection step? \nCould you perform an ablation study using a different, stronger VQA model—or a human-in-the-loop validation on a subset—to assess the impact of this 'gating' model choice on the final results?\n- Your analysis shows BLIP30 significantly outperforms other models on numerical color understanding. \nDo you have any hypotheses as to why its multimodal architecture might be better suited for this specific task compared to the other models tested?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "A discussion on the potential negative societal impacts is missing. For example, a benchmark that perfects model adherence to specific colors could be used for malicious purposes, such as flawlessly replicating copyrighted brand identities or creating more convincing disinformation."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5rWZCcuyXU", "forum": "E9zStzWz6M", "replyto": "E9zStzWz6M", "signatures": ["ICLR.cc/2026/Conference/Submission10925/Reviewer_9ap3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10925/Reviewer_9ap3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978477566, "cdate": 1761978477566, "tmdate": 1762922125665, "mdate": 1762922125665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GenColorBench, a large-scale benchmark (44K+ prompts) for evaluating the ability of text-to-image (T2I) models to generate precise colors. The benchmark is grounded in color systems like ISCC-NBS and CSS3/X11 and includes tasks for color name accuracy, object association, and, notably, numerical color understanding (e.g., RGB/hex codes). The authors first argue that VLM-based evaluators are unreliable (Table 2) and then propose a pixel-grounded evaluation pipeline using CIELAB color space and \"dominant hue\" extraction . The paper's main finding is that all current SOTA T2I models perform very poorly on these tasks , with the best average score being only 22.42% (Table 4)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Thorough Data Curation: The paper is thorough in its creation of the benchmark dataset, drawing from established color systems and creating a large number of prompts (44,464).\n- Perceptual Metric Choice: The choice to use CIELAB color space for evaluation  is sound, as it is more perceptually uniform than RGB."}, "weaknesses": {"value": "- Marginal/Trivial Contribution: The paper's core premise is flawed. It focuses on a niche, unimportant problem (hyper-specific color accuracy). This is a solved problem at a \"good enough\" level for most applications, and this benchmark does not measure any deeper semantic capability.\n- Flawed Methodology: The evaluation pipeline is fundamentally unsound. It relies on a VQA model (Janus-1.3B)  that the paper itself proves is unreliable (Table 2).\n- Non-Transparent Pipeline: The methodology relies on \"black box\" components, such as a GPT-generated \"negative label\" list, which makes the results un-auditable and impossible to reproduce reliably.\n- Poor Organization: The paper is poorly structured, making it difficult to read and assess."}, "questions": {"value": "- The paper's central weakness is its triviality. Can the authors provide any evidence that a model's (in)ability to generate \"moderate purplish pink\"  has any correlation with its ability to perform more complex, meaningful tasks like compositional reasoning or instruction following?\n- How can you justify using Janus-1.3B for object validation  when your own Table 2 shows this model has near-zero capability in related vision-language tasks? Please provide accuracy metrics for Janus-1.3B on the specific object presence task it is used for.\n- How can this benchmark be considered reliable when it relies on a non-transparent, GPT-generated list of \"negative labels\"? This seems like a critical, un-audited component that directly affects the final score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oSgVqCPh1r", "forum": "E9zStzWz6M", "replyto": "E9zStzWz6M", "signatures": ["ICLR.cc/2026/Conference/Submission10925/Reviewer_AJHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10925/Reviewer_AJHA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079949483, "cdate": 1762079949483, "tmdate": 1762922123751, "mdate": 1762922123751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}