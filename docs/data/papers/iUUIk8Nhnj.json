{"id": "iUUIk8Nhnj", "number": 16054, "cdate": 1758259224234, "mdate": 1759897265147, "content": {"title": "TailorPiece: Tailoring Linear Models for Joint Representation", "abstract": "The need to represent a long data series using a sequence of line segments abiding by a maximum error threshold arises in various domains. This problem, known as Piecewise Linear Approximation (PLA), has a long history and has recently gained attention with the rise of applications dealing with time-stamped data. State-of-the-art PLA methods achieve space savings over lossless compression techniques with tolerable precision loss by quantizing starting points and representing similar line segments jointly. However, these methods do not tailor line segments for their eventual joint representation and do not minimize the number of segments either. In this paper, we present TailorPiece, a suite of algorithms for lossy PLA-based compression that explicitly tailor linear segments for both small sequence length and joint representation under a given error threshold and starting-value quantization. Our first algorithm, TailorPieceDP, optimizes a mergeability criterion of PLA segment descriptions; in a degenerate form, it reduces to an algorithm that represents the data series by the minimum number of PLA segments. Our second algorithm, TailorPieceGD, greedily selects the endpoint of each PLA segment within a tunable search space that allows the subsequent segment to extend farther, thereby balancing compression and runtime. Through experimentation, we show that TailorPieceDP achieves improvements of up to 34% over prior art in compression ratio and TailorPieceGD gains similar savings with a runtime reduced by two orders of magnitude.", "tldr": "We introduce algorithms for lossy PLA compression of time series that tailor segments for joint representation.", "keywords": ["Piecewise Linear Approximation (PLA)", "lossy compression", "time series", "TailorPiece", "compression ratio"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30750e812e67505697ba3597a55710b8ed3379b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of lossy time-series compression under a fixed error bound with quantized starting values. It identifies limitations in the previous MIXPIECE method, which greedily maximizes segment length but may yield suboptimal segment counts and poor mergeability. They propose three variants: MINSEGMENTS, a dynamic programming approach for globally minimal segmentation under quantization; TAILORPIECEDP, which further optimizes for mergeability by maximizing slope interval width; and TAILORPIECEGD, a greedy lookahead version balancing compression quality and runtime."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly articulates the practical need to improve MIXPIECE’s segmentation under quantization and mergeability constraints.\n\n- The DP formulations are logically consistent and correctly defined for global optimality. The overall structure and explanations are straightforward, making the method easy to reproduce and understand.\n\n- The approach provides a tunable trade-off between accuracy and efficiency, which is valuable for real-world compression systems."}, "weaknesses": {"value": "- While technically correct, the paper’s contributions are incremental extensions of existing piecewise linear approximation (PLA) frameworks rather than fundamentally new ideas, which limits its novelty. No new theoretical model, loss formulation, or probabilistic insight into PLA is introduced. The paper essentially re-optimizes an existing heuristic with better parameterization. From a research standpoint, this positions the contribution as an engineering refinement, not a methodological breakthrough.\n\n- The paper offers no formal complexity analysis, approximation guarantees, or theoretical characterization of how the new objectives affect global optimality. For example, while MINSEGMENTS claims “globally minimal” segmentation, the proof is implicit. There is no formal definition of optimality under quantized constraints or derivation of time/space complexity. Similarly, for TAILORPIECEDP, the trade-off between segment count and interval width is handled empirically but never quantified analytically.\n\n- The evaluation focuses on synthetic or benchmark datasets (UCR) with standard metrics and lacks deeper diagnostic analysis.\n\n- No discussion on integration into full compression or streaming pipelines."}, "questions": {"value": "- How does the algorithm scale with very long or streaming sequences—can the DP version handle data in the order of millions of points?\n\n- Can the authors provide theoretical or empirical bounds linking segment count and slope interval width?\n\n- How sensitive is performance to quantization granularity or dataset smoothness?\n\n- Could the approach extend to multidimensional or irregularly sampled time series?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "98NbKEIP63", "forum": "iUUIk8Nhnj", "replyto": "iUUIk8Nhnj", "signatures": ["ICLR.cc/2026/Conference/Submission16054/Reviewer_72nW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16054/Reviewer_72nW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760772742717, "cdate": 1760772742717, "tmdate": 1762926249170, "mdate": 1762926249170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a library of algorithms for lossy-PLA compression of time-stamped data. The aim of the work is to reduce the storage requirement, and thereby, the cost, by achieving better compression rates through improving the joint representation of similar data. The algorithms were tested against 8 baselines using 41 datasets. The experiments demonstrated improvements in compression rate in addition to runtime improvements for some algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper attempts to address an important research area regarding storing big time-stamped data with the lowest possible storage cost, which is applicable to many sectors.\n- The experiments were conducted against 8 baselines.\n- The results show improvements compared with the baselines.\n- The algorithms presented in the paper are well-explained both in text and mathematically."}, "weaknesses": {"value": "- Very brief presentation and discussion of related work. The paper lacks a proper “related work” section, where the work from the literature is typically presented, discussed, and research gaps are listed. Without such section, it is not clear where the contribution of this paper stands in relation to the related literature, and whether it addresses an actual gap or not. There are scattered information about related methods across the paper; however, they do not properly replace a proper “related work” section.\n- The contribution of the paper is not clear. No specific research questions, clear setup of experiments, nor experiments goals are clearly presented.\n- When comparing the algorithms presented in the paper with the baselines, the results are presented in numerical forms without testing whether the improvements are statistically significant or not. The statistical significance of the claimed improvements in comparison with the baseline methods needs to be tested (using a Friedman test followed by Nemenyi post-hoc test, for example).\nRelatively weak benchmark. The experiments were run on a subset of datasets from the UCR Archives (41 out of 128 datasets). The authors mention that only datasets that do not contain undefined values were used. This is an issue for two reasons: (i) this is not always the case for real-world data, especially the sectors mentioned by the authors in the introduction (line 033); it is better to test the algorithms using all the datasets in the UCR Archive by passing the data as it is or preprocessing the data to handle the undefined values, or both. (ii) - The authors of the UCR Archive explicitly discourage against cherry-picking datasets from the archive [1].\n- The implications of lossy compression are not properly discussed. The paper proposed lossy compression as the solution for storing time-stamped data in a more efficient way compared with lossless compression. However, the implications of such choice are not properly discussed. For example, how accurate the original data can be reconstructed from the compressed representation and how severe do the lost information affect the usability of the data in any downstream tasks. In this end, it is important to be able to use the data after storing it.\n- Wording issues in the text that lead to ambiguity:\nLines 061–062: “TAILORPIECEDP, which, building on top of TAILORPIECEDP,”\n\n- Note: I understand that some sections might have been omitted due to the page limit. However, the authors could have made a better use of the appendix regarding the distribution of the content instead of omitting crucial details.\n\n[1] Dau HA, Bagnall A, Kamgar K, Yeh CC, Zhu Y, Gharghabi S, Ratanamahatana CA, Keogh E. The UCR time series archive. IEEE/CAA Journal of Automatica Sinica. 2019 Nov 8;6(6):1293-305"}, "questions": {"value": "- Why is the complexity of the time series data sample not considered when attempting to extract compressed representations? It might be beneficial to consider the complexity of the data when calculating the minimum-length PLA in Algorithms 3.2, for example (See, e.g. [2])\n- In all the algorithms presented in the paper, there is an error threshold (ε) as an input. However, it is not clear from the paper nor the appendix how this error threshold is calculated in the experiments and how can any future users of the algorithms calculate the error threshold. There is one brief explanation about this in lines 315–317, but it is still not clear how the values of the range are selected. Can you please elaborate on this? (the authors are encouraged to add a section about this in the paper or the appendix in any future versions).\n\n[2] Nagaraj, N., Balasubramanian, K. & Dey, S. A new complexity measure for time series analysis and classification. Eur. Phys. J. Spec. Top. 222, 847–860 (2013). https://doi.org/10.1140/epjst/e2013-01888-9"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zy4C2VJFZO", "forum": "iUUIk8Nhnj", "replyto": "iUUIk8Nhnj", "signatures": ["ICLR.cc/2026/Conference/Submission16054/Reviewer_RHoX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16054/Reviewer_RHoX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667490731, "cdate": 1761667490731, "tmdate": 1762926248784, "mdate": 1762926248784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TAILORPIECE, a method that performs piecewise linear approximation (PLA) under a maximum error constraint, while balancing the minimal number of segments and segment mergeability. By combining dynamic programming and greedy strategies, the approach effectively improves the compression ratio while maintaining approximation accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The framework explicitly optimizes for segment mergeability, allowing similar line segments to be grouped and jointly represented, thereby improving compression efficiency.\n\nS2. Two hyperparameters (p and q) are introduced to flexibly balance compression accuracy and runtime, enabling users to tune the method \naccording to application needs.|\n\nS3. TailorPiece advances beyond the previous state-of-the-art O(nlogn) complexity by introducing algorithms with O(Rn)."}, "weaknesses": {"value": "W1. The TAILORPIECEGD algorithm is a heuristic greedy approach that does not guarantee global optimality. Moreover, its performance depends on two hyperparameters p and q, which require manual tuning to achieve the desired trade-off between compression accuracy and runtime.\n\nW2. TailorPiece quantizes the segment starting value $v$ into discrete levels defined by the error bound $\\varepsilon$, using\n$$\nb^- = \\lfloor v / \\varepsilon \\rfloor \\times \\varepsilon, \\quad \nb^+ = \\lceil v / \\varepsilon \\rceil \\times \\varepsilon.\n$$\nwhen $v$ is an exact multiple of $\\varepsilon$, the lower and upper bounds collapse ($b^- = b^+$), leaving no feasible interval for numerical tolerance.\n\nW3. The baseline HIRE (Barbarioli et al., 2023) explicitly defines its reconstruction constraint in terms of L-infinity rather than L2. Therefore, classifying HIRE under L2-based methods could be misleading."}, "questions": {"value": "Q1. The compression ratios of some datasets in Table 2 (such as Rock and CinCECGTorso) are very high. Is this related to the data characteristics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TFyuBTSIo3", "forum": "iUUIk8Nhnj", "replyto": "iUUIk8Nhnj", "signatures": ["ICLR.cc/2026/Conference/Submission16054/Reviewer_6z6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16054/Reviewer_6z6d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897272997, "cdate": 1761897272997, "tmdate": 1762926248454, "mdate": 1762926248454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a suite of techniques for the Piecewise linear approximation. \nThe techniques are well described with definitions and problems.\nThe performances are compared with MIXpiece algorithm for the same task. Multiple experiments on compression and approximation are presented.\n\nThe proposed techniques are a slight variation on similar optimization\nFor example, in Figure 11, the TailorPieceGD shows a very similar compression ratio of TailorPieceDP with a reduced compression time. The best compression of TailorPieceDP is better but the improvement is less than 10%.\nThe comparisons with other techniques than MiXPiece are limited. Also other techniques base on euristics could be used for the same problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The approximation with sequence of values with Piecewise linear approximation achieves good results and the proposed techiques are better than MIxPiece technique"}, "weaknesses": {"value": "The comparison with other techniques also from other optimization paradigms is limited"}, "questions": {"value": "Can the algorithm of the proposed suite be integrated into a single technique that is optimized adaptively according to the approximation error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BegXwYxZuY", "forum": "iUUIk8Nhnj", "replyto": "iUUIk8Nhnj", "signatures": ["ICLR.cc/2026/Conference/Submission16054/Reviewer_DXa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16054/Reviewer_DXa6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917902410, "cdate": 1761917902410, "tmdate": 1762926248033, "mdate": 1762926248033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}