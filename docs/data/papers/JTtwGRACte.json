{"id": "JTtwGRACte", "number": 15071, "cdate": 1758247437994, "mdate": 1763630567967, "content": {"title": "Characteristic Root Analysis and Regularization for Linear Time Series Forecasting", "abstract": "Time series forecasting remains a critical challenge across numerous domains, yet the effectiveness of complex models often varies unpredictably across datasets. Recent studies highlight the surprising competitiveness of simple linear models,  suggesting that their robustness and interpretability warrant deeper theoretical investigation. This paper presents a systematic study of linear models for time series forecasting, with a focus on the role of characteristic roots in temporal dynamics. We begin by analyzing the noise-free setting, where we show that characteristic roots govern long-term behavior and explain how design choices such as instance normalization and channel independence affect model capabilities. We then extend our analysis to the noisy regime, revealing that models tend to produce spurious roots. This leads to the identification of a key data-scaling property: mitigating the influence of noise requires disproportionately large training data, highlighting the need for structural regularization. To address these challenges, we propose two complementary strategies for robust root restructuring. The first uses rank reduction techniques, including Reduced-Rank Regression and Direct Weight Rank Reduction, to recover the low-dimensional latent dynamics. The second, a novel adaptive method called Root Purge, encourages the model to learn a noise-suppressing null space during training. Extensive experiments on standard benchmarks demonstrate the effectiveness of both approaches, validating our theoretical insights and achieving state-of-the-art results in several settings. Our findings underscore the potential of integrating classical theories for linear systems with modern learning techniques to build robust, interpretable, and data-efficient forecasting models.", "tldr": "", "keywords": ["long term time series forecasting", "linear model", "characteristic roots", "modes", "noise robustness", "rank reduction", "root purge"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef8102197cfaf9cdf9db899f6c8ec39035601b95.pdf", "supplementary_material": "/attachment/a750d8be703131699382a806b7832b51b37e5f93.zip"}, "replies": [{"content": {"summary": {"value": "This paper advances a linear time-series forecasting framework centered on characteristic roots: in the noise-free regime, it shows that characteristic roots govern long-term dynamics; in noisy settings, it exposes the MSE training data-scaling law and the risk of spurious roots, indicating that simply enlarging the sample size is insufficient for robust generalization. Accordingly, the authors introduce two complementary structural regularization strategies: (i) low-rank constraints on the parameter matrix via Reduced-Rank Regression (RRR) and Direct Weight Rank Reduction (DWRR) to recover low-dimensional latent dynamics; and (ii) Root Purge, an adaptive loss that learns a noise null space and performs online root restructuring, with training applicable in either the time or frequency domain. Evaluated on benchmarks including Traffic, Electricity, Weather, Exchange, and ETT, the methods deliver strong results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. According to the reported results, the proposed methods achieve strong empirical performance across multiple datasets, which is particularly notable given the simplicity of the designs.  \n\n2. The paper provides substantial theoretical analysis with clear exposition to clarify the core concepts.  \n\n3. A clear roadmap is presented, and overall readability is high."}, "weaknesses": {"value": "1. The core insight “preserving dominant characteristic roots to suppress noise” is not novel and aligns with fundamental conclusions from classical Singular Spectrum Analysis (SSA).  \n\n2. The technical contributions of DWRR and Root Purge appear simple.  \n\n3. The Root Purge objective may induce model degeneration: the model could prefer mapping any input to zero, and the paper offers no theoretical guarantees to prevent or mitigate this failure mode.  \n\n4. The theoretical analysis depends on i.i.d. Gaussian noise and stationarity assumptions to justify data efficiency and the regularizer; these assumptions often do not hold in practice, and no generalization beyond them is provided."}, "questions": {"value": "1. Under complex, non-stationary noise typical of real-world series, do the proposed methods retain theoretical advantages?  \n\n2. Can the approach be extended to more complex deep learning architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "luT2Azb5Mv", "forum": "JTtwGRACte", "replyto": "JTtwGRACte", "signatures": ["ICLR.cc/2026/Conference/Submission15071/Reviewer_zgN9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15071/Reviewer_zgN9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761142849717, "cdate": 1761142849717, "tmdate": 1762925394478, "mdate": 1762925394478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a comprehensive study of linear models for time series forecasting, focusing on the role of characteristic roots in shaping model expressivity. Experiments validate our theoretical claims and demonstrate the effectiveness of both methods across a range of forecasting tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The research motivation of this manuscript is clear and its content is very rich。\n\n2. This manuscript is theoretical.\n\n3. Sufficient experiments have verified the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Theoretical analysis is an advantage of this work, but it can easily make it difficult for readers to understand. It is necessary to provide intuitive explanations in the key derivation process\n\n2. The manuscript lacks a comprehensive discussion of related work, which hampers readers, particularly those less familiar with the domain. Thus, it remains unclear how the proposed approach advances the state of the art or distinguishes itself from existing methods."}, "questions": {"value": "1. What is the transferability of the method?Can linear models be used in other areas than time series learning?\n\n2. Why is the frequency-domain linear layer necessary for Root-Purge instead of a mere implementation detail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9xePgM3DO9", "forum": "JTtwGRACte", "replyto": "JTtwGRACte", "signatures": ["ICLR.cc/2026/Conference/Submission15071/Reviewer_G42f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15071/Reviewer_G42f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896312373, "cdate": 1761896312373, "tmdate": 1762925393325, "mdate": 1762925393325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes linear time series forecasting via characteristic roots , identifying that noise creates \"spurious roots\" and a \"data scaling law\" that demands excessive data to mitigate this noise. To improve robustness and data efficiency, it proposes two structural regularization strategies: (1) post-hoc Rank Reduction (RRR) and (2) a novel \"Root Purge\" method. Root Purge is an adaptive training regularizer that encourages the model to learn a null space for the estimated noise (residuals)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Originality and Clarity: The \"Root Purge\" loss function is a novel, elegant, and intuitive regularization technique.\n\n2.Quality and Significance: The paper provides a valuable insight into why simple linear models fail in noisy, low-data regimes."}, "weaknesses": {"value": "1.Hyperparameter Sensitivity: The methods' performance relies heavily on tuning $\\rho$ or $\\lambda$. This data-specific tuning is difficult to estimate a priori and undermines the claim of robustness.\n\n2.Contingent on Low-Rank Assumption: The methods are motivated by an assumption that the true signal is low-rank. However, on large, complex datasets, the paper's results show the optimal rank is often full , and the proposed methods offer minimal benefit . This limits the methods' generality.\n\n3.Theory-Practice Gap (Trends): The theoretical analysis assumes homogeneous equations (constant/zero bias) . This is a poor fit for real-world data with non-stationary, dynamic trends."}, "questions": {"value": "1. Does the Low-Rank Assumption limit the generality of method, making it effective only for a specific subset of (simpler) time series datasets? \n\n2.If we encounter a complex trend in the future that Instance Normalization can't flatten, does your Root Purge method just fail? How can you convince us that it's still robust in that scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nvnue0O5xW", "forum": "JTtwGRACte", "replyto": "JTtwGRACte", "signatures": ["ICLR.cc/2026/Conference/Submission15071/Reviewer_xcu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15071/Reviewer_xcu7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936106238, "cdate": 1761936106238, "tmdate": 1762925391755, "mdate": 1762925391755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}