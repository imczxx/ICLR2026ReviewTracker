{"id": "ZWfDRKNeFe", "number": 15119, "cdate": 1758248005654, "mdate": 1759897327314, "content": {"title": "GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models", "abstract": "The widespread adoption of AI models, especially foundation models (FMs), has made a profound impact on numerous domains. However, it also raises significant ethical concerns, including bias issues. \nAlthough numerous efforts have been made to quantify and mitigate social bias in AI models, \n**geographic bias** (in short, geo-bias) receives much less attention, which presents unique challenges. \nWhile previous work has explored ways to quantify geo-bias, these measures are *model-specific* (e.g., mean absolute deviation of LLM ratings) or *spatially implicit* (e.g., average fairness scores of all spatial partitions). \nWe lack a **model-agnostic, universally applicable, and spatially explicit** geo-bias evaluation framework that allows researchers to fairly compare the geo-bias of different AI models and to understand what spatial factors contribute to the geo-bias. \nIn this paper, we establish an **information-theoretic framework for geo-bias evaluation**, called **GeoBS** (**Geo**-**B**ias **S**cores). We demonstrate the generalizability of the proposed framework by showing how to interpret and analyze existing geo-bias measures under this framework. Then, we propose three novel geo-bias scores that explicitly take intricate spatial factors (multi-scalability, distance decay, and anisotropy) into consideration. \nFinally, we conduct extensive experiments on 3 tasks, 8 datasets, and 8 models to demonstrate that both task-specific GeoAI models and general-purpose foundation models may suffer from various types of geo-bias. \nThis framework will not only advance the technical understanding of geographic bias but will also establish a foundation for integrating spatial fairness into the design, deployment, and evaluation of AI systems.", "tldr": "", "keywords": ["Fairness", "Geographical Bias;Model Evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/388846b72f5187165d368cb5aa757601b03f7779.pdf", "supplementary_material": "/attachment/6ae476da2541d935f73296103b9924673971c868.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a mathematical frame-work for measuring the geographic bias (geo-bias) of predictive models. The frame-work asserts that there are three main aspects in the design space of geo-bias metric: the map, the reference pattern, and the difference measure, whose variations can construct different metrics. The paper also proposes new metrics of geo-bias that can capture bias related to multi-scalability, distance decay, and anisotropy, and provides experiments that utilize these metrics to measure the geo-bias of several foundation models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The new metrics can be valuable for encouraging and enabling design of less geographically biased models. The insights about the existence of geo-bias in foundation models is also important for the users of these models in practice."}, "weaknesses": {"value": "1. I find the mathematical frame-work uninformative. In particular, it is not clear where and how the frame-work can be useful. For example, the new metrics constructed by the paper do not stem from the theoretical framework, nor rely on it for any mathematical guarantees about their validity.\n\n2. The figures in the paper lack clarity in my opinion, and particularly the captions are very uninformative.\n\n3. The justifications for the use of KL divergence is not convincing. The first point is that it is less computationally expensive, but there are other linear-complexity options such as total variation. The second point about KL having a “physical” interpretation seems incorrect to me, providing reference for this claim is important (note that KL is not bounded and also is not a proper distance). Lastly, it is not clear why KL is chosen compared to reverse KL.\n\n4. In Lines 374-377, the use of a binary metric for measuring continuous regression tasks seems unjustified, and the explanation of the binary metric lacks mathematical rigor.\n\n5. The exclusion of some ROIs from evaluation, as pointed out in Lines 400-401, is not well-justified.\n\n6. The choice of hyperparameters seems very ad-hoc, and it is unclear how changing these hyperparameters will affect the results and ranking of the models. This makes these metrics less useful in practice.\n\n7. The proposed metrics, and the reported results, lack any measure of statistical significance (and a way to compute confidence intervals). Therefore, it is unclear how meaningful the differences between these metrics are.\n\n8. The paper provides no validation for its proposed metrics for use in practical applications. How can a user trust conclusions drawn from the proposed metrics?\n\n9. The paper draws some hypotheses from its analyses (Lines 440-444 and 466-472), but does not explore them any further. Providing some evidence to reject/affirm these hypotheses would be valuable for the development of future models.\n\nTypos:\n\nIn Definition 4.3, “#” is undefined."}, "questions": {"value": "1. Can the authors clarify the practical importance of the theoretical frame-work? Does it provide any guarantees, etc?\n2. Can the authors provide any way to validate the metrics, for example, on synthetic data?\n3. What are confidence intervals for the metrics?\n4. What is the advantage of the proposed metric compared to performing independence test (chi-square on contingency tables) between model-correctness and geographical partitions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q5v4rw3WWB", "forum": "ZWfDRKNeFe", "replyto": "ZWfDRKNeFe", "signatures": ["ICLR.cc/2026/Conference/Submission15119/Reviewer_vQQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15119/Reviewer_vQQu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979004309, "cdate": 1761979004309, "tmdate": 1762925438840, "mdate": 1762925438840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on the geo-bias evaluation in AI models, the authors propose GeoBS, an information-theoretic, model-agnostic, and spatially explicit framework for fair geo-bias evaluation. They further introduce three SRE scores considering multi-scalability, distance decay, and anisotropy. Experiments on 3 tasks, 8 datasets, and 8 models show that both GeoAI and several general-purpose models exhibit geo-bias, highlighting the need for spatial fairness in AI design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I like this paper — combining spatial point pattern analysis with information theory for geo-bias evaluation is an innovative perspective that provides a theoretical lens. Although geographic bias might seem like a narrow topic within the broader fairness discussion, this kind of specialized, systematic, and in-depth evaluation has strong practical value.\n\n2. This work successfully demonstrates how existing geo-bias metrics (Unmarked SSI and Marked SSI) can be interpreted within this framework, demonstrating good extensibility. The proposed metrics are model-agnostic and applicable to different types of AI models. \n\n3. The spatially explicit nature makes evaluation results more interpretable. Experiments cover multiple tasks, datasets, and model types, enhancing the credibility of conclusions"}, "weaknesses": {"value": "1. The definition of \"spatial homogeneity\" is oversimplified; it's difficult to define what constitutes an \"unbiased\" reference pattern in practice. \n2. Lacks sensitivity analysis for different reference pattern choices; choice of KL divergence lacks sufficient justification.\n3. Only considers first-order statistics, potentially missing important spatial interaction information.\n4. Hyperparameter choices (ROI radius, grid size, ...) significantly impact results, but lack systematic selection guidance.\n5. Lacks statistical significance testing, making it difficult to judge whether observed differences are statistically meaningful."}, "questions": {"value": "1. How should appropriate \"unbiased\" reference patterns be selected for different application scenarios? Can you provide a  guiding discussions?\n2. Can you provide some discussions showing how these bias scores could guide practitioners in improving their models?\n3.  SRE scores themselves show high hyperparameter  sensitivity. How will you address this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mQl1bhXHCf", "forum": "ZWfDRKNeFe", "replyto": "ZWfDRKNeFe", "signatures": ["ICLR.cc/2026/Conference/Submission15119/Reviewer_y3HQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15119/Reviewer_y3HQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982009540, "cdate": 1761982009540, "tmdate": 1762925438347, "mdate": 1762925438347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an information-theoretic and model-agnostic framework for quantifying geographical bias. This framework can integrate existing metrics and proposes three new metrics that account for multi-scalability, distance decay, and anisotropy. The authors perform experiments across multiple models to reveal that there exist geobias in both task-specific geoAI models and foundation models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed framework is model-agnostic, addressing a key limitation of prior work, which tends to be restricted to specific tasks. They include ways to quantify bias by including intricate details about space, distance, and scale. This ensures the quantifying of bias in a granular way."}, "weaknesses": {"value": "In my opinion, the abstract of the paper is misleading because of the part: “ but will also establish a foundation for integrating spatial fairness into the design, deployment, and evaluation of AI systems”. The framework is purely diagnostic, and no mitigation scheme is mentioned.\n\nFigure 1 is a very important figure that helps motivate the importance of quantifying bias in different ways. But it's hard to understand what the different colored points represent since they are not all explicitly mentioned. This makes the overall message of the figure unclear. Additionally, using the same spatial area to illustrate each metric would make the differences between them easier to compare and understand. The acronym SSI is not defined and explained at this stage (it is done way later in the paper). It would have been to see if the authors described what they meant by geo bias clearly with some easy-to-understand intuitive examples.\n\nThe writing of the paper is occasionally difficult to follow. For instance, “Spatial point patterns always involve multiple locations (a single point will not form 'patterns'), so we define the unit to evaluate geo-bias as.”. This is followed by a series of undefined notations (e.g., what i,m,s, and t index). The first paragraph of Section 4 is missing logical flow. It starts off by mentioning the objectives, then explains how it serves the purpose. Where the purpose itself is not mentioned. Additionally, they state “three key factors we need to consider when designing new geo-bias metrics”, what are those key factors? It's very important to clarify the differences between unmarked SSI and marked SSI. Figure 2 can be a little misleading since two different locations are used to visualize the geo-bias scenarios. The main idea is that they would both have the same number of points, except one would just have the points against the unobserved background, and the other would have the same points with different colors representing high and low performance. The placement of Table 1 is rather odd, I would want to have it near the intro, not near the end, when I'm almost done reading. And these acronyms are used throughout the texts, not just in the following section.\n\nFor definition 4.3, having #P_k/#N outside the sum function makes no sense as to what the definition intends. They are supposed to be using the weight for each patch, not just one.\n\nConverting regression values to binary might lose valuable information, and the choice of threshold should be justified.\n\nSince SPAD is used as a baseline, details about what exactly it quantifies should be mentioned.\n\nThe claim that “The geo-bias scores are significantly lower than the task-specific counterparts” is not supported by the presented tables. While Table 4 shows lower bias values relative to Table 3, the comparison to Table 2 contradicts this assertion: many entries in Table 4 are comparable to, or higher than, those in Table 2. The authors should either (a) clarify exactly which comparisons support the “significantly lower” claim, (b) report formal significance tests (with effect sizes and confidence intervals) for the claimed differences, or (c) revise the statement to accurately reflect the mixed results.\n\nThe parameter sensitivity analysis is conducted only for a limited subset of models and datasets, without any justification for their selection. Without explaining why these particular models and datasets were chosen, the analysis cannot be considered a comprehensive or reliable assessment of parameter tuning.\n\nThe paper does motivate the importance of a model-agnostic approach and the importance of spatial info like (distance, direction, and scale). The discussion remains at a general level, reporting the presence of bias without interpreting its variation by scale, distance decay, or directional (anisotropic) patterns. Providing such an interpretation would strengthen the link between the theoretical motivation and the empirical findings.\n\nIn Figure 3, are the bars on the left for a single patch? The caption itself is not informative enough, and there's no mention of this in the text.\nDetails about the hyperparameter tuning of the models are also missing."}, "questions": {"value": "In addition to the weaknesses discussed earlier, I would appreciate the authors' responses on the following:\n\n* In Figure 3, are the bars on the left for a single patch?\n\n* Why was the parameter sensitivity analysis conducted only for a limited subset of models and datasets?\n\n*Why was GPT-4o chosen as one of the fms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tW6g2A1prF", "forum": "ZWfDRKNeFe", "replyto": "ZWfDRKNeFe", "signatures": ["ICLR.cc/2026/Conference/Submission15119/Reviewer_EaiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15119/Reviewer_EaiB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762208236404, "cdate": 1762208236404, "tmdate": 1762925437999, "mdate": 1762925437999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}