{"id": "WnRzN4U8Y8", "number": 23757, "cdate": 1758348025705, "mdate": 1759896798602, "content": {"title": "WIMFRIS: WIndow Mamba Fusion and Parameter Efficient Tuning for Referring Image Segmentation", "abstract": "Existing Parameter-Efficient Tuning (PET) methods for Referring Image Segmentation (RIS) primarily focus on layer-wise feature alignment, often neglecting the crucial role of a neck module for the intermediate fusion of aggregated multi-scale features, which creates a significant performance bottleneck. To address this limitation, we introduce WIMFRIS, a novel framework that establishes a powerful neck architecture alongside a simple yet effective PET strategy. At its core is our proposed HMF block, which first aggregates multi-scale features and then employs a novel WMF module to perform effective intermediate fusion. This WMF module leverages non-overlapping window partitioning to mitigate the information decay problem inherent in SSMs while ensuring rich local-global context interaction. Furthermore, our PET strategy enhances primary alignment with a MTA for robust textual priors, a MSA for precise vision-language fusion, and learnable emphasis parameters for adaptive stage-wise feature weighting. Extensive experiments demonstrate that WIMFRIS achieves new state-of-the-art performance across all public RIS benchmarks.", "tldr": "This paper introduces WIMFRIS, a framework that achieves state-of-the-art in referring image segmentation by proposing a novel HMF neck module to efficiently fuse text with visual features , overcoming a key performance bottleneck in prior methods.", "keywords": ["Referring image segmentation", "parameter efficient tuning", "computer vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6313a9d47a30740a11b3526fe351bfc6dc2cd66e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "WIMFRIS introduces a neck-heavy, parameter-efficient RIS framework that aggregates multi-scale DINOv2 features, fuses them with CLIP text via a windowed Mamba block, and adaptively re-weights each stage, setting new SOTA mIoU on RefCOCO/+/g with < 3 % trainable params."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First to plug a windowed SSM neck (WMF) into RIS; mitigates exponential decay of vanilla Mamba.\n2. Learnable emphasis per stage is simple yet novel for PET.\n3. Exhaustive ablations: window size, kernel configs, PET modules all explored.\n4. Plug-in HMF boosts ETRIS & DETRIS (Table 1), proving generic utility."}, "weaknesses": {"value": "1. All results are fine-tuned; real-world deployment often lacks target-domain labels.\n2. WMF prepends text to windows, but vision never feeds back to text; may miss visual disambiguation cues.\n3. Parameter efficiency ≠ inference speed; window partitioning + SSM may hurt parallelism."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i5RjdUj9Fg", "forum": "WnRzN4U8Y8", "replyto": "WnRzN4U8Y8", "signatures": ["ICLR.cc/2026/Conference/Submission23757/Reviewer_UdVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23757/Reviewer_UdVq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620737653, "cdate": 1761620737653, "tmdate": 1762942793307, "mdate": 1762942793307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel parameter-efficient tuning (PET) method named WIMFRIS for referring image segmentation. In contrast to existing PET methods that primarily focus on layer-wise feature alignment and are struggle to aggregate multi-scale features, the proposed approach introduces a simple yet effective neck architecture based on the Mamba module. WIMFRIS achieves state-of-the-art performance on standard RIS benchmarks, demonstrating both efficiency and strong segmentation capability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a new efficient parameter-efficient tuning (PET)–based referring image segmentation (RIS) approach named WIMFRIS.\n- The proposed algorithm enhances efficiency by replacing conventional blocks with an HMF block that actively leverages the Mamba architecture. In addition, it introduces several novel components—an SSM-based MTA, an MSA robust to multiple receptive fields, and an RFMixer—which together contribute to more precise vision-language fusion.\n- The method achieves state-of-the-art performance on popular RIS benchmarks, demonstrating both effectiveness and robustness."}, "weaknesses": {"value": "- Structural Issues in Writing\n   - In the Abstract, abbreviations such as HMF and WMF appear without their full names or descriptions, making it difficult for readers to understand them.\n   - Figure 1 lacks an explanation of the HMF module, requiring readers to infer that WMF is a sub-module of HMF only from context.\n- #Params of PET and Performance Comparison\n   - When comparing with existing PET methods, it would be fair to keep the number of PET parameters (#params) consistent across models. According to Table 1, when DINOv2-B/14 is used as the vision encoder, the proposed method shows only a slight improvement in performance compared to DETRIS, even though it uses more parameters. This raises concerns that the effectiveness of WIMFRIS may not be scalable.\n- Limited Novelty\n   - The paper proposes several modules (e.g., WMF, HMF, MSA, MTA), but the architectural novelty of each component seems limited. For instance, the HMF module appears to replace multiple cross-attention layers with a more efficient Mamba-based structure, but the use of Mamba itself is not novel. Similarly, the MSA and RFMixer are designed to handle multiple receptive fields, but this concept is not entirely new.\n   - The paper would benefit from additional discussion or evidence to substantiate the novelty of these architectural contributions.\n- Lack of Ablation Studies\n   - As mentioned above, the paper lacks experiments that demonstrate the effectiveness and novelty of the proposed modules. For example, it would strengthen the work to include comparisons between MSA/RFMixer and baseline or vanilla methods for handling multiple receptive fields.\n   - Table 3-(a) appears more like an engineering-oriented study rather than one providing clear scientific insight."}, "questions": {"value": "Please provide your responses with reference to the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MinqdN4erx", "forum": "WnRzN4U8Y8", "replyto": "WnRzN4U8Y8", "signatures": ["ICLR.cc/2026/Conference/Submission23757/Reviewer_CZEK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23757/Reviewer_CZEK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754617145, "cdate": 1761754617145, "tmdate": 1762942792923, "mdate": 1762942792923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WIMFRIS, a framework for Referring Image Segmentationthat focuses on both a novel intermediate fusion neck architecture (the Hierarchical Mamba Fusion, or HMF, block) and a parameter-efficient tuning strategy. The HMF block leverages a Window Mamba Fuser module to effectively aggregate and fuse multi-scale vision and language features, using window partitioning to tackle the exponential decay in information typical of state-space models. The PET strategy employs adapters to efficiently align textual and visual representations and a learnable stage-wise emphasis mechanism. Extensive experiments are conducted on major RIS benchmarks, demonstrating state-of-the-art results for WIMFRIS compared to both PET-based and full fine-tuning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- WIMFRIS achieves state-of-the-art or highly competitive performance across all standard RIS benchmarks (RefCOCO, RefCOCO+, G-Ref), outperforming previous parameter-efficient and full-tuning baselines. Table 2 clearly demonstrates these gains, including mixed-data setups.\n- Multiple ablation tables systematically dissect the contributions of each module and architectural choice.\n- The schematic diagrams provide clear breakdowns of the model pipeline, supporting the text’s descriptions of modular design and the flow of visual and textual feature processing. The visualizations  offer compelling qualitative evidence for improved segmentation, especially in challenging situations (e.g., clutter, occlusion).\n- The paper carefully characterizes the underlying exponential decay issue in SSM-based fusion, and the model’s windowed approach is well justified both mathematically and empirically.\n- WIMFRIS demonstrates competitive results while tuning a very small fraction of backbone parameters, highlighting the value for practical deployment.\n- The explicit, detailed description of contrastive, dice, and alignment losses (and their weighting) makes reproduction feasible and testable."}, "weaknesses": {"value": "- While MSA adapters and MTA are described and visualized in Figure 2, the specific methodology for choosing insertion layers for adapters in different backbones is only loosely justified. There is a missed opportunity for a principled, possibly automated or analytical policy for placement, and no ablation on layer choice is provided.\n- Although Table 3 (a) explores performance trade-offs for window size, the choice of optimal $4 \\times 4$ is only empirically justified. There is little theoretical or dataset-specific reasoning for why this size generalizes, and exploring task- or scale-adaptive policies would strengthen claims of robustness.\n- There are several grammatical errors and awkward phrasings, as well as the use of slightly non-standard abbreviations in the tables (e.g., \"vol\", \"m/s/6\", \"m/sfI\" in Table 1), which may disrupt readability and hinder quick assimilation for a broad audience."}, "questions": {"value": "- Can the authors provide a rationale for the placement of PET adapters (MSA, MTA) at specific depths in the vision/text backbone? Have they considered or tested more adaptive/learned strategies for insertion, and can they provide ablations or guidelines for optimal selection?\n\n- How is the concatenation between text class tokens and visual patch windows actually handled in practice (e.g., with respect to normalization, possible channel mismatch, and possible overfitting due to repetitive text tokens)? Would normalization before SSM scans improve performance or stability?\n\n- Have the authors empirically measured the actual decay rate of long-range dependencies for varying window sizes in SSM, and if so, can those be reported? Is the optimal window size truly dataset/task dependent?\n\n- Are there notable scenarios where the windowed approach harms segmentation accuracy, e.g., in very small or oddly-shaped object instances, or when referring expressions are ambiguous or highly context-dependent?\n\n- Will the complete code (including all adapter implementations and ablation regimes) be released for reproducibility, and if so, under what license and conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "F6qig8fkUO", "forum": "WnRzN4U8Y8", "replyto": "WnRzN4U8Y8", "signatures": ["ICLR.cc/2026/Conference/Submission23757/Reviewer_WGQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23757/Reviewer_WGQk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806929443, "cdate": 1761806929443, "tmdate": 1762942792641, "mdate": 1762942792641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a parameter-efficient framework that integrates a window-based intermediate fusion neck (HMF) and lightweight adapters (MTA, MSA, and emphasis parameters) to enhance vision–language alignment for referring image segmentation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a Hierarchical Mamba Fusion (HMF) block, which performs intermediate vision–language fusion by aggregating multi-scale features and applying a window-based Mamba module (WMF).\n- A parameter-efficient tuning (PET) strategy is presented, consisting of a Mamba Text Adapter (MTA) for modeling textual priors, a Multi-Scale Aligner (MSA) with RFMixer and cross-attention for visual–text alignment, and learnable emphasis parameters for adaptive layer weighting.\n- The overall framework, WIMFRIS, integrates these components and is experimentally compared against existing PET-based and full fine-tuning methods on multiple RIS benchmarks."}, "weaknesses": {"value": "* Lack of Novelty\n\nThe paper shows limited novelty. The **PET part** closely follows DETRIS, essentially extending its parameter-efficient tuning framework with minor Mamba-based modifications. The **neck design** heavily overlaps with the fusion architecture in fixation phase in SaFiRe, both adopting window-based Mamba fusion for intermediate vision-language alignment. Overall, the work mainly integrates these existing ideas rather than introducing a substantively new contribution.\n\n\n* Incomplete Manuscript\n\nThe paper appears **incomplete**. Section 3.2 is unfinished, and the crucial description of the **task decoder** is missing. This omission disrupts the continuity between Sections 2.3 and 2.4. The authors should carefully verify whether the submitted version is the complete manuscript.\n\n\n* Unfair and Limited Comparison\n\nFor Table 1\n\n\n1. **Unfair Comparison :**\nTo ensure fairness, (1) the parameters of PET-based methods should be adjusted to achieve **comparable model sizes**, and (2) the **backbones of all compared methods** should be unified.\n\n2. **Limited Comparison with State-of-the-Arts:**\nMore PET-based approaches should be included, as previous works (e.g., ETRIS, DETRIS, RISCLIP) have done, especially those involving **backbone-side modality fusion** in RIS, such as **PWAM in LAVT**, **SDF in VLT**, and **CFE in RISCLIP**, as well as classical parameter-efficient tuning methods like **LoRA** and **Adapter**.\n\n3. **Marginal Improvement of the WMF Neck:**\nCompared with **DETRIS**, the improvements achieved by the proposed **WMF Neck** are quite marginal.\n\n4. **Insufficient Comparison :**\nA more comprehensive comparison is needed to substantiate the claimed advantages of the proposed neck method, including detailed analyses of **parameter counts**, **computational cost (GFLOPs)**, and **inference speed**, particularly in comparisons with **ETRIS/DETRIS necks**.\n\nFor Table 2\n\n1. **Inconsistent Metrics:**\n   Table 2 mixes **mIoU** and **oIoU** without clarification. While RISCLIP, DETRIS, and WIMFRIS use **mIoU**, most other methods report **oIoU**. In particular, for works like **CGFormer** and **Polyformer**, which provide both metrics, the authors still report their **oIoU** values. Since **mIoU** is generally higher than **oIoU** on the RefCOCO family datasets, this inconsistency makes the performance comparison **unreliable**.\n2. **RISCLIP Issue:**\n   According to the authors’ own definition (line 44, “…keeping the vast majority of the backbone parameters frozen”), RISCLIP also freezes its CLIP backbone and should be considered a parameter-efficient tuning method. Moreover, the results of **RISCLIP-L** are missing, which appear **significantly higher** than those of the proposed “Ours-L” model (trained on RefCOCO+, mIoU: **RISCLIP-L** 74.38 / 78.77 / 66.84 vs. **Ours-L** 71.9 / 76.2 / 67.2).\n\n\n*  Efficiency Analysis\n\nAlthough this work emphasizes the **PET framework** and uses the **efficient Mamba architecture**, more detailed **efficiency analyses** should be provided—specifically **GFLOPs**, **inference speed**, and preferably **FPS**.\n\n\n* Minor Issues\n\nIn **Table 3(a)**, the content does not match the caption: *4×4* is **not** the smallest window size.\n\n\n\n***I would be happy to revise my score if the author addresses these points.***\n\n\n\n---\n\n**References:**\n\nDETRIS: Densely Connected Parameter-Efficient Tuning for Referring Image Segmentation AAAI2025\n\nSaFiRe: SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation NeurIPS 2025\n\nLAVT: Language-Aware Vision Transformer for Referring Image Segmentation CVPR2022\n\nVLT: Vision-Language Transformer and Query Generation for Referring Segmentation TPAMI2023\n\nRISCLIP:Extending CLIP’s Image-Text Alignment to Referring Image Segmentation NAACL2024\n\nLoRA: Low-Rank Adaptation of Large Language Models. ICLR2022\n\nParameter-Efficient Transfer Learning for NLP. ICML2019\n\nCGFormer: Contrastive Grouping with Transformer for Referring Image Segmentation CVPR2023\n\nPolyFormer: Referring Image Segmentation as Sequential Polygon Generation CVPR2023"}, "questions": {"value": "*  Could you clarify the **task decoder design**?\n\n*  In Table 1, which IoU metric is used—**mIoU** or **oIoU**? ETRIS reports oIoU from the original paper, but DETRIS uses mIoU.\n\n* In Table 2, please clarify metric issue and the RISCLIP issue mentioned in W1-B.\n\n* What are the **inference speed** and **GFLOPs** of the proposed model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l3NeqmvthW", "forum": "WnRzN4U8Y8", "replyto": "WnRzN4U8Y8", "signatures": ["ICLR.cc/2026/Conference/Submission23757/Reviewer_N61Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23757/Reviewer_N61Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909064939, "cdate": 1761909064939, "tmdate": 1762942792426, "mdate": 1762942792426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}