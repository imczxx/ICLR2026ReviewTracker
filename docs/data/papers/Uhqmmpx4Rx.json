{"id": "Uhqmmpx4Rx", "number": 10252, "cdate": 1758165124483, "mdate": 1759897662962, "content": {"title": "ORFLEX: Orthogonal Reparameterization with Flexibility for Multimodal Large Language Model Fine-Tuning", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key strategy for adapting pretrained large models with minimal trainable parameters. While most methods were developed for LLMs and later extended to multimodal domains, their direct application to multimodal large language models (MLLMs) often overlooks modality-specific discrepancies. In particular, although visual tokens are aligned with language tokens in feature space, differences persist during forward propagation, which existing LoRA-based approaches fail to address.In this work, we propose ORFLEX, a reparameterized PEFT method tailored for MLLMs. First, we observe that the LoRA column spaces associated with visual and text tokens tend to be strongly orthogonal when the parameters are decoupled. Further, we leverage this property by introducing modality-specific reparameterization branches and designing a QR-inspired decomposition of the LoRA matrix into a frozen orthogonal basis $\\hat{Q}$ and a lightweight learnable matrix $\\hat{R}$. In addition, we incorporate learnable Householder transformations to adaptively rotate $\\hat{Q}$ while preserving orthogonality, enhancing expressiveness.Extensive experiments demonstrate that our approach consistently outperforms strong baselines on both general and domain-specific multimodal benchmarks, underscoring the effectiveness of modality-aware reparameterization to advance PEFT for MLLMs.", "tldr": "We propose a PEFT method for MLLMs that enforces orthogonality while retaining flexibility in different modality matrix subspaces, achieving state-of-the-art performance across multimodal tasks.", "keywords": ["MLLM", "PEFT", "low rank adaptation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8e0e80935a70526f39b598ed460a1b31355dfdd.pdf", "supplementary_material": "/attachment/99a5b7cae40b3ecc80038b97eba25cbe7445584f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ORFLEX, aorthogonal reparameterization method for PEFT  of MLLMs.  \nThe authors observe that LoRA subspaces corresponding to visual and textual modalities are strongly orthogonal when decoupled.  \nBuilding on this insight, they introduce modality-specific reparameterization branches with a QR-inspired decomposition, splitting LoRA matrices into a frozen orthogonal basis and a lightweight learnable matrix.  \nThey further enhance flexibility by incorporating learnable Householder transformations* which adaptively rotate the orthogonal subspace without violating orthogonality.  \nExperiments on both general and domain-specific multimodal benchmarks (e.g., A-OKVQA, ScienceQA, PathVQA) show consistent improvements over strong PEFT baselines such as LoRA, AdaLoRA, and VB-LoRA, while maintaining comparable or lower trainable parameter counts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The work introduces an orthogonality-based view of modality decoupling in PEFT, combining QR decomposition and Householder transformations in a novel way.\n\n2. Theoretical justification for orthogonal subspace alignment is solid, and the empirical validation across datasets is comprehensive.\n\n3. The authors did comprehensive analysis, including ablation, rank-scaling, and hyperparameter studies that clearly support design choices."}, "weaknesses": {"value": "1. Experiments are restricted to mid-sized MLLMs (LLaVA-1.6-Mixtral-7B); results on larger models (e.g., Qwen3-VL) would strengthen generalizability. It is hard to know the generalization ability of the method across different backbone models.\n\n2. The paper notes that enforcing strong orthogonality might limit expressiveness for tasks requiring deep modality fusion; this tradeoff is not fully explored.\n\n3. Although authors claims they use fewer trainable parameter,  but training time and GPU memory usage comparisons to LoRA and AdaLoRA aren't reported. It would help quantify the claimed efficiency benefits."}, "questions": {"value": "1. How sensitive is ORFLEX to the number of Householder transformations? Could too many lead to instability or overfitting?\n\n2. Can ORFLEX generalize to non-vision modalities (e.g., audio-text or video-text), or is its design specific to vision-language alignment?\n\n3. Could the authors analyze computational overhead more directly (e.g., FLOPs or throughput) relative to LoRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3titNw5LqA", "forum": "Uhqmmpx4Rx", "replyto": "Uhqmmpx4Rx", "signatures": ["ICLR.cc/2026/Conference/Submission10252/Reviewer_mUpE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10252/Reviewer_mUpE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404044826, "cdate": 1761404044826, "tmdate": 1762921609801, "mdate": 1762921609801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a modality-decoupled orthogonal reparameterization of LoRA that preserves inter-modality column-space orthogonality via frozen orthonormal bases, learnable coefficients, and Householder rotations, enabling robust MLLM fine-tuning at near-LoRA cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well-organized, with fluent language that make the presentation easy to follow."}, "weaknesses": {"value": "1. The logic of the paper’s motivation is unclear. It states that “despite aligning visual tokens with language token feature spaces, discrepancies between modalities persist during the forward pass, which current LoRA-based methods overlook.” However, this issue also seems to exist under full fine-tuning for MLLMs and therefore cannot be said to arise specifically from using LoRA.\n\n2. The paper introduces a series of components that appear to have high time complexity. Please compare training wall-clock time and GPU memory usage against baselines such as LoRA.\n\n3. In Tables 2 and 3, the improvements over LoRA are modest. Please explain why other baselines underperform LoRA. Could the authors also compare with stronger baselines relative to LoRA, such as DoRA [1] and PiSSA [2]?\n\n4. It is unclear whether the orthogonality operation in the method makes sense; judging from the ablation study, the gains also appear limited. Could the authors provide further insights into enforcing orthogonality for LoRA across different modalities?\n\n[1] DoRA: Weight-Decomposed Low-Rank Adaptation\n\n[2] PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"}, "questions": {"value": "The SD in Table 2 seems large. Is this because the dataset subsets differ substantially?\n\nSince the proposed method’s gains appear limited, there is no significant parameter efficiency, and it can only be applied to multimodal fine-tuning, comparisons with stronger baselines (DoRA, PiSSA, etc.) are necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GjnWPRsaMj", "forum": "Uhqmmpx4Rx", "replyto": "Uhqmmpx4Rx", "signatures": ["ICLR.cc/2026/Conference/Submission10252/Reviewer_ALFh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10252/Reviewer_ALFh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489742772, "cdate": 1761489742772, "tmdate": 1762921609356, "mdate": 1762921609356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new PEFT framework designed specifically for MLLMs. The authors identify that existing LoRA-based PEFT approaches fail to handle the modality discrepancies between visual and textual tokens, whose embeddings and attention patterns remain partially disjoint during forward propagation. To address this, ORFLEX introduces modality-specific reparameterization branches and decomposes each LoRA matrix via a QR-inspired factorization into a frozen orthogonal basis and a lightweight learnable matrix. To further enhance adaptability, learnable Householder transformations are incorporated to rotate the orthogonal basis​ while preserving orthogonality, balancing efficiency and flexibility. Empirically, ORFLEX consistently outperforms state-of-the-art PEFT baselines such as LoRA, AdaLoRA, and VB-LoRA on multiple multimodal benchmarks (A-OKVQA, TextVQA, ScienceQA, PathVQA, Slake, etc.) while using only about 0.7% of the trainable parameters of full fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The methodology is well grounded in matrix theory, supported by rigorous analysis of subspace orthogonality using SVD, and validated by comprehensive ablation studies. The paper demonstrates both theoretical soundness and empirical rigor, with consistent performance improvements across diverse benchmarks. The orthogonal–flexible balance is thoroughly examined through quantitative experiments and controlled analyses, confirming the claimed benefits.\n\nExtensive experiments on both general-purpose (A-OKVQA, TextVQA, ScienceQA) and domain-specific (PathVQA, Slake, VisOnlyQA) benchmarks provide strong evidence of the method’s robustness and scalability. The parameter efficiency (only 0.7% trainable parameters) with superior or comparable accuracy to full fine-tuning demonstrates high practical value."}, "weaknesses": {"value": "1. Unclear interpretation of orthogonal embeddings: The paper claims that visual and textual embeddings exhibit strong orthogonality, yet this contradicts the common assumption that well-trained multimodal representations should be semantically aligned. If the embeddings are nearly orthogonal, the cosine similarity between corresponding visual and textual features would approach zero, undermining cross-modal semantic consistency. The authors should clarify this apparent paradox—whether “orthogonality” refers to parameter subspace independence rather than embedding-level semantic dissimilarity—and provide empirical evidence or visualization to support this interpretation.\n\n2. Limited theoretical depth in orthogonality justification: While the paper empirically verifies that modality-specific LoRA subspaces exhibit orthogonality, the theoretical reasoning for why such orthogonality leads to improved generalization or reduced covariance is relatively shallow. A more formal derivation or theoretical framework (e.g., based on linear subspace interaction or feature disentanglement) would strengthen the conceptual foundation.\n\n2. Lack of downstream interpretability or visualization: Although the paper discusses orthogonality in subspaces, it does not provide visual or interpretive evidence (e.g., token projection visualizations, correlation matrices) showing how the reparameterized subspaces improve modality separation or feature alignment. Adding such qualitative analysis would enhance interpretability and strengthen the intuitive understanding of the method’s effect.\n\n3. Additional ablation needed for fairness verification: The paper should include an experiment where image and text branches use different low-rank matrices trained independently (without the orthogonal constraint) to directly compare against ORFLEX. This would reveal whether the observed gains truly come from orthogonal reparameterization or simply from modality-specific parameterization.\n\n4. Experiments on different modality pairs: The current evaluation focuses only on image-text multimodality. Given that the motivation emphasizes modality-specific discrepancies, extending experiments to text-audio tasks would better validate the method’s universality and its potential impact on broader multimodal fine-tuning.\n\n5. Evaluation on different scale models: Although the authors acknowledge hardware constraints, all experiments are conducted on LLaVA-1.6-Mixtral-7B. It remains unclear whether the observed gains persist for larger backbones (e.g., 13B, 34B) or for architectures with stronger multimodal alignment such as Qwen-VL series or InternVL series. Evaluating scalability to larger models would substantiate the method’s general applicability."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2BM6WMPX3g", "forum": "Uhqmmpx4Rx", "replyto": "Uhqmmpx4Rx", "signatures": ["ICLR.cc/2026/Conference/Submission10252/Reviewer_rEFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10252/Reviewer_rEFZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637705352, "cdate": 1761637705352, "tmdate": 1762921608707, "mdate": 1762921608707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work observes that visual and text tokens in MLLMs, though projected into the same feature space, still exhibit significant distribution differences during forward propagation. The authors find that LoRA updates for the two modalities tend to become orthogonal when decoupled. Motivated by this, they propose ORFLEX, a modality-aware PEFT framework that introduces separate reparameterization branches for visual and text tokens and decomposes LoRA matrices into a frozen orthogonal basis and a learnable component."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1,Modality-aware parameter design: Identifies and leverages intrinsic differences between visual/text token spaces, addressing a real gap in previous LoRA methods.\n\n2,Clear theoretical grounding: Uses linear algebra tools (QR decomposition, Householder transforms) to enforce orthogonality with mathematical rigor.\n\n3,Strong empirical results: Demonstrates consistent gains across general and domain-specific MLLM benchmarks."}, "weaknesses": {"value": "1, Extra complexity / parameters: Dual branches and orthogonality maintenance introduce additional overhead compared to standard LoRA.\n\n2, Orthogonality assumption universality: Strong orthogonality observation may depend on specific architectures or datasets; unclear if it holds broadly.\n\n3, Limited modality scaling discussion: Method focuses on two modalities; generalization to audio/video/3D or >2 modalities is not explored."}, "questions": {"value": "1, Layer-wise behavior:\n\nDoes visual-text token orthogonality vary across model depth? Would applying decoupling only at selective layers achieve similar performance with lower cost?\n\n2, Scalability beyond two modalities:\n\nHow would this method generalize if more modalities (e.g., audio, depth, video tokens) are introduced, where pairwise orthogonality may not hold cleanly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uL7pCrnupL", "forum": "Uhqmmpx4Rx", "replyto": "Uhqmmpx4Rx", "signatures": ["ICLR.cc/2026/Conference/Submission10252/Reviewer_Ngdi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10252/Reviewer_Ngdi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927443623, "cdate": 1761927443623, "tmdate": 1762921608148, "mdate": 1762921608148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}