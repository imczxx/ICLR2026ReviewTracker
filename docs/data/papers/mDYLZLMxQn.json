{"id": "mDYLZLMxQn", "number": 1466, "cdate": 1756885031330, "mdate": 1759898207787, "content": {"title": "OBJVANISH: PHYSICALLY REALIZABLE TEXT-TO-3D ADV. GENERATION OF LIDAR-INVISIBLE OBJECTS", "abstract": "LiDAR-based 3D object detectors are fundamental to autonomous driving, where failing to detect objects poses severe safety risks.  Developing effective 3D adversarial attacks is essential for thoroughly testing these detection systems and exposing their vulnerabilities before real-world deployment. However, existing adversarial attacks that add optimized perturbations to 3D points have two critical limitations: they rarely cause complete object disappearance and prove difficult to implement in physical environments. We introduce the text-to-3D adversarial generation method, a novel approach enabling physically realizable attacks that can generate 3D models of objects truly invisible to LiDAR detectors and be easily realized in the real world. Specifically, we present the first empirical study that systematically investigates the factors influencing detection vulnerability by manipulating the topology, connectivity, and intensity of individual pedestrian 3D models and combining pedestrians with multiple objects within the CARLA simulation environment. Building on the insights, we propose the physically-informed text-to-3D adversarial generation (Phy3DAdvGen) that systematically optimizes text prompts by iteratively refining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To ensure physical realizability, we construct a comprehensive object pool containing 13 3D models of real objects and constrain Phy3DAdvGen to generate 3D objects based on combinations of objects in this set. Extensive experiments demonstrate that our approach can generate 3D pedestrians that evade six state-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and physical environments, thereby highlighting vulnerabilities in safety-critical applications.", "tldr": "This paper introduces a text-to-3D adversarial generation framework that creates LiDAR-invisible objects, exposing vulnerabilities in 3D detectors.", "keywords": ["3D generation; adversarial attack; lidar object detection;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bb64bbf4713b668f1578abc26934acf0ca82dc2.pdf", "supplementary_material": "/attachment/9363d7f25067c596076a26cd188043f67f7a55b6.zip"}, "replies": [{"content": {"summary": {"value": "Phy3DAdvGen is a novel text-to-3D adversarial generation framework that—motivated by an empirical study showing object combinations most strongly undermine LiDAR detection—optimizes discrete verb–object–pose prompts (constrained to a pool of 13 real-world objects) to synthesize physically realizable human–object compositions that can make pedestrians “vanish” from six state-of-the-art LiDAR detectors, achieving up to 94.6% attack success in CARLA simulation and validated physical tests."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The first method leverages text prompts to generate physically realizable adversarial objects targeting LiDAR detectors.\n\n2. Experimental results demonstrate strong cross-detector transfer and successful real-world deployment.\n\n3. The authors evaluate their approach in both simulation and real-world environments."}, "weaknesses": {"value": "1. Heavy reliance on specific 3D generator (e.g., LGM) and prompt set.\n\n2. The proposed method is evaluated only on LiDAR-based detectors; the paper would be strengthened by also testing multimodal perception systems (e.g., LiDAR–camera fusion) to assess transferability and robustness.\n\n3. Evaluations restricted to pedestrian scenarios; unclear generality."}, "questions": {"value": "1. How transferable are the generated adversarial human–object compositions — for example, do objects optimized against one detector (or one text-to-3D backbone) remain evasive across different LiDAR detectors, detectors trained from other datasets and etc?\n\n2. How were the 13 objects selected?\n\n\n3. Defense Implications: What are the most promising defenses against this type of semantically-plausible attack? Is data augmentation with generated scenes a sufficient countermeasure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wMszUM5Iov", "forum": "mDYLZLMxQn", "replyto": "mDYLZLMxQn", "signatures": ["ICLR.cc/2026/Conference/Submission1466/Reviewer_egro"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1466/Reviewer_egro"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813867719, "cdate": 1761813867719, "tmdate": 1762915775928, "mdate": 1762915775928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Phy3DAdvGen—a novel text-to-3D adversarial generation-based framework for adversarial attacks. Existing 3D adversarial attacks on them have limitations. The new text-to-3D adversarial generation method enables physically realizable attacks making objects invisible to LiDAR detectors. Experiments show it can evade six SOTA LiDAR 3D detectors in simulations and real-world scenarios, uncovering safety-critical application vulnerabilities.\n\n\nThis paper fails to address a core issue: it relies excessively on CARLA simulation experiments, with no dedicated discussion on the simulation-reality gap. While the newly added real-world experiments partially demonstrate generalization, most conclusions still draw from simulations—undermining the credibility of the experimental findings. It is therefore recommended to supplement in-depth analysis of the simulation-reality gap and add more real experimental evidence to support the core conclusions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper offers a thorough review of related work and presents a 3D generation-based algorithm for adversarial attacks. The authors carry out comparative experiments under real-world scenarios and incorporate extensive ablation studies. Overall, the paper features a clear structure and high readability."}, "weaknesses": {"value": "Weaknesses\n1.\tCompared with Reference [1] (which uses a generative approach, realistic point cloud simulator, and targets multi-modal perception systems), this paper’s method of placing 3D-generated objects in CARLA and focusing on pure point cloud systems (rarely used in current autonomous driving) fails to clarify its technical advantages.\n2.\tThe approach of concatenating rendered point clouds with original data oversimplifies critical occlusion effects in real-world scenarios; coupled with CARLA’s limitations in replicating real conditions, this further reduces the method’s practical relevance, and the authors neither justify this approach nor explore more realistic fusion techniques.\n3.\tThe proposed algorithm is referred to as \"Phy3DAdvGen\" in the main text but \"OBJVanish\" in the title, creating nomenclature inconsistency that may confuse readers.\n\n[1] Cao, Yulong, et al. \"Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks.\" 2021 IEEE symposium on security and privacy (SP). IEEE, 2021."}, "questions": {"value": "Please refer to the questions in the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "saCVPoamh6", "forum": "mDYLZLMxQn", "replyto": "mDYLZLMxQn", "signatures": ["ICLR.cc/2026/Conference/Submission1466/Reviewer_vcnG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1466/Reviewer_vcnG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965990196, "cdate": 1761965990196, "tmdate": 1762915775761, "mdate": 1762915775761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LiDAR 3D detectors are essential for autonomous driving, and missed detections pose severe safety risks. Existing adversarial point attacks seldom remove objects or translate to the physical world. This paper presents Phy3DAdvGen, a text-to-3D adversarial method that generates physically realizable pedestrian models invisible to LiDAR. It optimizes text prompts and restricts outputs to a pool of thirteen real objects for physical feasibility. In CARLA, the authors manipulate topology, connectivity, intensity, and object combinations to study vulnerabilities. Phy3DAdvGen iteratively refines verbs, objects, and poses to produce LiDAR-invisible pedestrians. Experiments show these models evade six state-of-the-art LiDAR detectors in simulation and physical tests."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) This idea is straightforward and can be easily understood.\n\n(2) The experiments consider multiple LiDAR 3D detectors, with seemingly more rigorous evaluation methodologies than many previous papers in this area."}, "weaknesses": {"value": "(1) My major concern about this paper is the novelty issues. The authors claim that their main contribution lies in the ability to use prompts to generate 3D adversarial examples. However, the entire text-to-3D adversarial generation method is largely built upon existing work, particularly Gaussian Splatting (e.g., LGM, Tang et al., 2024) for generating malicious examples. Therefore, the novelty and contribution of this work are quite limited, as it primarily extends existing methods without introducing substantial new techniques or insights.\n\n(2) The threat model is unclear. For example, is this a black-box or white-box attack? How much knowledge does the attacker have about the victim models (i.e., LiDAR detectors) and the associated datasets? If this is a white-box attack, how could it realistically be deployed in a real-world scenario? I recommend that the authors include a dedicated subsection titled “Threat Model” to clearly define their assumptions regarding the attacker’s knowledge, capabilities, and goals. This addition would help readers better understand the scope and practicality of the proposed method.\n\n(3) The experiments are not sufficient. For example, the paper only briefly discusses transferability in Table 4, where the evaluation is conducted on six LiDAR detectors on average. More thorough experiments are needed to analyze how adversarial examples generated on one model can be transferred and applied to other models. A detailed discussion of cross-model and cross-dataset transferability would significantly strengthen the experimental section and the overall credibility of the proposed approach.\n\n(4) Finally, the writing quality requires significant improvement. In several critical parts of the paper, it is difficult to understand what the authors actually did in terms of experimentation and analysis, as well as what motivated their design and methodological choices."}, "questions": {"value": "Please refer to my comments for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "O318NLefx4", "forum": "mDYLZLMxQn", "replyto": "mDYLZLMxQn", "signatures": ["ICLR.cc/2026/Conference/Submission1466/Reviewer_mVL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1466/Reviewer_mVL3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129667206, "cdate": 1762129667206, "tmdate": 1762915775594, "mdate": 1762915775594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}