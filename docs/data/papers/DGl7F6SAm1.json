{"id": "DGl7F6SAm1", "number": 667, "cdate": 1756763937668, "mdate": 1759898248200, "content": {"title": "Object-Oriented Transition Modeling with Inductive Logic Programming", "abstract": "Building models of the world from observation, i.e., *induction*, is one of the major challenges in machine learning. In order to be useful, models need to maintain accuracy when used in novel situations, i.e., generalize. In addition, they should be easy to interpret and efficient to train. Prior work has investigated these concepts in the context of *object-oriented representations* inspired by human cognition. In this paper, we develop a novel learning algorithm that is substantially more powerful than these previous methods. Our thorough experiments, including ablation tests and comparison with neural baselines, demonstrate a significant improvement over the state-of-the-art. The source code for all of our algorithms and benchmarks will be available online after publication.", "tldr": "We describe a novel algorithm for learning transition and reward models in the framework of object-oriented reinforcement learning.", "keywords": ["reinforcement learning", "transition modeling", "inductive logic program", "decision trees"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7f67cafde5a9c1776b127b6c1fd2069b7fe219e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper essentially trains a \"reinforcement learning\" agent (although the authors never use this word). Unlike other approaches (e.g., deep reinforcement learning), the agent employs first-order logical decision trees. A particular contribuiton is the facilitation of online learning/updating of the tree."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Very interesting and original idea to use first-order logical decision trees as a reinforcement learning agent\n- The learned trees are interpretable\n- The approach outperforms the previous approach (Stella & Loguinov 2024)"}, "weaknesses": {"value": "- The learning framework / task is not formally defined. Section 2 directly starts with an algorithm before clearly defining the problem setting.\n- The paper seems closely related to reinforcement learning. However, the authors do not use the term reinforcement learning and do not discuss how the paper is similar/dissimilar. This makes the paper rather confusing.\n- The approach is applied to toy problems like a robot traversing a maze. The practical/scientific relevance does not become fully clear.\n- The TreeLearning algorithm makes sense. However, it does not seem to be particularly challenging. Overall, without a clear problem definition and without a clear algorithmic contribution, I cannot see a significant contribution of this paper."}, "questions": {"value": "- How would you formally define the learning task?\n- How is your approach related to reinforcement learning?\n- How big are the generated trees (in number of nodes)? Are they easily interpretable by a human?\n- Why do you not use a classical reinforcement learning approach as a baseline? (e.g., in Figure 3)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lbFcMWlE7v", "forum": "DGl7F6SAm1", "replyto": "DGl7F6SAm1", "signatures": ["ICLR.cc/2026/Conference/Submission667/Reviewer_vpUZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission667/Reviewer_vpUZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760984182881, "cdate": 1760984182881, "tmdate": 1762915577602, "mdate": 1762915577602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TreeLearn/TreeThink, an inductive-logic-programming (ILP) approach to object-oriented transition learning. States are sets of typed objects with attributes; for each action, the method learns a collection of independent per-attribute rules represented as first-order logical decision trees. Prediction iterates over all objects/attributes and applies the corresponding rule; leaves return deltas that are added to current values. The reward is folded into the state via a special game[score] object that tracks the cumulative sum of rewards, so no separate reward model is learned. Experiments across grid-world-style domains compare TreeThink to QORA and neural baselines, claiming faster convergence, stability, and transfer to larger layouts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interpretable model class learning (FOLDT)\n- Object centric nature"}, "weaknesses": {"value": "- Method is described too abstractly in the main text; intuition and operational details are hard to extract\n- Strong modeling assumption: attributes are predicted independently; the paper does not justify, analyze, or ablate this restriction.\n- Reward modeling: including cumulative reward as part of state breaks the standard MDP Markov assumption unless extra conditions hold; this is neither acknowledged nor analyzed.\n- related work is thin given a long history in action-model learning, model-based RL, and relational/OO RL; the “object-oriented vs. relational” distinction is asserted but not clarified, and in practice they overlap."}, "questions": {"value": "1. Lack of intuition and clarity\nThe paper’s exposition focuses on what TreeThink does rather than why or how it works.  \nSections describing the algorithm list steps such as candidate tests, confidence intervals, and recursive updates, but provide little explanation or intuition.  \nThere is no worked example that illustrates the algorithm’s reasoning on a small domain, so it is difficult to build an understanding of how rules evolve or how the algorithm achieves its claimed stability.  \n\n\n2. Independence assumption across attributes\nThe model decomposes transition prediction into independent rules for each object attribute and class for each action.  \nThis implies that attributes vary independently given the action, which is an extremely restrictive assumption for structured environments where attributes are correlated.  \nNo justification, ablation, or discussion is provided for this modeling choice.  \nThe paper should either analyze the implications of this assumption or relax it to model dependencies among attributes.\n\n3. Folding cumulative reward into the state\nThe paper introduces a special \"game\" object with an attribute \"score\" that accumulates the total reward over time.  \nThis means that the state includes historical information, which breaks the Markov property of standard MDPs.\nThe inclusion of cumulative reward is motivated as a way to avoid learning a separate reward model, but this introduces conceptual and practical problems.  \nFirst, it changes the formal definition of the problem and makes direct comparison with other methods unclear.  \nSecond, it conflates transition modeling and reward modeling, since the same structure is now predicting both environment dynamics and reward accumulation.  \nThe paper does not analyze the theoretical consequences of this choice or show that it is harmless in practice.\n\n4. Limited and incomplete related work discussion\nThe paper’s discussion of prior work is narrow.  \nIt cites QORA and classic inductive logic programming references but does not connect to extensive research on action model learning, probabilistic STRIPS rules, context-specific effects, and relational model-based reinforcement learning. The distinction between \"object-oriented\" and \"relational\" is also unclear.  \nThe proposed framework uses relations among objects and existential quantification, which are standard in relational representations, so the terminology difference is superficial.  \nThis omission weakens the paper’s claims of novelty and situates it poorly within the literature (see references below for a starting point)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IGfSyxZ3AY", "forum": "DGl7F6SAm1", "replyto": "DGl7F6SAm1", "signatures": ["ICLR.cc/2026/Conference/Submission667/Reviewer_YQ9K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission667/Reviewer_YQ9K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648385135, "cdate": 1761648385135, "tmdate": 1762915577476, "mdate": 1762915577476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an easily trainable model with interpretability by generating first-order logical decision trees. The performance is evaluated through ablation tests and compared against two baselines: QORA and a neural physics engine. The experimental results indicate that the proposed method, TreeLearn, achieves good performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper conducts several ablation experiments to validate the effectiveness of the proposed method."}, "weaknesses": {"value": "Although the paper states its effectiveness, I believe it lacks adequate discussion of the performance differences among the various baselines. Additionally, some experiments still lack key comparative results. Furthermore, the presentation of this paper still needs to be improved. For example, the paper does not include a discussion of the preliminaries or related work, which would be useful for contextualizing the state-of-the-art models within the inductive logic programming and reinforcement learning research communities."}, "questions": {"value": "1. When analyzing the performance of the neural network baselines, how does the proposed method perform in Figure 5?\n2. What is the motivation behind choosing these particular baselines? Are there other baseline models that could be compared?\n3. In line 159, what do \"deltas\" refer to?\n4. How are the semantics of nodes determined in the learned first-order decision trees?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JJArC3nVkv", "forum": "DGl7F6SAm1", "replyto": "DGl7F6SAm1", "signatures": ["ICLR.cc/2026/Conference/Submission667/Reviewer_5ypM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission667/Reviewer_5ypM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666989331, "cdate": 1761666989331, "tmdate": 1762915577316, "mdate": 1762915577316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm for learning first-order logical decision trees to model transition dynamics in an object-oriented MDP with rewards. The algorithm builds a tree by ~ enumerating logical tests and accepting them if they predict better than the existing tests, and creating new branches out of leaf nodes if this improves accuracy. The algorithm outperforms a prior baseline QORA on a set of dynamics-learning environments. The authors also point out some inference time optimizations you can make for evaluating the learned decision trees."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach performs much better than prior work. It can handle variable bindings, nested quantifiers, and more complex rules. \n- The algorithm design seems reasonable, although many details of the implementation are not described well."}, "weaknesses": {"value": "Two main weaknesses: impact/significance, and lack of clarity describing the main algorithm. \n\nW1: impact and significance \n- Object oriented transition modeling is a domain with limited impact. This is good old fashioned AI, with symbolic observations, learning, and models. It's unclear to me what the insights from 2025 are compared with previous decades of research into this area. \n- The evaluation is not very thorough: only one baseline is compared to. The environments are small gridworld environments. Without more evaluation, it's hard to know what the impact of this algorithm will be. Applications? To demonstrate impact, it seems like it could be relevant to compare to research in robotics, which often uses symbolic states and action spaces. However, as presented, it's hard to contextualize the research to the broader field. \n- Similarly, it is hard to evaluate the broader strengths of the proposed algorithm. The authors convincingly demonstrate that the algorithm is better than prior work QORA, but that is all.\n\nW2: clarify describing the main algorithm\n- the description of the algorithm in the main text is incomplete with regard to many details.\n- no pseudocode of the algorithm is provided. Instead, in the appendix, a dump of python implementation is shown, which is highly unusual for a machine learning paper. tedious to read through and obscures the core ideas of the algorithm behind many lines of code. To make the algorithm easier to understand, you should (1) provide pseudocode of the main learning algorithm, (2) defer code to a github link with documentation.\n- see questions area for further questions about the algorithm.\n- there is no analysis of the complexity of the algorithm\n\nIn order for me to accept this paper, I think I would need a convincing argument for the impact of this algorithm, and better contextualization with other symbolic MDP research."}, "questions": {"value": "Q1. How are confidence intervals calculated and used to swap out a rule? The exact mechanism here eluded me.\nQ2. When updating nodes, which tests are compared to the current test? I guess you do some enumeration over possible new tests, and evaluate them? Is this described in the main text, or just in the Python code in the appendix?\nQ3. On demand queries: \"instead of computing every true fact\" — is this saying that you're not evaluating all possible enumerable facts? Is there even a finite set of true facts?\nQ4. How possible tests and outcomes are enumerated is very unclear. Should be made clearer in the main text.\nQ5. How does this algorithm compare to QORA in technical details? Should be made clearer in the main text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nPkuY9kb8F", "forum": "DGl7F6SAm1", "replyto": "DGl7F6SAm1", "signatures": ["ICLR.cc/2026/Conference/Submission667/Reviewer_ae51"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission667/Reviewer_ae51"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095970816, "cdate": 1762095970816, "tmdate": 1762915577188, "mdate": 1762915577188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}