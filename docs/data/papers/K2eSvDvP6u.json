{"id": "K2eSvDvP6u", "number": 21181, "cdate": 1758314633093, "mdate": 1760805352939, "content": {"title": "EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences", "abstract": "As large language models (LLMs) are deployed globally, creating pluralistic systems that can accommodate the diverse preferences and values of users worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure LLMs' and reward models' (RMs) steerability towards users' value and stylistic preference profiles grounded in psychology and human-LLM literature. \nTo address the gap in existing datasets that do not support controlled evaluations of RM steering, we synthetically generated 165,888 preference pairs -- systematically varying pairs along 4 value dimensions (traditional, secular-rational, survival, and self-expression) and 4 style dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER to evaluate whether, given a user profile and a pair of candidate value-laden and style-laden responses, LLMs and RMs are able to select the output that aligns with the user's preferences. We evaluate six open-source and proprietary LLMs and RMs under sixteen systematic prompting conditions and six preference comparison scenarios. Notably, our results show that, when given the user's full profile of values and stylistic preferences, the best models achieve $<$75\\% accuracy at choosing the correct response, in contrast to $>$ 99\\% accuracy when only relevant style and value preferences are provided. EVALUESTEER thus highlights the limitations of current RMs at identifying and adapting to relevant user profile information, and provides a challenging testbed for developing RMs that can be steered towards diverse human values and preferences.", "tldr": "Introduces a benchmark to measure the steerability of LLM-as-a-judge and reward models to values and preferences.", "keywords": ["pluralistic alignment", "llm alignment", "benchmarking", "reward model steerability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/663e3a831e75f8d001b6d465e190de2d7590139f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a benchmark dataset to evaluate RM's steerability against user value and stylistic preferences. The authors evaluated a set of LLM judges based RMs and found that they manifested noticeable gaps against Oracle labels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper studies an important problem on LLM alignment against user value and stylistic preferences.\n\n* The construction of user profiles and values draw from real human data from World Values Survey.\n\n* A diverse range of reward models are evaluated (based on models of varied sizes/capacity)."}, "weaknesses": {"value": "* When it comes to user value and preference assessments, having human in the loop is critical and necessary. The dataset construction has no human involvement, and it's unclear to what extent the generated pairs align with what people would prefer in practice. The fact that the authors use GPT-4o filter as the \"oracle\" basically caps the performance of human preference alignment by 4o's inherent capabilities. Having human validation will make the datasets much more credible and convincing.\n\n* The main results presented by the paper (Figure 2) are confounded by the fact that 4o is the oracle. It is not surprising that models smaller than 4o didn't match the validation by 4o. This is less about human value alignment but more about aligning against the judgements made by 4o.\n\n* The evaluation was only done on prompting-based LLM judges. When it comes to RM, it's important to test whether the introduced dataset can be used to improve RM through finetuning."}, "questions": {"value": "Please see points in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PoqomF0uU4", "forum": "K2eSvDvP6u", "replyto": "K2eSvDvP6u", "signatures": ["ICLR.cc/2026/Conference/Submission21181/Reviewer_gLY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21181/Reviewer_gLY2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930672714, "cdate": 1761930672714, "tmdate": 1762941582436, "mdate": 1762941582436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EVALUESTEER, a synthetic benchmark probing whether LLM-based reward models can steer to user value and stylistic profiles. Using WVS-derived value statements and controlled style transforms, the authors evaluate six RMs across 11 prompting setups, finding style-over-substance biases and ≤75% accuracy with full context."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Steerability to pluralistic values and styles is underexplored for reward models; the paper targets an important gap beyond generic reward benchmarks.\n\n2. Orthogonal manipulation of four value dimensions and four stylistic families with six pairwise comparison regimes is well designed for isolating effects.\n\n3. Large evaluation (165,888 pairs), explicit prompting conditions (11), and an “oracle” filtration step make the pipeline easy to reason about; human validation adds rigor."}, "weaknesses": {"value": "1. Completions are generated with GPT-4o and filtered by a GPT-4.1 oracle, while GPT-4.1-Mini is among evaluated judges. This tight coupling risks self-agreement bias and overestimates generalization. Suggestions: Use a disjoint oracle family (e.g., non-OpenAI) or a committee oracle with disagreement filtering; report results when the example pool is filtered by each of {OpenAI, Google, Meta, Alibaba} or by majority vote across families.\n\n---\n\n2. Even though you include a 4-vs-4 ablation, the main condition gives ≈200 value statements vs 4 style statements. This still encourages attention to surface style cues.\nSuggestions: (1) Add a profile-summarization condition: top-k relevant value sentences via retrieval; (2) a matched-token-budget condition; (3) require RMs to cite which profile sentences justify their choice to check relevance sensitivity.\n\n\n---\n\n\n3. While the synthetic design isolates factors, it may not reflect messy real conversations where values are implicit and style/value signals co-occur with other attributes (domain knowledge, safety, politeness norms).\nSuggestions: Include a human-in-the-loop subset: have annotators with known WVS-like profiles express preferences over the same pairs; report agreement and calibration vs EVALUESTEER labels.\n\n\n---\n\n4. “Readability,” “verbosity,” and “confidence” can correlate (longer → harder; confident → fewer hedges → sometimes simpler). Current stylometrics are univariate proxies.\nSuggestions: Provide multivariate separability analyses (e.g., logistic regression predicting each style label using all stylometric features), and release per-sample metrics distributions showing minimal cross-loading."}, "questions": {"value": "See Weakness Part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KMINlHDmPz", "forum": "K2eSvDvP6u", "replyto": "K2eSvDvP6u", "signatures": ["ICLR.cc/2026/Conference/Submission21181/Reviewer_wP4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21181/Reviewer_wP4b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995708469, "cdate": 1761995708469, "tmdate": 1762941580852, "mdate": 1762941580852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EVALUESTEER, a benchmark designed to evaluate reward model (RM) steerability towards users’ value and style preferences. Built on the World Values Survey (WVS) and enriched with synthetic stylistic variations, EVALUESTEER tests how well reward models align model outputs with user profiles reflecting diverse human values and communication styles.\n\nThe synthesized benchmark comprises 165,888 preference pairs spanning four value dimensions (traditional, secular-rational, survival, and self-expression) and four stylistic dimensions (verbosity, readability, confidence, and warmth). Six RMs, including GPT-4.1-Mini and Skywork variants, are evaluated under 11 prompting conditions.\n\nKey findings include:\n  * Even with full context, top models achieve only ~75% accuracy, leaving a 25% gap to the oracle setting.\n  * RMs show secular and self-expression biases on the Inglehart–Welzel value map.\n  * Most RMs exhibit stylistic bias toward verbosity and confidence.\n  * When values and styles conflict, RMs favor style over values (\"style over substance\" bias)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, the paper is easy to follow, and the findings are clearly presented.\n\n2. EVALUESTEER fills a research gap by systematically measuring RM steerability to user-specific values and styles."}, "weaknesses": {"value": "The main weakness of the paper lies in its quality, especially the synthesized benchmark and the evaluated model sizes. To support the claims made in the paper, a manual check and more evaluations across different model sizes are needed.\n\n1. [Quality] The samples in the benchmark are synthesized by GPT-4o, which raises concerns about their quality. For example, it is possible that GPT-4o may produce similar responses for opposites of certain types of WVS statements. In that case, the small gap in different values would not be caused by the inherent abilities of the evaluated models but by the benchmark itself. I strongly recommend conducting a manual check of the proposed benchmark, and if that is too difficult, consider reducing the number of samples in the benchmark.\n\n2. [Quality] While validation metrics are reported, the paper lacks direct human benchmarking, e.g., RM performance vs. human annotators on identical tasks.\n\n3. [Significance, Quality] All tested models are small (-mini/-flash or <10B), which affects the generality of the conclusions when extending to modern large-sized models. For instance, the 25% gap in accuracy may simply be mitigated by increasing model size. To address this concern, experiments on larger models are needed, such as Gemini-2.5-Pro, GPT-4.1/5, or DeepSeek-R1 (671B).\n\n4. [Novelty] Several existing works [1][2] also evaluate LLM steerability. It is worth comparing, or at least discussing, the main differences between EVALUESTEER and these works.\n\n5. [Clarity, Quality]\n    * It is strongly recommended to include a figure illustrating the overall synthesis pipeline.\n    * The font size in Figure 3 should be enlarged, especially the text in the right subfigure.\n    * Table 2: \"pm\" -> $\\pm$\n    * Figure 9: Some numbers extend outside the box.\n    * Line 1112: \">=\" -> \"$\\ge$\"\n\n### References\n[1]: Chen, Kai, et al. \"STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models.\" EMNLP 2025.\n\n[2]: Chang, Trenton, et al. \"Measuring steerability in large language models.\" Neurips Safe Generative AI Workshop 2024. 2024."}, "questions": {"value": "* I am wondering whether this style bias can be mitigated as model size increases, similar to a \"scaling law\" in model steerability.\n* I am wondering which version of GPT-4o is used.\n* In Section D, a small portion of the synthesized data is manually validated. I am curious about the background of the annotators (the PhD and MS students), in particular:\n  - 1) Whether they are experts familiar with different inter-country values.\n  - 2) Whether they are authors of the paper and may have biases or conflicts of interest in the rating results."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "* The benchmark is synthesized under different intercountry values, including religious beliefs, whose quality needs to be manually ensured before official release or usage."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7GHOSe0pSg", "forum": "K2eSvDvP6u", "replyto": "K2eSvDvP6u", "signatures": ["ICLR.cc/2026/Conference/Submission21181/Reviewer_hFaN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21181/Reviewer_hFaN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762621852725, "cdate": 1762621852725, "tmdate": 1762941580072, "mdate": 1762941580072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}