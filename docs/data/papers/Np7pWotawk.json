{"id": "Np7pWotawk", "number": 21221, "cdate": 1758315078389, "mdate": 1759896933755, "content": {"title": "Function Spaces Without Kernels: Learning Compact Hilbert Space Representations", "abstract": "Function encoders are a recent technique that learn neural network basis functions to form compact, adaptive representations of Hilbert spaces of functions. We show that function encoders provide a principled connection to feature learning and kernel methods by defining a kernel through an inner product of the learned feature map. This kernel-theoretic perspective explains their ability to scale independently of dataset size while adapting to the intrinsic structure of data, and it enables kernel-style analysis of neural models. Building on this foundation, we develop two training algorithms that learn compact bases: a progressive training approach that constructively grows bases, and a train-then-prune approach that offers a computationally efficient alternative after training. Both approaches use principles from PCA to reveal the intrinsic dimension of the learned space. In parallel, we derive finite-sample generalization bounds using Rademacher complexity and PAC-Bayes techniques, providing inference time guarantees. We validate our approach on a polynomial benchmark with a known intrinsic dimension, and on nonlinear dynamical systems including a Van der Pol oscillator and a two-body orbital model, demonstrating that the same accuracy can be achieved with substantially fewer basis functions. This work suggests a path toward neural predictors with kernel-level guarantees, enabling adaptable models that are both efficient and principled at scale.", "tldr": "We show that function encoders implicitly define kernels, enabling finite-sample guarantees, and present algorithms that learn compact bases capturing intrinsic dimension, validated on polynomial and nonlinear dynamical systems.", "keywords": ["function encoders", "kernel methods", "representation learning", "feature learning", "generalization bounds"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f822d7fea38c42ddff08ca403c8debb395412a04.pdf", "supplementary_material": "/attachment/7f9fe80808d0fe556208d6ce4286b9f464b432ec.zip"}, "replies": [{"content": {"summary": {"value": "This paper connects function encoders (neural networks that learn a compact set of basis functions forming an explicit feature map)  \n$\\(\\phi(x) = [\\psi_1(x), \\dots, \\psi_n(x)]^\\top\\)$ to the theory of kernel methods. The inner product $\\(\\langle \\phi(x), \\phi(x') \\rangle\\)$ defines a valid kernel, giving a kernel-theoretic interpretation of learned neural representations. Two PCA-inspired methods (progressive training and train-then-prune) are introduced to identify compact bases, and generalization guarantees are derived using Rademacher complexity and PAC-Bayes analysis. Small-scale experiments on polynomial and dynamical system benchmarks are conducted to study the empirical properties of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written and reasonably well organized. The exposition is coherent, and the authors succeed in presenting familiar ideas from kernel methods, neural feature learning, and function approximation under a unified formalism."}, "weaknesses": {"value": "In my opinion, the paper adds no real novelty. Its main claims correspond almost exactly to well-established results in the literature, and the work largely repackages known ideas under new terminology. More specifically:\n\n- **On the theoretical contribution:** The statement that function encoders define a kernel through inner products of learned features is a direct restatement of the defining property of reproducing kernel Hilbert spaces, rigorously established since Aronszajn (1950) and detailed in Steinwart and Christmann (2008, Ch. 4). The associated primal–dual correspondence is classical convex duality and has been repeatedly exploited in random features, Nyström approximations, and kernel dictionary learning.\n\n- **On the algorithmic side:** The proposed PCA-based “progressive” and “train-then-prune” methods are simple applications of standard subspace and rank-selection heuristics. Similar procedures have long been employed in representation learning, linear probing of pre-trained networks, and dictionary learning methods, such as K-SVD or matching pursuit.\n\n- **On the theoretical analysis:** The Rademacher and PAC-Bayes bounds are restatements of known results for ridge regression with fixed features. The analysis treats the learned feature map as fixed, thereby sidestepping the genuine statistical challenge of analyzing learned representations.\n\nFinally, the empirical evaluation is limited to small, low-dimensional systems and does not substantiate claims of scalability or generality. I also find that this paper overlooks many classical results from the kernel and approximation literature, while relying heavily on recent “Ingebrand et al.” papers, which provide a narrow and incomplete view of prior work."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IyOYg1fO2N", "forum": "Np7pWotawk", "replyto": "Np7pWotawk", "signatures": ["ICLR.cc/2026/Conference/Submission21221/Reviewer_bCyB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21221/Reviewer_bCyB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761110237325, "cdate": 1761110237325, "tmdate": 1762941631418, "mdate": 1762941631418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors show a connection to feature learning and kernel methods and propose two practical algorithms to learn a compact basis, progressive training and train-than-prune. They also provide theoretical analysis regarding generalization bound based on the Rademacher complexity and PAC-Bayes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors investigate learning a compact basis for feature learning from the perspective of both practical algorithms and theory. They provide algorithms to obtain a compact basis and show how the number of basis functions depends on the generalization bound. The topic is interesting and relevant to the community."}, "weaknesses": {"value": "Although both the practical algorithms and the theoretical analysis are interesting, I have the following concerns and questions.\n- The connection between the proposed practical algorithms in Section 4 and the theoretical analysis in Section 5 is unclear. For example, the proposed algorithms try to obtain basis functions which span the intrinsic dimension of data by computing the cumulative explained variance CEV. However, the setting of the theoretical analysis is general and how the practical algorithms affect to the generalization bound is not clear. Showing the dependency of the bound on CEV would be interesting.\n\n- In Section 5, the authors derive two bounds (Theorems 1 and 2). The first one depends on $n$ linearly, but the second one depends on $n^{3/2}$. Does this mean the second one is not tight? Why this difference appears?"}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UhH3uZYxLw", "forum": "Np7pWotawk", "replyto": "Np7pWotawk", "signatures": ["ICLR.cc/2026/Conference/Submission21221/Reviewer_Yvv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21221/Reviewer_Yvv4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761299167122, "cdate": 1761299167122, "tmdate": 1762941631158, "mdate": 1762941631158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies function encoders for regression problems. These are neural network architectures with a nonstandard training strategy that splits the training of the last layer coefficients and basis functions separately. Due to the finite number of bases, there is a connection to a data-adapted finite-dimensional RKHS spaces. Two additional training variants are proposed, one based on greedy PCA and another on fine-tuning. The authors develop generalization gap bounds for fixed basis functions. Numerical results for polynomial approximation and dynamical systems learning support the ideas."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well written and clear. The greedy basis-by-basis residual training introduced in Sec. 4 is interesting, though I am not sure how original this is. See for instance the paper Cyr et al, which does something similar. I like the cleanness of the polynomial experiment, especially Fig 5 in the SM. The paper centers on the timely topic of connecting neural networks (less theory) with kernels (more theory), which is important for the field.\n\n\n[REF] Robust training and initialization of deep neural networks: An adaptive basis viewpoint. Cyr, Eric C and Gulian, Mamikon A and Patel, Ravi G and Perego, Mauro and Trask, Nathaniel A. Mathematical and Scientific Machine Learning. 2020, PMLR."}, "weaknesses": {"value": "The paper does not do a good job of distinguishing training functional encoders from the underlying architecture. The architecture is the set\n\\begin{align}\n\tF_n=\\\\{ f=\\sum\\_{j=1}^nc\\_j\\psi\\_j: c\\in\\mathbb{R}^n, \\psi\\_j\\in B \\\\}\n\\end{align}\nwhere $B$ is some set of functions. But this is equivalent to a space of neural networks with fixed width $ n $. What distinguishes a functional encoder from a neural network is how it is trained, but fundamentally it is just a neural network architecture.\n\nContribution 1 (connecting function encoders to feature learning and kernel methods) is not novel. It is well known that the finite-dimensional feature map $ \\phi $ defines a finite rank kernel. The literature on Nystrom and random feature approximations of kernel methods exploits this fact, see REFS 1 and 2.\n\nContribution 3 is also an easy application of known (loose) generalization bounds. Although the authors claim they extend kernel-style analysis to neural predictors, the neural part is frozen and fixed, so it is exactly a kernel analysis that does not take into account any of the neural structure. The actually interesting neural contribution to the error is the empirical risk $\\hat{L}\\_m(f\\_{\\hat{c}\\_\\lambda})$, which is left unquantified as a function of $ n,m,N,\\lambda $. Furthermore, Eqn 21 is well known in learning theory to be loose, e.g., if the true function is in the RKHS then $\\\\|\\hat{c}\\_\\lambda\\\\|=O(1)$ instead of $O(\\lambda^{-1/2})$.\n\nThus, the main contributions that I see are Contribution 2 (training strategies) and the numerical part.\n\nThe paper also downplays the potential of kernel methods. For example, lines 79--85 says ``[kernel or approximate kernel methods] cannot adapt to data.'' But this is not true in practice. If one goes into scikitlearn and trains a kernel regressor on a dataset, it will automatically perform maximum likelihood estimation to adapt the hyperparameters of the kernel to the dataset. This data adapted kernel is then used to perform regression. There is also analysis of such data-adapted kernels, see REFs 3 and 4.\n\n\n\n1. Generalization properties of learning with random features. Rudi, Alessandro and Rosasco, Lorenzo. Advances in neural information processing systems, 2017.\n\n2. Less is more: Nystrom computational regularization. Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo. Advances in neural information processing systems, 2015.\n\n3. Maximum likelihood estimation and uncertainty quantification for Gaussian process approximation of deterministic functions. Karvonen, Toni and Wynne, George and Tronarp, Filip and Oates, Chris and Sarkka, Simo. SIAM/ASA Journal on Uncertainty Quantification, 2020\n\n4. Convergence of Gaussian process regression with estimated hyper-parameters and applications in Bayesian inverse problems. Teckentrup, Aretha L. SIAM/ASA Journal on Uncertainty Quantification. 2020."}, "questions": {"value": "Questions:\n\n1. What is the role of the set of datasets $\\\\{D_j\\\\}_{j=1}^N$? Do you typically view $ N=1 $ or much larger? Similarly, does Algorithm 1 even make sense in the common setting $ N=1 $? Because in that case Eqn 7 is rank one so $ CEV_r=1 $ always in this case and hence the method would stop with only one basis function.\n1. The training of $f\\in F_n$ is totally unclear from Eqn 4 line 128. What regularization norm appears in Eqn 4?  What is $f_j$ and $\\hat{f}_j$, and what spaces do they belong to? How is $\\hat{f}_j$ related to the function encoder hypothesis set $F_n$? Is $\\hat{f}_j\\in F_n$? If so, are the coefficients $c_j$ also learned jointly with the $\\psi_j$ in the ``offline'' step?\n1. What is the role of the offline step vs. the least squares step Eqns 2--3? Why not just stop at Eqn 4?\n1. I feel that the number of tasks $ N $ should appear somewhere in the analysis, especially with the PCA-type training given eqn 7. Why doesn't it?\n1. Comparing Eqn 12 and 13 shows that the PAC-Bayes approach gives a strictly worse upper bound; so why even state the result 13? The theorem hypotheses of Thm 1 and 2 appear to be the same.\n1. The numerical experiments do not report $ N $, the number of task datatsets for offline training.\n1. The authors claim that increasing the regularization parameter ``makes the model less prone to overfitting and improves generalization performance'' in line 683--684 does not consider that the empirical risk can (and will) blow up as $ \\lambda \\to\\infty $. Consistency requires $ \\lambda\\to 0 $, which again shows that the Rademacher bound is too loose.\n\n\nTypos:\n1. line 41: ``in the primal'' what? The sentence just ends.\n1. \\item line 133 sentence seems incomplete. And if the basis functions are not unique, then how can they be linearly independent as stated in line 134?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UXKfwqP8N3", "forum": "Np7pWotawk", "replyto": "Np7pWotawk", "signatures": ["ICLR.cc/2026/Conference/Submission21221/Reviewer_nAUD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21221/Reviewer_nAUD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428280814, "cdate": 1761428280814, "tmdate": 1762941630776, "mdate": 1762941630776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper deals with Function Encoders framework. The authors propose two PCA-inspired training algorithms: Progressive Training, which sequentially builds basis functions, and Train-Then-Prune, which trains many bases jointly and prunes redundant ones via covariance analysis.\n\nTheoretical contributions include finite-sample generalization bounds based on Rademacher complexity and PAC-Bayes analysis, showing how generalization depends on the number of bases $n$ and their supremum norms, dataset size, and regularization $\\lambda$.\nEmpirical studies on polynomial regression and nonlinear dynamical systems demonstrate that FEs achieve comparable accuracy to overparameterized neural networks using significantly fewer basis functions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and simple, making technical ideas accessible."}, "weaknesses": {"value": "The theoretical analysis in the paper is detached from the heuristics it proposes. Since the framework is primarily heuristic, its experimental evaluation becomes the key validation tool, yet the experiments are limited in scope and scale. The benchmarks are relatively simple (toy and low-dimensional physical systems). Lacks comparison with other kernel-learning or low-rank approximation methods."}, "questions": {"value": "How Theorem 1 and Theorem 2 can guide us in choosing better basis functions? Should we regularize $R^2n$ or $Rn^{3/2}$ at the stage of learning basis functions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aPogWZ3K4R", "forum": "Np7pWotawk", "replyto": "Np7pWotawk", "signatures": ["ICLR.cc/2026/Conference/Submission21221/Reviewer_BR88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21221/Reviewer_BR88"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992427188, "cdate": 1761992427188, "tmdate": 1762941630025, "mdate": 1762941630025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}