{"id": "og9fnUKHjw", "number": 7477, "cdate": 1758023850458, "mdate": 1759897850426, "content": {"title": "Geometry-Editable and Appearance-Preserving Object Composition", "abstract": "General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework.", "tldr": "We propose DGAD, a disentangled diffusion model for object composition. It uses semantic embeddings for geometry control and a dense cross-attention to preserve appearance.", "keywords": ["Object Composition", "Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6c690f2160cf40cfd8b1008d382758656e7ad42.pdf", "supplementary_material": "/attachment/a6fb8ff6851b374a0282b89621095835374795eb.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD), a diffusion-based approach for object-level customized image generation. The model takes in an object specified by a reference image and places it at a desired location in a background image. The goal is to keep the object fidelity while blending it into the background in a natural way. Prior works in this direction can be mainly categorized into two types: 1) encode objects to semantic tokens (e.g., with DINO or CLIP) and condition the model with cross-attention, 2) train a reference network to extract reference image features and inject to the generator. DGAD is a combination of the two directions -- it uses both cross-attention with sparse semantic object features, and correlation with dense reference network features. Both quantitative and user study results show that DGAD outperforms baselines in object preservation and image quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The combination of sparse semantic features and dense reference features is reasonable and turns out to improve performance.\n- The ablation on each component of the model is comprehensive."}, "weaknesses": {"value": "1. While combining two previous directions improves results, the technical novelty is still limited. Nothing is surprising to me after reading the paper. The paper writing feels more like a technical report instead of an insightful research paper. For example, can you conduct more analysis on what is learned in the gated dense cross-attention to see if the gating weights indeed highlight the target object?\n2. My biggest concern is that the baselines compared in the paper are too old. All these baselines are more than one year ago. Yet we know that personalized image generation or editing is a rapidly evolving field. It feels outdated to experiment on and compare with Stable Diffusion (U-Net) based methods as the community has moved to DiT. Can you compare with some recent approaches such as [1], [2], and [3]? It is hard to assess the effectiveness of the approach without comparing with SOTA methods\n3. I doubt if dense cross attention is really that important. Recently works seem to suggest that simple sequence-concat of noisy latents and conditioning tokens + large Transformer is enough to learn pixel correlations [4, 5, 6]. I wonder if the observations in this paper can really transfer.\n\n[1] Chen, Xi, et al. \"Unireal: Universal image generation and editing via learning real-world dynamics.\" CVPR. 2025.\n\n[2] Yu, Qifan, et al. \"Anyedit: Mastering unified high-quality image editing for any idea.\" CVPR. 2025.\n\n[3] Song, Wensong, et al. \"Insert anything: Image insertion via in-context editing in dit.\" arXiv preprint arXiv:2504.15009 (2025).\n\n[4] Tan, Zhenxiong, et al. \"Ominicontrol: Minimal and universal control for diffusion transformer.\" ICCV. 2025.\n\n[5] Labs, Black Forest, et al. \"FLUX. 1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space.\" arXiv preprint arXiv:2506.15742 (2025).\n\n[6] Wu, Chenfei, et al. \"Qwen-image technical report.\" arXiv preprint arXiv:2508.02324 (2025)."}, "questions": {"value": "1. Is the dataset the same as AnyDoor's evaluation dataset? The description of the dataset seems to be the same (30 DreamBooth objects + 80 COCO background images), yet the results of AnyDoor seems to be different from their paper.\n2. Does DGAD also supports text input? For example, can we specify the pose of the object in the generated image?\n3. Please polish the paper writing. For example:\n- Please unify the method names across the paper (method names in Tab.1 and the paper text are not consistent in capitalization).\n- The spacing between section headers and the text also seems a bit weird.\n- In the legend of Fig.2, \"Dense Cross-Attention\" should be `D` not `S`?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4uxSyfJN1n", "forum": "og9fnUKHjw", "replyto": "og9fnUKHjw", "signatures": ["ICLR.cc/2026/Conference/Submission7477/Reviewer_rrdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7477/Reviewer_rrdB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760682846598, "cdate": 1760682846598, "tmdate": 1762919594270, "mdate": 1762919594270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DGAD, a diffusion-based framework for “general object composition”: placing an object into a new background with arbitrary pose/placement while preserving its original appearance. The key claim is that previous methods either (a) enable flexible editing but lose fine texture (semantic embedding–driven methods like PBE / ObjectStitch / IP-Adapter) or (b) preserve texture but behave like copy-paste and can’t really reorient or rearticulate the object (appearance-reference methods like MimicBrush). DGAD tries to get both.\n\nThe core idea is to explicitly disentangle geometry control and appearance preservation.\n- Encoding stage (geometry-editable encoder): The object is encoded to a compact semantic representation (CLIP/DINO), and that is injected via cross-attention into an inpainting-style diffusion UNet that also sees the background latent and an explicit spatial layout prior (mask + context). This stage is supposed to teach the model to implicitly capture editable object geometry (pose, angle, deformation) without requiring dense supervision like full masks at train time for every new edit.\n- Decoding stage (appearance-preserving decoder): At decoding time, instead of letting the diffusion model hallucinate appearance, DGAD runs a dense cross-attention module between the geometry-edited latent features and dense appearance features from a reference network (BrushNet). There’s also a spatial gating mask α derived from the geometry features, so that appearance features are only injected where the object should be and don’t leak to background.\n- The model is trained end-to-end (keeping SD1.5-inpaint and BrushNet mostly frozen, training new attention modules) on ~386k images + 23k videos processed into triplets (object, background, target composite).\n- Experiments: quantitative comparisons across editability metrics (IR, FID), appearance metrics (LPIPS, DISTS), and semantic alignment metrics (CLIP / DINO score); user study; ablations removing either semantic guidance or appearance features; visual comparisons. DGAD reports SOTA on all metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method has a plausible inductive bias.\n    - Geometry is handled where diffusion models are already good (spatial reasoning inside UNet, with CLIP/DINO guidance and layout conditioning).\n    - Appearance is injected later with an explicit dense cross-attention that is spatially gated (α / β masks).\nThat decomposition feels principled: “coarse where it’s cheap, fine where it matters.” The mask-gated dense attention is more specific than generic “just cross-attend to reference features,” and the ablation suggests it matters (LPIPS/DISTS worsen without dense attention). \n\n2. Empirical results are broad: The authors compare against common baselines (AnyDoor, MimicBrush, IP-Adapter, PBE, ObjectStitch), both numerically and via user study (Composition Quality vs Visual Consistency). There are also ablations that knock out each core piece (semantic guidance, dense attention, layout prior, copied weights) and show consistent drops.\n\n3. Training practicality / reproducibility.\nDGAD is trained with 4×4090 GPUs, SD1.5-inpaint backbone, BrushNet reference net, and mostly lightweight cross-attention heads. Releasing code + weights and claiming minimal changes to SD1.5 helps with real-world impact and with ICLR reproducibility."}, "weaknesses": {"value": "### 1. Novelty vs. integration.\n\nDGAD resembles a careful integration of existing ideas rather than a fundamentally new principle. Concretely:\n- The geometry-editable encoder looks similar to taking an SD-inpaint UNet and augmenting it with CLIP/DINO features (in spirit, not unlike IP-Adapter / PBE style semantic injection) plus spatial priors reminiscent of AnyDoor.\n- The appearance-preserving decoder resembles BrushNet/MimicBrush-like per-pixel reference fusion. DGAD’s main differentiator appears to be the α/β-gated dense cross-attention meant to prevent appearance leakage outside the object region.\n\nOne could characterize DGAD as “IP-Adapter for pose + BrushNet for texture + a spatial gating mask,” i.e., incremental engineering. The current draft asserts novelty of the dense gated cross-attention, but does not provide a head-to-head baseline where everything is identical except for that gating. Without such a baseline, it is still plausible that the core gains stem from stitching together two known paradigms, not from a fundamentally new mechanism.\n\nWhat would help here is a controlled baseline: same geometry encoder + BrushNet-like fusion but without α/β gating, and then quantitative evidence that this baseline either leaks texture into background or fails under strong pose changes. That would isolate DGAD’s core claimed novelty.\n\n### 2. “Implicit geometry” may be overstated.\n\nThe paper repeatedly claims DGAD “implicitly captures geometric transformations” by leveraging the diffusion model’s spatial reasoning, rather than requiring explicit 3D supervision. DGAD still takes:\n- A spatial layout prior (mask/location);\n- A background latent that already encodes where the object should appear;\n- Training triplets constructed from compositing procedures (including video-derived pose variation), which explicitly supervise plausible placements.\n\nIn other words, DGAD is supervised on how to place the object. The model may not predict full 3D pose or depth, but it is not “free-form reasoning about geometry.” It learns from synthetic pairs how an object can plausibly sit in a scene.\n\nOne could say: “This is strong 2D supervision, not emergent geometric understanding.” That weakens the conceptual pitch that DGAD unlocks fundamentally new geometric control.\n\nThe paper would benefit from an extrapolation/robustness experiment: show DGAD handling extreme out-of-plane rotations, large scale changes, or deformations unlikely to appear in training triplets, outperforming baselines. That would support the claim of genuine geometric flexibility, not just memorization of common edits.\n\n### 3. Metric interpretation and validity.\n\nThe evaluation uses IR, FID, LPIPS, DISTS, and CLIP/DINO similarity and maps them to claims like “editability,” “appearance preservation,” and “semantic consistency.” \n\nSeveral issues arise:\n1. IR is underspecified. The paper refers to IR as reflecting “human preferences” or “editability,” but does not precisely define how IR is computed (which model, what prompts, what aspect it measures). Without that, IR risks sounding like “some black-box heuristic that happened to go up.”\n2. FID conflates several factors. A lower FID on the final composite is good, but it mostly says the overall image looks realistic. It does not directly prove correct geometric placement or pose plausibility.\n3. LPIPS/DISTS under pose change. LPIPS compares generated object appearance to the reference. But if DGAD rotates the object 60–90° relative to the reference, LPIPS will naturally worsen even if DGAD correctly preserves fine texture and color on visible regions. The paper claims LPIPS/DISTS support “appearance preservation even under geometric transformations,” but does not justify why those view-dependent distortions don’t invalidate the comparison.\n\nRight now, the mapping from metric → claimed property (“editability,” “appearance faithfulness,” etc.) is looser than it should be. The metrics only partially capture the stated property.\n\nThe paper would be helped by:\n- A precise definition of IR and an explanation of why it correlates with a human notion of correct composition / geometry.\n- A demonstrated correlation between IR (and maybe FID) and the user study’s “Composition Quality” score.\n- A viewpoint-normalized appearance metric (e.g., identity/texture similarity features designed to be rotation-tolerant) or a human study axis explicitly focused on “how well does the inserted object retain its original markings / texture.”\n\n### 4. Limited discussion of failure modes and robustness.\n\nThe qualitative figures in the main paper are mostly successes. There is little analysis, for instance, of (a) cases with thin structures, transparency, or strong specular reflections; (b) scenes requiring consistent lighting/shadow casting; (c) occlusion/ordering issues when the pasted object should go partially behind a foreground element; (d) sensitivity to mask quality. The method appears to assume that at inference time the user (or some upstream step) provides a fairly accurate silhouette / placement region. How robust is DGAD if that mask is approximate, shifted, or slightly too big?"}, "questions": {"value": "1.\tIR metric: How exactly is IR computed? Which pretrained model or scoring pipeline is used, and why should the community interpret it as “editability” or “composition realism” rather than just “image looks nice”? Please also report its correlation with the human “Composition Quality” score.\n2.\tTrain/test leakage: Are the 30 evaluation objects and 80 backgrounds guaranteed to be unseen during training, at the instance level? Are there near-duplicate frames from the 23k training videos that overlap with those objects, effectively letting DGAD memorize their textures from multiple views?\n3.\tNovelty of dense cross-attention gating: Can the authors provide a baseline that uses the same geometry encoder and BrushNet-like reference fusion but without α/β spatial gating? In other words, how much of DGAD’s gain is attributable specifically to that gating versus just smartly combining semantic guidance with an appearance-retrieval branch?\n4.\tMask robustness / user control: At inference, how precise must the spatial prior (mask / placement) be? Can DGAD take a coarse blob and still infer the right silhouette and pose, or does it require a nearly perfect cutout mask? Please quantify: for example, performance as a function of mask jitter or dilation.\n5.\tLighting / shadows / harmonization: Does DGAD ever synthesize shadows consistent with the target scene, or is relighting out of scope? Many of the qualitative examples appear well blended, but it is unclear whether this is due to relighting or simply picking scenes where lighting mismatch is mild.\n6.\tRuntime and practicality: What is the inference-time cost relative to SD1.5-inpaint alone? Is DGAD realistic for interactive editing, or does it target offline content creation?\n7.\tHandling of unseen viewpoints: When DGAD rotates an object to reveal surfaces never observed in the source reference, how are those surfaces hallucinated? Are those hallucinated regions still considered “appearance-preserving,” or are they closer to text-guided synthesis? Examples would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q4BV7Jannf", "forum": "og9fnUKHjw", "replyto": "og9fnUKHjw", "signatures": ["ICLR.cc/2026/Conference/Submission7477/Reviewer_5TGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7477/Reviewer_5TGd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802333074, "cdate": 1761802333074, "tmdate": 1762919593725, "mdate": 1762919593725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DGAD, an object insertion framework that claims to be geometry-editable and appearance-preserving. The unique contribution possibly lies in the dense cross-attention mechanism, which proposes a position-wise gating weight to fuse inserted object features with their mask regions. The experiments validate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing is good. This work proposes the DGAD model for object insertion and presents a plausible way to fuse the object's insertion location and appearance in the dense cross-attention mechanism. The story's claim of \"geometry-editable\" actually means no distortion of the background, and \"appearance-preserving\" actually means no distortion of the inserted object.\n\n2. The proposed \"dense cross-attention\" seems to distribute a dynamic, position-wise gating weight for fusing the query feature (inserted object's CLIP feature) and the key/value features (masks), but it is actually just the mask area, as shown in Equations (5) and (6)."}, "weaknesses": {"value": "1.  Many claims in this paper are overstated. The so-called \"geometry-editable\" feature is only compared to a simple baseline (Figure 1(a), directly inserting a CLIP feature) and does not represent true 3D geometry editing. This is misleading to the reader. A good paper requires a solid contribution, not just a compelling narrative.\n\n2.  Similarly, the \"appearance preserving\" claim is only compared to methods using a single-branch reference network.\n\n3.  The proposed \"dense cross-attention\" in Equations (5) and (6) appears to be a re-implementation of latent blending, simply parameterized with dynamic weights, as also validated in Algorithm 1.\n\n4.  The unique contribution appears very limited. Furthermore, the base model is SD 1.5, which is far outdated. The latest methods it is compared against are from CVPR 2024, which are also outdated."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RAjcmQYC5Y", "forum": "og9fnUKHjw", "replyto": "og9fnUKHjw", "signatures": ["ICLR.cc/2026/Conference/Submission7477/Reviewer_6PMo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7477/Reviewer_6PMo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900225126, "cdate": 1761900225126, "tmdate": 1762919593055, "mdate": 1762919593055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) model, aiming to address a core challenge in general object composition (GOC): how to preserve the appearance details of an object while editing its geometric properties. The authors decouple the two objectives of geometric editing and appearance preservation by implicitly learning geometric transformations using semantic embeddings during the encoding phase and explicitly aligning appearance features using a dense cross-attention mechanism during the decoding phase."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed decoupling framework is well designed, handling geometry and appearance separately in the encoding and decoding stages, thus avoiding the overload problem of a single representation. The idea of ​​implicitly capturing geometric attributes using the spatial reasoning capabilities of a pre-trained diffusion model is insightful.\n\n2. Quantitative experiments cover multiple evaluation metrics (editability, appearance preservation, semantic consistency).  The ablation experiments are well-designed, systematically validating the effectiveness of each component."}, "weaknesses": {"value": "1. Technical descriptions are sometimes overly lengthy; core contributions could be expressed more concisely. The system framework diagram in Figure 2 is unclear, making it difficult to quickly understand the overall architecture.\n\n2. The learning process of the position-gated weights $\\alpha$ is not sufficiently described\n\n3. There is a lack of failure case analysis, and the limitations and boundary conditions of the method are not discussed. The user research scale is small (only 25 people), resulting in insufficient statistical significance. Comparison of runtime and computational cost is not provided.\n\n4. There is a lack of theoretical explanation for why the decoupled design can simultaneously improve two objectives. The mechanism of implicit geometric learning lacks in-depth exploration. The paper claims to have learned geometric properties \"implicitly\" through semantic embeddings during the encoding phase, but how can we verify that the model actually captures the correct geometric transformations rather than simply learning statistical patterns in the dataset? Specifically:\n\n- Is the attention map visualization on the left side of Figure 4 sufficient to demonstrate geometric understanding?\n\n- Does the method still work when the geometric transformations of the test object exceed the training distribution (e.g., more extreme viewpoints or deformations)?\n\n- How can we ensure that the position-gated weights α in Dense Attention accurately correspond to the edited object region, rather than the original object region?"}, "questions": {"value": "1.  Please provide an analysis of some failure cases, especially under what conditions the method fails? For example, extreme viewpoint changes, severe occlusion, or non-rigid deformation.\n\n2.  How much more computational overhead does the Dense Attention mechanism add compared to standard cross-attention? What is the overall inference time of the method? How does the computational cost compare to the baseline method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v6h2xF0O71", "forum": "og9fnUKHjw", "replyto": "og9fnUKHjw", "signatures": ["ICLR.cc/2026/Conference/Submission7477/Reviewer_s7VW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7477/Reviewer_s7VW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995776023, "cdate": 1761995776023, "tmdate": 1762919592351, "mdate": 1762919592351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}