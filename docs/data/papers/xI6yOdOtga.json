{"id": "xI6yOdOtga", "number": 821, "cdate": 1756819775536, "mdate": 1763637249548, "content": {"title": "Echo: Towards Advanced Audio Comprehension via Audio-Interleaved Reasoning", "abstract": "The maturation of Large Audio Language Models (LALMs) has raised growing expectations for them to comprehend complex audio much like humans. Current efforts primarily replicate text-based reasoning by contextualizing audio content through a one-time encoding, which introduces a critical information bottleneck. Drawing inspiration from human cognition, we propose audio-interleaved reasoning to break through this bottleneck. It treats audio as an active reasoning component, enabling sustained audio engagement and perception-grounded analysis. To instantiate it, we introduce a two-stage training framework, first teaching LALMs to localize informative audio segments through supervised fine-tuning, and then incentivizing proficient revisiting via reinforcement learning. In parallel, a structured data generation pipeline is developed to produce high-quality training data. Consequently, we present Echo, a LALM capable of dynamically revisiting audio segments in demand during reasoning. On audio comprehension benchmarks, Echo achieves overall superiority in both challenging expert-level and general-purpose tasks. Comprehensive analysis further confirms the efficiency and generalizability of audio-interleaved reasoning, establishing it as a promising direction for advancing audio comprehension. We commit to releasing the model, code, and data.", "tldr": "We introduce Echo, a Large Audio Language Model with audio-interleaved reasoning that dynamically revisits audio segments, achieving superior performance in complex audio comprehension.", "keywords": ["LALMs", "Audio Comprehension", "Audio-Interleaved Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b82239cc65cc7239ef3c0fea0e26e129aaf37277.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces \"Echo,\" a Large Audio Language Model training framework designed for audio comprehension. The novelty of the training method lies in the introduction of a new reasoning method called \"audio-interleaved reasoning\". Previous works encode audio only once, which the authors argue limit their ability to reason about subtle or complex audio details. In contrast, audio-interleaved reasoning allows the model to dynamically revisit and analyze specific audio segments during its reasoning process. The proposed training framework starts with supervised fine-tuning to locate important audio segments, followed by reinforcement learning to improve its ability to strategically revisit them. \nThe authors also presented a data generation pipeline that uses an LLM to synthesize Audio-QA data and corresponding audio-grounded CoTs for use in their RL and SFT training. \nThe authors demonstrated the performance of Echo by comparing it with various baselines on three benchmarks. The authors also performed ablation studies to analyze the effectiveness of different components of the framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The authors introduced a novel audio reasoning framework, \"audio-interleaved reasoning\", that differentiates this work from the standard \"audio-conditioned text reasoning\" format. The data generation pipeline is original as well.\nQuality: The evaluation is comprehensive and the authors showed the advantage of the proposed framework over even proprietary systems. The ablation studies included analysis on different components of the training framework, further supporting the design decisions. \nClarity: The writing is easy to follow and the authors included many well-made figures to illustrate the framework and pipeline that are relatively complex. \nSignificance: The work has the potential to have significant impact on the field of audio understanding as it suggests a shift in paradigm that could inspire further works in audio-interleaved reasoning."}, "weaknesses": {"value": "1. The model seems to be learning how to reason by imitating the reasoning trace of a separate reasoning LLM, Deepseek-R1, during the SFT phase. It's questionable whether the proposed model really learned how to reason in a more general and flexible sense. \n2. As the authors have mentioned in the conclusion, the revisiting of the audio as implemented in this work is still rather straightforward."}, "questions": {"value": "1. Have the authors tried any other reasoning LLMs besides deepseek-R1 to generate the training data? Or since the QA–CoT triplets are generated by deepseek-R1, have the authors tried using another reasoning LLM for the re-evaluation step so that the final triplets are not restricted to the reasoning pattern of deepseek-R1?\n2. Did the authors tested the more advanced Gemini-2.5-Pro and Step-Audio-2 on MMAR? How do they compare?\n3. Considering that the model has close to double the latency of the base model, has the authors investigated if the model is revisiting some audio segments when it's not necessary? Have the authors tried to minimize the segments revisited?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x9mzE0dcvp", "forum": "xI6yOdOtga", "replyto": "xI6yOdOtga", "signatures": ["ICLR.cc/2026/Conference/Submission821/Reviewer_jS6b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission821/Reviewer_jS6b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760730038170, "cdate": 1760730038170, "tmdate": 1762915619410, "mdate": 1762915619410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response 1"}, "comment": {"value": "We sincerely thank all the reviewers for their insightful critiques and valuable suggestions regarding our manuscript. Your insights have been invaluable in enhancing the overall quality of our work. We have responded to the reviews individually and **updated a new manuscript version**. \n\nWe also wish to express our particular appreciation for the reviewers' recognition of our strengths, including the novelty of the audio-interleaved reasoning paradigm, the benefits of the data generation pipeline, the comprehensive empirical results, and the potential for inspiring future works. This positive feedback is greatly encouraging.\n\nIn the following, we summarize the main improvements made to the revision, and we remain fully available for any additional inquiries. \n\nMain manuscript (all changes are marked in blue):\n- Lines 72-76: Strengthened biological basis argument (@uQ9G).\n- Lines 77-78: Rephrasing of the advantage of the proposed audio-interleaved reasoning (@H3Dr).\n- Line 284 (Table 1): Performance of Gemini-2.5-Pro on MMAR (@jS6b).\n- Lines 317-320: Clarification for evaluation details (@rXCQ, H3Dr).\n- Lines 492-496: Details of potential biases introduced through LLM-generated data (@uQ9G).\n\nAppendix (titles of additional sections are marked in blue in the appendix menu):\n- Appendix F.2: Mitigation of noises in the source metadata (@rXCQ, uQ9G, Dkpf, H3Dr).\n- Appendix F.3: Influence of synthesizer, re-evaluator, and audio extractor (@jS6b).\n- Appendix F.4: Influence of the re-evaluation process (@rXCQ, Dkpf, H3Dr).\n- Appendix I: Quantitative analysis of re-listened segments (@rXCQ, uQ9G, Dkpf, H3Dr, jS6b).\n- Appendix J.2: The decision against direct process supervision (@rXCQ, Dkpf, H3Dr).\n- Appendix K: Generalizability to long audio (@rXCQ, Dkpf).\n- Appendix M: More case visualization (@uQ9G, Dkpf)."}}, "id": "MUkM1yNPO8", "forum": "xI6yOdOtga", "replyto": "xI6yOdOtga", "signatures": ["ICLR.cc/2026/Conference/Submission821/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission821/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission821/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763637389228, "cdate": 1763637389228, "tmdate": 1763637389228, "mdate": 1763637389228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Echo, an LALM designed to perform audio-interleaved reasoning. For this, the authors propose a novel framework that allows the model to actively revisit audio segments during reasoning, rather than relying on a single static audio encoding. Inspired by human cognitive processes, the approach aims to overcome the information bottleneck of prior audio-conditioned text reasoning. The authors train Echo using a 2-step framework, which includes an SFT stage and an RL stage. They also propose their own data generation pipeline that combines Qwen2.5-Omni captions and DeepSeek-R1 prompting (as a result, they propose 2 new datasets). The proposed model achieves significant improvements across audio understanding and reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tries something new, and I am glad that it explores the aspect that is missing in audio.\n- Strong empirical gains on diverse and challenging benchmarks.\n- Thorough ablation and analytical studies validating each training stage.\n- Introduction of high-quality synthetic Audio-QA datasets with CoTs.\n- Efficient implementation and transparent reproducibility commitments. (I hope the model is open-sourced)"}, "weaknesses": {"value": "- (Minor) Grounding for reasoning is not new and has been extensively explored in vision (images and videos). Here are some examples: \n- I strongly disagree with the claim that previous works suffer from “contextualizing audio content through a one-time encoding, which introduces a critical information bottleneck” — I understand what the authors are trying to convey, but the problem statement has not been framed well. This is a grounding issue that the authors are trying to solve. The other way to frame it would be: “Current methods do not ground to the audio while reasoning but Echo does” (I have cited some papers below that does grounding in vision and authors can check how they claim). — A good example is the fact that for prior methods, during the reasoning process, when each token is generated during or after the reasoning process, the model constantly focuses and re-focuses on separate parts of the audio for the next token (through attention maps in self-attention), which is also a form of “attending to the audio multiple times” — as claimed by the authors which Echo explicitly does. There are some papers on “playback” which better fits your narrative [].\n- The data generation pipeline heavily relies on synthetic annotations (DeepSeek-R1, Qwen2.5), which may introduce model-specific biases. I do not see any verification methods. Additionally, I have personally gone through several of AudioSet-Strong and some of MusicBench timestamps. They are very noisy. I wonder how good the generated dataset is.\n- (Minor) I am shocked by the 69.99 on MMAR. Did the authors evaluate with the entire thinking chain and the answer or just the answer?\n- (Minor) A human evaluation of the thinking chain would have been good."}, "questions": {"value": "- I have asked some questions about the weakness.\n- I may be confused, but how does interleaving happen during test time? Is a tool called for cutting the audio? Where is that information on the paper?\n- Does the reward include rewards for the thought? Since the entire audio is also part of the context, how do we know if the final answer comes from the interleaved thought or just the entire audio? I do not see a reward for if the useful segments were interleaved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "euviJQ9cbW", "forum": "xI6yOdOtga", "replyto": "xI6yOdOtga", "signatures": ["ICLR.cc/2026/Conference/Submission821/Reviewer_H3Dr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission821/Reviewer_H3Dr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965267595, "cdate": 1761965267595, "tmdate": 1762915619097, "mdate": 1762915619097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an audio-interleaving reasoning method to improve reasoning abilities in LALMs. The method revisits audio segments during the thinking process for more effective localization and analysis. In order to train this ability the paper proposes a data generation method that utilizes rich temporal annotations in AudioSet-SL and MusicBench. The paper proposes a SFT+RL framework with a custom reward that considers formats, semantic continuity, accuracy, and segment revisiting. Experiments reveal that the proposed ECHO method improves on MMAU and MMAR, and ablation studies demonstrate the effect of each part of the method and the detailed improvements on different aspects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The writing of the paper is very clear. \n- The segment revisiting approach is novel in the LALM domain and it makes a lot of sense to expect it to improve reasoning quality, so it is a good intuition. \n- The data generation and annotation pipeline is clear and easy for reproducing, thus benefiting the community to build upon this work.  \n- The main results indicate that applying ECHO has consistent better results on MMAU and MMAR. In addition, ablation studies clearly show the benefits of audio-interleaving RL and appending segments, and the improvements are consistent accross different subsets of MMAU. \n- There are extra ablation experiments and analysis including error analysis in the appendix, making the analysis of the proposed method very extensive."}, "weaknesses": {"value": "I think the main weakness of this paper is that current experiments do not prove the quality (such as accuracy) of the segment selection, which is a fundamental basis of this paper. In data generation, the paper uses AudioSet-SL and MusicBench. It is known that AudioSet-SL annotations are very noisy, and this could impose noisy training data during data generation. In RL training, the \"format\" and \"consist\" rewards are mostly for formats; the \"acc\" reward is for the final prediction; the \"seg\" reward encourages revisiting the segment but it does not necessarily guarantee whether the revisiting is reliable -- that is, the model might just hallucinate during this step and still gets points. In evaluation, the paper also does not evaluate whether the reasoning process correctly extracts useful segments and conducts reasonable thinking for the segment. In my view, there are two ways to fix this problem. First, explicitly parse the segments and per-segment analysis from the outputs and query a good LALM for verification. Second, one can construct a real or synthetic testset with known, accurate time annotations and see if the model can output exactly the same segments and desirable analysis."}, "questions": {"value": "- Does the framework apply to complex cases where the model has to listen to several distinct segments at the same time? \n- The segment appending operation seems to impact the model's performance on long audio understanding. How to address this efficiency issue? \n- How do you filter inaccurate training samples in Fig 3 especially if the original timestamps are very inaccurate (such as some inaccurately annotated samples in AudioSet-SL)?\n- How does the proposed reward make sure the per-segment reasoning is correct, not hallucination?\n- Can you evaluate the segmenting accuracy and per-segment analysis quality in order to further justify your framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uDITbMBYab", "forum": "xI6yOdOtga", "replyto": "xI6yOdOtga", "signatures": ["ICLR.cc/2026/Conference/Submission821/Reviewer_Dkpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission821/Reviewer_Dkpf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985375828, "cdate": 1761985375828, "tmdate": 1762915615353, "mdate": 1762915615353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop an  LALM to perform audio-interleaved reasoning, and report impressive results on existing benchmarks. The model actively revisits audio segments during reasoning, rather than relying on a single  encoding. Training is enhanced by adding an RL stage.  They also create data generation framework using  captions generated via Qwen2.5-Omni and reasoning prompts via DeepSeek-R1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Strong results on the latest benchmarks.\nNovel two stage training/inference, bringing RL and Audio revisiting into mainstream of LALMs\nDetails of dataset creation and frameworks used."}, "weaknesses": {"value": "I would like a stronger commitment to releasing the code than the vague one of releasing it \"in the future\"\n\nExamples of which segments are revisited would have been good.\n\nSome audio examples would be good.\n\nThe biological basis argument could be strengthened."}, "questions": {"value": "you mention the possibilit of bias introduced through the use of LLMs for generating data. Can you amplify this comment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BVfWKIVlPB", "forum": "xI6yOdOtga", "replyto": "xI6yOdOtga", "signatures": ["ICLR.cc/2026/Conference/Submission821/Reviewer_uQ9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission821/Reviewer_uQ9G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194104362, "cdate": 1762194104362, "tmdate": 1762915612920, "mdate": 1762915612920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Echo, a Large Audio Language Model trained for audio-interleaved reasoning: the model decodes <seg>start,end</seg> tags; whenever such a tag is generated, inference is paused and the corresponding raw audio segment is inserted back into the context, so the model “thinks with audio,” not just about it. Training is two-stage: (1) SFT to make the model reference informative segments in CoT; (2) RL with verifiable rewards to encourage correct formatting, consistency after </seg>, accuracy, and segment usage. A synthetic, LLM-curated pipeline builds the training sets (EAQA-SFT 75.9k with CoT; EAQA-RL 21.9k without), using Qwen2.5-Omni for audio descriptions/transcripts/music analysis and DeepSeek-R1 to synthesize and filter QA-CoT. Evaluations on MMAR, MMAU-mini, and MMAU show Echo beating open and proprietary baselines on average accuracy, with analyses on attention allocation and segment coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Turning audio from a static pre-embedding into an active element during reasoning is simple, intuitive, and well thought out with the <seg> scheme and inference adaptation. \n- The SFT to RL pipeline is coherent; rewards are explicitly specified (format, \"consistency\" after </seg>, accuracy, + a segment-use bonus), making the approach reproducible in principle.\n- The paper details how temporal metadata + Qwen2.5-Omni descriptors feed DeepSeek-R1 for QA-CoT synthesis and filtering into SFT and RL splits. \n- Results on MMAR and MMAU are good; Echo tops macro-average accuracy among peers and even improves o ver some proprietary models on MMAR. \n- Good ablation study with the recipe comparison table (A --> B --> C --> D) attributes gains, shows initial interleaving hurt without RL, then recovers to best accuracy-which is an useful insight."}, "weaknesses": {"value": "- No human evaluation of the thinking chain.\n- No qualitative analysis of the data being generated by the LLMs for training, this might induce bias or the training data might also contain some hallucinated data.\n- EAQA-SFT CoTs are constrained by source temporal metadata (the paper later notes SFT annotations limited to first 10s), yet claimed generalization beyond 10s is argued only by aggregate coverage stats; no targeted stress-tests show failure modes when informative cues cluster late. The generalization claim needs more and stronger evidence.\n- Since the model outputs thinking chains, the authors should specify whether they use the thinking chain as well during evaluating against MMAU and MMAR. The thinking traces might contain multiple answers and regex/string based answer matching might get compromised in this setting.\n- Although the RL pipeline and rewards are explicitly specified, the \"consistency\" reward checks whether the token after </seg> is capitalized or <; this is a formatting heuristic, not a perceptual constraint. The segment reward grants +0.5 if at least one segment is referenced and the answer is correct, this is easy to satisfy by superficial/random segment references once the model learns the distribution of answer keys. There’s no penalty for irrelevant segments or when a model generates lots of <seg>start,end</seg> tags across the timeline beyond minor format checks."}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lDjHC16ecZ", "forum": "xI6yOdOtga", "replyto": "xI6yOdOtga", "signatures": ["ICLR.cc/2026/Conference/Submission821/Reviewer_rXCQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission821/Reviewer_rXCQ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762410153361, "cdate": 1762410153361, "tmdate": 1762915612634, "mdate": 1762915612634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}