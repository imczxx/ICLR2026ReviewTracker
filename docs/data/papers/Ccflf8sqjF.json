{"id": "Ccflf8sqjF", "number": 5887, "cdate": 1757943394943, "mdate": 1763660860670, "content": {"title": "Replicable Reinforcement Learning with Linear Function Approximation", "abstract": "Replication of experimental results has been a challenge faced by many scientific disciplines, including the field of machine learning. Recent work on the theory of machine learning has formalized replicability as the demand that an algorithm produce identical outcomes when executed twice on different samples from the same distribution. Provably replicable algorithms are especially interesting for reinforcement learning (RL), where algorithms are known to be unstable in practice. While replicable algorithms exist for tabular RL settings, extending these guarantees to more practical function approximation settings has remained an open problem. In this work, we make progress by developing replicable methods for linear function approximation in RL. We first introduce two efficient algorithms for replicable random design regression and uncentered covariance estimation, each of independent interest. We then leverage these tools to provide the first provably efficient replicable RL algorithms for linear Markov decision processes in both the generative model and episodic settings. Finally, we evaluate our algorithms experimentally and show how they can inspire more consistent neural policies.", "tldr": "We provide results for replicable ridge regression, uncentered covariance estimation as well as RL in both the generative model and episodic linear MDP setting.", "keywords": ["reinforcement learning", "learning theory", "replicability", "stability", "linear MDP"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/257166cdbcc200018b53dfe68fbd93de8a84c66a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical challenge of replicability in reinforcement learning, where algorithms are notoriously unstable, especially when combined with function approximation[lines: 1243, 1253, 1254].The authors extend the formal, \"exact\" notion of replicability—where an algorithm must produce identical outputs from different data samples given the same internal randomness [lines: 1403, 1404]from the tabular setting [lines: 1246, 1270] to the more practical domain of linear function approximation[lines: 1247, 1323].\n\nTo achieve this, the paper makes several key contributions:\n1. It first develops two novel, provably replicable statistical tools: **R-Ridge-Regression** for the random design setting [lines: 1248, 1516] and **R-UC-Cov-Estimation** for uncentered covariance matrices[lines: 1248, 1616]. Both are of independent interest and are built upon randomized rounding techniques [lines: 1414, 1516, 1618].\n2.  Using these tools, the paper presents the **first provably efficient and replicable RL algorithms for linear MDPs**. This includes **R-LSVI** for the generative model setting [lines: 1249, 1647] and **R-LSVI-UCB** for the more challenging episodic exploration setting [lines: 1249, 1728].\n3.  The theoretical results are supported by experiments. First, on CartPole with random Fourier features, the authors show their method achieves replicability with far fewer samples than the loose theoretical bounds suggest [lines: 1799, 1805, 1843]. Second, they apply the *idea* of their work (quantization) to deep RL (PQN on Atari) and demonstrate that it leads to significantly higher policy agreement across random seeds without compromising performance [lines: 1800, 1869, 1915, 1916].\n\nThe paper's theoretical claims are well-supported and appear sound. The authors properly adopt the formal replicability framework from Impagliazzo et al. (2022) [lines: 1401, 1403] and build upon the standard analysis of linear MDPs from Jin et al. (2020). The core of the contribution lies in the novel replicable estimators for ridge regression and covariance. The proofs for these tools correctly use properties of strong convexity and concentration bounds to show that the estimators from two different datasets will be close enough to be mapped to the same rounded value [lines: 1517, 1525, 2759-2763]. The subsequent RL algorithm proofs correctly follow by induction, carefully accounting for the error propagation from batching and rounding [lines: 3747, 3757, 3887]. The experimental methodology is clear, and the results convincingly support the paper's claims.\n\nThe paper is exceptionally well-written, clear, and logically structured. It begins with a strong motivation for the problem, clearly defines the technical preliminaries (linear MDPs and the specific replicability definition) [lines: 1331-1436], and then intelligently builds its contributions. The decision to first introduce the replicable statistical \"tools\" (regression and covariance) in a standalone section [line: 1444] before applying them to the more complex RL setting is highly effective for clarity. The experiments are well-described, and the figures clearly visualize the trade-offs between performance and policy agreement [lines: 1809, 1866].\n\nThe contribution is original and significant. The lack of replicability is a major, well-documented practical problem in RL [lines: 1243, 1252], and this paper is the first to provide a *formal, theoretical* solution in the non-tabular setting of linear function approximation [lines: 1247, 1324]. The new algorithms, R-Ridge-Regression (for random designs) [line: 1515] and R-UC-Cov-Estimation [line: 1616], are novel contributions in their own right and may have applications beyond RL [line: 1447]. This work successfully bridges a major gap and opens up a new and important line of research into provably stable and reliable RL."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* **Novelty and Significance:** This is the first work to provide provably replicable algorithms for RL with linear function approximation [lines: 1247, 1324]. It tackles the well-known \"replication crisis\" in RL [line: 1243] with theoretical rigor, moving beyond the tabular setting.\n* **New Replicable Estimators:** The paper develops R-Ridge-Regression and R-UC-Cov-Estimation, two new statistical estimators with replicability guarantees [lines: 1248, 1445, 1446]. The extension of replicable regression to the *random design* setting [line: 1515] (as opposed to fixed design) is a key tool that enables the subsequent RL results.\n* **Comprehensive Theoretical Results:** The paper provides a complete theoretical treatment, developing replicable algorithms for both the generative model setting (R-LSVI with core set) [line: 1647] and the more difficult episodic exploration setting (R-LSVI-UCB) [line: 1728].\n* **Strong Experimental Validation:** The authors provide two sets of experiments:\n    1.  A direct validation of the theory on CartPole, which encouragingly shows that replicability is achieved with far fewer samples than the theoretical bounds require [lines: 1804, 1843].\n    2.  A practical extension to deep RL (Atari), showing the core idea of quantization inspired by the theory leads to measurably more consistent policies (higher action agreement) without a loss in performance [lines: 1800, 1915, 1916]."}, "weaknesses": {"value": "While this is a strong paper, its weaknesses are primarily the necessary trade-offs for achieving such a strong replicability guarantee.\n\n1.  **High Sample Complexity:** The most notable weakness, which the authors acknowledge, is the very high sample complexity of the provable algorithms [line: 1733]. For example, the bound for R-LSVI-UCB has high-polynomial dependencies on $d$ and $H$ (e.g., $d^{56}H^{62}$), making the *theoretical* bounds impractical, even if the empirical results are much better.\n2.  **Gap Between Linear Theory and Deep RL:** The theory is confined to linear MDPs, which assumes a good feature representation is already available [lines: 1734, 1794]. The paper does not address the (very hard) problem of how to *learn* these features in a replicable way [lines: 1797, 1956]. The deep RL experiment, while promising, is a heuristic (quantizing Q-values, not weights [lines: 1869, 1870]) and does not have the same theoretical guarantees.\n3.  **Core Set Assumption:** The generative model algorithm (R-LSVI) relies on access to a core set of state-action pairs [lines: 1701, 1704]. This is a standard assumption in related literature but is a strong one that sidesteps the exploration problem, which is (fairly) handled separately in the R-LSVI-UCB algorithm."}, "questions": {"value": "1.  The sample complexity bounds are the main limitation. The authors mention concurrent work (Hopkins et al., 2025) achieving low overhead for replicability in the *tabular* setting [lines: 1959, 1960]. What are the primary theoretical bottlenecks that prevent a similar low-overhead result in the linear setting? Is the high cost mainly from the R-Ridge-Regression, the R-UC-Cov-Estimation, or the complex interaction between the UCB bonus and the rounding error?\n2.  In the deep RL experiments, the authors chose to quantize the final Q-values [lines: 1869, 1870], whereas the theory for the linear setting involves rounding the *weights* (via R-Hypergrid-Rounding on the output of the regressor) [lines: 1516, 1443]. Could you elaborate on this design choice? Was rounding the neural network weights directly attempted, and if so, what challenges (e.g., training instability) did it present?\n3.  Figure 2 shows that for PQN, regularization alone did not significantly improve policy agreement, but quantization did [lines: 1912, 1916]. This is a very interesting result. Does this imply that the instability in deep RL is less about agents converging to different *local minima* (which regularization might help smooth) and more about the high sensitivity of continuous Q-value representations to small variations in data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "omXE4nSlmK", "forum": "Ccflf8sqjF", "replyto": "Ccflf8sqjF", "signatures": ["ICLR.cc/2026/Conference/Submission5887/Reviewer_2UsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5887/Reviewer_2UsG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512725339, "cdate": 1761512725339, "tmdate": 1762918326874, "mdate": 1762918326874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies replicability in RL in the setting of linear function approximation, building upon the seminal work of Impagliazzo et al (2022) and other works that explored replicability in bandits and tabular RL. In a nutshell, replicability asks for RL algorithms that output the same policy when executed twice in the same MDP environment, under shared internal randomness. This work provides the first replicable algorithms for RL in the linear function approximation setting. En route to that task the authors first develop two new, efficient, and replicable algorithms for replicable random design regression and replicable uncentered covariance estimation. They also provide empirical evaluation of their results."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper builds upon a line of work that has received interest and studies a natural problem.\n\n- Some first theoretical bounds are established in this setting, potentially inspiring future work."}, "weaknesses": {"value": "- I found the writing (especially in section 3) to be a bit confusing.\n\n- The results in section 3 seem to rely on fairly standard algorithmic templates of estimating quantities from samples at some sufficiently high accuracy and then rounding; the bounds also seem to be (fairly) suboptimal.\n\n- The situation is similar for the results in section 4; the replicability arguments are fairly standard (given the results of the previous section) and the sample complexity bounds are unsatisfactory."}, "questions": {"value": "- I'm a bit confused about the discussion around Theorem 3.1, and some notation in it. What does $E_{D_{[t]}^M}[(\\theta^\\top x - y)^2 + \\lambda ||\\theta||^2]$ mean? In particular, since $D_{[t]}$ is a sequence of distributions, which one of them are $(x,y)$ drawn from? I might be missing something obvious. Also, I didn't understand why the discussion around the algorithm is needed. Since one can show that the estimate $\\hat w$ from the vanilla ridge-regression is close to the optimal value (as stated in the theorem), why doesn't replicability follow from the (relatively) standard techniques of saying that across two executions $\\hat w_1, \\hat w_2$ will be close to each other (since they are close to $\\theta^*$), and then apply the rounding?\n\n- Why does Algorithm 4 need to have access to a coreset? Is that part of the standard description of RL with linear function approximation in the non-replicable setting?\n\n- As a general comment, I would appreciate it if you could help me understand the results and the necessity of the ideas and techniques in section 3 better. I'm happy to adjust my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YDTekWQwfd", "forum": "Ccflf8sqjF", "replyto": "Ccflf8sqjF", "signatures": ["ICLR.cc/2026/Conference/Submission5887/Reviewer_Byb9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5887/Reviewer_Byb9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897240093, "cdate": 1761897240093, "tmdate": 1762918326472, "mdate": 1762918326472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response"}, "comment": {"value": "Dear Reviewers, \n\nthank you all for your feedback on our submission. The feedback we received was very clear. It seems there is consensus on how the current version of the manuscript can be improved, which is exactly what one might hope for from a set of scientific reviews. In general, it seems that reviewers had trouble following the proof structure in section 3 and thus the contribution inherent to our analysis. To address this, we have updated our manuscript as follows\n\n* We acknowledge that we were a bit ambitious in attempting to provide all results, algorithms, and figures in the main text, which led to our proof descriptions ending up a bit short due to space constraints. We have spent a large chunk of the newly available additional page of content towards elaborating on our findings and the novelty of our contribution. \n* We increased the number of evaluation trajectories in our experimental results on Atari. Now, increased agreement also correlates with a decrease in return variance, as one might expect. We updated both plots; none of the claims changed.\n\nIn the updated manuscript, we highlight all major changes from the previous version in blue. We will summarize the major changes here and provide point-wise answers to each reviewer in the respective review sections."}}, "id": "GJRcTv1lkh", "forum": "Ccflf8sqjF", "replyto": "Ccflf8sqjF", "signatures": ["ICLR.cc/2026/Conference/Submission5887/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5887/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5887/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763659407497, "cdate": 1763659407497, "tmdate": 1763659407497, "mdate": 1763659407497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies establishing replicability of RL algorithm. By replicability, it means that the algorithm has the same output on arbitrary input with high probability. The authors first derive a replicability result on ridge regression and covariance estimation, which extends the result of randomized rounding result from Impagliazzo et al., 2022. Then, these tools are utilized for the analysis of replicability result of offline RL and online with exploration in the linear MDP case, which requires ridge regression and covariance estimation steps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors tackle an important issue of reproducibility of RL algorithms in theoretical sense. This is important as many deep RL algorithms are not easy to reproduce in practice.\n\n2. This seems to be the first result for replicability in linear MDP setting. The authors manage to connect the replicability result in supervised learning with theoretical RL in linear MDP literature.\n\n3. The theoretical results are complemented by experimental results to show that there are room for  improvement in the sample complexity result."}, "weaknesses": {"value": "1. While the theoretical results are new, the novelty of the algorithm or the proof is not clearly explained : 1) Section 3.1 does not clearly explain where the novelty of the algorithm lies compared with Esfandiari et al., 2023a; 2)The use of a core-set appears closely aligned with ideas already established in the literature (Lattimore and Szepesvári).\n\n2. The overall bound is not tight compared to that of Esfandiari et al., 2023a or Hopkins et al., 2025. The authors argue that this comes at the expense of generality. However, the rationale behind this tradeoff is not entirely convincing.  \n\n3. Algorithm 2 requires additional projection  of the estimated covariance matrix on to the cone after rounding is executed."}, "questions": {"value": "1.  The author could provide more comments on why the dependency on $\\epsilon$ is high in Theorem 4.1 or 4.2.\n\n\n2. As mentioned in the weakness, where does the large gap compared to the tabular case(Hopkins et al., 2025) come from?\n\n\n3. Can the authors provide more detailed comparison on how worse the bound is compaed to those of tabular cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "12WAA3m5Ft", "forum": "Ccflf8sqjF", "replyto": "Ccflf8sqjF", "signatures": ["ICLR.cc/2026/Conference/Submission5887/Reviewer_aY1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5887/Reviewer_aY1c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959076201, "cdate": 1761959076201, "tmdate": 1762918326138, "mdate": 1762918326138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops new replicable algorithms for reinforcement learning in the linear function approximation regime. There have been two prior works on replicable reinforcement learning and one concurrent work. Prior works operate in the setting where the number of actions and state spaces are bounded (and small). In contrast, the present work handles the case where the number of state and action spaces are infinite. However, it makes a linear feature representation of state-action pairs, which is also widely studied in reinforcement learning. In this model, they develop a replicable algorithm for reinforcement learning. This algorithm relies on two fundamental replicable primitives: (1) ridge regression algorithm and (2) uncentered covariance estimation algorithm. The covariance estimation algorithm appears to be novel, while the ridge regression algorithm is not completely novel: it existed for fixed design settings, and this paper extends it to the i.i.d. design setting when the distribution of covariates is bounded. Finally, the authors evaluate their algorithms experimentally and show that they lead to more consistent policies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Replicability is an interesting and valuable property of algorithms worth having. The paper presents a new replicable algorithm that operates the regime with a large number of state and action pairs where existing methods do not work. As such, I believe it's well in scope for ICLR, and studies an interesting topic."}, "weaknesses": {"value": "I found this paper very difficult to read.\n\nAt the highest level, there are two main issues. First, the novelty of the proof ideas compared to prior work in replicability and replicable RL is unclear. Second, the paper lacks comparison with existing RL algorithms in the linear regime—it's unclear whether this algorithm modifies existing approaches by using replicable subroutines or requires more fundamental changes.\n\nAt a lower level, the very large polynomial factors in the RL results are puzzling. Can these be improved, or are they necessary? While the simulations are a nice addition. However, then I am confused why the computation of the specific polynomial factors is necessary/important: if the subroutines are replicable, shouldn't the RL algorithm automatically be replicable? Why compute the overall sample complexity? I would have felt better about the large polynomial factors if there were concrete reasons why the polynomial dependences could not be improved (easily). A proof overview here would have been nice, but as far as I see, there were no proof overviews in the main body at least.\n\nRegarding readability issues:\nThe discussion before Theorem 3.1 is a bit confusing. I understand that the objective is strongly convex and hence the closeness in function values is proportional to the closeness in parameters space. But then the discussion of what is needed in the proof is not clear."}, "questions": {"value": "Minor issues with Theorem 3.1: The phrase \"from which we draw $M$ independent samples totalling $N$\" is unclear. Perhaps it meant \"We draw $M$ independent samples from each distribution, totaling $N = M * t$.\" In the definition of $\\theta^*$, which distribution is the expectation taken over?\n\n---\n\nMinor Typos/Grammatical errors\n1. Algorithm 2: “paramter”\n1. Malahanobis → Mahalanobis\n1. Related works: intrduced\n1. Section 5.2: unforseen\n1. Below theorem 3.1: staightforward\n1. Above Section 5: \n    1. inpractice → in practice\n    1. meticuluous → meticulous \n1. Section 5.1 end of first para has a repeated “the”\n1. Above Section 6: Tegularization → Regularization\n1. Punctuation/hyphenation missing at various places, e.g., in paragraph 2 of the paper “algorithms internal randomness” should be “algorithm’s internal randomness”\n1. Section F: house-hold grade → household-grade\n1. Section G: “sentence of word” → “sentence or word” also, in this section, I do not understand what “and work to specific theorem statements.” means.\n1. Many of the proofs throughout the appendix use $E$ instead of $\\mathbb{E}$ for denoting expectation.\n\nFurther minor suggestions \n1. The “i.e.” before Proposition 2.1 is unusual.\n1. Spurious space at the period at the end of proposition 2.1.\n1. $\\theta^*$ is mentioned in the statement of Theorem 3.2 but only defined in the statement of Theorem 3.1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dzcYYYgzT2", "forum": "Ccflf8sqjF", "replyto": "Ccflf8sqjF", "signatures": ["ICLR.cc/2026/Conference/Submission5887/Reviewer_jZdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5887/Reviewer_jZdx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960593509, "cdate": 1761960593509, "tmdate": 1762918325438, "mdate": 1762918325438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}