{"id": "Fy7V5dalvX", "number": 12846, "cdate": 1758210888801, "mdate": 1763408411833, "content": {"title": "There Was Never a Bottleneck in Concept Bottleneck Models", "abstract": "Deep learning representations are often difficult to interpret, which can hinder their deployment in sensitive applications. Concept Bottleneck Models (CBMs) have emerged as a promising approach to mitigate this issue by learning representations that support target task performance while ensuring that each component predicts a concrete concept from a predefined set. In this work, we argue that CBMs do not impose a true bottleneck: the fact that a component can predict a concept does not guarantee that it encodes only information about that concept. This shortcoming raises concerns regarding interpretability and the validity of intervention procedures. To overcome this limitation, we propose Minimal Concept Bottleneck Models (MCBMs), which incorporate an Information Bottleneck (IB) objective to constrain each representation component to retain only the information relevant to its corresponding concept. This IB is implemented via a variational regularization term added to the training loss. As a result, MCBMs yield more interpretable representations, support principled concept-level interventions, and remain consistent with probability-theoretic foundations.", "tldr": "", "keywords": ["concept bottleneck models", "information bottleneck", "representation learning", "variational inference"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eaee88bb6e6ec9bf03c9fa07e4a75eff74bc7a1b.pdf", "supplementary_material": "/attachment/852f1929f13268ced090145dab6b077b45ee4100.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the limitations of standard Concept Bottleneck Models (CBMs) in ensuring true concept-level interpretability. The authors argue that while CBMs predict human-defined concepts, their internal representations may still encode extraneous information, undermining interpretability and intervention validity. To address this, they propose Minimal Concept Bottleneck Models (MCBMs), which introduce an Information Bottleneck (IB) objective to ensure each representation component retains only concept-relevant information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-organized, and the arguments regarding CBM limitations are presented clearly, with intuitive figures that effectively contrast CBMs and MCBMs.\n- The paper provides formal justification for the proposed Information Bottleneck-based extension, connecting interpretability, information leakage, and probabilistic soundness.\n- Experiments across multiple datasets and model variants (e.g., CBM, CEM, AR-CBM, SCBM) offer a broad empirical perspective, strengthening the claim that MCBMs improve both alignment and disentanglement."}, "weaknesses": {"value": "- While the paper’s theoretical insights are valuable, the overall novelty appears modest. The Information Bottleneck principle is a well-established framework, and similar approaches applying it to CBMs seem to have been explored in [1] and [2]. Given this overlap, it would be helpful if the authors could better clarify the significance of their contribution—specifically, what distinguishes their approach from prior works.\n- In Table 5, the accuracy reported for SCBM in [3] is 86.00, which appears higher than the 84.8/84.9 achieved by MCBM. Could the authors clarify what accounts for this difference?\n- Introducing additional layers based on the Information Bottleneck framework would likely increase inference cost and memory usage. Could the authors quantify how much overhead this introduces?\n\n**References** \\\n[1] Parisini, E., Chakraborti, T., Harbron, C., MacArthur, B. D., & Banerji, C. R. (2025). Leakage and interpretability in concept-based models. arXiv preprint arXiv:2504.14094. \\\n[2] Makonnen, M., Vandenhirtz, M., Laguna, S., & Vogt, J. E. (2025). Measuring leakage in concept-based methods: An information theoretic approach. arXiv preprint arXiv:2504.09459. \\\n[3] Vandenhirtz, M., Laguna, S., Marcinkevičs, R., & Vogt, J. (2024). Stochastic concept bottleneck models. Advances in Neural Information Processing Systems, 37, 51787-51810."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EsbNTCIwRd", "forum": "Fy7V5dalvX", "replyto": "Fy7V5dalvX", "signatures": ["ICLR.cc/2026/Conference/Submission12846/Reviewer_6hx5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12846/Reviewer_6hx5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761110002203, "cdate": 1761110002203, "tmdate": 1762923643408, "mdate": 1762923643408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments"}, "comment": {"value": "We sincerely thank all reviewers for the time, effort, and constructive feedback dedicated to improving our submission. We have carefully addressed each of your comments in the individual replies. A revised version of the manuscript, with all modifications highlighted in blue, is now available.\n\nBeyond minor corrections suggested by the reviewers, we have incorporated several substantive changes and new experiments. The main updates are:\n\n- **Expanded evaluation:** We now include the Animals with Attributes 2 (AwA2) dataset, as requested by Reviewer dC8m, strengthening the empirical scope of our study.\n\n- **Additional baseline:** Energy-based CBMs (ECBMs) have been integrated as a new comparative method, following Reviewer xCkz’s suggestion.\n\n- **Improved interpretability evaluation:** We incorporate the Oracle Information Score (OIS) to better account for scenarios where concepts exhibit correlation, as recommended by Reviewer 5MFs.\n\n- **Practical guidance for tuning $\\gamma$:** We empirically show that interpretability metrics are strongly correlated with the amount of nuisance information in the representations (Table 7). This finding allows these metrics to serve as practical proxies for selecting $\\gamma$ when nuisance labels are unavailable (proposed by Reviewer dC8m).\n\n- **Added discussion of limitations and future directions:** Following Reviewer 5MFs’s recommendation, we now provide an explicit discussion of the main limitations of our work and promising research avenues.\n\nWe hope these improvements address your concerns and further clarify the contributions of our work. Thank you again for your careful reviews."}}, "id": "PBu9ygJbF9", "forum": "Fy7V5dalvX", "replyto": "Fy7V5dalvX", "signatures": ["ICLR.cc/2026/Conference/Submission12846/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12846/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12846/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763408428149, "cdate": 1763408428149, "tmdate": 1763408428149, "mdate": 1763408428149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents Minimal Concept Bottleneck Models (MCBMs), an advancement upon the traditional Concept Bottleneck Model (CBM) framework. The proposed method integrates an Information Bottleneck (IB) objective to mitigate \"concept leakage,\" thereby constraining the intermediate representations to encode only concept-specific information. This approach yields a significant advantage by establishing a sound theoretical basis that provides formal guarantees for the efficacy of concept-level interventions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the paper is sound and propose an interesting analysis of CBMs.\n- the problem of concept leakage in CBMs is very relevant and the results on experiments are encouraging."}, "weaknesses": {"value": "- \"Formally, given a task (...), Vanilla Models (VMs) are trained (...)\" here for how it is presented it seems the VMs work from c to y, while in Sec. 2 it is clear it is intended from x to y. Thus I'd rephrase like: \"their representations z should capture the information necessary to predict y given x accurately.\" and also rephrase at the beginning of the sentence: \"given an input x, a task y and a set of concepts c\". As a fussiness, I'd change \"Formally\" with \"Specifically\" as task, concepts are not precise terms, and to make this sentence formal I'd expect to know what are the space y, c, and x belong. \n- at the beginning of Sec. 2, y is not in bold, but immediately after it becomes bold.\n- In Sec. 2 some symbols are not defined or even just dropped without being commented, e.g. theta, phi, f_theta ...\n- I find a bit odd to have Section 5 discussing problem of intervention in CBMs that is instead addressed by MCBM. Indeed, I think it can strengthen the motivation to propose MCBM. But I guess this is done to don't interrupt the flow of the model.\n- no limitations of MCBMs are discussed.\n- no discussion about future work is reported.\n- not clear what bold denotes in table 5 and 6.\n- Other metrics to measure the concept leakage has been used in prior work (e.g. [1,2]). I think it would be useful to take into account also other metrics and not just rely on URR, as this is the main advantage MCBMs propose. \n\n\n[1] Marton Havasi et al. Addressing leakage in concept bottleneck models. In NeurIPS, 2022.\n[2] Anita Mahinpei et al. Promises and pitfalls of black-box concept learning models. In Workshop on Theoretic\nFoundation, Criticism, and Application Trend of Explainable AI @ ICML, 2021."}, "questions": {"value": "1) With how many concepts MCBM is supposed to properly work?\n2) Is the training timing significantly increased while using MCBMs or it requires more epochs to converge?\n3) concept leakage is generally associated with an increasing performances (as the nuisances can help the development of reasoning shortcuts to solve the problem). How do you motivate MCBMs has very close or even higher task accuracy on CUB than other methods suffering from leakage?\n4) Apart from concept leakage, MCBMs have also consequences on other aspects, e.g. in terms of disentanglement and OIS between concepts? See e.g. this recent paper [3] for a discussion on different metrics to evaluate concept quality.\n\n\n[3] Debole, Nicola, et al. \"If Concept Bottlenecks are the Question, are Foundation Models the Answer?.\" arXiv preprint arXiv:2504.19774 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OrYKOl9rll", "forum": "Fy7V5dalvX", "replyto": "Fy7V5dalvX", "signatures": ["ICLR.cc/2026/Conference/Submission12846/Reviewer_5MFs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12846/Reviewer_5MFs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841411548, "cdate": 1761841411548, "tmdate": 1762923643034, "mdate": 1762923643034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the theoretical foundations of Concept Bottleneck Models (CBMs) and argues that there is, in fact, no true “bottleneck” in existing CBMs.\nThe authors show that each latent variable $z_j$ may encode nuisance information unrelated to its corresponding concept $c_j$, thus undermining interpretability and invalidating standard intervention assumptions.\nTo address this, the paper proposes the Minimal Concept Bottleneck Model (MCBM), which introduces a per-concept Information Bottleneck (IB) constraint to enforce that each $z_j$ becomes a minimal sufficient statistic of $c_j$.\nExperiments on several datasets (CIFAR-10, CUB, Shapes3D, MPI3D) demonstrate improved disentanglement and more stable concept-level interventions compared to prior CBM variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear theoretical motivation:\nThe paper provides a precise and mathematically grounded critique of CBMs, identifying the lack of causal interpretability in the conventional bottleneck assumption.\n2. Principled formulation:\nThe proposed information bottleneck constraint is elegant and conceptually well-justified; it directly connects to the goal of removing nuisance information while retaining task-relevant concepts.\n3. Strong theoretical discussion:\nThe analysis of $p(z_j|c_j)$ and the derivation of the KL regularization term are rigorous and highlight a meaningful direction for improving causal interpretability in concept-based models."}, "weaknesses": {"value": "1. The paper does not clearly cite any existing literature to support the statement that “each concept $c_j$ must be recoverable from a designated component $z_j \\in z$.”\nThe original CBM (Koh et al., 2020) only enforces a per-concept prediction loss, not a one-to-one structural correspondence between $c_j$ and $z_j$.\n2. From a performance standpoint, MCBM is a “more interpretable but weaker” model.\nThe paper does not explicitly position its goal as interpretability improvement rather than predictive performance.\nIf the objective is purely interpretability, it remains unclear how encoding only a subset of concepts leads to better explanations, especially since MCBM explicitly removes part of the concept information.\n3. The paper does not discuss how much task accuracy loss is acceptable or provide any principle for balancing the IB weight $\\gamma$.\nThis trade-off is central to the model’s practical usability but left unanalyzed.\n4. The random split of the CUB attribute groups into c vs $n_y$ introduces noise and may compromise the validity of the disentanglement and CKA metrics.\nBecause semantically correlated features can be split across c and $n_y$, the reported “high disentanglement” might reflect random partitioning rather than true semantic separation.\n5. Intervention evaluation (Section 4.3) lacks robustness:\nno variance or confidence intervals are reported, and the curves for different $\\gamma$ values are nearly parallel, making the claim of “intervention gain invariance” unconvincing.\nMoreover, there is no comparison with baseline settings such as random or group-wise interventions.\nThe CUB intervention results in Figure 5 also differ substantially from those reported in CEM (Fig. 6 of their paper), raising reproducibility concerns."}, "questions": {"value": "1. In Lines 40–41, the authors state that “each concept $c_j$ must be recoverable from a designated component $z_j \\in z$.”  \n   In the original CBM paper, it is only assumed that an intermediate layer is resized to the number of concepts and trained with a concept loss, but not that each $z_j$ uniquely represents $c_j$.  \n   Could the authors please **cite the exact section or equation** in Koh et al. (2020) that formalizes this one-to-one correspondence?\n\n2. The paper assumes the factorization  \n   $\n   p(x, y, c, n) = p(x \\mid c,n)\\,p(y \\mid x)\n   $\n   Could the authors clarify the **causal assumptions** under which this holds?  \n   Specifically:  \n   (i) Do you assume $y \\perp (c,n)\\mid x$?  \n   (ii) If there exist direct dependencies of $y$ on $c$ or $n$ not mediated by $x$, how does this decomposition remain valid?  \n   A detailed justification or citation would strengthen the theoretical basis.\n\n3. In Appendix E.2, after aggregating CUB attributes into 27 semantic groups, why are **12 groups randomly selected for $c$** and the remaining **20 groups** used for $n_y$?  \n   How was this number (12 vs 20) chosen, and is it consistent across runs or random seeds?\n\n4. This partition is not semantically meaningful but rather a controlled perturbation design.  \n   Because many task-relevant attributes are discarded into $n_y$, even a fully faithful model cannot recover $y$.  \n   Under such an incomplete concept set, can MCBM still be regarded as a valid model that achieves both interpretability and predictive ability?\n\n5. Energy-based Concept Bottleneck Models also jointly model $p(y|x)$ and $p(y|c,x)$.  \n   Why were such approaches not included as baselines, given that they share similar modeling goals and provide a natural comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LJUtK2wvy2", "forum": "Fy7V5dalvX", "replyto": "Fy7V5dalvX", "signatures": ["ICLR.cc/2026/Conference/Submission12846/Reviewer_xCkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12846/Reviewer_xCkz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921773016, "cdate": 1761921773016, "tmdate": 1762923642701, "mdate": 1762923642701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces Minimal Concept Bottleneck Models (MCBMs), which address the key limitation of Concept Bottleneck Models (CBMs); Information leakage. Although CBMs ensure that each latent variable $z_j​$ can predict its corresponding concept $c_j$​, they allow $z_j$​ to retain nuisance information from the input $x$.\n- MCBMs formalize this issue as $I(Z_j;X|C_j) > 0$ and introduce an explicit Information Bottleneck (IB) objective to enforce minimal sufficient representations. By adding a variational regularization term, MCBMs constrain each $z_j$​ to encode only concept-relevant information.\n- Experiments show that MCBMs yield more disentangled and interpretable representations, reduce nuisance leakage, and enable consistent, concept-level interventions while maintaining comparable concept accuracy to standard CBMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear formulation of problems: While the issue of information leakage in CBMs is known, this paper clearly formalizes it from an information-theoretic perspective $(I(Z_j;X|C_j) > 0)$ and identifies the root cause as the lack of a minimal sufficient statistic.\n- Principled approach: The application of the classic Information Bottleneck (IB) framework to solve this problem is logical and well-founded.\n- Proposal of an information leakage metric: The paper introduces URR, a new metric to provide a quantitative way to approximate the information leakage $I(Z_j;X|C_j)$."}, "weaknesses": {"value": "- Impractical Hyperparameter ($\\gamma$): The model's behavior is dictated by $\\gamma$, which requires ground-truth $n_y$ labels for tuning. The authors provide no practical guidelines for setting $\\gamma$ on real-world datasets where such labels are unavailable.\n- Performance degradation on real datasets: The strong bottleneck effect from synthetic data does not translate to CIFAR-10 and CUB. On these real-world datasets, MCBM achieves only a marginal reduction in nuisance leakage (URR). This minimal gain results in a disproportionately large drop in task accuracy, raising questions about the method's practical utility."}, "questions": {"value": "- (Related to weakness 1) The model's performance varies extremely with $\\gamma$, from 100% to 24.9%. Please provide a concrete procedure for setting $\\gamma$ on a real dataset where ground-truth labels for $n_y$ are unavailable. \n- (Related to weakness 2) We observed a poor accuracy/nuisance-removal trade-off on CIFAR-10 and CUB. Could you provide insight into why this occurs? \n- (Related to weakness 2) To assess the method's generality, have the authors also conducted experiments on other standard concept benchmarks, such as AWA2 or OAI?\n- Unlike [1] and [2], where interventions on the CUB dataset reduced task error ([1], Fig. 4; [2], Fig. 5), this paper shows the opposite trends. Could the authors explain what experimental differences might cause this discrepancy?\n\n\n\nReference\n- [1] Koh et al., Concept Bottleneck Models, ICML 2020.\n- [2] Shin et al., A Closer Look at the Intervention Procedure of Concept Bottleneck Models, ICML 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ocHOfPlMXB", "forum": "Fy7V5dalvX", "replyto": "Fy7V5dalvX", "signatures": ["ICLR.cc/2026/Conference/Submission12846/Reviewer_dC8m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12846/Reviewer_dC8m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951121053, "cdate": 1761951121053, "tmdate": 1762923642198, "mdate": 1762923642198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}