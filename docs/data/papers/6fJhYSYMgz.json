{"id": "6fJhYSYMgz", "number": 16517, "cdate": 1758265513057, "mdate": 1759897235785, "content": {"title": "DS-VTON: An Enhanced Dual-Scale Coarse-to-Fine Framework for Virtual Try-On", "abstract": "Despite recent progress, most existing virtual try-on methods still struggle to simultaneously address two core challenges: accurately aligning the garment image with the target human body, and preserving fine-grained garment textures and patterns. These two requirements map directly onto a coarse-to-fine generation paradigm, where the coarse stage handles structural alignment and the fine stage recovers rich garment details. Motivated by this observation, we propose DS-VTON, an enhanced dual-scale coarse-to-fine framework that tackles the try-on problem more effectively. DS-VTON consists of two stages: the first stage generates a low-resolution try-on result to capture the semantic correspondence between garment and body, where reduced detail facilitates robust structural alignment. In the second stage, a blend-refine diffusion process reconstructs high-resolution outputs by refining the residual between scales through noise–image blending, emphasizing texture fidelity and effectively correcting fine-detail errors from the low-resolution stage. In addition, our method adopts a fully mask-free generation strategy, eliminating reliance on human parsing maps or segmentation masks. Extensive experiments show that DS-VTON not only achieves state-of-the-art performance but consistently and significantly surpasses prior methods in both structural alignment and texture fidelity across multiple standard virtual try-on benchmarks.", "tldr": "", "keywords": ["Diffusion Models", "Virtual Try-On", "Dual-Scale Framework"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d348e6b743e50eef89a3101d2ec9fb5050be4742.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DS-VTON, an enhanced dual-scale coarse-to-fine framework designed to overcome the two core challenges in virtual try-on: achieving accurate garment-body alignment and preserving fine-grained textures and patterns. \n\nThe proposed method first generates a low-resolution try-on image that establishes robust semantic correspondence between the garment and the target human pose, leveraging reduced detail to facilitate structural alignment. \n\nIn the second stage, a novel blend-refine diffusion process reconstructs high-resolution results by denoising the residual between scales through noise–image blending, which emphasizes texture fidelity and corrects fine-detail errors inherited from the coarse stage. \n\nNotably, DS-VTON operates without any human parsing maps or segmentation masks, offering a fully mask-free generation pipeline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- 1. This paper proposed a novel dual-scale, mask-free framework that enhances the coarse-to-fine process and is particularly well-suited for the try-on task.\n\n- 2. The mask-free formulation is a clear practical advantage—eliminating dependence on potentially brittle human-parsing or segmentation modules improves robustness and simplifies deployment.\n\n- 3. The blend-refine diffusion re-formulation is novel and well-motivated; explicitly bridging low- and high-resolution distributions with a tunable $α / β$ mixture gives the model tighter control over the coarse-to-fine transition and consistently reduces texture artifacts.\n\n- 4. Despite its additional stage, the method remains computationally reasonable: both stages share the same lightweight SD-1.5 U-Net backbone, inference is only ~5 s per image on a single A6000, and runtime is on par with or faster than most recent competitors."}, "weaknesses": {"value": "1. Training data are synthetically amplified with IDM-VTON generations; although the paper acknowledges the risk, visible entanglement still occurs—hair, accessories, or background sometimes change, indicating less-than-perfect disentanglement that could undermine identity preservation in real applications.\n\n2. The low-resolution stage is constrained to a fixed σ = 2; no adaptive or content-aware schedule is explored, so structural detail can be lost for unusually complex garments, and the choice feels ad-hoc given limited ablation (only σ ∈ {1, 2, 4} tested).\n\n3. Coefficients α and β are set to a constant 0.5 for all images; this rigid trade-off may be sub-optimal when garment or pose complexity varies, yet no mechanism to predict or learn sample-specific values is provided.\n\n4. The method inherits the gender, body-type, and skin-tone biases of the SD-1.5 backbone; failure cases on out-of-distribution body shapes are shown, and no bias-mitigation strategy or inclusive evaluation is offered.\n\n5. Runtime, although reasonable, is still sequential-two-stage; latency doubles compared with single-pass competitors, and no distillation or joint-training scheme is investigated to enable real-time deployment on edge devices."}, "questions": {"value": "- 1. In the authors’ Eq. (1), α and β appear to be crucial, yet Table 3 only gives the values found by ablation on a single split. Do these numbers generalize to every dataset, or must they be re-tuned whenever the data distribution changes? The paper offers neither statement nor evidence.\n- 2. AAAI 2025 [1] also presents an identical coarse-to-fine VTON framework—why has no comparison been made?\n\n[1] Li G, Wang Y, Luan J, et al. Cascaded diffusion models for virtual try-on: Improving control and resolution[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(5): 4689-4697.\n\n- 3. Since the Low-Resolution Stage only produces a coarse result, why not drop ReferenceNet and adopt a CatVTON-like pipeline without the garment-encoding branch, instead of deliberately slowing the model down?\n- 4. How computationally complex is this two-stage optimization architecture, which incorporates such a sophisticated ReferenceNet?\n- 5. Mask-free methods can still be evaluated using SSIM and LPIPS.\n- 6. Quantitative results for the Low-Resolution Stage are missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0xjWKkIgxt", "forum": "6fJhYSYMgz", "replyto": "6fJhYSYMgz", "signatures": ["ICLR.cc/2026/Conference/Submission16517/Reviewer_PwYY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16517/Reviewer_PwYY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760497929523, "cdate": 1760497929523, "tmdate": 1762926606254, "mdate": 1762926606254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "summary:\nThis article proposes a coarse-to-fine generation framework that generates coarse results at low resolution and uses them as high-resolution generation input to generate fine results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "strength：\n1.The coarse-to-fine framework is interesting and effective\n2. The experiments are sufficient and the experimental results are excellent"}, "weaknesses": {"value": "weakness：\n1. The overall approach is not innovative and is similar to IMAGDressing and MagicCloth.\n2. The results in Table 1 are quite different from those of FiTDiT.\n3. The generated results of the anime character in Figure 8 have obvious defects, such as the blue long sleeves.\n4. The work based on SD is slightly behind, and experiments based on DiT or Flux may be a better choice"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TjkvKWPatQ", "forum": "6fJhYSYMgz", "replyto": "6fJhYSYMgz", "signatures": ["ICLR.cc/2026/Conference/Submission16517/Reviewer_7i5o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16517/Reviewer_7i5o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760857653697, "cdate": 1760857653697, "tmdate": 1762926605870, "mdate": 1762926605870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method that perform image-based virtual try-on using two different diffusion models that deal two scale image content. The demonstrated results are nice compared to previous work. Another advantage is that the proposed method does not require an additional human body parsing/segmentation mask."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The overall proposed method is simple and straightforward.\n- The proposed method does not require a human parsing mask, makes it easier to deploy to real-world usage.\n- The empirical results are nice against other existing methods."}, "weaknesses": {"value": "- The overall method novelty is limited since the multi-scale image processing has been studied for a very long time. And the main technical difference of this method is to use two different diffusion models to handle input images that captures content under two different scales.\n- The justification and evaluation of the use of human parsing learned at the diffusion model is not demonstrated in the paper.\n- The necessity of the Blend-refine diffusion reformulation is doubtful. It is recommended to have comparison with other guidance methods to justify the needs of proposing a new method for this.\n- Although the results might not be simialrly nice, I am curious about what is the results of latest large image editing models, such as Nano Banana, Qwen image edit, and other similar models the authors can access before submission? Based on my understanding, these models also can generate virtual try-on methods. I think extensive comparison with these models are also valuable."}, "questions": {"value": "- I am wondering whether the proposed method can only work with SDXL? How to extend it to other more modern models, such as FLUX?\n- I am curious whether it is possible to compare the high resolution stage with other guided generation methods, such as controlnet? For example, it could be possible to generate many low resolution and high resolution garment pairs for training a controlnet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9R5y4xTuGX", "forum": "6fJhYSYMgz", "replyto": "6fJhYSYMgz", "signatures": ["ICLR.cc/2026/Conference/Submission16517/Reviewer_5Z7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16517/Reviewer_5Z7A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393454770, "cdate": 1761393454770, "tmdate": 1762926605433, "mdate": 1762926605433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DS-VTON, a dual-scale framework to address the problem of single-stage diffusion model for virtual try-on tasks. DS-VTON attempts to disentangle the global structure alignment process from the fine-grained texture restoration. For this purpose, DS-VTON employs a two-stage paradigm: 1) In the low-resolution stage, the model generates a coarse try-on result by suppressing high-frequency content, while 2) in the high-resolution stage, DS-VTON transforms the low-resolution output into high resolution, restoring fine textures and correcting fine-detail errors from the first stage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing of this paper is easy to follow. The motivation is well clarified, and the proposed method is easy to understand.\n\n2. The quantitative and qualitative comparisons with state-of-the-art methods on two public virtual try-on datasets demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. One of the major problem of this paper lies in the novelty of the proposed DS-VTON method. The idea of first generating low-resolution images and then transforming them into a high-resolution version has already been widely explored in the field of high-resolution image generation. The multi-scale latent upsampling technique used in [1][2][3] is quite similar to the dual-scale DS-VTON method. Could the authors make a comparison with these approaches to elaborate more clearly on their technical contributions?\n\n[1] Du R, Chang D, Hospedales T, et al. Demofusion: Democratising high-resolution image generation with no $$$[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024: 6159-6168.\n\n[2] Guo L, He Y, Chen H, et al. Make a cheap scaling: A self-cascade diffusion model for higher-resolution adaptation[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2024: 39-55.\n\n[3] Jeong J, Han S, Kim J, et al. Latent space super-resolution for higher-resolution image generation with diffusion models[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 2355-2365.\n\n2.  Another problem of DS-VTON is that it relies on the paired data to perform the model training. Although a generative model (i.e., IDM-VTON) is used to construct the input data, the quality of the generated pseudo-reference images will heavily affect the performance of DS-VTON. For example, IDM-VTON may preserve some clothing areas of the target samples in the pseudo-reference images. Training with these imprecise inputs, the model will consider that these areas should be preserved as background regions, thus establishing incorrect associations between the target clothing information and the human body or background pixels. This will lead to occlusions and clothing ghosts in the final results.\n\n3. proposed DS-VTON requires to train/fine-tune two diffusion models separately, and also needs to perform inference on these two models. This may introduce additional computational complexity and inference time. Could the authors provide some computational efficiency analysis of the training and inference stages?\n\n4. Could the authors present some failure cases and the related discussions to demonstrate the limitations of the proposed method?"}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h5CsCeQxID", "forum": "6fJhYSYMgz", "replyto": "6fJhYSYMgz", "signatures": ["ICLR.cc/2026/Conference/Submission16517/Reviewer_XGQG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16517/Reviewer_XGQG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075007043, "cdate": 1762075007043, "tmdate": 1762926604967, "mdate": 1762926604967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}