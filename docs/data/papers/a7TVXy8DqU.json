{"id": "a7TVXy8DqU", "number": 9444, "cdate": 1758122774410, "mdate": 1759897724478, "content": {"title": "Car4Cast: A Dataset and Benchmark for LLM-Based Motion Forecasting and Spatial Reasoning in Autonomous Driving", "abstract": "Recent advances in Large Language Models (LLMs) have shown promise in diverse reasoning tasks, yet their ability to perform structured spatial-temporal prediction remains underexplored. To address this, we introduce Car4Cast, a novel dataset and benchmark that casts 3D motion forecasting in autonomous driving as a spatial reasoning task and testbed, involving structured text generation. Car4Cast provides a comprehensive evaluation suite tailored to the unique challenges of language-based motion prediction, including both classical trajectory accuracy and LLM-specific issues, such as output formatting and hallucinations. Our benchmark also supports an optional visual modality, enabling future exploration of vision-language models in spatial reasoning tasks. Car4Cast is conceived to drive progress toward spatially intelligent language models, highlighting the need and providing data and evaluation tools for new methods and training paradigms that effectively bridge this existing gap.", "tldr": "A dataset and benchmark casting 3D motion forecasting as a spatial reasoning task for Large Language Models", "keywords": ["Large Language Models", "Vision Language Models", "Dataset", "Motion Forecasting", "Autonomous Driving", "Spatial Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fb7fcd55987c6257fd5cc5db8ba9c9ca7aca50c.pdf", "supplementary_material": "/attachment/9d3d38804281edbf5f5d7ef364ba9ba1a73a1edc.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new dataset designed to benchmark large language models (LLMs) as the prediction and planning module in autonomous driving. The authors curate scenarios from established datasets, add annotations, and provide an additional bird’s-eye-view (BEV) LiDAR input. They also benchmark several LLMs alongside a simple linear baseline, analyzing results across object categories and motion types (linear vs. nonlinear).\n\nThe paper is well written and provides timely contributions to the ongoing discussion about whether, and how, LLMs should be integrated into autonomous driving pipelines. However, the current framing feels somewhat too specialized toward spatial reasoning tasks. Making the dataset more broadly applicable would strengthen its utility."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The dataset is a valuable contribution, addressing the gap of standardized benchmarks for LLMs in autonomous driving.\n\nThe benchmarking is thorough, and the inclusion of a linear baseline is well chosen and informative."}, "weaknesses": {"value": "While the paper is strong overall, I see three notable weaknesses:\n\nNo map input is included, even though maps are a standard input in many non-LLM models.\n\nCamera inputs are missing. Their inclusion would enable cross-comparison with end-to-end (E2E) models and make the dataset more versatile.\n\nThe reproducibility of evaluation is unclear. Without a standardized evaluation suite, adoption by the community may be limited."}, "questions": {"value": "Addressing at least two of the following questions would raise my score, with more resolved leading to a higher increase:\n\nCan maps be provided as an additional input?\n\nCan camera data be included as well?\n\nHow will evaluation be standardized? For example, will a machine-readable format (e.g., JSON, as in nuScenes) and evaluation toolkit be provided? Otherwise, adoption might face significant barriers.\n\nThe linear baseline is a great choice. Could you also include a simple 2-layer MLP baseline to further strengthen the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XO3flI17HS", "forum": "a7TVXy8DqU", "replyto": "a7TVXy8DqU", "signatures": ["ICLR.cc/2026/Conference/Submission9444/Reviewer_H1Lv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9444/Reviewer_H1Lv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398570352, "cdate": 1761398570352, "tmdate": 1762921041731, "mdate": 1762921041731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Car4Cast, which casts 3D motion forecasting in autonomous driving as a spatial reasoning task. And it provides a unified evaluation suite with 3D forecasting and LLM-specific structured prediction metrics."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "It provides 1,000 scenes and over 100,000 trajectories from established 3D autonomous driving datasets. It provides a visual modality in the form of top-down LiDAR-based maps, enabling evaluation with Vision-Language Models (VLMs) and supporting multimodal extensions."}, "weaknesses": {"value": "1. The paper introduces 3D motion forecasting in autonomous driving as a spatial reasoning task; however, the incorporation of Large Language Models (LLMs) introduces hallucination issues. The paper does not clearly explain the rationale for framing 3D motion forecasting as a spatial reasoning task, nor does it elaborate on the advantages of this formulation compared to traditional approaches. As a result, it is difficult to assess the novelty and contribution of the work.\n2. The paper lacks comparisons with conventional motion forecasting methods, so how to prove that LLM/VLM is effective?\n3. The paper lacks visual analyses of motion forecasting results across different models. As a benchmark, it needs more details about the evaluation."}, "questions": {"value": "Same to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fURARJYQQy", "forum": "a7TVXy8DqU", "replyto": "a7TVXy8DqU", "signatures": ["ICLR.cc/2026/Conference/Submission9444/Reviewer_Fw34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9444/Reviewer_Fw34"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642216245, "cdate": 1761642216245, "tmdate": 1762921041506, "mdate": 1762921041506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Car4Cast, a dataset and benchmark that reframes 3D motion forecasting as a structured text generation problem for LLMs/VLMs. The benchmark includes over 11k scenarios from a collection of AD datasets (nuScenes, KITTI, PandaSet, Argoverse 2, CADC). It provides both textual inputs and an optional visual modality, and designed LLM-oriented evaluation metrics like formatting accuracy and hallucinated objects percentage. Experiments across LLMs/VLMs plus SFT on Qwen3-4B show: (1) LLMs generally perform better than a linear baseline but show worse average result due to some significant errors in some cases; (2) VLMs perform even worse when extra maps are provided; (3) SFT improves answer format but decreases numerical accuracy, indicating misalignment between token-level objectives and numerical correctness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe writing is clear. The proposed benchmark and contributions are easy to understand. Formatting multi-agent 3D forecasting as structured language output is novel, and the correspond evaluation metric design beyond the traditional ADE/FDE is reasonable.\n2.\tThe experiments cover a wide range of LLMs/VLMs with different model size/families, and thus eliminate the bias caused by individual models and shows informative results. A negative result on SFT is also informative and honest.\n3.\tProcessing pipeline, dataset, evaluation code, prompts and training setups are all provided, so this dataset is ready to be used and the pipeline is easy to reproduce."}, "weaknesses": {"value": "1.\tRegarding the contribution of this benchmark, most of the evaluations proposed in the paper can actually be accomplished on the nuScenes or KITTI datasets themselves through interface definitions + prompt design, not requiring much additional manual effort. The primary contributions regarding the dataset are merely the combination of datasets and the extra metrics.\n2.\tThe SFT result highlights cross-entropy’s misalignment with numerical/geometry accuracy, but other benchmarks like NuScenes-SpatialQA has shown some LLM’s ability to perform spatial inference. Why does it get even worse but not at least remain at a similar level? No solution that could possibly improve the performance is provided (e.g., structured losses by separating non-numerical tokens and numbers). If the dataset cannot improve the performance of motion forecasting, the persuasiveness of the contribution will be further diminished.\n3.\tThe reason of non-reasoning models overperforming reasoning models in general remains unclear. Given that CoT has demonstrated strong performance in other logical reasoning problems, it is puzzling."}, "questions": {"value": "1.\tIs the benchmark also compatible for LLM-based models? Since the pure LLMs/VLMs perform poorly in the benchmark, is it possible to include recent language-based forecasting systems (e.g., TrajLLM) to show the validity and practical significance of the benchmark?\n2.\tThe experiments for VLMs: The image prompt is just using a simple point-cloud-transformed BEV image, and point clouds sometimes contain noises (By checking the provided map images, noises are indeed observed). Is it possible to gain better results by using RGB BEV images? Or by denoising and increase the resolution of the image?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y6lsdHttrA", "forum": "a7TVXy8DqU", "replyto": "a7TVXy8DqU", "signatures": ["ICLR.cc/2026/Conference/Submission9444/Reviewer_fbaA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9444/Reviewer_fbaA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933193214, "cdate": 1761933193214, "tmdate": 1762921041238, "mdate": 1762921041238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Car4Cast, a dataset and benchmark designed for improving LLM-based spatial reasoning in autonomous driving using 3D motion forecasting. Car4Cast uses trajectory prediction and formulates a structured text generation task, requiring LLMs to predict future positions and orientations of vehicles in JSON-like format.\n\nKey contributions of Car4Cast are:\n- Dataset: ~11,985 scenes, 102,510 annotated instances from established 3D driving datasets (nuScenes, KITTI, Argoverse, etc.), with both textual and optional visual modalities (LiDAR-based maps).\n-  Combines classical metrics (ADE, FDE, rotation error) with LLM-specific metrics (formatting accuracy, hallucination rate, collision rate).\n- Experiments: Benchmarks multiple LLMs (14B–1000B parameters) and VLMs, analyzing effects of reasoning, vision modality, and supervised fine-tuning"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed dataset addresses a critical gap in evaluating spatial reasoning for LLMs.\n- The proposed dataset and evaluation metric includes both traditional forecasting metrics and LLM-specific criteria.\n- Spatial reasoning is essential for autonomous driving; the paper convincingly argues why LLMs struggle.\n- The paper conducts an insightful experimental analysis which covers reasoning, vision modality, and fine-tuning effects."}, "weaknesses": {"value": "- The proposed Car4Cast has limited real-world deployment perspective, since it mainly focuses on benchmarking using nuScenes, KITTI, Argoverse, etc.. rather than practical integration into driving stacks.\n- Can authors explain why  even best LLMs fail to match simple Linear baselines in mean ADD. \n- In Table 2 it shows clearly that visual modality is underutilized, why does VLMs perform worse than text-only models; It would be better to see an analysis of this issue, since currently this experiment is shallow not giving any reasoning or perspective.\n- In the current experimental set up fine-tuning has limitations:  authors did not explore slternative strategies (e.g., RLHF, geometry-aware loss) experimentally.\n- Experiments regarding safety implications are not discussed, i.e authors did not discuss on how hallucinations or formatting errors could impact downstream driving systems using the proposed Car4Cast dataset."}, "questions": {"value": "- Could chain-of-thought reasoning combined with physics priors improve performance on nonlinear trajectories?\n- How robust is Car4Cast to domain shifts (e.g., different cities, weather, or sensor setups)?\nPlease refer weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cgBRTE9v6o", "forum": "a7TVXy8DqU", "replyto": "a7TVXy8DqU", "signatures": ["ICLR.cc/2026/Conference/Submission9444/Reviewer_jh9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9444/Reviewer_jh9F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035477005, "cdate": 1762035477005, "tmdate": 1762921040954, "mdate": 1762921040954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}