{"id": "pojbQAEvJQ", "number": 13932, "cdate": 1758225430509, "mdate": 1763154970163, "content": {"title": "InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models", "abstract": "Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of CLIP text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Our proposed loss leverages different levels of cross-attention maps extracted from the U-Net decoder to enforce accurate object placement and a balanced object presence during sampling. Our method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive quantitative and qualitative evaluations demonstrate that, on widely adopted spatial benchmarks (VISOR and T2I-CompBench), our approach establishes a new state-of-the-art (to the best of our knowledge), delivering substantial performance gains and even surpassing fine-tuning-based baselines.", "tldr": "", "keywords": ["Diffusion Models", "Spatial Alignment", "Inference-Time Guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/cb45d134215617c844727c743579e313f280a3e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes InfSplign, a training-free, inference-time spatial alignment method for text-to-image diffusion models (Stable Diffusion). The approach computes cross-attention maps for each object token at multiple decoder layers, estimates centroids and variances, and applies a composite spatial loss during sampling. Extensive experiments on VISOR and T2I-CompBench demonstrate the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The formulation is intuitive and mathematically grounded in attention-space statistics.\n- Outperform in both inference-time and fine-tuning methods on major spatial reasoning benchmarks.\n- Ablation results demonstrate that all three loss components contribute meaningfully."}, "weaknesses": {"value": "- The paper lacks a convincing justification that cross-attention variance is a valid proxy for object presence and representation balance; while the method computes centroids and variances from decoder cross-attention, it does not establish (theoretically or empirically) that lower variance reliably implies preserved objects or balanced representations, which calls for direct evidence.\n- The paper lacks a clean isolation of spatial-alignment gains relative to STORM; results are reported only for the combined objective. so it remains unclear whether improvements stem from genuine spatial reasoning ($L_{spatial}$) or from easier benefits due to improved object visibility or balance ($L_{precense}, L_{balance}$). A fair comparison requires reporting an $L_{spatial}$ only variant against STORM and fine-grained ablations with presence and balance removed.\n- The runtime comparison is missing: how does the end-to-end inference time in vanilla Stable Diffusion compare to that of the proposed method?"}, "questions": {"value": "Does the method extend to transformer-based diffusion backbones (e.g, DiT, FLUX)? If so, what modifications are required to the attention hooks to make it work on these architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IfcY1kyIMG", "forum": "pojbQAEvJQ", "replyto": "pojbQAEvJQ", "signatures": ["ICLR.cc/2026/Conference/Submission13932/Reviewer_R376"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13932/Reviewer_R376"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670604603, "cdate": 1761670604603, "tmdate": 1762924436751, "mdate": 1762924436751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We would like to thank the reviewers for their insightful and constructive feedback. We do appreciate the time and effort invested in evaluating our submission.\n\nWe continue to believe that our proposed approach ($\\texttt{InfSplign}$) and the accompanying loss formulation are novel and represent a meaningful step forward in advancing the current state of the art in spatial cognizance of diffusion models. Our proposed method offers significant performance gains on the most widely adopted spatial cognizance benchmarks, **VISOR** and **T2I CombBench**. We would like to highlight that the focus on the four primary spatial relationships ($\\textit{on top of}$, $\\textit{at the bottom of}$, $\\textit{to the left of}$, and $\\textit{to the right of}$) stems from the fact that nearly all current state-of-the-art baselines employ these same relationships, and these two prominent benchmarks exclusively evaluate them.\n\nWhile we have already prepared additional results for more recent diffusion models, such as SDXL, for the rebuttal, we believe that extending our work to Transformer-based architectures or to a broader set of spatial relationships requires further development that cannot be meaningfully addressed within the rebuttal period. In light of this, we have decided to withdraw the paper at this time.\nWe once again thank the reviewers for their valuable comments, which will undoubtedly help us strengthen the work for future submission."}}, "id": "U49Blw8oT3", "forum": "pojbQAEvJQ", "replyto": "pojbQAEvJQ", "signatures": ["ICLR.cc/2026/Conference/Submission13932/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13932/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763154969471, "cdate": 1763154969471, "tmdate": 1763154969471, "mdate": 1763154969471, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces InfSplign, a training-free, inference-time guidance method to improce the spatial alignment of the text-to-image diffusion models. The core idea is to apply a compound loss over the features extracted from the attention maps of the UNet during inference time to nudge the noisy latent towards correct spatial alignment. The method is simple, intuitive, and demostrates impressive state-of-the art performance on the VISOR and T2I-CompBench benchmarks, even outperforming fine-tuning based approaches.\n\nHowever, the works' significant contribution is undermined by several critical weaknesses, The primary spatial loss is based on handcrafted, non-generalized set of rules that appear overfitted to the specific relations in the VISOR benchmark. Furthermore, The object presence and balance losses rely on heuristics that may fail in common compositional scenarios involving nested objects or objects of separate scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is simple, effective and plug-and play. Since the method doesn't require any retraining, it can be applied to Unet-based models.\n- Spatial placement (centroid margin), object dropping (variance minimization), and overshadowing (variance parity) map cleanly onto observed issues.\n- VISOR and T2I-CompBench numbers are consistently higher than both inference-time and fine-tuning-based baselines."}, "weaknesses": {"value": "- The spatial loss encodes a fixed set of relations (left/right/above/below/near) via axis-wise centroid differences with a margin. This is likely tuned to VISOR and does not cover richer or contextual relations (between, around, on, behind/in-front-of).\n- The method assumes the caption can be parsed cleanly into ⟨A,R,B⟩ and only enforces a single binary relation at a time. The paper does not study multi-object scenes with multiple simultaneous constraints, where losses could conflict.\n- The control relies on explicit, handcrafted geometry and a handful of tuned hyperparameters (α, m, λ’s), selected via grid search on a VISOR subset. This risks overfitting the control surface to that distribution."}, "questions": {"value": "- Your spatial loss in Eq. 6 is defined for a fixed set of five relations. How would your method handle more complex, non-axial relations like \"a car between two trees\" or \"a fence surrounding a house\"? Furthermore, the T2I-CompBench includes relations like \"on the side of,\" which is not in your list; how is this handled by your implementation? Does it default to the \"near\" condition, and if so, how does this affect the precision of the guidance?\n\n- The representation balance loss $\\mathcal{L}_{balance}$ penalizes differences in attention variance, implicitly assuming objects should have a similar \"footprint\" in the attention space. How does this loss behave in scenarios with intended scale differences or nesting, such as \"a man holding his child\" or \"a sticker on a laptop\"? Would this loss not provide a counterproductive signal in these common cases by trying to equalize the variance?\n\n- Your method uses centroids as a proxy for object location. STORM, another high-performing inference-time method, uses Optimal Transport to reshape the entire attention distribution. Could you please provide a conceptual comparison between your versus optimal transport? What are the fundamental advantages and disadvantages of each, beyond final benchmark scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xIQQJ9SPBB", "forum": "pojbQAEvJQ", "replyto": "pojbQAEvJQ", "signatures": ["ICLR.cc/2026/Conference/Submission13932/Reviewer_4aiC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13932/Reviewer_4aiC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772087988, "cdate": 1761772087988, "tmdate": 1762924436394, "mdate": 1762924436394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free, plug-and-play method to improve spatial alignment of T2I diffusion models. The approach involves adjusting the noise during each denoising step using a combination of three loss functions. The authors demonstrate the method's efficacy on U-Net-based diffusion models, specifically SD1.4 and 2.1."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is training-free and can be integrated into existing models as a plug-and-play module, which can be a good practical method for U-Net based T2I diffusion models.\n- The paper includes extensive ablation studies that thoroughly investigate the contributions of different components, particularly the loss hyperparameters."}, "weaknesses": {"value": "- The experimental validation is limited to relatively older U-Net architectures (SD1.4, SD2.1). It is unclear whether the spatial alignment issues addressed persist in more recent, state-of-the-art models, and whether the proposed method remains effective on them.\n- The method's design is heavily focused on the U-Net architecture, which has shifted to transformer-based backbones such as DiT and MMDiT (e.g., in SD3). These newer architectures employ different attention mechanisms (e.g., joint attention instead of cross-attention). The paper lacks experimental validation to confirm if the proposed approach is applicable and effective for these transformer-based diffusion models.\n- While the ablation studies are extensive, the results indicate significant sensitivity to hyperparameter tuning for the loss components. This raises concerns about the method's generalizability, as it may require extensive, model-specific, or test dataset-specific hyperparameter searching to achieve optimal performance, potentially undermining its plug-and-play nature."}, "questions": {"value": "- The method introduces loss calculations at every sampling step, which presumably impacts computational overhead. Could the authors quantify the additional latency (e.g., percentage increase in sampling time) introduced by this process? Given that diffusion sampling is already computationally intensive, significant overhead could be a practical disadvantage.\n- The introduction states that “state-of-the-art performances on spatial understanding are\naround 20%, significantly lagging behind performance on other aspects such as attribute binding\n(around 50%).” Could the authors please clarify which specific SOTA models and benchmarks this figure refers to? As far as I am aware, recent models (e.g., SD3.5, QwenImage, Infinity) appear to demonstrate significantly stronger text-image alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5mNJBydEly", "forum": "pojbQAEvJQ", "replyto": "pojbQAEvJQ", "signatures": ["ICLR.cc/2026/Conference/Submission13932/Reviewer_RCVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13932/Reviewer_RCVb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834767159, "cdate": 1761834767159, "tmdate": 1762924435909, "mdate": 1762924435909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes InfSplign, a training-free, inference-time method that improves the spatial alignment of objects in text-to-image generation by adjusting the noise at each denoising step via a compound loss. The method leverages hierarchical cross-attention maps from the U-Net decoder to enforce accurate object placement and balanced presence. Evaluations on spatial benchmarks (VISOR and T2I-CompBench) show that InfSplign outperforms existing inference-time methods and even some fine-tuning-based approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The method is training-free and requires no extra inputs, making it easy to deploy and compatible with various diffusion backbones.\n\n+ It achieves competitive results on standard benchmarks, significantly outperforming other inference-time methods.\n\n+ The hierarchical use of attention maps and the introduction of variance as a measure of uncertainty are thoughtful design choices."}, "weaknesses": {"value": "- The method introduces several hyperparameters (e.g., α, m, λs, λp, λb, η). While a grid search was conducted, their generalizability and robustness across different datasets or models are not fully verified, potentially affecting usability.\n\n- The approach primarily focuses on binary spatial relationships between two objects. Its applicability to more complex spatial layouts or scenes with multiple objects is not deeply explored, limiting its scope.\n\n- The experiments are conducted exclusively on U-Net-based architectures (Stable Diffusion v1.4/v2.1). There is no validation on newer, transformer-based diffusion architectures like SD3 or Flux, leaving the method's effectiveness and generalizability across different model architectures unproven."}, "questions": {"value": "- How would the InfSplign loss function be extended for text prompts containing more than two objects requiring precise spatial layout? Is the current triplet formulation 〈A,R,B〉 sufficient for handling more complex spatial descriptions?\n\n- The paper mentions performance limitations with uncommon object combinations. Beyond object omission, does InfSplign potentially sacrifice image plausibility or aesthetic quality to satisfy spatial constraints in such cases? Are there any related observations or metrics?\n\n- The guidance weight η is set to a large, fixed value (1000). What was the rationale for this specific choice? Is the performance sensitive to this parameter across different prompts or random seeds? Is there evidence of robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jZ7ZEKlS4G", "forum": "pojbQAEvJQ", "replyto": "pojbQAEvJQ", "signatures": ["ICLR.cc/2026/Conference/Submission13932/Reviewer_8KSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13932/Reviewer_8KSx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194464022, "cdate": 1762194464022, "tmdate": 1762924435238, "mdate": 1762924435238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}