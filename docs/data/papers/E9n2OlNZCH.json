{"id": "E9n2OlNZCH", "number": 2458, "cdate": 1757095273767, "mdate": 1762939055970, "content": {"title": "Verifying Out-of-Distribution Robustness in Multi-spectral Satellite Change Detection", "abstract": "Reliable multi-spectral change detection on-board satellites requires robustness under distribution shifts. We address this challenge from both the certification and empirical perspectives.\n\nOn the certification side, we adapt neural verification to the unique structure of change detection, accounting for sensor noise, encoder–decoder heads, and semantic evaluation. We introduce a tail-tapped verifier that transports input intervals to the final decoder tap and applies $\\alpha$-CROWN solely to the decision head. This yields per-pixel logit-margin lower bounds, which we summarize through task-aligned predicates such as coverage, false positives, and minimum island size.\n\nOn the empirical side, we study out-of-distribution robustness across three representative backbones — U-Net style encoder–decoder (FresUNet), lightweight convolutional attention encoder–decoder (FALCONet), and transformer-inspired global attention encoder–decoder (AttU-Net) — on the Onera Satellite Change Detection (OSCD) dataset. We find that existing certificates vanish even for mild perturbations ($\\varepsilon \\ge 1/255$), while empirical robustness varies widely across architectures.\n\nOur results highlight both the difficulty of certifying change detection and the promise of architecture design for achieving practical robustness. This establishes a foundation for principled verification and stress-tested deployment of satellite-based change detection models", "tldr": "We show why certified robustness collapses in satellite change detection and introduce a simple head-consistency training plus a diagnostic verifier and CropRot benchmark to achieve practical OOD stability.", "keywords": ["change detection", "certified robustness", "out-of-distribution generalization", "multi-spectral remote sensing", "encoder–decoder architectures"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/044fc0eec92bd964b85d365d6d2d7ed361447d86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of ensuring out-of-distribution (OOD) robustness for multi-spectral satellite change detection models, which is critical for their reliable deployment on-board satellites. The authors introduce two main contributions: Head-Consistency Training (HCT), a lightweight training objective that enforces stability at the model's decision head against physically-grounded perturbations like shadows and sensor drift, and a tail-tapped diagnostic verifier that applies $\\alpha$-CROWN verification only to the final head to analyze why robustness fails. The work also contributes an OOD evaluation protocol: four Sentinel-2–motivated synthetic families plus CropRot, a curated vegetation-change benchmark built by ∆NDVI thresholding, both share OSCD’s 13-band interface while stressing different change phenomena. Experiments across three encoder–decoder backbones show that strict GT-aware certificates vanish at ε ≥ 1/255, even as HCT lifts clean OSCD performance and improves OOD Dice on CropRot. Diagnostics attribute certificate collapse to rapidly widening encoder intervals despite tight head spans."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a highly relevant and challenging problem of ensuring the robustness of multi-spectral change detection models. This is a critical need for real-world applications, especially for on-board satellite systems that must operate reliably in the face of test-time distribution shifts.\n2. It introduces CropRot, a new, curated OOD benchmark specifically designed to test for vegetation-driven changes by using NDVI differencing with a visual quality assurance step . This dataset serves as an excellent semantic-shift benchmark to complement the urban-focused OSCD dataset, stressing models on different types of temporal dynamics.\n3. The paper introduces Head-Consistency Training (HCT), a novel and practical method for improving empirical robustness. Unlike generic noise, HCT is physically grounded, enforcing stability against a set of four \"sensor-calibrated\" perturbations (low-frequency drift, shadowing, passband shift, and blur) that are motivated by real-world satellite imagery artifacts. HCT improves the empirical OOD robustness of the FALCONet model on the CropRot benchmark on the OSCD dataset."}, "weaknesses": {"value": "1. Most results rely on OSCD (small, urban-change focus) and a curated CropRot set; the latter is explicitly geographically narrow and suggested for expansion across regions and seasons. The protocol is useful but not a strong OOD benchmark by itself.\n2. The paper presents two main technical contributions: the tail-tapped verifier and the Head-Consistency Training (HCT) method. However, these two contributions are entirely disconnected. The paper explicitly states that the empirical gains from HCT do not translate into certified robustness and that HCT does not make models certifiably robust. The paper essentially presents a verification method that fails to certify a model trained with HCT.\n3. The empirical results for HCT, while positive, are not overwhelmingly strong. The primary result is that HCT lifts a weaker backbone (FALCONet) to be more competitive. On the CropRot OOD benchmark, the baseline AttU-Net achieves a Dice score of 0.35. The proposed FALCONet with HCT only achieves 0.22. This suggests that architecture design (AttU-Net) is far more critical for OOD robustness than the proposed HCT training method.\n4. There are minor concerns in section 3.2. For instance, the authors do not justify why $\\tau$ was chosen from the range [0.05, 0.15]. Additionally, there is no ablation study for the $λ_{CE}$, $λ_{HCT}$, $λ_{Dice}$ values, and it is unclear what the $\\sigma()$ function represents in the Dice loss."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "By0CjfCC07", "forum": "E9n2OlNZCH", "replyto": "E9n2OlNZCH", "signatures": ["ICLR.cc/2026/Conference/Submission2458/Reviewer_J9Uo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2458/Reviewer_J9Uo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738537586, "cdate": 1761738537586, "tmdate": 1762916244728, "mdate": 1762916244728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of out-of-distribution robustness in multi-spectral satellite change detection. It introduces a margin-based training strategy using physically motivated perturbations and a verifier that applies certified robustness analysis only to the model’s decision head. Experiments across several architectures demonstrate the difficulty of maintaining certification under realistic perturbations and identify the encoder–decoder body as the main source of instability. While the proposed training improves empirical robustness and transfer to new domains, these gains do not extend to certified robustness, which remains unverified for the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Restricting alpha-CROWN verification to the head-only, tail-tapped computation is a clever and efficient design that offers valuable diagnostic insight into certification limits for satellite change detection models.\n\n2. The paper tackles an important and timely problem, evaluating out-of-distribution robustness in change detection, and introduces two meaningful evaluation setups: the CropRot benchmark and physically motivated perturbations of OSCD.\n\n3. The work is transparent in reporting negative results, such as the rapid collapse of certificates under small perturbations, and provides thoughtful diagnostic analysis to explain these outcomes rather than emphasizing benchmark gains alone.\n\n4. The domain-specific design of both the CropRot dataset and the physically grounded perturbations enhances the paper’s practical relevance for satellite imagery applications, going beyond generic data augmentation techniques."}, "weaknesses": {"value": "1. The architectural observations in Section 4.2.1 are insufficiently supported by experiments and are drawn from only three diverse architectures, limiting the strength of the conclusions.\n\n2. The empirical value of HCT is weakly demonstrated: improvements are shown only for FALCONet, with limited comparisons against baselines and no ablation of perturbation types, standard augmentations, or adversarial training methods. Some non-HCT models even outperform FALCONet + HCT.\n\n3. The tail-tapped verifier is not compared against any other verification approaches, leaving its impact on bound tightness relative to full end-to-end alpha-CROWN or other verifiers unclear.\n\n4. The relationship between HCT and tail-tapped verification is not well established. The two methods appear conceptually independent, with little evidence that they complement or reinforce one another. Moreover, while the verifier provides useful diagnostic insights, these are not applied to analyze HCT-trained models.\n\n5. The CropRot dataset relies on weakly supervised rather than manually curated labels, and the paper does not sufficiently justify or assess the reliability of these annotations."}, "questions": {"value": "1. Clarify how HCT and tail-tapped verification are conceptually connected. If they are intended as complementary components of a unified robustness framework, emphasize this integration; otherwise, justify why they are presented together.\n2. Include HCT-trained models in the diagnostic analyses to evaluate their certification behavior.\n3. Evaluate HCT on additional architectures such as FresUNet and AttU-Net to assess generality across model types.\n4. Clarify the rationale for using the perturbed OSCD benchmark given that all models collapse under these perturbations, and identify which perturbation types remain informative or realistic.\n5. Report whether any architectural remedies identified through diagnostic analysis were implemented and tested experimentally."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7hnVk8bB21", "forum": "E9n2OlNZCH", "replyto": "E9n2OlNZCH", "signatures": ["ICLR.cc/2026/Conference/Submission2458/Reviewer_KERw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2458/Reviewer_KERw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970753187, "cdate": 1761970753187, "tmdate": 1762916244571, "mdate": 1762916244571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank the reviewers for their constructive feedback and valuable suggestions. After careful consideration, we have decided to withdraw this submission and plan to resubmit a revised version to a venue more focused on formal verification."}}, "id": "4vfhETDuJQ", "forum": "E9n2OlNZCH", "replyto": "E9n2OlNZCH", "signatures": ["ICLR.cc/2026/Conference/Submission2458/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2458/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762936405430, "cdate": 1762936405430, "tmdate": 1762936405430, "mdate": 1762936405430, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to explore the question of how robust are trained change detection models for deployment on-board of satellites and proposes a methodology to study this and potentially improve the robustness. Their approach ultimately adds a loss that punishes if the model predictions deviate too much from the ground truth label given some perturbations. These perturbations are further somewhat inspired by the domain in quesiton, remote sensing data on-board of satellites - by using a change detection dataset made of Sentinel-2 images.\nThe authors of the paper plan to release their code and also a small dataset made in this paper to further examine the out-of-distribution robustness of trained models. The created dataset aims to target plant growth changes as they occur over seasons.\nThere are couple of good points about this paper, but also quite severe weaknesses - I will go into more details below."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The concept of this paper is relatively solid: indeed for machine learning models trained on-ground, there is a rigorous process needed before they can be deployed on-board of satellites. However, I would say, that there are quite some problems in framing the paper in this domain, without actually citing almost anything from either Remote Sensing literature, or the more specialised AI on-board (of satellites) literature.\n\nSecondly, it is appreciated, that the authors are planning to release their code and the data they collected."}, "weaknesses": {"value": "Jargon\nIt needs to be noted, that the paper starts with quite heavy jargon and doesn't define or explain the used terms very well. Also, it doesn't start with a general description of the methods and topics before going into the details and names of methods. There are several points where this could be done, either in the introduction, or in the background literature. (For comparison, for example one of the already cited papers \"Certified Adversarial Robustness via Randomized Smoothing\" does this better).\n\nLanguage\nThis point is somewhat connected to the heavy use of jargon, but goes beyond. The language used in this paper is somewhat strange and doesn't really match terminology used in machine learning literature. Some terms are left without definitions and it is upon the reader to try to figure out what these mean. (An incomplete list of confusing terms is attached bellow) Maybe the use of LLMs was to the detrement of this paper (if it's the LLMs that introduce this terminology).\n\nDataset\nThe paper presents a new dataset made from pairs of Sentinel-2 images and mostly automatically created change labels based on NDVI. This on its own might not be very reliable - the paper describes these limitations, but only in the appendix - it would be suggested to keep this discusion about these limitations inside the paper. Details about the dataset are also missing - there is no number of scenes, their extend, their geographical distribution around the world (typically in papers that present datasets, there would be a map showing this distribution). The dataset seems to be quite small, similarly to the OSCD dataset, which also has just a few labelled scenes. For these reasons, its difficult to estimate how well models generalise on these datasets altogether. Similarly, models trained on these likely won't be robust to any variance of data (here presented OOD perturbations, or even scenes from different location).\n\nDomains of Remote Sensing and AI On-board Satellites\nDespite using problems from these two domains as the backdrop of the paper, there is a large lack of almost any literature being cited from these two disciplines (and their intesection) (except some of the work by R.C Daudt and few other classical papers). It could be argued that this would be beyond the paper, but there needs to be a more details and knowledge cited from these disciplines. For example, the used perturbations were seemingly inspired from Sentinel-2 data - no citation is provided to any works that would encounter these. Also no references to how the multispectral data tends to be pre-processed before using by ML models. There are works that propose either use of ML models directly on-board of satellites, and explicitely address the problems of deployment with apriori unknown characteristics of the later used sensors (see \"Towards global flood mapping onboard low cost satellites with machine learning\" and \"In-orbit demonstration of a re-trainable machine learning payload for processing optical imagery\" both by Gonzalo Mateo-Garcia et. al.). There are also works that address change detection on-board of satellites and were later deployed in real space missions (\"RaVÆn: unsupervised change detection of extreme events using ML on-board satellites\" by Vit Ruzicka et al). In general, the task of change detection doesn't have to be solved by encoder-decoder networks, in fact the mentioned papers use unsupervised learning approaches that may be better for dealing with sensor degradation and OOD data at the time of model deployment.\nWhy this matters, is because this paper is in it's first sentence specifying that it focuses on multispectral change detection on-board of satellites, while it doesn't really go into the details that would arise in these situations. One example is that getting correctly co-registered data on-board might be very difficult (read slow to compute, hard to store on-board in limited storage etc.) - for example for that reason, a sensible perturbation might be misalignment of input tiles.\n\n\nResults\nThe formatting of the results in the table 1 could be improved - list of models without the proposed HCT technique is around the one model that uses it. Figure 3 doesn't have labels for classes (and also the shown predictions look like the models have not learned much, if I understand it correctly, the FALCONet is predicting \"change\" everywhere - this seems like a training collapse). The terms used in the diagnostics table (1a) also seem quite confusing to me. Finally, some section in the results aren't formatted as full text paragraphs, instead they are bullet points, this makes it hard to read (especially when the points are not really expanded on).\n\nConfusing terms (mentioned in the point about language):\n- \"end-to-end relaxations quickly become vacuous\"\n- \"not adversarial and not verifier-coupled\"\n- tap bounds\n- head relaxations\n- shallow-body verification\n- \"inside the graph\"\n- predicate outcomes\n- Head logit spans / Head logit span / head spans remain tight\n- Tap widths explode / Tap width_mean\n- keep tap boxes tight and head sensitivity\n- architecturally narrow the tap interval\n- AttU-Net \"transfers\" best (maybe generalises would be better?)\n- bottleneck lies in the body\n- The pipeline emits .csv manifests \n\n\n-----\nSmaller details:\n\nAPPENDIX\nThe paper sends the reader to the appendix quite a lot, it would be better to provide more explanations in the main text of the paper instead.\n\nABBREVS\nSome abbreviations were not expanded properly:\nchange detection (CD)\nvisual quality assurance (VQA)"}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LTMVlEmRSw", "forum": "E9n2OlNZCH", "replyto": "E9n2OlNZCH", "signatures": ["ICLR.cc/2026/Conference/Submission2458/Reviewer_LSTW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2458/Reviewer_LSTW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231473815, "cdate": 1762231473815, "tmdate": 1762916244393, "mdate": 1762916244393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates robustness of multi-spectral satellite change detection on both certification and empirical perspectives."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Satellite change detection seems an interesting application."}, "weaknesses": {"value": "1. unclear academic novelty\n2. unclear writing that is really hard to follow: many acronyms without explanations, missing references, lack of description on satellite change detection."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2t8FpLEhl3", "forum": "E9n2OlNZCH", "replyto": "E9n2OlNZCH", "signatures": ["ICLR.cc/2026/Conference/Submission2458/Reviewer_MuUM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2458/Reviewer_MuUM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762282087727, "cdate": 1762282087727, "tmdate": 1762916244233, "mdate": 1762916244233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}