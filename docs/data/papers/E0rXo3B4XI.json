{"id": "E0rXo3B4XI", "number": 15849, "cdate": 1758256070852, "mdate": 1759897277850, "content": {"title": "From Single to Dual Reference: Reinforcement-Aligned Multi-Image Instruction-Guided Editing", "abstract": "Most instruction-guided image editing models assume a single reference image. However, many real-world tasks—such as combining people into a group portrait, integrating a subject into a scene, or transferring clothing between individuals—require reasoning across multiple inputs. Current approaches either fail outright or rely on ad-hoc heuristics to merge results. \nIn this work, we present the systematic study of dual-image instruction-guided editing. To support this, we construct a synthesized dataset of dual-image instructions spanning five representative categories: animal–scene composition, person–scene insertion, group portraits, style transfer, and clothing replacement. Building on the open-source single-image reference editing model, we introduce a dual positional embedding scheme with LoRA fine-tuning that enables efficient multi-reference fusion without catastrophic forgetting. Furthermore, we apply reinforcement alignment with Diffusion Denoising Policy Optimization (DDPO), using vision language model as a reward model to better align generations with editing instructions. Despite being trained on relatively small-scale data, our method achieves strong qualitative and quantitative improvements, surpassing existing open source baselines in multi-reference editing.", "tldr": "We extend instruction-guided editing from single- to dual-reference with a two-stage framework: finetuning for multi-image fusion and RL alignment for instruction fidelity, enabling composition, insertion, and transfer beyond open-source baselines.", "keywords": ["Benchamark; Image Editing; Diffusion Model; Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7525b27102b906449cd196276d0d4805e200ea6c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a dual-image instruction-guided editing, addressing a significant gap in current open-source image editing models, which primarily focus on single-image references. To facilitate this, a novel synthetic dataset comprising five representative dual-image editing tasks (e.g., animal-scene composition, group portraits) is constructed. The proposed method extends the Flux Kontext-dev model by incorporating a dual positional embedding scheme with LoRA fine-tuning, enabling efficient multi-reference fusion without catastrophic forgetting of single-image capabilities. Furthermore, reinforcement alignment utilizing Diffusion Denoising Policy Optimization (DDPO) with a Vision-Language Model (VLM) (GPT-5) as a reward model is employed to enhance instruction-following and consistency. The approach demonstrates qualitative and quantitative experiments with baselines to assess its performance against existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work proposes an architectural extension for an existing single-image editor, utilizing a dual positional embedding scheme and LoRA fine-tuning for multi-reference fusion without compromising prior capabilities. Additionally, reinforcement alignment with a VLM reward model is integrated to enhance instruction following. The reported results indicate performance surpassing several open-source baselines and achieving comparable or superior outcomes to certain closed-source models in specific tasks."}, "weaknesses": {"value": "1. Lack of Novelty in the Proposed Method. The proposed method lacks novelty, as it simply applies LoRA fine-tuning to the Flux model using DDPO in a rather naive manner. The approach does not present any substantial originality or methodological innovation beyond existing techniques.\n\n2. Sole reliance on VLM-based evaluation is insufficient for a comprehensive assessment. The absence of objective metrics, such as CLIP score or CLIP directional similarity, limits a robust \n\n3. The qualitative results also fail to demonstrate superior performance compared to other models. As shown in Figure 2, Gemini-2.5 and GPT-4o preserve the originality of the images more effectively while better incorporating the provided references than the proposed method. This raises concerns about the robustness and reliability of the proposed approach.\n\n4. A demo link provided in the abstract of this paper reveals author-identifying information, specifically an account associated with the authors’ names. This raises a serious concern of violating ICLR’s double-blind review policy, as the information could potentially compromise the authors’ anonymity."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qZNzbde5Vd", "forum": "E0rXo3B4XI", "replyto": "E0rXo3B4XI", "signatures": ["ICLR.cc/2026/Conference/Submission15849/Reviewer_rxTc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15849/Reviewer_rxTc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638448131, "cdate": 1761638448131, "tmdate": 1762926072186, "mdate": 1762926072186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper: From Single to Dual Reference: Reinforcement-Aligned Multi-Image Instruction-Guided Editing.\nThe paper tackles dual-image instruction-guided editing (e.g., person–scene insertion, clothing transfer). It (i) builds a synthetic, task-balanced benchmark of five categories and 100 test pairs each; (ii) extends a single-reference editor (Flux Kontext-dev) with a dual positional-embedding fusion and LoRA fine-tuning to avoid catastrophic forgetting (model diagram on p.4, Fig. 2); and (iii) applies DDPO with a binary VLM judge (three yes/no questions for instruction fidelity, dual-reference consistency, and realism) to align generations (p.5, Fig. 3). Quantitatively, the method outperforms open baselines (UNO, Flux concat) and approaches or surpasses Gemini on several dual-image tasks; RL adds ~3% average reward over supervised fine-tuning (p.7, Table 2; p.8, Fig. 5). Single-image editing ability is preserved (p.7, Table 1; p.10, Fig. A.1)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Problem focus & gap: Dual-image instruction editing is practical yet underexplored; the paper formulates clear tasks and success criteria (p.2–3). \nArchitectural clarity: The dual positional-embedding fusion (diagonal concatenation + cross-attn) is compatible with pretrained weights and handles aspect-ratio mismatch (p.4, Fig. 2). \nPractical alignment: Binary-question VLM rewards reduce prompt drift and stabilize DDPO; the 3-question design is sensible (p.5–6). \nEmpirical signal: Gains over open baselines across all five tasks on two evaluators; RL adds ~3% reward on average (p.7–8, Table 2 & Fig. 5). \nNo regression on single-image: Single-ref performance is retained after dual-ref finetuning (p.7, Table 1; p.10, Fig. A.1)."}, "weaknesses": {"value": "Evaluator dependence / generality. The main metric is VLM-based (GPT-5, SeedVLM). No human study or reference-free classical metrics (e.g., FID-like realism proxies, identity/text checks) are reported; this risks evaluator bias. \nSynthetic benchmark only. The testbed is entirely synthetic (500 total samples). Lack of real-image or hard OOD cases (e.g., clutter, strong occlusion, lighting mismatch) limits external validity. \nAblation depth. Beyond showing RL vs. no-RL, there is little analysis of the fusion design (e.g., alternatives to diagonal concat, gating choices), LoRA ranks/targets, or reward shaping (weights per question). \nIdentity preservation quantification. The qualitative grid (p.6, Fig. 4) notes weaker identity retention than GPT-4o; no ID metric (face embedding similarity, retrieval) is reported for the human categories. \nReproducibility details. Training uses 8×H100 with certain step counts; more explicit compute budgets, wall-clock, and exact prompts/thresholds for the judge would help."}, "questions": {"value": "Human evaluation: Can you add a small human study (e.g., 100 cases, majority vote) to calibrate VLM scores and report agreement vs. GPT-5/SeedVLM? \nFusion ablations: How does performance change with alternative fusions (concat-then-attn; interleaved tokens; learned offset pe) and different LoRA ranks/targets? A table analogous to Table 2 would strengthen causality. \nReward shaping: You average three binary questions (fid/cons/real). Did you try weighted rewards or min-operator (pessimistic) to better penalize failures? Any reward-hacking symptoms? \nIdentity metrics: For person/group tasks, can you report ID-similarity (ArcFace/FaceNet) and pose/landmark consistency to substantiate claims in Fig. 4? \nBeyond two references: Is the fusion scalable to >2 references (e.g., three-person composites)? Any preliminary scaling results or constraints? \nReal-world/OOD: Can you include a small real-image slice (non-synthetic) and a hard-case subset (strong occlusion, lighting shifts) to test robustness? \nAdditional Feedback (constructive)\n\nAdd Pareto plots (instruction fidelity vs. reference consistency; realism as contours) to visualize trade-offs across models/ablations.\nReport CIs/SEs for all means in Table 2; include per-category difficulty analysis and typical failure taxonomies.\nRelease judge prompts and decision heuristics (box sizes, thresholds) in an appendix table; provide a seed list for rollouts.\nConsider a compact open-stack evaluator (e.g., GLIP/OWL-ViT + open VLM) for community replication alongside GPT-5/SeedVLM numbers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper uses synthetic data and focuses on evaluation/training methodology. If any human-subject images are used in future releases, include license/consent and identity-preservation caveats."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pDBZkR1mQe", "forum": "E0rXo3B4XI", "replyto": "E0rXo3B4XI", "signatures": ["ICLR.cc/2026/Conference/Submission15849/Reviewer_HxYW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15849/Reviewer_HxYW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654870149, "cdate": 1761654870149, "tmdate": 1762926071808, "mdate": 1762926071808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically investigates dual-image instruction-guided image editing, which is a task that has not been sufficiently explored. Most existing models only support single-image inputs, making them inadequate for real-world scenarios that require fusing semantic information from two reference images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed dual-image editing benchmark fills a critical gap in the field by providing the first standardized dataset for multi-reference image editing.\n2. The experiments are thorough, evaluating performance not only on the newly introduced benchmark but also verifying the preservation of single-image editing capabilities.\n3. The paper is clearly written, and the network architecture diagram effectively illustrates the model workflow."}, "weaknesses": {"value": "Major Weaknesses\n1. Limited data quality: All training and test data are synthesized by combining GPT-generated instructions with publicly available images. The resulting image pairs may lack physical plausibility (e.g., in lighting, scale, or occlusion), potentially causing the model to learn spurious correlations.\n2. Limited methodological novelty: Prior work, such as KOSMOS-G (arXiv:2310.02992), has already explored integrating multimodal large language models (MLLMs) with diffusion models to handle complex prompts involving multiple images and interleaved text, achieving similar functionality. Although the technical approaches differ, the core challenge—“instruction-guided multi-image editing”—is notably overlapping, which significantly diminishes the perceived novelty of this work.\n\nMinior Weaknesses\n1. Unverified evaluation capability: The paper uses GPT-5 as the reward model for DDPO, but it remains unclear whether GPT-5 actually has the demonstrated ability to judge visual fidelity, semantic alignment, or multi-image consistency in image editing tasks."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gmlBzvkjyK", "forum": "E0rXo3B4XI", "replyto": "E0rXo3B4XI", "signatures": ["ICLR.cc/2026/Conference/Submission15849/Reviewer_13cT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15849/Reviewer_13cT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920675977, "cdate": 1761920675977, "tmdate": 1762926071333, "mdate": 1762926071333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a dual-reference image editing framework that extends single-image instruction-guided editing models (e.g., Flux Kontext) to handle two input images. The authors design a synthetic dataset covering five dual-image tasks and propose a dual positional embedding mechanism with LoRA fine-tuning to enable efficient fusion. Additionally, reinforcement alignment using DDPO with a vision–language model (GPT-5) is applied to improve instruction adherence. Experiments show improvements over existing open-source baselines and competitive results compared to proprietary systems like Gemini."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents the first systematic benchmark and pipeline for dual-reference image editing, addressing a meaningful technical gap in the current literature.\n\n2. The use of DDPO for vision–language feedback is technically sound and shows potential to enhance model alignment with instructions."}, "weaknesses": {"value": "1. Unclear motivation： The motivation is not well articulated. From the examples shown, it remains unclear whether the proposed setting truly requires textual instructions; yet the title and framework emphasize instruction-guided editing. The paper should clarify the real necessity of instructions in dual-reference fusion tasks.\n\n2. Poor writing and layout quality: The manuscript appears rushed. Starting from Figure 2, several figures and captions are mismatched or misplaced, which severely affects readability. The paper needs substantial revision in both writing and formatting before it can meet publication standards."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "83jfK74ZcZ", "forum": "E0rXo3B4XI", "replyto": "E0rXo3B4XI", "signatures": ["ICLR.cc/2026/Conference/Submission15849/Reviewer_67Z7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15849/Reviewer_67Z7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996188498, "cdate": 1761996188498, "tmdate": 1762926070470, "mdate": 1762926070470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}