{"id": "NE6g1C90LT", "number": 17893, "cdate": 1758281677618, "mdate": 1763760175322, "content": {"title": "Domain-Aware Tensor Network Structure Search", "abstract": "Tensor networks (TNs) provide efficient representations of high-dimensional data, yet identification of the optimal TN structures, the so called tensor network structure search (TN-SS) problem, remains a challenge. Current state-of-the-art (SOTA) algorithms solve TN-SS as a purely numerical optimization problem and require extensive function evaluations, which is prohibitive for real-world applications. In addition, existing methods ignore the valuable domain information inherent in real-world tensor data and lack transparency in their identified TN structures. To this end, we propose a novel TN-SS framework, termed the tnLLM, which incorporates domain information about the data and harnesses the reasoning capabilities of large language models (LLMs) to $directly$ predict suitable TN structures. The proposed framework involves a domain-aware prompting pipeline which instructs the LLM to infer suitable TN structures based on the real-world relationships between tensor modes. In this way, our approach is capable of not only iteratively optimizing the objective function, but also generating domain-aware explanations for the identified structures. Experimental results demonstrate that tnLLM achieves comparable TN-SS objective function values with much fewer function evaluations compared to SOTA algorithms. Furthermore, we demonstrate that the LLM-enabled domain information can be used to find good initializations in the search space for sampling-based SOTA methods to accelerate their convergence while preserving theoretical performance guarantees. Our code is included in the supplementary materials.", "tldr": "", "keywords": ["Tensor Network Structure Search", "Domain-Aware", "Large Language Models"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5625dc80f631f5870985df6d3f3d47ee9a9ee959.pdf", "supplementary_material": "/attachment/8571aec1175d77c0ab011e627b472ffd76be8eb7.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes and analyzes LLM prompts to optimize the tensor network structure/rank problem. Informed with domain knowledge on the tensor dimensions, along with evaluations and history the LLM is able to propose network/structures that efficient compressions in a few iterations compared to non-domain informed searches. The model is applied to learn tensor structures for three  datasets of 3rd-order, 4th-order, and 5th order tensors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clear and interesting. Using an LLM to perform domain-informed optimization seems logical (replaces an expert users intuition and experience)."}, "weaknesses": {"value": "The main critique is the the limited amount of data. The two datasets (images and videos) are themselves very related. The other data is interesting but is rather low dimensional. Overall, for a method that should use domain knowledge to inform, a more diverse set of datasets from various domains should be tested to provide evidence to support the claims."}, "questions": {"value": "Have tensor network compression for physics data been used?\n\nGeospatial hyperspectral imaging across multiple time points (change detection) could provide rich data.  \n\nHow about biomedical data?\n\nHow about dynamic graph compression? Especially multiple graph layers or hyper-edges across time can be 3rd, 4th order tensors. \n\nNeuroimaging (MRI) with longitudinal subjects can be organized in space (3D), as well as multiple visits per subject and multiple subjects.\n\n Diffusion tensor imaging also has even higher-order?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YU7UPhevyk", "forum": "NE6g1C90LT", "replyto": "NE6g1C90LT", "signatures": ["ICLR.cc/2026/Conference/Submission17893/Reviewer_pkqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17893/Reviewer_pkqP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790773506, "cdate": 1761790773506, "tmdate": 1762927715199, "mdate": 1762927715199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the use of large language models (llm) to speed up the tensor network structure search (TN-SS). In particular it improves upon sampling based algorithms, by letting an llm predict a good initialization structure as well as picking a good candidate during the local neighbourhood search / sampling. In a sense the llm replaces a otherwise complex hardcoded heuristic in the optimization loop, automating human reasoning. In the experiments the paper demonstrates that this indeed successful over three different domains. However, I see some serious issues regarding related work and the fairness of the experiments as well as the general reproducability, as I will detail the weaknesses section."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea to let an llm guide the discrete optimization is interesting. While this is not the first work in this general direction, the proposed method for TN-SS is novel. While some may consider the mindest \"throwing an llm at the problem\" to be uninspiring, I believe it is important to thoroughly test und understand the limits of llms nonetheless. Replacing complex heuristics with concise prompts and llm reasoning has potential\n- The experiments show a quick convergence to good solutions with very few calls to the optimization objective on three different domains. This is important since calculating the objective function can be costly.  \n- Overall the paper is well written and structured\n- Tested the results with different models and show good end results across models."}, "weaknesses": {"value": "In its current form the paper has several weaknesses with regard to the related work and the experiments, which overall do not meet the standards for a publication at a top conference like ICLR. However, I believe most of them can be addressed and I will raise my score if this is done properly. I will go over them according to my priorities.\n\nMajor Weaknesses:\n1. The state of the art seems less clear than claimed in the paper. In particular, SVDinsTN claims to be much faster than TNLS and TnALE, while achieving similar compression quality and Guo et al. claim a speed and quality advantage over TnALE. However, these methods, even though they are cited, are neither properly discussed in the related work section nor are they included as baselines in the experiments. Since, SVDinsTN only seems to require a single pass over the data and still gives strong results in their evaluation it seems to be a perfect fit for initializing TnALE and TNLS. On the other hand, the program synthesis approach of Gua at el. improves upon the number of function evaluations among other things. Thus, for a fair state of the art comparison these two methods should be included in the experiments. It is also crucial to understand if data driven methods like SVDinsTN, might still provide the better initializations than the learned bias of llms. Which leads me to my next weakness.\n\n> Possible changes:\n> - Include all SOTA methods as baselines in the experiments. You could probably remove TNLS as the oldest method, if needed.\n> - If this is really not possible, remove the many claims about SOTA performance. And explain that there are different approaches to the problems you address already. You could refocus the paper to just evaluating if llms are useful for this task at all, and clearly show that this is indeed the case. As stated by ICLR you do not need to beat the SOTA to get accepted. \n\n\n2. The paper proposes a complete pipeline, but for me a major part of the contribution would be analyzing the two central parts, initialization and local optimization (TN discovery), independently. Right now it is entirely unclear how useful each part is on its own and thus if llms are actually useful in replacing local search heuristics *directly* in this setting. However, there are no samples for the results of the output of the llm during TN discovery and only final results after both steps. Providing results after the initialization and also showing the convergence behaviour over different runs would be essential for the paper. Additionally, this would allow a direct comparison with SVDinsTN for initialization.\n\n> Possible changes:\n> - Report the objective value of tnLLM after the initialization and ideally after each step in the appendix (since these are mostly less than 10, this should be easily doable). The data for this should be already there. Visaulizing this convergence behaviour of tnLLM in a plot would be really interesting, and should at least be included in the appendix. \n> - Provide examples of the llm output for the TN discovery parts\n> - To better judge the impact of the llm initialization and optimization loop independently, the experiment from Figure 7 could be repeated with less steps of tnLLM, ideally once with only the suggested initialization.\n> - Report results for SVDinsTN as comparison for llm initialization. I am aware that this might requiring significant extra work, but I believe its important to fairly assess the performance of llms vs. classic approaches. You can also do the comparison after the first optimization pass, since SVDinsTN also needs one.\n\n3. The comparisons in the experiment do not seem to be entirely fair. The baselines are initialized as \"fully-disconnected\" graph, while tnLLM starts with a complete graph, which seems to be much closer to the optimum. The llm, never seems to predict 1 which would remove an edge from the graph (it might have for time series dataset, since Table 6 misses several mode pairs). For a fairer comparison the baselines could also start with a complete graph. Finally, the graphs in Figure 7 show, that TNLS and especially TnALE are really close to the final objective function value early on and then seem to iterate for a long time without any visible improvement. This suggests to me that they could report convergence much earlier than they do right now leading to much fewer function evaluations.\n\n> Possible changes:\n> - Initialize baselines with a fully connected graph using low or medium sized ranks, could also be randomized or uniform.\n> - Clearify the convergence behaviour of the baselines, consider introducing a threshold that stops the iteration if the changes in the objective value are very small for several iterations. \n\n4. All algorithms in the paper are randomized, therefore each experiment should be repeated several (>=5) times and the results should include error bars in plots or report the median and mean average deviation in tables (or alternatively mean and std). However, this is currently only done for tnLLM and as it seems not even there in all experiments. Considering the longest run time is just over a day and most are only a few hours, this seems feasible until the author reviewer discussion period. Even if not, I would already be happy with some partial data for the faster cases and the full data until the final publication. Moreover, improving on 3. might help with reducing the time spent here.\n\n5. The paper claims to include code in the abstract, but the code in the provided supplemental material seems to be incomplete and comes without any documentation. It seems to me that the matlab files that would actually evaluate the objective functions are missing. All submitted artifacts should be up to the standard of ICLR, for me that means code with clearly defined dependencies and a documented straightforward setup. This would greatly increase the reproducibility and also usefulness of the submission to the machine learning community. Ideally, the code does not only include the proposed method, but also all code used to run the experiments.\n\n> For 4. and 5. the required changes should be clear and are necessary for a publication at a top conference from my perspective.\n\nLess important weaknesses:\n - The introduction mentions several use cases for tensor networks in general, but use cases for tensor network structure search specifically would be more relevant.\n- Using an llm for optimization *directly* has been studied in a ICLR 2024 paper: https://openreview.net/forum?id=Bb4VGOWELI, this seem like relevant related work to me. I initially thought this would be a novelty of this paper.\n - The paper should report the relative error and compression ratio along the objective function as it was done in previous works from the field, because they are much easier to understand by humans. \n- Some cited works in the TN-SS section are of little relevance. The paper from Meirom et al. is about contraction path optimization which has nothing to do with TN-SS. The work from Chen et al. is only about tensor rings and trains, thus the structure is mostly given already. Consider removing these citations, especially the first one.\n- The claim that the timeseries dataset \"is the largest dataset in terms of number of samples ever considered in the\nTN-SS problem\" is unecessary and hard to verify. Moreover, its still only about 1,2 MB large, so in terms of modern data and actual real world applications it is tiny and overall smaller than the video dataset. I would just remove the claim, it does not add anything to your work.\n\nSome minor things (irrelevant for the score):\n- The link to the data is broken, they probably changed something on their page: http://trace.eas.asu.edu/yuv/\n- The related work on TN-SS is basically repeated twice, once in the intro and then again in related work, I think it could be shortened in the intro\n- I am not convinced that the explanations by the llm are very useful for *domain experts*, however they can be useful for debugging just like any other runtime artifact. It seems similar to stepping through a hardcoded heuristic in a debugger."}, "questions": {"value": "- Why do you take the natural logarithm of the usual objective function in TN-SS in equation (1)\n- Why are so many mode pairs missing in table 6? According to your own prompt there should be 10, not 4\n- Algorithm 1:  Why you always check the whole history P for a better Graph and not only the last one, i.e. H? When does the Algorithm converge?\n- In Line 179-180 there is a single Tensor X, in equation (1) there are L tensors, why the switch?\n- Since you run tnLLM ten times in the hybrid algorithm, why is the objective still so bad at the beginning in Figure 7, how does that relate to the results in Table 1? There it seems 10 runs are almost always sufficient for achieving -0.41."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5tYrnNWVvX", "forum": "NE6g1C90LT", "replyto": "NE6g1C90LT", "signatures": ["ICLR.cc/2026/Conference/Submission17893/Reviewer_o65j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17893/Reviewer_o65j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870687965, "cdate": 1761870687965, "tmdate": 1762927714782, "mdate": 1762927714782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an LLM-guided tensor network (TN) topology search framework that exploits large language models' ability to parse high-level semantic relationships between mode and domain knowledge. The prompting follows CoT with three prompts—behavior directive, task directive and optimization directive—enable controllable exploration of the discrete TN configuration space.\nPerformance is quantified on held-out tensor families using the objective function involving compression ratio and structural complexity. The authors report  the competing performance over  three LLM-based baselines, with better convergence rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* By injecting tensor mode interdependence (e.g., shared physical indices in quantum many-body systems) into prompts, the LLM implicitly learns to suppress topologically invalid contractions, reducing the rate of malformed outputs.\n* The experimental results on three types of tensor data across different domains demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "* While a scalarized objective (Eq.(1) in the paper) is used, the paper omits raw compression ratio and relative reconstruction error across the Pareto frontier — the de facto standard in TN compression. Reporting only $\\mathcal{L}$ obscures trade-offs and hinders comparison with heuristics.\n* As shown in Table 2, performance degrade with weak LLMs and may suffer from hallucination issue of LLMs. No robustness analysis is provided."}, "questions": {"value": "* How does the context length affect search performance?\n* It would be good to compare the proposed method with non-LLM based methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "x4fVHl1W4D", "forum": "NE6g1C90LT", "replyto": "NE6g1C90LT", "signatures": ["ICLR.cc/2026/Conference/Submission17893/Reviewer_wwNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17893/Reviewer_wwNg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906004649, "cdate": 1761906004649, "tmdate": 1762927714223, "mdate": 1762927714223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the tensor network structure search (TN-SS) problem, which involves identifying optimal tensor network (TN) structures. Current state-of-the-art approaches typically treat TN-SS as a purely numerical optimization or search task, relying on sampling, Bayesian inference, spectral methods, or reinforcement learning, which leads to high computational costs and limited interpretability. To overcome these limitations, the authors propose tnLLM, a domain-aware large language model (LLM)-guided framework that incorporates domain knowledge about tensor modes through tailored prompting. tnLLM enables the LLM to generate promising initial TN structure configurations and iteratively refine them using significantly fewer objective evaluations. Experiments on real-world tensors of orders 3, 4, and 5 demonstrate that tnLLM achieves performance comparable to state-of-the-art sampling-based TN-SS methods while drastically reducing the number of evaluations required."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **S1**. The idea of leveraging LLMs to incorporate domain information into a fundamentally combinatorial search problem (TN-SS) is compelling, which brings a fresh angle to tensor network design.\n- **S2**. The experimental results demonstrate substantial reductions in the number of required evaluations (e.g., up to ~78× fewer than a baseline method) while maintaining comparable performance on the target objective.\n- **S3**. The ability to provide human-interpretable explanations for the resulting tensor network structures enhances transparency and trustworthiness.\n- **S4**. The overall methodology, including the prompt design and search workflow, is clearly presented, and the evaluation spans diverse domains (images, video, and time-series)."}, "weaknesses": {"value": "- **W1**. The core technical contribution mainly relies on prompt engineering with LLMs, which limits the methodological novelty and may not be considered a strong theoretical contribution.\n- **W2**. The framework lacks theoretical guarantees regarding when the LLM will propose reliable structures and avoid hallucinations, making its behavior difficult to predict.\n- **W3**. The experimental evaluation is restricted to relatively small-scale and limited domains (images, videos, and time-series), leaving the generalizability to more complex high-order tensors uncertain.\n- **W4**. The explanation capability, while interesting, is not quantitatively evaluated; the correctness and usefulness of the generated explanations remain unclear."}, "questions": {"value": "- **Q1**. The method relies heavily on high-quality domain information. If such information is difficult to obtain, does the advantage of using an LLM diminish significantly?\n- **Q2**. The approach requires carefully designed prompts. When extending to new tasks or domains, how should the prompts be adapted or improved to maintain performance?\n- **Q3**. Do the authors plan to introduce quantitative measures (e.g., fidelity metrics) to evaluate the accuracy and usefulness of the model-generated explanations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vQkk1tuNRn", "forum": "NE6g1C90LT", "replyto": "NE6g1C90LT", "signatures": ["ICLR.cc/2026/Conference/Submission17893/Reviewer_fA79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17893/Reviewer_fA79"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968688582, "cdate": 1761968688582, "tmdate": 1762927713716, "mdate": 1762927713716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}