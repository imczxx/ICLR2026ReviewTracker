{"id": "tuK01yGJbt", "number": 14441, "cdate": 1758235474047, "mdate": 1759897369877, "content": {"title": "Escaping Plato’s Cave: JAM for Aligning Independently Trained Vision and Language Models", "abstract": "Independently trained vision and language models inhabit disjoint representational spaces, shaped by their respective modalities, learning objectives, and architectures. The Platonic Representation Hypothesis (PRH) suggests these models may nonetheless converge toward a shared statistical model of reality. This raises a fundamental question: can we move beyond post-hoc detection of such alignment and explicitly optimize for it? We argue this challenge is particularly important for tasks such as fine-grained contextual distinctions—where multiple descriptions share global semantics but differ in subtle compositional details. We tackle this setting with the Joint Autoencoder Modulator (JAM), which aligns frozen unimodal models by jointly training modality-specific autoencoders with coordinated reconstruction and cross-modal alignment objectives. We systematically evaluate JAM across three design axes: (i) alignment objectives, introducing our multimodal Spread Loss that outperforms classic contrastive methods; (ii) the layer depth at which alignment is most effective; and (iii) the role of foundation model scale in representational convergence. Our findings show that JAM reliably induces alignment (outperforming innately multimodal models and post-hoc alignment baselines with absolute error reduction of up to 10\\%, and relative error reduction of up to 80\\%), offering both fundamental insight into the structure of shared semantics and practical guidance for transforming generalist unimodal foundations into specialist multimodal models.", "tldr": "We show Platonic alignment: JAM post-hoc aligns frozen vision and language models through lightweight autoencoders, bridging disjoint modalities and capturing fine-grained contextual distinctions to reveal shared structure.", "keywords": ["Representation Learning", "Representation Alignment", "Multimodal Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efa226af4d4d4cbb8966436875904caf453d7c19.pdf", "supplementary_material": "/attachment/130c7d12f6e72b3e1836e367dbd56bf63fa978f5.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents the Joint Autoencoder Modulator model that learns to align frozen representations from different modalities, e.g. language, speech, and vision. The model is trained using a combination of loss functions that aim to align images with text (Con), push hard negatives samples away from each other (NegCon), and an objective that simultaneous learns to align paired examples while pushing award the hard negative samples (Spread Loss). The experiments use the Winoground and Sugarcrepe datasets to finetune the representations of a collections of vision-only and language-only models. The results show that finetuning the frozen representations of these models enables them to outperform printed CLIP models on the Sugarcrepe and Winoground tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The overall goal is clearly explained and the language and vision backbones are representative of state of the art models. The ablation experiments in Table 3 show the contribution of the different components of the overall framework."}, "weaknesses": {"value": "W1: The rhetorical argument of the paper is centered around the Platonic Representation Hypothesis, but the experiments in the paper are about learning projections from pre-extracted unimodal representations into a shared embedding space. This doesn't make sense to me as a means to \"escaping the Platonic cave\" because you are directly optimizing the projection.\n\nW2: The models are fine-tuned and evaluated on the same datasets (SugarCrepe and Winoground). There is no experimental evidence that this improves overall alignment of the projected representations, or if the JAM representations are overly-specialized to these datasets.\n\nW3: It is not clear what the authors mean by compositionality throughout the paper. For example, L53 in the introduction reads \"contextual sensitivity to subtle compositional cues\". Is this compositonality in the visual space or in the linguistic space?"}, "questions": {"value": "Q1: L206: \"In addition to the VAE reconstruction objective\". This was not clear to me because nothing in the preceding text suggested that JAM was a variational autoencoder, and this was not clear from Figure 3. Please explain clearly how you sample from a random variable and how this is integrated into JAM.\n\nQ2: How does your SpreadLoss model perform on other datasets like ImageInWords (Garg et al. EMNLP 2024) or DOCCI (Onoe et al. ECCV 2024)?\n\nQ3: Why does performance decrease for the SugarCrepe Replace and Swap tasks when finetuning CLIP? Did you apply the same early stopping criteria as described in L306-307?\n\nQ4: What type of compositonality are you trying to address in this paper? Are you working from a more general formal definition, e.g. \"The meanings for complex expressions are derived from the meanings of their parts via specific composition functions.\" (Partee 1984)? There is some prior work on compositionality in multimodality that can contribute to this theoretical perspective (Nikolaus et al. 2019, Surís et al. 2020)\n\n* Barbara Partee. 1984. Compositionality. Varieties of formal semantics, 3:281–311\n\n* Surís, Dídac, et al. ECCV 2020. Learning to learn words from visual scenes.\n\n* Nikolaus, Mitja, et al. CoNLL 2019. Compositional Generalization in Image Captioning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1FkMm49cD4", "forum": "tuK01yGJbt", "replyto": "tuK01yGJbt", "signatures": ["ICLR.cc/2026/Conference/Submission14441/Reviewer_vJRR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14441/Reviewer_vJRR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761161930169, "cdate": 1761161930169, "tmdate": 1762924848385, "mdate": 1762924848385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests that common alignment metrics (e.g., CKA/CCA/SVCCA) fail to distinguish hard non-matches—caption-image pairs that differ only in fine details (color, relation, spatial role). To address this, the authors insert a lightweight Joint Autoencoder Modulator (JAM) on the embeddings of frozen pretrained vision and language backbones. They introduce Spread Loss, which (i) first push semantically similar items (true caption + hard negatives) away from unrelated items, then (ii) distinguish within that local set (true caption + hard negatives) by recognizing only the true caption as positive. Experiments on Sugarcrepe (Replace/Add/Swap) and Winoground show consistent gains over contrastive and NegCon baselines, as well as pretrained CLIP. Ablations vary the α trade-off across layers, analyze the role of reconstruction, and provide a scaling study (larger LLM/backbones)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Experiment and performance.** Consistent improvements on Sugarcrepe/Winoground; ablation experiments are helpful and adequate (α across layers; with/without reconstruction; projection-only vs JAM).\n- **Efficiency.** Frozen backbones + small JAM adapters seem easy to train and low data/compute friendly. Post-hoc alignment avoids catastrophic forgetting often seen in small-data finetuning of large multi-modal models.\n- **Objective is well motivated.** The fine-grained alignment is indeed a problem, and the nicely designed loss clearly targets the hard non-match case."}, "weaknesses": {"value": "- **The paper is narrative (the “Plato’s Cave” framing) & asymmetry of supervision.**\nThe paper motivates “escaping coarse alignment” but operationally does so by making text-side supervision more fine-grained inside a context set (true + hard-negative captions). Because there are no image-side hard non-matches, the objective is fundamentally asymmetric (stronger supervision in image→text; standard in text→image), which undermines the claim of truly bidirectional, multimodal fine-grained alignment. In effect, the method sharpens caption discrimination around a fixed image manifold rather than learning symmetric cross-modal structure. This is also maybe reflected by the LLM scaling result: if stronger text priors in larger LLMs plus fine-grained text supervision already saturate the local decision, then making the LLM larger predictably shows little gain.\n- **Need a bit stronger or more explanation on baselines and evaluation.** It’s unclear how CLIP was finetuned (training receipe). Please gives clear training protocol for comparison. A stronger, fair setup should include LoRA/adapter-tuned CLIP under the same data/compute/time budget, and a projection-only + reconstruction control with matched parameter count. Results mainly reports Recall@1 in 2-way/5-way setups on SugarCrepe/Winoground. That’s a very narrow slice of fine-grained ability. A convincing case should add large-scale retrieval, and at least one open-ended task (short captioning or discriminative VQA) using JAM as reranker.\n- **Related work coverage is not enough, the work needs to survey a lot more of the old CLIP variant work.**\nThe related work section is surprisingly sparse (e.g., highlights BLIP-2) and under-discusses a large body of CLIP-style alignment improvements—token/region-word fine-grained methods (e.g., token-level matching, FG-CLIP), hard-negative mining for image–text retrieval (e.g., VSE++-style), synethtic data TripletCLIP, hybrid contrastive/self-supervised regularizers, and some of the recent works. Positioning against these would clarify what is genuinely new. As is, novelty reads more compositional than fundamental."}, "questions": {"value": "- What are the default hyperparameters used across all experiments? Do you fix LR and training strategy for all experiments?\n- What is the LR tuning effort for JAM?\n- Is the input resolution identical for CLIP, DINOv2, SigLIP, ResNet-50, etc.?\n- For “projection-only vs JAM”, are parameter counts matched?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dhvQfcOh04", "forum": "tuK01yGJbt", "replyto": "tuK01yGJbt", "signatures": ["ICLR.cc/2026/Conference/Submission14441/Reviewer_QP9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14441/Reviewer_QP9e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632688117, "cdate": 1761632688117, "tmdate": 1762924847809, "mdate": 1762924847809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formalizes “Platonic Alignment” by showing that observational metrics (CCA/CKA/SVCCA/CKNNA) detect only coarse cross‑modal similarity and fail on fine‑grained mismatches. It proposes JAM, a lightweight post‑hoc alignment module that attaches modality‑specific autoencoders to frozen vision and language encoders, uses reconstruction to preserve modality‑specific information, and enforces alignment in a shared bottleneck. The core loss, multimodal Spread Loss, blends a context‑level contrastive term (ConCon) that pulls positives and hard negatives of the same context together with an instance‑level InfoNCE term (ContextNCE) that separates within‑context fine details, balanced by α. On SugarCrepe and Winoground, JAM with Spread Loss outperforms contrastive baselines and several CLIP variants while being compute‑efficient; ablations show the reconstruction term regularizes against collapse, a layer‑wise curriculum reveals earlier layers need stronger fine‑grained supervision (larger α), and scaling backbone size yields limited gains under low‑data fine‑grained settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*   The paper effectively identifies and tackles the challenge of moving beyond coarse-grained observational alignment to explicitly optimize for fine-grained cross-modal distinctions. This is achieved while deliberately keeping robust unimodal foundations frozen, which is a significant practical consideration.\n\n*   Coherent and Well-Derived Multimodal Spread Loss: The formulation of the multimodal Spread Loss is a key technical strength. It is logically derived from the paper's core objectives: preserving contextual coherence via the Context-level Contrastive term (ConCon) while simultaneously preventing representational collapse within a context using the Instance-level InfoNCE-style term (ContextNCE)."}, "weaknesses": {"value": "1. Evaluation scope and external validity. The empirical focus is almost entirely on controlled, fine-grained **image-to-text retrieval** with small candidate sets (binary and 5-way), primarily using datasets like SugarCrepe and Winoground. While this setting is appropriate for stress-testing subtle distinctions in compositional grounding and representation alignment, it remains unclear how well the induced alignment transfers to broader retrieval catalogs, including robust **text-to-image retrieval**, or to generative and more complex multimodal tasks such as **multi-choice VQA**. The current evidence could be strengthened by at least one evaluation that stresses open-set retrieval or generalization to tasks.\n\n2. The paper's emphasis on tackling \"fine-grained contextual distinctions\" and leveraging \"hard non-match\" samples, particularly through the multimodal Spread Loss, while effective for achieving superior performance on datasets like SugarCrepe, inherently relies on meticulously curated and annotated data with subtle compositional cues. This dependency poses significant challenges for the transferability of the methodology to typically weakly supervised datasets for contrastive training, as the acquisition or generation of such high-quality, fine-grained negative samples can be prohibitively expensive and complex. \n\n3. The JAM framework is designed for post-hoc alignment of independently trained, frozen unimodal models. This design fundamentally diverges from integrated multimodal models like LLaVA or Qwen-VL, where vision encoders are already deeply coupled with large language models through specific bridging architectures and joint training. Consequently, JAM cannot be directly applied to these prevalent, pre-aligned multimodal systems to further enhance their alignment, which may limit its practical utility and impact on improving real-world vision-language applications."}, "questions": {"value": "- On ablations of JAM's capacity: How sensitive is performance to the autoencoder's latent size, depth, and the presence of residual MLP blocks? A capacity‑controlled sweep would disentangle the contribution of Spread Loss from representational bottleneck effects.\n\n- At test time, are decoders discarded and only encoders used to produce the aligned latent?\n\n- On observational metrics post‑alignment: The paper shows that observational metrics fail to distinguish hard negatives before post-alignment. Do these metrics become more discriminative after JAM training in the shared bottleneck? Reporting pre/post scores would help reconcile observational tests with functional alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "isFr01UTji", "forum": "tuK01yGJbt", "replyto": "tuK01yGJbt", "signatures": ["ICLR.cc/2026/Conference/Submission14441/Reviewer_zH8e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14441/Reviewer_zH8e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907041736, "cdate": 1761907041736, "tmdate": 1762924847350, "mdate": 1762924847350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of aligning independently‐trained vision and language models, which—despite being optimized on distinct modalities, datasets, and objectives—may nonetheless encode a “shared” semantic structure (the so‐called Platonic Representation Hypothesis, or PRH). The authors show that standard observational alignment metrics (e.g., CKA, CCA) detect coarse‐level similarity (e.g., matching vs unrelated pairs) but fail to capture fine‐grained contextual distinctions (such as attributive or compositional differences between captions). To address this, they propose the Joint Autoencoder Modulator (JAM): a framework that takes embeddings from frozen unimodal vision and language backbones, passes each through its own autoencoder (which reconstructs the embedding) and simultaneously aligns them in a shared bottleneck latent space using both reconstruction and cross‐modal alignment objectives. A novel “Spread Loss” is introduced to better handle “hard negatives” (captions that share global semantics but differ in fine‐grained detail) by pulling together semantically similar context groups while pushing apart fine‐distinctions within each group."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper formalizes an intriguing philosophical and empirical perspective — that independently trained unimodal models may converge toward a shared “statistical model of reality.” By connecting representation learning to Plato’s Allegory of the Cave, the authors provide a compelling lens for studying cross-modal alignment. Instead of merely measuring similarity between models (as done in prior work), the paper introduces the idea of actively inducing alignment between disjoint modalities, bridging diagnostic and optimization perspectives.\n2. The proposed architecture is elegantly simple yet effective — it preserves modality-specific detail via reconstruction while encouraging cross-modal semantic coherence through a shared bottleneck. This modularity makes it broadly applicable to diverse pretrained backbones. The Spread Loss innovatively handles hard negatives by balancing context-level cohesion and intra-context differentiation, addressing a key limitation of contrastive learning that treats all negatives equally. This allows finer-grained visio-linguistic distinctions to emerge in the shared latent space. By systematically examining layer depth, loss design, and model scale, the authors provide unusually thorough empirical insight into where and how alignment emerges, beyond merely reporting performance metrics.\n3. JAM with Spread Loss achieves significant improvements over post-hoc alignment and even multimodal baselines (up to 10% absolute / 80% relative error reduction), validating that explicit optimization can surpass innate multimodal training in specialized settings. The framework is tested across multiple independently trained language and vision models (e.g., Gemma2, LLaMA3, OLMo2, DINOv2, ResNet, ViT), demonstrating generality and robustness across architectures and scales. Comparisons against contrastive and NegCon losses, as well as ablations without reconstruction, strengthen causal claims about the contributions of JAM and Spread Loss.\n4. The work connects theoretical notions of representational convergence with practical architectures for multimodal learning, offering design principles for future multimodal systems built from unimodal foundations. Because JAM is modular and lightweight, it doubles as a diagnostic tool for probing where alignment fails — providing scientific & engineering value."}, "weaknesses": {"value": "1. Although the paper demonstrates strong results on fine-grained benchmarks like SugarCrepe and Winoground, these datasets are relatively narrow and small-scale. It remains unclear whether JAM’s advantages would persist in larger, more diverse multimodal datasets (e.g., COCO Captions, LAION) or in open-world tasks such as caption generation and visual question answering.\nThe framework is limited to aligning vision and language models. Its scalability or adaptability to additional modalities (e.g., audio, video, 3D data) — or to tri-modal alignment — is not explored.\n2. While the PRH provides an appealing philosophical motivation, the paper does not formalize it mathematically or demonstrate empirical necessity for it beyond observational correlation. The connection between PRH and the success of JAM remains largely conceptual rather than mechanistic. The use of small, symmetric autoencoders might limit representational richness. It is unclear whether improvements stem from the specific architecture or simply from added nonlinear projection capacity over frozen backbones.\nruction and alignment objectives: Although reconstruction is said to preserve modality-specific information, the paper does not deeply analyze the trade-off or potential conflicts between reconstructive fidelity and cross-modal alignment quality.\n3. The baselines are primarily CLIP-style contrastive methods. More modern multimodal fusion approaches (e.g., BLIP-2, Kosmos-2, LLaVA variants) could have provided a stronger benchmark for assessing alignment quality. The study focuses on retrieval-style metrics, but does not test whether the induced alignment improves generative or reasoning capabilities in downstream multimodal tasks.\nWhile JAM aims to reveal shared semantics, the paper does not visualize or analyze what the shared latent space captures (e.g., through clustering, probing, or feature attribution).\n4. Although the autoencoder framework is lightweight, the paper lacks a discussion of computational efficiency, stability, and scalability to higher-dimensional or larger foundation models. Spread Loss introduces additional hyperparameters (e.g., α, temperature τ), yet the paper provides little analysis of their effect on alignment behavior or robustness across datasets."}, "questions": {"value": "1. The Platonic Representation Hypothesis is a compelling framing, but how do you envision empirically verifying or falsifying it beyond representational correlation? Could JAM serve as a testbed for evaluating the degree of shared “reality modeling” between unimodal systems?\n2. How central is the PRH to the success of JAM? In other words, do you think JAM works because PRH approximately holds, or could it induce alignment even if the underlying representations were unrelated?\n3. Does the “Platonic alignment” notion suggest there’s a single latent structure shared across all modalities—or might different modality pairs converge to distinct but overlapping subspaces?\n4. Why did you choose autoencoders rather than simpler projection heads (e.g., linear adapters) or more expressive cross-attention mechanisms for alignment?\n5. How sensitive is JAM to the bottleneck dimension or architecture depth? Did you observe cases where excessive compression hurt fine-grained distinctions?\n6. The reconstruction loss is weighted by a decaying factor λ(t). Did you explore alternative schedules, such as cyclical or adaptive weighting, and how might those influence the balance between modality fidelity and alignment?\n7. The Spread Loss depends on a hyperparameter α balancing context-level cohesion and intra-context contrast. How robust are results to α, and is there an intuitive way to tune it?\n8. How do you select or generate hard negatives in practice, and do you believe their quality (or semantic closeness) is a key factor in Spread Loss’s success?\n9. JAM achieves substantial performance gains on SugarCrepe and Winoground. Have you tested it on larger-scale or open-domain datasets (e.g., COCO, LAION) to assess generalization?\n10. How does JAM perform when aligning heterogeneous model scales (e.g., a small vision encoder with a large LLM)?\n11. Have you examined downstream multimodal reasoning tasks (e.g., VQA or caption generation) to test whether improved fine-grained alignment translates into better functional performance?\n12. Could you elaborate on why fine-tuning CLIP tended to overfit? Did you control for differences in data scale or parameter count relative to JAM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tun7EAZo0H", "forum": "tuK01yGJbt", "replyto": "tuK01yGJbt", "signatures": ["ICLR.cc/2026/Conference/Submission14441/Reviewer_jgkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14441/Reviewer_jgkz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961507347, "cdate": 1761961507347, "tmdate": 1762924846514, "mdate": 1762924846514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of aligning independently‐trained vision and language models, which—despite being optimized on distinct modalities, datasets, and objectives—may nonetheless encode a “shared” semantic structure (the so‐called Platonic Representation Hypothesis, or PRH). The authors show that standard observational alignment metrics (e.g., CKA, CCA) detect coarse‐level similarity (e.g., matching vs unrelated pairs) but fail to capture fine‐grained contextual distinctions (such as attributive or compositional differences between captions). To address this, they propose the Joint Autoencoder Modulator (JAM): a framework that takes embeddings from frozen unimodal vision and language backbones, passes each through its own autoencoder (which reconstructs the embedding) and simultaneously aligns them in a shared bottleneck latent space using both reconstruction and cross‐modal alignment objectives. A novel “Spread Loss” is introduced to better handle “hard negatives” (captions that share global semantics but differ in fine‐grained detail) by pulling together semantically similar context groups while pushing apart fine‐distinctions within each group."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper formalizes an intriguing philosophical and empirical perspective — that independently trained unimodal models may converge toward a shared “statistical model of reality.” By connecting representation learning to Plato’s Allegory of the Cave, the authors provide a compelling lens for studying cross-modal alignment. Instead of merely measuring similarity between models (as done in prior work), the paper introduces the idea of actively inducing alignment between disjoint modalities, bridging diagnostic and optimization perspectives.\n2. The proposed architecture is elegantly simple yet effective — it preserves modality-specific detail via reconstruction while encouraging cross-modal semantic coherence through a shared bottleneck. This modularity makes it broadly applicable to diverse pretrained backbones. The Spread Loss innovatively handles hard negatives by balancing context-level cohesion and intra-context differentiation, addressing a key limitation of contrastive learning that treats all negatives equally. This allows finer-grained visio-linguistic distinctions to emerge in the shared latent space. By systematically examining layer depth, loss design, and model scale, the authors provide unusually thorough empirical insight into where and how alignment emerges, beyond merely reporting performance metrics.\n3. JAM with Spread Loss achieves significant improvements over post-hoc alignment and even multimodal baselines (up to 10% absolute / 80% relative error reduction), validating that explicit optimization can surpass innate multimodal training in specialized settings. The framework is tested across multiple independently trained language and vision models (e.g., Gemma2, LLaMA3, OLMo2, DINOv2, ResNet, ViT), demonstrating generality and robustness across architectures and scales. Comparisons against contrastive and NegCon losses, as well as ablations without reconstruction, strengthen causal claims about the contributions of JAM and Spread Loss.\n4. The work connects theoretical notions of representational convergence with practical architectures for multimodal learning, offering design principles for future multimodal systems built from unimodal foundations. Because JAM is modular and lightweight, it doubles as a diagnostic tool for probing where alignment fails — providing scientific & engineering value."}, "weaknesses": {"value": "1. Although the paper demonstrates strong results on fine-grained benchmarks like SugarCrepe and Winoground, these datasets are relatively narrow and small-scale. It remains unclear whether JAM’s advantages would persist in larger, more diverse multimodal datasets (e.g., COCO Captions, LAION) or in open-world tasks such as caption generation and visual question answering.\nThe framework is limited to aligning vision and language models. Its scalability or adaptability to additional modalities (e.g., audio, video, 3D data) — or to tri-modal alignment — is not explored.\n2. While the PRH provides an appealing philosophical motivation, the paper does not formalize it mathematically or demonstrate empirical necessity for it beyond observational correlation. The connection between PRH and the success of JAM remains largely conceptual rather than mechanistic. The use of small, symmetric autoencoders might limit representational richness. It is unclear whether improvements stem from the specific architecture or simply from added nonlinear projection capacity over frozen backbones.\nruction and alignment objectives: Although reconstruction is said to preserve modality-specific information, the paper does not deeply analyze the trade-off or potential conflicts between reconstructive fidelity and cross-modal alignment quality.\n3. The baselines are primarily CLIP-style contrastive methods. More modern multimodal fusion approaches (e.g., BLIP-2, Kosmos-2, LLaVA variants) could have provided a stronger benchmark for assessing alignment quality. The study focuses on retrieval-style metrics, but does not test whether the induced alignment improves generative or reasoning capabilities in downstream multimodal tasks.\nWhile JAM aims to reveal shared semantics, the paper does not visualize or analyze what the shared latent space captures (e.g., through clustering, probing, or feature attribution).\n4. Although the autoencoder framework is lightweight, the paper lacks a discussion of computational efficiency, stability, and scalability to higher-dimensional or larger foundation models. Spread Loss introduces additional hyperparameters (e.g., α, temperature τ), yet the paper provides little analysis of their effect on alignment behavior or robustness across datasets."}, "questions": {"value": "1. The Platonic Representation Hypothesis is a compelling framing, but how do you envision empirically verifying or falsifying it beyond representational correlation? Could JAM serve as a testbed for evaluating the degree of shared “reality modeling” between unimodal systems?\n2. How central is the PRH to the success of JAM? In other words, do you think JAM works because PRH approximately holds, or could it induce alignment even if the underlying representations were unrelated?\n3. Does the “Platonic alignment” notion suggest there’s a single latent structure shared across all modalities—or might different modality pairs converge to distinct but overlapping subspaces?\n4. The Spread Loss depends on a hyperparameter α balancing context-level cohesion and intra-context contrast. How robust are results to α, and is there an intuitive way to tune it"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tun7EAZo0H", "forum": "tuK01yGJbt", "replyto": "tuK01yGJbt", "signatures": ["ICLR.cc/2026/Conference/Submission14441/Reviewer_jgkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14441/Reviewer_jgkz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961507347, "cdate": 1761961507347, "tmdate": 1763071427533, "mdate": 1763071427533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces JAM, a joint autoencoding loss term to align independently trained unimodal models without multimodal pretraining. There are two ideas at play: 1) the alignment of pre-trained, frozen, unimodal encoders to obtain a multimodal model; 2) the focus on fine-grained semantic details (i.e., ensuring slight syntactical variations that impact the semantics of captions are properly reflected at the latent level). The main experiments are conducted on the SugarCrepe and Winoground dataset, showing promising results and interesting ablations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Interesting problem for the representation alignment community.** It fits quite nicely in the model re-use and representation alignment literature and the layer-depth curriculum adds interpretability value.  \n- **Quality.** The method is overall well specified, with clear ablations on the loss, curriculum, and reconstruction terms. Results are consistent across several language and vision backbones on the selected fine-grained benchmarks.  \n- **Clarity.** The writing and figures are overall clear and informative. Only minor notational inconsistencies remain.  \n- **Significance.** The strong performance on fine-grained retrieval with frozen unimodal foundations suggests a practical and resource-efficient direction for multimodal alignment."}, "weaknesses": {"value": "- **Missing related work baselines**   (Introduction, opening of the second paragraph; Related Work §1.2 “testing for alignment”).\n  The paper omits closely related approaches that align **frozen unimodal encoders** or **stitch** independently trained models and should be treated as baselines/positioning references (e.g.,[a, b, c, d, e]).\n  These are directly relevant to the setting discussed in §1 and §1.2 and their absence weakens the comparison and framing.\n\n- **Notation inconsistencies**   (Section 2, third paragraph).\n  Symbols are introduced or used inconsistently: **B** (batch) is defined, **n** is left undefined, and **C** appears as if previously defined when it wasn’t. This likely stems from an editing truncation and should be clarified to make the derivations more readable.\n\n- **Terminology slip on VAEs**  (Section 3.1, “Loss functions”).\n  The text says “In addition to the **VAE** reconstruction objective…”, but the rest of the paper consistently uses “autoencoder” and no KL/divergence term is described. Please clarify whether the learned modules are actual VAEs; if not, replace “VAE reconstruction” with “autoencoder reconstruction” for consistency. If yes, a reference to Kingma & Welling's work is missing.\n\n- **Evaluation scope**   (Table 2).\n  Table 2 clearly demonstrates **fine-grained semantic** improvements on the considered datasets. However, it does not show whether **general-purpose alignment** is preserved. This could be addressed either with **modality-specific control tasks** (stronger) or by reporting performance on a **standard CLIP-style task** over a general-purpose dataset (e.g., a **subset of LAION**, as a weaker but indicative check).\n\n- **Motivation without post-alignment confirmation**   (Figure 2).\n  Figure 2 compellingly motivates the need for fine-grained semantics. Since it underpins the method’s rationale, the same plots/metrics reported **after** JAM alignment would make the case conclusive.\n\n- **Scaling confounders**   (Section 4.3: performance vs. model size).\n  The analysis mixes **different text-encoder families**, introducing confounding factors (architecture, tokenizer, pretraining differences). A **within-family scaling** comparison would isolate the model-size effect more cleanly.\n\n---\n\n   [a] Norelli et al., **ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training** (NeurIPS 2023).  \n\n   [b] Maiorca et al., *Latent Space Translation via Semantic Alignment* (NeurIPS 2023).  \n\n  [c]  Li et al., *CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features* (ICLR 2025).  \n\n   [d] Gröger et al., *With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You* (ICML 2025).  \n\n   [e] Moschella et al., *Relative representations enable zero-shot latent space communication* (ICLR 2023)."}, "questions": {"value": "1. Are the encoders true VAEs (with a KL term) or deterministic autoencoders with MSE reconstruction? Please clarify or adjust terminology.  \n2. Could you report the same alignment metrics used in Figure 2 after JAM training to visualize the improvement?  \n3. Do the JAM-aligned representations retain general-purpose alignment performance, for example on standard retrieval or classification tasks?  \n4. Could you include or discuss other post-hoc alignment baselines such as those that align frozen unimodal encoders or stitch pretrained models?  \n5. Please standardize notation in Section 2, ensuring all symbols are properly introduced and used consistently.  \n6. Would you consider running a within-family scaling study to better isolate the model size effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zN9XKaPgKB", "forum": "tuK01yGJbt", "replyto": "tuK01yGJbt", "signatures": ["ICLR.cc/2026/Conference/Submission14441/Reviewer_f9Rp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14441/Reviewer_f9Rp"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997604712, "cdate": 1761997604712, "tmdate": 1762924846038, "mdate": 1762924846038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}