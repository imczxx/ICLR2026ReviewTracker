{"id": "DRqmwamsD8", "number": 5849, "cdate": 1757939937673, "mdate": 1759897949889, "content": {"title": "WHY TEACHER–STUDENT SELF-SUPERVISED LEARNING WORKS: A MUTUAL INFORMATION PERSPECTIVE", "abstract": "We study teacher-student (TS) self-supervised learning methods (e.g., BYOL, SimSiam), which learn strong representations without relying on negative samples but currently lack a clear information-theoretic explanation. Building on the InfoMax perspective that unifies many multi-view SSL families, we show that TS-SSL implicitly maximizes a variational lower bound on the mutual information \\(I(Z_\\theta; X)\\) between inputs and the teacher representations \\(Z_\\theta\\). Concretely, we prove that the predictor yields an implicit variational estimate of \\(I(Z_\\theta; (Z_\\phi, X))\\), the mutual information between teacher and student representations, and that the alternating optimization—student prediction (with stop-gradient) followed by teacher updates—implicitly maximizes a lower bound on \\(I(Z_\\theta; X)\\). Then, we derive convergence results characterizing the evolution of the teacher representation’s entropy and alignment during training. Eventually, motivated by these theoretical insights, we introduce a simple mutual-information–based regularizer on the student latent space that enforces monotonic growth of \\(I(Z_\\theta; X)\\) and yields consistent downstream improvements on both natural-image and medical-imaging benchmarks.", "tldr": "We show that teacher–student SSL implicitly maximizes mutual information, derive convergence results, and propose an MI-based regularizer that boosts BYOL and SimSiam performance on natural and medical imaging benchmarks.", "keywords": ["Representation Learning", "Non-Contrastive Learning", "Mutual Information", "Teacher-Student framework"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71925a64abe6c0bdb2c5faebc6a2105436d0c0d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new multi-view self-supervised learning (SSL) loss that is motivated by the InfoMax principle, aiming to maximise a lower bound on $I(X;Z)$.\n\nThe derived loss is empirically evaluated against self-implemented baselines (Barlow Twins, VicReg, SimCLR, Dino, Barlow Twins, Mocov2, SimSiam, BYOL) on natural image datasets (Cifar10, Cifar100, STL10, Imagenet100) as well as medical image datasets (BloodMNIST, PathMNIST, DermaMNIST, Camelyon16, BRACS) and compares mostly favourably to those."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The overview of related literature in introduction and background and related work is extensive \n\n- It is great to see SSL methods tried out on medical imaging benchmarks"}, "weaknesses": {"value": "- The core equations of the method (Eqns 18 and 19) seem to have mistakes in them: in Eq 18 the entropy term $H(Z_{\\phi})$ should be subtracted not added I believe, in Eq 19 the last term should be subtracted not added, and also the $\\beta$ from Equation 18 is nowhere to be found in Eq 19. Lastly, why would the term that I think is meant to estimate $H(Z_{\\phi})$ in Eq 19 (i.e. this last term) involve $f_{\\theta}$?\n\n- The natural image experiments are not conducted on full ImageNet, which has been the standard in the literature (BYOL, SimCLR, …). This raises doubts about the scalability of the proposed approach and also raises questions about fair comparison, since no independent source can be compared against for accuracy (e.g. baselines could be suboptimally trained by authors - not even on purpose. As someones who has trained these beasts before I know how hyperparameter sensitive they can be)\n\n- Without the last term in Eq 18, the suggested modification to BYOL would look almost identical to what is proposed and investigated with an entropy regulariser in Rodriguez-Galvez et al. (2023). This warrants an ablation of its effect to better understand novelty and significance of the contribution, but it cannot be found in the paper."}, "questions": {"value": "- Contribution 1. - what is ‘thus bringing to an estimator of […].’ supposed to mean? Bringing what?\n- Eq 2, 3 and 4: how is H(p || q) to be understood? I don’t know the notation in this case, I only know || for divergence notations but not for entropy. \n- Eq 2: how is $q$ defined here?\n- Eq 9: how is H(A|B;C) to be understood? I would understand H(A|B) as the conditional entropy of A given B, but with a third variable separated by a semicolon I am not familiar with the notation. \n- Assumption 1: how can this assumption be justified or otherwise verified in practice?\n- Figure 2: how is the MI measured here?\n- Where did $\\beta$ from Eq 18 go in Eq 19?\n- Line 478: where is this premature saturation shown in the paper?\n- Line 480: typo? Should it not be $I(Z_{\\phi};X)$ instead according to Eq 18 and 19?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OrJ1D2WKmJ", "forum": "DRqmwamsD8", "replyto": "DRqmwamsD8", "signatures": ["ICLR.cc/2026/Conference/Submission5849/Reviewer_Sd2r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5849/Reviewer_Sd2r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761326491828, "cdate": 1761326491828, "tmdate": 1762918303625, "mdate": 1762918303625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates teacher-student (TS) style self-supervised learning and aims to provide an information-theoretic explanation for its effectiveness. Initially, the authors analyze BYOL from an information-theoretic perspective and demonstrate that the student update implicitly maximizes a lower bound on the mutual information between the representation $Z_\\theta$ and the input X. Subsequently, under the assumption of a Gaussian isotropic latent space, they derive the incremental dynamics of the teacher's entropy and find that increasing the variance ratio can promote better alignment between the teacher and student. Based on this insight, the paper introduces an additional regularization term into BYOL's optimization objective. Experimental results show that this regularization improves performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents interesting research with adequate theoretical analysis."}, "weaknesses": {"value": "1. Insufficient discussion of related work: The paper claims several key contributions, including theoretical analyses of the predictor, stop-gradient mechanism, and EMA, explanations for the non-collapsing behavior of TS-SSL, and the introduction of mutual information-based constraints. However, similar arguments have been made in existing literature [1, 2, 3], and the paper fails to explicitly discuss distinctions from these prior works.\n\n\n2. Overstated title: TS-SSL encompasses diverse methods, such as the classic DINO. Yet, this paper only analyzes BYOL and SimSiam, which are insufficient to represent the broader TS-SSL framework, making the title overly broad.\n\n\n3. Limited experimental validity: Experiments are conducted on small-scale datasets, which lack representativeness in the current era. This raises concerns about the method’s effectiveness in real-world application scenarios."}, "questions": {"value": "In Line 344, the assumption $Z_{\\theta^{t+1}} = \\tau Z_{\\theta^t} + (1 - \\tau) Z_{\\phi^t}$ is introduced. Does the EMA update in the parameter space directly translate to the same update rule in the representation space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hlp75WxZ34", "forum": "DRqmwamsD8", "replyto": "DRqmwamsD8", "signatures": ["ICLR.cc/2026/Conference/Submission5849/Reviewer_zrcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5849/Reviewer_zrcp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812305329, "cdate": 1761812305329, "tmdate": 1762918303434, "mdate": 1762918303434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks to understand Teacher-Student SSL methods from an information theoretic perspective to justify their performance, analogously to the mutual information maximisation perspectives of other SSL methods, such as infoNCE."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The aim of understanding these SSL methods from an information theoretic perspective, interpreting the implicit distributional assumptions they make and making principled improvements is a sound approach.\n\nThe material improvement in results on a number of benchmark datasets suggest the proposed method is useful."}, "weaknesses": {"value": "The main weaknesses of the paper are readability and the mathematical arguments don't seem correct or well-presented and I believe need to be materially re-worked.\n\n* readability - the notation is hard to follow (SSL typically considers 2 related samples x, x' and their representation z, z', which is easier to follow than tracking subscripts).\n   - The paper overloads symbols and mixes random variables with their realizations. For instance, $Z_\\theta=f_\\theta(t(X))$ and $Z_\\phi=f_\\phi(t(X))$ have the same augmentation $t$, then later switch to multi‑view $v_i^{(1)}$, $v_i^{(2)}$ (Eq. (4)). The dependence structure (one view vs two independent views) must be fixed up front, e.g. with $t,t'\\stackrel{iid}{\\sim}\\mathcal{T}$. As written, it implies $Z_\\theta, Z_\\phi$ are deterministic functions of the same $t(X)$, which is not the BYOL/SimSiam setup and creates confusion for all MI statements. \n* The mathematical arguments are difficult to follow, e.g.\n   - it would help if the loss function being explained were stated upfront (is this RHS of Eq 5?).\n   - definition of conditional entropy (Eq 1) is already an expectation over X (e.g. see https://en.wikipedia.org/wiki/Conditional_entropy)\n   - \"alignment\" is typically considered between representations of related samples, Eq 1 doesn't consider that so hard to see how this relates, e.g. to Wang & Isola.\n   - Eqs 2/3 seems to be the Barber & Agakov bound, eg see \"On Variational Bounds of Mutual Information\" by Poole et \nal (as cited!). This is simply an instance of cross entropy lower bounding entropy (Eq 2), which is a fundamental of machine learning and far from a \"novelty\".\n   - the last term in Eq 2 should be in expectation (over X).\n   - rather than referring to \"kernel density estimation\" (vague/general), it should be stated in explicit terms if the first term of Eq 3 is equivalent to Eq 4 under specific Gaussian assumptions for p and q (assuming that is the case)? It is well known that the cross entropy of two Gaussians has the general form in Eq 4 (under specific assumptions that are not made clear here).\n   - Eq 4 mixes exact expectations and MC estimates (from samples). \n   - Eq 4 is a function of z under two different distributions (where z is a representation of a particular view of x), it is unclear how that then becomes a function of different z's/views.\n      - From the outset, it would be clearer to refer to x and x' as different views (or similar), as is common to avoid confusion.\n   - it is unclear what happens to the entropy term in Eq 3.\n   - in the context of the number of operations in a neural network, it is invalid to suggest that a multiplicative factor is set to 1 for \"computational efficiency\" (rather this is part of the p/q assumptions above)\n* the assumptions (including Eq 11) seem strong, unintuitive or not very well justified, particularly Assumption 2 and Eq 11.\n\nDetails\n* 268 - t already used to define augmentation\n* 269 - Z's are defined as deterministic functions of the same t(X) and therefore of each other? It is unclear why this relationship would be time invariant given that other functions are not.\n* 321 - this seems highly unintuitive for a relationship presumed to hold throughout training for a finite model.\n\nMinor\n* 177 - include the domain of v = t(x) (presumably $\\mathcal{X}$)\n* 192 - an unusual way of writing conditional cross entropy (double lines usually reserved for divergence)"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eIhN4BaaFi", "forum": "DRqmwamsD8", "replyto": "DRqmwamsD8", "signatures": ["ICLR.cc/2026/Conference/Submission5849/Reviewer_XwQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5849/Reviewer_XwQc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187202226, "cdate": 1762187202226, "tmdate": 1762918303209, "mdate": 1762918303209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study teacher-student self-supervised learning (TS-SSL) methods and focus on addressing the issue that these methods lack a clear information-theoretic explanation. They show that TS-SSL implicitly maximizes a lower bound on the mutual information between inputs and the teacher representations. They also give convergence results characterizing the evolution of the teacher representation’s entropy and alignment during training. By introducing a mutual-information–based regularizer on the student latent space, the authors give empirical results that show improvements on natural-image and medical-imaging benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors show theoretically that TS-SSL maximizes a lower bound on the mutual information between inputs and the teacher representations, which helps to design a mutual-information–based regularizer that leads to empirical improvements on real datasets. The results are interesting."}, "weaknesses": {"value": "This manuscript contains some technical weaknesses, such as referring to missing Sections, lacking of justification for important assumptions and inconsistent derivations, which I list in detail in the QUESTIONS part."}, "questions": {"value": "1. In Section 4.1 the authors refer to Sections A.5 and A.6 in the appendix. However, I cannot find Sections A.5 and A.6 in the manuscript.\n\n2. In Assumption 2, the function f is assumed to exhibit first-order variation (i.e., the derivative remains constant), which is claimed to hold for both the teacher and the student terms. This assumption is somewhat too strong. Please justify. Furthermore, the parameter variation is intertwined with the variation of the mutual information term (Eq. 12) in Lemma 1, which serves as a core of the analyses and impacts the validity of the conclusion.\n\n3. From Lemma 1 and Eq. 9 in Section 3.1, the authors analyze the variation of the teacher term given the student term and input indicating that this conditional entropy can be used to assess the lower bound of mutual information. However, the conditional entropy in Eq. 16 does not incorporate the student term as a condition. Instead it directly asserts that the entropy conditioned solely on the input should be reduced. This is somewhat inconsistent with the previous derivations.\n\n4. Regarding Eq. 17, the authors’ explanation of this equation can be insufficient. They do not clarify the rationale for introducing the entropy constraint in Eq. 17. In the preceding content, the authors illustrate that the improvement direction involves reducing the conditional entropy and increasing the marginal entropy, but they fail to explain why the conditional entropy constraint is specifically introduced here.\n\n5. Regarding Eq. 18, the authors previously mentioned the need to reduce one variance ratio and increase another. However, in Eq. 18, both regularization terms are assigned coefficients greater than zero, which is strange. \n\n6. Typos/small mistakes:\n\nIn Eq. (18), is the first Lagrangian multiplier term \\lambda H(Z_{\\phi})?\n\nIn Table 1, ROBYOL is actually lower than BYOL on IN100 (R50).\n\nIn Abstract, SSL is used without definition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CGwNJvq9z0", "forum": "DRqmwamsD8", "replyto": "DRqmwamsD8", "signatures": ["ICLR.cc/2026/Conference/Submission5849/Reviewer_Rsjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5849/Reviewer_Rsjj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762208518737, "cdate": 1762208518737, "tmdate": 1762918302906, "mdate": 1762918302906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}