{"id": "KS7vlU94eU", "number": 10515, "cdate": 1758174398200, "mdate": 1759897646200, "content": {"title": "SAGE-CoT: Self-Adaptive Generated Chain-of-Thought for Jailbreaking", "abstract": "Chain-of-thought (CoT) reasoning has strengthened the problem-solving ability of large reasoning models (LRMs), improving both interpretability and safety alignment. Yet this transparency introduces new attack surfaces: Recent jailbreak methods exploit CoT traces to elicit unsafe behaviors. Existing approaches, however, are limited by their reliance on observable CoT traces during attack construction or on manual prompt engineering. Moreover, many proprietary LRMs do not expose CoT traces to external users, making traditional CoT-based attacks difficult or even infeasible in realistic black-box scenarios.\nWe propose \\textbf{SAGE-CoT} (Self-Adaptive Generated Chain-of-Thought for Jailbreaking), a black-box framework that leverages an LRM's own meta-level reasoning to autonomously generate CoT templates capable of decoding hidden malicious instructions. SAGE-CoT consists of two key stages: (i) \\textit{CoT Template Generation}, where a meta-instruction guides the LRM to elaborate a simple intent recovery template into a bespoke reasoning template tailored for malicious intent decoding, and (ii) \\textit{Intent Obfuscation}, where the malicious instruction is disguised through semantic obfuscation, indexed word permutation, and noise injection. This design ensures that malicious intent is neither directly exposed in the input nor easily filtered during reasoning, allowing the attack to bypass both internal safety mechanisms and external defenses. Extensive experiments across six state-of-the-art jailbreak baselines and diverse LRMs demonstrate the effectiveness of SAGE-CoT. On GPT-o3-mini, it achieves a 90\\% attack success rate, and on Gemini-2.5-Pro-Thinking, it reaches 96\\%. We further show that SAGE-CoT maintains high effectiveness under advanced safety defenses. All code and datasets will be publicly released to ensure reproducibility. \\textcolor{red}{(Warning: this paper contains potentially harmful content generated by LRMs.)}", "tldr": "SAGE-CoT is a black-box jailbreak framework that exploits meta-level reasoning to circumvent the safeguards of Large Reasoning Models (LRMs) —requiring no access to internal CoT traces or manual prompt engineering.", "keywords": ["Large Language Model", "Automated Jailbreaking", "Chain-of-Thought", "Self-Induced Reasoning Paths", "Trust and Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8d02ea729f79dc915834524f1d7e8de461f842d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents SAGE-CoT, a CoT-based jailbreak method for LRMs. It uses three chained stages: first neutralizing the initial content but keeping the original intent; then split the text into a dictionary-based payload with randomly added noise (improving stealthiness); finally a prompt-guided intent-recovery template is used to guide the jailbreaks. The claim is that contemporary LLMs reconstruct the hidden intent during reasoning and produce harmful outputs, yielding high attack success rates (ASR) across models, with some evaluation comparisons to prior prompt-based or CoT-style attacks and a few defenses as support."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper highlights an increasingly relevant threat surface for LLM safety researches: the model’s explicit reasoning process. Framing prompt obfuscation as a lightweight pre-processing pipeline is clear and easy to reproduce in principle. The work also gestures toward empirical comparisons and ablations, which—if robust—could help quantify how much each obfuscation step contributes to success. As a safety paper, the topic is timely and the writing is accessible."}, "weaknesses": {"value": "Despite the apparent simplicity, the evidence chain is not convincing in its current form. \n\nFirst, I was unable to reproduce the central claims on GPT-5 and Gemini-2.5-Pro using the appendix prompts and attack questions—3 runs each yielded no successful attacks—casting doubt on the reported cross-model effectiveness. If such attack is well-mitigated currently, it will not a big concern for LLM safety research now\n\nSecond, the core motivation hinges on “stealth”: obfuscation is said to evade alignment layers and external filters. Yet the proposed transformations introduce conspicuous artifacts (index markers, fragmented syntax, noise tokens). The paper does not demonstrate detectability or alert rates under intent classifiers, anomaly detectors, or rule-based filters, so the stealth claim remains largely asserted rather than measured. \n\nThird, the defense suite is shallow and partially outdated; beyond LlamaGuard3 there is little engagement with stronger SOTA defenses (e.g., JBShield, Legilimens, please check recent security papers in CCS, S&P, Security and NDSS), nor is there a mechanistic analysis explaining why specific defenses fail or succeed. \n\nFourth, I wonder why you use customized HS rather than StrongReject score as you already evaluated StrongReject benchmark.\n\nFinally, the evaluation is under-scoped: only subsets of AdvBench and StrongReject are used, with limited breakdowns by hazard type, little robustness analysis (temperature, top-p, context length, system prompt), and limited transfer across models."}, "questions": {"value": "1. How exactly the attack is performed?  Which way did you use to evaluate the attack, API or chat website? For thinking model like GPT-5 and Gemini, how is the reasoning effort/thinking budget set? How are the designed stages actually performed and chained together (with its model usage, prompt, etc.)? If a different model usage can fail the attack chain (e.g., I can choose to use GPT5 all the time to execute the whole pipeline) , what are the necessary key points needed for this specific step to work? A well-documented artifact would help addressing the effectiveness of this attack.\n2. What is the evaluation results on the whole AdvBench and StrongReject benchmarks?\n3. Evaluate against more SOTA defenses, in my view, an adapted JBShield/Legilimens defense may mitigate this kind of attack well.\n4. Try to explain how the internal LLM reasoning process fails its safety alignment and propose ways to mitigate it accordingly, either on the alignment side or on the guardrail side."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W9e0u6oyQW", "forum": "KS7vlU94eU", "replyto": "KS7vlU94eU", "signatures": ["ICLR.cc/2026/Conference/Submission10515/Reviewer_69Yr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10515/Reviewer_69Yr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760608392723, "cdate": 1760608392723, "tmdate": 1762921798513, "mdate": 1762921798513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new paradigm of jailbreak attacks that target the chain-of-thought (CoT) reasoning traces of large reasoning models (LRMs), without having access to these reasoning traces. The proposed approach uses the LRM to complexify a starting template mentioning some text encryption and decryption protocols. It then uses a set of strategies to obfuscate the malicious prompt before embedding it in the complexified template, resulting in the final malicious query. The approach shows remarkable effectiveness on contemporary LRMs such as o3 and Gemini-2.5-Pro-Thinking, and maintains resilience to standard defense strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is simple, intuitive and highly effective - using the LRM itself to generate a jailbreak prompt with minimal token overhead, and obtaining very high ASRs on several contemporary LRMs - outperforming baselines on standard datasets. The scope of evaluation is also quite large, covering nearly every model provider.\n- Detailed ablations confirm that each component of the attack pipeline is crucial for its performance, and the attack also proves more resilient to defense strategies than baselines, even without access to the model’s reasoning traces.\n- The work is quite timely and relevant with the increasing use of LRMs. The paper is also presented quite well."}, "weaknesses": {"value": "- While the attack is highly effective, it is a derivative of a long line of existing jailbreak attacks that all use LLMs to generate obfuscations of the input prompt, and compose various jailbreak attacks to increase their effectiveness [1][2][3][4] - somewhat limiting its originality. Some of these approaches would have also proven to be appropriate baselines for this work, given their similar nature.\n- The evaluation metric used in this work is nonstandard - using a combination of HS and KSS. Are there studies showing that this metric aligns with human preferences? Does SAGE-CoT still outperform other approaches when evaluated using more standard approaches such as HarmBench-Llama? Evaluation is also not performed on the entirety of AdvBench and StrongReject.\n\n[1] AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao.\n[2] h4rm3l: A language for Composable Jailbreak Attack Synthesis. Moussa Koulako Bala Doumbouya, Ananjan Nand, Gabriel Poesia, Davide Ghilardi, Anna Goldie, Federico Bianchi, Dan Jurafsky, Christopher D. Manning.\n[3] Jailbroken: How Does LLM Safety Training Fail? Alexander Wei, Nika Haghtalab, Jacob Steinhardt.\n[4] GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing."}, "questions": {"value": "- Why were the specific strategies of semantic obfuscation, indexed word permutation and noise injection chosen for the intent obfuscation layer? Were other strategies tried? Did the authors try allowing the LRM itself to perform the intent obfuscation?\n- Some details are missing regarding the generation of the self-complexified template. The paper mentions that this is done once for each target LRM. Does the LRM ever refuse to generate these harmful query strings? How many generations are created for each target LRM to get these templates, and how is the best template selected from these generations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IPOluxumVd", "forum": "KS7vlU94eU", "replyto": "KS7vlU94eU", "signatures": ["ICLR.cc/2026/Conference/Submission10515/Reviewer_zXiG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10515/Reviewer_zXiG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674537068, "cdate": 1761674537068, "tmdate": 1762921798122, "mdate": 1762921798122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAGE-CoT, a black-box jailbreak framework that induces a target Large Reasoning Model (LRM) to self-generate a bespoke CoT program and then feeds it a concealed malicious payload the template can decode. Stage 1 concatenates a simple intent-recovery template with a meta-instruction, prompting the target model itself to elaborate a model-specific reasoning template  that looks benign yet embeds decoding rules (persona, reporting schema, decryption steps) and is reused across attacks on that model.  In Stage 2, the attacker transforms a harmful query using semantic obfuscation via an auxiliary LLM. Experiments span proprietary and open-source LRMs, evaluated on AdvBench and StrongReject .  SAGE-CoT reports state-of-the-art ASR/HS across models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The two-stage design—self-generated CoT template + layered obfuscation, is well-specified with equations, example rules , and a precise assembly step.\n\n2. The method does not assume access to internal traces and is evaluated on closed models (GPT-5, GPT-o-series) and CoT-exposing models, demonstrating broad applicability under a uniform medium reasoning budget.\n\n3. Tables show consistent improvements over six baselines on both AdvBench and StrongReject, for instance, on StrongReject, SAGE-CoT reaches 92.59% ASR on Gemini-2.5-Pro-Thinking versus 75.93% for ICRT."}, "weaknesses": {"value": "1. HS is computed by GPT-4o and includes a 20-point bonus for absence of refusal keywords, which can reward verbosity and stylistic features embedded in (R_m) (e.g., mandated ≥600-word outputs and JSON schemas) rather than substantive harmfulness; this may advantage SAGE-CoT over baselines not designed to game HS components. The paper should compare against human raters and report inter-rater reliability to validate HS.\n\n2. H-CoT is adapted by using the final response as a proxy for CoT where traces are unavailable, which likely weakens it relative to its intended design; the paper does not quantify the performance drop from this adaptation or provide an alternative baseline that preserves H-CoT’s strengths under black-box constraints.\n\n3. Defenses are limited to LlamaGuard3, SmoothLLM, and DRO; modern multi-stage provider stacks (content classifiers + safety prompts + output filters + rate-limiters) and dynamic detectors are not evaluated. The evasion advantage versus LlamaGuard3 may not hold once multi-modal or server-side heuristics are added."}, "questions": {"value": "1. The paper asserts the harmful intent is never explicitly present … nor does it surface within the model’s reasoning process, yet shows long forensic-style reasoning and JSON that could itself cue the judge; no introspective or log-prob probes are provided to substantiate the claimed internal-only recovery mechanism. Stronger evidence (e.g., trace auditing on models that expose CoT, or external detectors monitoring intermediate tokens) is needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4uBz2hNIpA", "forum": "KS7vlU94eU", "replyto": "KS7vlU94eU", "signatures": ["ICLR.cc/2026/Conference/Submission10515/Reviewer_Tsuo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10515/Reviewer_Tsuo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795188382, "cdate": 1761795188382, "tmdate": 1762921797728, "mdate": 1762921797728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This work proposes a black-box jailbreak attack mechanism exploiting the Chain-of-Thought (COT) reasoning approach adopted by recent large language models.\n- This work claims to propose an attack approach that doesn't rely on the availability of \"internal COT traces\", which approach is claimed to be novel.\n- This work conducts empirical experiments comparing the proposed approach to 6 previously reported attack mechanisms while targeting 6 state-of-the-art LLMs showing that the proposed approach is competitively performant."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work adheres to the established black-box jailbreak attack, and the particular category of chain-of-thought based black-box attacks.\n- This work uses previously adopted datasets of illicit instructions such as AdvBench and StrongReject.\n- This work reports Attack Success Rates and Harmfulness Scores for 6 other related jailbreak attacks across 6 target LLMs.\n- This work reports the performance of the proposed attack mechanism against 3 defense mechanisms."}, "weaknesses": {"value": "- This work emphasizes the ability of performing black-box COT attack without access to \"Internal COT traces\" as a key contribution. However the definition of \"Internal COT traces\" is not sufficiently clear, which makes their specific contribution hard to qualify.\n- This work does not discuss prior work such as [3], which performs black-box COT jailbreaking in a zero-shot manner.\n- This work adopts many unclear terminologies around the idea of \"internal COT traces\" that need substantial clarification.\n- Formalisms employed in this work, namely in Eq 1, Eq 2 and Eq 3 could be greatly simplified. Black-box jailbreak attacks can be represented as string-to-string transformations, and compositions thereof (see [4] and [5]).\n- This work should clarify how GPT-4o is employed \"as an external judge to measure both the attack success rate (ASR) and the harmfulness score (HS)\", and justify the validity of how those metrics are computed. The authors should discuss the related ternary taxonomy proposed by [4], which [5] used to construct an LLM-based judge, and validate it with human annotations.\n\n\n# References\n- [1] Kuo, M., Zhang, J., Ding, A., Wang, Q., DiValentin, L., Bao, Y., ... & Chen, Y. (2025). H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large - reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint arXiv:2502.12893.\n- [2] Yao, Y., Tong, X., Wang, R., Wang, Y., Li, L., Liu, L., ... & Wang, Y. (2025). A mousetrap: Fooling large reasoning models for jailbreak with chain of iterative chaos. arXiv - preprint arXiv:2502.15806.\n- [3] Shaikh, O., Zhang, H., Held, W., Bernstein, M., & Yang, D. (2023). On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.\n- [4] Wei, A., Haghtalab, N., & Steinhardt, J. (2023). Jailbroken: How does llm safety training fail?. Advances in Neural Information Processing Systems, 36, 80079-80110.\n- [5] Doumbouya, M. K. B., Nandi, A., Poesia, G., Ghilardi, D., Goldie, A., Bianchi, F., ... & Manning, C. D. h4rm3l: A Language for Composable Jailbreak Attack Synthesis. In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "- How does this work compare to [3], which doesn't appear to require access to \"internal reasoning traces\"?\n- Line 59 \"While effective, these approaches typically require privileged access to internal CoT traces or manual prompt engineering, limiting applicability in realistic black-box scenarios.\"\n    - is this accurate, given that [1] explicitly claims that the intermediate COT traces are often displayed for transparency and interpretability reasons?\n- What do \"meta-instruction\" and \"meta-level reasoning\" mean? How are they different from \"intermediate reasoning steps\" and \"reasoning traces\"? I suggest that the authors avoid unnecessary neologism that may induce confusion in readers, adopt consistent terminology, and prefer clarity and simplicity.\n- What are \"specific internal reasoning pathways\" (Line 215)\n- How are Attack Success Rates and Harmfulness Scores calculated using GPT-4o? Why is the employed method valid?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SpEueFGt3N", "forum": "KS7vlU94eU", "replyto": "KS7vlU94eU", "signatures": ["ICLR.cc/2026/Conference/Submission10515/Reviewer_LMAm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10515/Reviewer_LMAm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129381138, "cdate": 1762129381138, "tmdate": 1762921797316, "mdate": 1762921797316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}