{"id": "dvB0QDmnry", "number": 13156, "cdate": 1758214245175, "mdate": 1759897460011, "content": {"title": "Controllable Sequence Editing for Biological and Clinical Trajectories", "abstract": "Conditional generation models for longitudinal sequences can produce new or modified trajectories given a conditioning input. However, they often lack control over when the condition should take effect (timing) and which variables it should influence (scope). Most methods either operate only on univariate sequences or assume that the condition alters all variables and time steps. In scientific and clinical settings, interventions instead begin at a specific moment, such as the time of drug administration or surgery, and influence only a subset of measurements while the rest of the trajectory remains unchanged. CLEF learns temporal concepts that encode how and when a condition alters future sequence evolution. These concepts allow CLEF to apply targeted edits to the affected time steps and variables while preserving the rest of the sequence. We evaluate CLEF on 8 datasets spanning cellular reprogramming, patient health, and sales, comparing against 9 state-of-the-art baselines. CLEF improves immediate sequence editing accuracy by up to 36.74% (MAE). Unlike prior models, CLEF enables one-step conditional generation at arbitrary future times, outperforming them in delayed sequence editing by up to 65.71% (MAE). We test CLEF under counterfactual inference assumptions and show up to 63.19% (MAE) improvement on zero-shot conditional generation of counterfactual trajectories. In a case study of patients with type 1 diabetes mellitus, CLEF identifies clinical interventions that generate realistic counterfactual trajectories shifted toward healthier outcomes.", "tldr": "CLEF is a controllable sequence editing model for conditional generation of immediate and delayed effects. We evaluate CLEF on cellular, patient, and sales trajectories, including a real-world case study on patients with type 1 diabetes.", "keywords": ["conditional generation", "sequence editing", "time series forecasting", "counterfactual prediction", "multivariate sequences", "concept-based learning", "longitudinal modeling"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0518e73fde65d03a808526a89f8ef3026f1b3b45.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies controllable time‑series editing and proposes CLEF, a framework for conditional generation of time‑series given an intervention time and value. The model encodes past history, time difference, and the intervention into a temporal context embedding that modulates the extent of output change. The method is evaluated on conditional and counterfactual generation tasks across eight datasets from multiple domains, including cellular dynamics, healthcare, and sales."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The work addresses counterfactual outcome estimation, an important problem in computational biology and healthcare.\n* The introduction motivates the problem well, and Fig. 1 is informative. The writing is generally clear. Figures are descriptive and polished, although they are currently too small to read without zooming, so they should be enlarged.\n* The model design is simple and clean.\n* The method is evaluated on a substantial number of datasets from three domains; however, the experimental setup is difficult to follow from the main text at times (see Weaknesses)."}, "weaknesses": {"value": "The paper exhibits several weaknesses: the proposed method shows unexpectedly poor performance relative to a simple linear baseline, the experimental setup lacks clarity in several places, and the results as presented in the Experiments section are not fully aligned with the abstract. I am willing to reconsider my score if these concerns are addressed convincingly.\n\n**(i) Performance relative to simple baselines**\n\nThis is the main weakness. In R1 and R2, performance is unexpectedly poor relative to a simple linear baseline: the linear baseline appears better on average than non‑CLEF models and comparable to CLEF models. This raises concerns about applicability and practicality. If these models do not outperform simply using the previous time point, the value of training and deploying them is unclear.\n\n**(ii) Ambiguity in experimental setup and model variants**\n\nIt is unclear how CLEF and non‑CLEF models differ. Lines 297–298 state that “CLEF and non‑CLEF differ only in the components needed to learn temporal concepts,” which is ambiguous, and Appendix D is difficult to follow. Please clarify whether non‑CLEF models (e.g., Transformer, xLSTM) are implemented exactly as in their original papers or adapted in some way. It is surprising that these models underperform the simple baseline on average in Fig. 4.\n\n**(iii) Abstract–experiment misalignment and selective reporting**\n\nThe abstract’s presentation of results does not accurately reflect the evidence shown in the Experiments section. It is unclear where the abstract’s claims are reported in the main results. Reporting “accuracy gain up to” an arbitrary or worst baseline is not a fair summary. In Fig. 4, the simple baseline appears on par with the proposed complex models. Examples:\n* Lines 21–22: “… immediate sequence editing accuracy by up to 36.74% (MAE).”\n* Lines 22–23: “… delayed sequence editing by up to 65.71% (MAE).”\n\n**(iv) Task definition: R5 vs. R4**\n\nThe definition of R5 (zero‑shot conditional generation of counterfactual trajectories) is not clear. The distinction from counterfactual outcome estimation in R4 should be specified.\n\n**(v) Intervention mechanics in R6**\n\nFor R6, it is unclear how interventions on temporal concepts are performed to decrease or increase glucose levels. As a sanity check, can authors construct a simple baseline also for this task?"}, "questions": {"value": "* Question: Have the authors considered/experimented with other concept encoder and concept decoder architectures?\n* Suggestion: Lack of qualitative trajectory examples. The paper (and related counterfactual outcome estimation work [Bica+20; Seedat+22; Melnychuk+22]) does not provide example estimation trajectories. Including sample time‑series from each dataset with baseline and proposed model estimates would illustrate data characteristics and model behavior.\n* Suggestion: The paragraph about \"controllable text generation\" in the Introduction (starting ln. 59) does not seem directly relevant to the problem at hand. The authors could reconsider removing it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B24Nn5W62j", "forum": "dvB0QDmnry", "replyto": "dvB0QDmnry", "signatures": ["ICLR.cc/2026/Conference/Submission13156/Reviewer_hatS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13156/Reviewer_hatS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686365795, "cdate": 1761686365795, "tmdate": 1762923870403, "mdate": 1762923870403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of sequence editing and forecasting. The paper proposes a new method to achieve this goal. The proposed method is based on encoder decoder solution. Given a condition, the proposed method changes the sequence to meet the given condition. The paper splits the problem into two subproblems: immediate sequence editing and delayed sequence editing. Experiments are conducted on variety of biomedical and financial datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an interesting problem with significant practical applications.\n- The paper is well-written and the proposed method is clearly motivated.\n- Experimental section is thorough and convincing."}, "weaknesses": {"value": "- The related work section could be expanded. Certain areas, such as reinforcement learning, are closely related to this problem but are not discussed in the current version.\n- The paper could also benefit from including more baselines by adapting other existing and closely related methods to the experimental setup. For example, some methods proposed for biological, protein, and DNA sequence editing could be applied to the problem studied in this paper, even though they were originally designed to address slightly different tasks in other biological domains. However, this is not necessary for this paper whereas it can improve its strengths'."}, "questions": {"value": "- After reading the paper, it is not entirely clear how the proposed solution and results differ between immediate sequence editing and delayed sequence editing. Could you elaborate on this? My understanding is that immediate sequence editing can be viewed as a special case of delayed sequence editing.\n- There are existing approaches originally proposed for biological sequence design that could be applied to this problem. For example, Bayesian optimization such as the one introduced in “Accelerating Bayesian optimization for biological sequence design with denoising\nautoencoders” (Stanton et al., 2022) could be considered. Another possible direction is to model the problem using reinforcement learning, where $x$ represents the state and $s$ denotes the value of that state. Note that the term state in this paper may differ from its usage in reinforcement learning. Furthermore, GFlowNets have been widely applied in biological sequence design and specifically in sequence editing as introduced by “GFlowNet-Assisted Biological Sequence Editing” (Ghari et al., 2024) and multi-objective generation as introduced by \" Multi-objective GflowNets\" (Jain et al., 2023). Although direct comparison with these methods is not necessary for this paper, I recommend including a discussion of such approaches to motivate future research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7RncRgLajM", "forum": "dvB0QDmnry", "replyto": "dvB0QDmnry", "signatures": ["ICLR.cc/2026/Conference/Submission13156/Reviewer_v6m5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13156/Reviewer_v6m5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032480462, "cdate": 1762032480462, "tmdate": 1762923869954, "mdate": 1762923869954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose CLEF, a new framework for Controllable Sequence Editing, designed to modify longitudinal sequences (e.g., patient health trajectories, cellular development) based on a specified condition. [cite_start]The key contributions are twofold: 1) a new problem formulation, \"delayed sequence editing,\" which involves applying a condition at a future time and generating the output in a single step, and 2) the CLEF model, which learns \"temporal concepts\" to represent the change from the last observed time to the future time.\n\nThe core mechanism of CLEF is to predict a future state as an element-wise product of the last observed state and the learned temporal concept. This concept $c$ is generated by a \"concept encoder\" that takes the encoded history ($h_x$), the encoded condition ($h_s$), and the time delta ($\\Delta_{t_i, t_j}$) as input."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The formalization of \"delayed sequence editing\"  as a one-step generation task is interesting contribution. This distinguishes the problem from standard auto-regressive forecasting\n- Good empirical results CLEF-based models demonstrate consistent and often large improvements over their non-CLEF counterparts across all primary tasks: immediate editing , delayed editing , and zero-shot counterfactual generation"}, "weaknesses": {"value": "- Does the model assume that the entire, complex dynamic evolution of each specific variable (e.g., a single lab test) over an arbitrary time $\\Delta t$ can be modeled as a single multiplicative scaling factor $c$ for that variable? This still seems dynamically and biologically implausible, as it ignores the coupled, differential nature of these systems.\n- Btw if that is the case, then if any single variable in the last observed state, $x_{k, t_i}$, is 0, then the predicted value for that variable, $\\hat{x}_{k, t_j}^s$, must also be 0 (since $c_k \\odot 0 = 0$). This is a severe limitation for any data that is sparse or has variables that can cross zero."}, "questions": {"value": "- See weaknesses\n- Also I'd argue that c is more of a ratio than a rate of change"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sjzNY0ftyc", "forum": "dvB0QDmnry", "replyto": "dvB0QDmnry", "signatures": ["ICLR.cc/2026/Conference/Submission13156/Reviewer_QAjY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13156/Reviewer_QAjY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083907922, "cdate": 1762083907922, "tmdate": 1762923869504, "mdate": 1762923869504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLEF, a novel framework for controllable sequence editing, designed to address the lack of granular control over timing and scope in the conditional generation of longitudinal data. The core idea is to learn \"temporal concepts,\" which represent the rate of change for each variable, and apply them via a simple multiplicative decoder to generate future states in a single, non-autoregressive step. This approach is designed to handle both immediate and delayed edits—modifying a sequence at a distant future time point without generating intermediate steps. The authors extend this framework to counterfactual outcome estimation, arguing that the architecture provides implicit balancing that outperforms state-of-the-art methods, even without explicit balancing losses. The method's effectiveness is demonstrated through an extensive empirical evaluation on 8 datasets spanning biology, clinical medicine, and finance, where CLEF-augmented models show significant improvements over numerous baselines, particularly in the novel delayed editing task and in zero-shot counterfactual generation. A case study on generating \"healthier\" trajectories for diabetic patients further highlights the model's practical utility and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*   **Clear, useful problem framing for controllable sequence editing.** In my opinion, the split between immediate vs. delayed editing—and the choice to define delayed editing as a single-step, non-autoregressive jump—cleanly targets a real pain point (compounding error) in scientific/clinical forecasting. This makes the task definition itself a contribution.\n\n*   **Broad, careful empirical evaluation with new benchmarks.** I think the scope (8 datasets, 4 contributed benchmarks, 9 baselines, plus generalization and zero-shot counterfactual tests) is a major strength. It shows the idea isn’t brittle and gives the community reusable testbeds. The writing is also notably clear about how this fits vs. related work.\n\n*   **Simple, modular architecture that travels across encoders.** In my view, the “temporal concept” (multiplicative edit) is easy to implement, computationally light, and integrates with varied sequence encoders. That portability lowers the barrier to adoption and facilitates ablations/reuse.\n\n*   **Actionable interpretability via direct concept edits.** I like that domain experts can intervene on the learned concept vector and see resulting counterfactuals; the Type 1 Diabetes case study makes this tangible for in-silico hypothesis exploration. It’s not interpretability in the semantic-concept sense, but it is a practical, controllable handle on predictions.\n\n*   **Good engineering trade-offs for stability vs. flexibility.** In my opinion, the “one-step to $t_j$” design is a pragmatic choice: it sacrifices full trajectory modeling to reduce error accumulation and make edits predictable. Given many operational needs are “endpoint-centric,” that’s an appealing trade-off."}, "weaknesses": {"value": "*   **“Implicit balancing” feels asserted more than demonstrated.** In my opinion, the paper leans on architectural intuition and empirical accuracy to suggest that the learned representation is balanced, but it doesn’t directly test balance. It might read more cautiously if the claim were framed as “consistent with improved balance,” and, if feasible, paired with light diagnostics (e.g., predicting treatment from the learned representation and reporting an IPM-style distance such as MMD/HSIC between treated vs. control in the rep space).\n\n*   **Decoder family may be too restrictive for coupled or constrained systems.** I read the diagonal, multiplicative generator $\\hat{x} = c \\odot x$ as elegant but narrow. In my view, this per-variable scaling risks missing cross-variable couplings, conservation/compositional constraints, or saturation effects. A short limitations note—and, if bandwidth allows, a small ablation with a lightly coupled decoder $(I+W)(c \\odot x)$ where $W$ is sparse or low-rank—could clarify where the current choice shines and where it struggles.\n\n*   **One-step “jump” to $t_j$ trades off path information for stability.** Personally, I see the single-step design as a smart way to avoid compounding autoregressive error, but it does mean path-dependent phenomena (transients, intermediate interventions, accumulation) aren’t modeled explicitly. A brief scope statement that CLEF targets endpoint editing—and a short-horizon chaining stress test (3–5 steps) to show how rollouts behave—would, in my opinion, set expectations more cleanly.\n\n*   **Evaluation blends predictive strength with causal validity.** My sense is that strong MAE/RMSE/AUC is being read as support for the causal story, but predictive accuracy alone doesn’t confirm deconfounding. It may help to report a couple of lightweight causal diagnostics alongside accuracy—(i) treatment predictability from the learned reps, (ii) a balance/divergence metric, and (iii) a small sensitivity analysis—to separate “good predictions” from “good balancing.”"}, "questions": {"value": "* On the \"Implicit Balancing\" Claim: The paper's causal claims rely on the idea of \"implicit balancing,\" which is currently supported by downstream predictive accuracy. To substantiate this claim more directly, could you provide a more targeted diagnostic for balance? For instance, could you report on either (a) the predictability of treatment assignment from the learned representations (where a lower AUC would indicate better balance) or (b) an IPM-style distance like MMD between the representation distributions for treated versus control groups?  \n\n* On the Limitations of the Multiplicative Decoder: The diagonal multiplicative decoder, x^=c⊙x, is elegant but seems to impose a strong linearity assumption. Could you please add a discussion on its potential limitations in systems with known non-linear couplings, compositional constraints (e.g., variables that must sum to a constant), or saturation effects? Clarifying the boundaries of this design choice would help readers understand its ideal application scope.  \n\n* On the Scope of Endpoint vs. Trajectory Generation: The one-step forecast is a key design choice to avoid compounding error. Could you clarify the model's intended scope (i.e., is it primarily for endpoint editing rather than full trajectory simulation)? To help illustrate this, could you include a short-horizon chaining test (e.g., 3–5 steps) to show how error accumulates when the model is applied autoregressively compared to the one-step approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pgCxlxJGAY", "forum": "dvB0QDmnry", "replyto": "dvB0QDmnry", "signatures": ["ICLR.cc/2026/Conference/Submission13156/Reviewer_goGg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13156/Reviewer_goGg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113659066, "cdate": 1762113659066, "tmdate": 1762923869188, "mdate": 1762923869188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}