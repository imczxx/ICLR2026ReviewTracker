{"id": "fRt0Lvr0BK", "number": 12462, "cdate": 1758207991658, "mdate": 1762925603116, "content": {"title": "Latent Compactness: A Unified Perspective on Generative Autoencoders from VAE to VQ-VAE", "abstract": "For a long time, the generative capability of VAE has been explained through the lens of variational inference. Conventional theories treat the KL divergence constraint and reparameterization as a unified mechanism that jointly shape the latent space. In this work, we disentangle these two components through experiments, demonstrating that the KL divergence is the key factor in fostering the formation of semantic manifolds. Reparameterization plays two roles: first, it ensures that latent representations are not deterministic points but rather anisotropic Gaussian ellipsoids, promoting a more uniform distribution in the latent space; second, it enriches the set of semantically defined points during training. This latter role is crucial for enabling meaningful sampling-based generation. Finally, we propose a unified framework where both VAE and VQ-VAE emerge as special cases. The compactness enforced by the KL divergence regularizes the latent structure, and this principle also explains why VQ-VAE, despite lacking stochasticity or a continuous prior can still achieve effective generation.", "tldr": "This paper disentangles the roles of KL divergence and reparameterization in VAE for image generation, demonstrating that the KL divergence is key to establishing semantic manifolds, while reparameterization serves as a regularization mechanism.", "keywords": ["Variational Autoencoder", "VQ-VAE", "latent space compactness", "generative models", "representation learning", "latent regularization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b7b299b9050ec8c137ff4ba18070b21c49369e96.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a conceptual reinterpretation of Variational Autoencoders (VAEs), aiming to disentangle the distinct functional roles of the Kullback–Leibler (KL) divergence and the reparameterization trick within the standard VAE formulation. Contrary to the conventional view that treats these mechanisms as merely components of variational inference, the authors argue that they are structurally complementary: the KL divergence enforces latent compactness and semantic organization, while reparameterization ensures latent enrichment and sample diversity. \nEmpirical analyses support these interpretations. The authors propose alternative VAE variants with explicit Gaussian-ball regularization extracted from the encoder (mean and variance of the variational inference distribution). They also experiment the link between the joint training of encoder, latent representation and decoder showing their interdependence. They compare with VQ-VAE which falls in the same category of models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The importance of the joint model training is very well explained and experiments support this fact.\nThe geometrical representation of the latent space and the link (given the ball-parametrisation) with VQ-VAE is interesting."}, "weaknesses": {"value": "One paper on the exact same viewpoint lacks in the related works and comparison with this particular paper would have help.\nA Geometric Perspective on Variational Autoencoders. Chadebec, C. and Allassonnière, S. Neural Information Processing Systems (NeurIPS 2022).\n\nResults are only qualitative. Quantitative analysis is lacking."}, "questions": {"value": "Since you consider the geometric structure of the latent space, why do you perform linear interpolation?\nWhat is the impact of the latent dimension? In particular when considering the latent balls, does this representation enables to compact the information into lower dimensional space?\nWhen computing the nearest neighbours, you use the Euclidean distant whereas you have local anisotropy. Is the anisotropic distance performing better/worse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeuKm7ToE0", "forum": "fRt0Lvr0BK", "replyto": "fRt0Lvr0BK", "signatures": ["ICLR.cc/2026/Conference/Submission12462/Reviewer_C9Di"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12462/Reviewer_C9Di"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761317373278, "cdate": 1761317373278, "tmdate": 1762923342066, "mdate": 1762923342066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2Y3ijp5exM", "forum": "fRt0Lvr0BK", "replyto": "fRt0Lvr0BK", "signatures": ["ICLR.cc/2026/Conference/Submission12462/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12462/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762925601026, "cdate": 1762925601026, "tmdate": 1762925601026, "mdate": 1762925601026, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies distinct roles of the KL divergence and reparameterization in VAEs and argues that the KL term is primarily responsible for forming a well-organized latent space. It proposes a unified framework where both VAE and VQ-VAE can be seen as special cases of latent compactness regularization. Their findings are supported through empirical experiments on multiple datasets in vision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly structured and is easy to follow. The intuitive illustrations are helpful.\n2. The authors present ablation studies systematically. These ablations help isolate the contributions of each component."}, "weaknesses": {"value": "1. The main insights have been discussed extensively in prior works and are well-known to VAE users. For instance, the following VAE tutorial shows the same idea: https://avandekleut.github.io/vae/\n2. If the paper wants to study this phenomenon deeply, then it should contain rigorous statements and derivations. Currently the claims are qualitative. For instance, when introducing Gaussian balls, why could each code represent a stochastic neighborhood? How is that precisely defined? What is the underlying statistical principle? For another instance, for the Coefficient of Variation metric, why is it defined as such? What are the theoretical benefits and properties of this measure?\n3. Experiments are small-scale and the datasets are too simple, which constrains the generality of the qualitative insights."}, "questions": {"value": "1. What is exactly new in this paper and how would that help users build better VAE?\n2. For Gaussian balls, why could each code represent a stochastic neighborhood? How is that precisely defined? What is the underlying statistical principle? For the Coefficient of Variation metric, why is it defined as such? What are the theoretical benefits and properties of this measure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7pLBwiPwQ1", "forum": "fRt0Lvr0BK", "replyto": "fRt0Lvr0BK", "signatures": ["ICLR.cc/2026/Conference/Submission12462/Reviewer_R8r4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12462/Reviewer_R8r4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623984840, "cdate": 1761623984840, "tmdate": 1762923341760, "mdate": 1762923341760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the paper, the authors disentangle the components in VAE and show that KL divergence is the primary force behind forming meaningful semantic manifolds. While reparameterization helps by making latent codes continuous and Gaussian-shaped, the paper argues its more critical role lies in diversifying semantic samples, enabling effective sample-based generation. Based on these insights, the authors present a unified perspective in which both VAE and VQ-VAE are special cases governed by latent compactness enforced by KL regularization. Intriguingly, this framework also explains how VQ-VAE, despite lacking stochasticity and a continuous prior, can still generate high-quality samples. Overall, the paper provides a fresh conceptual lens on VAEs, highlighting KL divergence—not stochasticity—as the key driver of semantic structure in latent space."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work revisits how VAEs achieve meaningful latent representations by examining the distinct roles of KL divergence and reparameterization. Rather than taking the traditional variational-inference perspective for granted, the study explores whether VAE behavior can be understood from a more empirical and geometric viewpoint. Through controlled experiments and tailored qualitative metrics, the authors provide evidence suggesting that KL divergence may be particularly important for encouraging organized semantic structure in latent space, while reparameterization appears to support stable sampling and enrich latent coverage. Although not intended as a definitive theoretical explanation, this empirical analysis offers a complementary perspective that may help broaden our understanding of latent-space behavior in VAEs and related models such as VQ-VAE."}, "weaknesses": {"value": "1. The paper lacks theoretical justifications that supports their hypothesis. They use a small amount of visualization on small dataset to showcase and support their hypothesis. \n\n2. They claim their method is connected to VQ-VAE and unifires VQ-VAE, whereas to me, it seems more like a combination with VA-VAE rather than unification. \n\n3. The empirical study does not provide any ablation study to understand the strength of the variance or the regularizer, and fails to provide convincing evidence of the advantage of the proposed method.  \n\n4. Generally I feel like the contribution of the paper is below the acceptance bar of ICLR."}, "questions": {"value": "Please see above for weakness questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FsG0AMi0LI", "forum": "fRt0Lvr0BK", "replyto": "fRt0Lvr0BK", "signatures": ["ICLR.cc/2026/Conference/Submission12462/Reviewer_WE5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12462/Reviewer_WE5H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949503021, "cdate": 1761949503021, "tmdate": 1762923341489, "mdate": 1762923341489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The papers presents an empirical investigation of  the effects of (1) KL regularization and (2) reparameterization in VAEs. The paper also presents a unified framework the generalizes both VAE and VQ-VAE."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The work addresses the important research topic of understanding the workings of VAEs.\nThe authors propose different metrics to measure the characteristics of VAEs."}, "weaknesses": {"value": "Further proofreading is required: Mainly (1) the axes in the figures are not consistent (Figure 5, 2B), and the claims are hard to understand (see below).\nLine 215: The authors write \"or does it also stem from the increased density of semantic definition that more regions of the space are actively used?\" It's not clear what is meant here.\nLine 243: \"Figure 4B ...... produces incoherent and meaningless outputs,\" I don't believe this is clearly shown in the figure, many examples resemble MNIST digits. can you be more precise to what the reader should look for?\nLine 249: the authors write \"This demonstrates that reparameterization does more than just regularize the latent space: it effectively enriches the set of defined points, acting as a contractual mechanism between the encoder and decoder.\" I don’t see how this follows from figure 4a. Does it not just means the new decoder is robust to small perturbations?\nSection 3.1 L265: Can you explain what the \"Dynamic Latent Coverage\" metric? what is the formula and exactly how it is related to the prior work?\nSection 3.1 seems out of place, under Section 3: VQ-VAE\nSection 3.2 and Figure 6: The authors write \"Model Architecture. Path A: for our experiment, where the encoder output is regularized toward the nearest code in a learnable codebook. Path B: VQ-VAE, which uses discrete latent codes.\" I don't understand the difference here! In VQ-VAE also the encoder output is regularized toward the nearest code in a learnable codebook."}, "questions": {"value": "Please address the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hjDiA8v1gO", "forum": "fRt0Lvr0BK", "replyto": "fRt0Lvr0BK", "signatures": ["ICLR.cc/2026/Conference/Submission12462/Reviewer_cNSf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12462/Reviewer_cNSf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997794711, "cdate": 1761997794711, "tmdate": 1762923341228, "mdate": 1762923341228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}