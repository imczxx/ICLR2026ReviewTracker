{"id": "4tasfBIPxp", "number": 21299, "cdate": 1758316009835, "mdate": 1763157845683, "content": {"title": "DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials", "abstract": "Large-scale atomistic simulations are essential to bridge computational materials and chemistry to realistic materials and drug discovery applications. In the past few years, rapid developments of machine learning interatomic potentials (MLIPs) have offered a solution to scale up quantum mechanical calculations. Parallelizing these interatomic potentials across multiple devices poses a challenging, but promising approach to further extending simulation scales to real-world applications. In this work, we present \\textbf{DistMLIP}, an efficient distributed inference platform for MLIPs based on zero-redundancy, graph-level parallelization. In contrast to conventional space-partitioning parallelization, DistMLIP enables efficient MLIP parallelization through graph partitioning, allowing multi-device inference on flexible MLIP model architectures like multi-layer graph neural networks. DistMLIP presents an easy-to-use, flexible, plug-in interface that enables distributed inference of pre-existing MLIPs. We demonstrate DistMLIP on four widely used and state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN. We show that existing foundation potentials can perform near-million-atom calculations at the scale of a few seconds on 8 GPUs with DistMLIP.", "tldr": "DistMLIP is a first-of-its-kind graph parallel inference platform for machine learning interatomic potentials.", "keywords": ["machine learning interatomic potential", "molecular dynamics", "atomistic simulation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f70ceead1281dfbd797b1a61ec5fbe957ee79744.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a novel distributed inference platform, DistMLIP, for machine learning force fields.  DistMLIP enables efficient MLIP parallelization through graph partitioning, overcoming the requirement for LAMMPS. The results show that this platform 1)  significantly outperforms previous SOTA regarding the inference efficiency, 2) supports large-molecule inference, and 3) supports multiple network architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. DistMLIP significantly improves the inference speed for several models.\n2. DistMLIP enables large molecular inference.\n3. DistMLIP is model-agnostic.\n4. DistMLIP achieves above features via full-atom level graph partition instead of spatial partition, omitting ghost atoms utilized by SevenNet, which are methodologically novel."}, "weaknesses": {"value": "1. This platform does not support training.\n2. This platform does not support multi-node inference.\n3. The vertical wall is ad hoc. Although it performs well for the selected molecular system, it could fail for anisotropic molecules."}, "questions": {"value": "Weakness 3 seems like an inherent limitation of DistMLIP by nature, but do you have any plan to address it, or to show that the method could also perform well for anisotropic molecules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LlpB24ukHP", "forum": "4tasfBIPxp", "replyto": "4tasfBIPxp", "signatures": ["ICLR.cc/2026/Conference/Submission21299/Reviewer_18ns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21299/Reviewer_18ns"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760690302619, "cdate": 1760690302619, "tmdate": 1762941680462, "mdate": 1762941680462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DistMLIP is a distributed inference platform for ML interatomic potentials (MLIPs) that parallelizes graph-based models across GPUs with zero redundant compute. It partitions the atom graph (and the three-body “bond” line graph) and exchanges border features every GNN layer, so existing models (MACE, CHGNet, TensorNet, eSEN) can run multi-GPU without architectural changes. It can use up to 3.4× larger systems and is up to 8× faster than prior multi-GPU approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Works with popular MLIPs (MACE, TensorNet, CHGNet, eSEN) with minimal adaptation, so we don’t need model-specific rewrites.\n2. The “vertical” partition rule is reported up to 8x faster than standard graph partitioners (e.g., METIS/RCMK). And against SevenNet’s distributed inference, DistMLIP has up to 10x higher max capacity and is 4x faster."}, "weaknesses": {"value": "Majors:\n1. The authors say the design keeps backprop intermediates in their contribution claims, but they only benchmark inference; there’s no distributed-training result or accuracy/stability study over long MD runs.\n2. All inference timing is on one cluster of 8x A100-80GB; there’s no multi-node or NVLink study to justify capability of large scale simulation.\n\nMinors:\n1. Line 39: \"CHARM\" -> \"CHARMM\"\n2. Line 44: \"coupled clustering\" -> \"coupled cluster\"\n3. Line 132: Citation format\n4. Line 201: \"G_n\" -> \"G_p\"\n5. Line 271: \"Pytorch, Jax\" -> \"PyTorch, JAX\"\n6. Line 346: \"partitoining\" -> \"partitioning\""}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "quSF9gOpQ9", "forum": "4tasfBIPxp", "replyto": "4tasfBIPxp", "signatures": ["ICLR.cc/2026/Conference/Submission21299/Reviewer_SsYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21299/Reviewer_SsYg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767638259, "cdate": 1761767638259, "tmdate": 1762941680042, "mdate": 1762941680042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DistMLIP is a distributed inference platform for ML interatomic potentials (MLIPs) that parallelizes graph-based models across GPUs with zero redundant compute. It partitions the atom graph (and the three-body “bond” line graph) and exchanges border features every GNN layer, so existing models (MACE, CHGNet, TensorNet, eSEN) can run multi-GPU without architectural changes. It can use up to 3.4× larger systems and is up to 8× faster than prior multi-GPU approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Works with popular MLIPs (MACE, TensorNet, CHGNet, eSEN) with minimal adaptation, so we don’t need model-specific rewrites.\n2. The “vertical” partition rule is reported up to 8x faster than standard graph partitioners (e.g., METIS/RCMK). And against SevenNet’s distributed inference, DistMLIP has up to 10x higher max capacity and is 4x faster."}, "weaknesses": {"value": "Majors:\n1. The authors say the design keeps backprop intermediates in their contribution claims, but they only benchmark inference; there’s no distributed-training result or accuracy/stability study over long MD runs.\n2. All inference timing is on one cluster of 8x A100-80GB; there’s no multi-node or NVLink study to justify capability of large scale simulation.\n\nMinors:\n1. Line 39: \"CHARM\" -> \"CHARMM\"\n2. Line 44: \"coupled clustering\" -> \"coupled cluster\"\n3. Line 132: Citation format\n4. Line 201: \"G_n\" -> \"G_p\"\n5. Line 271: \"Pytorch, Jax\" -> \"PyTorch, JAX\"\n6. Line 346: \"partitoining\" -> \"partitioning\""}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "quSF9gOpQ9", "forum": "4tasfBIPxp", "replyto": "4tasfBIPxp", "signatures": ["ICLR.cc/2026/Conference/Submission21299/Reviewer_SsYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21299/Reviewer_SsYg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767638259, "cdate": 1761767638259, "tmdate": 1763173105067, "mdate": 1763173105067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DistMLIP, a platform designed for efficient distributed inference of Machine Learning Interatomic Potentials (MLIPs), particularly targeting large-scale atomistic simulations (up to millions of atoms). The authors argue that while MLIPs offer quantum accuracy at lower cost, scaling them further requires multi-device parallelization . Existing methods, primarily based on spatial partitioning (like in LAMMPS), suffer from computational redundancy (ghost atoms) and are often ill-suited for modern, long-range GNN-based MLIPs . DistMLIP proposes a graph-level parallelization strategy based on graph partitioning, aiming for zero redundancy. It partitions the atom graph (and optionally higher-order graphs like bond graphs) across multiple GPUs and manages the communication of necessary node/edge features between partitions at each GNN layer . A key feature is its design as a flexible, easy-to-use, standalone platform that can wrap existing MLIPs without requiring model modification or reliance on specific simulation packages . The effectiveness is demonstrated on four diverse MLIPs (CHGNet, MACE, TensorNet, eSEN), showing significant improvements in maximum simulatable system size (up to 3.4x larger) and speed (up to 8x faster) compared to previous methods, enabling near-million-atom calculations in seconds on 8 GPUs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Scaling MLIP simulations to biologically and materially relevant sizes (millions of atoms) is a major challenge. DistMLIP provides a much-needed solution specifically tailored for efficient distributed inference of modern GNN-based MLIPs.\n- DistMLIP is designed as a model-agnostic, plug-in platform . This allows researchers to apply it to their existing, pre-trained MLIPs with minimal adaptation (as demonstrated with four different models), significantly lowering the barrier to large-scale simulations. Its independence from specific simulation software like LAMMPS increases flexibility.\n- The reported results are substantial: linear scaling of capacity, significant speedups in strong scaling tests (up to 8x faster), and the ability to perform near-million atom calculations in seconds on modest hardware (8 GPUs). Comparisons vs LAMMPS MACE and SevenNet further highlight the advantages."}, "weaknesses": {"value": "- Graph partitioning inherently requires communication between GPUs after each message-passing layer to exchange border node information. The paper acknowledges scaling isn't always ideal (Fig 2b, 2c) partly due to overheads, but a more detailed analysis of communication cost vs. computation cost, and how it scales with the number of GPUs, graph density, and partition quality, would be valuable.\n- The paper appears to be lacking comparisons against several relevant baselines. For instance, a critical benchmark is missing: how does the speed (e.g., throughput or latency) of the proposed system compare to other established distributed systems, such as Allegro?"}, "questions": {"value": "- Can the authors provide a breakdown of inference time into computation, communication, and graph construction/partitioning for different scenarios (varying GPU counts, system sizes)?\n- What are the main challenges and potential strategies for extending DistMLIP to multi-node environments?\n- How much effort is typically required to integrate a new MLIP model into the DistMLIP framework? Does it require modifications to the model's forward pass implementation? (Code 2 gives hints, but more context would be useful)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "85MQiSIMmI", "forum": "4tasfBIPxp", "replyto": "4tasfBIPxp", "signatures": ["ICLR.cc/2026/Conference/Submission21299/Reviewer_9NL9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21299/Reviewer_9NL9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907050510, "cdate": 1761907050510, "tmdate": 1762941679380, "mdate": 1762941679380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DistMLIP, a distributed inference platform designed to scale Machine Learning Interatomic Potentials (MLIPs) to large-scale atomistic simulations. The core problem it addresses is the computational bottleneck of running modern, high-accuracy MLIPs—many of which are based on Graph Neural Networks (GNNs) with long-range interactions—on systems with millions of atoms.\n\nThe key contribution is a \"zero-redundancy, graph-level parallelization\" strategy. This method contrasts with conventional spatial partitioning (e.g., in LAMMPS), which suffers from high computational redundancy due to the need to calculate \"ghost atoms\" at partition boundaries. DistMLIP partitions the atomic graph itself and distributes subgraphs to different GPUs, enabling efficient parallel inference. The platform is presented as a flexible, \"plug-in\" library that does not depend on third-party simulation packages like LAMMPS.\n\nThe authors demonstrate DistMLIP's effectiveness by benchmarking four popular MLIPs: CHGNet, MACE, TensorNet, and eSEN. The results show that DistMLIP can simulate systems 3.4x larger and achieve up to 8x faster performance compared to previous multi-GPU methods , enabling near-million-atom simulations on a single 8-GPU node."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "### High-Impact Problem: \n\nThe paper tackles a critical and timely bottleneck in computational science. Scaling MLIPs to the meso-scale (millions of atoms) is essential for bridging quantum-accurate simulations with real-world applications in materials science, chemistry, and biology.\n\n### Sound and Novel Method: \n\nThe graph-level parallelization approach is fundamentally better suited for GNN-based MLIPs than traditional spatial partitioning. The paper clearly articulates the \"zero-redundancy\" advantage , which correctly avoids the cubic scaling of redundant computations (ghost nodes) that spatial partitioning faces as the MLIP interaction range increases. The method's native support for both atom graphs and higher-order bond graphs (used in models like CHGNet) is a significant advantage.\n\n- Comprehensive and Rigorous Empirical Validation: This is the paper's strongest aspect.\n\n- Diverse Models: The method is validated on four distinct and widely-used MLIPs, demonstrating its generality.\n\n- Strong Baselines: The authors provide direct comparisons against two crucial baselines: (1) The industry-standard spatial partitioning (LAMMPS-MACE) and (2) Another graph-parallel method (SevenNet). DistMLIP shows superior performance in maximum capacity and speed against both.\n\n- Excellent Scaling Analysis: The paper provides clear strong and weak scaling plots (Fig. 2) , as well as detailed analyses of how performance scales with model parameters and, most importantly, interaction range (Fig. 3). The linear scaling with interaction range (vs. cubic for spatial partitioning) is a key result.\n\n### Pragmatic Design Insight:\n\nA standout finding is the justification for using a simple \"vertical wall\" partitioning scheme. The authors correctly identify that for MD, the partitioning latency (which must be paid at every time step) is a critical bottleneck . Their simple heuristic is shown to be up to 8x faster than more complex graph partitioning algorithms like METIS or RCMK (Table 4), demonstrating a deep, practical understanding of the problem domain.\n\n### Practicality and Usability: \n\nBy designing DistMLIP as a standalone, \"plug-and-play\" Python-based library , the authors have significantly lowered the barrier to adoption for the broad community of researchers who use these models but are not experts in distributed computing."}, "weaknesses": {"value": "### Single-Node Limitation:\n\n The paper states the current implementation only supports \"single-node multi-GPU inference\". This is a significant limitation for scaling to truly massive systems (tens of millions+ atoms), which would require a multi-node, multi-GPU setup. The paper would be stronger if it discussed the roadmap and key challenges (e.g., managing communication overhead of border node features across a network interconnect) for a multi-node implementation.\n\n### Clarification of \"8x Faster\" Claim: \n\nThe abstract and introduction claim \"up to 8x faster\". However, the direct end-to-end inference comparison with SevenNet shows a ~4x speedup , and the LAMMPS-MACE comparison shows similar (not 8x faster) speeds, albeit with a non-compiled model. The 8x speedup figure appears to be sourced from the partitioning algorithm comparison in Table 4. The authors should clarify this in the main text to avoid overstating the end-to-end simulation speedup.\n\n### Scaling of High-Order Graphs: \n\nThe paper honestly reports \"suboptimal\" weak scaling for CHGNet, attributing it to the three-body graph construction cost. This is an important detail, as it suggests that the performance benefits of DistMLIP may be partially bottlenecked by models with complex, high-order interactions. A brief discussion of whether this construction is (or can be) parallelized within DistMLIP would be beneficial.\n\n### eSEN Performance: \n\nThe eSEN model consistently performs poorly, with high memory usage and frequent OOM errors. While this is likely due to the model's architecture rather than DistMLIP, its poor performance slightly detracts from the platform's \"general and versatile\" claim."}, "questions": {"value": "## 1. Multi-Node Scalability: \n\nThe current work is an excellent demonstration of single-node, multi-GPU scaling. Could you elaborate on the primary challenges for extending DistMLIP to a multi-node environment? Specifically, how do you envision managing the atom_transfer step across a network interconnect, and what do you anticipate will be the new performance bottleneck (e.g., network latency vs. bandwidth)?\n\n## 2. High-Order Graph Construction Bottleneck: \n\nThe suboptimal weak scaling of CHGNet is due to the three-body graph construction. Is this construction step (described in Algorithm 2) fully parallelized within DistMLIP, or is it a separate, serial (or partially parallel) step that acts as a bottleneck before the GNN forward pass? Does this imply a fundamental limitation for DistMLIP's performance on future models that might incorporate even higher-order (e.g., four-body) interactions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Ethics Concerns"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Os9Xvt1lnU", "forum": "4tasfBIPxp", "replyto": "4tasfBIPxp", "signatures": ["ICLR.cc/2026/Conference/Submission21299/Reviewer_dy5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21299/Reviewer_dy5L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920357255, "cdate": 1761920357255, "tmdate": 1762941678992, "mdate": 1762941678992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Uploaded revised version of the paper"}, "comment": {"value": "We would like to thank all reviewers for their reviews! \n\nWe're glad the reviewers were impressed with the state of the art performance (in both speed and scale) of DistMLIP as well as its generality and ease of use. We have uploaded a new PDF incorporating all changes in response to the reviewer’s suggestions.\n\n\nOur modifications, highlighted in blue text in the pdf, are as follows:\n\n\nFrom Reviewer dy5L:\n* Added clarification in the main text that the 8x speedup refers to the partitioning algorithm comparison in Table 4.\n\nFrom Reviewer 9NL9:\n* Incorporated plots showing the breakdown of inference time between forward pass, backward pass (necessary for force calculations), data transfer, and graph construction overhead to Appendix E. We provide 2 plots: one showing the breakdown where the total number of atoms are fixed and the number of GPUs increases and one showing the inference breakdown where the number of atoms per GPU is fixed and the total number of GPUs increases. \n* Added clarification in Appendix J elaborating on how to implement a new MLIP onto the DistMLIP platform.\n\nFrom Reviewer SsYg:\n* Fixed typos.\n* Included example of stable simulation over 2 nanosecond simulation on 8 GPUs (Appendix K).\n* Added performance metric of large scale training on classical MD labels using DistMLIP (Appendix M).\n\nWe believe that DistMLIP will enable scientists and practitioners to study physical phenomena at scales that were unprecedented before – scales that were previously unstudy-able using molecular dynamics simulation at high accuracy – and accelerate scientific discovery ranging from protein dynamics all the way to battery cathode discovery."}}, "id": "JtxPk2LLpD", "forum": "4tasfBIPxp", "replyto": "4tasfBIPxp", "signatures": ["ICLR.cc/2026/Conference/Submission21299/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21299/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission21299/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763158281339, "cdate": 1763158281339, "tmdate": 1763158281339, "mdate": 1763158281339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}