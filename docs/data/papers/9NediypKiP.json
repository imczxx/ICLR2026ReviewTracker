{"id": "9NediypKiP", "number": 3273, "cdate": 1757391105930, "mdate": 1763519314445, "content": {"title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning", "abstract": "Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in novel software, require both long-horizon planning with software domain knowledge and precise, fine-grained execution. Existing approaches suffer from a trade-off: generalist agents excel at planning but falter in execution, while specialized agents show the opposite weakness. While recent compositional frameworks attempt to bridge this gap by combining a \"planner\" and an \"actor\", they are typically static and non-trainable, preventing adaptation from experience—a critical limitation given the scarcity of high-quality data in novel software.\nTo address these limitations, we introduce CODA, a novel and trainable compositional framework that synergizes a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained with a dedicated two-stage training pipeline. The first stage, Specialization, employs a decoupled GRPO approach to train an expert planner for each novel software individually. The second stage, Generalization, aggregates all positive trajectories from all specialized experts. This consolidated, high-quality dataset is then used to perform supervised fine-tuning (SFT) on the final planner, equipping it with the robust, cross-domain capabilities of a generalist.\nEvaluated ScienceBoard benchmark with diversified novel softwares, our framework significantly outperforms the baseline and establishes a new state-of-the-art SOTA among open-source models with strong generalizability to novel software and unseen executor like code agent. \nAll the code and models will be made publicly available to foster further research.", "tldr": "We introduce CODA, a framework that uses decoupled reinforcement learning to train specialist planners by separating high-level planning from low-level execution, then merges their knowledge to create a powerful generalist agent.", "keywords": ["LVLMs", "agent", "computer use agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/939618abd533d0068856ffa4cf80a0c40cab49c5.pdf", "supplementary_material": "/attachment/c5a825b72065194af2ff4e10888cfe56e0f0e9ed.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces CODA, a dual-module (planner-executor) framework for GUI agents inspired by human brain anatomy. The core idea is to separate a \"cerebrum\" (Planner) responsible for high-level reasoning from a \"cerebellum\" (Executor) that handles precise action execution. The method's novelty lies in its two-stage training paradigm: (1) **software-specific specialist** planners are first trained via decoupled reinforcement learning (with a static executor); (2) successful trajectories from all specialists are then aggregated for **supervised fine-tuning (SFT)** of a generalist planner. To avoid manual annotation, the paper details an automated \"Judge System\" to provide reward signals and presents a scalable distributed infrastructure for data collection. Experiments on ScienceBoard and OSWorld demonstrate improvements over baselines and claim robust generalization to unseen software and, surprisingly, heterogeneous executors like code agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Principled Decoupling: The cerebrum-cerebellum analogy is intuitive, but more importantly, it offers a practical and modular solution to the long-standing tension between high-level planning and fine-grained execution. This design allows for efficient adaptation to new software without retraining the entire agent.\n2. Technically Rigorous Two-Stage Training: The \"specialist-to-generalist\" curriculum (RL -> SFT) is well-motivated, directly addressing the limitations of monolithic, end-to-end training. The ablation studies and cross-domain results (Tables 3, 4) provide strong empirical support for this choice.\n3. Automated Judging System: The judging system provides scalable reward signals via ensemble voting and multi-resolution input (Table 2), which significantly reduces the dependency on manually labeled trajectories, a strong point for real-world applicability.\n4. Solid and Consistent Empirical Gains: CODA achieves new SOTA performance for open-source models on ScienceBoard, with consistent improvements shown across multiple software domains and metrics.\n5. Commitment to Open Science: The authors' pledge to release all code and models is excellent for the community and reproducibility."}, "weaknesses": {"value": "Despite the strengths, the paper fails to convince me on several critical points:\n\n1. Lack of Critical Baselines; SOTA Claims Unfounded:\n   - This is the paper's most severe flaw. The baseline comparison in Table 1 is missing the most crucial and direct competitor: a top-tier closed-source planner (like GPT-4o) paired with the *same* open-source executor (like UI-TARS). This is the \"gold standard\" for evaluating the CODA planner's performance. Without it, the SOTA claim is unsubstantiated.\n   - Furthermore, as listed in \"Potentially Missing Related Work,\" a large body of work on \"decoupled architectures\" and \"hierarchical RL\" already exists. The paper makes no empirical comparison to these structurally similar open-source works (e.g., Wang et al. 2024, Chen et al. 2023), clouding CODA's true innovative contribution.\n2. Significant Concerns about the Automated Judge System:\n   - Strange Ensemble Architecture: The Judge's design is perplexing. The authors claim to \"distill\" knowledge from GPT-4o/Gemini into the 72B-GUI-Judge, but then must ensemble this \"student\" model with the original pre-trained model for the final system. This seems to be a vote of no confidence in the distillation itself. If the fine-tuning was successful, why is the original model still needed?\n   - Lack of Teacher Benchmarks: The paper never reports the accuracy (Precision/Recall) of the \"teacher\" models (GPT-4o/Gemini) on the judging tasks. We have no way to know the quality of the distillation data source or to evaluate how well the student model learned.\n   - Low Metrics and Potential for Bias: The reported Precision/Recall numbers in Table 2 (many below 80%) are worrying. Low precision introduces false positive rewards, which can destabilize RL training and teach \"hallucinated\" behaviors. Low recall discards valuable successful trajectories, hurting the data efficiency of the SFT stage.\n   - Why not use simpler, more robust methods? For many tasks, determining \"success\" could be achieved with near-100% accuracy and without model-induced bias. This could be done by training a task-specific model, using rule-based oracles, or leveraging other oracle information. Why did the authors opt for this complex, lower-accuracy, and potentially biased distillation approach instead of these simpler, more reliable methods?\n3. Insufficient Evidence for Cross-Executor Generalization:\n   - This is one of the paper's most surprising claims (Table 4), but it is the least explained. Why would a planner that has only ever been trained to command a GUI executor suddenly understand how to command a code agent it has never seen?\n   - This generalization is counter-intuitive. It implies the planner learns a highly abstract, executor-agnostic policy (\"Thought\"). If true, this is a major finding, but the paper provides zero case studies to analyze this. We need to see a side-by-side comparison of the \"Thought\" process as CODA commands a GUI agent versus a code agent for the same task.\n4. Unfair and Incomplete Case Studies:\n   - The case study in Figure 6 is not a convincing comparison. It shows Qwen2.5-VL failing on Case A and UI-TARS failing on Case B. This is a classic \"cherry-picking\" setup.\n   - A meaningful comparison must test all models (Qwen, UI-TARS, CODA) on the exact same cases. How does UI-TARS perform on Case A? How does Qwen perform on Case B? How does CODA perform on both? The current analysis is uninformative.\n5. The Static Executor: A Potential Bottleneck:\n   - The CODA framework assumes the \"cerebellum\" (executor) is perfect and static. This is a risky assumption for real-world applications. If faced with a truly novel, out-of-distribution (OOD) GUI layout or widget, the fixed executor will fail, and the planner has no recourse.\n   - The paper never discusses this \"execution bottleneck\" or the conditions under which a static executor would become the system's limiting factor."}, "questions": {"value": "1. **(Re: Baselines)** Can the authors provide results for the single most important baseline: **GPT-4o (or Gemini 2.5 Pro) as the Planner** paired with the UI-TARS-1.5 Executor?\n2. **(Re: Judge System)**\n   - Please justify the \"distilled model + pre-trained model\" ensemble. Why is this necessary if the distillation was successful?\n   - What is the Precision/Recall of the \"teacher\" models (GPT-4o/Gemini) on your judging tasks?\n   - Why not use a task-specific model or a rule-based oracle for rewards to achieve higher accuracy and avoid model bias?\n3. **(Re: Cross-Executor Generalization)** Please provide a detailed case study for the **same task**, showing CODA's full \"Thought\" and \"Action\" sequence when commanding (a) the GUI executor vs. (b) the Code agent.\n4. **(Re: Case Study)** Can the authors provide a fair version of Figure 6, showing Qwen2.5-VL, UI-TARS-1.5, and CODA on the **same set of tasks**?\n5. **(Re: Static Executor)** How do the authors view the risk of the static executor becoming a bottleneck on OOD interfaces? Does the CODA framework have any mechanism to handle this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "woWIFZmBDG", "forum": "9NediypKiP", "replyto": "9NediypKiP", "signatures": ["ICLR.cc/2026/Conference/Submission3273/Reviewer_JvNe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3273/Reviewer_JvNe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836974996, "cdate": 1761836974996, "tmdate": 1762916641041, "mdate": 1762916641041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a decoupled reinforcement learning framework that trains a high-level planner and pair it with a fixed low-level executor for agentic GUI-based computer use tasks. It introduce a two-stage training pipeline for the high level planner to first train software-specialized planner with RL and then train a generalizable planner with SFT on the specialized planner trajectories. Moreover, it alleviates the need for costly human label for RL by proposing an automated judging system powered by vision LLM to provide high quality reward signal automatically. Results on ScienceBoard and OSWorld demonstrate the effectiveness of the planner & executor for both in-domain tasks and out-of-domain generalizability."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. While the formalism of decoupled planner-executor design for agentic framework is well established, this work still provides good insights into further fine-tuning the planner with frozen executor can yield significant benefits in domains like GUI agents, where the low-level grounding can be performed with high accuracy but high-level planning is still challenging with the lack of domain knowledge.\n2. The proposed automated judge/reward system largely alleviate the need for human labels, further increase the efficiency of the proposed framework.\n3. The experiments and analysis are pretty comprehensive."}, "weaknesses": {"value": "1. While the author states that in stage 1 training they train four specialist models for each software in ScienceBoard. However, as far as I understand, ScienceBoard contains tasks across 6 domains with one software per-domain, so how can four specialized agents cover six softwares? Moreover, as mentioned in ScienceBoard, there are cross-application scenarios which requires more than one software to accomplish the tasks, how are these software specialized models handle cross-application tasks?"}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EYBt67Zwz7", "forum": "9NediypKiP", "replyto": "9NediypKiP", "signatures": ["ICLR.cc/2026/Conference/Submission3273/Reviewer_gKdc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3273/Reviewer_gKdc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882572251, "cdate": 1761882572251, "tmdate": 1762916640839, "mdate": 1762916640839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CODA, a trainable compositional framework for GUI agents that addresses the challenge of automating complex software tasks requiring both high-level planning and precise execution. Inspired by the functional division between the cerebrum and cerebellum in the human brain, the authors propose decoupling these responsibilities into a learnable Planner (based on Qwen2.5-VL) and a fixed Executor (UI-TARS-1.5 or GUI-Actor). The result is a two-stage training pipeline using decoupled reinforcement learning: (1) a Specialization stage where individual expert planners are trained for specific software using Group Relative Policy Optimization (GRPO) with rewards from an automated judging system, and (2) a Generalization stage where trajectories from all specialists are aggregated to train a unified generalist planner via supervised fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well-presented and easy to follow.  The analogy to the human brain's cerebrum-cerebellum division provides an intuitive conceptual framework that helps readers understand the motivation for decoupling planning from execution. The experimental evaluation demonstrates that the presented framework performs strongly better than closed larger models. The fact that the model trained on ScienceBoard shows meaningful performance on the unseen OSWorld benchmark is a good sign as well. The paper also includes comprehensive ablations of all the components and overall I was satisfied with the analysis."}, "weaknesses": {"value": "While CODA works better than closed models would have been good to include other agentic frameworks for comparison. Also Table 1 needs to be explained better. it is not very clear what Average @1, 8... stand for\n\nMinor thing, LVLM acronym is introduced before explaining what is it (first parag of Sec 2)"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JBYhMd2cGY", "forum": "9NediypKiP", "replyto": "9NediypKiP", "signatures": ["ICLR.cc/2026/Conference/Submission3273/Reviewer_n3de"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3273/Reviewer_n3de"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927459858, "cdate": 1761927459858, "tmdate": 1762916640401, "mdate": 1762916640401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CODA introduces a two-component system—Cerebrum (planner) and Cerebellum (executor)—to overcome the common trade-offs between high-level planning and low-level execution in autonomous GUI agents. The paper claims to outperform prior systems by decoupling these tasks, allowing the planner to adapt via reinforcement learning, while the executor remains fixed. It presents a two-stage training process that first specializes the planner for specific software and later generalizes it across domains. Despite these grand claims, the paper largely fails to address key concerns, both in terms of practicality and theoretical soundness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Generalization Across Software: By leveraging a specialist-to-generalist approach, CODA achieves strong generalization across novel software environments. Its ability to adapt to different software systems without requiring human-labeled data is a significant improvement over many existing systems."}, "weaknesses": {"value": "Ambitious Design: The idea of decoupling high-level planning from low-level execution is an interesting attempt to mimic human cognition. The use of a \"Cerebrum\" and \"Cerebellum\" model, while conceptually engaging, feels overly complex for the problem at hand. It’s as though the authors were trying too hard to sound cerebral, when a simpler solution might suffice.\n\nPotential Overfitting: While the system is trained to generalize, there is a risk of overfitting during the specialization phase, especially when only a limited set of software environments are used. Ensuring that the generalist model is truly robust and does not perform poorly on unseen tasks remains a challenge.\n\nQuestionable Innovation: The “Cerebrum-Cerebellum” split is framed as a revolutionary idea. Yet, many systems already rely on modular planning-execution pipelines. The authors' framework adds little to the discussion in terms of novel methodologies. It feels like the paper is rebranding an old idea under a shiny new name."}, "questions": {"value": "The system is based on reinforcement learning—what happens when you encounter tasks that are outside the scope of the training set? Is there an inherent limitation in CODA’s ability to generalize, or can it truly handle novel scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M77ZCvQs2B", "forum": "9NediypKiP", "replyto": "9NediypKiP", "signatures": ["ICLR.cc/2026/Conference/Submission3273/Reviewer_XXbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3273/Reviewer_XXbi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762578391092, "cdate": 1762578391092, "tmdate": 1762916640067, "mdate": 1762916640067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}