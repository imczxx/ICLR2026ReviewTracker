{"id": "slTQdWWQI9", "number": 20372, "cdate": 1758305246308, "mdate": 1763415830247, "content": {"title": "Contrastive Learning with Quantum Projection Heads and Kernels", "abstract": "Self-supervised contrastive learning is sensitive to architectural choices and to how similarity is defined. Motivated by claims that quantum circuits can induce useful non-classical geometries, we present a systematic empirical analysis of two natural drop-in quantum components for the projection/similarity stage: (i) variational quantum circuits (VQCs) as projection heads and (ii) fixed quantum feature maps whose state fidelities act as similarity measures (``quantum kernels''). Within a controlled SimCLR pipeline on STL-10 (ResNet18 encoder) using mainstream \\emph{analytic} simulators, we report three findings. First, under realistic resource constraints (low qubit count, shallow depth), a tuned classical MLP head consistently matches or outperforms VQC heads. Second, fidelity-based quantum kernels largely mirror cosine similarity without a clear uplift. Third, increasing circuit size rapidly incurs prohibitive latency, exposing scaling bottlenecks that restrict current explorability. These results constitute a useful null baseline for hybrid quantum-classical contrastive learning and point to concrete directions: batching-friendly simulators for higher throughput, lower-variance/better-conditioned feature maps to avoid similarity collapse, and modest, low-latency hardware as a realistic near-term testbed. We release anonymized code and consolidated hyperparameters to facilitate replication and future extensions.", "tldr": "This paper investigates quantum-inspired enhancements to self-supervised contrastive learning by replacing the projection head with variational quantum circuits and introducing quantum kernels as alternative similarity measures.", "keywords": ["Contrastive learning", "Self-supervised learning", "quantum meachine learning", "variational quantum circuits", "quantum kernels", "representation learning", "InfoNCE loss", "quantum fidelity"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0806b984c68ee2a286ff650fd9d9f5cf7c0561e2.pdf", "supplementary_material": "/attachment/21266d698aed002df3ded9191ff22a211450cf9e.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic empirical study investigating the integration of quantum components into a self-supervised contrastive learning (SimCLR) pipeline. The primary research question is whether replacing classical elements with quantum analogues—specifically, a Variational Quantum Circuit (VQC) with angle encoding as a projection head and a fixed quantum feature map (QFM) for similarity computation—yields a tangible performance benefit."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors performed experiments with replacing classical components with quantum circuits in the scenario of contrastive learning."}, "weaknesses": {"value": "It provides no contribution to the society and the reasons to cause such empirical results have been well analyzed and understood. The quantum circuits under angle encoding are inherently trucated Fourier series; the difference is that quantum circuits can have exponentially increasing spectrum span. This explains the observations in the experiment. The same goes to the quantum feature map. In summary, everything that is presented in this paper has been well understood. The paper also needs better sturcture layout."}, "questions": {"value": "No."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l55WShq2Ds", "forum": "slTQdWWQI9", "replyto": "slTQdWWQI9", "signatures": ["ICLR.cc/2026/Conference/Submission20372/Reviewer_uPfa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20372/Reviewer_uPfa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761231566228, "cdate": 1761231566228, "tmdate": 1762933825325, "mdate": 1762933825325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "D4LZVm70UW", "forum": "slTQdWWQI9", "replyto": "slTQdWWQI9", "signatures": ["ICLR.cc/2026/Conference/Submission20372/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20372/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763415829495, "cdate": 1763415829495, "tmdate": 1763415829495, "mdate": 1763415829495, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper primarily investigates the effectiveness of quantum projection heads and quantum kernels in contrastive learning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation behind this paper is clear."}, "weaknesses": {"value": "1. It is recommended that the authors include a flowchart or framework diagram of the algorithm proposed in the paper.\n\n2. Since the authors' conclusions are based on small-scale quantum systems, it is recommended that they verify them on larger-scale quantum systems (e.g. 20 qubits). Because in small-scale Hilbert spaces, quantum computing may not necessarily have an advantage [1].\n\n3. The paper uses a fixed quantum feature map in experiments, which is inappropriate. And inappropriate quantum feature maps tend to be inferior to that of classical kernels [2,3]. So the reviewer kindly suggest that the authors could use training embedding kernels [4] to conduct all the experiments.\n\n4. To demonstrate the practical performance of quantum machine learning algorithms, the reviewer kindly suggest the authors include experiments with relevant noisy quantum computers and noisy simulators.\n\n5. Reviewer are pleased to see the authors include SVM classifier in the experimental results.\n\n[1] Does provable absence of barren plateaus imply classical simulability?\n\n[2] Power of data in quantum machine learning.\n\n[3] The Inductive Bias of Quantum Kernels.\n\n[4] Training quantum embedding kernels on near-term quantum computers."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sg7r3Mdfid", "forum": "slTQdWWQI9", "replyto": "slTQdWWQI9", "signatures": ["ICLR.cc/2026/Conference/Submission20372/Reviewer_bgtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20372/Reviewer_bgtN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829436908, "cdate": 1761829436908, "tmdate": 1762933825011, "mdate": 1762933825011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the potential benefits of integrating quantum computing components into a modern self-supervised visual representation learning pipeline (SimCLR-style). The authors replace the classical projection head and similarity measure with quantum counterparts (VQCs and quantum kernels, respectively). The study is carefully controlled and benchmarked against classical MLP heads. The core finding is a \"null baseline\": under realistic NISQ-era constraints (small circuits), the quantum components provide no performance benefit over a well-tuned classical MLP. The authors also note that scaling quantum circuits quickly becomes computationally prohibitive in simulation. They conclude this baseline is valuable for guiding future research."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The study is meticulous in its experimental setup, providing a fair \"apples-to-apples\" comparison between classical and quantum components. This thoroughness is commendable.\nThe authors are transparent about their negative results, which adds to the paper's trustworthiness.\nEstablishing a \"null baseline\" is indeed valuable for the research community, as it prevents wasted effort and helps set a clear bar for future work to surpass.\nThe paper correctly identifies practical bottlenecks (e.g., simulation latency) that hinder research in this area."}, "weaknesses": {"value": "In all tested settings, the quantum variants did not outperform a tuned classical MLP head. This is the central finding, and it is a negative one. The paper fails to demonstrate any quantum advantage or even competitive performance, which is a significant weakness for a submission to ICLR.\nOnly small/shallow quantum circuits were feasible; larger circuits were too slow to simulate. While the authors correctly identify this as a bottleneck, it also means the study was unable to explore the regime where quantum advantage might actually exist. This makes the \"null result\" conclusion limited to the (already suspected) non-advantageous NISQ regime.\nThe quantum components explored are fairly standard (a basic VQC head, a fixed quantum kernel). The paper does not propose any new quantum techniques; it only tests existing ones in a new context and finds they do not work."}, "questions": {"value": "1.I suggest the authors state clearly whether any run or variant ever beat the MLP head, even marginally. If not, saying so explicitly is valuable to the community and reinforces the \"null\" finding.\n2.A brief quantitative note on how runtime explodes when increasing qubits/depth would help clarify the current scaling bottleneck.\n3.It would help to mention one promising next step (e.g., a trainable quantum feature map rather than a fixed kernel) so readers see a path forward.\n4. For completeness, listing the VQC head setup (qubit count, entanglement pattern, optimizer, LR, epochs) in an appendix would address fairness concerns"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wUlRA3NcR3", "forum": "slTQdWWQI9", "replyto": "slTQdWWQI9", "signatures": ["ICLR.cc/2026/Conference/Submission20372/Reviewer_ywA9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20372/Reviewer_ywA9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915999335, "cdate": 1761915999335, "tmdate": 1762933824038, "mdate": 1762933824038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors study whether plugging quantum components into a standard contrastive/self-supervised pipeline actually helps. Concretely, the authors take a SimCLR-style setup on STL-10 with a ResNet-18 encoder and replace the usual MLP projection head with (i) a variational quantum circuit (VQC) projection head, and (ii) a fixed quantum feature map whose pairwise fidelities are used as the similarity/kernel inside InfoNCE. The authors find several results, including tuned classical MLP heads that match or beat VQC heads; fidelity-based quantum kernels mostly behave like cosine similarity; and as soon as you increase qubits/depth, latency explodes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper controls for encoder, data pipeline, optimizer, and temperature, so comparisons between classical and quantum heads are fair. The authors also give full hyperparameters and environment details (Appendix A–C), which are suitable for reproducibility.\n\n2. The message “current, realistic quantum components do not help SimCLR on STL-10” is valuable for the community because it narrows the plausible claim space."}, "weaknesses": {"value": "1. The paper shows that (their) VQC heads and (their) quantum kernels don’t beat a classical MLP on STL-10. That is important as a benchmark, but it is also precisely what many people in quantum-ML would expect when the encoder is strong and the quantum budget is tiny. There is no new quantum architecture, no new training rule, no theory on when a quantum head should help, and no scaling law beyond informal latency comments. For ICLR, we usually want at least one of these. \n\n2. The authors conducted everything on STL-10, with ResNet-18, and SimCLR/InfoNCE. The authors mention CIFAR-10/100 in the supplementary, but there is no real multi-dataset narrative in the main paper. This makes it hard to generalize the null result to other modalities or to harder SSL settings.\n\n3. The VQC is angle-encoding + ring entanglement + shallow depth + simple multi-observable readout. That’s reasonable, but the paper then strongly concludes “VQC doesn’t help.” In reality, what they’ve shown is “this VQC, at this depth, with this readout, on this dataset, under analytic simulation, doesn’t help.” That’s weaker than the way the Discussion is written. \n\n4. Table 4 shows fidelities in the 0.08–0.19 off-diagonal range with small variance, and accuracy drops when nq or depth grows. But the paper doesn’t connect this to existing theory to explain that, if you do shallow-angle encoding on already-good features. As a result, the paper reads more like a description than an explanation.\n\n5. All the experiments are classically simulated. Although the authors note that latency is the main bottleneck (200–460 s/batch), the paper still frames it as “a useful near-term baseline.\""}, "questions": {"value": "1. Can the authors show a second dataset (e.g., CIFAR-100) in the main paper, not only in the supplement, using the same quantum heads, to prove the null result is not STL-10 specific?\n\n2. Can the authors try a data-reuploading VQC, or a hardware-efficient ansatz with mid-circuit measurements, to rule out “you picked an especially weak circuit”?\n\n3. Can the authors show even a tiny hardware run (4 qubits, 1 layer) to validate that the latency story is not simulation-only?\n\n4. Can the authors add a classical “heavy” head (e.g., 2–3× params) to show you are not just capacity-bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fGOdc3tGwa", "forum": "slTQdWWQI9", "replyto": "slTQdWWQI9", "signatures": ["ICLR.cc/2026/Conference/Submission20372/Reviewer_eEBm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20372/Reviewer_eEBm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920400001, "cdate": 1761920400001, "tmdate": 1762933822925, "mdate": 1762933822925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}