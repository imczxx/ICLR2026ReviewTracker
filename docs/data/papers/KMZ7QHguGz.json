{"id": "KMZ7QHguGz", "number": 22563, "cdate": 1758332898224, "mdate": 1763233278405, "content": {"title": "Automatically Advancing LLM Expertise in Technology Judgment", "abstract": "Large language models (LLMs) are rapidly becoming core tools for science, engineering, and innovation. Their promise lies not just in remembering facts, but in putting knowledge to work. Despite their impressive ability to answer increasingly difficult questions, it remains unclear whether LLMs truly use their knowledge when confronted with new and challenging tasks. We address this question with a patent classification task that requires deep conceptual understanding: distinguishing objectively different but semantically similar patents. To evaluate this approach, we introduce a challenging new benchmark of 1.3 million post-2015 computer science patent pairs, characterized by dense technical jargon and strategically complex writing. We find that LLMs often fail our benchmark and struggle to distinguish among semantically similar patents. To probe this failure, we introduce a novel framework that decomposes model errors into two sources: missing and unused knowledge. Our approach asks models to generate clarifying questions to improve their understanding, and then compares three settings: raw performance, self-answered questions, and externally supplied answers. This decomposition reveals that LLMs often possess the relevant knowledge internally but fail to deploy it, while a smaller share of errors arises from genuine knowledge gaps. We then ask whether the ability of models to construct a task-specific database of questions and answers differs across models. We find that smaller models generate simpler, broadly transferable questions, while larger models propose more complex but less generalizable ones. This suggests new strategies for combining strengths across models. Taken together, our findings highlight a critical limitation of current LLMs and their evaluation: models often know more than they can use. By shifting evaluation from recall of static facts to application of dynamic knowledge, our approach provides a more informative lens on model capabilities and opens a path toward building systems that better support technological discovery and innovation.", "tldr": "Using a new dataset of 1.3 million patent pairs, we test self-questioning as both a practical mechanism for automatically enhancing LLM comprehension of technologies and a diagnostic probe into how internal and external knowledge are organized.", "keywords": ["technology judgement", "LLM understanding", "benchmark", "self-questioning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f0ec69ceabf09e1773ae7538e95d3a69e4e21a3a.pdf", "supplementary_material": "/attachment/eadae7226e4509d9ba6f02d3740d059bb930ddec.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a benchmark that consists of 1.3M+ patents, where the task is: given a pair of patent abstracts, the LLM has to decide if they're the same or different. Patents are chosen because they're inherently different from each other (one cannot get a patent approved if it already exists), and are somewhat ambiguously worded, making the task not easy. Results are reported across the LLaMa 3.1 family of models, wherein the 8B parameter version struggles and the 405B manages to correctly judge pairs at close to a 90\\% accuracy rate. The paper investigates the errors made by LLMs on the benchmark to determine if they are due to 1) the models not reasoning enough about the given information or 2) the models not having enough information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The dataset is thorough (1.3M+ samples) and creative, featuring an out-of-the box domain\n- The paper is easy to understand"}, "weaknesses": {"value": "I think one limitation of the paper is that it mainly evaluates LLaMa 3.1 models, which are not among the top models, even among open-source ones. Still, the best LLaMa model reaches nearly 90\\% accuracy, and GPT-4o mini performs similarly. Given this, it seems likely that stronger models such as Qwen-32B or GPT-5 would exceed 90\\%, which raises questions about how challenging the benchmark remains and, in turn, about its broader interest to the community.\n\nIn general, is there a reason why abstracts rather than full texts are used? Because abstracts do not contain all the patent information, could it be the case that for some pairs, it is impossible to decide if they differ based on just abstract text alone?\n\nIn addition, the analysis might have a confounding factor. When the model is given scientific papers related to the patent, it also receives more tokens and does more processing. How would the results change if, instead of related papers, the model were given the full patent text (as Knowl_{ij} in Figure 3)?  It would be helpful to see examples where the scientific context improved performance and what information made the difference.\n\nFinally, if I understand correctly, in some cases the model is given the same patent. How is this handled? Is the text modified when patent_1 = patent_2 to make the question non-trivial?\n\n\nMinor:\n- line 249: Appendix should be capitalized\n- the plots seem a bit cluttered and information dense. In Figure 5 for example, perhaps some of the plots could be put in the appendix, which would leave room for bigger plots, axes and labels, and more information in the caption. Also could be beneficial: despining the plots, making the legend backgrounds transparent, and having one legend instead of two in the case of Figure 2 (because the legends are the same). Also: the individual captions under the plots (for example, Figure 1) could be removed in favor of a longer, more in depth general caption, like there is at the bottom of Figure 1."}, "questions": {"value": "1. How would the results change if GPT-5, Qwen32B or Claude Sonnet 4.5 were used? Because these models are more expensive to run, results on a subset of the benchmark could be significant here. \n\n2. How would results change if the full text of the patent was used and not just the abstracts? How would the self-answer/scientific analysis change if the patent text was used in place of $Knowl_{i,j}$?\n\n3. When scientific context does help reduce errors, would it be possible to get examples of the missing knowledge that helped the LLM come to a decision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N8EhY9HMZu", "forum": "KMZ7QHguGz", "replyto": "KMZ7QHguGz", "signatures": ["ICLR.cc/2026/Conference/Submission22563/Reviewer_PXH4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22563/Reviewer_PXH4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761422919546, "cdate": 1761422919546, "tmdate": 1762942281152, "mdate": 1762942281152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "ypTipuNeSI", "forum": "KMZ7QHguGz", "replyto": "KMZ7QHguGz", "signatures": ["ICLR.cc/2026/Conference/Submission22563/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22563/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763233277481, "cdate": 1763233277481, "tmdate": 1763233277481, "mdate": 1763233277481, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to assess whether LMs truly use their knowledge when confronted with new and challenging tasks. To address this problem, they collected a challenging new benchmark of 1.3 million CompSci patent pairs. With a novel framework that decomposes model erros into two sources (missing and unused knowledge), this proposed approach asks models to generate clarifying questions. This self-question framework highlights a critical limitation of current LLMs: models often know more than they can use."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The patent dataset is large, and would be useful for compsci researchers, including but not limited to LLM developers.  \n- The research question asking whether LLMs truly use their knowledge when confronted with new and challenging tasks is valuable."}, "weaknesses": {"value": "- While the research question (whether LLMs truly use their knowledge when confronted with new and challenging tasks) and the solution (examine with a dataset involving patent judgment) are both valuable, they do not seem to me to have the same scope for a paper. I highly recommend that the authors reconsider the scope of the research question in the introduction: instead of asking \"whether LLMs can enhance their own understanding\", ask \"whether LLMs can perform the technology judgment task that involves distinguishing patents that are near duplicates\". This would require rewriting the abstract and the introduction, etc., but the revised story will have a more specific scope and will therefore fit the empirical contribution better. Following this framing, the \"self-questioning\" can be established as a proposed solution. This should be compared to several baselines (e.g., Transformer-based classification, prompt-based classification). I think centering on the dataset is an easier way to improve this paper. (The alternative is to use additional datasets that answer the question \"whether LLMs can enhance their own understanding\" more thoroughly -- but given the vagueness of this question, I prefer the current recommendation to the authors.)  \n- Several other mismatches between the research question and the research evidence should be fixed by more careful wording. For example, \"how LLMs' internal/external knowledge are organized and transferred across model scales\" is not studied thoroughly enough in the experiments. The current experiment, on the other hand, studies \"how LLMs across different scales have knowledge about technology judgment\". Note how this subtle difference in choice of words reframes the research question."}, "questions": {"value": "Aside: A previous version of this paper was in my batch at another conference a few months ago. I see this version has made some edits, but still, this version is not good enough."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xGcICHVLFg", "forum": "KMZ7QHguGz", "replyto": "KMZ7QHguGz", "signatures": ["ICLR.cc/2026/Conference/Submission22563/Reviewer_GGwd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22563/Reviewer_GGwd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589460049, "cdate": 1761589460049, "tmdate": 1762942280944, "mdate": 1762942280944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new dataset of over 1.3 million close-but-distinct computer science patent pairs for patent classification task. Using this dataset, it investigates two error sources (missing or unused knowledge) by comparing performance across three settings: raw performance, self-answered questions, and externally supplied answers. The results show that LLMs often possess the relevant knowledge internally but fail to deploy it. Further analysis shows that smaller models generate simpler, broadly transferable questions, while larger models propose more complex but less generalizable ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed patent dataset appears highly valuable for LLM evaluation. As noted by the authors, patents are hard to read and understand, hard to distinguish from one another, and often strategically obfuscated. This makes them an important and valuable source within broad technological domains.\n\n- The idea that “a central aspect of understanding is differentiation” is interesting.\n\n- The dataset’s scale is impressive, encompassing more than 1.3 million patents."}, "weaknesses": {"value": "- My primary concern is that the current task formulation, based on binary classification, appears too simple. Several models, including Llama 3.1-405B, Qwen2.5-7B, Mistral-7B, and Llama 3.1-8B, already achieve close to 0.90 accuracy under simple zero-shot prompting (as reported in Appendix F). Some of these are not the latest models, suggesting the task may be insufficiently challenging and potentially very sensitive to randomness. It would strengthen the benchmark to explore more complex evaluation setups, such as requiring models to articulate the differences between two patents or to simulate expert-style examination reports. Including tasks across varying difficulty levels could make the benchmark both harder and more comprehensive, thereby increasing its practical utility.\n\n- The plots are confusing, particularly due to inconsistent and unclear y-axis labeling. Replacing labels such as “judge as the same patent” or “judge as the different patent” with standard performance metrics (e.g., accuracy) would greatly improve readability. Several other figures could also benefit from clearer organization and visual consistency.\n\n- In Figure 1(b), please include comparison data from non-patent documents. Without such context, it is difficult for readers to assess whether patents are similar.\n\n- The main body only reports results for Llama 3.1, while results for other models are relegated to the appendix. Results from one model are not convincing and I highly recommend moving other results to the main paper.\n\n- The conclusion that “LLMs do not know what they know” has been widely reported in prior work. While the presented results provide additional evidence to that, I am looking for something new and interesting."}, "questions": {"value": "- It may be worth exploring a setting without predefined question types, allowing models to freely generate questions.\n\n- In Appendix D, some examples include numbered elements such as “(18), (12), (14).” Could the authors clarify whether referenced evidence associated with these numbered components are included in the input? If so, does this substantially increase the context length, and how do the authors manage context window constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wVblO1GrXl", "forum": "KMZ7QHguGz", "replyto": "KMZ7QHguGz", "signatures": ["ICLR.cc/2026/Conference/Submission22563/Reviewer_1WND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22563/Reviewer_1WND"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635114729, "cdate": 1761635114729, "tmdate": 1762942280645, "mdate": 1762942280645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces (i) a benchmark and a training dataset for the evaluation and fine-tuning of large language models on the scientific-to-policy brief generation and (ii) a corpus of science-policy pairs through a multi-step filtering and in-context polishing pipeline, to fine-tune three open-source models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The task addressed in the paper is (as far as I see) original and scientifically relevant\n- The proposed taxonomy is well structured \n- The pipeline to define and build the dataset is well detailed \n- The experimental section comprises a large number of LLMs"}, "weaknesses": {"value": "The introductory motivation could be improved in the following phase  “LLMs struggle with policy brief generation” (page 1), which is used as a core motivation but is not contextualized with quantitative evidence or prior benchmarks. It would strengthen the introduction to include a preliminary reference or citation illustrating the current performance gap that motivates the work.\n\nThe selected venues (Nature Energy, Nature Climate Change, Nature Cities, Nature Sustainability, and JHSB) seem to focus on environmental and social sciences. While this is a valid and important focus, the approach’s generalizability to other domains, such as engineering, AI, or technological innovation, remains unclear. A discussion of cross-domain applicability would be helpful.\n\nThe reliance on an LLM-as-a-judge evaluation approach is both innovative and potentially fragile.\nSince the fine-tuned models and the evaluation models may share similar biases, the LLM-as-a-judge method risks circularity. The authors mention mitigation strategies in appendix , but this remains a critical methodological assumption that should be discussed more in depth.\n\nThe experimental section is large and full of analysis. Unfortunately, in the opinion of this reviewer, a more detailed set of comments about those numbers would have helped to gain more insights.\n\nSome figures (e.g., Fig. 1) could be more clearly annotated, and minor typographical issues persist. A tighter organization of tables and figures would improve readability. The acronym JHSB (Journal of Health and Social Behavior) appears in Figure 1b but is only defined later in the paper (page 23), which disrupts readability."}, "questions": {"value": "See previous box"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "e9GAQvG2BK", "forum": "KMZ7QHguGz", "replyto": "KMZ7QHguGz", "signatures": ["ICLR.cc/2026/Conference/Submission22563/Reviewer_CuVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22563/Reviewer_CuVx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919031138, "cdate": 1761919031138, "tmdate": 1762942280168, "mdate": 1762942280168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}