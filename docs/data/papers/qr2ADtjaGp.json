{"id": "qr2ADtjaGp", "number": 10337, "cdate": 1758167422176, "mdate": 1763021868461, "content": {"title": "Toward Robust In-Context Learning: Leveraging Out-of-distribution Proxies for Target Inaccessible Demonstration Retrieval", "abstract": "Although studies have demonstrated that Large Language Models (LLMs) can perform well on Out-of-Distribution (OOD) tasks, their advantage tends to diminish as the distribution shift becomes more severe. Consequently, researchers aim to retrieve distributionally similar and informative demonstrations from the available source domain to boost the inference capabilities of LLMs. However, in practical scenarios where the target domain is inaccessible, evaluating the unknown distribution is challenging, which indirectly impacts the quality of the selected demonstrations. To address this problem, we propose \\textbf{DOPA}, a demonstration search framework that incorporates an OOD proxy to approximate the inaccessible target domain and guide the retrieval process. Building on proxy-based evaluation, DOPA further introduces a Mahalanobis distance-based global diversity constraint to ensure sufficient diversity among the retrieved demonstrations. Experimental results on multiple LLMs and natural language understanding tasks demonstrate that DOPA effectively enhances robustness in OOD settings.", "tldr": "This paper proposes DOPA, a framework that leverages an OOD proxy and diversity constraints to improve demonstration retrieval and enhance LLM robustness in OOD tasks.", "keywords": ["In-Context Learning", "Out-of-distribution Proxies", "Demonstration Retrieval", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fb702feb77c3856607788ba4d9a812ee0aca300e.pdf", "supplementary_material": "/attachment/4633c149860d8bfde1cb3000590bfae6c1bb3e86.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles ICL under distribution shift when the target domain is inaccessible and proposes DOPA. It scores source examples by a log-perplexity ratio between the two proxies to approximate a density-ratio–style OOD score, then performs multi-granularity retrieval. Experiments on the OOD benchmark show consistent gains over baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The “target-inaccessible” setup is realistic; using a pair of proxies is a coherent way to approximate density ratios without target data.\n\n\n- Consistent improvements across tasks and models."}, "weaknesses": {"value": "- A core appeal of ICL is improving performance without additional model training. DOPA introduces an instruction-tuned source proxy and relies on dual perplexity scoring, which shifts the method toward a trained, retrieval-time pipeline rather than pure in-context use. The paper should justify this trade-off and more clearly position DOPA relative to standard “no-finetune” ICL.\n- The main components—proxy scoring akin to a density ratio, similarity-based ranking, and diversity control (Mahalanobis)—are each established (as cited in the paper). The paper should sharpen what is conceptually new and clearly delineate how its OOD-proxy estimation differs from closely related prior work [1], beyond implementation details.\n\n\n- Please justify the realism of the assumptions in the theorem in OOD settings and clarify whether they are standard in prior work. While KL–ball assumptions are common in distributionally-robust testing/estimation, global pointwise density lower bounds are much stronger and may fail on high-dimensional supports or heavy-tailed data.\nMore importantly, the proof appears incorrect. From $D\\_{KL}(P_t\\Vert P_t^{\\text{proxy}})=\\sum_x P_t(x)\\log\\frac{P_t(x)}{P_t^{\\text{proxy}}(x)}\\le\\varepsilon_t$, the manuscript infers a pointwise two-sided bound\n $\\big|\\log\\frac{P_t(x)}{P_t^{\\text{proxy}}(x)}\\big|\\le \\varepsilon_t/m_t$\n by contradiction using $P_t(x)\\ge m_t$. This step ignores that the summands $P_t(x)\\log\\frac{P_t(x)}{P_t^{\\text{proxy}}(x)}$ can be negative; hence a large negative log-ratio at some (x) does not contradict a small average KL. As written, the key inequality does not follow, so the theorem’s final bound is not established.\n\n\n[1]Zhang et al. Your finetuned large language model is already a powerful out-of-distribution detector."}, "questions": {"value": "- Do you train the same model to serve as the source-domain proxy? If so, how is this handled for closed-source models such as GPT-4 and GPT-3.5, which cannot be fine-tuned directly? Please clarify how the proxy is implemented or approximated in these cases. Also, please provide the finetuning curves.\n\n\n- How do you guarantee that instruction-tuning the source proxy uses only source-domain data (no leakage of target distribution or template artifacts)? Please describe safeguards, data splits, and checks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9y6k8elwUh", "forum": "qr2ADtjaGp", "replyto": "qr2ADtjaGp", "signatures": ["ICLR.cc/2026/Conference/Submission10337/Reviewer_6vej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10337/Reviewer_6vej"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766012517, "cdate": 1761766012517, "tmdate": 1762921670397, "mdate": 1762921670397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "BzBWfFMuTR", "forum": "qr2ADtjaGp", "replyto": "qr2ADtjaGp", "signatures": ["ICLR.cc/2026/Conference/Submission10337/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10337/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763021867816, "cdate": 1763021867816, "tmdate": 1763021867816, "mdate": 1763021867816, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DOPA (Demonstration Optimization via Proxy Assessment), a framework designed to improve the robustness of Large Language Models under Out-of-Distribution conditions. The core idea is to calculate the OOD score by the divergence between models' outputs of seen and unseen given data. The similarity and diversity (Mahalanobis Distance) are considered to search for  demonstrations as well."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Reasonable motivation and intuition of key modules. \n- The core idea of OOD score calculation is model-agnostic, which could motivate similar methods in other OOD approaches and applications. \n- The bound provided is pure and mathematically elegant."}, "weaknesses": {"value": "- About DOPA's generalization and the model-agnostic method. The models have sparse coverage, encompassing multiple model sizes. However, each size uses different architectures, lacking an ablation about scales on a fixed series, especially those larger than 7B. \n- The limitation of vaccum bound is not discussed. $m_{\\cdot}$ is the lower bound larger than 0, and the divergence's upper bound $\\epsilon$ is combined as $\\epsilon/m_{\\cdot}$, an upper bound of an upper-unlimited value. The cases do happen according to the confidence and perplexity vary that $m_{\\cdot}$ is near zero. \n- The table design, e.g., 1pt row spacing and over-simplified entities, makes it hard to read, even though there's so much space available."}, "questions": {"value": "- In some cases, Random and KNN beat other latest methods. Are there any reasons behind? How about an ablation grid of scale on a fixed LLM series? \n- Uniform proxy simplifies the analyses, but how about practically specific cases and general prior assumptions that make the bound tight? Especially when using proof by contradiction in the Appendix, the derivation of the conclusion appears remarkably straightforward. Is there any more quantitative upper bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8AOj5igHtc", "forum": "qr2ADtjaGp", "replyto": "qr2ADtjaGp", "signatures": ["ICLR.cc/2026/Conference/Submission10337/Reviewer_1bSD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10337/Reviewer_1bSD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791994880, "cdate": 1761791994880, "tmdate": 1762921669617, "mdate": 1762921669617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles robust in-context learning (ICL) under out-of-distribution (OOD) shift when the target domain is inaccessible. It proposes DOPA, a demonstration retrieval framework that uses an OOD proxy to approximate the target distribution and guide selection from a source pool. DOPA scores source samples with the proxy signal, then enforces a global diversity constraint via (Mahalanobis) distance to avoid redundancy and proxy-induced myopia. Experiments across multiple LLMs (e.g., LLaMA-3.2-3B, Qwen-1.7B) and tasks (SA, TD, NLI, NER) show consistent gains over KNN and several baselines. Sensitivity studies vary the candidate pool size k and demonstration count N, and visualizations (KDE/energy, t-SNE) plus a small case study suggest proxy-selected demos are behaviorally closer to the target while remaining diverse."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1 Addresses the realistic “target-inaccessible” regime, where many ICL retrieval methods quietly assume access to target data.  \n2 Broad evaluation: Multiple tasks and two model families; ablations on k and N; qualitative analyses supporting the proxy’s behavioral alignment and the value of diversity."}, "weaknesses": {"value": "1 It’s not fully clear how the OOD proxy is obtained, calibrated, and validated without any target access.  \n2 The experimental part focuses on in-context learning, however, the compared baselines only involve several baselines that are not very recent. The more complex ICL methods should be considered to improve comprehensiveness.\n3 Scope of tasks: All are NLU-style classification. It remains unclear whether benefits hold for generation (reasoning, QA, code)"}, "questions": {"value": "Do results transfer to generation tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ksHWg4eqG4", "forum": "qr2ADtjaGp", "replyto": "qr2ADtjaGp", "signatures": ["ICLR.cc/2026/Conference/Submission10337/Reviewer_hZX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10337/Reviewer_hZX7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023915780, "cdate": 1762023915780, "tmdate": 1762921669237, "mdate": 1762921669237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}