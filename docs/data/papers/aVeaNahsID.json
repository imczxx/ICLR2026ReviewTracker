{"id": "aVeaNahsID", "number": 10466, "cdate": 1758172516451, "mdate": 1759897649067, "content": {"title": "Zephyrus: An Agentic Framework for Weather Science", "abstract": "Foundation models for weather science are pre-trained on vast amounts of structured numerical data and outperform traditional weather forecasting systems. However, these models lack language-based reasoning capabilities, limiting their utility in interactive scientific workflows. Large language models (LLMs) excel at understanding and generating text but cannot reason about high-dimensional meteorological datasets. We bridge this gap by building a novel agentic framework for weather science. Our framework includes a Python code-based environment for agents (ZephyrusWorld) to interact with weather data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying for geographical masks from natural language, weather forecasting, and climate simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather agent that iteratively analyzes weather datasets, observes results, and refines its approach through conversational feedback loops. We accompany the agent with a new benchmark, ZephyrusBench, with a scalable data generation pipeline that constructs diverse question-answer pairs across weather-related tasks, from basic lookups to advanced forecasting, extreme event detection, and counterfactual reasoning. Experiments on this benchmark demonstrate the strong performance of Zephyrus agents over text-only baselines, outperforming them by up to 35 percentage points in correctness. However, on harder tasks, Zephyrus performs similarly to text-only baselines, highlighting the challenging nature of our benchmark and suggesting promising directions for future work.", "tldr": "We built an AI weather assistant that lets scientists explore meteorological data through natural conversation, and created a benchmark to evaluate LLMs for weather science.", "keywords": ["Agents", "Large Language Models", "Weather Science", "Code Generation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a962f9a156c3937593fcac901b620b6498030d02.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This is a very. nice study that creates two contributions that are novel and difficult: the first (zephyrusworld) is a system for evaluating weather focused tasks by LLM agents. This is difficult because of the data compute nature of these questions. The second is the\nbenchmark built on ERA5 combining questions from humans and synthetic creation to cover a wide range of weather probelms with varying difficulty. I'm impressed especially with the hard questions-- this goes beyond what is usually done. Graduate students wrote python code to solve the problems. The authors used these together to evaluate a set of frontier LLMs on the tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The strength is the difficulty of the tasks that are created both for the execution environment and for the benchmark itself. Both of these are impressive. These problems are not easy to set up or to create. Expertise is required for both. It is this difficulty that differentiates this paper from the large number of  other benchmarking papers.  I also like the creativity of the tasks--particularly meteorological claim verification, looking for data to support the claims from weather reports.\n\nThe evaluation metrics chosen are quantitative and rigorous (eg not employing LLM as judges etc)\n\nThe paper also contains a substantive supplementary information that gives lots of detail on tasks, performance breakdowns, etc."}, "weaknesses": {"value": "the number of different tasks is pretty limited -- 46 total of which 30 are synthetic. Yet I acknowledge how hard it is to create tasks and so this is an understandable weakness. I think there is a lot of creativity in the question generation.\n\nAnother weakness is that the benchmark was built to a large extent for teh set of tools that the LLM has access to. Thus I can't tell how real the results are: what if the LLM were driving a real GCM, would the results be different? What matters for the quality of ther esults?\n\n10 or so of the tasks are yes/no questions -- evaluation is much easier here, one can just guess ;-) based on climolotagical prior\n\nBaselines aren't given other than the raw text LLM. What about a LLM given access only to climatology? ie to what extent is performance due to memorized patterns versus real skill with data analysis"}, "questions": {"value": "I dont understand how your choice of solvers (GCM simulator, geolocator,Forecaster) affect the scores on the tasks. Which of these were important for which tasks? \n\nThere could be more ablation studies showing why the scores are what they are. What if you delete tools? \n\nStronger baselines -- What about a LLM given access only to climatology? ie to what extent is performance due to memorized patterns versus real skill with data analysis. Are there other good baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WfMtUuuePn", "forum": "aVeaNahsID", "replyto": "aVeaNahsID", "signatures": ["ICLR.cc/2026/Conference/Submission10466/Reviewer_nucp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10466/Reviewer_nucp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883536250, "cdate": 1761883536250, "tmdate": 1762921763533, "mdate": 1762921763533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Zephyrus, an “agentic” framework designed to enable large language models (LLMs) to interact programmatically with meteorological data. The system includes tools to directly interact with weather and geographic data, and forecasting and simulator models.\n\nThe authors also curate a weather reasoning benchmark with 2062 question answer pairs in 46 meteorological tasks. The authors show that their system Zephyrus-Reflective which allows observations and reflections is better than Zephyrus-Direct which only allows the system to execute in one step. Both these systems are better than a text-only baseline on easy and medium tasks but do not show improvement on hard tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper clearly articulates a gap between numerical weather foundation models and language-based reasoning systems. Positioning the work at this intersection is timely and relevant, especially given the rising interest in LLM-augmented science workflows.\n\nS2. The paper presents a very reasonable set of tools and agentic systems (Zephyrus-Reflective and Zephyrus-Direct) for reasoning over meteorological questions and data and is a solid first step in this direction.\n\nS3. The dataset is well-constructed and leverages a good mix of human expertise and LLM aided construction."}, "weaknesses": {"value": "W1.  The authors do not report error bars or statistical significance. This makes it hard to assess whether observed performance differences are meaningful or consistent across runs.\n\nW2. The observation that adding tools leads to better performance or allowing observations and actions in a multiple steps improves performance is not novel. From my perspective, the main contribution of the work is the incorporation of relevant and helpful tools in the agentic workflow to begin with. Thus, it would be more insightful to see how different baseline tools or the inclusion/exclusion of tools impact performance in Zephyrus-Reflective. \n\nW3. The analysis could be strengthened to understand why the agent succeeds or fails. Given that models all perform roughly equal on the harder subset, are we limited by the tool? agentic function calling? or the LLM reasoning? Such an analysis could make more clear what is needed to improve."}, "questions": {"value": "What is the prompt for the text-only baseline?\n\nCan you explain why text-only seems to be better in Table 16, 17, and 18? The performance trends also appear inconsistent between different models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xDLiDSDPsf", "forum": "aVeaNahsID", "replyto": "aVeaNahsID", "signatures": ["ICLR.cc/2026/Conference/Submission10466/Reviewer_gtMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10466/Reviewer_gtMv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938200364, "cdate": 1761938200364, "tmdate": 1762921763026, "mdate": 1762921763026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an agentic framework - Zephyrus - to enable LLMs to interact with weather data via many datasets/tools to address weather science tasks. They also introduces a benchmark (ZephyrusBench) to evaluate how well frontier LLMs can assist with whether science queries/tasks both directly and within their agentic framework. Their agentic framework includes access to different tools to address weather queries including observation data (from ERA5 via WeatherBench2), Geolocator, forecasting models, simulation models, as well as a practical and fast code execution environment. \n\nTo create the ZephyrusBench benchmark, they had 2 approaches one that involved human annotation and another that used a semi-synthetic pipeline to generate queries. For the human generated, graduate students come up with 15 task types (5 types each in Easy, Medium, Hard category) and associated question template and solution code. They then substitute the variables in the templates with different parameters e.g. location etc. to generate a wide number (~1.8k) of queries. For the semi-synthetic pipeline they extract claims from papers to formulate queries/tasks and create 31 task types (30 medium, 1 hard) and similar to the human version, substitute variables to create ~300 questions.\n\nThey compare performance of the frontier-LLMs in 3 settings: (1) text-only: here the LLM does not get  access to any any external resources/tools, (2) Zephyrus-direct: here the LLM gets to generate the entire code for the solution in a single response turn, but is allowed to get execution feedback and correct errors upto 5 times. (3) Zephyrus-reflective: here the LLM gets to build out the code solution in chunks in a multi-turn fashion, getting the execution outputs for each code chunk which would allow the LLM to plan and break the task into components (and they allow error correction for upto 20 times)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Good weather science agentic framework contribution: The paper introduces a practical agentic framework for weather science that integrates a number of different tools for getting observation data, Geolocator, forecasting models, simulation models, as well as a fast execution environment to run model generated code to compute the solution.\n\n* Interesting and large benchmark to evaluate weather science agents. The paper has a thoughtful approach to curating a set of 46 tasks /templates and over 2100 questions. They used both human annotation to generate queries and solutions, and also an interesting semi-synthetic pipeline to generate some of the questions (see summary).\n\n* Their evaluation set up to compare frontier LLMs without access to external resources, and with access to their agentic framework under different computational resources settings seems sound and the results are meaningful clearly highlighting the value of their dataset and the agentic framework.\n\n* the paper is written well and is easy to follow."}, "weaknesses": {"value": "1. The benchmark creation has humans generate templates and task types to enable semi-automated creation of questions (with suitable solutions). I wonder if this perhaps reduces the diversity that one might observe in realistic settings which may deviate substantially from the templates,\n\n2. The results could be analyzed a few additional ways e.g. based on synthetic tasks and human tasks, and also by 2-3 task-types to see if these chracterizations of the benchmark can provide more insights.\n\n3. It's not entirely clear that there is much of a difference between Zephyris-direct and Zephyrus-reflective agentic settings. Perhaps the small difference could be that Zephyrus-direct gets to make fewer model queries and lower amount of error correction. It's not exactly a major weakness but a more interesting experiment would be to see if the agentic environment would infact benefit from planning as part of the agentic framework or if current \"reasoning\"/\"thinking\" models are capable of doing the reasoning fully by themselves.  \n\n4. It would be nice to see at least one open source LLM performance in the agentic framework."}, "questions": {"value": "Please address the weaknesses.\n\n1. Since the questions are templated based on tasks, do you see similar performance on most questions of a task type?\n\n2. Can you separate evaluation for synthetic queries vs human queries.\n\n3. Having built an interesting semi-synthetic task/query generation pipeline, why not generate more synthetic tasks/questions? were there any issues? Is there value in having more queries?\n\n4. Related to weakness-3. It appears that there may not be quite as much of a difference between the Zephyrus-direct and Zephyrus-reflective agents and the small difference we see may be attributed to differences in # of LLM queries and error-corrections allowed in both settings. So, question: what if you gave Zephyrus-direct ~100 error correction attempts (to compensate both increasing LLM queries and opportunities for error feedback), would it be on-par or better than Zephyrus-reflective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rcxtiKaCst", "forum": "aVeaNahsID", "replyto": "aVeaNahsID", "signatures": ["ICLR.cc/2026/Conference/Submission10466/Reviewer_M2k6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10466/Reviewer_M2k6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071332344, "cdate": 1762071332344, "tmdate": 1762921762708, "mdate": 1762921762708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Zephyrus, an AI agent that can perform tasks in weather science such as identifying weather events from data, an API / code execution server and a new benchmark.\n\nThe LLM based agent is connected to a code execution server, that provides access to several weather science related APIs.\nThe agent uses either single step code generation/execution or multistep, with several execution / result steps before producing the result\n\nThe benchmark is produced from two types of task templates: \n\n* manual (grad student) written pairs of template + reference implementation\n* semi-synthetic: mines patterns from weather related texts to produce templates and solution code. Templates are human verified\n\nActual benchmark examples are generated by setting variables such as time and location in the \ntemplates.\n\nEvaluation is done conditional on the answer type, for example locations are resolved and compared with earth movers distance. Text based answer are compared after deconstruction into sets of claims.\n\nExperimental results confirm, as expected, that allowing several agentic steps improves results and that any access to code execution is much better than a non-agentic baseline. An interesting outcome is that this difference gets smaller as the tasks get harder."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Significant contributions: Agent, Framework + Benchmark\n\nOverall well written paper with good related work section"}, "weaknesses": {"value": "While the overall number of examples in the benchmark is over 2k, these are based on only 46 templates. It is not clear to this reviewer whether having several examples per task improves evaluation strength.\n\nGiven that this work evaluates a complex agentic system, it would be interesting to see more in-depth analysis of the behaviour and comparison between common problems between models\n\nGiven that the number of examples is not too high, evaluating top line models like GPT5 or o3, Claude Opus or Gemini Pro would be interesting and not too cost intensive."}, "questions": {"value": "From what I understand, all problems for a template can be solved by a similar generated code with filled in variables. Does this mean that the performance of the evaluated models correlated strongly within a template cluster?\n\nCan you provide more statistics on how the APIs were used? Which problems occured, if code errors were correctly fixed from the feedback, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v2ou6ryt5z", "forum": "aVeaNahsID", "replyto": "aVeaNahsID", "signatures": ["ICLR.cc/2026/Conference/Submission10466/Reviewer_GNmT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10466/Reviewer_GNmT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185470746, "cdate": 1762185470746, "tmdate": 1762921762243, "mdate": 1762921762243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}