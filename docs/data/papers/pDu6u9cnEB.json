{"id": "pDu6u9cnEB", "number": 9210, "cdate": 1758115287631, "mdate": 1759897737573, "content": {"title": "Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images", "abstract": "This paper presents Omni-View, which extends the unified multimodal understanding and generation to 3D scenes based on multiview images, exploring the principle that ``generation facilitates understanding\". Consisting of understanding model, texture module, and geometry module, Omni-View jointly models scene understanding, novel view synthesis, and geometry estimation, enabling synergistic interaction between 3D scene understanding and generation tasks. By design, it leverages the spatiotemporal modeling capabilities of its texture module responsible for appearance synthesis, alongside the explicit geometric constraints provided by its dedicated geometry module, thereby enriching the model’s holistic understanding of 3D scenes. Trained with a two-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the VSI-Bench benchmark, outperforming existing specialized 3D understanding models, while simultaneously delivering strong performance in both novel view synthesis and 3D scene generation.", "tldr": "Generation helps Understanding in 3d scene.", "keywords": ["unified model; generation helps understanding; 3d scene understanding; novel view synthesis"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/512bd5286a484d79dd3d51448605df72b6424825.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Omni-View, a unified 3D scene understanding and generation model based on multi-view imagery, exploring the principle of \"generation promotes understanding.\" The model, composed of an understanding module, a texture module, and a geometry module, jointly models scene understanding, novel view synthesis, and geometry estimation. Using a two-stage training strategy, Omni-View achieved a state-of-the-art score of 55.4 on the VSI-Bench, surpassing existing dedicated 3D understanding models while also performing well in novel view synthesis and scene generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Unified modeling is novel: This is the first systematic exploration of the \"generation-driven understanding\" mechanism in 3D scenes, which is both inspiring and forward-looking.\n\nRational modular design: The generation module is split into texture and geometry components, modeling appearance and structure respectively, effectively improving understanding capabilities.\n\nEffective training strategy: The two-stage training (unified training + generation fine-tuning) balances understanding and generation performance, and the D2S mechanism improves robustness.\n\nComprehensive experiments: The effectiveness of the method is verified on multiple 3D understanding, spatial reasoning, and generation tasks, with results significantly outperforming existing unified models.\n\nNo 3D input required: Relying solely on multi-view images, this improves the model's practicality and generalization capabilities."}, "weaknesses": {"value": "Weak theoretical analysis: Although \"generation promotes understanding\" has been proposed, there is a lack of theoretical or interpretable analysis of its underlying mechanisms.\n\nGeneration quality still has room for improvement: Despite leading in PSNR/SSIM, inter-frame consistency under large viewpoint variations remains suboptimal (see Appendix visualization).\n\nThe geometry module relies on synthetic data: The depth map is synthesized by Voyager, which may limit the realism and accuracy of geometry predictions.\n\nLimited long sequence generation capability: The model currently does not support long sequence scene generation, limiting its application in open-world scenarios.\n\nThere is still a gap compared to state-of-the-art dedicated models: In particular, in the 3D grounding task, there is still a significant gap compared to methods that rely on 3D input.Strengthen theoretical analysis or visual explanation of the \"generation promotes understanding\" mechanism;\n\nCompare with state-of-the-art methods on more 3D grounding tasks and analyze the sources of the gap."}, "questions": {"value": "Strengthen theoretical analysis or visual explanation of the \"generation promotes understanding\" mechanism;\n\nCompare with state-of-the-art methods on more 3D grounding tasks and analyze the sources of the gap."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cwrqwh8XP2", "forum": "pDu6u9cnEB", "replyto": "pDu6u9cnEB", "signatures": ["ICLR.cc/2026/Conference/Submission9210/Reviewer_GSip"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9210/Reviewer_GSip"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637177675, "cdate": 1761637177675, "tmdate": 1762920875489, "mdate": 1762920875489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Omni-View, a unified model for 3D scene understanding and generation that explicitly investigates the hypothesis that “generation facilitates understanding.” The contributions of the paper are:\n1. Unified architecture integrating 3D scene understanding and generation, composed of three main components:\n1.1 Understanding model (for spatial reasoning and QA)\n1.2 Texture module (for novel view synthesis)\n1.3 Geometry module (for depth and pose estimation)\n2. Proposed a novel two-stage training strategy:\n2.1 Jointly trains understanding and generation to encourage mutual benefits through geometry and spatiotemporal modeling.\n2.2 Fine-tunes generation with RGB-Depth-Pose joint learning for better geometric consistency.\n3. Empirical validation showing state-of-the-art (SOTA) performance on the VSI-Bench (score 55.4), outperforming both specialized and unified 3D models in reasoning tasks.\nOverall, Omni-View demonstrates how generative modeling (novel view synthesis, geometry estimation) can enhance 3D reasoning, localization, and understanding—a conceptually elegant and empirically supported contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear intuition and solid empirical validation.\nThe paper builds upon a clear and intuitive idea — that generation can facilitate understanding — and the overall logic is easy to follow. Quantitative results across multiple benchmarks convincingly demonstrate the benefits of the proposed design, especially in spatial reasoning and novel view synthesis.\n2. Architectural innovation.\nBy decomposing the generation process into texture and geometry modules, the authors present a meaningful and modular architecture that captures both appearance and structure. This decomposition aligns well with human visual reasoning and can be viewed as an innovative contribution for the community.\n3. Comprehensive ablation studies.\nThe ablation results thoroughly verify the contributions of the proposed contributions. These analyses effectively demonstrate that each component contributes to the final understanding and reasoning performance."}, "weaknesses": {"value": "1. The qualitative results in the appendix are sparse, and there are no depth estimation visualizations or broader test cases. This makes it difficult to verify the model’s generalization and effectiveness beyond the reported metrics. For instance, the quality and consistency of metric-scale prediction from the geometry module remain uncertain — the reported results could be influenced by selective visualization or data bias, since the paper lacks convincing examples that demonstrate accurate geometric reasoning across varied real-world scenes.\n2. The technical details provided for training, implementation, and comparison setups are relatively limited. Without clearer supplementary material (e.g., dataset statistics, architecture specifics, or convergence behavior), it is challenging to fully reproduce the reported results or assess robustness under different conditions.\n3. The absence of released code or live demonstrations restricts the ability of other researchers to validate or extend this work. Although acceptable for review, the paper would be strengthened by open-sourcing its checkpoints or providing additional evaluation on long-range world generation and 3D visual grounding."}, "questions": {"value": "1. Could the authors provide qualitative results of the geometry module’s depth estimation, camera pose estimation with metric-scale? Without any depth estimation results or broader test cases, it is difficult to assess whether the geometry module truly learns meaningful 3D structure rather than overfitting to training priors, without any generalizability.\n2. How well does Omni-View generalize to unseen or real-world multi-view scenes, rather than the well captured ones in the appendix? Have the authors tested its performance to verify the robustness and effectiveness of the learned spatial reasoning?\n3. Could the authors clarify key implementation and training details—such as dataset splits, optimizer configurations, training epochs, and computational cost—to ensure reproducibility? Including more specifics or releasing supplementary materials would improve transparency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no ethics concern from the reviewer's side."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2fnzQztzsg", "forum": "pDu6u9cnEB", "replyto": "pDu6u9cnEB", "signatures": ["ICLR.cc/2026/Conference/Submission9210/Reviewer_U2JG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9210/Reviewer_U2JG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954532149, "cdate": 1761954532149, "tmdate": 1762920875107, "mdate": 1762920875107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper proposes Omni-View, a unified multimodal model that jointly performs 3D scene understanding, geometry estimation, and novel view synthesis from multiview images, based on the principle that generation facilitates understanding.\n\n- This paper introduces a dual-path architecture consisting of a texture module (appearance generation) and a geometry module (depth/pose estimation), enabling bidirectional synergy between generative and understanding tasks.\n\n- This paper uses a two-stage training strategy where joint training enhances understanding via generative signals, followed by refinement for high-quality 3D scene generation; achieves state-of-the-art performance on VSI-Bench and strong results in NVS and 3D Q&A."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper demonstrates that generative 3D tasks (novel view synthesis, geometry estimation) can actively enhance 3D scene understanding, rather than being separate objectives.\n\n- This paper has a unified architecture for 3D reasoning, with separate texture and geometry modules allow complementary learning of appearance and spatial structure, leading to better localization, spatial reasoning, and depth-aware Q&A.\n\n- This paper outperforms specialized models in 3D understanding benchmarks while maintaining competitive NVS and scene generation performance, closing the gap between multimodal understanding and 3D generative models.\n\n- A systematically organized evaluation and ablation study would strengthen the credibility of this paper."}, "weaknesses": {"value": "- It would be beneficial to include a diagram that more precisely illustrates the functionality of each module and the architecture, compared to the current version.\n\n- Additionally, visualizing 3D scene understanding / spatial reasoning / NVS from a single view as a video could also be an effective way to present the capabilities of the system.\n\n- Is there a reason why you refer to Texture Module and Geometry Module in the equations (e.g., (eq. 1), (eq. 2)) without using italics? Also, I believe writing them as TextureModule and GeometryModule (without a space) would improve readability and be more suitable for mathematical notation."}, "questions": {"value": "Mentioned in the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ez231SW9PJ", "forum": "pDu6u9cnEB", "replyto": "pDu6u9cnEB", "signatures": ["ICLR.cc/2026/Conference/Submission9210/Reviewer_Lkur"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9210/Reviewer_Lkur"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960982970, "cdate": 1761960982970, "tmdate": 1762920874674, "mdate": 1762920874674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Omni-View, a unified model for 3D scene understanding and generation from multiview images that tests the “generation facilitates understanding” hypothesis. Built on Bagel, it splits generation into a texture module (flow matching with Plücker pose encoding, autoregressive NVS) and a geometry module (depth and camera pose via flow matching with cross-attention to understanding features). A two-stage training recipe first jointly trains understanding/texture/geometry with a dense-to-sparse curriculum, then fine-tunes generation with RGB-Depth-Pose joint learning. Omni-View achieves SOTA on VSI-Bench (55.4), improves QA/localization versus unified baselines without 3D inputs, and delivers strong NVS/scene generation results on Re10k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a unified 3D understanding–generation framework that cleanly separates texture and geometry, a simple yet original design that operationalizes “generation facilitates understanding.”\n- The two-stage recipe with dense-to-sparse curriculum and autoregressive NVS is well-motivated, technically sound, and shows careful loss design and gradient routing to benefit the understanding model.\n- Writing is clear and structured, with concrete training details, datasets, metrics, and ablations that isolate the contribution of each module and training choice.\n- The empirical significance is strong, with SOTA results on VSI-Bench and competitive 3D QA/localization, plus solid NVS and scene generation metrics, demonstrating broad impact across 3D reasoning and generation."}, "weaknesses": {"value": "- Limited novelty relative to prior unified frameworks (Bagel, VILA-U, BLIP3o, Harmon)  \nThe core idea of leveraging generation to aid understanding has precedents in 2D unified models and recent 3D works that inject reconstruction priors (e.g., Ross3D; VG-LLM/Spatial-MLLM via VGGT features). The split into texture vs. geometry resembles established “appearance vs. structure” decouplings in 3D pipelines (e.g., ViewCrafter, Voyager). Clarify what is fundamentally new beyond integrating these pieces within Bagel, and compare to a “single-branch with multi-heads” backbone.\n\n- Ambiguity in camera control and absolute metric grounding   \nThe paper reports strong perceptual metrics but acknowledges difficulty in precise camera control and absolute depth scale. Because the gains on VSI categories like Abs. Dist. hinge on metric grounding, add analyses: scale consistency across scenes, depth-scale calibration via known baselines, and camera-pose accuracy vs. ground truth under diverse motions.\n\n- Dataset overlap and generalization concerns  \nThough the authors state they avoid using understanding images for generation training, several datasets share scene domains with Re10k-like indoor content, risking leakage of priors. Please report cross-dataset generalization (e.g., ScanNet -> Replica, RealEstate10K -> ACID/CO3D subsets) to support robustness claims.\n\n- Incomplete ablations on design choices and routing  \nThe geometry module conditions only on the last-layer texture latent and uses cross-attention to the understanding model. Test alternatives: multi-scale latents, earlier-layer features, and gating that controls gradient flow to avoid potential interference. Provide compute/latency breakdowns for stage 1 vs. stage 2, and show sensitivity to $\\lambda_{geo}$, pose-query design, and Plücker vs. other pose encodings."}, "questions": {"value": "- Clarify the novelty beyond architectural decoupling: In what ways is the texture/geometry split more than a clean engineering separation compared to prior “appearance vs. structure” decouplings (e.g., ViewCrafter, Voyager) and unified frameworks (BAGEL, VILA-U, BLIP3o, Harmon)? Could you provide a controlled comparison to a single-branch generator with two prediction heads (texture, geometry) at equal parameter count?\n- The ablations show AR improves spatiotemporal reasoning. Can you report exposure-bias analyses at inference time, e.g., teacher-forcing vs. free-running rollouts? Does diffusion forcing mitigate compounding errors, and how does performance vary with rollout length (8/16/32 frames)? Have you tested scheduled sampling or token-level AR only on camera poses while keeping texture bidirectional?\n- Provide qualitative and quantitative failure analyses: cases where geometry improves understanding but harms texture fidelity (and vice versa), per-category VSI error tied to pose/depth errors, and sensitivity to large viewpoint changes where you noted inconsistencies. Would integrating a small, explicit 3D proxy (e.g., Gaussians or sparse point clouds) at training but not inference close these gaps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0U5CuZzdRY", "forum": "pDu6u9cnEB", "replyto": "pDu6u9cnEB", "signatures": ["ICLR.cc/2026/Conference/Submission9210/Reviewer_Meww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9210/Reviewer_Meww"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090475762, "cdate": 1762090475762, "tmdate": 1762920874363, "mdate": 1762920874363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}