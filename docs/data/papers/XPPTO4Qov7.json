{"id": "XPPTO4Qov7", "number": 768, "cdate": 1756817517114, "mdate": 1759898242970, "content": {"title": "Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning", "abstract": "Referring expression understanding in remote sensing poses unique challenges, as it requires reasoning over complex object–context relationships. While supervised fine-tuning (SFT) on multimodal large language models achieves strong performance with massive labeled datasets, they struggle in data-scarce scenarios, leading to poor generalization. To address this limitation, we propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm for few-shot geospatial referring. Geo-R1 enforces the model to first generate explicit, interpretable reasoning chains that decompose referring expressions, and then leverage these rationales to localize target objects. This \"reason first, then act\" process enables the model to make more effective use of limited annotations, enhances generalization, and provides interpretability. We validate Geo-R1 on three carefully designed few-shot geospatial referring benchmarks, where our model consistently and substantially outperforms SFT baselines. It also demonstrates strong cross-dataset generalization, highlighting its robustness.", "tldr": "Improving Few-Shot Geospatial Referring with Reinforcement Fine-Tuning", "keywords": ["Remote Sensing VLM", "Few-shot Learning", "Referring Expression Task"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e5c850aa77a4738e15d12caa95e9bd6e5c80aa8.pdf", "supplementary_material": "/attachment/d66f9a3c446b16d32f3a903d93b33d9a32ba0d9d.pdf"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenges of Referring Expression Understanding (REU) in remote sensing images under few-shot settings by proposing a novel method named Geo-R1. The approach leverages GRPO-based reinforcement learning that performs reasoning before action. Experimental results on three distinct REU datasets demonstrate the effectiveness of the proposed method and highlight its strong cross-dataset generalization capability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Inspired by DeepSeek-R1-Zero, the paper adapts the “reason-then-act” GRPO reinforcement learning strategy to the remote sensing REU domain. Remarkably, it achieves strong performance even with extremely limited dataset (e.g., 1-shot).\n2. The paper provides thorough experimental evaluations, including learning curves, cross-dataset generalization, the impact of training data scale, and model size."}, "weaknesses": {"value": "1. The work is primarily engineering-oriented, focusing on the application of general-purpose methods to remote sensing tasks, with limited theoretical or methodological innovation.\n2. The quality of the generated reasoning chains is not assessed. The DeepSeek-R1-Zero–style training paradigm imposes no explicit constraints on the reasoning process itself. Notably, Figure 7 includes examples where reasoning appears correct but leads to misaligned localization, or where the reasoning itself is flawed—highlighting the need for dedicated reasoning evaluation.\n3. Since GRPO requires generating multiple responses per training instance, its computational cost is substantially higher than standard supervised fine-tuning (SFT). Therefore, comparing primarily against SFT baselines may not be equitable. A more appropriate comparison would involve other few-shot learning approaches that operate under similar resource constraints."}, "questions": {"value": "1. Were the few-shot training examples selected randomly, or were they curated to possess certain characteristics (e.g., diversity, difficulty, or representativeness)?\n2. How is the reward computed for outputs involving multiple bounding boxes or keypoints? For instance, in GRES, if the model outputs {..., \"keypoint1\": [82, 122], \"keypoint2\": [179, 122]} while the ground truth is {..., \"keypoint1\": [179, 122], \"keypoint2\": [82, 122]}, or if the prediction is a single large bounding box encompassing two smaller ground-truth boxes—how is alignment and reward determined in such cases?\n3. To the best of my knowledge, GRPO-based methods typically require training data on the order of thousands of examples. Is this work the first to successfully apply GRPO in a true few-shot data(e.g., 1-shot or 5-shot)?\n4. It is recommended that the authors complement the performance on OPT-RSVG[1] to further validate the generalization of the proposed method\n\n[1] Language-guided progressive attention for visual grounding in remote sensing images[J]. IEEE Transactions on Geoscience and Remote Sensing, 2024, 62: 1-13."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tSghClwRKW", "forum": "XPPTO4Qov7", "replyto": "XPPTO4Qov7", "signatures": ["ICLR.cc/2026/Conference/Submission768/Reviewer_DU3H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission768/Reviewer_DU3H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461813728, "cdate": 1761461813728, "tmdate": 1762915600268, "mdate": 1762915600268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Geo-R1, a novel Reinforcement Learning (RL) post-training methodology aimed at enhancing Geospatial Referring Expression Understanding (REU) in few-shot settings. The study focuses on three representative REU tasks: Referring Expression Comprehension (REC), Open-Vocabulary Detection (OVD), and Generalized Referring Expression Segmentation (GRES). By employing task-specific reward functions, Geo-R1 is designed to generate explicit intermediate reasoning hypotheses, thereby iteratively optimizing spatial localization and consequently improving the model's generalization capabilities under data-scarce conditions. Empirical results indicate that models fine-tuned with Geo-R1 surpass standard Supervised Fine-Tuning (SFT) models across various benchmarks, particularly demonstrating stronger cross-dataset generalization with limited sample sizes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The application of the RL paradigm to the few-shot remote sensing scene is a valuable and relevant direction for the field.\n\n2. The work successfully extends from simpler REC and RES tasks to the more complex GREC and GRES tasks. Furthermore, compared to methods like Seg-Zero, this approach achieves end-to-end segmentation optimization. While existing works (e.g., LENs) have explored similar end-to-end structures, this is still a strong contemporary contribution."}, "weaknesses": {"value": "1. The paper suffers from significant redundancy and conceptual ambiguity. The frequent and repeated mentions of \"few-shot\" are excessive and feel redundant, especially since the improved performance with fewer samples using an RL paradigm is a generally acknowledged finding. More critically, the definition and scope of the core task, REU, are inconsistent: on Line 161, it is stated that REC, RES, GREC, GRES, VG, OVD, and OVS are all REU tasks, yet on Line 152, REU is defined as a combination of GREC and GRES. This lack of clarity and conflicting definitions makes the paper difficult to parse. Similarly, the detailed definitions in Lines 165–174 are largely redundant. Moreover, dedicating nearly an entire page of Section 2.1 to explaining the GRPO algorithm, which is a well-established and widely accepted method, significantly detracts from the paper's novel contributions.\n\n2. I have serious reservations regarding the experimental results presented in Table 3. Under the low-data (few-shot) condition, the perplexing finding is that only the GTF metric shows an improvement, while performance on all other metrics inexplicably declines. This counter-intuitive result requires extensive justification. Additionally, I need clarification on the training details: What is the data repetition (data repeat/epoch) count for the available training samples in both the RL and SFT procedures? If the iteration count is $1$ in both cases, the models likely have not converged, which would fundamentally invalidate the experimental comparison.\n\n3. To genuinely validate the real effectiveness of the proposed RL paradigm, a full-scale experimental comparison is mandatory. The current results are insufficient; the authors must provide a comparison against full-SFT models trained with full datasets to demonstrate the ultimate potential and effectiveness of Geo-R1."}, "questions": {"value": "Pertaining to Figure 1, the definition provided, \"1-shot == 26 samples,\" appears to be a highly unconventional use of the 'shot' terminology. Is this a specific, ad-hoc definition adopted for the purpose of this paper? Could the authors please provide a detailed and rigorous justification for selecting precisely 26 samples to represent a single \"shot\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hnMSY9HesX", "forum": "XPPTO4Qov7", "replyto": "XPPTO4Qov7", "signatures": ["ICLR.cc/2026/Conference/Submission768/Reviewer_T8hh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission768/Reviewer_T8hh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625624367, "cdate": 1761625624367, "tmdate": 1762915600022, "mdate": 1762915600022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies reinforcement learning, namely GRPO, to several different tasks in the field of geographical problem solving. The paper compares the RL approach to SFT and shows that RL significantly outperforms SFT in several tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "+ The empirical results show a strong improvement over the previous SFT methods.\n+ The comprehensive experiments substantially demonstrate the claimed improvement for RL vs SFT."}, "weaknesses": {"value": "+ There is as far as I can tell very little conceptually or methodologically novel in this work. The proposed method is an application of GRPO, an algorithm introduced in previous work. The constructed datasets are, again as far as I can tell, a curated subset of previous datasets such as VRSBench. The main result is that on-policy reinforcement learning approaches outperform SFT (behavioral cloning). This is an extremely well-substantiated result in RL, e.g. [1, 2]. Therefore, while it is interesting to see this result in a particular application domain, in my opinion the contribution is not significant enough to warrant publication at ICLR.\n\n+ The paper is somewhat confusing to read, especially with the authors usage of the 'n-shot' terminology. After a close reading I understand that this is used in a meta-learning/task-adaptation sense, where a few examples are provided of a specific task. This is confusing in the language-model context, where few-shot typically refers to a number of provided examples being given in the context of a model before concluding with a final evaluation example.\n\n+ Related to the above, the paper does not have a particularly clear research question or takeaway, due to changing too many variables at once between the SFT and GRPO setting. In particular, as far as I can tell, the 'SFT' setting (which is not defined precisely in the text) refers to training the model with maximum likelihood on the correct response, provided immediately after the prompt. Meanwhile the RL approach uses chain-of-thought reasoning. In order to substantiate a generalizable difference between SFT and RL settings, the paper should compare either RL with no reasoning (i.e. constrained to give the answer immediately) or compare RL with reasoning to SFT over chain-of-thought rollouts filtered for correctness (i.e. true behavioral cloning). \n\n\n  [1] Ross, Bagnell, Efficient Reductions for Imitation Learning, AISTATS 2010.\n\n  [2] Morales, Sammut, Learning to Fly by Combining Reinforcement Learning with Behavioural Cloning, ICML 2004."}, "questions": {"value": "+ What are the key features that make the work significant and notable enough to be published at ICLR, given the well-established superiority of on-policy RL compared to SFT (behavioral cloning)?\n+ What is the motivation for comparing RL with chain of thought against SFT without chain of thought?\n+ What are the results when comparing RL without chain of thought to SFT or RL with chain of thought to SFT on successful demonstrations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ndLd5d5dh9", "forum": "XPPTO4Qov7", "replyto": "XPPTO4Qov7", "signatures": ["ICLR.cc/2026/Conference/Submission768/Reviewer_srr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission768/Reviewer_srr5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944933914, "cdate": 1761944933914, "tmdate": 1762915599897, "mdate": 1762915599897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) framework for few-shot geospatial referring expression understanding (REU). Instead of traditional supervised fine-tuning, Geo-R1 trains vision-language models to “reason first, then act,” generating explicit <think> reasoning chains before outputting bounding boxes or masks. Using Group Relative Policy Optimization (GRPO) and task-aligned rewards (IoU, mAP, MaskGIoU), the method directly optimizes for localization accuracy even in data-scarce regimes. Evaluated on three few-shot benchmarks (VRSBench-FS, NWPU-FS, EarthReason-FS), the results show that Geo-R1 consistently outperforms SFT by 5–15 points and generalizes better across the datasets considered. The authors also present analyses to demonstrate sample-efficiency, training stability, and interpretability (through reasoning traces) for the approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\n- Introduces a reasoning-centric reinforcement fine-tuning (RFT) framework for few-shot geospatial referring expression understanding, a novel combination of reasoning-driven RL and remote-sensing multimodal grounding.\n- Develops task-aligned reward functions (IoU, mAP, MaskGIoU) tailored to spatial grounding, including an innovative non-differentiable “BBox + SAM” pipeline for segmentation.\n\nQuality:\n- Empirical results are consistent, and reproducible across three few-shot benchmarks (VRSBench-FS, NWPU-FS, EarthReason-FS), each designed for distinct sub-tasks (REC, OVD, GRES).\n- Decent methodological rigor: identical base models, equalized training budgets, and clear comparisons between SFT and GRPO.\n\nClarity:\n- The paper is well-structured, with clear motivation, and intuitive figures (especially Fig. 1 pipeline diagram and learning-curve plots).\n- The reasoning format think…think → answer…answer makes the method interpretable and easy to follow, and technical choices (rewards, GRPO details) are described transparently.\n\nSignificance:\n- Demonstrates a sample-efficient alternative to SFT for adapting vision-language models in data-scarce geospatial settings.\n- Establishes reasoning-based RL as viable for improving few-shot multimodal generalization.\n- By releasing the few-shot datasets and code, the work provides a basis for future research on interpretable, reasoning-enhanced VLMs for remote sensing."}, "weaknesses": {"value": "- Marginal methodological novelty:\nThe paper has a modest conceptual leap. The “reason-first, then-act” framing builds directly on recent reasoning-centric reinforcement learning trends (e.g., DeepSeek-R1, o1), extending them to geospatial referring tasks rather than introducing new algorithmic principles. The innovation lies in task-specific rewards and adaptation to remote sensing rather than in a new learning paradigm.\n- Insufficient ablation and causal transparency:\nThe study does not clearly isolate which design components (reasoning format enforcement, task-aligned reward metrics, or GRPO optimization) drive the reported gains. Without systematic ablations or controlled reward variations, it is difficult to assess whether improvements stem from the reasoning structure itself or from auxiliary factors such as reward shaping or hyperparameter tuning.\n- Lack of uncertainty quantification and robustness analysis:\nResults are presented without standard deviations, confidence intervals, or variance across random seeds. In a few-shot context where performance can fluctuate substantially, the absence of uncertainty reporting undermines claims of reliability and generality. A simple table showing variance across multiple runs would materially strengthen the paper’s empirical credibility.\n- Asymmetric comparison in segmentation (GRES):\nBecause the reward pipeline is non-differentiable, the GRES experiments exclude a direct SFT baseline and instead compare against another RL-based system (SegEarth-R1). This asymmetry limits interpretation: the benefit of reinforcement fine-tuning from the design advantage of the BBox + SAM mechanism cannot easily be separated.\n- Compute intensity relative to problem scale:\nAlthough Geo-R1 is positioned as a data-efficient adaptation method, training requires roughly 8×H100 GPUs for 10–20 hours. Such requirements limit accessibility and partially offset the appeal of few-shot efficiency. \n- Narrow evaluation scope:\nAll benchmarks use RGB aerial imagery drawn from related distributions. The approach’s robustness to domain shifts remains untested. As a result, the claimed cross-dataset generalization speaks more to benchmark transfer than to real-world generalization across modalities.\n- Missing competitive adaptation baselines:\nThe comparison set omits other modern low-data adaptation strategies such as LoRA, adapter tuning, or visual prompt tuning, which are computationally lighter to reinforcement fine-tuning. Including at least one of these would better contextualize Geo-R1’s advantages."}, "questions": {"value": "Questions and Rebuttal Suggestions for the Authors\n- Ablations on reward design:\nCould you provide quantitative ablations isolating the effects of each reward component: (i) format reward enforcing structure, (ii) task-aligned metric rewards (IoU, mAP, MaskGIoU), and (iii) any length or penalty terms? Understanding which components are most responsible for the observed gains would clarify the causal mechanism behind Geo-R1’s improvements.\n- Variance and robustness reporting:\nPlease include results from multiple random seeds (or at least three runs) with standard deviations or confidence intervals. This is essential for assessing robustness, especially in few-shot regimes where noise and initialization effects can be substantial.\n- Cross-task generalization and domain shifts:\nHow does Geo-R1 perform under genuine domain shifts (different sensors, seasons, or geographies)? Even one experiment using multispectral or SAR imagery, or data from a distinct geographic region, would substantively strengthen claims of generalization.\n- GRES comparison and fairness:\nSince SFT cannot be directly applied to the non-differentiable BBox + SAM pipeline, could you comment on whether a proxy SFT baseline (e.g., trained with a differentiable mask loss) was attempted? Clarifying this would help attribute the segmentation gains to RL rather than to pipeline differences.\n- Compute and scaling trade-offs:\nCould you quantify Geo-R1’s compute cost relative to SFT and discuss whether smaller or more efficient variants (e.g., lower-precision training, distillation, or smaller backbones) preserve most of the gains? This would help judge its practicality.\n- Comparison to lightweight adaptation methods:\nWere LoRA, adapter tuning, or visual prompt-based few-shot methods considered? Even a brief empirical or conceptual comparison would better position Geo-R1 within the broader adaptation landscape."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JHe5GgUaLC", "forum": "XPPTO4Qov7", "replyto": "XPPTO4Qov7", "signatures": ["ICLR.cc/2026/Conference/Submission768/Reviewer_PFsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission768/Reviewer_PFsJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986172858, "cdate": 1761986172858, "tmdate": 1762915599748, "mdate": 1762915599748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}