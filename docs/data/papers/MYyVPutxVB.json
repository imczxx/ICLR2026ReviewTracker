{"id": "MYyVPutxVB", "number": 15467, "cdate": 1758251671439, "mdate": 1759897305038, "content": {"title": "Injecting Sensitivity Constraint Into Continual Learning Significantly Enhances Surrogate-Aided Optimization", "abstract": "A myriad of scientific and engineering optimization and learning tasks involve\nrunning a numerical model to guide optimization directly or generate training\ndata for function mapping algorithms. Surrogate models can greatly accelerate\nthese tasks, but they often fail to capture the true input-output relationships (sensi-\ntivities) so they lose the ability to guide high-dimensional and long-horizon op-\ntimization. Online continual learning (OCL) – iteratively obtaining numerical\nresults to continue training the surrogate – can mitigate this issue, but may still\nbe insufficient. Here we propose scheduled injection of sensitivity constraints\n(SC, matching the Jacobian of the surrogate model with that of the true numer-\nical model) for the surrogate into OCL to enforce realistic output-parameter re-\nlationships. We evaluate this approach across diverse datasets and optimization\nframeworks where continual surrogate training is used: (1) multi-objective multi-\nfidelity surrogate-assisted Bayesian optimization and Pareto front exploration; (2)\nhybrid end-to-end training of coupled neural networks and process-based mod-\nels; and (3) a modified unifying framework for generative parameter inversion\nand surrogate training. Across all of these tasks, inserting SC accelerates the de-\nscent to optimality and consistently improves the main optimization outcome, as\nit critically improves the future trajectory of optimization. OCL improves data\nrelevance and SC ensures sensitivity fidelity, and they together produces an ef-\nficient surrogate model that almost achieves the same effect as the full physical\nmodel, only achievable by OCL+SC. It consistently outperform pretrained-only\nsurrogate models with SC or OCL without SC, not to mention the pretrained-only\nmodel without SC, so the benefits of two procedures reinforce each other. Even\ninfrequent surrogate finetuning with SC injection (once every 5 epochs) can in-\nduce large benefits in optimization outcome. Together, these results demonstrate\nthe possibility to enable large-scale optimization of complex systems for big-data\nlearning and knowledge discovery", "tldr": "Here we propose a sensitivity-constrained online training framework that jointly improves parameter learning and prediction accuracy  with high efficiency", "keywords": ["Optimization", "Surrogate models", "Online continual Learning", "Sensitivity Constraints"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff5056275ec507b330cdca0b63f01f04b1c9189f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors study how to introduce Jacobian matching between surrogate and oracle functions into online continual learning - specifically in how to improve the surrogate model by matching not only the function values, but also their derivatives. The method is explored across 4 different tasks, including a combination of both synthetic benchmarks and real-world environments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The contribution of this work is interesting and intuitive.\n2. I appreciate the ablation over the frequency of SC injections, and also the experiments on how the method affects the overall runtime."}, "weaknesses": {"value": "3. The sensitivity loss weight hyperparameter lambda seems to be manually tuned per task. Since gradient magnitudes differ dramatically across models and domains, performance may be sensitive to lambda. There is no relevant ablation study or discussion of this limitation from what I can see.\n4. It is challenging for me to understand the Introduction within the context of the larger scope of this work, and required multiple reads at least on my end. For instance, what was the point of discussing knowledge discovery in the second paragraph? Catastrophic forgetting is also not explored at all in this work.\n5. The sensitivity loss term necessitates the ability to compute (or at least approximate) first order derivatives of the true numerical model. This is not necessarily the case in practice (e.g., discrete design spaces, wet-lab experiments, MD simulators, etc).\n6. $n = 5$ seeds is too few to make any meaningful conclusions – for example, I believe the 95% CIs for Offline + SC and OCL + SC (2) would overlap for the $R^2$ metric for the Hydrologic Model in Table 1.\n7. There doesn’t seem to be actual experimental evidence of catastrophic forgetting in the paper. Did the authors observe any signs of catastrophic forgetting or degradation in performance outside the immediate optimization regions (e.g., checking overall $L_2$ error on the entire initial buffer B) in the OCL-only versus OCL+SC cases?\n8. The manuscript would generally benefit from additional proofreading and editing. I generally do not feel super strongly about having a submission being 100% perfect in terms of grammar and spelling, but there are a large number of grammatical mistakes in this work that made it challenging to read through and required a couple of iterations to understand. I started detailing a few below in the \"Minor Comments\" section, but stopped somewhere in the middle so the list is by no means exhaustive.\n9. I understand that the SC loss term is not the main contribution, but rather empirically studying how it can be effectively incorporated into OCL in different settings. However, this was only studied in only 3-4 applications, and the ablation studies are significantly lacking (e.g., see point 1 above, also ablating buffer size, number of warm-up epochs, dataset sizes, Jacobian approximation accuracy, etc). This makes my enthusiasm for the empirical contribution of this work significantly tempered.\n10. I would also consider adding experience replay, elastic weight consolidation, and gradient episodic memory as baseline methods to compare against.\n\nMinor Comments:\n - Abstract: \"they together produces\" should be \"they together produce\"\n - Abstract: \"an efficient surrogate model\" - I don’t think the authors mean \"efficient\" here, should it be \"effective\"?\n - Abstract: \"consistently outperform\" in line 31 should be \"consistently outperforms\"\n - Line 68: I’m not quite sure what \"rigidity\" means here.\n - Line 77: \"infinite-dimension\" should be \"infinite-dimensional\"\n - Line 90: \"solely the recent examples\" should be \"solely on the recent examples\"\n - Line 181: I don’t think HV-KG is a well-known acronym – the authors should define it first.\n - Line 181: The sentence that begins with \"In the HV-KG framework\" is not a complete sentence.\n - In general, it is unclear when better metric values are higher vs lower. Up and down arrows should be added to clarify."}, "questions": {"value": "11. What is the definition of $M$ in equation (1)?\n12. How is the value of lambda in Algorithms 2 and 3 determined?\n13. In line 255, the authors state that a subset of the gradients were used to estimate the full Jacobian? What exactly was this algorithm? \n14. Why is the performance of OCL + SC (2) better than OCL + SC (1) in Table 1? Shouldn’t more frequent continued training improve the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZRr2Fwo6ek", "forum": "MYyVPutxVB", "replyto": "MYyVPutxVB", "signatures": ["ICLR.cc/2026/Conference/Submission15467/Reviewer_73Xu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15467/Reviewer_73Xu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760570155346, "cdate": 1760570155346, "tmdate": 1762925758824, "mdate": 1762925758824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of reduced guidance capability in high-dimensional, \nlong-horizon optimization. By integrating sensitivity constraints (SC) with online continual learning (OCL), \nthe authors present a generalizable framework that can be applied across diverse modeling contexts.\nThe paper demonstrated their effectiveness of this approach in several domains, including multi-objective multi fidelity \nBayesian optimization, hybrid training of coupled neural networks and process-based models, and generative parameter inversion with surrogate training, \nhighlighting its versatility and potential broad impact."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The OCL+SC framework is versatile and can serve as a plug-in for multiple models, \npotentially broadening its impact across various applications.\n2. The paper provides a clear and detailed explanation of the experimental settings, including application in multiple tasks"}, "weaknesses": {"value": "1. The paper does not clearly specify which Online Continual Learning (OCL) algorithm or variant \nis employed among the many existing approaches (e.g., RAR[1], OCA[2],...) \n\n2. The design of the sensitivity constraint is insufficiently justified. \nThere are multiple ways to inject or formulate sensitivity information \n(e.g., input–output Jacobian norms, local Lipschitz bounds), yet the paper adopts one specific form without discussion. \nIt would be valuable to clarify why this particular sensitivity formulation was chosen, \nhow it affects optimization behavior, and whether alternative forms were tested.\n\n3. Experimental comparison is limited; important baselines such as MF-OSEMO [3] and iMOCA [4] are missing in MO-MFBO.\nThe evaluation on MO-MFBO were mainly on Branin–Currin; including other synthetic benchmarks (e.g., Park, Levy, Rosenbrock) \nand real-world problems (e.g., Mechanical Plate Vibration Design, Thermal Conductor Design, NAS) would strengthen the evidence.\n4. The paper lacks comparisons to recent joint forward–inverse operator methods such as Latent Neural Operator (LNO) [5]. \nI also recommend evaluating the FUSE pipeline on additional PDE tasks beyond Darcy flow (for example airfoil, Navier-Stokes)\n\nThe paper lacks empirical comparisons with crucial baselines that could be straightforwardly adjusted to the proposed setting. \nThis omission weakens the overall experimental support for the claimed effectiveness of the method."}, "questions": {"value": "1. Which specific OCL strategy was implemented or was it simply a replay buffer? What motivated this choice?\n2. Beyond computational savings, does the use of OCL provide any additional benefit over \nsimply retraining the surrogate model with concatenated data in an online optimization setting? If so, can you explain it \n3. Could you please provide an analysis showing how sensitive your results are to the choice of $\\lambda$ (the weighting factor for the \nsensitivity-constraint term), and explain how this parameter should be selected for each task?\n\n**Missing related works** : \n\n[1] Repeated Augmented Rehearsal: A Simple but Strong Baseline for Online Continual Learning\n\n[2] Online Curvature-Aware Replay: Leveraging 2nd Order Information for Online Continual Learning\n\n[3] Multi‑Fidelity Multi‑Objective Bayesian Optimization: An Output Space Entropy Search Approach \n\n[4] Information‑Theoretic Multi‑Objective Bayesian Optimization with Continuous Approximations \n\n[5] Latent Neural Operator for Solving Forward and Inverse PDE Problems\n\n[6] Holistic Physics Solver: Learning PDEs in a Unified Spectral-Physical Space\n\n[7] Parameterized Physics-informed Neural Networks for Parameterized PDEs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "03YecWUUmR", "forum": "MYyVPutxVB", "replyto": "MYyVPutxVB", "signatures": ["ICLR.cc/2026/Conference/Submission15467/Reviewer_p5Sm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15467/Reviewer_p5Sm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963095453, "cdate": 1761963095453, "tmdate": 1762925758378, "mdate": 1762925758378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes combining Online Continual Learning (OCL) with Sensitivity Constraints (SC) to improve surrogate model performance in optimization tasks involving expensive numerical simulations. The core methodology augments standard data fitting loss with a sensitivity-matching term that aligns the Jacobian of the surrogate model with that of the physical model. The authors evaluate this approach across three optimization frameworks: (1) multi-objective multi-fidelity Bayesian optimization using the Branin-Currin benchmark, (2) hybrid end-to-end training of neural networks coupled with differentiable hydrological (dHBV) and ecosystem (δpsn) models, and (3) the FUSE framework for joint generative parameter inversion and surrogate training on 2D Darcy flow. Results show that OCL+SC consistently outperforms offline surrogates, OCL-only, and offline+SC baselines, with R2 improvements from 0.45 (offline) to 0.71 (OCL+SC) in the hydrological case and from 0.21 to 0.45 in the ecosystem case."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The evaluation across three structurally different optimization frameworks (MOMF-BO, hybrid differentiable training, joint generative-forward modeling) provides evidence of generality beyond a single application context. Each framework uses surrogates differently, yet OCL+SC consistently improves performance.\n\nThe failure of accurate surrogate models to provide good optimization guidance is a genuine challenge in computational science and engineering. The paper correctly identifies that sensitivity fidelity is often overlooked in surrogate model evaluation.\n\nThe hydrological experiments use the widely-recognized CAMELS dataset (531 catchments), and the ecosystem experiments use SAPFLUXNET (100+ sites, 120+ species), lending credibility to the validation on real observational data rather than only synthetic problems."}, "weaknesses": {"value": "The paper extensively discusses SC-FNO (Behroozi et al., 2024), DINO (O'Leary-Roseberry et al., 2024), and DE-DeepONet (Qiu et al., 2024) but does not include them as baselines. The \"Offline+SC\" baseline appears to be a simple implementation rather than these published methods. The central claim that OCL+SC outperforms existing approaches cannot be validated without direct comparison. Specifically, the paper does not test whether applying online continual learning to SC-FNO or DINO would yield similar results, which is the most direct competing approach.\n\nWith only 5 random seeds and no hypothesis testing, the statistical validity of the results is questionable. The ecosystem model result where OCL+SC surpasses the physical model benchmark (0.45 > 0.438) is particularly concerning and receives no investigation. Effect sizes, p-values, and confidence intervals are entirely absent, making it impossible to assess whether observed differences are meaningful or due to chance.\n\nThe FUSE experiment uses 1000 samples on a 32×32 grid, which is orders of magnitude smaller than typical neural operator papers in 2024-2025 (often 10,000+ samples on 256×256+ grids). The Branin-Currin benchmark is a 2D analytical function, not a physical simulation, contradicting the paper's emphasis on \"expensive physical models\" and \"high-dimensional problems.\" The experimental scale does not match the claimed scope.\n\nTable 5 claims 1/30 speedup but the comparison is unfair. The physical model's 4176s/epoch likely includes adjoint gradient computation, which the authors also use for SC. The analysis omits initial buffer generation cost, cumulative OCL evaluation costs, and end-to-end time-to-convergence. A complete cost accounting would likely show much smaller speedups. The claim should compare total cost to achieve equivalent performance, not per-epoch runtime for different operations.\n\nKey design decisions lack justification: (a) Why constrain only the \"four most sensitive parameters\" in the hydrology experiment rather than all 12? (b) Why evaluate gradients at only \"middle and last timesteps\" rather than all 730 timesteps? (c) How sensitive are results to λ (set to 1 in FUSE, unspecified elsewhere)? (d) How does the number of SC sampling points M affect results? (e) What sampling strategy for SC evaluation points is best (random, uniform, adaptive)? Without these ablations, it is unclear when and how to apply the method."}, "questions": {"value": "Why are SC-FNO, DINO, and DE-DeepONet not included as baselines? These are the most directly relevant competing methods. How does OCL+SC compare to simply applying online continual learning to SC-FNO? Without this comparison, the contribution cannot be validated. If computational constraints prevented full comparison, can you at least test SC-FNO on one domain?\n\nPlease provide statistical significance tests. For all results in Table 1, report p-values (paired t-tests or Wilcoxon signed-rank tests) comparing OCL+SC to baselines. With standard deviations of 0.02-0.07 and only 5 seeds, are the observed differences statistically significant at p<0.05?\n\nHow does the ecosystem model surrogate outperform the physical model benchmark (R2: 0.45 vs 0.438)? This is physically implausible. Is this: (a) overfitting to the validation set, (b) an issue with the physical model implementation, (c) noise in observations, or (d) statistical fluctuation? Please investigate and explain.\n\nPlease provide complete computational cost accounting. For the ecosystem model, report: (a) cost to generate initial buffer, (b) cumulative cost of all physical model evaluations during OCL, (c) gradient computation cost for SC, (d) end-to-end time to reach R²=0.42 for each method. What is the true total speedup when all costs are included?\n\nWhat accounts for the inconsistency where OCL alone helps in hydrology but hurts in FUSE? Under what conditions does OCL improve vs. degrade performance? This inconsistency suggests important boundary conditions that should be characterized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KyB9DL1p1X", "forum": "MYyVPutxVB", "replyto": "MYyVPutxVB", "signatures": ["ICLR.cc/2026/Conference/Submission15467/Reviewer_MJ5d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15467/Reviewer_MJ5d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994142046, "cdate": 1761994142046, "tmdate": 1762925757939, "mdate": 1762925757939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes combining online continual learning (OCL) with scheduled sensitivity constraints (SC) to improve surrogate-based optimization. The idea is to continually refine a surrogate model with incremental data while periodically enforcing consistency between surrogate sensitivities (Jacobians) and those of the true numerical model. The authors argue that this hybrid approach better preserves meaningful gradients for long-horizon, high-dimensional optimization tasks. The method is evaluated across several settings including multi-fidelity Bayesian optimization, hybrid physics–ML models, and generative parameter inversion frameworks.\n\nThe paper tackles an important problem with broad applicability, namely improving surrogate fidelity, particularly with respect to sensitivities, to support efficient optimization. The results generally support the claim that OCL combined with SC improves performance over OCL or SC alone. However, the contribution would benefit from clearer positioning relative to extensive prior work in active learning, adaptive surrogate refinement, and dynamic model updating, as well as more clarity on the application context and practical meaning of improvements. While promising, the novelty and scope currently feel somewhat diffuse due to the breadth of examples and insufficient literature contextualization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The paper addresses sensitivity preservation in surrogate modeling, a recognized challenge in surrogate-based optimization. Novel combination of continual learning and sensitivity constraints, and the scheduling strategy for SC injection is interesting. Diverse illustrative applications across optimization, hybrid modeling, and inverse problems show potential generality.\n\nQuality: Empirical results generally support claims that OCL+SC yields better optimization trajectories than alternatives and the paper demonstrates that sparse or infrequent SC finetuning can still give meaningful gains.\n\nClarity: The motivation is clear, and algorithm steps are reasonably described. Claims are consistently stated and supported numerically. It is really positive that the authors based their work on available open-source codes that are acknowledged and discuss the generalizability of this approach as differentiable programming becomes more available (which the Reviewer agrees is true).\n\nSignificance: If properly contextualized, the idea could be useful in large-scale scientific surrogate-assisted optimization. The observation that limited initial data can suffice (if demonstrated rigorously) could be impactful for expensive simulations."}, "weaknesses": {"value": "1. The manuscript engages in an active research area (surrogate-assisted learning, continual learning, active/adaptive data acquisition) but does not sufficiently clarify: How OCL relates to or differs from adaptive learning, online learning, or active learning in scientific modeling Whether prior works combining sensitivities with incremental data exist in related communities (e.g., multifidelity active learning, physics-guided update strategies)? Without clearer boundaries, the novelty claim (first to combine SC with OCL) is difficult to verify. Actionable suggestion: explicitly define continual learning vs. adaptive surrogate refinement vs. online active learning, and cite key lines of work in each.  In literature review, explain examples from a clearly continual learning need (e.g., not general field of \"design using surrogates\", which is a huge area where is continual learning needed, or not?)\n\n2. Related to above comment, it is not clear whether the method targets: Dynamic systems with evolving parameters, or static systems where data is progressively acquired to improve surrogates, or both? The mixed examples blur this distinction and make it harder to map contributions to existing streams. Clarify target problem class and restructure literature and examples accordingly.\n\n3. Practical significance not well discussed: While results improve metrics, the real impact is unclear. For instance: In the hydrological case, what does a validation error difference of 1.3 vs 1 translate to physically? Does the sensitivity improvement significantly change real-world decisions? Suggestion: add brief discussion connecting numerical improvements to domain relevance.\n\n4. Experimental clarity: Dimensionality and complexity of the hydrological model are insufficiently specified. Results focus on relative loss improvement; little insight into uncertainty, stability, or robustness. Suggestion: provide dimensionality details, and if possible discuss sensitivity to noise/initialization.\n\n5. Breadth vs depth: Multiple different applications are showcased, which suggests generality, but also makes evaluation feel surface-level. Suggestion: Consider deepening analysis in one domain or adding a conceptual unifying framework to help the reader navigate the diversity of settings."}, "questions": {"value": "1. How exactly do you define “continual learning” in this context, and how does it substantively differ from well-established active learning, adaptive surrogate modeling, or online Bayesian optimization?\n\n2. Is the approach primarily intended for dynamic systems that evolve over time, or static systems where additional samples are sequentially acquired, or both? Specifically in case of static systems, what are some practical examples of a steady-state design problem that would need continual learning vs. active learning for example?\n\n3. Can you detail the dimensionality and computational scale of the hydrological test case? How challenging is it relative to existing hydrology benchmarks?\n\n4. What does the improvement in surrogate loss translate to in terms of real decision-making or physical interpretability in the hydrological and other settings?\n\n5. Did you consider uncertainty-aware baselines (e.g., BO with uncertainty-driven refinement)? If so, how does the method compare?\n\n6. Is SC applicable if the physical model does not support efficient Jacobian computation, and what are scalability limits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bHy7nx8lZs", "forum": "MYyVPutxVB", "replyto": "MYyVPutxVB", "signatures": ["ICLR.cc/2026/Conference/Submission15467/Reviewer_jTub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15467/Reviewer_jTub"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000382149, "cdate": 1762000382149, "tmdate": 1762925757331, "mdate": 1762925757331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}