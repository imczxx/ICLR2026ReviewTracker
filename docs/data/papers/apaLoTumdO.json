{"id": "apaLoTumdO", "number": 4052, "cdate": 1757592212761, "mdate": 1759898055709, "content": {"title": "CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation", "abstract": "Generalizing local navigation policies across diverse robot morphologies is a critical challenge. Progress is often hindered by the need for costly and embodiment-specific data, the tight coupling of planning and control, and the \"disastrous averaging\" problem where deterministic models fail to capture multi-modal decisions (e.g., turning left or right). We introduce CE-Nav, a novel two-stage (IL-then-RL) framework that systematically decouples universal geometric reasoning from embodiment-specific dynamic adaptation. First, we train an embodiment-agnostic General Expert offline using imitation learning. This expert, a conditional normalizing flow model named VelFlow, learns the full distribution of kinematically-sound actions from a large-scale dataset generated by a classical planner, completely avoiding real robot data and resolving the multi-modality issue. Second, for a new robot, we freeze the expert and use it as a guiding prior to train a lightweight, Dynamics-Aware Refiner via online reinforcement learning. This refiner rapidly learns to compensate for the target robot's specific dynamics and controller imperfections with minimal environmental interaction. Extensive experiments on quadrupeds, bipeds, and quadrotors show that CE-Nav achieves state-of-the-art performance while drastically reducing adaptation cost. Successful real-world deployments further validate our approach as an efficient and scalable solution for building generalizable navigation systems.", "tldr": "We introduce CE-Nav, an IL-then-RL framework with a multi-modal VelFlow expert that rapidly adapts universal geometric plans to a specific robot's dynamics for efficient, generalized navigation.", "keywords": ["embodied navigation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bff74ac46566a2c357be6041c033609a37c716d.pdf", "supplementary_material": "/attachment/2950dae365ce21e363acbe2674ef44a5a414648c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for learning navigation policy given the goal direction and its surrounding environment. The method proposes to output robot command velocity which is embodiment agnostic. To learn the policy, it first use imitation learning to learn a reference velocity estimator. The estimator is a generative model that can captures the multimodal distribution. Then it learns a refined policy with RL for smooth and contact-free navigation. The policy is learned with cube robot and procedural scenes. \n\nIn the experiment, the paper demonstrates how the design can outperform existing baselines and shows cross-embodiment results in simulation. It also shows demos on real-world deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The hybrid planner / refiner and use velocity as the navigation policy interface is straightforward and reasonable. It is also reasonable for cross-embodiment policy.\n2. The ablation experiment is thorough, suggesting that the training paradigm is very important.\n3. The cross-embodiment result proves the cross-embodiment claim in the design."}, "weaknesses": {"value": "1. The policy does not take the size of the robot into account. Therefore, it is not designed for potential narrow environment.\n2. The cross-embodiment does show certain variation between different platforms. What is the failure case here?\n3. The paper primarily considers static obstacle for training. It is important to consider dynamic ones."}, "questions": {"value": "1. What are the failure cases of different robots? Is it tracking failure or planning failure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zrmNZ7GlaT", "forum": "apaLoTumdO", "replyto": "apaLoTumdO", "signatures": ["ICLR.cc/2026/Conference/Submission4052/Reviewer_3iX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4052/Reviewer_3iX8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853931077, "cdate": 1761853931077, "tmdate": 1762917156284, "mdate": 1762917156284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CE-Nav, a two-stage cross-embodiment navigation framework that combines imitation learning and reinforcement learning. In the first stage, a General Kinematic Expert, implemented as a conditional normalizing flow model, is trained offline on large-scale trajectories generated by a classical planner (DWA) in 2D to learn diverse kinematic behaviors. In the second stage, this expert is frozen and used as a guiding prior for a Dynamics-Aware Refiner, trained online via reinforcement learning to adapt trajectories to the dynamics of a specific robot. This approach allows for efficient policy transfer across robot embodiments while maintaining robust and feasible motion planning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents CE-Nav, a framework for cross-embodiment local navigation, combining imitation learning and reinforcement learning in a hierarchical setting. Its originality stems from the integration of a General Kinematic Expert trained offline on a classical planner’s dataset, with a Dynamics-Aware Refiner trained online to adapt to new embodiments. This formulation enables transferability across robots while maintaining high navigation performance.\n\nIn terms of technical quality, the work convincingly demonstrates several key points: hierarchical methods decouple planning from control, which introduces difficulties in compensating for unmodeled dynamics; the use of low-level controllers for demonstration collection introduces embodiment-specific bias, limiting generalizability; and velocity-based imitation methods neglect tracking errors, which can result in suboptimal navigation. The experiments, particularly the DWA-only ablations, support these claims. Furthermore, the adoption of a conditional normalizing flow to model the multi-modal expert action distribution is well-motivated and experimentally validated, addressing the “disastrous averaging” problem of regression-based approaches.\n\nThe clarity of the paper is strong. The problem formulation, system design, and experimental setup are well explained, with clear justifications for methodological choices. \n\nThe significance of the work lies in its potential to reduce the cost and complexity of training navigation policies for new robots. By freezing the expressive General Expert and fine-tuning only a lightweight Refiner, CE-Nav allows rapid adaptation to new embodiments while avoiding the need to collect full datasets for each robot."}, "weaknesses": {"value": "While the application is compelling, the paper’s technical novelty is moderate. \n- The idea of fine-tuning frozen expressive policies with residual RL has precedent in other domains, such as manipulation (e.g., From \"Imitation to Refinement – Residual RL for Precise Assembly\"). \n- Similarly, the proposed curriculum-guided RL phase, while practically effective, is not fundamentally new. The paper “Adaptive Behavior Cloning Regularization for Stable Offline-to-Online Reinforcement Learning” (like many others) has done something similar.\n\nThe related work discussion is incomplete. \n- The authors do not cite or compare against prior work on fine-tuning expressive policies, such as diffusion models or flow matching approaches, using RL. \n- The authors do not cite or compare against prior work on combining demonstrations with RL.\n\nThese are core aspects of their method and should be addressed.\n\nSeveral clarity and experimental limitations exist: the discussion of hierarchical methods is inconsistent, particularly regarding hierarchical velocity planners versus waypoint/trajectory-based planners. The authors could clarify what they mean by trajectory representations and highlight differences from existing hierarchical methods, because a trajectory can be represented by velocities.\n\nSome experimental gaps include:\n- Lack of analysis of the cost trade-off between collecting data with a low-level controller in the loop versus the proposed 2D simulation expert dataset.\n- Limited diversity in embodiments tested; all are similar in size, leaving open questions about generalization to larger robots (shape in X-Y plane).\n- No ablation on the effect of DWA hyperparameters, which could impact the quality of training trajectories.\n- Reward design (progress towards goal: direction and distance) may not adequately capture the task advancement (i.e going away from the goal to avoid an obstacle). These can lead to local minima.\n- General Expert performance is extremely low and the authors explain it as a covariate shift problem: further analysis of state-space coverage before and after RL refinement would strengthen claims, because the policy is still applied in the same auto-regressive way after RL - so technically the same problem remains.\n- The authors claim that normalizing flows offer superior inference speed compared to diffusion models, citing the iterative sampling cost of diffusion. While this is generally true, recent work (e.g., Diffusion-Based Approximate MPC:Fast and Consistent Imitation of Multi-Modal Action Distributions) demonstrates that diffusion‑based models can reach real‑time rates (250 Hz+) by using only a handful of denoising steps (5–10) with minimal performance loss. And their setup is more complex that simple 2D navigation.\n- The hardware experiments are not convincing. The trajectories are mainly in free space. More cluttered scenarios would be nice.\n\nOverall, the paper would benefit from additional analysis, expanded experiments, and deeper engagement with prior literature to strengthen the technical contribution and support its claims."}, "questions": {"value": "1.Can the authors clarify how their curriculum-guided RL differs from prior works?\n\n2.Could the authors discuss and compare their method to approaches that fine-tune expressive policies (e.g., diffusion or flow matching models) using RL?\n\n3. How does CE-Nav relate to other demonstration + RL pipelines in robotics? \n\n4. The paper mentions hierarchical velocity planners versus trajectory-based planners. Can the authors clarify what representation they mean by “trajectory” (e.g., velocities, waypoints, or full kinematic paths) ?\n\n5. Could the authors provide an analysis of the cost trade-off between collecting data with a low-level controller in the loop versus generating a 2D simulation expert dataset? This would quantify the claimed savings and clarify practical applicability.\n\n6. How sensitive is the method to DWA hyperparameters, such as circular agent radius or clearance settings? Could the authors provide ablation studies showing the effect on final performance?\n\n7. The embodiments tested are fairly similar in X-Y size. Can the authors comment on generalization to larger or differently shaped robots (e.g., a 1 m-radius robot)?\n\n8. Since the generalist expert policy performs poorly on its own, can the authors provide a state-space coverage analysis before and after RL refinement to illustrate whether covariate shift is still limiting performance?\n\n9. The paper claims normalizing flows are faster than diffusion for inference. Given recent work showing diffusion can run at 250 Hz with 5–10 denoising steps, can the authors reduce their number of denoising steps and compare the resulting performance?\n\n10. Can the authors provide experiments in more cluttered or constrained environments for the hardware platform? The current trajectories appear mostly in free space, which may not stress-test the policy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "POgMoWuJNt", "forum": "apaLoTumdO", "replyto": "apaLoTumdO", "signatures": ["ICLR.cc/2026/Conference/Submission4052/Reviewer_Ko6r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4052/Reviewer_Ko6r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864910675, "cdate": 1761864910675, "tmdate": 1762917155109, "mdate": 1762917155109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CE-Nav, a hierarchical two-stage learning framework for cross-embodiment local navigation. The first stage involves offline imitation learning with a conditional normalizing flow model, VelFlow, to capture a multi-modal, embodiment-agnostic action distribution purely from synthetically generated planner data. The second stage leverages the frozen expert to guide a lightweight, dynamics-aware RL refiner that adapts rapidly to the unique characteristics of each new robot embodiment using a curriculum for annealing guidance. Extensive experiments, including both sim-to-real deployments on heterogeneous robots (quadrupeds, bipeds, and aerial platforms), are presented, demonstrating strong performance and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The CE-Nav framework elegantly separates high-level geometric reasoning from embodiment-specific dynamic adaptation, supporting modularity and rapid transfer. \n-  By employing a conditional normalizing flow (VelFlow), the approach addresses the \"disastrous averaging\" problem in standard regression-based IL.\n-  The “Principled Deviation” loss treats VelFlow outputs as soft guidance and anneals $\\lambda$ over training to balance imitation and exploration; equations and adaptive scaling are specified.\n-  The experiments cover a spectrum of morphologies (legged, bipedal, quadrotor), with both simulated \"obstacle forest\" and real-world environments."}, "weaknesses": {"value": "- **Planner-only prior bias.** VelFlow is trained on 10M (state, action) pairs from a geometric DWA planner, which may induce inductive bias and limit out-of-domain robustness under complex dynamics or sensor noise; the RL refinement helps, but evidence for non-LiDAR or multi-sensor settings is limited.\n- **Compatibility under large embodiment shifts.** While VelFlow models multi-modal actions, the paper lacks analysis of how its support aligns with the feasible action sets of very different embodiments (e.g., quadrotor vs. biped). Section 3.4.1 would benefit from theory or additional experiments clarifying when expert proposals become invalid/unsafe for a given low-level controller and how the refiner recovers.\n- **Real-time constraints.** Reported >10 Hz inference may be marginal for high-dynamics platforms (e.g., quadrotors). A latency-versus-performance/safety study and robustness under compute contention or thermal throttling are missing."}, "questions": {"value": "1. How sensitive is VelFlow to the DWA-generated dataset’s biases (2D, disc robot)? Any results when training the prior on non-DWA data or mixed sources?\n2. Can the authors provide quantitative results or ablations on the impact of the conservative scaling factor used in the guidance loss (Eqn., Appendix A.3)? How does this affect the amplitude/diversity of expert actions and the degree to which the RL refiner needs to “undo” guidance for more dynamic robots?\n3. How robust is VelFlow’s proposal distribution under substantial embodiment mismatch, especially for robots whose feasible velocity/action set is much smaller or differently shaped than the expert’s? Can unsafe or unrecoverable expert proposals be generated?\n4. For real-world deployments, how does the absence of RGB perception affect performance overall—not just for rare glass walls, but also in environments with ambiguous or non-LiDAR-reflective obstacles? Has the pipeline been tested or profiled in such settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rJWfJSNQFW", "forum": "apaLoTumdO", "replyto": "apaLoTumdO", "signatures": ["ICLR.cc/2026/Conference/Submission4052/Reviewer_rjvc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4052/Reviewer_rjvc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893667329, "cdate": 1761893667329, "tmdate": 1762917154888, "mdate": 1762917154888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CE-NAV, a two-stage framework for addressing the cross-embodiment local navigation problem. In the first stage, a conditional normalizing flow model is trained to generate kinematically valid actions from a large-scale synthetic dataset produced by the DWA planner across diverse environments. In the second stage, a local policy is trained from scratch via reinforcement learning (RL) to compensate for the target robot’s specific dynamics. The authors introduce a hybrid loss function that combines RL and behavioral cloning (BC) objectives to balance imitation and exploration. The method is evaluated in both simulation and real-world experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The work includes a real-world deployment demonstration\n\n- The proposed hybrid loss combining RL and BC can be useful for the robotics community (though similar ideas have appeared in prior works).\n\n- The paper is clearly written and easy to follow."}, "weaknesses": {"value": "- The paper studies local navigation, a well-explored problem with many strong existing methods. However, the chosen baselines are not representative of the current state of the art. For example, in the BARN Challenge [1], the algorithms used by the top-performing teams better represent state-of-the-art local navigation approaches in cluttered environments.\n\n- The contribution is limited:\n    - The method appears unnecessarily complicated. Since the normalizing flow model is not fine-tuned during the RL stage, its benefit is unclear. A simpler approach would be to directly use the DWA planner to generate reference velocities for the local policy.\n    \n    - The paper tends to rebrand existing ideas as new contributions. For instance, \"VelFlow\" is essentially a normalizing flow model that predicts velocities. Likewise, the proposed “Principled Deviation” objective closely resembles prior works such as Intermimic [2], Learning Complex Dexterous Manipulation with Deep RL and Demonstrations [3], and Learning and Adapting Agile Locomotion Skills by Transferring Experience [4].\n\n    - The method still requires retraining for each embodiment from scratch, which undermines its practicality. In modern robotics, zero-shot transfer across embodiments is the key challenge; requiring retraining makes this approach far less impactful. Moreover, the training process remains time-consuming (hours, not minutes), and the reported success rate (0.76) is not impressive. The real-world demos also take place in simple, open environments, offering limited insight into robustness.\n\n\nReferences:\n1. https://cs.gmu.edu/~xiao/Research/BARN_Challenge/BARN_Challenge25.html\n2. InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions\n3. Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations\n4. Learning and Adapting Agile Locomotion Skills by Transferring Experience"}, "questions": {"value": "- Why use a normalizing flow instead of a rectified flow approach? Normalizing flow is an older method and not as widely used today. Rectified flow models are newer, often more effective, and can also run efficiently in practice.\n\n- The diffusion policy baseline (CE-Nav dp-rl) performs poorly, likely because diffusion models are not well suited for predicting low-level velocity actions. Would performance improve if the diffusion model were trained to predict waypoints instead, with an MPC controller handling the low-level execution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PQD8nshp0t", "forum": "apaLoTumdO", "replyto": "apaLoTumdO", "signatures": ["ICLR.cc/2026/Conference/Submission4052/Reviewer_oUen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4052/Reviewer_oUen"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948558899, "cdate": 1761948558899, "tmdate": 1762917154586, "mdate": 1762917154586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}