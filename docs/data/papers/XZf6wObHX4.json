{"id": "XZf6wObHX4", "number": 6318, "cdate": 1757967479511, "mdate": 1763655220122, "content": {"title": "Activation Function Design Sustains Plasticity in Continual Learning", "abstract": "In independent, identically distributed (i.i.d.) training regimes, activation functions have been benchmarked extensively, and their differences often shrink once model size and optimization are tuned. In continual learning, however, the picture is different: beyond catastrophic forgetting, models can progressively lose the ability to adapt—loss of plasticity—and the role of the non-linearity in this failure mode remains underexplored. We show that activation choice is a primary, architecture-agnostic lever for mitigating plasticity loss. Building on a property-level analysis of negative-branch shape and saturation behavior, we introduce two drop-in nonlinearities—Smooth-Leaky and Randomized Smooth-Leaky—and evaluate them in two complementary settings: (i) supervised class-incremental benchmarks and (ii) reinforcement learning with non-stationary MuJoCo environments designed to induce controlled distribution and dynamics shifts. We also provide a simple stress protocol and diagnostics that link the shape of the activation to the adaptation under change. The takeaway is straightforward: thoughtful activation design offers a lightweight, domain-general way to sustain plasticity in continual learning without extra capacity or task-specific tuning.", "tldr": "Activation design—guided by simple first-principles rules—yields drop-in choices that keep models plastic across sequences and generalize better under distribution shift in continual supervised learning and RL.", "keywords": ["loss of plasticity", "continual learning", "lifelong learning", "continual reinforcement learning", "activation functions"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea262f1ec87645a6df4d66bebff738a6d8ef4bef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents two case studies to investigate the activation functions in the context of continual learning and identifies three principles for designing plasticity-friendly nonlinearities. Building upon these insights, the authors propose two new activation functions: **Smooth-Leaky** and **Randomized Smooth-Leaky**. Empirical studies on continual supervised learning and continual reinforcement learning demonstrate that the proposed activation functions achieve superior performance compared to existing alternatives."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The case studies are interesting and thoughtfully executed, providing solid and well-supported conclusions. \n- The large-scale empirical studies on continual supervised learning and continual reinforcement learning clearly demonstrate the superiority of the proposed activation function over prior alternatives.\n- The proposed activation functions show promise for broader application in domains that rely on network plasticity.\n- The authors also provide code to improve reproducibility."}, "weaknesses": {"value": "- As the experiments already include multiple runs, I recommend that the authors report not only the mean results but also measures of statistical significance—such as standard deviation or 95% confidence intervals—for example, in Table 1 and Table 2.\n- The continual reinforcement learning evaluation appears limited to a single experimental configuration. Expanding this analysis to include additional environments or settings would provide stronger support for the generality of the proposed activation functions.\n- *Case Study 1* and *Case Study 2* are important parts of the paper, but too many related results and figures are placed in the appendix. To enhance readability, the authors might consider reorganizing the content despite the page limit.\n- The term $C^1$ transition first appears in the introduction (line 60), but without explanation."}, "questions": {"value": "- Do the proposed activation functions result in greater wall-clock time compared to existing activation functions?\n- Could the authors provide the results of the **Smooth-Leaky** activation function in the continual reinforcement learning experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eUc1stMV08", "forum": "XZf6wObHX4", "replyto": "XZf6wObHX4", "signatures": ["ICLR.cc/2026/Conference/Submission6318/Reviewer_fK2M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6318/Reviewer_fK2M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761429671050, "cdate": 1761429671050, "tmdate": 1762918614336, "mdate": 1762918614336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that activation choice is a primary lever for sustaining plasticity in continual learning. It presents a property-level analysis (negative-branch responsiveness, saturation, “dead-band” width) of activation functions and proposes two drop-in $C^1$ activations, Smooth-Leaky and Randomized Smooth-Leaky, that maintain a non-zero derivative floor. The paper evaluates the activation functions on standard Continual Learning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a substantial empirical study on the properties of many activation functions under the lens of continual learning. The case studies are well thought out and well conducted, they isolate negative-branch slope, saturation, and “dead-band width” as key predictors under shift. The study of desaturation dynamics under shocks is an important diagnostic of activation functions and ties well activation shape to recovery behavior after controlled shifts.\nThe suggested drop-in replacements are intuitive and easy to implement."}, "weaknesses": {"value": "In general, the paper is verbose and somewhat difficult to parse. Table 2 reports statistical significance but does not provide confidence intervals or which statistical tests were used. There also appears to be an imbalance in the hyper-parameter sweeps: the two proposed activation functions are explored across 175 and 450 combinations, respectively, whereas other hyper-parameters are examined over only a handful of settings. This imbalance could bias the experimental results."}, "questions": {"value": "What distribution over pre-activations is used for the effective negative slope in Figure C-1(per-layer? per-epoch? across tasks?), and how sensitive are the conclusions to that choice? Please report variability across layers/epochs and any normalization applied before computing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VVdoRsHXNk", "forum": "XZf6wObHX4", "replyto": "XZf6wObHX4", "signatures": ["ICLR.cc/2026/Conference/Submission6318/Reviewer_5MGv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6318/Reviewer_5MGv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879534606, "cdate": 1761879534606, "tmdate": 1762918613832, "mdate": 1762918613832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a thorough investigation on the role of activation functions in addressing plasticity loss in continual learning. They propose two activation functions, Smooth-Leaky and Randomized Smooth-Leaky, which involve maintaining a non-zero derivative floor, optimizing negative-side responsiveness and ensuring smooth transitions at the origin. They evaluate these activations across supervised continual learning benchmarks and RL environments, showing their effectiveness in maintaining plasticity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The main strength of the paper lies in its rigorous evaluation of activation functions across continual learning benchmarks (supervised and RL).\n- Proposed activation functions, and the concept of a \"Goldilocks zone\" for negative-side responsiveness, offer a interesting perspective on mitigating plasticity loss in continual learning. ​The findings highlight the role of activation functions in maintaining plasticity.\n- Metrics like saturation fraction and dead-band width provide valuable insights into activation function behavior under non-stationary settings.\n- The paper is well-structured and clearly written, with visualizations, explanations."}, "weaknesses": {"value": "- While the paper discussed impact unconstrained adaptive slopes (e.g., PReLU), it does not explore alternative methods to guide parameters into the \"Goldilocks zone\" which itself is primarily identified empirically, but lacks theoretical grounding or formal analysis. Therefore, explaining why this range is optimal would help the paper overall.\n- The paper does not explore how activation functions interact with different optimizers or learning rate schedules. For example, adaptive optimizers like Adam or RMSProp may behave differently with the proposed activations. Are the proposed activations universally effective across all continual learning settings? Since the dynamics of plasticity loss may vary significantly between tasks, a more nuanced analysis of task-specific performance could provide deeper insights.\n- The reinforcement learning experiments focus solely on MuJoCo tasks, Have authors tried running experiments on domains like sparse reward or high-dimensional tasks?"}, "questions": {"value": "Please refer to the comments in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qC5h9Zh4ii", "forum": "XZf6wObHX4", "replyto": "XZf6wObHX4", "signatures": ["ICLR.cc/2026/Conference/Submission6318/Reviewer_hFaa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6318/Reviewer_hFaa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980692368, "cdate": 1761980692368, "tmdate": 1762918613463, "mdate": 1762918613463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment by Authors"}, "comment": {"value": "We wish to thank all reviewers for the time and consideration they have devoted to reviewing our work, as well as for their constructive feedback. We took these concerns very seriously and feel the revised manuscript is significantly stronger as a result. We have made a compilation list with all changes made to the paper to speed up the reviewers' process and to help in identifying revisions directly.\n\n## Main Paper Structural & Content Additions\n\n1. **Figure C1** *(Performance vs. $\\overline{s}$ and Dead Units vs. $\\overline{s}$)* was moved from the appendix to **Section 3**. This provides direct visual evidence for the \"Goldilocks zone\" and its associated \"dead unit\" failure mode.\n\n2. **Figure C2** *($\\lambda_{\\max}$ vs. $\\overline{s}$ and Effective Rank vs. $\\overline{s}$)* was moved from the appendix to **Section 3**. This provides the evidence for the second failure mode (high curvature) that defines the \"Goldilocks zone\".\n\n3. The text in **Section 3** regarding *Failure Modes for Slope Magnitude* was revised, condensed, and updated to improve readability.\n\n4. The text in **Section 4** was revised. **Figure D1** *(Correlations with Dead-Band Width)* was moved from the appendix to **Section 4**. This provides the key evidence that our analytical DBW metric strongly predicts empirical saturation and non-recovery rates.\n\n6. We revised **Section 7** and updated **Table 3** and **Figure 6** with a **Min-Max Normalized IQM** score rather than a vanilla average, following literature standards (see answer to Reviewer mY97).\n\n8. We updated **Table 4** to provide $\\Delta_{\\mathrm{IQM}}$ rather than median.\n\n10. We added clarifications to **Section 7.1** with respect to **Plasticity Score** and $\\Delta(\\mathrm{GAP})$ are a two-metric system designed to expose the \"trainability-transfer tension\", and that the RL test environment perturbation is a **\"held-out friction variant,\"**\n\n12. A clarification regarding what optimizer was used (**Adam**) was included in **Section 2**.\n\n13. We updated **Tables 1, 2, and 4** to include standard deviations (Tables 1 and 2) and confidence intervals (Table 4) as requested by reviewers fk2M and mY97.\n\n14. A clarification regarding dead-unit definition for non-ReLU activation functions was included in **line 143**. The threshold used to quantify those for Case Study 2 was indicated in **line 174**. The implementation resides in `utils/dead_units.py`.\n\n## New Experimental Baselines\n\n1. We have added **Deep Fourier Features (DFF)** as a new baseline in the Continual Supervised Learning benchmarks (**Table 2**) and Continual RL experiments. Since it was not competitive with the set of activations shown in C-RL experiments, the RL results only appear in the complete appendix tables (**Tables F1 and F2**).\n\n2. We have added **SwiGLU** as a new baseline in the CSL benchmarks (**Table 2**) and Continual RL experiments; the same applies as for DFF (**Tables F1 and F2**).\n\n3. The text in **Section 6** was updated to introduce these new baselines and provide context for their inclusion.\n\n## New Additions to the Appendix\n\n1. We added a new subsection to **Appendix B (Experimental Design)** providing a clear *Rationale for Optimizer Choice*. This section explains our deliberate choice of the Adam optimizer as a \"stress test\" for plasticity, citing relevant literature that implicates it as a contributor to plasticity loss.\n\n2. We added a targeted experiment in a new **Appendix Section E.3: \"Continual Learning Interventions Targeting Loss of Plasticity\"** showing the performance of **L2 Init, SNR, and EWC** when combined with all activation functions studied throughout the paper. Results are reported in **Tables E.4–E.11**.\n\n3. We added the Generalization **GAP** results for the CSL benchmarks in a new appendix section. While we maintain that **Total Average Online Accuracy** remains the standard and most appropriate metric for these benchmarks (as detailed in our response regarding the distinction between SL generalization and RL robustness to reviewer mY97), we agree that reporting GAP enhances completeness and transparency.\n\n4. We updated **Section C1 and Figure C1** to reflect statistical changes due to the addition of new hyperparameter configurations. The conclusion remains the same.\n\n5. We updated **Table B1** with more hyperparameter values for activations such as **Leaky-ReLU, SeLU, eLU, cELU, Swish, and GeLU**.\n\n6. We updated **Section F** after introducing the metric IQM. We also introduced a new **Table F2** that reports raw rewards per environment for all activation functions.\n\n7. We updated results regarding **Bo-PReLU** and **R-SeLU** as other custom activation suggestions and compared them to their inspirations (**PReLU and SeLU**), and our other custom activations as suggested by Reviewer hFaa (Section G). We showed how they are competitive and how we can develop activations based on ideas studied in the paper."}}, "id": "ZzZbnoLoRo", "forum": "XZf6wObHX4", "replyto": "XZf6wObHX4", "signatures": ["ICLR.cc/2026/Conference/Submission6318/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6318/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6318/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656464065, "cdate": 1763656464065, "tmdate": 1763656464065, "mdate": 1763656464065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors explore the role of activation function design on plasticity in continual learning. The authors introduce the Smooth-Leaky and the Randomized Smooth-Leaky activation functions.The authors evaluate their proposed activation functions in the supervised class incremental continual learning benchmarks and in RL environments and show that the randomized smooth leaky activation function consistently outperforms existing activation functions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The empirical results clearly show that the proposed activation function randomized smooth-leaky consistently attains superior average online task accuracy. \n- The experiments consider a comprehensive set of baseline or existing activation functions.\n- The discussion and experiments regarding the generalization gap, specifically, that plasticity loss can manifest in reduced training loss and test loss in different ways, is a positive touch to the paper which is not usually evaluated in this space. As the authors point out, maximizing plasticity via maximizing training loss may result in over-fitting. While earlier papers have argued that solely analyzing test loss in continual settings can be confounded by issues of regularization and generalization. The focus that the authors put here is a strength of this paper."}, "weaknesses": {"value": "- There is a paper by Lewandowski et al. \"Plastic Learning with Deep Fourier Features\" which also examines the role of activation function design on plasticity. It would be useful if the authors could comment on this line of work, and ideally, incorporate this paper's contribution as a baseline competitor. \n- It would be worthwhile to apply a few existing SOTA methods for plasticity loss such as reset-based: SNR and regularization-based: L2 Init, and to generate a table with results for a reasonable set of METHOD X + ACTIVATION FUNCTION Y. When given an additional continual learning intervention how important is the choice of activation function?"}, "questions": {"value": "- Given the recent popularity of the SwiGLU, why isn't this activation function evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hTKpqaB0xw", "forum": "XZf6wObHX4", "replyto": "XZf6wObHX4", "signatures": ["ICLR.cc/2026/Conference/Submission6318/Reviewer_kvHD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6318/Reviewer_kvHD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052109052, "cdate": 1762052109052, "tmdate": 1762918612999, "mdate": 1762918612999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at the effect of various characteristics of activation functions on the plasticity of neural networks on supervised continual learning and RL benchmarks. These characteristics include things like the slope of the activation function for negative inputs, where they identify a “Goldilocks zone,” and dead-band width which is the portion of the activation function domain that has a small gradient. They propose two different activation functions, Smooth-Leaky and Randomized Smooth-Leaky which are modifications of the Leaky-ReLU activation function with a smooth transition region rather than a kinked one."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The initial part of the paper with the case studies on the negative slope and dead band width are valuable experiments for the community.\n- The diversity of metrics and analyses presented for the experiments is quite high."}, "weaknesses": {"value": "- The presentation of the paper can use some work. Most of the results for the experiments in Sections 3 and 4 are not presented in the main paper, and are instead presented in an Appendix. This is fine for one or two auxiliary results, but here, most of the main results for a significant portion of the paper are presented elsewhere. I’d ask that either the authors figure out a way to present a coherent story with results in the main paper, either by moving some parts of the paper to the appendix (not just the graphs), or if that is not possible in the page limit, consider another venue.\n- The paper introduces two activation functions Smooth-Leaky and Randomized Smooth-Leaky. These activation functions introduce 3 and 4 new hyperparameters respectively which are searched for in a grid, with 175 and 450 combinations respectively. Compare this to the baseline of Leaky-ReLU which had 12 configurations evaluated. Furthermore, these hyperparameters were individually tuned on each setting. The amount of extra tuning given to the new methods is an unfair advantage, and could definitely be the cause of any improvement in performance rather than something inherent about the methods themselves.\n- The metrics used to measure plasticity in the RL section seem to be flawed. If we look at just the rewards, Sigmoid seems to significantly outperform the other activation functions on 2/4 environments. Yet the plasticity score seems designed in a way to remove that effect, taking the median across environments to result in a number where the proposed method does better. A better approach could be to do some kind of normalization of the return and presenting the mean. Taking the median across 4 environments specifically excludes the environment where none of the activation functions except for Sigmoid are able to learn.\n- For generalizability, there’s another metric presented which is a non-standard metric. It looks at the gap between train performance and a test environment which is some perturbation of the train environments. The metric reported compares the difference between the gap after cycle 3 and cycle1. First, it’s unclear what the perturbations are. Second, this metric has no notion of what the actual reward obtained in each of the settings was. A small gap with low absolute rewards is less preferable to a large gap with high absolute rewards, but this metric doesn’t capture that. Finally, measuring generalizability is much easier in the supervised setting, and has been done in several prior works. The results on those settings should be reported before creating such a metric.\n- Tables 1, 2, and 4 are missing standard deviations."}, "questions": {"value": "- Could you further clarify the reasoning behind the criteria for dead units defined for non-ReLU activation functions? For ReLU it makes sense because there is a region where the value of the activation function is 0 with slope 0. That doesn’t seem to be the case with most of the other activation functions studied, and it seems a bit odd to talk about dead-unit fractions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nfgbIAl7YD", "forum": "XZf6wObHX4", "replyto": "XZf6wObHX4", "signatures": ["ICLR.cc/2026/Conference/Submission6318/Reviewer_mY97"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6318/Reviewer_mY97"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762372825470, "cdate": 1762372825470, "tmdate": 1762918612670, "mdate": 1762918612670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Case Study 1 and Case Study 2 enhancements"}, "comment": {"value": "Since this has been requested by two independent reviewers, we would like to propose a unified answer. First of all, we thank both reviewers (fk2M and mY97) for this critical feedback. You are both correct that the original 9-page limit forced us to move essential figures for Case Studies 1 and 2 to the appendix, which weakened the main paper's narrative and readability. We apologize for this and, thanks to the newly allotted extra page, we have made significant structural changes to Case Study 1 and Case Study 2 to directly address this concern. Changes to other sections are reported in the main, general comment that serves as a summary of our revised draft.\n\nWe are confident that these changes make Sections 3 and 4 a self-contained, coherent, and empirically supported story, which we agree greatly improves the paper. Specifically, the following key changes were made:\n\n---\n\n## Section 3 (Case Study 1: “Goldilocks Zone”)\n\n- We moved **Figure C1** from the appendix into the main text. This is the most critical figure for this section, as its subplots visually:  \n  1. define the “Goldilocks zone” by plotting *Performance vs.* $\\overline{s}$, and  \n  2. show the first failure mode ($\\overline{s} \\to 0$) by plotting *Dead Units vs.* $\\overline{s}$, directly supporting our claim about the “dead-unit regime.”  \n\n- We also moved **Figure C2** into the main text. This figure provides evidence for the second failure mode ($\\overline{s} \\ge 0.9$), by plotting the “curvature explosion” (*$\\lambda_{\\max}$ vs.* $\\overline{s}$) and the resulting drop in representational diversity (*Effective Rank vs.* $\\overline{s}$).  \n\n- Both figures have been condensed into a single figure (now **Figure 1**). As we also report, we collected results for a larger hyperparameter sweep (as pointed out by reviewers 5MGv and mY97), and we now include them. The updated **Figure 1** provides stronger visual confirmation of the two competing failure modes that define the “Goldilocks zone”:\n\n  - **Left side ($\\overline{s} \\to 0$):** performance collapse correlates with a surge in dead units (gradient starvation).  \n  - **Right side ($\\overline{s} \\ge 1$):** performance collapse correlates with a spike in principal curvature and a change in effective rank, showing that despite high unit activity, the optimization landscape becomes unstable.  \n\n  Together, this visually confirms that plasticity requires balancing these two opposing constraints.\n\n---\n\n## Section 4 (Case Study 2: Desaturation Dynamics)\n\n- We moved **Figure D1** from the appendix into the main text. This figure is the driving argument of the section: it provides explicit, strong correlations showing that our analytical **Dead-Band Width (DBW)** metric is a valid predictor of empirical failure rates (namely *Avg. AUSC* and *Avg. SF Non-Recovery Rate*).  \n\n- This section has also been rewritten and condensed. In particular, the surrounding text in Section 4 has been made clearer, less repetitive, and updated to integrate the newly added Figure D1."}}, "id": "QkNHQzg5U7", "forum": "XZf6wObHX4", "replyto": "XZf6wObHX4", "signatures": ["ICLR.cc/2026/Conference/Submission6318/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6318/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6318/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763657456322, "cdate": 1763657456322, "tmdate": 1763657456322, "mdate": 1763657456322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Hyper-parameter sweeps and extra combinations"}, "comment": {"value": "Since this has been requested by two independent reviewers, we would like to propose a unified answer. First of all, we thank both reviewers (5MGv and mY97). \n\nWe fully accept the reviewer’s critique regarding the discrepancy in the hyperparameter search space. The reviewer is correct that, in an ideal compute-normalized comparison, we would allocate each activation function the same overall computational budget (e.g., total number of training runs). Because our proposed functions have 3–4 degrees of freedom while baselines like Leaky-ReLU have only 1, we opted to provide the same computation budget to each free hyperparameter, such that each would be sufficiently fit. We acknowledge that this gave our method more total shots at the goal, and that a general weakness of our proposed activation function is the additional hyperparameters that must be tuned.\n\nHowever, we respectfully submit that this discrepancy does not account for the performance gap, for two key reasons:\n\n1. **Single-parameter baselines saturate quickly.**  \n   Leaky-ReLU has a single shape degree of freedom (negative slope $\\alpha$). In 1-parameter spaces, performance typically saturates quickly. “Spending” 400 more runs on fine-grain hyperparameter tuning for Leaky-ReLU yields negligible gains compared to the coarse search alone. To verify this—and to directly address the reviewer’s concern—we conducted supplemental experiments (added to Appendix B, Table B1) where we significantly expanded the search grids for Leaky-ReLU, SeLU, ELU, and Swish. This suggests the baselines were already operating at their performance ceiling, and the “unfair advantage” of extra compute would not have helped them further.\n\n2. **Our gains reflect a higher ceiling, not lucky tuning.**  \n   Smooth-Leaky (SL) and Randomized Smooth-Leaky (RSL) expose additional shape controls for transition width, scale factor, or the upper and lower bounds for the randomized $\\alpha$. A significant portion of these configurations outperformed the best-tuned baseline. This indicates that our method’s superiority is not due to finding one “lucky needle in a haystack” (which would imply overfitting to hyperparameters), but rather due to the inherent geometric properties of the function (smoothness + variance) that unlock a higher performance ceiling than is mathematically possible with fixed or 1-parameter functions.\n\nIn the revised manuscript (Section B.2), we explicitly discuss this trade-off: while multi-parameter designs introduce a larger search space (a practical limitation), they break the performance ceiling that limits simpler activations."}}, "id": "JMkX9So9FB", "forum": "XZf6wObHX4", "replyto": "XZf6wObHX4", "signatures": ["ICLR.cc/2026/Conference/Submission6318/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6318/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission6318/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763658362913, "cdate": 1763658362913, "tmdate": 1763658362913, "mdate": 1763658362913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}