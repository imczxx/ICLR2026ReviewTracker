{"id": "h6VKMq5ZrD", "number": 23833, "cdate": 1758349091214, "mdate": 1759896794770, "content": {"title": "TPI-VA: Third-Party Interruption-Aware Voice Assistant", "abstract": "While recent progress in Spoken Language Models (SLMs) has enabled increasingly natural voice-based interactions, they remain vulnerable to third-party interruptions (TPI). To address this challenge, we present a holistic framework for building and evaluating TPI-aware voice assistants. We first introduce TPI-Train, a large-scale dataset of 80K instances spanning 26 realistic interruption scenarios. For evaluation, we introduce TPI-Bench, which includes TPI-Test for measuring response strategies under interruptions and Janus-Test for probing whether models can distinguish true multi-speaker utterances from acoustically single-speaker yet textually misleading speech. To ensure reproducible and interpretable assessments, we also design two complementary metrics: Response Strategy Following (RSF) and Overall Helpfulness (OH). Experiments demonstrate that models fine-tuned with our approach achieve robust performance on TPI-Bench while preserving general dialogue capabilities on VoiceBench, effectively avoiding reliance on textual shortcuts. Human evaluations further confirm that both our dataset and trained models align with human preferences, establishing the first comprehensive solution for TPI-aware voice assistants. Our dataset will be publicly available, Demo samples: https://tpi-va.github.io/.", "tldr": "We define Third-Party Interruption (TPI) awareness and develop a corresponding framework, dataset, and benchmark for developing TPI-aware voice assistants.", "keywords": ["Third-Party Interruption", "Voice Assistant", "Spoken Language Model", "Spoken Dialogue System"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f5e41e44a45d0a65307009c55937134321bf516.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The study's major success lies in establishing the first comprehensive TPI-Awareness framework and developing the innovative Janus-Test benchmark, which rigorously isolates acoustic perception to prevent semantic shortcut learning. The use of a composite training strategy with Hard-Negatives is validated as effective in achieving genuine acoustic discrimination, with the final model's responses being highly aligned with user preferences. However, the study's weaknesses center on generalization and depth: the current response strategy diversity is limited to \"actionable\" and \"ignorable\" categories; the reliance on synthesized speech for the TPI-Corpus may restrict the model's robustness against complex, real-world acoustic dynamics; and the system lacks demonstrated capability in handling long-term dialogue context."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel and Comprehensive Framework for TPI-Awareness:** The paper introduces and formalizes the concept of **TPI-awareness** (Third-Party Interruption-Awareness), establishing the first holistic framework for tackling this crucial, yet underexplored, real-world challenge in conversational AI. The definition, based on two core capabilities (*Discerning Speaker Interruption* and *Situation-Discriminative Response*), provides a clear and actionable path for future research.\n\n2. **Creation of Rigorous Benchmarks to Isolate Acoustic and Semantic Cues:** The authors develop **TPI-Bench** (comprising TPI-Test and the innovative **Janus-Test**) specifically to diagnose the critical failure mode of \"shortcut learning.\" The Janus-Test is a strong methodological contribution as it cleverly forces models to rely on **acoustic evidence** rather than misleading textual cues, enabling a truly robust evaluation of TPI-aware models.\n\n3. **Effective Training Strategy Validated by Ablation and Mechanism Analysis:** The work proposes a novel **composite training approach** that incorporates carefully constructed **Hard-Negatives**. Experimental results, particularly the successful performance on the Janus-Test and the visualization of clearly separated embedding clusters, provide compelling evidence that this strategy is effective in mitigating semantic shortcut learning and enforcing genuine acoustic discrimination."}, "weaknesses": {"value": "1. **Limited Strategy Diversity, Lack of Richer Response Modes:** Although the paper proposes the binary framework of \"Actionable\" ($C_A$) and \"Ignorable\" ($C_I$), this categorization of response strategies is still relatively limited. In complex real-world multi-party dialogues, VAs may require richer response modes such as **seeking confirmation from the primary speaker**, **temporarily maintaining silence**, **logging the third-party input without immediate action**, or **explicitly asking for clarification of intent**. The current framework lacks exploration and benchmarking for these more nuanced, socially sophisticated response modes, limiting the model's practical utility and naturalness in complex human-AI interactions.\n\n2. **TPI-Corpus Reliance on Synthesized Speech, Limited Acoustic Realism:** The entire TPI-Corpus and TPI-Bench are built using **speaker-adaptive Text-to-Speech (TTS)**. While WER verification ensures high transcription quality, synthesized speech often fails to capture the full acoustic complexity of genuine human-human interruptions (e.g., shifts in emotion, accelerated speaking rate during overlaps, spontaneous dynamics). This limitation may cause the model's performance to degrade when generalizing to the subtle acoustic cues present in **real-world, unscripted speech**.\n\n3. **Lack of Capability in Handling Long-Term Dialogue Context:** The paper primarily focuses on the immediate handling of **single interruption events** ($U_{p \\to tp} = (U_p, U_{tp})$). However, in practical applications, a third-party interruption might be a persistent issue or require reference to **earlier dialogue history**. The model does not demonstrate the ability to **maintain third-party identity and context** across multiple turns (e.g., remembering a constraint set by the third-party in an earlier turn). A benchmark for evaluating TPI-awareness in complex, long-term multi-party dialogues is needed."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R3XiXD1wVd", "forum": "h6VKMq5ZrD", "replyto": "h6VKMq5ZrD", "signatures": ["ICLR.cc/2026/Conference/Submission23833/Reviewer_ThYP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23833/Reviewer_ThYP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477336987, "cdate": 1761477336987, "tmdate": 1762942825316, "mdate": 1762942825316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on the robustness of voice assistants in third-party interruption (TPI) scenarios, proposing:\n\n1. The TPI-Corpus (containing approximately 80,000 training samples covering 26 interruption scenarios) and the TPI-Bench evaluation set (comprising two parts: TPI-Test for real dual-speaker interruptions, and Janus-Test for text-identical but single-speaker re-synthesized interruptions to expose text shortcut issues)\n\n2. Evaluation across two dimensions: RSF (Response Strategy Following, indicating adherence to processing strategies) and OH (Overall Helpfulness).\n\n3. This work builds upon the Qwen2.5-Omni-7B foundation, progressively incorporating VoiceAssistant-400K and 8k “hard-negative” single-speaker samples to suppress text shortcuts.\n\nResults show that without significantly compromising general speech task capabilities (evaluated using VoiceBench), the model better follows strategies on the TPI-Test and is less misled on the Janus-Test. Additionally, human preference experiments reveal model outputs align closely with reference answer preferences, corroborating the effectiveness of the strategy and training approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- TPI represents a key obstacle to enhancing the practicality of voice assistants in real-world settings. This paper accurately identifies and systematically addresses this issue, demonstrating significant practical relevance and research value.\n- The ingenious design of the Janus-Test stands as one of the most prominent highlights of this paper. By controlling for consistent textual content while varying acoustic features, it provides a direct and powerful tool for detecting shortcut learning issues. This significantly enhances the persuasiveness of evaluation conclusions, demonstrating that the model possesses auditory capabilities.\n- The experimental section is comprehensively constructed. Table 1 (e.g., Qwen-it performs well on the TPI-Test but fails on the Janus-Test) reveals the essence of the TPI problem, clearly demonstrating the contribution of each training data component to model performance, and strongly supports the core argument."}, "weaknesses": {"value": "- The entire TPI-Corpus is constructed using text generated by LLM and speech synthesized by TTS systems. The synthesized speech may not fully capture the complex prosodic, emotional, and temporal dynamics present in real human interruptions (e.g., the urgency in the interrupter's voice, the precise timing of overlapping segments, etc.). Supplementing the corpus with a small-scale TPI test set recorded by real humans would further enhance the reliability of the conclusions.\n- Simply categorizing interruptions into “actionable” and ‘ignorable’ types is an effective simplification, but more complex scenarios may exist in practice. For instance, in “ignorable” situations, highly intelligent assistants might choose to acknowledge the interruption during their response (e.g., prompting the speaker to continue or confirming whether the interruption should be ignored) rather than completely disregarding it. The binary nature of the current framework may limit exploration of more granular interaction strategies."}, "questions": {"value": "- In both dataset construction (e.g., scenario dialogue generation) and final evaluation, this work employed Qwen3-235B. Could this introduce bias in the LLM's evaluation of models trained on this dataset, potentially leading to inaccurate experimental results? Could the authors provide evaluation results using other LLMs as judge models, or demonstrate consistency between LLM and human evaluations on smaller datasets?\n- The current approach to handling “ignorable” interrupts is to completely disregard them. Have you considered alternative, more nuanced handling strategies, such as providing a brief acknowledgment or pause before responding to the primary user? What considerations led to the decision to ignore them entirely (e.g., simplifying the task, or deeming this the optimal strategy)?\n- Does the fine-tuned model still produce similar results on the TPI-Test when using speech data from other languages (non-English), non-LibriSpeech domain-specific speakers, and non-Chatterbox domain-specific TTS models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b6gYlo1xg9", "forum": "h6VKMq5ZrD", "replyto": "h6VKMq5ZrD", "signatures": ["ICLR.cc/2026/Conference/Submission23833/Reviewer_2Vzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23833/Reviewer_2Vzi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492079383, "cdate": 1761492079383, "tmdate": 1762942824168, "mdate": 1762942824168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the ability of voice assistant systems to handle third-party interruptions using acoustic and textual cues. To support this, the authors construct a large synthetic dataset of interruptions by injecting simulated interjections from third speakers into user-VA dialogues, and propthat integrates audio features and is trained with hard negative examples designed to challenge text-only shortcutting. First, they show that baseline models struggle to handle these synthetic interruptions when speaker identity is manipulated, and that training on synthetic interruptions significantly improves performance on a evaluation set sampled from the synthetic corpus. Then, they introduce a paired contrast set designed to isolate acoustic grounding, and show that hard negative training further improves robustness by encouraging models to rely on speaker change cues rather than transcript semantics alone."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work introduces a synthetic data pipeline for constructing interruptions which can be used to finetune models to improve their robustness to third party interruptions.\n- The work shows that they are able to finetune models on this dataset without significant degradation to other voice assistant capabilities.\n- The Janus-Test counterfactual setup is a clever way to probe whether models are actually using speaker change information versus relying on semantic shortcuts."}, "weaknesses": {"value": "The work does not provide any clear separated validation for the effectiveness of their synthetic data pipeline. The paper generates interruptions synthetically and solely validate the utility of finetuning on this data by evaluating on data generated using the same synthetic data generation process. This leaves me significantly unclear whether this dataset helps with real world interruptions.\n\nWhile the synthetic data process seems likely valuable for training, I would expect to see one of the following things:\n(1) TPI-Test could be validated by heavily quality filtering using human judgements to confirm that this subset is one that humans agree is high quality and realistic of real world interruptions. \n(2) TPI-Corpus (train split) could be validated using a relatively small sample of real human interruptions gathered from the web (for example, filtered from large web-scale audio corporate such as YODAS2). \n\nReal world interruptions involve far more auditory cues than simply changing speaker voice (some speakers may be closer or further away from the microphone for example) and it seems necessary to do at least some amount of validation of the data using either human judgements on the proposed test data or additional human evaluation on the trained models. Otherwise, I worry that this dataset could only be improving the ability of the model to respond to the synthetic data distribution."}, "questions": {"value": "- What is just Qwen2.5-Omni-it? The other models seem to be defined, but this one isn't defined within the paper. https://huggingface.co/collections/Qwen/qwen25-omni doesn't show this being one of the release Qwen 2.5 Omni models, but the lack of definition seems odd if this is newly defined in this paper.\n\n- Was there any form of non-synthetic validation done of the data generation process? Given that the core argument is that this should help the model deal with real auditory\n\n- Is there any form of synthetic variation for the TPI's created other than synthetic speaker swaps? It seems possible to add other forms of realistic noise induced by synthetic interruptions.\n\n- Are all synthetic interruptions strictly turn based (as shown in the figures)? Real world interruptions seem likely to have overlapping speech, but this isn't mentioned in the work.\n\n- Do the LLM-based helpfulness judgments correlate with human judgments in any way? Were any human evaluations of the model responses collected to ground those metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JX95dTB1r3", "forum": "h6VKMq5ZrD", "replyto": "h6VKMq5ZrD", "signatures": ["ICLR.cc/2026/Conference/Submission23833/Reviewer_Uiy5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23833/Reviewer_Uiy5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888046633, "cdate": 1761888046633, "tmdate": 1762942823874, "mdate": 1762942823874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic study of voice assistants’ performance in third-party interruption scenarios, introducing the first large-scale TPI-aware dataset and a comprehensive evaluation framework. The work is innovative in dataset construction, task definition, and evaluation methodology, and it can significantly advance the practical deployment of voice assistants in complex multi-speaker environments. The overall structure is clear, experiments are thorough, and the paper offers substantial academic and practical value."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The dataset is very useful and provides a solid foundation for research on third-party interruptions in voice assistants.\n2. The experiments are comprehensive and effectively demonstrate the method’s effectiveness.\n3. The paper is well-structured and clearly written"}, "weaknesses": {"value": "1. The evaluation is not comprehensive enough, as results for some state-of-the-art closed-source dialogue models (such as GPT-4o-audio, Gemini 2.5 Pro, etc.) are missing.\n2. The case presentation is somewhat insufficient; while the paper showcases some cases from the dataset, it lacks output cases from comparative models, making it difficult to intuitively understand the differences in metrics.\n3. Although the paper proposes a valuable dataset and evaluation strategy, there is a lack of methodological innovation in model training."}, "questions": {"value": "1. In constructing the TPI-Corpus, how do you ensure diversity across different scenarios and speakers?\n2. User preferences for handling interruptions may vary. What directions for improvement do you see for your work in this aspect?\n3. In Section 3.4 you mention that the open-source LLM’s performance is close to closed-source models. Is there a quantified comparison for this task? Also, how large is the gap between LLM-generated and human-generated results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "chhHxodv7y", "forum": "h6VKMq5ZrD", "replyto": "h6VKMq5ZrD", "signatures": ["ICLR.cc/2026/Conference/Submission23833/Reviewer_zKni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23833/Reviewer_zKni"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990327643, "cdate": 1761990327643, "tmdate": 1762942823576, "mdate": 1762942823576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}