{"id": "1pIa8ckFtF", "number": 10101, "cdate": 1758160701153, "mdate": 1759897674018, "content": {"title": "TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning", "abstract": "Time series reasoning is crucial to decision-making in diverse domains, including finance, energy usage, traffic, weather, and scientific discovery. While existing time series foundation models (TSFMs) can capture low-level dynamic patterns and provide accurate forecasting, further analysis usually requires additional background knowledge and sophisticated reasoning, which are lacking in most TSFMs but can be achieved through large language models (LLMs). On the other hand, without expensive post-training, LLMs often struggle with the numerical understanding of time series data. Although it is intuitive to integrate the two types of models, developing effective training recipes that align the two modalities for reasoning tasks is still an open challenge.\nTo this end, we propose TS-Reasoner that aligns the latent representations of TSFMs with the textual inputs of LLMs for downstream understanding/reasoning tasks. \nSpecifically, we propose a simple yet effective method to curate diverse, synthetic pairs of time series and textual captions for alignment training. We then develop a two-stage training recipe that applies instruction finetuning after the alignment pretraining. Unlike existing works that train an LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it during training. \nExtensive experiments on several benchmarks demonstrate that TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision Language Models (VLMs), and Time Series LLMs, but also achieves this with remarkable data efficiency, e.g., using less than half the training data.", "tldr": "", "keywords": ["Time Series", "Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e7d788140eb31c807371d06564abfb79dd536e1.pdf", "supplementary_material": "/attachment/e9b49060a70083339a42e7f8299158e212f50bc9.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors proposed a new way to align time series foundation models with LLM to enhance its reasoning ability. Specifically, the authors used the embedding of a frozen TSFM as input and map the embedding to the semantic space of text input. They further proposed a captioning method to generate the training data. Extensive experiments demonstrate the effectiveness of the proposed model TS-REASONER."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper is well-written and easy to follow.\nS2: The idea of integrating TSFM with LLM is interesting and sounds technical.\nS3: Extensive experiments are conducted."}, "weaknesses": {"value": "W1: There are two factors affects the results (the third one two-stage training is from existing work): 1). Use TSFM's embedding as input. 2). Generated a caption dataset for alignment. It is not clear which one contributes to the final improvement. I would suggest doing the following control experiments: a) training ChatTS with the generated datasets to see whether using generated datasets can improve performance. b) training TS-Reasoner with ChatTS's datasets to show whether using TSFM can improve performance.\nW2: The authors did not mention whether they would share the trained model and the generated caption datasets, which would make the reproducibility of the paper a bit difficult."}, "questions": {"value": "Q1: Did the authors evaluate the reasoning model (e.g. deepseek-R1)? They are expected to perform better on reasoning tasks.\nQ2: I am not sure whether the proposed approach has a data leakage risk. Are there any overlaps between the time series datasets used to train TimesFM and those in TimeSeriesExam and MTBench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fcjWfWbeB6", "forum": "1pIa8ckFtF", "replyto": "1pIa8ckFtF", "signatures": ["ICLR.cc/2026/Conference/Submission10101/Reviewer_Fudn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10101/Reviewer_Fudn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569290425, "cdate": 1761569290425, "tmdate": 1762921482908, "mdate": 1762921482908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TS-REASONER, a framework designed to enhance time series reasoning by aligning a pre-trained Time Series Foundation Model (TSFM) with a Large Language Model (LLM). Its core architecture consists of a frozen, pre-trained TSFM (TimesFM) for encoding time series patches , a trainable TS-to-Text adapter (MLP) to project these features , and a pre-trained LLM (Qwen-2.5-7B).\nThe model employs a two-stage training process: (1) an alignment pre-training stage, and (2) an instruction fine-tuning stage.\nA key contribution is the \"attribute-aware captioning\" method developed to address the scarcity of time series-text pairs. This method involves generating high-quality, diverse captions by feeding an advanced VLM (like GPT-4.1) visual plots of the time series, along with prompts enriched with key attributes (e.g., trend, periodicity, noise).\nExtensive experiments on the TimeSeriesExam and MTBench benchmarks demonstrate that TS-REASONER significantly outperforms LLM, VLM, and TSLLM baselines of comparable size and shows remarkable data efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "• Novel Data Curation Method: The \"attribute-aware captioning\" method is the core strength. Using a VLM to process time series plots (rather than raw numerical strings) to generate rich, diverse captions creatively solves a well-known data scarcity problem in the field.\n\n• Strong Empirical Performance: The paper demonstrates SOTA results, with TS-REASONER significantly outperforming a wide range of similarly-sized baselines (LLMs, VLMs, TSLLMs) on two challenging benchmarks.\n\n• Data and Parameter Efficiency: The framework is highly efficient. By leveraging a pre-trained and frozen TSFM, the model inherits a strong foundation for temporal understanding, allowing it to achieve superior performance with less than half the training data of baselines like ChatTS-7B. This makes the approach highly practical.\n\n• Thorough Ablation and Analysis: The paper includes comprehensive ablation studies (Table 3) that validate every key design choice: the necessity of the TSFM, the importance of the attribute-aware captions, and the contribution of each training stage. Further analysis on TSFM (Table 2) and LLM (Figure 6) choices also supports the claims.\n\n• Clarity: The paper's writing and organization are excellent. Figures 3 and 4 are clear, intuitive visualizations of the model architecture and the novel captioning workflow."}, "weaknesses": {"value": "• Reliance on Proprietary Models: The key contribution (attribute-aware captioning) relies on an advanced proprietary model, GPT-4.1, to generate the training data. While the paper shows this is effective (Figure 7 demonstrates GPT-4.1 captions work best ), this creates a dependency and a potential reproducibility challenge, as the alignment data quality is tied to the capability of an external, closed-source model.\n\n• Limited Exploration of TSFMs: The paper primarily uses TimesFM. While Table 2 compares it to MOMENT, the paper would be stronger if it explored a wider range of modern TSFMs (e.g., Chronos, Moirai) as the backbone. This would help establish the generality of the \"frozen TSFM\" approach.\n\n• Insufficient Justification for \"Frozen\" TSFM: The TSFM is kept frozen throughout training. The ablation in Table 3 (\"-TSFM\") removes it entirely, which is a different experiment. A more direct ablation would compare a frozen TSFM to a fine-tuned TSFM. While fine-tuning might risk catastrophic forgetting, it could also potentially improve alignment. The paper's justification of \"preserv[ing] its pretrained temporal knowledge\" is intuitive but not empirically compared against the fine-tuning alternative."}, "questions": {"value": "• Keeping the TSFM parameters frozen is a key design choice. The ablation in Table 3 compares this to removing the TSFM. Did the authors experiment with fine-tuning the TSFM (i.e., not freezing it) during the alignment or instruction-tuning stages? If so, how did its performance and training stability compare to the frozen approach?\n\n• The data efficiency results in Figure 5 are impressive. The comparison is made to ChatTS-7B, which the authors note trains its time series encoder from scratch. Is the efficiency gain primarily due to using a pre-trained TSFM, rather than the novel captioning method? How data-efficient is TS-REASONER if trained only on the template data from (Xie et al., 2024), without the new LLM-generated captions? This would help isolate the efficiency contribution of the captioning method itself.\n\n• The captioning process relies heavily on plotting the time series as an image and using a VLM. The qualitative case study in Figure 9 suggests this is superior to using raw text. Was a quantitative comparison performed? For example, what is the performance of TS-REASONER if trained on captions generated by GPT-4.1 (a strong LLM) using the numerical string (not the plot) as input? This would further validate the importance of the visual representation for caption generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sB7r6vQUlL", "forum": "1pIa8ckFtF", "replyto": "1pIa8ckFtF", "signatures": ["ICLR.cc/2026/Conference/Submission10101/Reviewer_cNdi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10101/Reviewer_cNdi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796214757, "cdate": 1761796214757, "tmdate": 1762921482498, "mdate": 1762921482498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TS-REASONER, a framework designed to improve the reasoning capabilities of Large Language Models for time series analysis. The core idea is to align the representations of a specialized Time Series Foundation Model with the input of an LLM, combining the TSFM's strength in capturing temporal patterns with the LLM's advanced reasoning skills in a 2 stage approach of first learning time-series alignment, then finetuning via instruction tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Wide range of baselines in evaluation and good exploration of scaling. \n* Code release at submission time"}, "weaknesses": {"value": "* My primary concern is that this paper seems to be over-claiming on their novelty by stating \"our work makes unique contributions .... sets up connection between the connection between TSFMs and LLMS .... simple yet effective time series caption method...\". From what I understand of this paper, most of it seems very similar if not exactly the same as the prior work [1]. The 2 stage approach with alignment + fine-tuning, exactly mimics the approach used in [1]. For example, the \"G pertinent attributes of time-series\" in stage 1, directly copies how [1] generates synthetic time-series and captions. Additionally, in the Ablation studies, \"(1) Attribute-aware captioning is critical for robust language-timeseries alignment\" and \"(2) Absence of any training stage significantly harms the performance. \" were key insights already found in [1]. There is no discussion in this paper about the parallels of this work to [1] as well, which makes this all the more concerning. \n* I acknowledge that there are minor changes from [1], but such changes are incremental i.e. using an LLM prompt to help generate synthetic text instead of template only. I define these as incremental because there is not an in depth analysis on how the prompting improves diversity compared to a template.\n* Another similarity to [1] is the ts-to-text adapter, but a K indicator is used for multiple time-series as input. However, I can't seem to find any results or discussion of the actual datasets or experimental results that exploit this property, either via multivariate time-series or using multiple at once. \n* Further experimental results explore unnecessary directions, such as differing TSFMs or differing LLMs. This is unnecessary because the methodology does not depend on the pre-trained methodology, so any results do not yield further insight other than \"this pre-trained model did better for some abstract reason\".\n\n[1] Chow, Winnie, et al. \"Towards time series reasoning with llms.\" arXiv preprint arXiv:2409.11376 (2024)."}, "questions": {"value": "* what are the key differences between this work and Chow et al. ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vGEvFOcngY", "forum": "1pIa8ckFtF", "replyto": "1pIa8ckFtF", "signatures": ["ICLR.cc/2026/Conference/Submission10101/Reviewer_9Cxb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10101/Reviewer_9Cxb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885996366, "cdate": 1761885996366, "tmdate": 1762921481818, "mdate": 1762921481818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TS-REASONER, a two-stage framework that aligns a Time Series Foundation Model (TSFM) with an LLM for time-series understanding and reasoning. The authors evaluate on TimeSeriesExam and MTBench, reporting gains over same-size LLM/VLM/TS-LLM baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The two-stage design (alignment → instruction tuning) is easy to follow.\n\nShows improvements on both benchmarks against same-size LLM/VLM/TS-LLM.\n\n Removing Stage-1/Stage-2, attributes/LLM-caption, or the TSFM all lead to notable drops.\n\nImplementation is provided."}, "weaknesses": {"value": "Undefined “time-series reasoning.” The paper does not clearly define the term. The main benchmark, TimeSeriesExam, is positioned as a time-series understanding exam and mainly tests recognition/comprehension.\n\nLimited novelty. The pipeline—TSFM representations → adapter to LLM embeddings → two stages (caption-alignment pretrain → instruction tuning)—is very close to prior work (e.g., ChatTS with attribute-aware synthetic series + textual descriptions + alignment training; ChatTime on unified time-series↔text modeling).\n\nNarrow evaluation. Focuses on template multiple-choice tasks; lacks open-ended QA and natural-language explanations (e.g., the open-QA setup used in ChatTS).\n\nInconsistent trainable modules. The main text states “TSFM frozen, LLM trainable,” while the appendix says “all backbone parameters are finetuned.” This conflict affects reproducibility and cost claims.\n\nInsufficient statistical robustness. Missing multi-seed results, confidence intervals, or bootstrap tests; some gains appear within typical variance.\n\nEfficiency claims lack evidence. No end-to-end measurements for training time, peak memory, inference latency/throughput, or cost per accuracy/token, which are needed to support the efficiency claim."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3vAdyfJRvC", "forum": "1pIa8ckFtF", "replyto": "1pIa8ckFtF", "signatures": ["ICLR.cc/2026/Conference/Submission10101/Reviewer_P6KN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10101/Reviewer_P6KN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917423500, "cdate": 1761917423500, "tmdate": 1762921481320, "mdate": 1762921481320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}