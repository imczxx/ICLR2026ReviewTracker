{"id": "BZKLrLzKlq", "number": 9339, "cdate": 1758119432536, "mdate": 1759897730597, "content": {"title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward Supervision", "abstract": "We introduce SAIL-RL, a reinforcement learning~(RL)–based post-training framework that advances the reasoning capability of Multimodal Large Language Models (MLLMs) by teaching them when and how to think. Existing approaches are hindered by outcome-only supervision, which rewards correct answers without sound reasoning, and uniform thinking strategies, which cause overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Quality Reward supervises how models reason by evaluating factual grounding, logical coherence, and answer consistency, while the Thinking Judge Reward determines when to engage deep reasoning versus direct answering. Experiments on the state-of-the-art SAIL-VL2 demonstrate that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieves leading results on OpenCompass, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs.", "tldr": "", "keywords": ["Vision-language Model; Multi-modal Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a73f1ff32e7fb38403ed79a45463c06ce7429e13.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "SAIL-RL proposes a novel reinforcement learning post-training framework. The motivation for this is to resolve two challenges: 1. Nowadays the reward is only designed based on the final answer without evaluation of reasoning steps 2. The models sometimes overthink on simple problems and simplifies the complex problems. The SAIL-RL consists of a dual-reward system:\n- Thinking reward, which consists of Logical Coherence, Factual Grounding and Answer Consistency.\n- Judging reward, justifies whether the problem is worth thinking.\n\nThe training strategy consists of two steps: LongCoT SFT and RL Finetuning. The evaluation results validates the models' superiority on multimodal reasoning benchmarks such as MathVista, LogicVista, and DynaMath."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the SAIL-RL is good. The framework for RL post-training is novel, it teaches the model when to think and what to think.\n2. The RL post-training framework can reduce the hallucination rate of SAIL-RL.\n3. The evaluation set up is comprehensive, authors evaluate on both mathematical reasoning benchmarks like mathverse, mathvision, and also multimodal understanding benchmarks like MMMUval.\n4. The structure of the paper is clear and easy to read."}, "weaknesses": {"value": "1. In L296, **High computational cost**: the method use Gemini as reward model, as RL needs many roll out, this setup is computationally expensive and hard to reproduce. \n2. **Stability**: Continuous request of Gemini is not stable for large-scale deployment.\n3. **Potential Reward Hacking**: The solely usage of Gemini as reward model, the model may overfit to the pattern of Gemini, instead of truly improve reasoning abilities.\n4. As illustrated in L433, 1.3% and 0.5% improvement is not significant gain, and **can not prove avoid overthinking is crucial**. I would also like to know is the experiment result only for one time inference or multi-times?\n5. **Lack of the prompt and supplementary materials**: the paper lacks detailed prompt descriptions, which weakens its transparency, reproducibility, and interpretability. e.g. 1. the prompt for the judge? 2. the structure of the CoT thinking? 3. the format of final answer output? These significantly reduce the reproducibility of this paper."}, "questions": {"value": "1. In line 291, the author claims that they have 100k data but in line 255 the author claims there is a high-quality dataset of 70K problems for RL training. What's the difference between those two datasets?\n2. In L296, 100k data and many rollouts leads to 1000k+ requests, and consider the long chain-of-thought, so I want to know the cost for that. In the meantime, the prompt format? Could answer provides examples as I listed in weakness?\n3. In Table2, The paper only reports the final SAIL-RL results. It would be helpful to include intermediate results (e.g., after one epoch of RL) to better illustrate the effect and learning progress of RL tuning.\n4. L413, your method is blue? answer only? Is this a typo here?\n5. In L433, is the experiment average after multiple inference or only once? I cannot see evidence for avoiding overthinking is crucial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S58zRO8l8K", "forum": "BZKLrLzKlq", "replyto": "BZKLrLzKlq", "signatures": ["ICLR.cc/2026/Conference/Submission9339/Reviewer_35oa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9339/Reviewer_35oa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866848087, "cdate": 1761866848087, "tmdate": 1762920970682, "mdate": 1762920970682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAIL-RL, a RL framework designed to improve the reasoning capabilities of MLLMs via a dual-reward design: thinking reward that evaluates the quality of the reasoning process, and judging reward that decides when to engage in deep reasoning or direct answering.\n\nAfter applying this framework to the SAIL-VL2 model, and the resulting SAIL-VL2-Thinking shows state-of-the-art performance on multiple mathematical reasoning and general multimodal understanding benchmarks, outperforming comparable open-source models and competing with powerful closed-source models like GPT-4o."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To solve the lucky guess with flawed reasoning and over-thinking/simple-thinking in different questions, this paper design thinking reward and judge reward. \n2. Propose a LongCoT dataset for SFT and extra dataset for RL training.\n3. Equipping with SAIL-RL, SAIL-VL2 presents impressive performance on various benchmarks, significantly improving over its baseline and surpassing other leading open-source models.\n4. The paper is well-written, logically structured, and easy to follow."}, "weaknesses": {"value": "1. The components of the Thinking Reward (Logical Coherence, Factual Grounding, Answer Consistency) and the Judging Reward are all binary (0 or 1). A more continuous or granular scoring system might provide a richer and more accurate learning signal.\n2. Are there specific types of problems or reasoning patterns where SAIL-RL struggles? For example, does it handle ambiguity or multi-step causal reasoning as effectively? An exploration of its boundaries would be interesting.\n3. For thinking rewards, only average operation is used for integrate three sub-rewards. Did you try other options for the integration of different rewards?\n4. Did you apply this RL framework in other open-source models beyond SAIL-VL2\n5. Why the DAPO is used for RL training? Any other RL algorithms are tried.\n6.It's better to give more details about the creation of ground truth labels based on task complexity."}, "questions": {"value": "1. More continuous or granular scoring methods can be explored for reward.\n2. How to define the ground truth labels based on task complexity? \n3. Did the authors try different integration methods for different rewards \n4. Proposed RL framework should be applied to some existing open-source MLLMs to see its perfmance improvement. \n5. Any other RL optimization methods are explored?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hz5V1V5pOC", "forum": "BZKLrLzKlq", "replyto": "BZKLrLzKlq", "signatures": ["ICLR.cc/2026/Conference/Submission9339/Reviewer_sUvu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9339/Reviewer_sUvu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877961586, "cdate": 1761877961586, "tmdate": 1762920970403, "mdate": 1762920970403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing RL approaches on MLLMs are limited by outcome-only supervisionand uniform thinking strategies.\nThis paper introduces a dual reward system that explicitly supervises both the quality of reasoning and the adaptivity of thinking strategies.\n\nExperiments on SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieves state-of-the-art results among models of comparable size."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a dual reward system to address two key issues: the lack of rewards for reasoning trajectories and the insufficient adaptivity of thinking strategies.\n- The authors perform extensive experiments to validate the effectiveness of the proposed method."}, "weaknesses": {"value": "- The related work section should include a more detailed discussion of existing RL methods for MLLMs, and explicitly clarify how the proposed approach differs from prior works.\n- Table 1 omits the performance results of the base model (SAIL-VL). Also, please clarify whether SAIL-VL-Instruct refers to the model fine-tuned with the proposed LongCoT dataset or the base model itself. The paper should include the results of the base model, the SFT model (after stage 1), and the final model, to better illustrate the performance improvement achieved at each training stage.\n- The legend in Figure 5 is incorrect and should be flipped\n- The paper lacks sufficient details on the prompts used to instruct Gemini as a reward judge. It is also unclear how the reward scores are derived from Gemini’s textual outputs.\n- The paper introduces a cascading product formulation for combining the judge, think, and answer rewards instead of an additive combination. It would be good if the paper could provide reference or ablation studies validating this design choice."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R7QaLDMazy", "forum": "BZKLrLzKlq", "replyto": "BZKLrLzKlq", "signatures": ["ICLR.cc/2026/Conference/Submission9339/Reviewer_aBKU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9339/Reviewer_aBKU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975121700, "cdate": 1761975121700, "tmdate": 1762920969921, "mdate": 1762920969921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an RL post-training framework for multimodal LLMs that teaches both *when to think* and *what to think*. The method adds two learning signals on top of the RLVR answer and format rewards: a Thinking Reward that scores logic, factual grounding, and answer consistency, and a Judging Reward that determines whether to trigger detailed reasoning or provide an answer directly. Gemini-2.5-Pro serves as the reward judge during RL. Results report gains on OpenCompass reasoning and general V+L benchmarks and show adaptive “thinking trigger” rates by task type."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* outcome-only RL can reward lucky guesses and uniform “always think” policies. The dual rewards target both reasoning quality and adaptivity. \n* logic, grounding, and consistency are explicitly checked in the Thinking Reward. The total reward uses a multiplicative “AND-gate” to reduce reward hacking.  \n* Figure 4 reports low trigger rates and high rates on math-heavy ones, suggesting resource allocation by task type. \n* The 8B “Thinking” model improves over its “Instruct” base and is strong among open-source models on both reasoning and general V+L benchmarks."}, "weaknesses": {"value": "1. The Thinking Trigger analysis (Figure 4: “Evaluation results on thinking trigger”) only shows the adaptive allocation of reasoning resources after training. It will be good also to compare at least: a) the original model without training, b) GRPO with only R-answer and no judge in the output, and c) a variant that includes the judge output but removes the *when to think* reward. These baselines would show whether SAIL-RL truly teaches a sharper judge for *when to think* and by how much.\n2. Beyond Figure 4, the *when to think* analysis is thin. For example, compared to a model trained without the *when to think* reward, by how much does total token usage drop overall?\n3. Related work is weak, which makes it hard to assess novelty and the paper’s uniqueness relative to recent work. Literature on System 1 and System 2 thinking is relevant here, as the judge essentially serves as a router that chooses between the two. Additionally, the main design novelty lies in rewards; therefore, the RL section should discuss recent designs, such as work that adds length-relevant rewards (for your *when to think* design) and others that incorporate fine-grained LLM-Judge-based process rewards for the thinking process (for your *what to think* design), for a more thorough examination of GRPO-style RLVR."}, "questions": {"value": "1. I am curious whether you observe some cases where SAIL-VL2-Thinking judges that thinking is needed, but during the thinking process, it realizes the initial judgment was wrong and the problem can be solved quickly. The reverse may also occur. Intuitively, this is possible. Humans also misjudge difficulty at first. Unlike your current output format, humans can adjust the decision dynamically while thinking. Does the model encounter this, and if so, what happens as a result?\n2. The legend in Figure 5 looks reversed.\n3. One point of confusion: the training stage uses Gemini-2.5-Pro as the VLM-Judge, while evaluation uses GPT-4o-Mini as the judge. I am not sure GPT-4o-Mini is strong enough as an evaluator, given that some baselines include stronger models like GPT-4o. Why use a weaker model as the LLM judge for evaluation? Since you can use Gemini-2.5-Pro, you should also be able to use a model of similar strength, for example, GPT-5 or at least GPT-5-mini."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rXTBzj0KKD", "forum": "BZKLrLzKlq", "replyto": "BZKLrLzKlq", "signatures": ["ICLR.cc/2026/Conference/Submission9339/Reviewer_MRiy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9339/Reviewer_MRiy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185024016, "cdate": 1762185024016, "tmdate": 1762920969194, "mdate": 1762920969194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}