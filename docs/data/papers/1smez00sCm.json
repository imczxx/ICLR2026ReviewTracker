{"id": "1smez00sCm", "number": 25103, "cdate": 1758364178195, "mdate": 1759896734028, "content": {"title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models", "abstract": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models.", "tldr": "", "keywords": ["Unified Multimodal Large Models", "Text-to-image generation", "Reasoning Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bde2a3a86f6c2cb4bb358e69993c9cafc2cfe3a0.pdf", "supplementary_material": "/attachment/885f61c737ef0cbcd4031dda5a720fee5c36dcc4.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents R3, a Reason-Reflect-Refine framework for jointly improving unified multimodal model's understanding and generation ability. The proposed framework iteratively reasons on the generated image and refines it based on its own feedback, and further uses RL to finetune the model for better conducting R3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed framework significantly improved the model's instruction following generation power as well as the understanding power.\n- The use of RL to further improve the R3 power gives significant improvements in the designed scores.\n- Such an approach may lead to more advanced multimodal reasoning paradigms."}, "weaknesses": {"value": "- My major concern is that the effect of RL is unclear. Based on results in the paper, RL seems improved the average understanding and generation capability in one Reflection-Regenerate step. But it is unclear whether 1) RL improved the highest possible performance given an unlimited Reflection-Regenerate round budget, OR 2) RL reduced the number of Reflection-Regenerate rounds to have a converged performance.\n- The proposed framework is only tested on one single model, BAGEL. It's necessary to test its generalizability over different MLLM backbones."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MGTtCPfviA", "forum": "1smez00sCm", "replyto": "1smez00sCm", "signatures": ["ICLR.cc/2026/Conference/Submission25103/Reviewer_k1uR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25103/Reviewer_k1uR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959346382, "cdate": 1761959346382, "tmdate": 1762943327162, "mdate": 1762943327162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the long-standing trade-off between generation and understanding in multimodal large models. The authors argue that these two abilities compete for model capacity due to misaligned optimization objectives. To overcome this, they propose the Reason–Reflect–Refine (R3) framework, which reconceptualizes generation as a multi-step process: the model first reasons about the input prompt, then reflects on whether its generated output aligns with the intent, and finally refines the output accordingly. A Tree-based Reinforcement Learning strategy and stage-wise rewards enable stable optimization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This paper demonstrates the trade-off between understanding and generation, where fine-tuning for one capability degrades the other, and naive co-training yields negligible gains.\n* The Tree-RL strategy and stage-wise reward formulation stabilize training.\n* The results demonstrates consistent improvements across multiple benchmarks, including newly introduced VQA and ITA evaluations."}, "weaknesses": {"value": "* The experiments does not seem sufficient. Many other unified understanding and generation models are missing from the comparison. Furthermore, the comparison against proprietary models should not be limited to just one; others, such as Gemini 2.5 Flash or Gemini 2.0 Flash, should also be considered.\n* The proposed method performs worse than GPT-4o on both the GenEval++ and TIIF benchmarks, and this performance gap should be discussed and explained in more detail.\n* The reward model design is more empirial and heuristic. The optimization may still bias toward the reward model rather than true bidirectional alignment with understanding and generation. More theoretical analysis is needed to explain the underlying optimization alignment mechanism."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bSusf6fImx", "forum": "1smez00sCm", "replyto": "1smez00sCm", "signatures": ["ICLR.cc/2026/Conference/Submission25103/Reviewer_c1QJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25103/Reviewer_c1QJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962868085, "cdate": 1761962868085, "tmdate": 1762943326806, "mdate": 1762943326806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the trade-off between generation and understanding capabilities in multimodal models by proposing the Reason-Reflect-Refine (R3) framework. The authors argue that this conflict arises from competing optimization objectives and propose decomposing generation into a multi-step \"generate-understand-regenerate\" process. It proposes R3 (Reason–Reflect–Refine): turn single-shot image generation into a multi-stage loop that (i) plans, (ii) self-evaluates alignment of the produced image to the prompt, and (iii) edits until a learned stop condition (“No further edit needed”). Training alternates policies for the Reason and Reflect-Refine stages with a tree-RL scheme (GRPO for text; FlowGRPO for diffusion) and stage-wise rewards from a VLM judge. Experiments (GenEval/GenEval++/TIIF) report sizable gains in instruction-following and smaller but non-trivial gains on bespoke understanding tests (VQA/ITA built over model-generated images and VLM judgments)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated problem: Recasting generation as generate→understand→regenerate is a well-motivated method aligning with the philosophy of multiple prior COT-related methods. The paper explains the loop clearly and gives a concrete stop rule. \n\n- Optimization design: The tree-RL split reduces variance compared to end-to-end trajectory optimization; stage-specific rewards and formatting checks are well-motivated.\n\n- Empirical gains: On GenEval++ (GPT-4.1 judge), BAGEL+R3 improves overall score vs. BAGEL and edges Echo-4o; understanding proxies (ITA/VQA) also rise. The inference-time scaling plot shows most gains after the first reflect-refine, with saturation after ~4–5 turns—useful for deployment budgets."}, "weaknesses": {"value": "- The whole framrwork idea does not look particularlu novel to me: Self-correction and iterative refinement for generation have been extensively studied.\n\n- I am abit concerned about the practicability of this work. In practice, developers tend to develop a single-shot generation instead of allowing multiplt RR turns, which is expensive. \n\n- Evaluation validity: Generation quality relies on GPT-4.1 as the arbiter for GenEval++ and understanding relies on Gemini 2.5 Flash to create “ground truth” for VQA/ITA—both proprietary VLMs that may share alignment biases with the reward/eval models. This risks overfitting to judge preferences and limits claims about human-perceived quality. Human or crowd-sourced evaluations are highly recommended to validate the effect outside model-judge ecosystems.\n\n- Because reflection quality and termination are rewarded via VLM scores and format checks, the system could learn to optimize the judge (e.g., produce edits that increase the VLM’s “alignment” without visibly improving images, or prematurely stop when the judge saturates). A test where the judge is swapped could be helpful.\n\n- Generalizability: Authors themselves observe domain-specific improvements (training on “counting” helps counting much more than other attributes). That tempers the central claim that R3 “reconciles” the optimization dilemma broadly. Evidence beyond compositional instruction following (e.g., style/photorealism, long-prompt semantics) is limited."}, "questions": {"value": "- Can you report human A/B on a held-out prompt set and the cost per image and effectiveness per RR turn? \n\n- What is the exact “immediate rollout” procedure (sampling temperature, selection by reward diversity, replay buffer size), and how sensitive are gains to it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uicQUri2g2", "forum": "1smez00sCm", "replyto": "1smez00sCm", "signatures": ["ICLR.cc/2026/Conference/Submission25103/Reviewer_r2cT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25103/Reviewer_r2cT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979972987, "cdate": 1761979972987, "tmdate": 1762943326501, "mdate": 1762943326501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the \"optimization dilemma\" in multimodal models, where improving generative capabilities often degrades understanding, and vice versa . The authors posit this conflict arises from competing training objectives for finite model capacity . They propose the Reason-Reflect-Refine (R3) framework, which recasts generation as an iterative \"generate-understand-regenerate\" process. The model first Reasons to create a plan and initial image , then iteratively Reflects on its output (using its understanding) and Refines the image until it aligns with the prompt. The framework is trained end-to-end with reinforcement learning, using a novel Tree-RL strategy for improved stability . Experiments show that R3 not only enhances generation quality but also improves the model's understanding capabilities (e.g., counting accuracy), effectively mitigating the conflict"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Important Problem: The paper addresses the \"generation vs. understanding\" trade-off, a critical and widely recognized challenge in developing unified multimodal models.\n\n\n\nNovel & Intuitive Framework: The R3 framework is a novel solution that embeds understanding as a functional sub-process within generation, creating a synergistic loop rather than a competitive one .\n\n\nStrong Empirical Validation: The core claim is well-supported. Experiments show simultaneous improvements in generation (GenEval++) and dedicated understanding tasks (VQA/ITA), with clear gains in skills like counting .\n\n\n\nEffective Training Strategy: The proposed Tree-RL strategy (Fig. 3) is a solid technical contribution, demonstrating superior stability and reward over standard full-trajectory RL (Fig. 4) ."}, "weaknesses": {"value": "1. The new VQA and ITA benchmarks rely on \"ground truth\" labels generated by Gemini 2.5 Flash. Thus, high scores may reflect better alignment with Gemini's judgment rather than an objective, absolute improvement in understanding.\n\n2. I'm curious whether this method is effective for image editing tasks, especially when spatial understanding is required.\nFor example, suppose there's an image with four people, and my prompt is: \"Move the third person behind the second person,\" instead of simply arranging all four people side by side.\nCan the model correctly interpret and execute such spatially grounded instructions?"}, "questions": {"value": "Overall, I find this method to be simple, effective, and intuitive. I will take the opinions of the other reviewers into consideration as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PlhRGlMJvN", "forum": "1smez00sCm", "replyto": "1smez00sCm", "signatures": ["ICLR.cc/2026/Conference/Submission25103/Reviewer_xGzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25103/Reviewer_xGzx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762749359914, "cdate": 1762749359914, "tmdate": 1762943326073, "mdate": 1762943326073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}