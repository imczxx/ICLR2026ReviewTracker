{"id": "t4WNcclzLE", "number": 20395, "cdate": 1758305479691, "mdate": 1759896979766, "content": {"title": "DADA: Dual Averaging with Distance Adaptation", "abstract": "We present a novel parameter-free universal gradient method for solving convex optimization problems. Our algorithm—Dual Averaging with Distance Adaptation (DADA)–is based on the classical scheme of dual averaging and dynamically adjusts its coefficients based on the observed gradients and the distance between its iterates to the starting point, without the need for knowing any problem-specific parameters. DADA is a universal algorithm that simultaneously works for a wide range of problem classes as long as one is able to bound the local growth of the objective around its minimizer. Particular examples of such problem classes are nonsmooth Lipschitz functions, Lipschitz-smooth functions, Hölder-smooth functions, functions with high-order Lipschitz derivative, quasi-self-concordant functions, and (L0, L1)-smooth functions. Furthermore, in contrast to many existing methods, DADA is suitable not only for unconstrained problems, but also constrained ones, possibly with unbounded domain, and it does not require fixing neither the number of iterations nor the accuracy in advance.", "tldr": "Dual Averaging with Distance Adaptation (DADA), a novel parameter-free universal gradient method for solving convex optimization problems.", "keywords": ["Adaptive Optimization", "Universal Gradient Method", "Dual Averaging"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0187e2f3273d73b171b574a1490be93a44a031c8.pdf", "supplementary_material": "/attachment/4e86f009beba4aa72350afce3f7de3203cbc6ff5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel universal gradient method for solving convex optimization problems, termed Dual Averaging with Distance Adaptation (DADA). The algorithm is based on the classical dual averaging scheme and dynamically adjusts its coefficients based on observed gradients and the distance between iterates and the starting point. This adaptation eliminates the need for problem-specific parameters, making the method broadly applicable. The authors provide theoretical convergence guarantees and demonstrate the practical effectiveness of DADA through experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed DADA algorithm offers a parameter-free approach to convex optimization, enhancing its applicability across various problems.\n\n*  The paper provides rigorous theoretical convergence guarantees for the proposed method.\n\n* Empirical results demonstrate the practical effectiveness of DADA in solving convex optimization problems."}, "weaknesses": {"value": "* I am a bit concerned about the assumption that $x^* \\in \\mathrm{int}(dom(f))$ for constrained problems. For example, in a linearly constrained problem where the constraint set $\\mathcal{Q}$ is a polyhedron, the optimal solution will often lie on the boundary. It seems that the current work may not fully handle constrained settings, and the results may primarily hold for unconstrained problems (please correct me if I am wrong). I suggest clarifying this or considering an unconstrained version of problem (1).\n\n* I suggest using $g$ rather than $\\nabla f(x)$ to denote a subgradient. The notation $\\nabla f$ is typically reserved for the gradient of differentiable functions, and using it for subgradients may cause confusion.\n\n* As discussed in the conclusion, it would be interesting to explore whether the proposed methods can be extended to nonconvex and stochastic settings.\n\n* Are the proposed algorithms universal for convex functions satisfying the KL property? Some discussion or analysis would be helpful.\n\n* In Figure 4, could the authors explain why WDA and DADA exhibit sudden decreases after a large number of oracle calls, whereas other methods show smoother decreases? Additionally, in Figure 4, DADA’s performance is not as good as UGM or Prodigy; could the authors provide insight into this behavior?\n\n* Could the authors clarify what convergence rate the proposed algorithm achieves for strongly convex functions?"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RLU7tWQeLh", "forum": "t4WNcclzLE", "replyto": "t4WNcclzLE", "signatures": ["ICLR.cc/2026/Conference/Submission20395/Reviewer_UHdw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20395/Reviewer_UHdw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637481050, "cdate": 1761637481050, "tmdate": 1762933843720, "mdate": 1762933843720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies gradient methods that can adapt to the distance between the initial solution and the optimal solution. Previous works have developed such algorithms for various settings, both deterministic and stochastic with different assumptions on the gradients. This work shows a single algorithm that works simultaneously for several classes of functions including Lipschitz functions, Lipschitz-smooth functions, Holder-smooth functions, functions with higher order Lipschitz derivative, quasi-self-concordant functions, and (L0, L1) smooth functions. The algorithm is based on the dual averaging method combined with a recent technique for adapting to the distance to the optimal solution, previously applied to gradient descent algorithms."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The new method works simultaneously for many classes of functions, for both constrained and unconstrained problems. The proof is done via a unified approach and the bounds for each function class is a relatively simple corollary of the main proof."}, "weaknesses": {"value": "The new method does not work for the stochastic case whereas some previous works include the stochastic case.\n\nThere is already a previous method that is universal for several classes of functions listed but that work did not consider others like Lipschitz higher order derivative.\n\nIn theory, one can use classical methods like the doubling trick to adapt to the distance and can combine even with tools like acceleration. Thus, the main benefit here is that one does not need to restart."}, "questions": {"value": "Could you please comment on the barrier for obtaining a method with acceleration?\n\nThe previous work Orabona. https://arxiv.org/pdf/2308.05621 seems to suggest that one can combine normalized gradient descent with different algorithms in a similar fashion. The previous \"universal\" result by Khaled et al. used gradient descent whereas this paper uses DA. Is there a fundamental reason for this or do you see your technique being applicable to gradent descent as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ilHxdJyhCG", "forum": "t4WNcclzLE", "replyto": "t4WNcclzLE", "signatures": ["ICLR.cc/2026/Conference/Submission20395/Reviewer_xex2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20395/Reviewer_xex2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875682857, "cdate": 1761875682857, "tmdate": 1762933843138, "mdate": 1762933843138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DADA, a parameter-free version of dual averaging (DA). The main idea of parameter-free methods is to adaptively tune the step sizes based on the observed distance of iterates from the starting point to reduce the cost incurred by the (estimated) initialization scale (denoted by $\\rho$ in the paper). Applying this to DA, we can adjust the parameters based on a running estimate of an adaptively estimated lower bound of $D_t$, and the authors show that this yields convergence guarantees having a smaller $\\log^2 \\rho$ dependence while maintaining rates with respect to other factors. The paper also shows theoretical results under applications to a wide variety of function classes (nonsmooth Lipschitz, Lipschitz-smooth, Hölder-smooth, functions with Lipschitz higher-order derivatives, quasi-self-concordant, and $(L_0, L_1)$-smooth). The rates usually match previously known methods up to logarithmic factors while maintaining the benefits of distance adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It seems like a good idea to use an adaptive, increasing estimate of $D$’s, with a similar spirit (but different in detail) as in methods like D-adaptation, to reduce the cost incurred by estimated initialization scale to a logarithmic term. This allows the algorithm to gain similar benefits in (almost) all the broader classes of convex problems that classical dual averaging can deal with.\n- The theoretical statements, proof sketches, applications to examples, and comparison with previous work are all clear. There also are empirical results that align with the theory proposed in the paper."}, "weaknesses": {"value": "- I am currently giving a rating of 2 because **the format of the submission (exceeding 9 pages) falls into desk rejection criteria.** (*I will update my score if the paper somehow avoids desk rejection.*)\n- See Questions for other details."}, "questions": {"value": "- Are the guarantees for applications should be essentially thought of as considering unconstrained problems? The problem and the main theorem claims to focus an objective with a constrained form, but we have to assume that projections are be easy to compute in order to seamlessly translate iteration complexity into the actual computational costs, and for the applications we also need assumptions like $\\nabla f(x^{\\star}) = 0$ to do the theories, both of which makes me feel like it’s a slight overclaim to advocate DADA as an algorithm with clear theoretical guarantees for constrained problems. Can the authors provide what can we say about DADA on constrained cases, or are there experimental results on constrained problems that can show consistency of the analyses?\n    - The updates of dual averaging type algorithms have some structural similarities to FTRL updates of the form [1, 3]\n    $$ x_{t+1} = \\operatorname{argmin}\\_{x \\in \\mathcal{X}} \\left\\\\{ \\sum_{i=0}^{k-1} \\langle g_i, x_i \\rangle + \\frac{1}{2 \\eta_k} \\\\| x - \\textcolor{red}{x_0} \\\\|^2  \\right\\\\} $$\n    or Frank-Wolfe updates of the form [2, 3]\n    $$ \\begin{align*}\n    d_k &= \\sum_{i=0}^{k-1} g_i + \\frac{1}{2 \\eta_k} (x - \\textcolor{red}{x_0}) \\\\\\\\\n    v_k &= \\operatorname{argmin}\\_{v \\in \\mathcal{X}} \\\\langle d_k, v \\\\rangle \\\\\\\\\n    x_{k+1} &= (1 - \\sigma_k) x_k + \\sigma_k v_k\n    \\end{align*} $$\n    which are both good ways to solve *constrained* optimization problems (FW being *projection free*). In particular, [3] considers updates (with the ${\\textcolor{red}{x_0}}$'s) that look exactly as above but uses $\\eta_k \\equiv \\eta$ and $\\sigma_k \\equiv \\sigma$ for technical reasons (in order to use automated proof generation via PEP).\n    - I am not very familiar with the proposed (classical) dual-averaging algorithms in the paper; could there be a possibility that using distance adaptation and appropriately scaled regularizers (ex. try to match the $\\alpha_k$ and $\\beta_k$ of dual averaging and DADA) somehow yield regret/FW-gap upper bounds with better dependence on initialization scale, or this doesn't make any sense because of the $\\alpha_k$ and $\\beta_k$'s or for some other reason? (This is a light question, I only expect a simple high-level answer on the authors' take on this.)\n- Have the authors tried experiments on the stochastic setting as well? Up to my knowledge, methods like D-adaptation also work quite well under stochastic settings, despite the theoretical statements holding only for deterministic oracles, and I expect something similar for DADA as well.\n\n[1] Hazan, E. and Kale, S. 2012. Projection-free online learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning (ICML'12).\n\n[2] Orabona, F. (2019). A Modern Introduction to online learning. ArXiv, abs/1912.13213.\n\n[3] Weibel, J., Gaillard, P., Koolen, W. M., and Taylor, A. Optimized projection-free algorithms for online learning: construction and worst-case analysis. 2025. ⟨hal-05097004⟩"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Sy1FxmKLU5", "forum": "t4WNcclzLE", "replyto": "t4WNcclzLE", "signatures": ["ICLR.cc/2026/Conference/Submission20395/Reviewer_7W2a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20395/Reviewer_7W2a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980221516, "cdate": 1761980221516, "tmdate": 1762933842503, "mdate": 1762933842503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}