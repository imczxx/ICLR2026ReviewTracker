{"id": "ZrrVEMyQeU", "number": 11870, "cdate": 1758204393002, "mdate": 1759897549751, "content": {"title": "WATS: Wavelet-Aware Temperature Scaling for Reliable Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance on relational data; however, their confidence estimates often misalign with actual predictive correctness, posing significant limitations for deployment in safety-critical settings. While existing graph-aware calibration methods seek to mitigate this limitation, they primarily depend on coarse one-hop statistics, such as neighbor-predicted confidence, or latent node embeddings, thereby neglecting the fine-grained structural heterogeneity inherent in graph topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a post-hoc calibration framework for node classification that assigns node-specific temperatures based on tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the scalability and topology sensitivity of graph wavelets to refine confidence estimates, all without necessitating model retraining or access to neighboring logits or predictions. Extensive evaluations across nine benchmark datasets with varying graph structures and two GNN backbones demonstrate that WATS achieves the lowest Expected Calibration Error (ECE) among most of the compared methods, outperforming both classical and graph-specific baselines by up to 41.2\\% in ECE and reducing calibration variance by 10.18\\% on average compared with graph-specific methods. Moreover, WATS remains computationally efficient, scaling well across graphs of diverse sizes and densities. The implementation is available at \\url{https://anonymous.4open.science/status/WATS-057A}", "tldr": "Calibrating Graph Neural Networks with Graph Wavelet", "keywords": ["Graph Neural Network", "Calibration"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa6ad037a1de0c1bf5f4ed916849aa4ab270234b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "WATS proposes a post-hoc, label-free calibration method for GNNs using graph wavelet features. A small MLP predicts a per-node temperature, guided by local spectral characteristics computed via heat-kernel wavelets. This allows finer calibration than global temperature scaling and mitigates overconfidence in deep GNNs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple yet effective node-wise temperature scaling without retraining the base GNN.\n2. The wavelet embedding captures multi-scale structural cues for confidence correction.\n3. Extensive experiments across datasets and backbones with consistent ECE reduction.\n4. Clear complexity and sensitivity analysis, including practical hyperparameter guidance."}, "weaknesses": {"value": "1. Relies on the correlation between structure and calibration error, which may not hold for heterophilous graphs.\n2. The wavelet scale parameter tuning could be expensive for a large graph"}, "questions": {"value": "1. How robust is WATS to heterophily or noisy topology?\n2. Can wavelet-based calibration transfer to edge or graph classification tasks?\n3. Does precomputing wavelets limit adaptability to dynamic graphs?\n4. Could you briefly explain the intuition using wavelets to design a calibration algorithm in GNN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6TVy5ngrLX", "forum": "ZrrVEMyQeU", "replyto": "ZrrVEMyQeU", "signatures": ["ICLR.cc/2026/Conference/Submission11870/Reviewer_Wzpk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11870/Reviewer_Wzpk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914196469, "cdate": 1761914196469, "tmdate": 1762922889128, "mdate": 1762922889128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WATS, a post-hoc calibration framework for GNNs that learns node-wise learnable temperatures from graph wavelet-based structural features. By incorporating hierarchical structural signals, WATS improve miscalibration of GNNs compared to one-hop-based temperature scaling approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The application of graph wavelet transform in calibration domain is interesting.\n- WATS substantially surpasses prior work in both small- and large-scale datasets."}, "weaknesses": {"value": "- The discussion in Section 3.2 is a bit confusing. Even if model confidence exhibit high uncertainty in cases of disagreeing neighbors, a single node-level bias itself may not be problematic, since calibration error is inherently a population-level quantity, not measurable at a single-node level in practice. Could the authors provide additional explanation on this?\n- According to Figure 1, deeper GCNs become more confident but less correct, which seems that increasing the receptive field rather worsens calibration. Furthermore, such phenomenon is likely attributable to over-smoothing rather than multi-scale connectivity or structural differences. This interpretation appears sometwhat misaligned with the design philosophy of WATS, which assumes that calibration errors arise when the calibration network fails to capture higher-order context. Could the authors clarify this?\n- Minor) Found a typo in line 128."}, "questions": {"value": "- Does similar trends of accuracy and confidence according to different numbers of layers in Figure 1 persist under heterophilous graphs?\n- Could the authors show the wall-clock time analysis and memory consumption of the proposed method compared with baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IbCPwpIXKN", "forum": "ZrrVEMyQeU", "replyto": "ZrrVEMyQeU", "signatures": ["ICLR.cc/2026/Conference/Submission11870/Reviewer_jtMi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11870/Reviewer_jtMi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931294039, "cdate": 1761931294039, "tmdate": 1762922888518, "mdate": 1762922888518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of calibrating model outputs for GNN architectures. Recent work shows that GNNs tend to be underconfident, which sets them apart from a lot of the NN literature and calls for bespoke strategies. A few strategies have already been suggested, that essentially try to predict the temperature parameter based on graph features. The central contribution of this paper is to replace this by a graph wavelet transform. The authors then show that their method performs well (where performance is measured using ECE) in several graphs, and outperforms competitors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper's exposition is clear.\n2) The results on experimental data look convincing."}, "weaknesses": {"value": "As a disclaimer, I am not an expert on calibration (let alone on the calibration of GNNs), so here are a couple of points that I was hoping the authors could comment on.\n\n1) Overall, the idea of using the graph structure to calibrate the uncertainty is fairly natural, but I wonder if the authors could try to phrase mathematically what they are trying to achieve.\nFor instance: I could imagine a setting where the GNN doesnt account perfectly for the signal on the graph, and as a consequence, the residuals are correlated.  For instance, let's simplify things by assuming that you're doing regression to predict $y_i = (\\alpha X_i + \\beta (\\sum_{j \\sim i} X_i/ d_i)  + \\epsilon_i$  but that you're not using the graph structure, so $\\hat{y}_i = \\hat{\\alpha} X_i$.\n\nConsequently, using the wavelets that you are using is a much better idea than using predictors (like degree), as the wavelets will encode some deeper structural information (e.g where the errors are relative to one another) that will probably allow to calibrate more accurately the error, without having the overhead of expensive computations like GAT, etc.  \nAnother scenario could be that you've explained the graph structure away using your GNN, but the errors $\\epsilon_i$ are not iid (network effects --- e.g. friends talk to one another and exert mutual influence on each other). In this case, maybe a way of formulating the problem is that each node has a neighborhood effect: \n$y_i = (\\alpha X_i + \\beta (\\sum_{j \\sim i} X_i/ d_i)  +  u_i + \\epsilon_i$ \nwhere $u_i \\sim N(0, \\Sigma)$ where $\\Sigma$ encodes dependencies between nodes. It is probably the case that the architecture you're suggesting is able to find dependencies that allow you to account for this random effect.\nIs any of these close to what you were envisioning for the effect of the wavelets? Can you explain why the wavelet is a better idea?\n\n2) The authors discuss the computational complexity of the method. It would have been insightful to compare the running time of each methods as well, on top of reporting the ECE.\n\n3) It looks like the model doesnt crucially depend on K or s. What if, instead of the spectral wavelets, the authors used the Laplacian embedding of the graph --- that would prevent them from computing huge svd (just keep the top K) --- would we have similar results?"}, "questions": {"value": "(1) The characterization of conformal prediction as an \"in-training approach [that]  uncertainty estimation within the model optimization process\" seems off. CP acts as a wrapper, with no need to train any algorithm, so it doesnt interfer with the optimization process at all. It belongs to the post hoc methods.\n\n(2) Could you explain this sentence: \"GNNs tend to be systematically underconfident: their predicted confidence scores are consistently\nlower than their true accuracy\" --- what does \"predicted confidence scores\" mean, computed how?\n\n(3) Tables 7-9 seem to show that the performance of the method is independent of the temperature parameter $s$ and $k$ -> any insight from the theory perspective?\n\n4) For the ablation study, why not use the features? (Im not necessarily asking for more experiments, just curious to hear why it was not considered)? \n\n\nNotes: Line 68: \".Differs\" --- the full stop there seems to be an error.\nLine 269: \"The hyper-parameter k sets the maximum receptive-field size\" -> k should probably be K?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lCEQzRzJrM", "forum": "ZrrVEMyQeU", "replyto": "ZrrVEMyQeU", "signatures": ["ICLR.cc/2026/Conference/Submission11870/Reviewer_md66"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11870/Reviewer_md66"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947167056, "cdate": 1761947167056, "tmdate": 1762922888116, "mdate": 1762922888116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to address the calibration issue in node classification. Existing methods only consider local topology when performing calibration, which limits their effectiveness. To overcome this issue, the author proposes using graph wavelets, which can capture information from more distant nodes, to determine the temperature for calibration. Experimental results demonstrate that the proposed method outperforms existing approaches on both GCN and GAT models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a method that can consider beyond one-hop neighborhood information\n- Demonstrates the effectiveness of the proposed method across datasets with diverse characteristics\n- The approach is lightweight, as it only requires running a 2-layer MLP on the validation set"}, "weaknesses": {"value": "- The method novelty of the proposed approach appears limited. Graph wavelets are already well-established, and in this work, they are applied to the calibration setting with only minor engineering and no specialized adaptation, which diminishes the originality of the method.\n- The method involves training a MLP on the validation set, where the current split allocates 10% to validation and 20% to training. This is a relatively large validation portion, and in practical scenarios, such a large validation set may not be feasible. Consequently, the performance might degrade as the validation size decreases.\n- Experiments are conducted only on relatively old architectures such as GCN and GAT, with no evaluation on recent GNN architectures, particularly those that include skip connections. Thus, it remains unclear whether the proposed method generalizes well to modern GNNs.\n- Although the paper emphasizes the advantage of capturing beyond one-hop information, the evaluation is limited to 2-layer GCN and GAT, making it uncertain whether the method performs equally well on deeper architectures."}, "questions": {"value": "Please see the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EvFgKI3K8M", "forum": "ZrrVEMyQeU", "replyto": "ZrrVEMyQeU", "signatures": ["ICLR.cc/2026/Conference/Submission11870/Reviewer_Cr2Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11870/Reviewer_Cr2Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983877790, "cdate": 1761983877790, "tmdate": 1762922887780, "mdate": 1762922887780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}