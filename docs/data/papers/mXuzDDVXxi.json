{"id": "mXuzDDVXxi", "number": 13090, "cdate": 1758213485357, "mdate": 1759897466077, "content": {"title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models", "abstract": "Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six  categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated  to assess the visual reasoning capabilities of MLLMs from multiple perspectives.  We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30\\% accuracy—only slightly above the 25\\% random baseline and far below the 51.4\\% achieved by humans—revealing significant gaps in visual reasoning.", "tldr": "", "keywords": ["Multi-modal Large Language Models", "Benchmark", "Visual Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6924aa633154f542444f3d2f37dcdd1760df1c7c.pdf", "supplementary_material": "/attachment/4bc6f0c7832f65fb36e94c75d29bc16d04c46ef0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes VisuLogic, a 1,000-item benchmark purpose-built to evaluate vision-centric reasoning in multimodal large language models (MLLMs) without permitting language-only shortcuts. The dataset spans six carefully annotated categories—Quantitative (35.3%), Spatial (23.1%), Positional (13.6%), Attribute (8.2%), Stylistic (9.0%), and Other (10.8%)—and is curated via an automated pipeline (web fetching, cleaning, structuring) followed by duplicate filtering (text overlap and pHash) and full human verification. \n\nThe authors argue that many prior evaluations (e.g., math-with-diagrams or caption-to-LLM pipelines) conflate vision with language reasoning; they empirically validate this by showing that text-only LLMs fed detailed GPT-4o image captions perform near random (e.g., Qwen2.5-72B 28.0%, DeepSeek-R1 26.6%) on VisuLogic, indicating that genuine visual cues are not recoverable through textual description alone. Comprehensive experiments across 31 models reveal large gaps to humans: the random baseline is 24.9%, humans achieve 51.4% (83.6% with hints), while strong MLLMs—OpenAI-o3 (29.5%), GPT-4o (26.3%), Gemini-2.0-Pro (28.0%), InternVL3-78B (27.7%)—cluster around 25–30%. Chain-of-thought prompting offers negligible gains for visual reasoning, suggesting current CoT training is misaligned with multimodal needs. Hint prompts deliver modest improvements (typically +3–10 points) yet still leave models far from human performance, reflecting brittle and inconsistent reasoning procedures. \n\nFine-grained analyses show distinctive weaknesses: LLMs struggle most on Spatial tasks; MLLMs are particularly poor on Stylistic reasoning (often worse than random); humans maintain comparatively low error on Positional reasoning, underscoring a human–model cognitive gap in interpreting spatial relations and transformations. Scaling within families helps only slightly (e.g., InternVL2.5 38B→78B), and closed-source models do not markedly outperform open-source counterparts. The authors further explore reinforcement learning with RLOO using format and accuracy rewards, observing improvements over supervised fine-tuning (e.g., InternVL2.5-38B-RL to 31.1%), hinting that RL may better shape stepwise visual reasoning. \n\nOverall, the paper contributes a carefully controlled benchmark that suppresses text-only shortcuts, a transparent curation and taxonomy, multi-prompt evaluation protocols (non-CoT, CoT, hints), and diagnostic analyses that map where current MLLMs fail and how hints/RL partially mitigate those failures; code and data will be open-sourced to facilitate reproducibility and future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "VisuLogic contributes something genuinely new by testing models on vision-first reasoning instead of letting them “cheat” via text. It deliberately blocks caption-only shortcuts and evaluates six well-defined reasoning types (quantity, spatial, positional, attribute, stylistic, other). This taxonomy is broader and more structured than many prior datasets. The dataset is built with a transparent pipeline (crawl → clean → structure → pHash dedup → human checks), answers are evenly balanced across choices, and the evaluation covers 31 models under consistent settings (No CoT, CoT, and hints), with per-category analyses and instructive hint studies. The paper shares prompts, hyperparameters, and even RL baselines (using RLOO), which makes the work reproducible and goes beyond just releasing a benchmark. Writing and figures are clear. Most importantly, results expose a big gap: top models are near chance (about 25–30%), while humans reach 51.4%, and 83.6% with hints. This shows VisuLogic is a strong diagnostic target for advancing multimodal reasoning methods (e.g., multimodal CoT, PRM/RFT, RL-style test-time compute)."}, "weaknesses": {"value": "Some tasks may still be solvable by pattern matching rather than true multi-step reasoning. The paper doesn’t formally map each item to “atomic” visual operations (like rotate by k degrees, reflect, count, fold adjacency), nor does it run ablations that remove superficial cues to force deeper reasoning. Data provenance is under-detailed: there’s no near-duplicate audit against common pretraining datasets or a breakdown of sources, which weakens claims that tasks are “new” to models. The human study uses a relatively uniform STEM cohort and reports limited stats; hints derived from solutions might subtly bias phrasing. Methodologically, the caption→LLM control depends on GPT‑4o’s caption quality, but there’s no human-captioned upper bound; CoT conclusions come from text‑centric prompts without multimodal/process variants; and the RL setup mainly rewards final answers/format, with little process supervision or overfitting checks. Finally, statistical reporting is light (few confidence intervals or significance tests), and there’s no item difficulty/discrimination analysis—even though some categories are near chance—so reliability and calibration remain open questions."}, "questions": {"value": "1. Contamination: Can you run a near‑duplicate search against common pretraining data and provide detailed source distributions and licensing to assess training leakage and bias?\n2. Hints: Will you ablate hint specificity/strength and add category‑specific templates (e.g., face adjacency lists for folding) to separate perception vs. reasoning deficits?\n3. Human baselines: Can you add confidence intervals and per‑category variance, recruit a more diverse participant pool, and report item ambiguity checks (e.g., inter‑annotator agreement)?\n4. Reliability: Will you provide item difficulty/discrimination (e.g., IRT), prune ambiguous/low‑discrimination items?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DEnR48Q07A", "forum": "mXuzDDVXxi", "replyto": "mXuzDDVXxi", "signatures": ["ICLR.cc/2026/Conference/Submission13090/Reviewer_7QFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13090/Reviewer_7QFv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761520042092, "cdate": 1761520042092, "tmdate": 1762923815300, "mdate": 1762923815300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisuLogic, a vision-language reasoning benchmark designed to minimize text-based shortcuts and emphasize genuine visual reasoning. The benchmark includes 1,000 human-verified problems across six categories. The authors evaluate a range of open and proprietary multimodal large language models (MLLMs), revealing significant limitations in their ability to perform vision-centric reasoning. While humans achieve around 51% accuracy, most models remain below 30%, underscoring a clear gap between current MLLMs and human reasoning ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and timely problem - evaluating whether MLLMs can solve genuinely vision-based reasoning tasks where textual descriptions alone are insufficient. This topic is clearly relevant to the ICLR community.\n2. The VisuLogic benchmark comprises 1,000 human-verified samples across six task categories, providing a well-structured and challenging evaluation setting. The tasks are demanding not only for MLLMs but also for humans, yet the inclusion of hints demonstrates that they remain solvable and meaningful.\n3. The inclusion of problem hints is particularly valuable. They illustrate both the intrinsic difficulty of the tasks (as seen from improved human performance with hints) and offer a complementary lens for analyzing model reasoning behavior\n4. The evaluation is thorough, encompassing a diverse set of both open-source and proprietary models.\n5. The paper is clearly written, well-organized, and easy to follow."}, "weaknesses": {"value": "1. (Section 3.1, Data Collection) The benchmark is collected from various internet sources via a scraping pipeline. While the authors state that they verified licenses and regulations, the paper should include a list of data sources for each problem. This is important to (1) properly credit original authors, (2) verify licensing and usage compliance, and (3) allow others to check whether the benchmark overlaps with data used to train evaluated MLLMs. Without this information, reproducibility and ethical transparency are limited.\n2. (Section 3.1, Quality Control) The authors mention conducting a “thorough human-led review,” but methodological details are insufficient. Appendix C.1 indicates that five annotators spent two hours each reviewing data - an effort that appears limited given the dataset’s size and difficulty. It remains unclear how many annotators reviewed each problem, how disagreements were resolved, and what the inter-annotator agreement was during the “Manual Inspection” phase.\n3. The benchmark aims to address the lack of vision-focused reasoning evaluations for MLLMs. However, this issue has been recognized in prior research. The importance of emphasizing the visual component was highlighted as early as [Goyal, 2017]. Several vision-centric benchmarks already exist, including Raven’s Progressive Matrices [Barrett, 2018; Zhang, 2019], Bongard Problems [Małkiński, 2025; Wüst, 2025], and ARC [Chollet, 2019; Moskvichev, 2023]. Spatial reasoning was studied in SPACE [Ramakrishnan, 2025]. MMMU-Pro [Yue, 2024] explicitly eliminates text-only solutions to MMMU. As a result, the novelty of VisuLogic is limited.\n4. Consequently, the paper shows that current MLLMs struggle with visual reasoning, but this limitation has already been observed in earlier work (including several cited by the authors). Consequently, the paper’s impact is somewhat limited by reiterating well-known weaknesses rather than uncovering a new research direction.\n\nBarrett, David, et al. \"Measuring abstract reasoning in neural networks.\" International conference on machine learning. 2018.\n\nChollet, François. \"On the measure of intelligence.\" arXiv:1911.01547. 2019.\n\nGoyal, Yash, et al. \"Making the V in VQA matter: Elevating the role of image understanding in visual question answering.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\nMałkiński, Mikołaj, et al. \"Reasoning limitations of multimodal large language models. A case study of bongard problems.\" International conference on machine learning. 2025.\n\nMoskvichev, Arseny, et al. \"The ConceptARC benchmark: Evaluating understanding and generalization in the ARC domain.\" Transactions on machine learning research. 2023.\n\nRamakrishnan, Santhosh Kumar, et al. \"Does spatial cognition emerge in frontier models?\" International conference on learning representations. 2025.\n\nWüst, Antonia, et al. \"Bongard in Wonderland: Visual puzzles that still make AI go mad?\" International conference on machine learning. 2025.\n\nYue, Xiang, et al. \"MMMU-Pro: A more robust multi-discipline multimodal understanding benchmark.\" Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. 2025.\n\nZhang, Chi, et al. \"RAVEN: A dataset for relational and analogical visual reasoning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019."}, "questions": {"value": "1. The reported 51.4% average human performance appears unexpectedly low. Could the authors clarify the reasons behind this result? Even with hints, average human accuracy rises to 83.6%, but substantial gaps remain in certain categories, such as Spatiality (68.5% with hints). Could the authors provide a problem difficulty analysis, particularly for the Spatiality category? Are there problems that no human solved, or conversely, did any annotator solve nearly all assigned problems?\n2. Since the dataset was scraped from the internet, how do the authors ensure that these samples are not already included in the training data of existing MLLMs?\n3. The paper does not describe how the Reference Solutions were obtained. Were these available in the scraped material, or were they manually created by the authors?\n4. The hints were generated by GPT-4o based on the Reference Solutions. Were these hints verified by human annotators to ensure they do not reveal the correct answer directly?\n5. The paper identifies gaps in MLLMs’ visual reasoning. Could the authors elaborate on how these insights might guide future model design or training strategies?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "(Section 3.1, Data Collection) The benchmark is collected from various internet sources via a scraping pipeline. While the authors state that they verified licenses and regulations, the paper should include a list of data sources for each problem. This is important to (1) properly credit original authors, (2) verify licensing and usage compliance, and (3) allow others to check whether the benchmark overlaps with data used to train evaluated MLLMs. Without this information, reproducibility and ethical transparency are limited.\n\nThe paper reports human performance on the introduced benchmark, but doesn't mention IRB approval."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jHlMfHty8b", "forum": "mXuzDDVXxi", "replyto": "mXuzDDVXxi", "signatures": ["ICLR.cc/2026/Conference/Submission13090/Reviewer_igUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13090/Reviewer_igUC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775747901, "cdate": 1761775747901, "tmdate": 1762923815048, "mdate": 1762923815048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a novel benchmark, VisuLogic, to examine how and where MLLMs fail at abstract visual reasoning (AVR). VisuLogic includes a 1000-instance human-verified challenges in terms of multiple-choice RAVEN-style abstract pattern recognition questions. Each question is also equipped with a non-revealing hint that provides additional solving cues. VisuLogic’s leaderboard shows that most of the state-of-the-art MLLMs do not surpass 30%, only slightly over the random chance of 25%, while the human baseline is at around 51%. This large gap indicates that VisuLogic possesses a genuine challenge to existing MLLMs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "I applaud the authors for presenting a highly comprehensive work. I generally could not find anything to nitpick about when it comes to the design of the benchmark and the documentation of the experiments. Moreover, I greatly appreciate the authors’ extra effort on shortcut prevention when it comes to curating the Hints in VisuLogic, showing they pay extra attention on benchmark data integrity and ensure that the hints and cues do not give away the solutions from the get-go, according to Appendix C. Such a level of meticulousness is a rare sight."}, "weaknesses": {"value": "I only have one thing to worry about regarding the data quality of VisuLogic. Judging from Table 8, it seems to me that even with hints(solution cues) given, several categories (esp. Spatiality and Style) still pose great difficulty to human annotators. Could this indicate that **a decent portion of VisuLogic’s current instances, under such ‘difficult’ categories, are indeed unsolvable to start with**? If so, we would risk seeing inflated benchmarking performances and obtaining non-robust models in return."}, "questions": {"value": "Please find my main concern in the Weaknesses section. \n\nAs a minor suggestion, I believe it’d be more helpful to at least add a reference link to Table 8 within the caption of Table 1 - readers would love to see how the baseline models compare ‘with Hint vs. without Hint’ across all the individual categories in detail, rather than just seeing the ‘Hint’ averages under a single aggregated column at Table 1 in its current state."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Z9NXPionkC", "forum": "mXuzDDVXxi", "replyto": "mXuzDDVXxi", "signatures": ["ICLR.cc/2026/Conference/Submission13090/Reviewer_Fnrw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13090/Reviewer_Fnrw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861728651, "cdate": 1761861728651, "tmdate": 1762923814728, "mdate": 1762923814728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VisuLogic, a benchmark for evaluating visual reasoning in MLLMs without relying on textual shortcuts. It contains 1000 human-verfied logic puzzles across six categories [ quantative, spatial, positional, attribute, sytlistic and other]. The examples in this benchmark require extensive visual centric pattern reasoning. They also did extensive evaluations on multiple models on this benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a gap in visual evaluation on MLLM, so that the model must look at images instead of language reasoning for solving some task.\n- Clear data collection, data cleaning pipeline.\n- The paper is also easy to read, though as a human, the questions are more of IQ test questions, hard and require a lot of thinking to solve.\n- Evaluates 31 models, including open and closed MLLM. Extensive evaluation."}, "weaknesses": {"value": "- The paper says the models fail at those task; however, it's unclear whether the low performance comes from visual encoder limitation [ trying different vision foundation models in the same architecture], alignment of MLLM, or prompting limitations.\n- Models may help if in-context prompting is given. Ablation Study of such is needed. \n- A more broader motivation of where these puzzles will be needed in real life to solve would help."}, "questions": {"value": "- Humans score 51.4% suggesting it's a very hard benchmark even for humans. It would be helpful if authors could report per per-category score performance of humans. Is it the same as MLLMs?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The datasets were crawled from the web and may require licensing."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DHSNccbNI1", "forum": "mXuzDDVXxi", "replyto": "mXuzDDVXxi", "signatures": ["ICLR.cc/2026/Conference/Submission13090/Reviewer_Npe8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13090/Reviewer_Npe8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975298282, "cdate": 1761975298282, "tmdate": 1762923814440, "mdate": 1762923814440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}