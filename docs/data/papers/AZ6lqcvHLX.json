{"id": "AZ6lqcvHLX", "number": 30, "cdate": 1756728165216, "mdate": 1759898278246, "content": {"title": "Half-order Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer", "abstract": "The probabilistic diffusion model (DM), generating content by inferencing through a recursive chain structure, has emerged as a powerful framework for visual generation. After pre-training on enormous data, the model needs to be properly aligned to meet requirements for downstream applications. How to efficiently align the foundation DM is a crucial task. Contemporary methods are either based on Reinforcement Learning (RL) or truncated Backpropagation (BP). However, RL and truncated BP suffer from low sample efficiency and biased gradient estimation, respectively, resulting in limited improvement or, even worse, complete training failure. To overcome the challenges, we propose the Recursive Likelihood Ratio (RLR) optimizer, a Half-Order (HO) fine-tuning paradigm for DM. The HO gradient estimator enables the computation graph rearrangement within the recursive diffusive chain, making the RLR's gradient estimator **an unbiased one with lower variance** than other methods. We theoretically investigate the bias, variance, and convergence of our method. Extensive experiments are conducted on image and video generation to validate the superiority of the RLR. Furthermore, we propose a novel prompt technique that is natural for the RLR to achieve a synergistic effect.", "tldr": "", "keywords": ["perturbation-based gradient estimation", "diffusion model", "post-training"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c4cb7e5e1ed617120bf74e26bf181ee341f737f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an innovative \"half-order\" fine-tuning paradigm, making a substantial contribution to diffusion model optimization. By intelligently combining three gradient estimation strategies, it achieves a high level of theoretical and practical application. Rigorous mathematical proofs and extensive experimental validation demonstrate the RLR method's significant advantages over traditional approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The innovative concept of \"half-order\" fine-tuning paradigm is proposed, which fills the gap between traditional first-order and zero-order methods.\n2. FO, HO and ZO complement each other's strengths and find the optimal balance between variance and computational cost by optimizing the h and j parameters, taking into account the actual computational budget constraints."}, "weaknesses": {"value": "1. The problem with FO is its high cost, and the problem with ZO is its high variance, but the author does not provide a clear analysis to explain this. For example, for a specific scenario, how many NFEs are needed for FO, ZO, and HO respectively, how is this calculated, what are the variances of these three, and why is there such a large variance problem. I think this needs a clearer analysis.\n2. The visualization results look average, and the improvement is not significant enough. Lacks comparison with recent works such as FlowGRPO and ReFL."}, "questions": {"value": "I wonder when I use ZO, when the noise is large, do I still need to go through the entire denoising trajectory to get the reward? If this is the case, then ZO also seems to have a high cost problem, which might be solved by process reward model such as SPO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EeMI5marpQ", "forum": "AZ6lqcvHLX", "replyto": "AZ6lqcvHLX", "signatures": ["ICLR.cc/2026/Conference/Submission30/Reviewer_4kwL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission30/Reviewer_4kwL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission30/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709935210, "cdate": 1761709935210, "tmdate": 1762915439666, "mdate": 1762915439666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the crucial task of efficiently aligning diffusion models (DMs) to meet downstream application requirements after pre-training.   Contemporary fine-tuning methods, such as Reinforcement Learning (RL) and truncated Backpropagation (Truncated BP), suffer from high variance and biased gradient estimation, respectively.  The authors propose the Recursive Likelihood Ratio (RLR) optimizer, a novel \"Half-Order\" (HO) fine-tuning paradigm. RLR constructs a new gradient estimator by cleverly combining First-Order (FO), Half-Order (HO), and Zeroth-Order (ZO) estimators within the recursive chain. The paper theoretically proves that the RLR estimator is unbiased (overcoming the defect of truncated BP) and has a lower variance than RL/ZO methods. Extensive experiments on text-to-image and text-to-video tasks validate the superiority of RLR. It not only outperforms baseline methods (like DDPO and Alignprop) on multiple reward benchmarks, but critically, it also avoids the \"model collapse\" problem caused by truncated BP9. Furthermore, the paper proposes a novel prompt technique called \"Diffusive Chain-of-Thought\" (DCoT), which synergizes naturally with RLR's HO estimator, allowing the model to optimize for specific generation scales (e.g., \"fine-grained\" details)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses the crucial task of efficiently aligning foundation diffusion models. This represents a highly important and practically valuable problem. \n- This paper proposes the Recursive Likelihood Ratio (RLR) optimizer, a novel \"Half-Order\" (HO) fine-tuning paradigm that successfully overcomes these challenges. T\n- he paper also introduces a novel prompt technique that synergizes naturally with the RLR optimizer, further enhancing the originality of the contribution. \n- The paper rigorously demonstrates its method's advantages in terms of bias, variance, and convergence from both theoretical and experimental standpoints, while also proving its practical effectiveness."}, "weaknesses": {"value": "- The paper exhibits significant inconsistencies in its core methodology description, particularly regarding the sampling strategy for the Half-Order (HO) sub-chain starting point, $j$. In Section 4.2 (Methodology), the paper describes $j$ as being sampled from a categorical distribution based on gradient norms. However, in Section 5.3 (DCoT Experiment), $j$ is described as being selected from a uniform distribution ($j \\sim \\mathcal{U}(1, T-h)$). This contradictory description makes it impossible to determine which sampling strategy the standard implementation of RLR is supposed to use.\n- Furthermore, the implementation of DCoT (Diffusive Chain-of-Thought) introduces a critical external dependency. As shown in Section 5.3 and Appendix F, DCoT relies on an external Large Language Model (LLM) to generate the 'coarse-mid-fine' grained prompts. This raises doubts about its robustness in practical deployment."}, "questions": {"value": "- As in weakness, why different descriptions occur, and how to choose the sampling strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZsZmRR1b1A", "forum": "AZ6lqcvHLX", "replyto": "AZ6lqcvHLX", "signatures": ["ICLR.cc/2026/Conference/Submission30/Reviewer_KciP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission30/Reviewer_KciP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission30/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988698337, "cdate": 1761988698337, "tmdate": 1762915439359, "mdate": 1762915439359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Half-order Fine-tuning method for efficiently adapting large-scale diffusion models (e.g., Stable Diffusion) to downstream datasets.  The authors also present a theoretical proof that RLR is unbiased, has lower variance, and enjoys convergence guarantees. The experiments demonstrate RLR’s efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel fine-tuning scheme and devises gradient estimators for the diffusion model’s chain-of-thought, which appears genuinely innovative.\n- The theoretical analysis is careful and offers a credible justification for the proposed approach."}, "weaknesses": {"value": "- Missing a comparison with related diffusion model fine-tuning baselines, such as D3PO[1].\n- The experiments are limited to SD 1.4 and SD 2.0, which are now dated. Moreover, the method’s generalization to the Flux architecture remains unclear.\n\n$\\text{[1] Yang, Kai, et al. \"Using human feedback to fine-tune diffusion models without any reward model.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.}$"}, "questions": {"value": "NA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M7caMEE5FJ", "forum": "AZ6lqcvHLX", "replyto": "AZ6lqcvHLX", "signatures": ["ICLR.cc/2026/Conference/Submission30/Reviewer_joeE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission30/Reviewer_joeE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission30/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997266573, "cdate": 1761997266573, "tmdate": 1762915439038, "mdate": 1762915439038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}