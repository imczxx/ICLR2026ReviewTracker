{"id": "yIoMqDes7O", "number": 2875, "cdate": 1757295578850, "mdate": 1759898121600, "content": {"title": "When Agents “Misremember” Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems", "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose ManBench, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on ManBench to quantify the Mandela effect, and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.", "tldr": "We explore the Mandela Effect (collective cognitive biases) in large language models, examining its existence and causes with a new benchmark, ManBench, and propose methods to mitigate the effect.", "keywords": ["LLM for Social Science", "Mandela Effect", "Multi-agent System", "Cognitive Bias"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0bd2b51d8efa076e610677b5475641c5da624c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "An interesting paper studies a collective cognitive bias in multi-agent LLM systems akin to the human “Mandela effect,” where group interaction and specious evidence induce shared false memories that can persist over time. The authors also introduce MANBENCH to measure this phenomenon."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces MANBENCH, the first systematic benchmark to evaluate the Mandela effect in LLM-based multi-agent systems, covering tasks, interaction protocols, and metrics.\n2. Quantifies how the effect manifests and operates across model families, group compositions, group sizes, memory timescales, and knowledge domains.\n3. Proposes a two-layer defenses—prompt engineering and alignment via SFT—with dataset design and training details, and empirically validates their effectiveness."}, "weaknesses": {"value": "1. While the paper proposes two types of mitigation strategies (prompt-level and model-level defenses), these approaches remain relatively preliminary. For instance, the prompt-level methods rely heavily on predefined rules (e.g., cognitive anchoring and source scrutiny), which may not generalize well to unseen cases or adversarial scenarios. \n\n2. The paper would benefit from adding a discussion section to explore the practical risks of the Mandela Effect in real-world tasks, such as sensitive decision-making scenarios."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wNohgElkYy", "forum": "yIoMqDes7O", "replyto": "yIoMqDes7O", "signatures": ["ICLR.cc/2026/Conference/Submission2875/Reviewer_H3cb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2875/Reviewer_H3cb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552731794, "cdate": 1761552731794, "tmdate": 1762916424705, "mdate": 1762916424705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the \"misremember\" effects that could potentially happen among multi-agent systems. It involves repurposing an existing QA benchmark and investigating several settings where agents could be wrong. \nEvaluation with several current SOTA LLMs shows that all suffer from such effects.\nFurthermore, this work investigates prompting and sft methods to defend models against such effects and shows improvement."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I agree that multi-agent systems are increasingly involved in complex workflows, and investigating the failure case of a multi-agent system is an important issue. This paper focuses on a specific phenomenon, proposes an evaluation, and shows improvement methods. The findings align with expectations, and the ablations show more insight into the failure of mult-agent systems in LLM era."}, "weaknesses": {"value": "Overall, I’ve seen many recent works draw on psychological concepts from human society and apply them to multi-LLM agent systems. However, we should keep in mind that LLMs differ fundamentally from humans; they have near-perfect memory, remain largely homogeneous, and are trained to play sycophantic roles as “user” and “assistant,” making them naturally inclined to agree with users. Specifically, I have several concerns about the work. \n\nFirst, the idea of a specific malicious agent that may render the whole multi-agent system down is not new, for example, https://arxiv.org/abs/2408.00989 (btw, this work is not cited in the paper). Although the specific topics might slightly differ from each other, I find it hard to tell how the evaluation framework is fundamentally different from previous work. \n\nSecond, the contribution of the dataset seems to be simply repurposing an existing dataset, and I would argue that such context might not be the most practical scenario where the ground truth is aware to each party. That being said, the proposed benchmark is probably not a challenging benchmark, as each model would actually have the correct answer, and the most straightforward approach is to simply prompt every agent not to listen to each other. This hypothesis seems to be verified by the authors' later effort in climbing on the benchmark. The real challenge, however, is for scenarios where agents have to collaborate with each other to get information or knowledge. Can authors think of more scenarios in this direction?\n\nThird, the experiments are only done with a single LLM for each simulation. Do authors have plans to run multiple LLMs in one scenario? Especially, this setting would be more aligned with the human society Mandela effect, where each party has different knowledge."}, "questions": {"value": "see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "Esff3aaVAf", "forum": "yIoMqDes7O", "replyto": "yIoMqDes7O", "signatures": ["ICLR.cc/2026/Conference/Submission2875/Reviewer_LqGM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2875/Reviewer_LqGM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594044665, "cdate": 1761594044665, "tmdate": 1762916424494, "mdate": 1762916424494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the Mandela effect (collective false memory) in LLM-based multi-agent systems, a critical yet underexplored issue in collaborative AI. The authors introduce MANBENCH, a novel benchmark with 4,838 questions across 4 task domains (e.g., History, Domain-Specific Knowledge) and 5 interaction protocols (e.g., Role-based Short-term/Long-term) to evaluate the phenomenon. They test 13 LLMs (7 commercial, 6 open-source), confirming all models are susceptible to the Mandela effect—for instance, Qwen3-235B’s error rate rises from 25.48% (baseline) to 74.75% under the Role-based Short-term Protocol."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The idea proposed in this paper is interesting and insightful. \n+ This study is the first to systematically explore collective false memory (Mandela effect) in multi-agent systems, addressing a critical gap between individual LLM hallucination and group-level cognitive biases.\n+ The paper provides comprehensive experiments and analysis, including diverse interaction protocols (simulating short/long-term memory and generic/role-based groups), and tailored metrics (reality shift rate, σ_max) etc."}, "weaknesses": {"value": "- Although MANBENCH tasks are adapted from BIG-Bench Hard, they may not fully reflect the complexity of real-world multi-agent interactions (e.g., dynamic role changes, unstructured dialogue), potentially limiting the ecological validity of results.\n\n- While defenses reduce the Mandela effect on MANBENCH, the paper provides little evidence of their performance across unseen tasks or domains (e.g., highly specialized fields like healthcare), leaving uncertainty about their broader applicability.\n\n-  The interaction protocols predefine agent roles (e.g., Error Conclusion Initiator) and consensus direction, but real multi-agent systems often involve uncoordinated, conflicting inputs—this simplification may underestimate or distort how the Mandela effect emerges naturally."}, "questions": {"value": "1. For the model-level defense, how does the balance between the resilience set and cooperative set (e.g., ratio adjustments) impact performance? Could a dynamic ratio (tailored to task domains) further improve both error resistance and knowledge absorption?\n2. The paper notes that some models (e.g., GPT-5) self-correct false memories long-term, while others (e.g., Claude 3.5 Haiku) do not. What underlying LLM characteristics (e.g., context window size, training data) drive this difference in memory integrity?\n3. MANBENCH focuses on verifiable factual tasks—would the Mandela effect manifest differently in subjective or creative tasks (e.g., collaborative content generation), and how might the proposed defenses adapt to such scenarios?\n4. For role-based groups, the \"suspicion-induced vigilance\" effect reduces the Mandela effect when group size exceeds 9. Does this threshold vary across LLM types (e.g., open-source vs. commercial) or knowledge domains, and can this effect be proactively leveraged in defense design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s4I1JH4cIy", "forum": "yIoMqDes7O", "replyto": "yIoMqDes7O", "signatures": ["ICLR.cc/2026/Conference/Submission2875/Reviewer_ABwn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2875/Reviewer_ABwn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639446890, "cdate": 1761639446890, "tmdate": 1762916424327, "mdate": 1762916424327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MANBENCH, a novel benchmark to measure and diagnose the Mandela effect—the formation of shared false memories—in systems of collaborating LLM agents. MANBENCH comprises 4 838 multiple-choice questions drawn from BIG-Bench Hard, organized into four knowledge domains, and five interaction protocols varying in group composition (Generic vs. Role-based) and memory timescale (Short-term vs. Long-term). The authors evaluate 13 state-of-the-art LLMs, quantify a large reality-shift effect across all models, analyze key drivers (group size, domain, model scale, etc.), and propose both prompt-level (cognitive anchoring, source scrutiny) and model-level (supervised fine-tuning with resilience/cooperative data) defenses that reduce the effect by up to 74.4%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem framing is novel. This paper is the first systematic study of collective false memories in LLM-based multi-agent systems, extending beyond individual hallucinations to social contagion effects.\n\n2. This paper conducts a comprehensive evaluation across 13 models (commercial + open-source) and five protocols, with well-defined metrics (error rate, reality shift rate, maximal shift rate).\n\n3. This paper provides not only an evaluation benchmark but also mitigation methods: two prompt-based strategies and an SFT-based model intervention."}, "weaknesses": {"value": "1. The “specious evidence” dialogues are synthetic. It remains unclear how these engineered narratives map onto real-world multi-agent deployments or user-driven misinformation.\n\n2. The benchmark is limited to multiple-choice questions. The transfer of findings to free-form, open-ended tasks (e.g., long-form debate or planning) is not evaluated."}, "questions": {"value": "1. It is better to include significance tests (e.g., paired bootstrap) to support claims of “significant reduction.”\n\n2. It will be interesting to involve humans in the loop in multi-agent collaboration and to see if the findings still hold. For example, what is the human performance on MANBENCH.\n\n3. Is there any cross-task contamination or transfer of false memories when agents move between distinct domains within the same session?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2x885dhXK", "forum": "yIoMqDes7O", "replyto": "yIoMqDes7O", "signatures": ["ICLR.cc/2026/Conference/Submission2875/Reviewer_BRew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2875/Reviewer_BRew"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993601784, "cdate": 1761993601784, "tmdate": 1762916424141, "mdate": 1762916424141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}