{"id": "29ETLxTQAN", "number": 21942, "cdate": 1758323870328, "mdate": 1763620935871, "content": {"title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?", "abstract": "Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. The main underlying idea is to reduces the best possible accuracy, i.e., Bayes accuracy, by injecting randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.", "tldr": "", "keywords": ["benchmark", "LLM", "Bayes error", "Bayes accuracy", "memorization"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5afef0d849edc5f30f6412a7c7f55b39e811a68.pdf", "supplementary_material": "/attachment/60e0f16b11a9f750552196c12e46730e9b4d110d.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a method called CapBencher, Which is introduced as a way to release LLM benchmarks while hiding the true answers. This is quite an important problem in the world of LLMs as shown by other works which show that there is a significant amount of contamination on test set in LLM benchmarks, leading to Benchmark Inflation. \nFor each question, the authors define a set of logically correct, realized answers and sample one at the published time. This lowers the base accuracy of the release benchmark. The model which has memorized the release data will exceed the ceiling and thus, serve as a contamination signal. They give a binomial test for the detection and have proposed theory showing the length between the capped and original scores, an unbiased estimator for original accuracy, and an auditor to catch any partial contamination."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Their theorem elegantly shows the relationship between CAP and original scores, provides unbiased estimator for accuracy, and the binomial test is principled for contamination detection. They discuss their experimental approach across 11-plus benchmarks across multiple model families, and the figures are well made. \n\nTheir design works with black box models, which is a very important practical consideration and thus requires no special infrastructure and can be applied immediately to existing benchmarks, which is a scalable approach.\n\nThe private eval simulation and model merging method is quite useful for showing how overfitting can be caught."}, "weaknesses": {"value": "There are some assumptions about independence and uniform shift which might be violated in real pipelines. For instance, biased shift choices or model dependent following of shift instructions that might not be extremely reliable in real world settings.\n\nThere is a precision trade-off because the capped estimator increases the variance and closed models that may blur small deltas might modulate that. The paper does quantify this but there is very less guidance on choosing the K and L for typical data scales. If the authors can provide that, it would be quite useful.\n\nThe randomized rules publication plus a realized answer can enable some recovery of originals. The results are missed and well below recovery completely on hard sets. The attack surface and defenses deserve a tighter threat model."}, "questions": {"value": "1. How do the authors ensure the practical independence holds and that shift options are effectively uniform in real templates, especially when prompts leak patterns that models may learn to exploit. If the authors can show empirical uniformity and no model shift correlation, that would be useful.\n\n2. Can authors please provide concrete guidance to pick the cap level that can balance contamination, detectability, and variance. A table that could map the values would be useful.\n\n3. More and more tasks are turning into long-form reasoning tasks as well as code synthesis. So for such tasks, say code synthesis without unit tests, what capping schemes retain semantics?\n\n4. The multilingual results show that detection is easier in European languages than in Asian languages on certain tasks. Can you please quantify the sensitivity by language family and also recommend language JVS capping to improve detection."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bCRvxeYYMy", "forum": "29ETLxTQAN", "replyto": "29ETLxTQAN", "signatures": ["ICLR.cc/2026/Conference/Submission21942/Reviewer_dSGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21942/Reviewer_dSGm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934610435, "cdate": 1761934610435, "tmdate": 1762941989325, "mdate": 1762941989325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical problem of benchmark data contamination in an era of large-scale, web-crawled training data. Current mitigations, such as private held-out test sets, are costly, rely on trusted third parties, and are still vulnerable to test-set overfitting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The primary strength is the originality of the core idea. Instead of treating contamination as a post-hoc detection problem, CapBencher integrates the detection mechanism into the benchmark's design. Using the Bayes accuracy as a statistical \"ceiling\"  is an elegant and principled solution.\n\n2. The paper does not just rely on empirical results. Theorem 1, which proves the affine relationship between capped and original scores, provides a strong theoretical foundation. This guarantee—that model ranking is preserved—is crucial for the method's adoption as an evaluation tool, not just a detection tool."}, "weaknesses": {"value": "My main concerns are that the method, while clever, may be brittle against a non-naive adversary and that its central premise of \"concealment\" is overstated.\n\n1. The paper's detection mechanism hinges on the assumption that a contaminated model will \"try to memorize the realized values\"  (e.g., memorize \"19\" for the \"3x6\" question). This seems like a naive threat model. A more plausible attack, especially if this method were adopted widely, would be for the model to learn the randomization rule itself. A model trained on thousands of CapBencher examples would not learn \"Q: 3x6, A: 19\". It would learn \"Q: 3x6, A: 18\" and \"Rule: When prompt says 'add/subtract 1', apply this rule\". In such a scenario, the contaminated model would (a) know the true answer is 18, but (b) at test time, correctly follow the randomization rule and output \"19\" or \"17\" randomly. It would therefore score ~50%, passing the contamination test while still being fully contaminated. The experiments do not seem to test against this \"rule-memorization\" attack.\n\n2. The paper's title and main \"Concealment\" strategy claim to publish without giving the true answers away. This is not strictly true. The \"add 1 or subtract 1\" rule  explicitly leaks that the true answer is one of two possibilities. For MCQs (Figure 9), the rule \"randomly choose one of the two options that is immediately adjacent\"  is even more informative, reducing the true answer to one of two choices. This is not true concealment; it is a simple, reversible obfuscation. While it prevents exact string matching in a training corpus, it still provides a strong signal for any model capable of simple reasoning, which seems to be the focus of the reverse-engineering (RE) experiments in Sec 3.\n\n3. The paper correctly criticizes private benchmarks for requiring \"trust in a single organization\". However, the proposed extension for partial contamination (Section 2.3) re-introduces this exact flaw. This \"auditor\" mechanism requires models to submit their original (pre-shift) answers $f(X_i)$ to an auditor who holds the true, private labels $Y_i$. This is, by definition, a private, trusted evaluation server. This extension seems to contradict and undermine the paper's primary motivation of creating a fully open, public benchmark.\n\n4. The \"Concealment\" strategy seems to work well for questions with simple, enumerable answers (e.g., numbers, MCQ labels). It is highly unclear how this strategy could be generalized to complex, open-ended tasks like evaluating the correctness of a mathematical proof, the coherence of a generated story, or the factual accuracy of a summary, where a simple operation like \"add 1\" has no meaning. The \"Disclosure\" strategy  (e.g., adding \"apple\") is more general but is just a slightly modified canary string, which is a much weaker contribution."}, "questions": {"value": "1. Could the authors comment on the threat model of a non-naive attacker? Specifically, what prevents a contaminated model from learning the randomization rule from the prompt (e.g., \"randomly add 1 or subtract 1\" ) rather than the realized answer (e.g., \"19\")? Such a model could defeat the detection mechanism by scoring at the Bayes cap, while still having benefited from knowing the true answers.\n\n2. The \"auditor\" extension in Section 2.3 requires a trusted third party with access to the true labels $Y_i$ to check the model's original answers $f(X_i)$. How is this fundamentally different from the private leaderboard model the paper initially criticizes? Doesn't this negate the primary benefit of a fully public benchmark?\n\n3. How do the authors envision the \"Concealment\" strategy  being applied to more complex, open-ended tasks? For example, in a benchmark for a creative writing or complex reasoning task, what would a meaningful, reversible randomization rule that \"caps\" the Bayes accuracy look like?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5cHjc7GtPU", "forum": "29ETLxTQAN", "replyto": "29ETLxTQAN", "signatures": ["ICLR.cc/2026/Conference/Submission21942/Reviewer_4Ekr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21942/Reviewer_4Ekr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956115116, "cdate": 1761956115116, "tmdate": 1762941989054, "mdate": 1762941989054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a way to modify and publish benchmarks that can aid in blackbox contamination detection while still evaluation a model's task performance. The method consists in modifying the quesiton-answer pairs to intentionally reduce Bayes accuracy, by (1) concealing the 'true' answer to the quesiton with other correct answers ('realized answers') that can only be responded with by the model if it has been trained on and contaminated by the sample; (2) pushing the model to implicitly disclose it was trained on the sample by requesting it to mention an unrelated word in its response. And contaminated models will exceed the 50% Bayes accuracy ceiling. The authors provide comprehensive theoretical guarantees and demonstrate empirically that their method is better at contamination detection than existing methods (canary strings, min-k%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- the authors empirically show that their method (CapBencher) detects contamination more reliably than SoTA detection baselines.\n- a wide range of benchmarks, and models, and baselines.\n- the strategies are simple and can be easily implemented on several benchmarks by string concatenation.\n- the presented method improves over the contamination detection comparators."}, "weaknesses": {"value": "- the practical strategy \"Disclosure allowed\" seems weak (as acknowledged by the authors) and lacks novelty. It is simple prompt injection.\n- both practical strategies are easily circumventable.\n- the paper does not have a Limitations section."}, "questions": {"value": "- How long does it take to generate the updated samples for each of the benchmarks used? And how automateable is this for a new benchmark?\n- Can you add a limitations section to the paper?\n- Can you expand on why you think the paper's contributions are novel?\n\nMinor suggestions:\n-  Typo in the abstract (lines 18-19) “reduces”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4S0irPtqRY", "forum": "29ETLxTQAN", "replyto": "29ETLxTQAN", "signatures": ["ICLR.cc/2026/Conference/Submission21942/Reviewer_j93W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21942/Reviewer_j93W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979097007, "cdate": 1761979097007, "tmdate": 1762941988858, "mdate": 1762941988858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Publicly released benchmarks suffer from contaminating the training data of future models. The authors propose a method to release benchmarks while avoiding this problem: By altering each question to have several logically correct answers. This allows us to catch LLMs who were trained on the benchmark and memorised the answer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The method is straightforward and intuitive. \n* They compare their method to reasonable baselines like canary string and min-k%. I agree with their comparisons with these baseline, on how their method avoids some of the baseline's shortcomings.\n* The paper is fleshed out and well-detailed. I appreciate how the questions asked in the red paragraph headers were answered, and in particular I found myself asking a lot of those questions before finding them already answered."}, "weaknesses": {"value": "* Most notably, This method needs to be done on the benchmark _before_ it is released. This limits its utility as it requires benchmark creators to adopt this method and only release the CapBenched version. Given that pre-release adoption is a necessary condition for this method to work, the following weaknesses are then considered because they may hurt adoption:\n* First, It is not clear whether the true accuracy can be well estimated if the benchmark is capped (See clarification questions labelled W). This is important to clarify because for this method to work you would want to give both benchmark providers and model providers confidence that the performance on the capped benchmark is meaningful.\n* Also, it is more complex than other methods like canary strings, which may hurt adoption. I consider this minor as this can be solved by clearer writing (See feedback)"}, "questions": {"value": "Clarification Questions\n* Take the question in Fig 2 as an example. Under this paradigm, when you release a benchmark, my understanding is that you designate one of the answers (18 or 20) as the \"correct\" answer (i.e. the ground truth label) for the dataset, is that right? \n* Follow-up question: If so, what is stopping a malicious actor from figuring out the ground truth answer, then flipping the ground truth labels randomly before adding it into their training dataset to \"undo\" the capbenching?\n* Do you have any idea why Min-k% did not work on GPQA?\n*  Can I clarify if the most parts of the paper (e.g. reverse engineering) apply to MCQ benchmarks, and that the MCQ benchmarks mentioned use the \"circular choice\" concealment of Figure 9? For example, if in the reverse engineering section, am I right to say that for MCQ questions the base rate of reverse engineering success is 50% since you gave it the original answer and the two possible ones are the adjacent answers?\n* (W1) Are you able to use your unbiased estimator to get the average error on a more comprehensive set of models/benchmarks?\n* (W2) Is Figure 3 the average across all benchmarks for each model?\n* (W3) Can you comment on why the correlation values are worse for some benchmarks, such as GPQA? Benchmark creators may be hesitant to adopt this if they feel that this might misrepresent their benchmark, and I think the GPQA results may come across as that.\n\n* It would be good to give a \"Best practices\" on how potential benchmark creators could easily cap their benchmark, according to the different types of benchmarks (e.g. MCQ, open-ended), recommended Bayes accuracy, and so on. Could you comment on what your recommendations would be and why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VoThUx7pUb", "forum": "29ETLxTQAN", "replyto": "29ETLxTQAN", "signatures": ["ICLR.cc/2026/Conference/Submission21942/Reviewer_MwPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21942/Reviewer_MwPj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994018227, "cdate": 1761994018227, "tmdate": 1762941988609, "mdate": 1762941988609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}