{"id": "YDaxkBBRB5", "number": 11719, "cdate": 1758203299757, "mdate": 1763047779942, "content": {"title": "Robust Image Self-Recovery against Tampering using Watermark Generation with Pixel Shuffling", "abstract": "The rapid growth of Artificial Intelligence-Generated Content (AIGC) raises concerns about the authenticity of digital media. In this context, image self-recovery, reconstructing original content from its manipulated version, offers a practical solution for understanding the attacker’s intent and restoring trustworthy data. However, existing methods often fail to accurately recover tampered regions, falling short of the primary goal of self-recovery. To address this challenge, we propose ReImage, a neural watermarking-based self-recovery framework that embeds a shuffled version of the target image into itself as a watermark. We design a generator that produces watermarks optimized for neural watermarking and introduce an image enhancement module to refine the recovered image. We further analyze and resolve key limitations of shuffled watermarking, enabling its effective use in self-recovery. We demonstrate that ReImage achieves state-of-the-art performance across diverse tampering scenarios, consistently producing high-quality recovered images.", "tldr": "We embed a shuffled version of the image as a watermark and recover tampered regions via image enhancement module. Our method restores fine details better than existing approaches.", "keywords": ["Self-recovery", "Watermarking"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/46fbac6297b9ebeffd5e2fd3857af20b0770fdf4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission aim to embed self-recovery watermarks into images to enable tamper localization and original content reconstruction. This work advances prior work by addressing spatial alignment fragility via fine-grained pixel shuffling, introducing a learned Watermark Generator (WG) to suppress high-frequency artifacts, and adding an Image Enhancement (IE) module."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Results show that the recovery quality of the method is higher. In many tests, the results are much better than the existing methods."}, "weaknesses": {"value": "1. It seems the applied validation dataset is different from the applied baselines. It seems how these baseline models are prepared remain unclear.\n\n2. It seems the applied methodology lacks novelty. Like Invertible networks and mask free generation are also applied in Imuge.\n\n3. It seems the reviewer cannot easily benchmark the advantage of this method, either via theoretical or empirical analysis, or via source code (no code or API provided. though it is completely optional, the reviewer cannot play with the model to address some concerns).\n\n4. It seems the role of accurate attack localization is underestimated (why does the propose mechanisms also boost localization? And why the compared methods fail in the detection stage in many provided samples?) The review can only see the lower results of the compared methods via figures and tables, but barely see the performance of attack localization (except table 8, deeply hidden in the supplement). The reviewer suspects that the focus on localization training of the proposed method more contributes to the final result (especialy considering the reported large performance gap). The reviewer think in this task, authors need to report fair quantitative comparison of recovery using same localization result (either being it a good one or a poor one), in order to benchmark the contribution of modules like WG, IE, etc. In the current version, the two factors are simply interwined.\n\n5. The reviewer thinks the visualization of the proposed method can be rather biased. The authors do show the improvement in the end-to-end result (better recovered images). However, the reviewer barely see any visualization of intermediate results, i.e., prediction mask, or any other signals that could somehow interpret the successes. The authors claim that the compared baseline methods can be less robust or show less generalizability. However, in many shown cases, the baselines simply cannot locate the attack, which can significantly lower their overall performance. So the reviewer personally is in favor of a more well-rounded result report than a biased one.\n\n6. Figure 6 reports results under simulated attacks, which can be less convincing, since attacking images using circles and rectangles can easily leave traces for localization. Also, the. difference between table 7 and table 9 is worth inquiring: does table 7 report in-domain (train and test from a same dataset) performance and table 9 report cross-domain? If not, the reviewer cannot understand why there is a large gap of the baselne performances between the two tables. Also, the figure 10, the watermarks can be barely seen for all methods.\n\n7. Finally, the baselines can be further improved. From the results, we see that the baselines have a gap with the proposed method. Meanwhile, imuge and imuge+ are proposed by a same team, and W-RAE is also using INN for this task. Thus, the reviewer think we need to add more stronger baselines. Remind: it seems this task can be easily achieved by many other image translation networks, such as transformers, diffusions, mambas, etc."}, "questions": {"value": "Please refer to the previous section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n.a."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vaUoUk1YAM", "forum": "YDaxkBBRB5", "replyto": "YDaxkBBRB5", "signatures": ["ICLR.cc/2026/Conference/Submission11719/Reviewer_w6yU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11719/Reviewer_w6yU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760686353227, "cdate": 1760686353227, "tmdate": 1762922760391, "mdate": 1762922760391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "GCbQpguQke", "forum": "YDaxkBBRB5", "replyto": "YDaxkBBRB5", "signatures": ["ICLR.cc/2026/Conference/Submission11719/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11719/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763047778973, "cdate": 1763047778973, "tmdate": 1763047778973, "mdate": 1763047778973, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on image self recovery, which in short is to locate the tampered areas within an image and try to recover the original area. The proposed method ReImage proposes embedding a shuffled version of the target image as a watermark via a novel framework to misalign corrupted regions and their watermarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Authors design several modules to improve the self-recovery, and the recovered images are better.\n- Authors provides many experimental results to show the performance in different aspects."}, "weaknesses": {"value": "- This paper to me is more like an engineering paper. Many parts (from the pipeline: two-staged, similar to the compared imuge, to the model: inn, similar to W-RAE, again to the training details: mixed and fixed jpeg, filtering, etc.) show minor academic innovation. The proposed pixel shuffling for watermark and the two modules (IE and WG) are also trivial designs.\n- Pixel shuffling improved image recovery, but also increase the entropy of the information to be hidden. After reading Section 3.3.2, I still don't understand why in the proposed method, both imperceptibility and recovery are both improved (simply via using transformer blocks in INN?)\n- Experimental details seems lacking, and the compared results deviate from the ones reported in the paper."}, "questions": {"value": "Major:\n- How are the methods compared? I see a significant performance gap between the original reported ones and those in here. E.g., In W-RAE, the averaged reported PSNR was 32.42 (db), while in this paper mostly lower than 25db.\n- Why do the authors use a novel dataset setting for comparison? Rather than accepting an existing experiment setting? Why a third party paper Editguard's criterion is applied here?\n- In the tables and figures i don't see the mask result. Also in table 9 the psnr of two compared method are as low as 15- db. Authors should give mandatory test details to indicate that this was not a mistake.\n- Ensuring weak data hiding and good performance is a trade-off. Just like what is mentioned in \"weaknesses\", still do not understand how is the method so efficient in the watermarking? Also, what block size do you use in the experiments? If i understand it corrent, the P=4 refers to the num of block in transformer? Besides, why the curve of recovered image also go down with the increase of num of blocks? WIth hidden image (ground truth) shuffled further, we should expect that the recovery capacity should increase, despite the burden on the embedding side.\n- Unclear motivation: \"Since we apply the shuffling algorithm, the recovered image ˆIorg exhibits globally distributed degradation\" i \n\nMinor:\n- Real-scene attacks can be more likely compound attacks than rare attacks like hue adjust and contrast change. E.g., dual compression, or rescale than JPEG.\n- It is not easy for me to distinguish which table and figures are reporting results on real-scene attacks, which are on simulated conditions.\n- Writing issue - you should refrain from always using the right (closing) quotation mark in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RVVWu10k2H", "forum": "YDaxkBBRB5", "replyto": "YDaxkBBRB5", "signatures": ["ICLR.cc/2026/Conference/Submission11719/Reviewer_ZhA1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11719/Reviewer_ZhA1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760687132630, "cdate": 1760687132630, "tmdate": 1762922759783, "mdate": 1762922759783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose ReImage, a novel framework for image self-recovery designed to restore original image content from a tampered version. The core problem this paper addresses is the failure of existing methods to accurately reconstruct tampered regions, which often results in blurry or low-fidelity output."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Self-Embedding Strategy: The method introduces a novel self-recovery framework by embedding a pixel-shuffled version of the image into itself as a watermark. This shuffling strategically creates spatial misalignment, which ensures that if a region is tampered with, the information required for its restoration is preserved in a different, intact part of the image.\n\n- Watermark Generation Module: The paper identifies a critical limitation of shuffling: it introduces high-frequency components that degrade the container image quality and extraction accuracy. The proposed Watermark Generator (WG) is a key innovation that solves this by transforming the original image into a secret image optimized for watermarking, specifically by applying a loss that ensures smoothness after the shuffling operation .\n\n- Performance: ReImage demonstrates superior performance, significantly outperforming existing state-of-the-art methods across diverse and challenging tampering scenarios, including AIGC inpainting, splicing, and manual Photoshop editing. The effectiveness is rigorously validated using extensive experiments and strong metrics, which specifically measures the recovery quality within the tampered regions."}, "weaknesses": {"value": "- High Memory Consumption: The method requires 4570MB of VRAM to process a single 256x256 image, which is over 8 times higher than the W-RAE baseline. This high cost makes it possibly difficult to deploy and virtually unfeasible to scale to modern high-resolution images (e.g., 1024x1024) without significant modification.\n\n- Critical Dependency on Tamper Localization: The final recovered image $\\hat{I}_{rec}$ is a composite, created by selectively combining the recovered image and the tampered image using a predicted mask $\\hat{M}$. This creates a critical dependency: if the Tamper Localization (TL) module is not perfectly accurate and fails to cover the entire tampered area, the final restored image will retain parts of the manipulated content, undermining the method's goal."}, "questions": {"value": "Please refer from Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UMOarKMunM", "forum": "YDaxkBBRB5", "replyto": "YDaxkBBRB5", "signatures": ["ICLR.cc/2026/Conference/Submission11719/Reviewer_qAz2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11719/Reviewer_qAz2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729678611, "cdate": 1761729678611, "tmdate": 1762994475960, "mdate": 1762994475960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReImage, a self-recovery framework for images based on neural watermarking. The approach embeds a shuffled, high-frequency-suppressed version of the target image into itself as a watermark using an invertible neural network (INN)-based system. The key contributions include a specialized watermark generator, pixel shuffling to disrupt spatial correlation for better tamper recovery, and an image enhancement module to further improve output fidelity. Extensive experiments on the MS-COCO2017 dataset and various tampering scenarios show state-of-the-art performance, both in visual quality and robustness to common degradations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Effective Framework: The paper proposes ReImage, a well-designed neural watermarking-based self-recovery method that leverages pixel shuffling to spatially misalign watermark content with the image. This innovation addresses a known issue of recovery failure due to alignment between tampered and watermarked regions along with clustered tampered regions in secret image.\n\n2. Thorough Design and Ablation Study: The architecture is modular and interpretable, consisting of components like the invertible watermarking network, a learned watermark generator, image enhancement, and tamper localization. Ablation experiments clearly demonstrate the impact of each component on recovery performance.\n\n3. Superior Performance on Tampering Tasks: On various tampering types (e.g., SD-Inpaint, SDXL, and splicing), ReImage achieves state-of-the-art results, showing improvements over prior work like W-RAE and Imuge+."}, "weaknesses": {"value": "1. Insufficient Evaluation under diverse degradations: While robustness to three types of degradations is briefly evaluated (Gaussian noise, JPEG compression, and Poisson noise), the degradation types are limited, and there is no geometric degradation included (Imuge has included cropping in its experimental evaluation). It is well known that geometric degradations, such as cropping, pose significant challenges for watermarking models, suggesting a potential trade-off between robustness and image quality.\n\n2. Limited Real-World Validation: The limited range of tested degradations raises concerns about the method’s practicality, as real-world scenarios often involve more complex and compound distortions—such as slight cropping, minor rotations, or cases where a region of an image is cropped and spliced onto another image. These types of manipulations were not sufficiently explored, making it unclear how well the method generalizes beyond controlled experimental settings."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uu9mEwa7IT", "forum": "YDaxkBBRB5", "replyto": "YDaxkBBRB5", "signatures": ["ICLR.cc/2026/Conference/Submission11719/Reviewer_ArYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11719/Reviewer_ArYg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984273643, "cdate": 1761984273643, "tmdate": 1762922758568, "mdate": 1762922758568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Authors, \n\nReceived your message and noted your concern. I think that reviewer qAz2 has corrected it. If you have any problem, please message me. \n\nRegards,\nAC"}}, "id": "dfbDgIycn9", "forum": "YDaxkBBRB5", "replyto": "YDaxkBBRB5", "signatures": ["ICLR.cc/2026/Conference/Submission11719/Area_Chair_RQDf"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11719/Area_Chair_RQDf"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission11719/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762999015197, "cdate": 1762999015197, "tmdate": 1762999015197, "mdate": 1762999015197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}