{"id": "qwxSBnEPEc", "number": 6687, "cdate": 1757992306103, "mdate": 1759897900952, "content": {"title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization", "abstract": "Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. In the second stage, these learned patterns are embedded as in-context, self-declarative guidance, teaching the model to proactively plan its reasoning budget. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\% while improving accuracy by 2.3\\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.", "tldr": "", "keywords": ["Efficient reasoning; Reinforcement learning; Large Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aaca8ad7812346815fd3615e18f628692074ea79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LAPO, a two-stage reinforcement learning framework to improve the reasoning efficiency of large models. The first Discovery stage learns from the statistical distribution of successful solutions, using a length-aware reward that encourages lengths within a P30-P70 percentile range. The second Internalization stage embeds the discovered median length from this distribution as an in-context, self-declarative statement. The model is then trained with an adherence reward to follow this self-proposed plan. The authors claim this process reduces token usage by up to 40.9% while improving accuracy by 2.3%."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "+ The primary strength is the methodological novelty. The core idea of a two-stage discover-then-internalize process is elegant.\n\n+ The Discovery stage's use of a statistical median of successful solutions is a clever and simple heuristic, contrasting with more complex difficulty-prediction models.\n\n+ The Internalization stage's mechanism, which combines an in-context, self-declarative statement with an explicit RL adherence reward, is a novel and interesting approach to instilling a policy for resource control.\n\n+ The paper is well-written, and the internal ablation studies are purposeful and effectively demonstrate the logic of the proposed components."}, "weaknesses": {"value": "**Major**\n\n+  **Discrepancy in Baseline Performance and Lack of Statistical Variance:** The paper's reported baseline performance for DeepScaleR-1.5B-Preview shows a notable discrepancy with its original source (e.g., 35.5% Pass@1 on AIME2024 in this paper, vs. 43.1% reported in [1]). This raises questions about the experimental setup and the validity of the baseline reproductions. Furthermore, for high-variance tasks like long CoT reasoning, reporting statistical variance is crucial. The paper reports all results as single numbers without providing standard deviations or confidence intervals, which makes it difficult to assess the stability and significance of the results.\n\n+  **Narrative of Internalization Not Fully Aligned with Implementation:** The paper's core claim of internalization appears partly inconsistent or not fully aligned with its implementation. The planned length is not generated by the model; it is an external statistic computed and then forcibly injected into the model's context. The model is then compelled to follow this external plan via a strict, external adherence reward. This demonstrates \"behavioral adherence training\" to a specific prompt, rather than a true \"intrinsic\" internalization of planning, as the model is never trained to predict this length itself.\n\n+  **Statistical Patterns Lack Deeper Justification:** The paper claims to use \"discovered statistical patterns\" but relies on a single heuristic (the median/P50). The paper does not appear to provide a deeper investigation into why the median is a suitable statistic. It does not analyze the underlying distribution of successful solution lengths or explore alternatives. This makes the choice of the median seem statistically arbitrary, as it may not be a robust representative of the data.\n\n+  **Lack of Quantitative Evidence for Adherence:** The paper's core mechanism relies on the model learning to adhere to its plan, yet it provides no direct, quantitative analysis on any benchmark to show the degree of alignment between the \"planned length\" ($l_{plan}$) and the \"actual length\" ($l_{actual}$). There is no pearson correlation, rmse, or scatter plot to substantiate this claim, which would make the evidence for the adherence reward's effectiveness more compelling.\n\n+  **Missing Key Ablation Study:** The paper does not appear to fully demonstrate the necessity of the Stage 1 (LAPO-D) training phase. A key ablation seems to be missing: \"1. Collect M statistics from the base model (without LAPO-D training), then 2. Proceed directly to Stage 2 (LAPO-I) training.\" Without this, it remains unclear whether the computationally expensive LAPO-D training is necessary, or if simply collecting the statistics would be sufficient.\n\n**Minor**\n\n+ **Missing Citation for Key Model:** The paper does not provide a citation for its primary experimental model, DeepScaleR-1.5B-Preview. This is an omission that could hinder reproducibility.\n\n+ **Potentially Mismatched Primary Area Selection:** The reported primary area \"applications to computer vision, audio, language, and other modalities\" may not be the best fit. While the work is related to \"language,\" this category often implies a multimodal focus. The authors might consider whether categories such as \"reinforcement learning\" would more accurately reflect the paper's core contributions.\n\n+ **Missing Hyperparameter Sensitivity Analysis:** The choice of the P30-P70 range for the natural length is presented as a heuristic, but it is unclear how sensitive the results are to this specific choice. The paper does not appear to explore whether alternative ranges (e.g., P20-P80, P40-60) would significantly alter the results.\n\n***\n[1] Michael Luo, Sijun Tan, Justin Wong, et al. (2025). DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL. Notion Blog."}, "questions": {"value": "Please see weaknesses\n\n---\nDespite the novel methodology presented, I believe the paper in its current state is not yet at the level for acceptance due to the major weaknesses listed above. I would be willing to reconsider my score if these issues are comprehensively and satisfactorily addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G8Qbf8fTC4", "forum": "qwxSBnEPEc", "replyto": "qwxSBnEPEc", "signatures": ["ICLR.cc/2026/Conference/Submission6687/Reviewer_5Ein"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6687/Reviewer_5Ein"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921284259, "cdate": 1761921284259, "tmdate": 1762918983829, "mdate": 1762918983829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Length-Adaptive Policy Optimization (LAPO), a two-stage reinforcement learning framework designed to mitigate the \"overthinking\" problem in large language models by improving reasoning efficiency. The first \"Discovery\" stage trains the model to identify a distribution of effective reasoning lengths for given problems. The second \"Internalization\" stage embeds statistics from this distribution (specifically, the median length) as in-context, self-declarative guidance, training the model to adopt an appropriate computational budget as part of its own reasoning plan. The authors evaluate LAPO on several mathematical reasoning benchmarks, reporting that the method reduces token generation while maintaining or improving task accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses the critical and timely problem of computational inefficiency in chain-of-thought reasoning, which is a significant barrier to the practical deployment of large reasoning models.\n2. The core concept of a two-stage \"Discover-Internalize\" process is novel (to the best of my knowledge) and well-motivated. The idea of transforming length control from an external constraint into an intrinsic model capability seems a promising research direction.\n3. The experimental results demonstrate decent performance on the Pareto frontier of accuracy versus efficiency on 4 challenging mathematical reasoning benchmarks."}, "weaknesses": {"value": "1. **Marginal Gains Over Simpler Baselines Undermined by Methodological Complexity:** The performance improvement of LAPO-I over the simpler single-stage \"Acc-Only\" RL baseline is marginal (e.g., from 63.9% to 64.8% average accuracy on the DeepScaleR model). This small gain is achieved via a complex two-stage framework with numerous design choices and new hyperparameters (e.g., the percentile range [P30, P70], reward weights \\alpha and \\beta, and the gaussian standard deviation \\sigma). It seems from the ablation study in Table 3, which compares median, mean, and minimum suggests these choices were optimized based on final evaluation performance. This makes the comparison against the un-tuned \"Acc-Only\" baseline unfair and casts doubt on whether any true performance gain exists beyond what could be achieved by hyperparameter tuning the simpler baseline.\n2.  The paper does not include a sensitivity analysis for its numerous new hyperparameters, which is a critical omission for a method of this complexity. \n3. **Unfair Comparison of Training Compute:** The paper does not account for the significant additional computational cost of its two-stage training pipeline. A critical question is whether the \"Acc-Only\" baseline would match or exceed LAPO's performance if it were simply trained for more steps, using the same total compute budget allocated to LAPO's two stages. Without this analysis, it is impossible to conclude that the complexity of LAPO is justified.\n4. **Insufficient Baselines:** While the space is fast moving, some simple baselines such as ShorterBetter [1] and Arora et al's [2] work on efficient reasoning is not compared. This makes it difficult to figure LAPO's results within the current state-of-the-art.\n\n**Other minor weaknesses**\n\n5. **Overstated and Potentially Misleading Claims:** The abstract (and introduction) claims LAPO \"reduces token usage by up to 40.9% while improving accuracy by 2.3%.\" This is a \"best-of-both-worlds\" claim that conflates results from two different base models; the 40.9% token reduction corresponds to a 1.2 point accuracy gain on one model, while the 2.3 point gain corresponds to a 38.5% token reduction on another. This phrasing is misleading.\n6. Further, the choice to pre-declare a static reasoning length might be a limitation, or there maybe a better performing method that adaptively decides the reasoning length as it solves the problem. (Although this is a minor point and not considered for deciding the final score)"}, "questions": {"value": "1. Can authors confirm if the results are averaged over multiple seeds?\n2. Can authors also present results for AIME25, given it is less likely to be contaminated than AIME24 or other benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m4GENZ7V1e", "forum": "qwxSBnEPEc", "replyto": "qwxSBnEPEc", "signatures": ["ICLR.cc/2026/Conference/Submission6687/Reviewer_seX5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6687/Reviewer_seX5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940730025, "cdate": 1761940730025, "tmdate": 1762918983364, "mdate": 1762918983364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper deals with efficient reasoning language models, specifically solving problems with the minimum number of required reasoning tokens. The paper proposes LAPO, a two stage process for fine-tuning a language model. The first stage runs GRPO with a reward that incentivizes the model to generate within the 30-70th percentile of lengths. The second stage runs GRPO again with a reward that incentivizes the model generate the median length, and additionally provides the model with this length in its input."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The use of a prompted length within a RL loop so that the model also learns to generate the length and adhere to it is a nice extension of methods that manually prompted the model with a target length.\n- The proposed method improves upon the base model in terms of token reduction."}, "weaknesses": {"value": "- The paper proposes a two-stage method, but it is not clear why a two-stage method is needed. For example, it is unclear why optimizing a reward function that has a length penalty in a single stage would be worse than the proposed method. Additional experiments such as this or are needed to better justify the two-stage design.\n- The main experiments focus on justifying the method by comparing the new method with prior methods. However, the prior methods have several potential compounding factors, including different training data, number of training iterations, early stopping criteria, hyperparameters, etc. Additionally, when prior methods are introduced, not enough detail is given to gain insight into how the method differs from LAPO (i.e., which variable is being varied in the comparison). As a result, it is difficult to draw conclusions about the benefits of the specific innovations in LAPO compared to alternatives.\n- Even if we assume the confounding factors are not an issue, the proposed method often underperforms prior methods in terms of token reduction. To take one example, in Table 1 AutoThink achieves 2017 #Tok, while LAPO achieves 2354 #Tok. It is unclear whether the small difference in pass@1 is significant.\n- The paper motivates itself in terms of \"external\" vs. \"internal\" reasoning constraints. The paper categorizes its work as internal, and says that their work is a \"paradigm shift\". However, it is unclear why a length penalty that is incorporated into an RL reward, which has been investigated in other papers, would not fall under the umbrella of \"internal\" as they have defined it. Hence I do not see why the proposed method is claimed to be a paradigm shift.\n- The paper has several unjustified connections with human reasoning. For example, the paper claims that the proposed process mirrors how human experts develop intuition and allocate mental effort.\n- The paper has several claims that are not sufficiently defined, imprecise, or difficult to test. For example, \"A cornerstone of our framework is how it fosters genuine internalization\"; it was unclear what this means or how to test it."}, "questions": {"value": "Please respond to the points above, including:\n- Can you provide experiments that justify the two-stage approach?\n- Can you compare with the baseline methods in a consistent setting?\n- The first stage fits outputs to the 30-70th percentile of lengths, then the second stage optimizes for the median. What objective is this two-stage procedure optimizing? Is it possible to optimize in one stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rGbIN5Kjh8", "forum": "qwxSBnEPEc", "replyto": "qwxSBnEPEc", "signatures": ["ICLR.cc/2026/Conference/Submission6687/Reviewer_gbkb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6687/Reviewer_gbkb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941890299, "cdate": 1761941890299, "tmdate": 1762918982933, "mdate": 1762918982933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Length-Adaptive Policy Optimization (LAPO), a framework for training language models to adjust reasoning length based on problem complexity. After a standard RL training phase with a length-based additional reward term, the model is trained to follow a self-declarative length constraint forced as part of the response template. The paper evaluates on standard math reasoning benchmarks and demonstrates that LAPO reduces token usage while improving accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Overall, the proposed method is well-motivated and makes sensible design choices. The experimental results are comprehensive and strong, both in terms of Pass@1 and token count.\n- The paper is generally well-written with a clear structure."}, "weaknesses": {"value": "- For LAPO-D, the choice of [30, 70] percentile is a bit arbitrary, and I'm not fully convinced that it's necessary. Have you done ablations to determine whether percentile filtering is necessary?\n- The source of the performance gains is unclear to me. Examining the results, it appears that most of the gains over prior work are attributed to LAPO-D, and even the Acc-Only ablation is quite strong. Do you know why your baseline is so strong compared to prior works? Is there a difference in any key hyperparameters like batch size or number of epochs? It's strange that many of the reported numbers are even worse than the base model."}, "questions": {"value": "- Do you have statistics on how response length changes per question? Are the savings in average token count coming from a global reduction or from a decrease in the longest solutions? \n- I wonder if it's possible to context distill (i.e., https://arxiv.org/abs/2209.15189) so that we can get the performance/length of LAPO-I even without the length-conditioned rollout.\n- Do you have an analysis (probably qualitative) on whether the model actually proposes length guidance in the training format for new test examples, and whether the actual length follows this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tZws7tU4rs", "forum": "qwxSBnEPEc", "replyto": "qwxSBnEPEc", "signatures": ["ICLR.cc/2026/Conference/Submission6687/Reviewer_9csv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6687/Reviewer_9csv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762242827843, "cdate": 1762242827843, "tmdate": 1762918982493, "mdate": 1762918982493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}