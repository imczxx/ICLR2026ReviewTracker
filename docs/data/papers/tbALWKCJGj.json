{"id": "tbALWKCJGj", "number": 1182, "cdate": 1756860878978, "mdate": 1759898223225, "content": {"title": "Taming Text Alignment for Personalization: Disentangling Foreground Customization and Background Style in Diffusion Models for Personalized Image Generation", "abstract": "Personalized image generation focuses on synthesizing text-driven images conditioned on reference images aimed at specifying the details of the generated content. Typically, it involves two key subtasks: *image customization* and *style transfer*. Previous arts failed to align the generated image with the text prompts, specifying as misalignment with background for image customization and foreground for style transfer, owing to the dense image references that attend only to either the foreground or background, while overwhelming the sparse text prompts.  To tackle the text-alignment for personalized image generation, in this paper, we propose a **D**ual-**R**eference **P**ersonalization diffusion model, dubbed **DRP-Diff**, for both image customization and style transfer to achieve the text alignment for personalized image generation, where the crux lies in disentangling the foreground customization and background style, so as to separately align each of them with text prompts during different stages of the denoising process. To achieve the alignment between text prompt and customized reference focusing on background, throughout our *customized texture-disentangled matrix*, we concatenate the foreground texture of both the customized and style references as key in the cross-attention to reconstruct the query background of the denoised personalized image. To align the style reference focusing on foreground with text prompts, we serve the background of style reference via *style-disentangled matrix* as the key associated with its value to reconstruct the query foreground of the denoised personalized image. Each of the above two processes are conducted in the early and late stage of the whole denoising process guided by text prompts. To adaptively modulate the denoising timesteps between the early and late stages, the ratio of information entropy between the customized and style references is calculated. Extensive experiments validate the superiority of DRP-Diff over the state-of-the-art diffusion models for personalized image generation. *Our code can be accessed from the supplementary material package.*", "tldr": "", "keywords": ["Image customization", "Style transfer", "Diffusion models", "Personalized image generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8bd1519c72fef28d59ccda9ddb4768086b147a30.pdf", "supplementary_material": "/attachment/ea75bb6d4b4d5682f0c58896c328e48c219bc549.zip"}, "replies": [{"content": {"summary": {"value": "The authors proposed a customization method of a diffusion model, focusing on objects and style. The method requires a text prompt, object reference images, and style reference images separately. They also propose to disentangle the foreground and background of images and process them separately. The suggestion is to have different procedures for different sides of the image."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The only strength I see in this paper is the approach (not the idea) of disentanglement of foreground and background with the SVD method."}, "weaknesses": {"value": "1. Poor problem formulation - not justified separation of diffusion model customization and style transfer. In both cases, the model gets customized to concepts, and the concepts are either objects or style.\n\n2. The idea of background and foreground disentanglement is artificial and unnatural. No justification for why this method is superior. No justification for why to formulate image customization for foreground and style transfer for background.\n\n3. The problem mentioned in 083 - incorrect number of “woman” - can have different roots. That is not proof of the problem the authors described.  e.g., in the paper Make It Count [1], the diffusion model struggles to keep the count correct in generated images. They do not deal with any style transfer.\n\n4. Unclear logical chain in 086-089. It needs more evidence to make it true.\n\n5. The proposed method requires 2 types of reference images (dual reference): object images and style images.\n\n6. The method is complex.\n\n7. Poor fidelity of model customization on concepts in Figure 5. Specifically, the robot's attributes are very different in the original and DRP-Diff generated images: arm color, legs, and texture on the belly - all are different. Same thing with the butterfly example - there is no red texture on the original one.\n\n8. One of the most significant weaknesses is a very bad, unclear, and complex writing style: even the known knowledge is written badly with complex formulations. (e.g., Figure 3 caption, Table 1 b caption, method section, etc.). Also, poor visualization for tables and figures in quantitative and qualitative comparisons. They are very small, badly organized. Its customization, the visual attributes should be investigated properly.\n\n[1] Make It Count: Text-to-Image Generation with an Accurate Number of Objects\nLital Binyamin, Yoad Tewel, Hilit Segev, Eran Hirsch, Royi Rassin, Gal Chechik\nThe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}, "questions": {"value": "1. What are “sparse text prompt” and “dense text prompt”?\n2. How background of F get disentangled in the noise latent code (Fig. 2 caption)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4wU6Y8rmLD", "forum": "tbALWKCJGj", "replyto": "tbALWKCJGj", "signatures": ["ICLR.cc/2026/Conference/Submission1182/Reviewer_rfeM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1182/Reviewer_rfeM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761252467480, "cdate": 1761252467480, "tmdate": 1762915699384, "mdate": 1762915699384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DRP-Diff, a dual-reference diffusion model for personalized image generation that improves text alignment by disentangling foreground customization and background style. By using separate texture- and style-disentangled matrices, the method aligns background and foreground with text prompts in early and late denoising stages, respectively, guided by an adaptive timestep strategy based on information entropy. Experiments show superior performance over existing approaches."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall writing is easy to understand.\n2. The author analyzes the response of principal components in the attention mapping matrix to image foreground and background from a parametric perspective, supported by visualization. Furthermore, a backgroud-decoupled matrix $\\Delta W_{k}^{im}$ is introduced to refine the parameters. This design is reasonable and insightful.\n3. On basis of the above design, the author modifies the attention interaction, effectively preserving the main semantic content of the subject image while decoupling background information and irrelevant texture details, thus facilitating better integration of texture information from the style image. This is also validated by several experimental results.\n4. The author further introduces a background-sensitive matrix $\\overline{W}^{im}_k$ in the late stage to help the model better capture low-level features of the style image."}, "weaknesses": {"value": "1. Intuition and Justification for SVD-based Disentanglement: The paper explains the SVD inversion in Equation (5) to enhance background components. While mathematically presented, a deeper intuitive or empirical justification for choosing $\\frac{1}{\\sigma_i}$ over other potential weighting schemes could strengthen this crucial design choice. Did the authors explore other ways to manipulate singular values (e.g., simply setting the weight of foreground-sensitive principles to 0), and why was this specific form the most effective?\n2. Could there be further analysis on the SVD decomposition, such as the numerical distribution of $\\sum$ (i.e., the singular values), and whether the parameter matrices across different layers exhibit similar characteristics?\n3. This work appears heavily based on IP-Adapter, but it is unclear whether the encoding of reference/style images and attention interaction patterns remain consistent across different customized models. I am concerned about potential over-tuning. Could the authors provide further analysis to validate the generalizability of their method?"}, "questions": {"value": "please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "akcdTdAbtK", "forum": "tbALWKCJGj", "replyto": "tbALWKCJGj", "signatures": ["ICLR.cc/2026/Conference/Submission1182/Reviewer_Ukfn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1182/Reviewer_Ukfn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633130043, "cdate": 1761633130043, "tmdate": 1762915699227, "mdate": 1762915699227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DRP-Diff, which is a personalization method for T2I diffusion models that works for both image customization (with text condition) and style transfer (with image condition). The paper addresses the text condition & generated image and image condition & generated image misalignment in prior works by disentangling foreground customization and background style. Such disentanglement is enabled by the below two key methods:\n1. Customized texture-disentangled matrix and style-disentangled matrix obtained via SVD to separate foreground and background contributions in cross-attention.\n2. Early–late denoising stage division, modulated adaptively using an entropy ratio between customized and style references.\n\nComprehensive experiments show consistent improvements and better visual quality over baselines like IP-Adapter, MS-Diffusion, RB-Modulation, etc."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's motivation is clear and interesting.\n2. The method is, in general, well motivated and technically solid, with most of the details introduced in the paper.\n3. The paper's performance is strong across several different metrics and settings."}, "weaknesses": {"value": "1. The paper has several issues considering its writing. (1) The tables, figures, and their fonts are too small, very hard to see. (2) It is sometimes difficult to follow due to excessive formulas and symbols. There could be a notation table for better reference. (3) There appear to be many long sentences. The core intuition could be communicated more concisely.\n2. The claim in L204-205 that \"larger $\\sigma_i$ tend to correspond to the foreground, whereas smaller σi are more likely associated with the background\" lacks support from quantitative/statistical supports, only demonstrated by some visualizations.\n3. There is lack of analysis and discussions on the method efficiency, i.e., how much more time/compute is added by the method."}, "questions": {"value": "1. L090-L093: The long sentence: \"So far, neither image customization nor style transfer fails to well align the generated image with the text\nprompts, specifying as misalignment with background for image customization and foreground for style transfer, owing to the dense image references that attend only to either the foreground or background, while overwhelming the sparse text prompts.\" is very hard to understand, and seem to be in the opposite meaning.\n2. Whether the authors can provide quantitative/statistical support for the claim in L204-205?\n3. Whether the authors can provide analysis and discussions on the method efficiency, i.e., how much more time/compute is added by the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "voKVfkkws2", "forum": "tbALWKCJGj", "replyto": "tbALWKCJGj", "signatures": ["ICLR.cc/2026/Conference/Submission1182/Reviewer_uCgN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1182/Reviewer_uCgN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881819389, "cdate": 1761881819389, "tmdate": 1762915699119, "mdate": 1762915699119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DRP-Diff, a dual-reference personalized diffusion framework that addresses text-image misalignment in customized image generation. It identifies that existing methods overemphasize either the foreground or background, leading to mismatched content. DRP-Diff disentangles style and texture via SVD-based matrices and employs stage-specific processing: early stages align backgrounds using dual-reference textures, while late stages enhance text-consistent foregrounds through adaptive timestep modulation guided by information entropy. Extensive experiments on DreamBench and StyleBench show superior performance over CLIP-based baselines. The framework offers both practical improvements and theoretical insights into diffusion model behavior in personalized generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Systematically leverages the diffusion process’s global-to-local progression to address text alignment, introducing style disentanglement, texture disentanglement, and adaptive timestep modulation.\n\n2. Comprehensive ablations confirm each module’s contribution to detail preservation, style quality, and entropy-based adaptivity.\n\n3. Provides full mathematical derivations, implementation details, and open-source code for reproducibility and theoretical soundness."}, "weaknesses": {"value": "While the paper claims that \"the high-level noise in the early stage benefits background reconstruction\" and \"the late stage prefers foreground reconstruction,\" it lacks rigorous theoretical analysis connecting diffusion process characteristics to text alignment. The paper cites prior work (Wu et al., 2023; Chen et al., 2023) that observed semantic hierarchy in diffusion models, but fails to explain why this hierarchy specifically addresses the text alignment problem. For instance, Section 2.2 mentions \"owing to the high-level noise in the early stage to benefit the background reconstruction,\" although it may seem right, but provides no mathematical derivation or frequency analysis to support this claim."}, "questions": {"value": "1. The paper demonstrates the effectiveness of the adaptive timestep modulation mechanism through Figure 6 and Case F in Table 2(b). However, could the authors provide a more detailed analysis on the specific behaviors and handling strategies when H_cu and H_st exhibit extreme deviations?\n﻿\n2. The paper mentions generating C^te_bg by replacing the foreground in C^te with {}, but does not explain how to automatically identify the foreground components within the text prompts. For complex prompts such as \"The glowing moon rose above the distant hill while a group of birds flew across the calm sea,\" how does the system accurately distinguish multiple foreground objects from the background? Could the authors provide an error rate analysis and specify the exact entity recognition methodology employed?\n\n3. Figure 7 illustrates that StyleStudio encounters difficulties with multi-object prompts (e.g., \"incorrect number of 'woman'\"). However, the paper does not elaborate on how DRP-Diff specifically addresses this issue. How does the method's performance change as the number of objects increases (e.g., \"seven women\") or when inter-object relationships become more complex? Could the authors provide a quantitative relationship analysis between object count and performance metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Ptlgm6Zxs", "forum": "tbALWKCJGj", "replyto": "tbALWKCJGj", "signatures": ["ICLR.cc/2026/Conference/Submission1182/Reviewer_GChA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1182/Reviewer_GChA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894816213, "cdate": 1761894816213, "tmdate": 1762915698955, "mdate": 1762915698955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}