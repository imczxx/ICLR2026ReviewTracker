{"id": "omEK0fdKDD", "number": 5550, "cdate": 1757919843137, "mdate": 1759897968135, "content": {"title": "Paper2Video: Automatic Video Generation from Scientific Papers", "abstract": "Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: long-context inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Bench, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics—Meta Similarity, PresentArena, PresentQuiz, and IP Memory—to measure how videos convey the paper's information to the audience. Building on this foundation, we propose Agent, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel Tree Search Visual Choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Bench demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation.", "tldr": "Paper2Video -- Automatic Video Generation from Scientific Papers via Multi-Agent System", "keywords": ["AI for Research; Benchmark; Multi-Agent; Video Generation;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/317983fcb30e48abaccf17699fe6be703ca3db05.pdf", "supplementary_material": "/attachment/fa310cd9191f5f354d55b911d4f714ee8162323e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a benchmark called Paper2Video and a multi-agent framework, PaperTalker, for automatically generating academic presentation videos from academic papers. The authors first constructed a benchmark dataset consisting of 101 papers paired with author-recorded videos and proposed four new evaluation metrics (Meta Similarity, PresentArena, PresentQuiz, and IP Memory) to measure the quality of automatically generated videos. The authors then designed the PaperTalker framework, improving efficiency through a modular parallel generation strategy. Experimental results show that this approach outperforms existing methods (such as PresentAgent and Veo3) across multiple metrics and approaches the performance of manually produced videos in human evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper defines academic presentation video generation as an AI for Research task and provides a benchmark and evaluation criteria.\n2. Four custom metrics (Meta Similarity, PresentArena, PresentQuiz, and IP Memory) cover different dimensions of academic presentation videos, from content fidelity, audience comprehension, and memorability.\n3. The proposed multi-agent PaperTalker framework is well-designed, encompassing slide layout optimization, speech generation, talking head synthesis, and cursor grounding."}, "weaknesses": {"value": "1. The paper claims that videos generated by PaperTalker are \"more faithful and informative than existing baselines.\" However, the user study and subjective evaluations are based on a small sample size (only 10 participants), and the evaluator backgrounds, scoring criteria, and statistical significance are not disclosed, which undermines the credibility of the conclusions.\n2. Four custom metrics (especially PresentArena and PresentQuiz) rely heavily on VideoLLM's judgments, but the paper does not detail the bias control mechanisms. For example, PresentArena's pairwise comparisons may be affected by LLM bias, and no consistency or confidence intervals are reported.\n3. Paper2Video includes only 101 sample papers, primarily from AI conferences (such as NeurIPS, ICLR, and CVPR), with limited field coverage, which limits the model's generalizability. There is a lack of cross-disciplinary validation samples (such as biology and physics).\n4. In comparisons with baselines such as PresentAgent and Veo3, the authors did not explicitly control the consistency of input conditions (such as paper length, slide templates, and audio quality), which may affect the fairness of the conclusions. In particular, Veo3 is a natural video generator, and its task objectives are different.\n5. While Figure 7 and the related discussion demonstrate visual results, they lack in-depth qualitative analysis, such as a discussion of the interpretability of slide layout optimization, a classification of generated error types, or an analysis of specific failure cases.\n6. The paper assumes that the cursor remains stationary within each sentence and only moves between sentences. This is significantly inconsistent with real human speech. Speakers often dynamically move the cursor throughout a sentence to emphasize specific words, parts of a formula, or areas of a diagram. This simplified, discrete movement may feel unnatural to the audience. While the paper experimentally demonstrates that the presence of the cursor helps locate information, it does not evaluate the quality of the generated cursor trajectory itself, such as whether it compares to human-generated trajectories in terms of naturalness and effectiveness."}, "questions": {"value": "1. In the PresentArena metric, VideoLLM is used as an automated judge. Please explain how you control the consistency of LLM responses? Do you control for prompt or sequence bias?\n2. Are the full LaTeX source code, slides, and videos for each example publicly available in the Paper2Video dataset? Are copyright and ethical approvals obtained?\n3. What is the VLM scoring metric in the Tree Search Visual Choice module? Will the computational complexity of this module become a bottleneck in larger-scale generation?\n4. Please provide more details about PresentQuiz question generation: number of questions, coverage, difficulty control, and whether it was manually validated?\n5. Does the human evaluation use statistical tests (e.g., t-test or Wilcoxon) to verify the significance of the differences between PaperTalker and human-generated videos?\n6. Regarding the speaker videos, the paper mentions the use of models such as Hallo2 and FantasyTalking. These models primarily generate head and upper body movements. Does your system consider body language, such as gestures? Although the paper mentions that FantasyTalking supports upper body movements, it does not demonstrate or discuss the relevant generation effects."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V97CfrW6ap", "forum": "omEK0fdKDD", "replyto": "omEK0fdKDD", "signatures": ["ICLR.cc/2026/Conference/Submission5550/Reviewer_DjH8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5550/Reviewer_DjH8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761018523892, "cdate": 1761018523892, "tmdate": 1762918128881, "mdate": 1762918128881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents the Paper2Video benchmark dataset and the PaperTalker multi-agent system to convert papers into slide-based presentation videos. It uses four types of metrics to measure consistency with human works, preference, knowledge coverage, and memorability. The project demo shows the framework works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Directly turns papers into publishable presentation videos.\n- Uses PaperTalker to generate academic-style slides, subtitles, voice-over, and a talking-head presenter, together with cursor-synchronized guidance, forming a closed loop.\n- Uses personalized TTS and presenter-avatar video, and adds narration subtitles and cursor pointing, which can aid memory and visibility."}, "weaknesses": {"value": "- Insufficient data scale: the benchmark is small (101), limiting coverage and statistical power.\n- Comparative evaluation is incomplete: recent methods (e.g., EvoPresent, PreGenie, Preacher) are not included, making it hard to position SOTA. In addition, only 10 participants per paper rate the videos, so persuasiveness is limited.\n- Limited style flexibility: using Beamer enforces norms but leaves limited room for personalization."}, "questions": {"value": "- Increase the dataset size and include comparisons with the latest research projects.\n- Add slide stylization features.\n- In human evaluation, increase the number of participants per paper to improve the credibility of the scores.\n- In Figure 7, the conclusion that human-made videos have no cursor may not generally hold.\n- Two of the three automatic evaluations use (video) LLMs as the viewer/judge, which can couple the evaluation distribution with model preferences; consider adding larger-scale human evaluation or user-behavior metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9rPvXhdUwl", "forum": "omEK0fdKDD", "replyto": "omEK0fdKDD", "signatures": ["ICLR.cc/2026/Conference/Submission5550/Reviewer_cGYF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5550/Reviewer_cGYF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831866662, "cdate": 1761831866662, "tmdate": 1762918128423, "mdate": 1762918128423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The Paper2Video framework offers an advancement in the automatic generation of academic presentation videos. By integrating multiple agents for slide creation, speech synthesis, and visual grounding, it addresses challenges of multi-modal coordination. The Paper2Video benchmark, containing 101 papers and corresponding videos, serves as a comprehensive tool for evaluating video quality, knowledge coverage, and memorability. The proposed system outperforms existing methods, demonstrating high alignment with human-made presentations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The task of transforming academic papers into presentation videos is innovative and holds substantial future potential.\n2. The final results demonstrate high effectiveness, with supplementary materials showcasing impressive video outputs.\n3. The paper provides a detailed and clear description of the methodology, ensuring transparency and reproducibility."}, "weaknesses": {"value": "1. **Lack of Novelty in Tree Search Visual Choice**: The PaperTalker framework appears to primarily integrate existing techniques. Specifically, the Tree Search Visual Choice lacks significant novelty, as similar approaches for layout optimization have been explored in other domains. A more innovative direction could involve designing a mechanism to generate figures on each slide that align better with the content being presented, offering deeper insights into how figures are tailored for more effective explanations.\n\n2. **Incomplete Data in Table 2**: Table 2 contains numerous missing data points, which impacts the overall clarity and interpretability of the results. The absence of these data points could be addressed by either providing an explanation for the missing values or by utilizing alternative visual representations (e.g., bar charts or heatmaps) to present the missing information more effectively.\n\n3. **Insufficient Dataset Size**: The 101 paper-video pairs used for evaluation may be insufficient to rigorously assess the method's generalizability. Given that large conferences often feature thousands of papers and videos each year, expanding the dataset would provide a more robust foundation for evaluating the model's performance across diverse topics and presentation styles."}, "questions": {"value": "Can the PaperTalker framework handle papers from highly specialized domains? While the paper shows promising results in generating academic presentation videos, how well do you think PaperTalker will perform with highly specialized or niche academic papers, particularly in domains with complex, less standardized language or intricate visuals (e.g., advanced mathematical models or domain-specific terminologies)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WRh2vE3svM", "forum": "omEK0fdKDD", "replyto": "omEK0fdKDD", "signatures": ["ICLR.cc/2026/Conference/Submission5550/Reviewer_fecP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5550/Reviewer_fecP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874843605, "cdate": 1761874843605, "tmdate": 1762918127299, "mdate": 1762918127299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}