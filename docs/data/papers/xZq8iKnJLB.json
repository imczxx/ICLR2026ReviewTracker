{"id": "xZq8iKnJLB", "number": 7588, "cdate": 1758028751010, "mdate": 1759897844732, "content": {"title": "EmoFeedback²: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback", "abstract": "Continuous emotional image generation (C-EICG) is emerging rapidly due to its ability to produce images aligned with both user descriptions and continuous emotional values. However, existing approaches lack emotional feedback from generated images, limiting the control of emotional continuity. \nAdditionally, their simple alignment between emotions and naively generated texts fails to adaptively adjust emotional prompts according to image content, leading to insufficient emotional fidelity. To address these concerns, we propose a novel generation-understanding-feedback reinforcement paradigm (EmoFeedback²) for C-EICG, which exploits the reasoning capability of the fine-tuned large vision–language model (LVLM) to provide reward and textual feedback for generating high-quality images with continuous emotions. \nSpecifically, we introduce an emotion-aware reward feedback strategy, where the LVLM evaluates the emotional values of generated images and computes the reward against target emotions, guiding the reinforcement fine-tuning of the generative model and enhancing the emotional continuity of images. Furthermore, we design a self-promotion textual feedback framework, in which the LVLM iteratively analyzes the emotional content of generated images and adaptively produces refinement suggestions for the next-round prompt, improving the emotional fidelity with fine-grained content. \nExtensive experimental results demonstrate that our approach effectively generates high-quality images with the desired emotions, outperforming existing state-of-the-art methods in our custom dataset. The code and dataset will be released soon.", "tldr": "We propose EmoFeedback², a novel framework that leverages a fine-tuned vision-language model to provide reward and textual feedback, enabling high-quality continuous emotional image generation.", "keywords": ["Emotion Understanding", "Continuous Emotion Image Generation", "Reinforcement Fine-Tuning", "Self-Promotion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cd8cde75070e95fd122146b308731091ae3b674.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EmoFeedback², a novel generation-understanding-feedback reinforcement paradigm for Continuous Emotional Image Content Generation (C-EICG). This method aims to adaptively adjust prompts to control the emotional continuity of the generations. It employs a Large Vision-Language Model (LVLM) to serve as an emotion understanding model. This LVLM provide emotion-aware reward feedback to reinforcement fine-tune the generative model. During inference, it adopts a self-promotion textual feedback framework to iteratively analyze generated images and refine the user prompt during inference. The results on a custom EmoSet-118K based dataset show that the proposed method outperforms other techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### 1. Novel generation-understanding-feedback reinforcement paradigm\n\nThe whole paradigm is novel. It trains the LVLM as a reward model for emotion understanding and uses this feedback to guide the image generation. Furthermore, it can adaptively refine the prompt to enhance the emotional control.\n\n### 2. Comprehensive experimental validation\nThe method achieves superior quantitative performance across five metrics, including V-Error and A-Error,  CLIP-Score and CLIP-IQA, against many modern baselines (EmotiCrafter, EmoEdit, FLUX, SD3.5-L)."}, "weaknesses": {"value": "### 1.The whole pipeline is over complex and heavy\nThe multi-stage process (diffusion -> RL-train LVLM to produce reward -> GRPO optimization for generation model with LVLM evaluation -> iterative textual feedback with LVLM) is extremely resource-intensive. Even at test time, the iterative self-promotion textual feedback requires multiple LVLM calls per image. Compared with using only the generation model to produce multiple outputs and get the best results, it seems a little unpractical.\n\n### 2. Limited applicability\nThe model is trained on a custom dataset derived from EmoSet-118K with valence and arousal annotations, which might not generalize well to natural emotional cues. It is also unclear whether the model would perform well on out-of-distribution (OOD) scenarios."}, "questions": {"value": "1. Sec4.1, which MLLM do you use for generating emotional prompts?\n2. Line228 and Line194, you use epsilon for both reward threshold and clipping value of GRPO.\n3. What is the batch size during RL training, and the number of epochs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WItINBSy3z", "forum": "xZq8iKnJLB", "replyto": "xZq8iKnJLB", "signatures": ["ICLR.cc/2026/Conference/Submission7588/Reviewer_pvHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7588/Reviewer_pvHG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596072379, "cdate": 1761596072379, "tmdate": 1762919674910, "mdate": 1762919674910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework for continuous emotional image generation (C-EICG). It tackles the limitations of existing methods, namely their lack of feedback on the generated image's actual emotion and their poor adaptability to user prompts. The core innovation is using a Large Vision-Language Model (LVLM) to provide two types of feedback:\nIn training, an LVLM evaluates the emotional (Valence-Arousal) values of generated images, providing a reward signal to fine-tune the generative model using reinforcement learning.\nIn Inference, the LVLM iteratively analyzes images and provides textual suggestions to refine the prompt, improving emotional fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core generation-understanding-feedback paradigm is a significant contribution. Unlike prior methods that use emotion as a one-way condition, this work creates a closed-loop system. Using an LVLM as both a reward model (for training) and a text-based optimizer (for inference) is a novel and powerful approach.\n\n2. The paper's claims are supported by a set of experiments. Figure 4 and the appendix figures (e.g., 9-11) compellingly illustrate the model's ability to generate smooth and coherent transitions as V-A values change, a key goal of C-EICG that general T2I models (FLUX, SD3.5-L) fail to achieve.\n\n3. The authors effectively demonstrate the necessity of both feedback mechanisms. Figure 5 provides a clear qualitative ablation of RF and TF , and Tables 3 & 4 validate the design choices for the emotion understanding model itself (e.g., model size, reward function, and multi-task training)."}, "weaknesses": {"value": "1. The primary weakness stems from the dataset construction. Both the textual prompts and the crucial V-A labels are synthetic. Prompts are MLLM-generated , and V-A values are sampled from Gaussian distributions derived from discrete emotion categories, not obtained from human annotators. This raises concerns about whether the model is truly aligned with human emotional perception or just with the biases of the synthetic data pipeline.\n\n2. High Inference Cost: The Self-Promotion Textual Feedback framework, while effective, appears to be computationally expensive. The appendix states it uses 3 iterations, generating 8 images per iteration. This iterative process, which requires multiple calls to a 7B LVLM for analysis and prompt optimization , would result in significant latency, making real-time applications difficult."}, "questions": {"value": "Given that the reward model was trained only on synthetically sampled V-A values, how do the authors expect it to perform on a dataset with genuine human V-A annotations? Is there a risk of domain mismatch, where the model has optimized for the dataset's statistical quirks rather than true emotional representation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B4Y2Q1EouF", "forum": "xZq8iKnJLB", "replyto": "xZq8iKnJLB", "signatures": ["ICLR.cc/2026/Conference/Submission7588/Reviewer_Q4TF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7588/Reviewer_Q4TF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908460124, "cdate": 1761908460124, "tmdate": 1762919674244, "mdate": 1762919674244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement-based framework for continuous emotional image generation. The method integrates a Large Vision–Language Model (LVLM) as both a reward model and a textual feedback generator, enabling a closed loop of generation, understanding, and feedback. Through emotion-aware reward optimization and iterative prompt refinement, the approach enhances emotional continuity and fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The generation–understanding–feedback paradigm effectively unites emotional reasoning with reinforcement learning in diffusion models.\n\n2. The textual feedback loop provides an intuitive and interpretable way to refine emotional prompts beyond fixed embeddings.\n\n3. The method achieves the best V-Error, A-Error, CLIP-Score, and user preference rate, validating both emotional fidelity and visual quality."}, "weaknesses": {"value": "1. The dataset construction strategy closely follows EmotiCrafter (Dang et al., 2025), which already employed an MLLM to generate neutral and emotional prompts as well as Valence–Arousal annotations, resulting in limited methodological novelty.\n\n2. The ground-truth Valence–Arousal annotations are sampled from Gaussian distributions per emotion class, which can introduce label noise and semantic misalignment between images and emotional values.\n\n3. The constructed dataset depends entirely on automatically generated prompts and lexicon-based statistical sampling without any human verification, raising concerns about annotation reliability and perceptual validity.\n\n4. The proposed framework heavily relies on the large vision–language model (LVLM) for both reward computation and textual feedback generation, making the entire pipeline sensitive to model bias and reasoning instability.\n\n5. The self-promotion textual feedback requires iterative LVLM reasoning over multiple generated samples, which substantially increases inference cost, yet the paper does not provide any discussion or analysis of computational efficiency or scalability."}, "questions": {"value": "1. Given that the ground-truth Valence–Arousal annotations are sampled from Gaussian distributions per emotion class, how do the authors ensure that such synthetic labels accurately reflect the emotional content of the images and do not introduce semantic noise?\n\n2. As the dataset relies entirely on automatically generated prompts and lexicon-based annotations without any human verification, have the authors conducted any manual inspection or validation to assess label reliability and perceptual consistency?\n\n3. The proposed framework depends heavily on the LVLM for both reward computation and textual feedback generation. How robust is the system to the biases or reasoning instability of the LVLM, and could similar performance be achieved with a smaller or less powerful model?\n\n4. The iterative textual feedback mechanism requires multi-image evaluation and textual generation at each step. Could the authors provide quantitative analysis on inference time, computational cost, or scalability to demonstrate its practical feasibility?\n\n5. In Figure 3, the results produced by EmoEdit and EmotiCrafter appear highly similar. Could the authors clarify whether this similarity arises from shared input prompts, overlapping training data, or something else?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tlV86EOLsR", "forum": "xZq8iKnJLB", "replyto": "xZq8iKnJLB", "signatures": ["ICLR.cc/2026/Conference/Submission7588/Reviewer_Zyi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7588/Reviewer_Zyi6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912347447, "cdate": 1761912347447, "tmdate": 1762919673853, "mdate": 1762919673853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a generation-understanding-feedback reinforcement paradigm for continuous emotional image generation (C-EICG). It leverages a fine-tuned Large Vision-Language Model (LVLM) to address key limitations of existing methods—lack of emotional feedback and insufficient adaptability of emotional prompts—by introducing an emotion-aware reward feedback strategy and a self-promotion textual feedback framework, aiming to enhance emotional continuity and fidelity while maintaining image quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel Paradigm Design**: The proposed \"generation-understanding-feedback\" reinforcement framework fills a gap in C-EICG by integrating emotional feedback loops. Unlike existing methods that ignore post-generation emotional evaluation, it uses LVLM’s reasoning ability to close the loop between generation and optimization, bringing a new perspective to emotional image generation.\n2. **Well-Integrated Multi-Module Collaboration**: The emotion understanding model (fine-tuned via GRPO), emotion-aware reward feedback, and self-promotion textual feedback are logically coordinated. Each module addresses a specific pain point, and their synergy ensures both emotional accuracy and content consistency, demonstrating a coherent design philosophy.\n3. **Practical Training-Free Inference Optimization**: The self-promotion textual feedback framework enables adaptive prompt refinement during inference without retraining the generative model. This design enhances usability, as it can be easily integrated with existing diffusion models (e.g., Stable Diffusion 3.5-Medium) without heavy parameter tuning.\n4. **Comprehensive Ablation Studies**: Ablation experiments on LVLM size, reward function design, and single/multi-task training effectively validate the necessity of key design choices. These studies clarify the contribution of each component, strengthening the credibility of the proposed method.\n5. **Rich Evaluation Dimensions**: Beyond standard quantitative metrics (emotional accuracy, text-image alignment), the paper includes qualitative comparisons and user studies. This multi-faceted evaluation better reflects the method’s performance in real-world scenarios, aligning with the subjective nature of emotional perception."}, "weaknesses": {"value": "1. **Limited Discussion on LVLM’s Emotional Evaluation Mechanism**: The paper does not deeply explain how the LVLM (Qwen2.5-VL-7B-Instruct) specifically interprets visual content to assess emotions. The lack of analysis on which visual cues (e.g., color, composition) the LVLM prioritizes makes it hard to understand the mechanistic advantage of using LVLM for emotional feedback.\n2. **Insufficient Generalization Validation**: The experiments are primarily conducted on a custom dataset derived from EmoSet-118K. There is no validation on other public C-EICG datasets or cross-domain scenarios (e.g., different image styles, complex scenes), raising questions about the method’s generalizability.\n3. **Vague Explanation of Reward Hacking Mitigation**: While the paper mentions using PickScore to avoid content distortion caused by overfitting to emotional cues, it does not detail how PickScore is integrated with the reward function or why it is more effective than other human-preference metrics. This makes the mitigation strategy less transparent.\n4. **Lack of Comparison with LVLM-Based Alternatives**: With the rise of LVLM-driven generation optimization, the paper does not compare EmoFeedback2 with other LVLM-aided C-EICG methods (if any) or analyze its unique advantages over general LVLM-based feedback frameworks, weakening the demonstration of its competitiveness."}, "questions": {"value": "Please refer to the detailed points I raised in the \"Weakness\" section and respond to each numbered item in your rebuttal with clarifications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PpqHNqhcrZ", "forum": "xZq8iKnJLB", "replyto": "xZq8iKnJLB", "signatures": ["ICLR.cc/2026/Conference/Submission7588/Reviewer_jgo1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7588/Reviewer_jgo1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020256412, "cdate": 1762020256412, "tmdate": 1762919673463, "mdate": 1762919673463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}