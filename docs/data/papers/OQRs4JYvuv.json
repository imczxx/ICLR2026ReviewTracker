{"id": "OQRs4JYvuv", "number": 9962, "cdate": 1758152896449, "mdate": 1759897682908, "content": {"title": "Accelerating Model-Based Reinforcement Learning Using Equivariance", "abstract": "Model-based reinforcement learning (MBRL) is a promising approach for learning effective policies in a data-efficient manner by using learned dynamics models to generate synthetic rollouts for actor-critic trianing, thereby reducing the reliance on costly environment interactions. However, when the learned dynamics model is inaccurate, these synthetic rollouts can introduce bias and deteriorate performance. Fortunately, many domains exhibit symmetries that can serve as powerful inductive biases, enabling the learned models to generalize beyond their training data. In this work, we exploit these inherent symmetries in MBRL and formally define equivariant MBRL for POMDPs. Building on this formulation, we introduce EquiDreamer, a framework that integrates symmetry into both world modeling and policy learning through an equivariant latent dynamics architecture. Experiments on visual continuous control tasks demonstrate that our equivariant MBRL method outperforms both model-based and model-free baselines, achieving strong results with substantially fewer environment interactions.", "tldr": "We combine model-based reinforcement learning with equivariant neural networks to achieve better generalization and solving tasks with fewer environment interactions.", "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "World models", "Equivariance"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4902eadd8d49e2b74c51a029f2784dce1bfaa43b.pdf", "supplementary_material": "/attachment/8d33d1becd13da087566e07211e0ca49b02cf3b7.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies equivariance in model-based reinforcement learning under POMDPs. Its main innovation is an equivariant Recurrent State Space model that reconstructs equivariant feature embeddings, rather than raw images, from a pretrained encoder using frame averaging. The method is evaluated on four tasks from the DeepMind Control Suite and compared against DreamerV3 and DrQ-v2."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The problem is well-motivated; leveraging environment symmetries can meaningfully improve learning efficiency.\n* The approach of reconstructing equivariant feature embeddings instead of raw observations is conceptually interesting."}, "weaknesses": {"value": "1. The method assumes prior knowledge of the symmetry groups. It would be valuable to explore whether the approach could be extended to learn these symmetries automatically, as in [2,3].\n\n2. Additionally, the method cannot handle approximate symmetries, and this limitation is not discussed. Addressing both these points could significantly strengthen the contribution and impact of the work.\n\n3. I identified potential issues with Proposition 1 and its proof that questions soundness of the paper: Proposition 1 is derived from Equation (3), which in turn is taken from [1], an unpublished paper. Given the central role of this equation, the authors should better justify its validity rather than assuming it holds, especially since [1] has not undergone peer review and appears to be an informal note.\n\n4. The proof relies on two strong and arguably unrealistic assumptions: (1) the dynamics model perfectly approximates the true transition dynamics, and (2) the symmetries are exact and perfectly generalizable. These assumptions are almost impossible to hold in neural network-based models and, importantly, prevent the method from being applicable to environments with approximate symmetries.\n\n5. Results are reported on only four DMC environments out of more than twenty available. The selected tasks are all 2D with small action spaces, despite the method being motivated as generally applicable to 3D symmetries. This raises questions about scalability to more complex environments.\n\n6. The reported baseline results, particularly for DrQ-v2, are inconsistent with the original paper, where the method performs much better. Here, DrQ-v2 fails to learn in nearly all environments, suggesting potential issues with implementation or hyperparameter choices. If different settings were used, the authors should clarify this. In any case, the authors need to provide a proper representation of the baseline, particularly if the baseline is well-established in the community and has a high reproducibility.\n\n7. The paper is missing various baselines from equivariant representation learning algorithms. The method is only compared against Dreamer and DRQ-v2. At least one baseline from each of the following families should be included:\n    * Model-free equivariant methods: Deep homomorphic policy gradient [2], or EQR [3]. \n    * Model-based equivariant methods:  EDGI [4], equivariant MuZero [5], SEN [7], or [8].\n\n8. The contributions appear incremental compared to prior work on equivariant model-based RL. The paper does not cite or compare against some key methods in this area, such as [4] and [5].\n\nGiven the above methodological, theoretical, and empirical limitations, as well as concerns regarding the validity of the main proposition, I do not believe the paper meets the standard of this venue. Nonetheless, this is a promising research direction, and I encourage the authors to address these issues in future revisions.\n\n### References\n[1] Jiang, Nan. \"A note on loss functions and error compounding in model-based reinforcement learning.\" arXiv preprint arXiv:2404.09946 (2024).\n\n[2] Rezaei-Shoshtari, Sahand, et al. \"Continuous mdp homomorphisms and homomorphic policy gradient.\" Advances in Neural Information Processing Systems 35 (2022): 20189-20204.\n\n[3] Mondal, Arnab Kumar, et al. \"Eqr: Equivariant representations for data-efficient reinforcement learning.\" International Conference on Machine Learning. PMLR, 2022.\n\n[4] Brehmer, Johann, et al. \"Edgi: Equivariant diffusion for planning with embodied agents.\" Advances in Neural Information Processing Systems 36 (2023): 63818-63834.\n\n[5] Deac, Andreea, Th√©ophane Weber, and George Papamakarios. \"Equivariant MuZero.\" arXiv preprint arXiv:2302.04798 (2023).\n\n[6] Park, Jung Yeon, et al. \"Learning Symmetric Embeddings for Equivariant World Models.\" International Conference on Machine Learning. 2022.\n\n[7] Zhao, Linfeng, et al. \"Equivariant action sampling for reinforcement learning and planning.\" arXiv preprint arXiv:2412.12237 (2024)."}, "questions": {"value": "1. How are you imposing the value function to be invariant?\n2. How does the method work with approximate symmetries in the environment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lmvHeWiIAn", "forum": "OQRs4JYvuv", "replyto": "OQRs4JYvuv", "signatures": ["ICLR.cc/2026/Conference/Submission9962/Reviewer_bvVt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9962/Reviewer_bvVt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761599281874, "cdate": 1761599281874, "tmdate": 1762921406997, "mdate": 1762921406997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EQUIDREAMER, a model-based framework that incorporates equivariance into the latent dynamics model and policy learning for POMDPs. The method uses symmetries in the environment to improve sample efficiency and generalization, building on the Dreamer architecture by replacing image reconstruction with feature reconstruction and using equivariant neural networks. Experiments on visual control tasks demonstrate improved performance over DreamerV3 and DrQ-v2 baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Integrating equivariance principles into a model-based RL framework is an interesting research area, as well as revealing its sample efficiency in many problems.\n2. Experimental results show consistent improvements in sample efficiency on several continuous control tasks."}, "weaknesses": {"value": "1. The overall novelty is limited, as the core idea of applying equivariance to RL has been explored in prior work (e.g, [1][2]), and the extension to model-based RL, while sensible, does not constitute a significant conceptual advance.\n\n2. The claimed benefits of feature reconstruction over image reconstruction are not sufficiently analyzed from the equivariance component, making it difficult to attribute the gains to the proposed novelty.\n\n3. The model structure is confusing, particularly the use of a unified parameterization $p_{\\theta}$ for the transition, observation, and reward models. This convolutes the roles of distinct components and lacks a clear justification.\n\nminor typos:\n1. In line 149, it seems like the equation form is incorrect.\n2. In line 151, it should be \"In this paper,...\"\n\n[1] Mondal, A. K., Jain, V., Siddiqi, K., & Ravanbakhsh, S. (2022, June). Eqr: Equivariant representations for data-efficient reinforcement learning. In International Conference on Machine Learning (pp. 15908-15926). PMLR.\n\n[2] Grimm, C., Barreto, A., Singh, S., & Silver, D. (2020). The value equivalence principle for model-based reinforcement learning. Advances in neural information processing systems, 33, 5541-5552."}, "questions": {"value": "1. How does the method scale to more complex symmetry groups or real-world tasks where symmetries merely exist?\n2. Were there any environments or symmetry conditions where the equivariant model failed to improve upon the baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G6WtuObGXy", "forum": "OQRs4JYvuv", "replyto": "OQRs4JYvuv", "signatures": ["ICLR.cc/2026/Conference/Submission9962/Reviewer_i4Nz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9962/Reviewer_i4Nz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905677902, "cdate": 1761905677902, "tmdate": 1762921406740, "mdate": 1762921406740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a equivariant MBRL method to capture symmetries in specific POMDP domains. It helps with generalization to unseen equivariant states during training thus improving sample efficiency and meanwhile incorporates physical information. The proposed equivariant model-based RL method, EquiDreamer, shows higher sample efficiency compared to DreamerV3, demonstrating the effectiveness of method proposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It's an important research subject to discover symmetric patterns during training to reduce abundant exploration and boost sample efficiency.\n\n2. The effectiveness of components of method proposed is clearly supported and discussed with empirical evidence.  The idea of reconstructing in the feature space instead of the original pixel space shows advantage on complex tasks like reacher-hard."}, "weaknesses": {"value": "1. Visualization of the symmetries discovered could be presented to help understanding the method proposed. It remains somehow vague that whether EquiDreamer actually captured the equivariance between distinct state.\n\n2. There are only empirical results on five tasks in DMC, which weakens the evidence for the effectiveness and generalizability of the method proposed. Results in other continuous control domains like Robodesk, Meta-world which also seem to have inherent symmetries would be preferred."}, "questions": {"value": "1. Despite DMC, can you list more benchmarks or domains that inherit the equivariant feature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k2rTbKEeUq", "forum": "OQRs4JYvuv", "replyto": "OQRs4JYvuv", "signatures": ["ICLR.cc/2026/Conference/Submission9962/Reviewer_vinT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9962/Reviewer_vinT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933632645, "cdate": 1761933632645, "tmdate": 1762921406409, "mdate": 1762921406409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}