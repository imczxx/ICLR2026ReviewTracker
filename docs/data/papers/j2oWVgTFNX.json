{"id": "j2oWVgTFNX", "number": 5415, "cdate": 1757908057781, "mdate": 1759897976874, "content": {"title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference", "abstract": "Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences (TD) for training-time reinforcement learning and inference-time verification. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-$N$ (up to 6.6\\%) and tree-search (up to 23.7\\%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL --- achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain --- and yield higher-quality language model policies in 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). All code is available at https://anonymous.4open.science/r/TDRM-CDD6.", "tldr": "", "keywords": ["Language Models; Reward Models; Temporal Difference; LLM RL; LLM Inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3f4c83ed45e1f0868af22f435dffd65462de657.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes TDRM, an algorithm for training a process reward model (PRM) using n-step TD targets. The authors propose linearly combining the PRM’s score with a verifiable, rule-based reward during online RL. The claimed benefits are smoother, temporally consistent rewards; better inference-time computation results; and improved data efficiency in RL across several models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose a training scheme similar to training a value model in traditional RL using an n-step TD method. The idea is interesting. The empirical results show consistent gains in test-time computation and data efficiency for RL. Inference-scaling results compare many different models, which show its robustness across different LLMs."}, "weaknesses": {"value": "My biggest concern is that mixing the process reward models with the value models. My understanding is after the n-step TD training, the PRM is essential giving the value $V_t$ for the current state $s_t$, which is the potential of rewards for the current state. The potential is associated with the policy $\\hat \\pi$ that generated the training dataset. So the value is the potential of the policy $\\hat \\pi$, but when it is used to train a different policy $\\pi$, there is a distribution mismatch. In paper \"The Lessons of Developing Process Reward Models\nin Mathematical Reasoning\", they explicitly against not distinguishing PRMs from value models and also using soft labels. I think this is against the authors' claim. Although I understand that the focus of that paper is against using MC to train PRM, which the TDRM is not. \n\nMinor places:\n1. The writhing need to be strengthen. For example. In section 2.1, they say \"In our context, the state space corresponds to every possible token sequence generated so far, whereas the **action space comprises all possible tokens that can be selected next**\", then a few sentence away they say \"In our work, an action is defined as newly generated **sentence**\". Then in Figure 2 caption, a step is separated by double new lines. The inconsistency makes the text confusing.\n2. I found it's hard to find the definition of ScalarPRM and then I found it in appendix. It would be helpful to refer the section in appendix when first introduce ScalarPRM."}, "questions": {"value": "1. The training of PRM is using the data from the RLHFlow/Mistral-PRM-Data and using the reward function and n-step TD to compute the target, correct? \n2. Regarding the value model concern in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GgdPQggH1j", "forum": "j2oWVgTFNX", "replyto": "j2oWVgTFNX", "signatures": ["ICLR.cc/2026/Conference/Submission5415/Reviewer_2Vkf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5415/Reviewer_2Vkf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849471129, "cdate": 1761849471129, "tmdate": 1762918048694, "mdate": 1762918048694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method, TDRM, in the context of RLVR. TDRM uses n-step TD learning to construct a smoother and more reliable progress reward model (PRM) that accelerates GRPO. In particular, TDRM leverages cosine reward heuristics to assign initial rewards to the reasoning steps that lead to correct outcome, then uses n-step TD with cross-entropy loss to learn a value function on top of these heuristic rewards. Then, it uses a linear combination of the learned value function and the verifiable reward (ground truth reward) as the final PRM that is used in GRPO training. Empirically, the proposed method achieves 2-4% improvement in success rate across different base models (e.g., Qwen2.5-0.5/1.5B, GLM-Z1-9B, Qwe2.5-Math-1.5/7B, DS-R1-Distill-Qwwen-1.5/7B) on a range of math benchmarks (e.g., MATH500, Minerva Math, Olympaid Bench, AIME24, AMC23)."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The empirical evaluations are comprehensive. I especially appreciate the fact that the evaluations are done across many base models across many tasks. \n\n- While TD-learning has been explored extensive in reinforcement learning, using TD for learning PRM is a relatively new idea that has not been studied extensive as far as I am aware of. Harnessing the power of TD-learning in LLM RL is a challenging yet promising research direction, and the proposed method advances in this direction."}, "weaknesses": {"value": "- No discussion of what ScalarPRM is despite it being one of the core comparisons in the experiments. The paper mentions that \"TDRM exhibits lower mean and variance of TD errors than ScalarPRM\". This suggests that ScalarPRM also does TD-based learning for reward modeling. It is crucial to properly discuss ScalarPRM and how it is different from the proposed method.\n\n- Weak empirical results: From the empirical results, PRM generated by n-step TD performs worse than PRM generated by 1-step TD. Since n-step TD is one of the key components of the proposed approach, it begs the question whether the proposed method is actually better than prior approaches (e.g., the ones that use standard 1-step TD).\n\n- The writing of the paper can be improved. There are many typos and missing citations. To name a few:\n  - L51: \"Unlike prior approaches where TD was used to construct offline datasets\" -- missing citation.\n  - L98: \"$\\mathcal{S} \\times \\mathcal{A} \\to r, r \\in [0, 1]$\" -- the range of the function needs to be a set and not an element in the set.\n  - L138: $r_{i, t}(\\theta)$ does not appear in the equation above as the ratio is written out fully already.\n  - L195: \"double newline delimiter\" -- what does it mean?\n  - L162: \"ScalarPRM\" -- what is ScalarPRM? \n  - L237: \"..., and set the step size as 1\" -- what does the step size here mean? \n  - L293: $\\mathrm{PRM}_\\phi$ is not defined."}, "questions": {"value": "(1) In the algorithm box (Algorithm 1), L8, why is $s_{T-1}$ being used instead of $s_{T}$? \n\n(2) What are the baselines in Figure 1 (c)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Zd8ZdyrkXC", "forum": "j2oWVgTFNX", "replyto": "j2oWVgTFNX", "signatures": ["ICLR.cc/2026/Conference/Submission5415/Reviewer_1q9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5415/Reviewer_1q9o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869092646, "cdate": 1761869092646, "tmdate": 1762918048433, "mdate": 1762918048433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TDRM, which trains PRMs for LLMs via n-step TD learning. The rewards from the PRM are combined with verifiable rewards for GRPO training. The main results are improved performance under 2.5k samples compared to conventional PRMs or only using outcome rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses instability and inconsistency in PRMs\n- Empirical results show improved data efficiency across a number of benchmarks and models, as well as better inference-time verification"}, "weaknesses": {"value": "- I may have misunderstood something, but how is this fundamentally different from actor-critic methods that learn value functions via TD learning? It seems like the method is functionally very similar to classical value functions but it has been repackaged as a “reward model”. I understand there are some small technical differences, e.g., the “reward model” is learned offline before RL training, whereas critics are usually learned online, but the conceptual gap seems narrow. I would appreciate a conceptual description of the differences.\n- Related to my above point, there are a number of missing baselines: traditional PPO, PPO with a frozen critic, etc. \n- It’s unclear why the RL experiments only report results in the data-constrained 2.5k-sample setting. Showing results with the full training datasets would clarify whether the advantages persist when data is not the limiting factor. Do the improvements hold with larger-scale training, or is there a tradeoff where the method mainly helps in low-data regimes but does not perform well with more data?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fqjx8drtOI", "forum": "j2oWVgTFNX", "replyto": "j2oWVgTFNX", "signatures": ["ICLR.cc/2026/Conference/Submission5415/Reviewer_G1ng"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5415/Reviewer_G1ng"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870349687, "cdate": 1761870349687, "tmdate": 1762918048180, "mdate": 1762918048180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes to learn process reward model method by TD learning based on step-wise rewards. Through empirical analysis in both test-time inference and RL training, covering 8 models from 3 model families on 5 mathematical datasets show the performance improvement of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written. The details of the notations and technique are clearly presented.\n\n- Section 3.1 carefully presents smoothness in rewards (Section 3.1). This introduction presents the motivation for the proposed method.\n\n- The performance evaluation that covers two problem settings, 8 models, and 5 datasets is solid."}, "weaknesses": {"value": "- The discussion of why TD learning learns smoother rewards is missing.\n\n- My major concern is how the reward smoothness correlates with the reward performance. If smoothness is the key, simple reward smoothing methods as baselines are missing in the experiment. If the temporal consistency is the key to the reward performance, a discussion is needed.\n\n- The proposed method depends on n-step TD, TD $\\lambda$, and they introduce hyperparameters that require careful tuning to achieve the highest score (Section 4.3)"}, "questions": {"value": "- What dataset is evaluated in Figure 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S7VAy6A84t", "forum": "j2oWVgTFNX", "replyto": "j2oWVgTFNX", "signatures": ["ICLR.cc/2026/Conference/Submission5415/Reviewer_5dZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5415/Reviewer_5dZn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762086855271, "cdate": 1762086855271, "tmdate": 1762918047917, "mdate": 1762918047917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}