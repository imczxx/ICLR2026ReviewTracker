{"id": "obfPBQ9yFW", "number": 12254, "cdate": 1758206630649, "mdate": 1759897522411, "content": {"title": "ContrastGen: A Multi-agent Contrastive Framework for Hard Retrieval Data Generation", "abstract": "The embedding model vectorizes queries and passages separately and uses the distance between the two resulting vectors as the basis for retrieval matching. It serves as a core component in retrieval tasks. However, since training datasets often consist predominantly of simple queries, the embedding model is usually unable to develop the capability to handle complex, hard queries. This leads to a serious performance bottleneck and an upper limit on its effectiveness. To address the challenge of handling hard queries, existing methods propose new training strategies tailored for embedding models or simplification mechanisms during the query inference phase. In contrast and orthogonal to these approaches, this paper focuses on tackling the problem from the data level, aiming to improve the performance of the embedding model by generating high-quality hard query training data. More specifically, inspired by the ability of agents to closely simulate human behavior, and with the goal of generating queries that retain semantics and logical knowledge similar to those of human-generated queries, this paper proposes a multi-agent framework to generate hard queries, thereby enhancing the training performance of the embedding model. The core idea involves first using a generation agent to create new queries, followed by specialized agents—such as those focused on logical reasoning and semantic understanding—to filter and identify truly hard queries. Experimental results on different embedding models and datasets demonstrate that our method outperforms existing approaches.", "tldr": "", "keywords": ["Agent", "Collaborative Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1812b2778c449825bbc821be7c0886e59265d9be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors employ a multi-agent framework for hard query generation and use this for contrastive learning for retrieval tasks. The multi-agent framework consists of two steps - a consenus between a code agent and a CoT agent step , a multi-agent group discussion phase (if the first did not lead to a consensus). Theire results show their model improves upon other approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important problem. \n2. Their agentic solution seems intuitive and makes sense for applications. \n3. The need for hard queries and the challenges in building them is present both in research and industry and particularly important for domain heavy topics like medicine, science, engineering, telecom etc.\n4. their results are numerically superior."}, "weaknesses": {"value": "The authors measure the performance of their model by precision,recall, NDCG. But the core aspect to be measured is are the queries harder ? Where is this being measured? For example in Table 1, the original data has a lower precision, recall than the model - how do we know that the questions are harder? One possible option is measure on generated queries on pre-trained models and compare with provided data - harder queries should lead to poorer performance. \n\nOverall the paper is interesting but it is hard to understand if the benefits are actually significant or statistical noise and how much of it have the queries improved (become harder)?\n\nSpecific comments below\n\n1. What models are used in the different agents. How does the agent choice (size, model family) affect the results has not been analyzed. \n2. Numbers in tables need statisical significance testing - is the ContrastGen model statistically better than the second best model? In table 1 for example most improvements are less than 1 percent - this needs statisical validation.\n3. Figure 3 exagerrates the differences and no statisical testing is done. For example on top row left (recall@10) all numbers are 52.xx% but the image tends to give an impression of much higher variation. Without statistically testing how many are equal. Authors can report test results from say ANOVA or pairwise t-tests. \n4. In table 4, what does the 0 label mean? Do the datasets map \"6 quart crackpot\" to the text? More likely this is drawn from a negative. if so, is it a negative with high cosine similarity. Without clarity on these aspects it is hard to understand how to interpret this especially the label 0 scenario. \n5. Results from ablation study also inform little without statisical testing. \n6. The problem of generating hard queries is an important one both in research and in practice (industry). However, what is not clear how this can be extended to domain heavy scenarios. Also are the costs of generating a query of 102k tokens on average prohibhitive for large dataset generations? \n7. The datasets are public datasets and hence the LLMs would have seen them. An analysis on propietary data would be interesting here if possible (or future work)\n\n\nMinor comments:-\n\n1. Lines 91-104 are largely repetitive of content from Section 1. \n2. What is $\\mathbf{w}$ in $E(.;\\mathbf{w})$ is not very clear. Embedding models do not generally have any parameters other than dimension?\n3. Typo in table 1 - should be BGE-M4 not BEG-M3."}, "questions": {"value": "Please respond to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W4cMGkIx1H", "forum": "obfPBQ9yFW", "replyto": "obfPBQ9yFW", "signatures": ["ICLR.cc/2026/Conference/Submission12254/Reviewer_dtZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12254/Reviewer_dtZN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767879047, "cdate": 1761767879047, "tmdate": 1762923195497, "mdate": 1762923195497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper ContrastGen: A Multi-Agent Contrastive Framework for Hard Retrieval Data Generation introduces a multi-agent approach to generate challenging query–passage pairs for improving embedding-based retrieval models. The framework has two stages: first, a query generation agent rewrites queries while two contrasting evaluators, the Code Agent using rule-based logic and the CoT Agent using semantic reasoning, assess their match with the passage. When the two agents disagree, the sample is identified as a hard example. These hard cases are then refined through a multi-agent group discussion, where several expert agents debate and vote to decide the final label. This process produces training data that captures nuanced reasoning and complex semantics, enhancing the model’s ability to distinguish relevant from irrelevant passages. Experiments on Shopping Queries, arXiv, and MS MARCO datasets show consistent improvements in retrieval metrics, and ablation studies confirm that both the dual-agent contrast and the discussion mechanism are essential. The main contribution is reframing retrieval data generation as a multi-agent contrastive process that uses agent disagreement and collaboration to create high-quality hard samples that improve model robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) Instead of following the conventional path of improving model architectures, the authors take a data-centric approach by generating high-quality hard samples to enhance the discriminative capability of retrieval models.\n\n2) The idea of defining sample difficulty through the disagreement between a rule-based Code Agent and a reasoning-based CoT Agent is both intuitive and interesting.\n\n3) The authors validate the framework across multiple public datasets and diverse embedding models, consistently showing performance gains."}, "weaknesses": {"value": "1) The system’s performance is tightly coupled to specific prompts, role descriptions, and reasoning styles. The lack of a systematic sensitivity study leaves open whether the method would remain stable under different model versions, prompting templates, or domain shifts. Besides, the final label aggregation relies on an ad-hoc greedy rule that mixes majority votes with the first CoT decision. This design lacks theoretical grounding and could bias results toward early, possibly noisy agent judgments.\n\n2) The observed performance peak at a moderate amount of generated data is not theoretically or empirically explained. A deeper analysis of data diversity, redundancy, or quality filtering would clarify why excessive generation degrades results.\n\n3) The reported improvements lack confidence intervals or significance tests, and the authors do not provide the code, making it difficult to assess robustness or reproducibility."}, "questions": {"value": "Q1: How does the proposed pipeline scale when generating millions of samples?\n\nQ2: What is the average time and cost per generated example, and how does that compare to human annotation or standard augmentation methods?\n\nQ3: Did you test other aggregation mechanisms, such as weighted confidence or probabilistic fusion? And how do you handle ties or inconsistent reasoning paths across discussion rounds?\n\nQ4: What are the most common failure modes for the Code Agent and CoT Agent?\n\nQ5: Did you observe any systematic biases in the group discussion outcomes (e.g., majority bias or echoing effects)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K0becA1xTp", "forum": "obfPBQ9yFW", "replyto": "obfPBQ9yFW", "signatures": ["ICLR.cc/2026/Conference/Submission12254/Reviewer_HeEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12254/Reviewer_HeEt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973987100, "cdate": 1761973987100, "tmdate": 1762923194725, "mdate": 1762923194725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is well-motivated, presenting a novel perspective for improving retrievers by tackling hard tasks. We are convinced by the sufficient baselines used to verify the proposed method and believe it inspires future work on more challenging tasks. I have several minor concerns, including potential unreliability in using LLMs for relevance evaluation, the lack of a simpler majority vote baseline, prohibitive computational costs, and some lack of clarity in the agent selection process. We further seek clarification on case analyses for hard samples, the potential benefit of incorporating more specialized agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well-motivated. It tries to solve hard tasks in a new perspective of view, improving retriever. Sufficient baseline retrievers adopted to verify the proposed method to improve the retriever. This work inspires the future direction on solving more challenging tasks like BrowseComp."}, "weaknesses": {"value": "1. Previous work has shown that asking LLMs themselves to evaluate the relevance of queries and documents are not that reliable [[1]](https://arxiv.org/abs/2505.21870). Applying Code Agent and CoT Agent is also within this paradigm. It would be better if there are experiments conducted to verify the relevance between inconsistency (among Code and CoT Agent) and query difficulty.\n2. The initial CoT agent's decision ($y^{g}_0$) is only overturned if all L discussion groups unanimously disagree. Thus, it seems that a simpler majority vote is a more standard and intuitive baseline, but no comparison is offered.\n3. Prohibitive computational cost makes the approach infeasible for generating datasets of any significant size. (1 hard example=~58 API calls and 100k tokens)\n4. Lack of Clarity on Agent Selector and Validator."}, "questions": {"value": "1. Line186 & 208. Inconsistency among Code Agent and CoT Agent indicates a hard sample. Could the author give any case analysis?\n2. Related to Weakness1, will it be more trustworthy by incorporating more Agents besides Code Agent and CoT Agent? E.g., agents that specialize searching. Or, the current setup is enough.\n3. Typo? Line243: $y_{i,k}^{t}$ and $y_{i,k}^{t}$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XNQqhVwc7L", "forum": "obfPBQ9yFW", "replyto": "obfPBQ9yFW", "signatures": ["ICLR.cc/2026/Conference/Submission12254/Reviewer_71BY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12254/Reviewer_71BY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051158084, "cdate": 1762051158084, "tmdate": 1763003011651, "mdate": 1763003011651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}