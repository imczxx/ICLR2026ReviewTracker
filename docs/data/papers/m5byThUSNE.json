{"id": "m5byThUSNE", "number": 23678, "cdate": 1758347056605, "mdate": 1763669347334, "content": {"title": "Optimistic Task Inference for Behavior Foundation Models", "abstract": "Behavior Foundation Models (BFMs) are capable of retrieving high-performing policy for any reward function specified directly at test-time, commonly referred to as zero-shot reinforcement learning (RL). While this is a very efficient process in terms of compute, it can be less so in terms of data: as a standard assumption, BFMs require computing rewards over a non-negligible inference dataset, assuming either access to a functional form of rewards, or significant labeling efforts. To alleviate these limitations, we tackle the problem of task inference purely through interaction with the environment at test-time. We propose OpTI-BFM, an optimistic decision criterion that directly models uncertainty over reward functions and guides BFMs in data collection for task inference. Formally, we provide a regret bound for well- trained BFMs through a direct connection to upper-confidence algorithms for linear bandits. Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and observe that it enables successor-features-based BFMs to identify and optimize an unseen reward function in a handful of episodes with minimal compute overhead.", "tldr": "We propose an algorithm for fast online task inference in behavior foundation models.", "keywords": ["Behavior Foundation Models", "Zero-Shot Reinforcement Learning", "Deep Reinforcement Learning", "Fast Adaptation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a476a14ebe53fba2f09f7e25bd303eba2f94734.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an ICRL (Interactive or In-Context Reinforcement Learning) algorithm based on Behavior Foundation Models (BFMs). The method performs online task inference by estimating the task embedding from observed rewards during inference, enabling dynamic policy adjustment. The authors provide partial theoretical analysis and show that the proposed method can achieve near-oracle performance within only a few episodes. Moreover, the approach can be extended to handle non-stationary reward settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is grounded on a solid theoretical foundation.\n\n2. It is innovative and computationally efficient.\n\n3. The approach is extensible and may inspire further research in this direction."}, "weaknesses": {"value": "1. The experimental section only compares against Oracle and LoLA; it should include comparisons with other ICRL methods and evaluate optimization speed under out-of-distribution (OOD) settings.\n\n2. In Appendix A5.3, line 1036, equation (84), the variable _x_ should likely be _ψ_.\n\n3. In Algorithm 1, the formula for updating the estimator should reference the corresponding equation number for clarity."}, "questions": {"value": "1. How does the proposed estimation procedure perform in sparse-reward settings (e.g., when the reward is only given at the end of an episode)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pQycvz9lpO", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Reviewer_pxZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Reviewer_pxZn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825661669, "cdate": 1761825661669, "tmdate": 1762942758766, "mdate": 1762942758766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We would like to thank all reviewers for providing a positive and detailed evaluation of our submission. We are happy that our contribution is deemed “important”, “timely” or “innovative” (afmy, eb95, pxZn), supported by “solid theoretical guarantees” (afmy, eb95, REKP, pxZn) and  “good empirical performance” (afmy, eb95). We would like to use this general response to address two specific comments, and summarize the changes in the revision.\n\n### Lifting assumptions\n\nOur method operates under the assumptions that successor features are well-trained (A1), and the reward is well-captured by the features (A2). We are happy to comment on what happens when either of these two assumptions does not hold.\n\nAs reviewers afmy and REKP point out, and we mention on line 239, the assumption that USF training was perfect (A1) will almost certainly be violated when using neural networks. \nEmpirically, we find that OpTI-BFM is robust to USF inaccuracies, at least to the degree that arises in standard benchmarks (i.e., DMC). We further investigate the degree of robustness of OpTI-BFM in controlled experiments in a new ablation in Appendix B.5, and find that very large perturbations are necessary to render the method uninformative.\nFormally, inaccuracies in the successor features results in an additional term in instantaneous regret (Equations 44, 98) in the worst case. We have added a formal study in Appendix A.5.\n\nAssumption A2, which posits that the reward function is exactly linear in the features $\\phi$, also does not hold in practice. Constructing a reward function with a strong orthogonal component to the feature basis is non-trivial: we have resorted to learning and scaling orthogonal components, producing the empirical evaluation in Appendix B.6, which studies performance degradation as the projection error increases. To complement this evaluation, we have also included a proxy empirical study, which, as suggested, injects noise to reward feedback (see Appendix B.7). Accordingly, we find that significant noise injection slows down convergence.\nFormally, misalignment between feature and rewards breaks the elliptical potential lemma (Theorem 1), and renders analysis more challenging. However, we may extend results from the literature on misspecified linear bandits: misspecification results in regret terms, whose growth can be related to the degree of misspecification. We provide a detailed analysis in Appendix A.5.\n\n### Per-step updates\n\nExtending guarantees to the (empirically stronger) step-level variant of OpTI-BFM is an exciting prospect, but unfortunately this introduces a few additional challenges. The current proof, for instance, relies on episodic resets to an initial state that is not influenced by the policy in order to invoke results from contextual linear bandit theory. To the best of our knowledge, step-level guarantees would likely need to be derived from existing results in reinforcement learning settings, and thus deal with a much more complex analysis.\nOne way to obtain such guarantees would be to introduce strong smoothness assumptions: intuitively, if the posterior evolves slowly, and the agent’s behavior is smooth with respect to its task embedding, its trajectories would not significantly vary from those induced by episode-level updates. While this may produce a regret guarantee, proving better performance with respect to the episode-level counterpart would remain challenging. Intuitively, faster policy updates can induce lower regret, as the agent may react faster to new information, but guaranteeing that the feedback is “as useful” is the main challenge.\nIn short, we agree that this missing guarantee represents an important direction for future work, and we have highlighted this as such at the end of the main text on line 478. We are happy to consider and discuss any suggestions that might simplify this analysis.\n\n### Changes\n\nWe would like to finally summarize the main topics of discussion, and corresponding updates to the revision (which we highlight in color).\n- We extended the range of considered environments to consider a sparse-reward navigation problem in Appendix B.4.\n- A new variant of OpTI-BFM relying on gradient-based optimization of the UCB objective is introduced and studied in Appendix B.3.\n- We investigated the method’s behavior under violation of the two main assumptions (accuracy of successor features and linearity of rewards), both formally (Appendix A.5), and in controlled ablations (Appendix B.5, B.6 and B.7).\n- We updated several parts of the main paper and appendix, following the reviewers’ suggestions.\n\nWe believe that these updates were instrumental in strengthening this work, and we would like to thank all reviewers for suggesting these extensions. We look forward to the upcoming discussion phase."}}, "id": "7rwhwOf2HO", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763669622804, "cdate": 1763669622804, "tmdate": 1763669622804, "mdate": 1763669622804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper adresses task inference for BFMs without relying on a labeled “task‑inference” dataset at test time. Instead of estimating a point task embedding offline, the agent actively interacts with the environment and updates a belief over reward parameters. The core idea is to view policy search over task embeddings as linear bandit optimization that comes from the insight that for USF-based BFMs, the relationship between successor features and returns is approximately linear. So the paper proposes OpTI‑BFM, which maintains a least‑squares estimate over the unknown reward weights, then selects the task embedding optimistically. It provides theoretical sublinear regret bounds $O(d\\sqrt(n))$ demonstrates empirically that the method can identify tasks within 5 episodes on DMC benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- One of the core original ideas is the new task‑space bandit formulation for BFMs. The paper formulates online task inference as linear bandit optimization in the task-embedding space: with well-trained USFs, the expected episode return is approximately linear in the successor features of the policy conditioned on a task embedding, i.e., $\\mathbb{E}[\\hat{G}_k \\mid s_0, \\pi_z] \\approx \\langle \\psi(s_0, z), z_r \\rangle$. This lets the agent choose $z$ using a rule over confidence sets on the unknown reward weights $z_r$ (Eqs.~(7)--(10), \\S3.2). The ``two-context'' twist (estimating with features $\\phi$ but acting with successor features $\\psi$) is new in the BFM literature and differentiates the method from standard LinUCB.\n- Another contribution shows that running least-squares on reward-level pairs $(\\phi, r)$ yields tighter confidence sets than regressing on episode-level returns and empirical SFs $(\\tilde{\\psi}, \\hat{G})$.\n- The paper has efficient implementation details that are non-trivial. For example: the optimizer is computationally light and table 1 shows that the machinery is deployable in real-time control. \n- The paper is mathematically sound and strong."}, "weaknesses": {"value": "- The theory assumes perfect USFs and that the policy conditioned on $z$ is (near) optimal for reward $z$ (A1), strictly linear rewards with sub-Gaussian noise (A2), and an optimization oracle for Eq.~(10) (A3) and are introduced before Algorithm1 on p.5. In practice, USFs are learned with function approximation and the acquisition is solved by random shooting. The paper notes ``we found OpTI-BFM to perform well even when [A1--A2] are violated'' but does not quantify robustness to misspecification. \n- Also the regret bounds are proven for a variant that only updates $z$ at episode starts (\\S3.3), while the recommended/practical algorithm updates $z$ every step, which is empirically much better (Fig.4). So the bound doeen't exactly cover the method actually used. (p.5; Fig.4 p.8)."}, "questions": {"value": "- Add controlled experiments that systematically break assumptions. For example inject bounded bias into $\\psi$ and report regret vs.\\ SF error or create rewards with a tunable component orthogonal to $\\phi$ or ablate sub-Gaussian noise level. Reporting regret/performance vs.\\ the projection error $\\|r - \\phi^\\top z\\|$ would make the empirical section align with A2/A1. \n\n- Provide either 1) a theoretical extension to per-step updates, or 2) a head-to-head comparison that keeps the episodic-only variant as the default and shows the practical gap in other settings too, with a candid discussion of why the bound should still be true."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Hf9a1RnIC", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Reviewer_REKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Reviewer_REKP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863431187, "cdate": 1761863431187, "tmdate": 1762942758546, "mdate": 1762942758546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle a key limitation of behavioural foundation models (BFMs) based on Universal Successor Functions (USFs): the need for a dataset of labeled (state, reward) pairs. For cases where such data may be impractical or prohibitively costly to acquire, an alternative framework based on actively collecting a smaller amount of data online during deployment is explored.\n\nTo navigate this new framework the authors propose OpTI-BFM(Optimistic Task Inference for Behavioural Foundation Models) which leverages the linear relationship between features and rewards to update its belief over the space of rewards (i.e., incrementally improves its estimate of the task embedding using observed rewards). OpTI-BFM maintains a confidence ellipsoid over possible task embeddings and selects actions optimistically to efficiently explore the reward space.\n\nLeveraging the fact that policy search for well-trained USF-based BFMs reduced to online optimisation of a linear function, a regret bound for OpTI-BFM in an episodic setting is established - the expected regret over n episodes $R_n\\leq \\mathcal{O}(d\\sqrt{n})$. OpTI-BFM is evaluated empirically using a common zero-shot RL benchmark (ExORL) consisting of Walker, Cheetah and Quadruped environments with four reward functions each. OpTI-BFM achieves oracle performance (upper bound) within five episodes (5k steps) on all tasks, outperforming LoLA and the “Random” (lower bound) baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The authors introduce a new framework for task inference in BFMs without labeled offline (state, rewards) data. In this framework, the relationship between BFM policy search and linear bandits is exploited to develop, and prove a regret bound for, the OpTI-BFM algorithm for online task inference. OpTI-BFM is timely and tackles the problematic requirement for labeled data with implications for many real world applications. The empirical results support the efficacy of OpTI-BFM in three standard zero-shot tasks (Walker, Cheetah, Quadruped), outperforming LoLA and reaching Oracle-level performance within 5 episodes."}, "weaknesses": {"value": "- Whilst the experiment section is quite strong already it could be improved further if the authors are able to show the performance of OpTI-BFM on an alternative environment to those of the DeepMind Control suite. e.g. an alternate task with pixel observations.\n\n- There is not much discussion of how OpTI-BFM could be deployed for real-world use. The authors say that their method would enable BFMs to work “beyond domains in which rewards are readily available”, but it is not obvious to me how it would interact with a real environment to get immediate reward labels. It would be appreciated if this could be explained further by the authors."}, "questions": {"value": "Could the authors comment on whether OpTI-BFM generalises beyond continuous control environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uHE7d7iM1z", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Reviewer_eb95"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Reviewer_eb95"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961770168, "cdate": 1761961770168, "tmdate": 1762942758346, "mdate": 1762942758346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OpTI-BFM, an optimistic decision criterion that directly models uncertainty over reward functions and guides BFMs in data collection for task inference. Authors frame this online task inference problem as a linear bandit problem and maintain a probabilistic belief (a confidence ellipsoid) over the true task embedding z_r by performing real-time least-squares regression on reward-level feedback. Experiments on zero-shot benchmarks show that OpTI-BFM matches or surpasses offline reward inference methods with much less data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Tackles a significant and practical bottleneck. Online task inference with only a few active-interaction episodes is important for real-world applications.\n  \n- Solid theoretical guarantees via connections to linear bandit algorithms.\n  \n- Good empirical performance with additional experiments and analysis, e.g. episode-level updates and non-stationary rewards."}, "weaknesses": {"value": "- The paper operates under assumptions of a perfect BFM / successor feature model. It is unclear what would happen for the theoretical guarantees or how the empirical results would change with approximation errors.\n  \n- The formal regret bound is proven for an episodic-update variant of OpTI-BFM. However, the experiments (Sec.5.3, Fig. 4) show that the step-update variant is empirically superior and converges much faster. While it is a positive result that the practical algorithm is even better, it means the theory doesn't formally cover the best-performing algorithm presented.\n  \n- The optimistic search for z is currently done by random shooting and may fail for larger, more complicated spaces."}, "questions": {"value": "- How is the method's robustness to misspecification? For example, how does OpTI-BFM perform if the true reward function r(s) has a significant non-linear component, or if the pre-trained BFM is of lower quality (violating A1)? How does its degradation compare to the offline \"Oracle\" regression, which would also suffer from this misspecification?\n  \n- How does the random shooting for UCB optimization scale? Have you explored the sensitivity to this number of samples? Would a gradient-based approach be more robust or scalable?\n  \n- Could the theoretical guarantees be extended to the step-level update case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nRxU354mKg", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Reviewer_afmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Reviewer_afmy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997679564, "cdate": 1761997679564, "tmdate": 1762942758016, "mdate": 1762942758016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}