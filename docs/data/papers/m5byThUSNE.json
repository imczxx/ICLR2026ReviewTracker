{"id": "m5byThUSNE", "number": 23678, "cdate": 1758347056605, "mdate": 1759896801862, "content": {"title": "Optimistic Task Inference for Behavior Foundation Models", "abstract": "Behavior Foundation Models (BFMs) are capable of retrieving high-performing policy for any reward function specified directly at test-time, commonly referred to as zero-shot reinforcement learning (RL). While this is a very efficient process in terms of compute, it can be less so in terms of data: as a standard assumption, BFMs require computing rewards over a non-negligible inference dataset, assuming either access to a functional form of rewards, or significant labeling efforts. To alleviate these limitations, we tackle the problem of task inference purely through interaction with the environment at test-time. We propose OpTI-BFM, an optimistic decision criterion that directly models uncertainty over reward functions and guides BFMs in data collection for task inference. Formally, we provide a regret bound for well- trained BFMs through a direct connection to upper-confidence algorithms for linear bandits. Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and observe that it enables successor-features-based BFMs to identify and optimize an unseen reward function in a handful of episodes with minimal compute overhead.", "tldr": "We propose an algorithm for fast online task inference in behavior foundation models.", "keywords": ["Behavior Foundation Models", "Zero-Shot Reinforcement Learning", "Deep Reinforcement Learning", "Fast Adaptation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bebde3624e939a6f56a4d90a9cc0f0c5bc1b2f5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an ICRL (Interactive or In-Context Reinforcement Learning) algorithm based on Behavior Foundation Models (BFMs). The method performs online task inference by estimating the task embedding from observed rewards during inference, enabling dynamic policy adjustment. The authors provide partial theoretical analysis and show that the proposed method can achieve near-oracle performance within only a few episodes. Moreover, the approach can be extended to handle non-stationary reward settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is grounded on a solid theoretical foundation.\n\n2. It is innovative and computationally efficient.\n\n3. The approach is extensible and may inspire further research in this direction."}, "weaknesses": {"value": "1. The experimental section only compares against Oracle and LoLA; it should include comparisons with other ICRL methods and evaluate optimization speed under out-of-distribution (OOD) settings.\n\n2. In Appendix A5.3, line 1036, equation (84), the variable _x_ should likely be _ψ_.\n\n3. In Algorithm 1, the formula for updating the estimator should reference the corresponding equation number for clarity."}, "questions": {"value": "1. How does the proposed estimation procedure perform in sparse-reward settings (e.g., when the reward is only given at the end of an episode)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pQycvz9lpO", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Reviewer_pxZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Reviewer_pxZn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825661669, "cdate": 1761825661669, "tmdate": 1762942758766, "mdate": 1762942758766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper adresses task inference for BFMs without relying on a labeled “task‑inference” dataset at test time. Instead of estimating a point task embedding offline, the agent actively interacts with the environment and updates a belief over reward parameters. The core idea is to view policy search over task embeddings as linear bandit optimization that comes from the insight that for USF-based BFMs, the relationship between successor features and returns is approximately linear. So the paper proposes OpTI‑BFM, which maintains a least‑squares estimate over the unknown reward weights, then selects the task embedding optimistically. It provides theoretical sublinear regret bounds $O(d\\sqrt(n))$ demonstrates empirically that the method can identify tasks within 5 episodes on DMC benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- One of the core original ideas is the new task‑space bandit formulation for BFMs. The paper formulates online task inference as linear bandit optimization in the task-embedding space: with well-trained USFs, the expected episode return is approximately linear in the successor features of the policy conditioned on a task embedding, i.e., $\\mathbb{E}[\\hat{G}_k \\mid s_0, \\pi_z] \\approx \\langle \\psi(s_0, z), z_r \\rangle$. This lets the agent choose $z$ using a rule over confidence sets on the unknown reward weights $z_r$ (Eqs.~(7)--(10), \\S3.2). The ``two-context'' twist (estimating with features $\\phi$ but acting with successor features $\\psi$) is new in the BFM literature and differentiates the method from standard LinUCB.\n- Another contribution shows that running least-squares on reward-level pairs $(\\phi, r)$ yields tighter confidence sets than regressing on episode-level returns and empirical SFs $(\\tilde{\\psi}, \\hat{G})$.\n- The paper has efficient implementation details that are non-trivial. For example: the optimizer is computationally light and table 1 shows that the machinery is deployable in real-time control. \n- The paper is mathematically sound and strong."}, "weaknesses": {"value": "- The theory assumes perfect USFs and that the policy conditioned on $z$ is (near) optimal for reward $z$ (A1), strictly linear rewards with sub-Gaussian noise (A2), and an optimization oracle for Eq.~(10) (A3) and are introduced before Algorithm1 on p.5. In practice, USFs are learned with function approximation and the acquisition is solved by random shooting. The paper notes ``we found OpTI-BFM to perform well even when [A1--A2] are violated'' but does not quantify robustness to misspecification. \n- Also the regret bounds are proven for a variant that only updates $z$ at episode starts (\\S3.3), while the recommended/practical algorithm updates $z$ every step, which is empirically much better (Fig.4). So the bound doeen't exactly cover the method actually used. (p.5; Fig.4 p.8)."}, "questions": {"value": "- Add controlled experiments that systematically break assumptions. For example inject bounded bias into $\\psi$ and report regret vs.\\ SF error or create rewards with a tunable component orthogonal to $\\phi$ or ablate sub-Gaussian noise level. Reporting regret/performance vs.\\ the projection error $\\|r - \\phi^\\top z\\|$ would make the empirical section align with A2/A1. \n\n- Provide either 1) a theoretical extension to per-step updates, or 2) a head-to-head comparison that keeps the episodic-only variant as the default and shows the practical gap in other settings too, with a candid discussion of why the bound should still be true."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Hf9a1RnIC", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Reviewer_REKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Reviewer_REKP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863431187, "cdate": 1761863431187, "tmdate": 1762942758546, "mdate": 1762942758546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle a key limitation of behavioural foundation models (BFMs) based on Universal Successor Functions (USFs): the need for a dataset of labeled (state, reward) pairs. For cases where such data may be impractical or prohibitively costly to acquire, an alternative framework based on actively collecting a smaller amount of data online during deployment is explored.\n\nTo navigate this new framework the authors propose OpTI-BFM(Optimistic Task Inference for Behavioural Foundation Models) which leverages the linear relationship between features and rewards to update its belief over the space of rewards (i.e., incrementally improves its estimate of the task embedding using observed rewards). OpTI-BFM maintains a confidence ellipsoid over possible task embeddings and selects actions optimistically to efficiently explore the reward space.\n\nLeveraging the fact that policy search for well-trained USF-based BFMs reduced to online optimisation of a linear function, a regret bound for OpTI-BFM in an episodic setting is established - the expected regret over n episodes $R_n\\leq \\mathcal{O}(d\\sqrt{n})$. OpTI-BFM is evaluated empirically using a common zero-shot RL benchmark (ExORL) consisting of Walker, Cheetah and Quadruped environments with four reward functions each. OpTI-BFM achieves oracle performance (upper bound) within five episodes (5k steps) on all tasks, outperforming LoLA and the “Random” (lower bound) baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The authors introduce a new framework for task inference in BFMs without labeled offline (state, rewards) data. In this framework, the relationship between BFM policy search and linear bandits is exploited to develop, and prove a regret bound for, the OpTI-BFM algorithm for online task inference. OpTI-BFM is timely and tackles the problematic requirement for labeled data with implications for many real world applications. The empirical results support the efficacy of OpTI-BFM in three standard zero-shot tasks (Walker, Cheetah, Quadruped), outperforming LoLA and reaching Oracle-level performance within 5 episodes."}, "weaknesses": {"value": "- Whilst the experiment section is quite strong already it could be improved further if the authors are able to show the performance of OpTI-BFM on an alternative environment to those of the DeepMind Control suite. e.g. an alternate task with pixel observations.\n\n- There is not much discussion of how OpTI-BFM could be deployed for real-world use. The authors say that their method would enable BFMs to work “beyond domains in which rewards are readily available”, but it is not obvious to me how it would interact with a real environment to get immediate reward labels. It would be appreciated if this could be explained further by the authors."}, "questions": {"value": "Could the authors comment on whether OpTI-BFM generalises beyond continuous control environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uHE7d7iM1z", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Reviewer_eb95"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Reviewer_eb95"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961770168, "cdate": 1761961770168, "tmdate": 1762942758346, "mdate": 1762942758346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OpTI-BFM, an optimistic decision criterion that directly models uncertainty over reward functions and guides BFMs in data collection for task inference. Authors frame this online task inference problem as a linear bandit problem and maintain a probabilistic belief (a confidence ellipsoid) over the true task embedding z_r by performing real-time least-squares regression on reward-level feedback. Experiments on zero-shot benchmarks show that OpTI-BFM matches or surpasses offline reward inference methods with much less data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Tackles a significant and practical bottleneck. Online task inference with only a few active-interaction episodes is important for real-world applications.\n  \n- Solid theoretical guarantees via connections to linear bandit algorithms.\n  \n- Good empirical performance with additional experiments and analysis, e.g. episode-level updates and non-stationary rewards."}, "weaknesses": {"value": "- The paper operates under assumptions of a perfect BFM / successor feature model. It is unclear what would happen for the theoretical guarantees or how the empirical results would change with approximation errors.\n  \n- The formal regret bound is proven for an episodic-update variant of OpTI-BFM. However, the experiments (Sec.5.3, Fig. 4) show that the step-update variant is empirically superior and converges much faster. While it is a positive result that the practical algorithm is even better, it means the theory doesn't formally cover the best-performing algorithm presented.\n  \n- The optimistic search for z is currently done by random shooting and may fail for larger, more complicated spaces."}, "questions": {"value": "- How is the method's robustness to misspecification? For example, how does OpTI-BFM perform if the true reward function r(s) has a significant non-linear component, or if the pre-trained BFM is of lower quality (violating A1)? How does its degradation compare to the offline \"Oracle\" regression, which would also suffer from this misspecification?\n  \n- How does the random shooting for UCB optimization scale? Have you explored the sensitivity to this number of samples? Would a gradient-based approach be more robust or scalable?\n  \n- Could the theoretical guarantees be extended to the step-level update case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nRxU354mKg", "forum": "m5byThUSNE", "replyto": "m5byThUSNE", "signatures": ["ICLR.cc/2026/Conference/Submission23678/Reviewer_afmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23678/Reviewer_afmy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997679564, "cdate": 1761997679564, "tmdate": 1762942758016, "mdate": 1762942758016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}