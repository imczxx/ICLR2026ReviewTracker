{"id": "0ylAe3Orfy", "number": 2816, "cdate": 1757259380154, "mdate": 1759898125696, "content": {"title": "Multi-Object System Identification from Videos", "abstract": "We introduce the challenging problem of multi-object system identification from videos, for which prior methods are ill-suited due to their focus on single-object scenes or discrete material classification with a fixed set of material prototypes. To address this, we propose MOSIV, a new framework that directly optimizes for continuous, per-object material parameters using a differentiable simulator guided by geometric objectives derived from video. We also present a new synthetic benchmark with contact-rich, multi-object interactions to facilitate evaluation. On this benchmark, MOSIV substantially improves grounding accuracy and long-horizon simulation fidelity over adapted baselines, establishing it as a strong baseline for this new task. Our analysis shows that object-level fine-grained supervision and geometry-aligned objectives are critical for stable optimization in these complex, multi-object settings. The source code and dataset will be released.", "tldr": "", "keywords": ["Object Property Identification", "Physics-based Modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99cf96583da4ef9ad90bea4767f18b221eaa9eec.pdf", "supplementary_material": "/attachment/63257c056e57720c17cab5e65c7505826802726e.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a synthetic video dataset that presents contact and interaction between different materials. A baseline framework that identifies the continuous, object-specific physical properties is also proposed, with geometric-driven supervision and object-aware dynamic component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I found this paper reused and properly directed to external sources for development that helps the readers from diverse backgrounds.\n- A few features I see interesting to have together in this framework: simulation-ready continuum for multiple objects across all time, permaterial parameters to predict future motions."}, "weaknesses": {"value": "My background is not in physics simulation, so I look at the paper from the perspective of a computer vision researcher.\n- It is unclear how long the predicted horizon is. The example videos appear quite short—only a few seconds in duration, and seem not to show motion prediction (which I find different from trajectory prediction. Trajectory prediction is accumulating past locations, while motion prediction is to predict future locations). Please clarify the temporal length of the predictions.\n- How many object categories are supported in the dataset? A summary table presenting dataset statistics would be helpful for readers. Also, I find that only two object interactions are limited compared to existing datasets such as CLEVRER.\n- Additionally, please discuss or evaluate the model's ability to generalize to out-of-distribution in both object categories and interactions. This would also help to strengthen section subsection 3.7."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dGvZ6wnwJM", "forum": "0ylAe3Orfy", "replyto": "0ylAe3Orfy", "signatures": ["ICLR.cc/2026/Conference/Submission2816/Reviewer_SAcW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2816/Reviewer_SAcW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926062814, "cdate": 1761926062814, "tmdate": 1762916390307, "mdate": 1762916390307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MOSIV (Multi-Object System Identification from Videos), a novel framework designed to infer continuous physical constitutive parameters for multiple interacting objects directly from multi-view video observations. By employing per-object geometric reconstruction followed by the system identification of continuous parameters, MOSIV moves beyond prior work that relies solely on selecting from a fixed, categorical library of expert constitutive models. This advancement significantly enhances the fidelity, precision, and physical plausibility of subsequent dynamic simulations."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "+ Pioneering Continuous Parameter Identification: The primary strength is the innovative shift from categorical material classification to the estimation of continuous constitutive parameters. This allows for highly granular, object-specific physical calibration, overcoming a major limitation of current video-to-physics pipelines.\n+ Effective Multi-Object Handling: The framework successfully tackles the complexity of simultaneous system identification across multiple objects that are undergoing complex interactions (e.g., contact, collision) within the same observed scene, which is a critical capability for real-world robotics and simulation.\n+ Robust Framework Integration: The successful integration of geometric reconstruction and a differentiable physics pipeline suggests a robust, end-to-end optimization strategy capable of generating strongly calibrated and faithful physical models from raw visual data."}, "weaknesses": {"value": "- Overall, this paper presents a clear goal and a coherent overall narrative. However, the main issue lies in the lack of clear motivation behind many of the detailed design choices. In several instances, important implementation details are missing, which can lead to reader confusion and make the paper difficult to follow.\n- According to the description in subsection 3.3, objects are first represented as Gaussians, then converted into voxels, and finally transformed into particles. However, the whole process is quite unclear given the current simple explanation. Moreover, what parameters define these particles? Are they simply points? If so, why not use point clouds as the initial representation instead?\n- In subsection 3.5, why are silhouettes and surfaces used as the objective? Is this a common approach for such tasks? If so, please provide appropriate references; if not, please clarify the rationale behind this choice.\n- In Figure 4, different methods appear to have different observed frames. Does this imply that they were simulated using different parameters or settings? Shouldn’t the observed frames be consistent across methods to ensure a fair comparison?\n- Given that the method involves per-object geometric reconstruction and the iterative optimization of continuous parameters, the computational expense is likely very high. Could the authors provide a rigorous analysis of the computational complexity (e.g., model size, training time, and inference time) and a comparison with other methods?\n- How robust is MOSIV to different levels of reconstruction uncertainty? Did the authors conduct a sensitivity analysis showing how small errors in the initial reconstruction propagate into the estimated values of the constitutive parameters?\n- While the parameters are continuous, the underlying physics model must still be chosen. Does MOSIV include a mechanism for a priori selecting the correct base constitutive model type, or is that externally provided?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MzunPHGh7s", "forum": "0ylAe3Orfy", "replyto": "0ylAe3Orfy", "signatures": ["ICLR.cc/2026/Conference/Submission2816/Reviewer_yotJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2816/Reviewer_yotJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956912985, "cdate": 1761956912985, "tmdate": 1762916390020, "mdate": 1762916390020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MOSIV, a new framework created to solve the problem of identifying the physical properties of multiple interacting objects simultaneously from a video. MOSIV works by using a differentiable simulator. It directly optimizes the specific, continuous material parameters for each object by trying to match the geometry observed in the video. MOSIV also presents a new synthetic benchmark with contact-rich, multi-object interactions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the vision of learning physical properties and its interactions directly from videos seems appealing\n- The overall approach is logical and builds upon several state-of-the-art components, including the dynamic Gaussian Splatting for reconstruction and a differentiable MPM for physics-based parameter identification\n- the author introduces a new multi-object dataset with diverse geometry, materials properties and physical motions, that could be used from the community"}, "weaknesses": {"value": "- the paper studies multi-object system interaction, but the proposed dataset only contains two-object interactions, also 30 frames of interactions seems quite short for evaluation, given the authors claim that the calibrated models generalizes to \"long-horizon predictions of complex multi-object dynamics\""}, "questions": {"value": "- The method appears to be computationally expensive? It involves (1) optimizing a 4D Gaussian scene and assigning instance partitions, (2) converting object's reconstruction into simulation-ready continuum (3) running a differentiable MPM simulation to optimize per-object parameter vectors, and (4) optimizing this entire unrolled simulation. What are the typical run time and memory footprint? how does it compare with baseline methods?\n- what are the novel interactions in section 3.7? if my understanding is correctly, the physical motions are kept the same, you only change the materials?\n- what consitutive models have been used? how are they chosen, can you provide more details on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BQSUJtbjqD", "forum": "0ylAe3Orfy", "replyto": "0ylAe3Orfy", "signatures": ["ICLR.cc/2026/Conference/Submission2816/Reviewer_yKeE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2816/Reviewer_yKeE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996818233, "cdate": 1761996818233, "tmdate": 1762916389857, "mdate": 1762916389857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework to recover material properties from multi-view videos of multi-object scenes. The task setting is similar to PAC-NeRF except that multiple objects exist in the same scene. In additional to pixel supervision, the method also reconstructs particle trajectories using a 4D Gaussian framework, so that 3D surface loss can also be used as supervision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The experiments are comprehensive and promising."}, "weaknesses": {"value": "- The contribution is limited. The following two works are not cited. Justification of contributions needs revision.\n    - The task of multi-object system identification from videos is not novel: [1] also tackles multi-object system ID based on NeRF.\n    - Geometry-driven supervision is not novel: [2] uses 4D Gaussian to reconstruction mesh sequence from multiview videos and use 3D loss to tune physical parameters of cloth.\n\n[1] Li, J., Gao, Y., Song, W., Li, Y., Li, S., Hao, A. and Qin, H., 2024, October. CoupNeRF: Property‐aware Neural Radiance Fields for Multi‐Material Coupled Scenario Reconstruction. In Computer Graphics Forum (Vol. 43, No. 7, p. e15208).\n\n[2] Zheng, Y., Zhao, Q., Yang, G., Yifan, W., Xiang, D., Dubost, F., Lagun, D., Beeler, T., Tombari, F., Guibas, L. and Wetzstein, G., 2024, September. Physavatar: Learning the physics of dressed 3d avatars from visual observations. In European Conference on Computer Vision (pp. 262-284). Cham: Springer Nature Switzerland.\n\n- Some key descriptions of the proposed method are missing: \n    - How discrete constitutive models of each objects are set? If the types are given, \"Predicted Categorical Distribution\" in Fig 1 is very misleading. And [1] already handled this case. If the types are predicted, the paper does not mention the procedure at all in method section.\n    - How silhouettes are rendered? Are they rasterized directly from MPM point clouds?\n    - How are \"extracted surfaces\" (Line 256) extracted in detail?\n\n- No real-world examples are provided."}, "questions": {"value": "Key questions that needs justifications or clarifications are mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vyNXlFrdVM", "forum": "0ylAe3Orfy", "replyto": "0ylAe3Orfy", "signatures": ["ICLR.cc/2026/Conference/Submission2816/Reviewer_dfDn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2816/Reviewer_dfDn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116880664, "cdate": 1762116880664, "tmdate": 1762916389704, "mdate": 1762916389704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}