{"id": "kaW0pt8pLR", "number": 7894, "cdate": 1758041511151, "mdate": 1759897823764, "content": {"title": "Labelling Data with Unknown References", "abstract": "An evaluator is trustworthy when there exists some agreed-upon way to measure its performance as a labeller. The two ways to establish trustworthiness are either by testing it, or by assuming the evaluator 'knows' somehow the way to label the corpus. However, if labelled references (e.g., a development set) are unavailable, neither of these approaches work: the former requires data, and the latter is an assumption, not evidence. To address this, we introduce an algorithm (the 'No-Data Algorithm') by which to establish trust in an evaluator without any existing references. Our algorithm works by successively posing challenges to said evaluator. We show that this is sufficient to establish trustworthiness w.h.p., in such a way that when the evaluator actually knows the way to label the corpus, the No-Data Algorithm accepts its output; and, conversely, flags untrustworthy evaluators when these are unable to prove it. We present formal proofs of correctness, empirical tests, and applications to LLMs-as-judges on low-resource languages.", "tldr": "We introduce an algorithm based on cryptography by which to establish trust on an evaluator when labels do not exist--for example, an LLM-as-a-judge in low-resource languages--with formal guarantees of correctness.", "keywords": ["evaluation", "LLMs-as-judges", "algorithms", "meta-evaluation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c0705ede8555f48d434e44e4a1726e06929e400.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the \"No-Data Algorithm\", a method to establish trustworthiness in an evaluator (e.g., LLM-as-judge) for labeling data without any pre-existing labeled references. The approach relies on an Evaluator-Verifier protocol, where the evaluator generates similar datapoints and partial labels, and the verifier challenges them based on structural (equality up to permutation) and valuation (equality up to isomorphism) checks using a predefined rubric and aggregator. Theoretical proofs show correctness under assumptions like decomposable rubrics and binary data/labels, and the algorithm bounds the probability of undetected lies to (1/4)^r after r rounds. Experiments validate this on synthetic binary datasets (using decision trees and LLMs) and a real-world low-resource language task (West Frisian labeling with LLMs), demonstrating that success rates distinguish knowledgeable from untrustworthy evaluators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a critical gap in evaluator trustworthiness without references, drawing from zero-knowledge proofs to create a verifiable challenge-response system. The formal proofs provide strong theoretical guarantees, bounding error probabilities in a probabilistic manner, which is a refreshing departure from empirical-only approaches in LLM evaluation literature.\n- The method enables trust establishment in data-scarce domains like low-resource NLP or medical labeling, without relying on potentially contaminated benchmarks. Ablations in appendices (e.g., on generation strategies and multi-model comparisons) add depth, showing robustness across evaluators like decision trees and various LLMs (o3-mini, GPT-4o, etc.).\n- Overall the paper is well-structured, with intuitive definitions (e.g., rubric and aggregator) and adequate theoretical depth."}, "weaknesses": {"value": "- My biggest concern is that the algorithm's core assumption assumes the true labeling function f decomposes into a rubric C (criteria) and aggregator σ (e.g., majority vote); it is reasonable in theory but challenging in practice. Defining a \"total\" rubric that fully decomposes nonlinear criteria requires domain expertise and may not capture complex, implicit decision-making in real evaluators like LLMs, which often rely on emergent behaviors rather than explicit rules. Also, the binary string representation of data (X ⊂ {0,1}^n) and binary labels (Y = {0,1}) simplify proofs but limit direct applicability to multi-class or continuous-label tasks; while the discussion mentions extensions (e.g., via one-hot encoding), these could increase runtime by a factor of k-1 for k classes, potentially making it inefficient for high-dimensional data.\n- Experiments are confined to synthetic binary strings and a niche low-resource language (West Frisian). Importantly, the EV protocol relies on evaluators generating \"similar\" datapoints (thus assuming high-quality generation capabilities), but ablations show LLMs struggle with this (e.g., better performance when picking from datasets vs. generating anew), raising scalability issues for complex data - where the reference data are sparse and thus No-data algorithm could actually be useful. In unknowable scenarios, tuning phi (flip probability) based on expected accuracy is heuristic and assumes some prior knowledge, contradicting the \"no-data\" claim.\n- While results align with theory, the datasets are small (e.g., 498 entries for synthetic, 1,015 for West Frisian), and comparisons are limited. No baselines like prompt engineering or ensemble methods for trust establishment are evaluated, making it unclear if the algorithm outperforms simpler alternatives.\n- The verifier needs access to the rubric, which must be provided upfront—begging the question of how to obtain a reliable rubric without data. The protocol's multi-round calls could be computationally expensive for large datasets or real-time applications, and there's no analysis of failure modes like adversarial evaluators exploiting rubric ambiguities."}, "questions": {"value": "- How do you propose defining and validating rubrics in real-world settings where the true f is unknown? For instance, in the West Frisian experiment, how was the rubric designed and quality-controlled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DnQO278ysP", "forum": "kaW0pt8pLR", "replyto": "kaW0pt8pLR", "signatures": ["ICLR.cc/2026/Conference/Submission7894/Reviewer_kuS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7894/Reviewer_kuS7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413354921, "cdate": 1761413354921, "tmdate": 1762919925892, "mdate": 1762919925892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the No-Data Algorithm, a theoretically motivated protocol for establishing the trustworthiness of automated evaluators, such as LLMs-as-judges, in scenarios where no labeled reference data is available. The protocol is inspired by zero-knowledge proofs and interactive challenge-response systems, allowing a verifier to probabilistically assess whether an evaluator truly knows the labeling function. Theoretical correctness and error bounds are provided, and empirical validation is conducted on synthetic tasks and a low-resource language labeling scenario."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses an important and challenging problem, which is how to trust automated evaluators without labeled data.\n- The authors propose a novel, theoretically grounded protocol inspired by zero-knowledge proofs and interactive proofs. Additionally, this paper provides formal correctness guarantees and explicit error bounds.\n- Empirical results on synthetic and real-world task (LLMs-as-judges on low-resource languages) support the theoretical claims."}, "weaknesses": {"value": "- The proposed methods assume the existence of a shared, explicit rubric/aggregator decomposition, which may not be realistic for many annotation tasks.\n- Similarity between datapoints is not well-defined for complex or unstructured domains.\n- Experiments are limited to binary labels and do not cover more complex or open-ended settings. Scalability to large datasets and high-dimensional input is untested."}, "questions": {"value": "- How can the protocol be adapted to tasks where the rubric is not easily decomposable or is inherently subjective/ambiguous?\n- How is 'similarity' between datapoints defined or computed in real-world, high-dimensional, or unstructured domains?\n- How robust is the method to imperfect, partial, or subjective rubrics, or to stochastic evaluators/verifiers (e.g., LLMs with temperature > 0)?\n- What are the practical computational and annotation overheads for deploying the No-Data Algorithm in real-world settings?\n- What does “w.h.p.” mean in the abstract?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AtPKHoNQF2", "forum": "kaW0pt8pLR", "replyto": "kaW0pt8pLR", "signatures": ["ICLR.cc/2026/Conference/Submission7894/Reviewer_vyZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7894/Reviewer_vyZn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852469512, "cdate": 1761852469512, "tmdate": 1762919925396, "mdate": 1762919925396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the \"No-Data Algorithm\" for establishing trustworthiness of evaluators (including LLMs-as-judges) without requiring labeled reference data. The approach uses an Evaluator-Verifier (EV) protocol based on zero-knowledge proof concepts, where evaluators must pass challenges about datapoint similarity to establish credibility. The work provides theoretical correctness bounds and empirical validation on synthetic data and a West Frisian language evaluation task. The authors also introduce a new West Frisian dataset that is annotated by language experts and contains 1,015 items. The approach resembles semi-supervised or unsupervised based learning methods for learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The primary contribution, the \"No-Data Algorithm,\" is a novel, formal method for establishing trust without references. Moving beyond simple correlation metrics (which require references) to a proof-based system based on a challenge-response protocol is a commendable direction. The work clearly states when labels are fundamentally unknowable and when the algorithm's purpose is establishing trust rather than producing labels.\n2. Combines a synthetic binary-string setting (clean control of rubrics) with a realistic low-resource language application (West Frisian), reinforcing external validity of the theoretical predictions.The authors also create a new West Frisian dataset that is expert annotated and from a low-resourced language (1,015 items) that is a helpful contribution towards the community. \n3. The findings also can help with the future research in terms of further studying the evaluator in itself since the paper makes a crucial distinction between an evaluator's accuracy and its trustworthiness. The results compellingly show that a deceptive evaluator can achieve high accuracy (e.g., in the OOP case) but will be flagged by a low success rate. This finding that the success rate is the more reliable metric for trust is a key takeaway."}, "weaknesses": {"value": "1. The approach theoretically is an approach that could be applied in general to any classification task where an LLM judge could be utilized, but the paper has limited score in terms of experimental evaluation. In terms of benchmarking, any classification task could have been used for the paper as a potential task to compare how this approach would do against the human labels. \n2. The most critical weakness is using GPT-4.1 as both evaluator and verifier in the natural language experiments (Section 6.2). This creates a circular validation where the system is essentially checking itself. While the paper acknowledges this creates questions about \"what are the results measuring\" (Section 7.2), this undermines the core claim of establishing independent trustworthiness. The theoretical framework assumes an independent verifier, but the implementation violates this assumption. \n\n3. In terms of baselines for this experiment set is an algorithm that relies on supervised learning, but a strong baseline is not used for comparison in this paper but should be utilized in the next iterations. Either a classifier that is trained on the dataset, in-context learning (few shot prompting)."}, "questions": {"value": "More of suggestions for improvement. \n\n1. Writing and the organizing of the paper needs a bit of work, there should be a set of contributions or research questions set early in the research paper, just to highlight everything in the paper and to guide the narrative. The authors contribute (or at least they’ve created) a new human annotated dataset for Frisian language but this is not highlighted until later in a section. The narrative can be easily followed if there was a clear RQ set. \n2. The generalizability of the synthetic experiment is questionable. The LLM evaluator was tasked to pick a datapoint from a provided list rather than generate a new one, reportedly because generation led to poor performance. This \"picking\" strategy seems to be a significant concession and a deviation from the described protocol. The implications of this methodological shortcut on the validity of the results, and how this relates to the natural language experiment where generation is performed, are not sufficiently discussed.\n3. It would have been helpful if there was a figure to handle the overall narrative, Fig 1 doesn’t capture the entirety of the entire research workflow proposed in the paper. \n4. More on model selection for a task that is low-resourced, it would have been helpful if the impact was compared against a baseline for multi-lingual such as Cohere's Aya model (https://cohere.com/research/aya). Why was the gpt 4.1 model identified over a multi-lingual model that is trained for that language (West Firisian)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UrF4Xgivor", "forum": "kaW0pt8pLR", "replyto": "kaW0pt8pLR", "signatures": ["ICLR.cc/2026/Conference/Submission7894/Reviewer_t3tA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7894/Reviewer_t3tA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016560771, "cdate": 1762016560771, "tmdate": 1762919924722, "mdate": 1762919924722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a fundamental problem of trusting an evaluator (e.g., an LLM-as-a-judge) without any labelled reference data, which is increasingly common in low-resource settings, domains with expensive annotation, and scenarios where benchmark contamination is suspected. The authors propose the No-Data Algorithm, which interacts with an evaluator through a verifier using a multi-round Evaluator–Verifier (EV) protocol. The evaluator must generate similar datapoints and justifications, while the verifier issues random structural and semantic challenges. Passing both challenges over repeated rounds is provably difficult for dishonest evaluators. The algorithm incorporates probabilistic flipping of labels upon failure and yields a “success rate” metric that reflects evaluator trustworthiness.\nOverall this is a well written paper and I enjoyed reading this paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The following are 3 strong points of this paper:\n1. This paper considers a novel and timely problem. This is one of the core weaknesses of LLM-based evaluation: absence of ground truth. \n\n2. The analytical formal guarantees presented in this paper links deception probability to the number of challenge rounds, offering provable confidence.\n\n3. The West Frisian experiment demonstrates impact beyond toy settings and highlights where evaluator trust matters in real deployments."}, "weaknesses": {"value": "The following are 3 weak points of this paper:\n1. Semantic similarity assumption is hand-wavy in natural language and the same is acknowledged in this paper.. But the authors did not handle this properly. \n\n2. Authors note that trust in an evaluator becomes trust in a prompt. Without robust prompting strategies, reproducibility may suffer and this undermines the practical reliability of this paper.\n\n3. While binarisation is suggested, the mathematical treatment does not fully generalize to multi-class, hierarchical labels,continuous scoring systems."}, "questions": {"value": "(a) Please answer the above 3 weak points.\n\n(b) Can you please clarify what similarity means for natural language settings.. while do so, consider embedding-based constraints.\n\n(c) Can you provide an ablation on the prompt design aspects.\n\n(d) How easy or difficult it is to expand your approach beyond binary labels setting? O/w the impact of this work would become limited. \n\n(e) Proving more light on understanding how dishonest evaluators are caught would improve interpretability.. and this enhances the quality of this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m4pJxtkHoo", "forum": "kaW0pt8pLR", "replyto": "kaW0pt8pLR", "signatures": ["ICLR.cc/2026/Conference/Submission7894/Reviewer_f2Lc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7894/Reviewer_f2Lc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762693810508, "cdate": 1762693810508, "tmdate": 1762919924376, "mdate": 1762919924376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}