{"id": "EQPfioF7c5", "number": 179, "cdate": 1756730421909, "mdate": 1763123204188, "content": {"title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in text-to-image diffusion models, enabling the personalisation of visual concepts such as characters, styles, and objects. However, existing approaches struggle to effectively compose multiple LoRA adapters, particularly in open-ended settings where the number and nature of required skills are not known in advance. In this work, we present LoRAtorio, a novel train-free framework for multi-LoRA composition that leverages intrinsic model behaviour. Our method is motivated by two key observations: (1) LoRA adapters trained on narrow domains produce unconditioned denoised outputs that diverge from the base model, and (2) when conditioned out-of-distribution, LoRA outputs show behaviour closer to the base model than when conditioned in distribution. In the single LoRA scenario, personalisation and customisation show exceptional performance without catastrophic forgetting; the performance, however, deteriorates quickly as multiple adapters are loaded.\nOur method operates in the latent space by dividing it into spatial patches and computing cosine similarity between each patch’s predicted noise and that of the base model. These similarities are used to construct a spatially-aware weight matrix, which guides a weighted aggregation of LoRA outputs. To address domain drift, we further propose a modification to classifier-free guidance that incorporates the base model’s unconditional score into the composition. We extend this formulation to a dynamic module selection setting, enabling inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio achieves state-of-the-art performance, showing up to a 1.3\\% improvement in ClipScore and a 72.43\\% win rate in GPT-4V pairwise evaluations, and generalises effectively to multiple latent diffusion models. Code will be made available.", "tldr": "", "keywords": ["MultiLoRA composition", "diffusion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67e6af7654d3e1d6dabaff41d59fd9a9a7a5d84d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LoRAtorio, a train-free framework for composing multiple LoRA adapters in text-to-image diffusion models. The method leverages intrinsic model behavior by computing patch-wise cosine similarity between LoRA outputs and the base model in latent space, using these similarities to weight contributions during denoising. It also proposes a modification to classifier-free guidance to mitigate domain drift and extends the approach to dynamic module selection for inference-time adaptability. Experiments on the ComposLoRA benchmark and Flux architecture show improvements in CLIPScore, GPT-4V evaluations, and human preference metrics compared to prior state-of-the-art methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novelty: The idea of using intrinsic similarity for inference-time LoRA composition is original and avoids retraining, addressing practical constraints.\n\nComprehensive Evaluation: Includes automated metrics (CLIPScore), GPT-4V-based evaluation, and human studies across multiple datasets and architectures.\n\nDynamic Module Selection: Extends beyond static composition, which is relevant for real-world scenarios.\n\nClear Motivation: Observations about domain drift and latent divergence are well-supported by empirical analysis and visualizations.\n\nModel-Agnostic Design: Demonstrates applicability to both Stable Diffusion and Flux architectures."}, "weaknesses": {"value": "Computational Overhead: The method scales linearly with the number of LoRAs, making it impractical for large pools in dynamic settings. Latency (61–122s) is significantly higher than simpler baselines like Switch or Merge.\n\nLimited Theoretical Depth: While cosine similarity is justified empirically, the theoretical motivation is relegated to an appendix and lacks rigorous formalism.\n\nEvaluation Bias: Heavy reliance on CLIPScore and GPT-4V pairwise comparisons, which do not fully capture compositional fidelity or semantic correctness. Human evaluation is limited to three experts.\n\nFailure Cases: The paper acknowledges severe failure modes (e.g., nonsensical outputs, concept confusion, duplicate limbs) but does not propose concrete mitigation strategies.\n\nAssumption of LoRA Quality: The approach assumes LoRAs are well-trained and semantically coherent, which is unrealistic in community-driven repositories.\n\nScalability Concerns: Dynamic selection still requires loading all LoRAs into memory, which is infeasible for large-scale deployments.\n\nOverstated Generalization: Claims of model-agnostic robustness are based on only two architectures; broader applicability remains unproven.\n\nEthical Section Superficiality: Mentions risks but lacks actionable guidelines or safeguards for misuse.\n\nGeneralizability: The methods have not been applied to large language models or multimodal large language models."}, "questions": {"value": "How does LoRAtorio perform when LoRAs are trained on highly heterogeneous datasets with conflicting semantics?\n\nCan the similarity-based weighting be approximated earlier in the pipeline to reduce computational cost?\n\nHow sensitive is the method to patch size and temperature hyperparameters in real-world scenarios?\n\nWhy was λ fixed at 0.5 for re-centering? Did you explore adaptive strategies?\n\nCould metadata-driven pre-filtering or clustering of LoRAs improve dynamic selection efficiency?\n\nHow does the method handle cases where LoRA adapters introduce adversarial or biased features?\n\nIs there any quantitative analysis of memory footprint for dynamic settings?\n\nWould integrating learned gating (e.g., lightweight attention) outperform intrinsic similarity without full retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SjjsvjM6SE", "forum": "EQPfioF7c5", "replyto": "EQPfioF7c5", "signatures": ["ICLR.cc/2026/Conference/Submission179/Reviewer_GGbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission179/Reviewer_GGbc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761177337397, "cdate": 1761177337397, "tmdate": 1762915462586, "mdate": 1762915462586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LoRAtorio, a train-free framework for composing multiple LoRA adapters in text-to-image diffusion models. The authors identify key challenges in multi-LoRA composition—namely, semantic drift and performance degradation as more LoRAs are added—and address these by leveraging intrinsic model behavior rather than external supervision or retraining. Their approach partitions the latent space into spatial patches and computes cosine similarity between LoRA and base model noise predictions, constructing a spatially-aware weight matrix for adaptive skill fusion. They also propose a modification to classifier-free guidance to mitigate domain drift. Furthermore, LoRAtorio supports dynamic module selection at inference time, selecting relevant LoRAs on-the-fly. Experimental results on the ComposLoRA benchmark and additional settings demonstrate that LoRAtorio achieves better performance in both static and dynamic scenarios, outperforming prior works across automated metrics (CLIPScore), GPT-4V evaluations, and human assessments"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates originality by proposing a train-free, intrinsically guided framework for multi-LoRA composition, departing from the reliance on weight merging or learned gating. The quality of the work is evident in the methodology, including spatial patch-based weighting, re-centered guidance, and dynamic module selection. The paper is clearly written, with effective visualizations and thorough empirical support."}, "weaknesses": {"value": "While the paper presents an innovative and effective approach, there are several notable weaknesses that merit attention. First, the authors do not release their code, which hinders reproducibility and weakens the reliability of the claimed results. Second, the core mechanism—spatial patch-based weighting—raises concerns when dealing with heterogeneous LoRA types. For example, style-oriented LoRAs may introduce global stylistic shifts across all spatial regions, while object-specific LoRAs affect only localized areas. The current similarity-based weighting may fail to harmonize such differences, potentially leading to outputs where object identity is distorted by the base model's style or vice versa, contrary to the intended composition. Third, a key advantage of LoRA is the ability to merge multiple adapters at inference with negligible overhead, but LoRAtorio requires evaluating all adapters independently at each step, increasing inference cost significantly. The paper lacks a discussion or analysis of this added complexity, which could impact its scalability in real-world applications. Addressing these issues would strengthen both the practicality and theoretical grounding of the work."}, "questions": {"value": "1. Given the complexity of the method, open-sourcing the code would be essential for reproducibility and to support broader adoption in the community.\n\n2. Have the authors evaluated or analyzed the behavior of their patch-based weighting mechanism when composing LoRAs of fundamentally different types—e.g., one encoding global stylistic shifts and another modeling localized objects? In such cases, a patch that diverges from the base model may not necessarily indicate higher relevance. Can the authors provide qualitative examples or ablations showing the composition quality in these mixed scenarios?\n\n3. Relatedly, is there a risk that the current weighting method leads to mismatched compositions—e.g., an object generated with correct shape but styled according to the base model, while the background reflects the intended LoRA style? If so, how might this be mitigated?\n\n4. The proposed method computes conditional and unconditional predictions for each LoRA independently at every denoising step, which appears to scale linearly with the number of adapters. Could the authors clarify the runtime and memory impact of their method compared to weight-merging baselines?\n\n5. In the dynamic module setting, how well does the method scale when the number of candidate LoRAs is very large (e.g., dozens or hundreds)? Is the top-k selection based purely on per-step cosine similarity stable across timesteps, or is there a risk of inconsistency during denoising?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oOqM9NPaA6", "forum": "EQPfioF7c5", "replyto": "EQPfioF7c5", "signatures": ["ICLR.cc/2026/Conference/Submission179/Reviewer_ftW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission179/Reviewer_ftW8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723546864, "cdate": 1761723546864, "tmdate": 1762915462477, "mdate": 1762915462477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LoRAtorio is a train-free framework for composing multiple LoRA adapters in text-to-image diffusion models. It leverages the observation that LoRAs deviate from the base model on in-distribution inputs but remain close on out-of-distribution ones, using cosine similarity between LoRA and base outputs in latent space to guide spatially weighted aggregation. The method also introduces a modified classifier-free guidance term to mitigate domain drift and supports dynamic, inference-time selection of relevant adapters. LoRAtorio achieves state-of-the-art performance, improving CLIPScore and human-evaluated visual quality across multiple diffusion architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors propose spatially-aware similarity metric to use as a proxy for LoRA adapter's confidence, with sound theoretical motivation.\n2. The authors extend the task of multi-LoRA composition to a dynamic module selection setting, which is a good, real-world skill composition scenario."}, "weaknesses": {"value": "1. The first contribution seems to be incremental - MultLFG (2nd best method) proposes \"... training-free frequency-aware multi-LoRA merging. The key idea is to decompose LoRA-based noise predictions into frequency subbands and perform adaptive merging based on relevance scores.\" (https://arxiv.org/pdf/2505.20525), whereas this paper proposes patched cosine distance instead of frequency subbands.\n2. The second contribution - re-centering - is, per your results in Table 6a, only better by 0.01 (36.543 with vs 36.532 w/o) CLIPScore, on a limited ablation study (see Weakness 3), which makes me believe it does not improve anything. What are the standard deviations for these results?\n3. You compare your method to 8 reference methods in Table 1, just 4 in Table 2 and only 2 in the rest (Table 3, 4, 5c/d). The 2nd best performing method (per Table 1), MultLFG, is only shown once and never mentioned again. Why is that?\n4. Ablation study of the proposed method (Table 6) lacks the same detail as, for example, Table 1. It only analyzes 2 or 3 component scenarios (missing 4 and 5), only on one subset (anime), on one backbone (stable-diffusion-v1.5)."}, "questions": {"value": "1. Could you extend the analysis of the weight matrix Ω? For example, I am wondering if taking the most dominant adapter's category per each patch could result in semantic masks appearing. Especially non global categories like character or object may be visible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kWTVvPwSsc", "forum": "EQPfioF7c5", "replyto": "EQPfioF7c5", "signatures": ["ICLR.cc/2026/Conference/Submission179/Reviewer_2mHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission179/Reviewer_2mHn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104077989, "cdate": 1762104077989, "tmdate": 1762915462378, "mdate": 1762915462378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of changes"}, "comment": {"value": "We thank all reviewers for their time and constructive feedback. We are encouraged by the recognition of our paper’s originality in proposing a train-free, intrinsically guided framework for multi-LoRA composition (ftW8), its clear structure and comprehensive evaluation across automated metrics, GPT-4V, and human studies (rah2, ftW8), and its applicability to both Stable Diffusion and Flux architectures (rah2, ftW8). Reviewers also highlighted the sound theoretical motivation behind our spatially-aware similarity metric (2mHn) and the practical relevance of dynamic module selection for real-world scenarios (2mHn).\n\nTo address some of the reviewers' concerns, we have updated the submitted manuscript with the following:\n- Lines 115-120: We update to provide a motivation for the dynamic extension of the task (rah2)\n- Line 213: We correct the patch size to d^2 (rah2)\n- Lines 456-462: We provide clarification on key differences with frequency-based approaches (2mHn)\n- Fig 8 caption and lines 806-811: clarify the use of lambda and how it affects image quality. (2mHn)\n- Fig 10: Omega map for character visualised (2mHn)\n\nDetailed responses are added to each reviewer’s comments."}}, "id": "NbDTL9XSIY", "forum": "EQPfioF7c5", "replyto": "EQPfioF7c5", "signatures": ["ICLR.cc/2026/Conference/Submission179/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission179/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission179/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763123237776, "cdate": 1763123237776, "tmdate": 1763123266864, "mdate": 1763123266864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LoRAtoria, a novel train-free framework for multi-LoRA composition that leverages intrinsic model behavior. \nThe framework consists of two parts: skill composition on the patch level and re-centering of the unconditional noise output.\nAlso, the paper introduces MultiLoRA composition task with a dynamic LoRA selection"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well structured and easy to follow.\n2. The proposed approach demonstrates better results with increasing the active LoRA adapters\n3. Re-centering of the unconditional noise could be used independently\n4. Both UNet and DiT-based models are checked\n5. The human and VLM-based evaluations are fully described\n6. Extensive appendix"}, "weaknesses": {"value": "1. MultiLoRA composition task with a dynamic LoRA selection probably requires more detailed description as now it lacks motivation (at least some potential use cases)\n2.The majority of the comparisons are done using CLIPScore that is a good proxy metric; however, a more extensive human or VLM-based evaluation is suggested\n3. Only composition of LoRas for the Character, Style and Background are considered. No compositions with LoRAs for faster inference (e.g., LCM) are checked \n4. see questions"}, "questions": {"value": "1) inconsistent d:\n* lines 213-214: \"$d$ is the number of pixels per patch\"\n* lines 226-227 mention upscaling to $H/d \\times W/d$\nPlease use $d^2$ as the number of pixels in patch or upscaling to $\\sqrt(d)$ in the blocks description\n2) the commonly used number of diffusion steps for SD1.5 is 30-50 steps (towards 30 if DPM++ solver is used); however, in the section 3.1 authors mentioned 100 steps for realistic subset and 200 steps for the anime subset without any further explanation.\n3) The human evaluation mentioned only 3 experts. Have you considered running the quality assessment using GPT4v?\n4) The results for the Rectified Flow are presented only on FLUX.1-dev checkpoints that is guidance distilled (re-centering couldn't be applied) while Stable Diffusion 3.5 is not checked.\n5) Comparison with AutoGuidance is presented in the appendix; however, AutoLoRA(https://arxiv.org/abs/2410.03941) shows that AutoGuidance-ish approach could be combined with CFG for LoRAs\n6) I believe that the skill composition process could be better illustrated"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qKEBDwLeew", "forum": "EQPfioF7c5", "replyto": "EQPfioF7c5", "signatures": ["ICLR.cc/2026/Conference/Submission179/Reviewer_rah2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission179/Reviewer_rah2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126997790, "cdate": 1762126997790, "tmdate": 1762915462272, "mdate": 1762915462272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}