{"id": "bQQkWXYjuy", "number": 11378, "cdate": 1758197734800, "mdate": 1759897579066, "content": {"title": "AJF: Adaptive Jailbreak Framework Based on the Comprehension Ability of Black-Box Large Language Models", "abstract": "Recent advancements in adversarial jailbreak attacks have exposed critical vulnerabilities in Large Language Models (LLMs), enabling the circumvention of alignment safeguards through increasingly sophisticated prompt manipulations. Our experiments find that the effectiveness of jailbreak strategies is influenced by the comprehension ability of the target LLM. Building on this insight, we propose an Adaptive Jailbreak Framework (AJF) based on the comprehension ability of black-box large language models. Specifically, AJF first categorizes the comprehension ability of the LLM and then applies different strategies accordingly: For models with limited comprehension ability (Type-I LLMs), AJF integrates layered semantic mutations with an encryption technique (MuEn strategy), to more effectively evade the LLM's defenses during the input and inference stages. For models with strong comprehension ability (Type-II LLMs), AJF employs a more complex strategy that builds upon the MuEn strategy by adding an additional layer: inducing the LLM to generate an encrypted response. This forms a dual-end encryption scheme (MuDeEn strategy), further bypassing the LLM's defenses during the output stage. Experimental results demonstrate the effectiveness of our approach, achieving attack success rates of \\textbf{98.9\\%} on GPT-4o (29 May 2025 release) and \\textbf{99.8\\%} on GPT-4.1 (8 July 2025 release). Our work contributes to a deeper understanding of the vulnerabilities in current LLMs alignment mechanisms.", "tldr": "", "keywords": ["Black-Box LLMs; Jailbreak attacks; Comprehension ability of LLMs; Encryption; Adaptive strategy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9e617750bcba7a0e90e0619fa6ef8acc702c98e.pdf", "supplementary_material": "/attachment/ebe4463732b950a53b671061388d71f635489109.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes AJF, a capability-aware jailbreak attack strategy against black-box LLMs. By categorizing models into Type-I and Type-II based on comprehension ability, it applies customized encryption and mutation techniques to bypass input, inference, and output-stage defenses."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper introduces a model categorization framework that classifies aligned LLMs into two distinct types based on their comprehension ability, providing useful insights into model vulnerability patterns and guiding adaptive jailbreak strategies."}, "weaknesses": {"value": "1. The novelty is limited. The core components are just the extentions if the existing work.\n2. The classification of LLMs into Type-I and Type-II is based on a single Caesar cipher test, which is heuristic and may not reflect real comprehension capability.\n3. The attack prompt templates are deterministic and structurally repetitive, which makes them potentially vulnerable to static pattern-based defenses.\n4. The framework is only tested under static conditions and does not assess resilience against adaptive defenses, fine-tuning, or dynamic moderation strategies.\n5. The paper compares AJF with only a limited set of baselines."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "COUNrlGNoa", "forum": "bQQkWXYjuy", "replyto": "bQQkWXYjuy", "signatures": ["ICLR.cc/2026/Conference/Submission11378/Reviewer_XM8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11378/Reviewer_XM8h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651913353, "cdate": 1761651913353, "tmdate": 1762922502318, "mdate": 1762922502318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Adaptive Jailbreak Framework (AJF), a novel method for attacking black-box large language models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Conceptual Framework: The paper introduces a compelling new perspective by directly linking jailbreak effectiveness to the target model's comprehension ability. The classification of LLMs into Type-I and Type-II, while simple, is a conceptually insightful approach that moves the field beyond one-size-fits-all attacks. This adaptivity represents a more sophisticated paradigm for adversarial attacks on LLMs.\n- Significant Implications for LLM Safety: By demonstrating that a model's advanced comprehension can be turned against its own safety mechanisms, this work uncovers a deep and potentially fundamental vulnerability. It provides invaluable insights for the LLM safety and alignment community, suggesting that future defense mechanisms must account for this attack vector. The research serves as a powerful red-teaming contribution, highlighting critical areas for improvement."}, "weaknesses": {"value": "- While the paper introduces a novel \"adaptive\" perspective, its underlying technical components are largely clever orchestrations of existing primitives rather than fundamental breakthroughs. Programmatic obfuscation via code-like structures and the use of encryption to bypass safety filters are well-established paradigms, explored in prior work such as CodeChameleon and CipherChat. Therefore, the primary contribution lies in the strategic combination of these techniques to form a multi-stage attack, rather than in the invention of a new attack modality itself.\n- Furthermore, the framework's central claim to adaptiveness hinges on a model categorization criterion that lacks robustness and generalizability. The classification of LLMs into a rigid Type-I/Type-II binary is based on a single, highly-engineered probe prompt. This approach oversimplifies the multi-dimensional nature of LLM comprehension, which exists on a continuous spectrum, and creates a potential single point of failure where misclassification could lead to suboptimal attack strategies. The paper does not address the handling of models on the boundary or validate the criterion's consistency across diverse tasks.\n- Finally, the evaluation protocol suffers from significant methodological weaknesses that undermine the reliability of the reported results. The exclusive reliance on an \"LLM-as-a-judge\" framework introduces a risk of systemic, homologous bias. More critically, the baseline comparisons are not rigorous; many results are directly adopted from previous studies, which likely used different datasets and experimental settings. The absence of controlled, head-to-head experiments prevents a fair and scientifically valid assessment of the method's purported superiority over the state of the art."}, "questions": {"value": "Seen in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zw5O6FJoYE", "forum": "bQQkWXYjuy", "replyto": "bQQkWXYjuy", "signatures": ["ICLR.cc/2026/Conference/Submission11378/Reviewer_2YX4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11378/Reviewer_2YX4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825894872, "cdate": 1761825894872, "tmdate": 1762922501933, "mdate": 1762922501933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Adaptive Jailbreak Framework (AJF), a novel method for bypassing the safety mechanisms of black-box LLMs. The framework is built on the core insight that the effectiveness of a jailbreak attack should be tailored to the target model's comprehension ability. AJF first classifies LLMs into two categories—Type-I (limited comprehension) and Type-II (strong comprehension)—using a specialized probe prompt. For less capable Type-I models, it employs the MuEn strategy, which combines programmatic prompt mutation with structural encryption to evade input and inference defenses. For more advanced Type-II models, it uses a more sophisticated MuDeEn strategy, which adds a dual-end encryption layer that compels the LLM to generate an encrypted response, thereby circumventing output-level moderation. Through extensive experiments, the authors demonstrate the framework's high efficacy, achieving near-perfect attack success rates of 98.9% on GPT-4o and 99.8% on GPT-4.1. They show that more capable models can be manipulated into executing complex, multi-stage attacks that bypass their own safety filters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novel and well-motivated framework: Adapting adversarial attacks to the comprehension level of the target LLM is an insightful and novel contribution to the field of LLM jailbreaking.\n* Strong empirical performance: AJF demonstrates state-of-the-art performance, achieving extremely high ASR against some of the most powerful publicly available models.\n* The framework is well-designed to circumvent multiple layers of LLM defenses, including input, inference, and output moderation, which explains its high success rate.  The core idea of classifying models into Type-I and Type-II and applying tailored strategies (MuEn vs. MuDeEn) is a sophisticated conceptual contribution that reflects a deeper understanding of the LLM attack surface.\n* Efficiency: unlike iterative or query-intensive methods (e.g., fuzzing or optimization-based attacks), AJF is designed as a single-query attack."}, "weaknesses": {"value": "### Oversimplified LLM Categorization:\nThe framework's foundational step—classifying LLMs into a binary Type-I/Type-II distinction—relies on a single, complex probe prompt, which is a potential single point of failure and may not be robust. ``Comprehension'' is a spectrum, but the framework reduces it to a binary classification based on a single, engineered task (as in Sec. 3.2). The paper does not investigate the consequences of misclassification (e.g., applying the MuDeEn strategy to a Type-I model) or the sensitivity of the probe to small perturbations, making the robustness of this critical step unclear.\n\n### Insufficient Ablation Study to Isolate Component Contributions\nThe ablation study in Section 4.3, while demonstrating the value of En_response, is too limited to fully disentangle the contributions of all framework components and validate the core adaptive hypothesis.\n* Contribution of Mu is Unclear: The study never isolates the effect of the programmatic mutation (Mu). An experiment comparing MuEn (mutation + encryption) against an En-only strategy is missing. This makes it impossible to know how much of the success on Type-I models is due to the mutation versus the encryption.\n* Core Hypothesis Not Empirically Tested: The central claim is that strategies must be adapted. The most direct test of this would be to apply the \"wrong\" strategy (e.g., the complex MuDeEn on a Type-I model) and show that it fails. The paper hypothesizes this would happen (Lines 475-477) but provides no experimental results to prove it.\n\nOverall, I feel that the evaluation section could have been significantly improved to meet the publication quality, in terms of self-containedness and completeness, and extensiveness."}, "questions": {"value": "* Robustness of the categorization probe: The binary Type-I/II classification is foundational to your adaptive framework. Could you comment on the robustness of using a single probe prompt? How sensitive is the classification to minor variations in the prompt's wording or the model's output formatting? Did you consider using a suite of prompts to generate a more continuous capability score, which might offer a more robust basis for strategy selection?\n\n* The central thesis of the paper is that the attack strategy must be matched to the model's capability. To provide direct evidence for this, did you run experiments applying the ``wrong'' strategy? Specifically, what was the performance when applying the complex MuDeEn strategy to a model you classified as Type-I (e.g., Llama2-13b)? A significant drop in performance would provide powerful empirical validation for your core claim.\n\n* In Section 3.4, there appears to be a notational inconsistency. An is first defined as the intermediate natural language answer (Line 261), but then in Equation (6) it is used to represent the final decrypted output. The prose also introduces An* (Line 268). Could you clarify the precise definitions of An and An* and ensure the notation is consistent throughout the section to avoid confusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EzJmE8PzW0", "forum": "bQQkWXYjuy", "replyto": "bQQkWXYjuy", "signatures": ["ICLR.cc/2026/Conference/Submission11378/Reviewer_bZ8G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11378/Reviewer_bZ8G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000137722, "cdate": 1762000137722, "tmdate": 1762922501582, "mdate": 1762922501582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Adaptive Jailbreak Framework (AJF) provides a prompting strategy to design jailbreak prompts by hiding the malicious task behind sophisticated benign tasks. The main idea being -- when the model focuses on the complex comprehension and decryption, the safety filters are not triggered and the output ends up answering the malicious prompt. The paper demonstrates that this attack is able to successfully attack latest state of the art models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper highlights an important failure mode of LLMs when trying to solve multiple tasks simultaneously.\n2. The two pronged approach handles both weak and strong models.\n3. Demonstrate the success of the attack against latest models."}, "weaknesses": {"value": "1. The paper's title is a bit misleading. The proposed attack does not seem adaptive against a defense that knows about the attack.\n2. Authors have argued that AJF can successfully evade three types of safeguards: input filtering, internal safeguards, and output filtering. However, the evaluation fails to evaluate the attack along these dimensions. The attack has not been tested against specialized filters such as LlamaGuard or ShieldGemma.\n3. While the additional comprehension and decryption tasks decrease the refusal rate, it might impact the quality of the malicious response.\n4. The considered baselines are outdated (GCG, GPTFUZZER). The authors should compare their attack against more recent and stronger attacks such as GOAT and TAP.\n5. The paper fails to discuss or compare against important related work such as Many-shot jailbreaking by Anil et al, DeepInception by Li et al, and ArtPrompt by xiang et al."}, "questions": {"value": "How will the attack work if the defense is explicitly finetuned for AJF's binary tree syntax?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hZGLu3TOGT", "forum": "bQQkWXYjuy", "replyto": "bQQkWXYjuy", "signatures": ["ICLR.cc/2026/Conference/Submission11378/Reviewer_kQ78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11378/Reviewer_kQ78"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046734152, "cdate": 1762046734152, "tmdate": 1762922501167, "mdate": 1762922501167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}