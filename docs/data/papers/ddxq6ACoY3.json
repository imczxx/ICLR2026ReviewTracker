{"id": "ddxq6ACoY3", "number": 2902, "cdate": 1757299259187, "mdate": 1759898120084, "content": {"title": "MatchingPolicy: Correspondence-aware Policy For Cross-object In-context Learning", "abstract": "In-context imitation learning enables few-shot task generalization by conditioning policies on demonstrations, but existing methods often fail on unseen objects or novel scenarios. We introduce MatchingPolicy, a correspondence-aware framework that explicitly decouples demonstration–scene matching from policy learning. At its core, MatchingPolicy employs a graph-based diffusion policy that adapts robot actions based on dense correspondences extracted by vision foundation models. This separation alleviates the burden of simultaneous correspondence inference and action adaptation, enabling robust transfer across diverse tasks. Our approach further integrates an online adaptive matching algorithm to dynamically establish reliable correspondences during execution. Empirical results on both RLBench and real-world manipulation tasks show that MatchingPolicy achieves strong few-shot performance, demonstrating consistent generalization across unseen object instances and categories.", "tldr": "We propose an in-context imitation learning framework that explicitly leverage semantic correspondence for generalizable robotic manipulation", "keywords": ["Robotic Manipulation; In-context Imitation Learning; Semantic Correspondence"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d423aec6da28ccacd2bd7f125b6627cdc3b4fe12.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **MatchingPolicy** for in-context imitation learning. The authors hypothesize that this failure stems from forcing a single model to simultaneously infer cross-scene correspondences and adapt robot actions. MatchingPolicy consists of a two-stage matching algorithm that leverages off-the-shelf Vision Foundation Models to establish dense semantic correspondences between demonstration frames and the current scene, and a correspondence-aware graph-based diffusion policy that takes these explicit correspondences as input and generates actions. The paper demonstrates that the method outperforms the SOTA baseline (Instant Policy) in simulation (RLBench) and real-world manipulation tasks, and achieves generalization to unseen objects and novel layouts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear motivation**: The paper hypothesizes the problem of existing methods and proposes a corresponding solution.\n2. **Strong empirical result**: The proposed method significantly outperforms the existing method both in simulation and real-world. The real-world experiment addresses the concern of sim-to-real transfer and demonstrates the potential of further applications.\n3. **Novel approach**: The method of combining semantic correspondences and graph-based in-context imitation learning is novel."}, "weaknesses": {"value": "1. **Simulated Data and Simple Primitives**: The policy is trained on a synthetic dataset generated with privileged information (object meshes, viewpoints) in simulations. The tasks themselves are composed of simple, predefined motion primitives. While the model can be fine-tuned, the experiments only show this on simulated data. This raises concerns about scalability to more complex, unstructured real-world tasks that cannot be easily decomposed into simple primitives and do not have privileged perception.\n\n2. **Limitations of Keypoint Representation for Complex Tasks**: As noted in the paper's limitations, the focus is on short-horizon tasks. The abstraction to keypoint correspondences, while powerful for geometric alignment, may fundamentally limit the policy's ability to understand and execute longer, more complex tasks. Such tasks may require reasoning about abstract goals, tool use, or sequential logic that cannot be captured by point-to-point matching. Especially when in-context learning, the contexts are often OOD of the training set.\n\n3. **Constrained Hardware Assumptions**: The high-performance \"two-stage matching\" algorithm relies on a specific multi-camera setup (three cameras with 360 coverage) to resolve pose ambiguity. This is a significant hardware constraint that may limit the method's applicability outside of structured, tabletop environments and makes it less general than a single-camera approach.\n\n4. **Clarity and Organization**: The paper's narrative structure was somewhat difficult to follow. For instance, the policy architecture is detailed before its primary inputs (the correspondences) are fully introduced and explained. The subsections felt disconnected at times, requiring the reader to jump back and forth to construct a complete picture of the method's pipeline."}, "questions": {"value": "1. Could the authors provide qualitative visualizations for the matching results referenced in Figure 5? The paper claims \"more accurate semantic correspondence,\" and visualizing this comparison would be very beneficial. Furthermore, can the authors elaborate on why their matching stage is superior? The distinction of performing projection before or after correspondence matching does not seem so critical.\n\n2. There is prior work [1] using rejection sampling for more robust visual (DINO) and geometry (FPFH) features for keypoint matching. They also achieve cross-object and cross-layout generalization in a low-data regime. How does this keypoint proposal method compare to your method?\n\n3. Given the current task complexity and horizon, is in-context learning really needed? How well will a multitask, keypoint-conditioned policy [1] perform on these tasks in terms of success rate and data efficiency? Or even optimization-based methods [2] with foundation models (VLMs, VFMs) in a zero-shot manner?\n\n4. The paper hypothesizes two problems with existing in-context learning methods. Are there references or experiments to support them?\n\n[1] Fang, Xiaolin, et al. \"KALM: Keypoint Abstraction Using Large Models for Object-Relative Imitation Learning.\" 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025.\n\n[2] Huang, Wenlong, et al. \"Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation.\" Conference on Robot Learning (CoRL). 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vyHbEwfAmq", "forum": "ddxq6ACoY3", "replyto": "ddxq6ACoY3", "signatures": ["ICLR.cc/2026/Conference/Submission2902/Reviewer_Dys6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2902/Reviewer_Dys6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449260754, "cdate": 1761449260754, "tmdate": 1762916436215, "mdate": 1762916436215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MatchingPolicy proposes a novel in-context imitation learning method. They isolate a challenge of in-context learning to the combined difficulty of inferring correspondences between the demo and current scene, and then adapting new actions to the scene. To address this challenge, MatchingPolicy decomposes the correspondence matching between demo and current scene from the action adaptation part. First, a group of representative points from the point cloud are selected, which can be done by taking the center of the semantic segmentation of an object. This allows matching temporally (across different frames) with off-the-shelf models, and also matching between the demo frames and current frames. The actions can extracted from the gripper keypoints. The model is trained using a procedurally generated dataset (pretraining) and a human collected dataset in sim and real (finetuning). The model is a graph-based diffusion model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction presents the idea well.\n2. The experiments section is fairly thorough in describing the different tasks.\n3. The test-time planning ablation seems orthogonal to the thesis of the paper, but it is interesting that Overlap outperforms half so much."}, "weaknesses": {"value": "1. Figure 2 is a bit confusing. More details can be added to the caption explaining the orange and blue lines? Even in the text, I was a bit confused by exactly what the lines meant.\n2. I feel like additional baselines are missing. \n3. The authors could probably explain more about how InstantPolicy is different from MatchingPolicy, especially given that it is the only other baseline."}, "questions": {"value": "1. DINO is mentioned in the intro and never mentioned again. How would you use DINO in this decoupled in-context learning design?\n2. How much training data is generated? \n3. How much finetuning data is collected? How diverse and semantically different is this finetuning dataset?\n4. The selection of points is not super clear to me. How exactly are the points chosen from the pointcloud? \n5. How many diffusion steps are used for the policy during training and inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s7ymQzJrYP", "forum": "ddxq6ACoY3", "replyto": "ddxq6ACoY3", "signatures": ["ICLR.cc/2026/Conference/Submission2902/Reviewer_ScEQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2902/Reviewer_ScEQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674683969, "cdate": 1761674683969, "tmdate": 1762916435901, "mdate": 1762916435901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MatchingPolicy, an approach for in-context imitation learning for robotic manipulation that decouples demonstration-to-scene matching with a correspondence-aware graph-based diffusion policy. To deploy in the real world, the authors introduce a two-stage feature matching algorithm using VLMs for real-time correspondence extraction. They identify the optimal alignment between the testing and demonstration view, perform 2d feature point matching, project into 3d point cloud, and then apply farthest point sampling to obtain representative keypoints with encoded geometric features for selected points in local neighborhoods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper studies an important topic of how to leverage in-context learning for robotic policy learning. The proposed approach seems to drastically outperform the prior baseline on in-context imitation learning [Vosylius and Johns, 2024] in unseen tasks, demonstrating strong generalization. The real-world results are promising, given that the policy is only provided 1-2 in-context demonstrations and trained entirely in simulation. Particularly as the simulation tasks are [pick-and-place, grasping, push-pull, random path following], but the experimental results show that the policy is able to perform pouring, opening, and cutting tasks in the real-world without additional fine-tuning.\n\n- The paper is generally well-written and easy to follow."}, "weaknesses": {"value": "- For Appendix B Figure 10, it looks like the demonstrations show a human hand guiding the robot for kinesthetic teaching. In that case, I may expect that the performance gain from MatchingPolicy comes from effectively masking out the human demonstrator’s hand in the visual information for the policy. Given a teleoperation setup such as ALOHA or GELLO to enable directly collecting demonstrations in the scene without interference, how well would the baseline InstantPolicy perform?\n\n- Only a single in-context imitation learning baseline (InstantPolicy) is used for comparison. How does MatchingPolicy fare against zero-shot approaches that use VLMs to extract representations for real-world robot manipulation, such as ReKep [1] or Voxposer [2]?\n\n- Regarding simulation results on RLBench, a standard robotics benchmark, it would be helpful to compare against a wider range of baselines beyond a single in-context imitation learning baseline, particularly on “Unseen Tasks”. How does MatchingPolicy perform compared to VLAs? \n\n- The real-world evaluation tasks only include at most two objects in the scene. How well does MatchingPolicy work when there are other objects in the scene that may be distractors? How about when there are more than two objects involved in a manipulation interaction? For instance, picking up a plate with multiple utensils on it.\n\n- The proposed approach requires 3 cameras in a 360deg setup to capture the entire 3d scene and resolve pose ambiguity.\n\n[1] https://arxiv.org/abs/2409.01652\n\n[2] https://arxiv.org/abs/2307.05973"}, "questions": {"value": "Figure 4: Where are the visualizations of the “Cut Egg” task?\n\nHow does MatchingPolicy perform with more than 2 in-context demonstrations?\n\n[minor]\n\nL199-201: This sentence is difficult to understand, what are “in demonstration frames”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/a"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2RZ6Pi7SL9", "forum": "ddxq6ACoY3", "replyto": "ddxq6ACoY3", "signatures": ["ICLR.cc/2026/Conference/Submission2902/Reviewer_iphX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2902/Reviewer_iphX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941815302, "cdate": 1761941815302, "tmdate": 1762916435709, "mdate": 1762916435709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MatchingPolicy, a new framework for ICIL, that explicitly decouples correspondence extraction from policy learning. Rather than simply throwing the keypoints from demonstrations and the ongoing rollout into a GNN diffusion policy, the proposed method explicitly connects similar nodes (grippers and scene nodes in ongoing rollouts to those in the demos) before feeding it into the GNN diffusion policy. This explicit representation connexting the state and demonstrations makes it easier for the GNN diffusion policy to predict relevant actions. These direct correspondences are obtained using off-the-shelf models. The method might be seen as InstantPolicy + more explicit representations. This results in improved generalization capabilities to unseen tasks in both sim (RLBench) and real (with novel objects, layouts, etc)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Explicit representations were very essential to meta learning in supervised setting, including but not limited to Matching Networks, ProtoNets, and SimpleShot. I am very excited by a similar approach to meta policy learning. \n\nThe results on generalization to new objects, new layouts, and in \"cross-category\" generalization are very interesting.\n\nThe significant improvements over instant-policy are very nice to observe. The ablations are helpful in establishing the effectiveness of the matching procedure."}, "weaknesses": {"value": "Missing related work with similar (or atleast relevant) high-level ideas: A few highly relevant papers in in-context imitation learning are missing from the related work, and possibly from the baselines. The following papers [1, 2, 3] focus on generalization to novel objects, tasks, and environments. In fact, both RICL and REGENT follow a RAG approach that also decouples matching and acting.\n\n[1] RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models, CoRL 2025\n\n[2] REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context In New Environments, ICLR 2025\n\n[3] Generalization to New Sequential Decision Making Tasks with In-Context Learning, ICML 2024\n\nAre other baselines in the keypoint ICIL methods such as KAT [4] necessary in this context? If so, they are currently not mentioned in the paper.\n\n[4] Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics, RSS 2024"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zEnPNNHJ4J", "forum": "ddxq6ACoY3", "replyto": "ddxq6ACoY3", "signatures": ["ICLR.cc/2026/Conference/Submission2902/Reviewer_ejwA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2902/Reviewer_ejwA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173284911, "cdate": 1762173284911, "tmdate": 1762916435530, "mdate": 1762916435530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}